{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tCreate a new directory, Exercise01.01 in the Chapter01 directory to store the files for this exercise.\n",
    "\n",
    "2.\tOpen your terminal (macOS or Linux) or command window (Windows), navigate to Chapter01 directory, and type `jupyter notebook`.\n",
    "\n",
    "3.\tIn the Jupyter Notebook, click the Exercise01.01 directory and create a new notebook file with Python3 kernel.\n",
    "\n",
    "4. Read in the dataset file and check how big it is as shown in the following code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ../../Datasets/clickbait-headlines.tsv \n",
      "Size: 0.55 MBs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dataset_filename = \"../../Datasets/clickbait-headlines.tsv\"\n",
    "\n",
    "print(\"File: {} \\nSize: {} MBs\".format(dataset_filename, round(os.path.getsize(dataset_filename)/1024/1024, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the `os` libary from Python which is a standard library for running Operating System level commands. \n",
    "\n",
    "We defined the path to the dataset file as the variable `dataset_filename`.\n",
    "\n",
    "We printed out the size of the file using the `os` library and the `getsize()` function, seeing in the output that the file is less than one megabyte in size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Read the contents of the file form disk and split each line into a data and label component, as shown in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Egypt's top envoy in Iraq confirmed killed\", 'Carter: Race relations in Palestine are worse than apartheid', 'After Years Of Dutiful Service, The Shiba Who Ran A Tobacco Shop Retires']\n",
      "['0', '0', '1']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "with open(dataset_filename) as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for line in reader:\n",
    "        try:\n",
    "            data.append(line[0])\n",
    "            labels.append(line[1])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "\n",
    "print(data[:3])\n",
    "print(labels[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the CSV Python library which is useful for processing our file, which is in tab seprated file (TSV) format.. \n",
    "\n",
    "We define two empty arrays, `data` and `labels`.\n",
    "\n",
    "We open the file, create a CSV reader, and indicate what kind of delimeter (\"\\t\", or a tab characters) is used. We then loop through each line of the file, and the first element to the data array and the second element to the labels array. If anything goes wrong, we print out an error message to indicate this.\n",
    "\n",
    "Finally, we print out the first three elements of each of our arrays. They match up, so the first element in our data array is linked to the first element in our labels array. From the output we see that the first two elements are '0' or 'not clickbait' while the last element is identified as '1', indicating a clickbait headline.\n",
    "\n",
    "6. Create vectors from our text data using the sklearn library, while showing how long it takes, as shown in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of our vectors:\n",
      "(10000, 13169)\n",
      "- - -\n",
      "CPU times: user 865 ms, sys: 176 ms, total: 1.04 s\n",
      "Wall time: 1.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(data)\n",
    "print(\"The dimensions of our vectors:\")\n",
    "print(vectors.shape)\n",
    "print(\"- - -\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line is a special Jupyter Notebook command saying that the code should output the total time taken. Then we import a TfidfVectorizer from the `sklearn` library. We initialise a vectorizer call the `fit_transform()` function which assigns each word to an index and creates the resulting vectors from the text data in a single step. \n",
    "\n",
    "Then we print out the shape of the vectors, noticing that they are 10000 rows (the number of headlines) by 13169 columns (the number of unique words across all headlines).\n",
    "\n",
    "We can see from the timing output below that it took a total of around 200 ms to run this code.\n",
    "\n",
    "7. check how much memory our vectors are taking up in their sparse format and compare this to how much space they would have used if we had used a dense format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data type of our vectors\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "- - -\n",
      "The size of our vectors (MB):\n",
      "0.6759414672851562\n",
      "- - -\n",
      "The size of our vectors in dense format (MB):\n",
      "1004.7149658203125\n",
      "- - - \n",
      "Number of non zero elements in our vectors\n",
      "88597\n",
      "- - -\n"
     ]
    }
   ],
   "source": [
    "print(\"The data type of our vectors\")\n",
    "print(type(vectors))\n",
    "print(\"- - -\")\n",
    "print(\"The size of our vectors (MB):\")\n",
    "print(vectors.data.nbytes/1024/1024)\n",
    "print(\"- - -\")\n",
    "print(\"The size of our vectors in dense format (MB):\")\n",
    "print(vectors.todense().nbytes/1024/1024)\n",
    "print(\"- - - \")\n",
    "print(\"Number of non zero elements in our vectors\")\n",
    "print(vectors.nnz)\n",
    "print(\"- - -\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We printed out the type of the vectors and saw that this was `csr_matrix` or a \"compressed sparse row matrix\", which is the default data structure used by sklearn for vectors. In memory this takes up only 0.68 MB of space. Next we call the `todense()` function which converts the datastructure to a standard dense matrix. We check the size again to see it is now over 1 GB in size. \n",
    "\n",
    "Finally, we output the `nnz` (number of non-zero elements) and see that there were around 88 thousand non-zero elements that we store. Because we had 10000 rows and 13169 columns, the total number of elements is 131 690 000, which is why the dense matrix is so much larger in memory use.\n",
    "\n",
    "8. For machine learning we need to split our data into a train portion for training and a test portion to evaluate how good our model is. Do this with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 13169)\n",
      "(2000, 13169)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectors, labels, test_size=0.2)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We imported the `train_test_split` function from sklearn and split our two arrays (vectors and labels) into four arrays (X_train, X_test, y_train, y_test). The `y` prefix indicates labels and the `X` prefix indicates vectorized data. We use the argument `test_size=0.2` to indicate that we want 20% of our data held back for testing.\n",
    "\n",
    "We then print out each shape to show that 80% (8000) headlines are in the training set and that 20% (2000) headlines are in the test set. Because each dataset was vectorized at the same time, each still has 13169 dimensions or possible words.\n",
    "\n",
    "9. Initialise the SVC classifier, train it, and generate predictions with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.7 ms, sys: 7.66 ms, total: 44.4 ms\n",
      "Wall time: 50.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm_classifier = LinearSVC()\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "predictions = svm_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the `LinearSVC` model from `sklearn`, and initialise an instance of it. Then we give it the training data and training labels (note that it does not have access to the test data at this stage). \n",
    "\n",
    "Finally we give it the testing data, but without the testing labels, and ask it to guess which of the headlines in the held-out test set are clickbait. We call these `predictions`.\n",
    "\n",
    "To get some insight into what is happening, let's take a look at some of these predictions and compare them to the real labels.\n",
    "\n",
    "10. Output the first 10 headlines along with their predicted class and true class by running the following code.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction, label\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n"
     ]
    }
   ],
   "source": [
    "print(\"prediction, label\")\n",
    "for i in range(10):\n",
    "    print(y_test[i], predictions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for the first 10 cases, our predictions were spot on. Let's see how we did over all the test cases.\n",
    "\n",
    "11. Evaluate how well the model performed using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"Accuracy: {}\\n\".format(accuracy_score(y_test, predictions)))\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieve 97% accuracy, which means around 1940 of the 2000 test cases were correctly classified by our model. This is a good summary score, but for a fuller picture we then print out the full classification report. The model can be wrong in different ways: either classifying a clickbait headline as normal, or by classifying a normal healdine as clickbait. Because the precision and recall scores are similar, we can confirm that the model is not biased towards a specific kind of mistake."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
