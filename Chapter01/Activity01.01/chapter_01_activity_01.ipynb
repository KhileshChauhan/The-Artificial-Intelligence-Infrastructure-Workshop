{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this activity, we will create another text classifier. Instead of training a machine learning model to discriminate between clickbait headlines and normal headlines, we will train a similar classifier to discriminate between positive and negative movie reviews.\n",
    "\n",
    "The objectives for our activity are \n",
    "* Vectorize the text of IMDB movie reviews and label these as Positive or Negative \n",
    "* Train an SVM classifier to predict whether a move review is Positive or Negative\n",
    "* Check how accurate our classifier is on a held-out test set\n",
    "* Evaluate our classifier on out-of-context data\n",
    "\n",
    "NOTE: as we will be using some randomizers in this activity, it is helpful to set the global random seeds to ensure that the results you see are the same as in the examples. Sklearn uses the numpy random seed, and we will also use the `shuffle` function from the built in random library. You can ensure you see the same results by adding the following code above your main code.\n",
    "\n",
    "\n",
    "```\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(1337)\n",
    "np.random.seed(1337)\n",
    "```\n",
    "\n",
    "We'll use the aclIMDB dataset of 100k movie reviews from IMDB, 50k each for training and testing. Each dataset has 25k positive reivews and 25k negative ones, so this is a larger dataset than our headlines one. The dataset is available in the Datasets folder in the `aclImdb` folder.\n",
    "\n",
    "In exercise 1, we had one file, with each line representing a different data item. Now each data item is a bit longer and in its own file, so keep in mind that we'll need to restructure some of our training code accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "random.seed(1337)\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the `os` library and `random` library, and define where our training and test data is stored using four variables: one for training_positive, one for training_negative, one for test_positive and one for test_negative, each pointing at the respective dataset sub directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "dataset_train_pos_path = \"../../Datasets/aclImdb/train/pos/\"\n",
    "dataset_train_neg_path = \"../../Datasets/aclImdb/train/neg/\"\n",
    "\n",
    "dataset_test_pos_path = \"../../Datasets/aclImdb/test/pos/\"\n",
    "dataset_test_neg_path = \"../../Datasets/aclImdb/test/neg/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. define a `read_dataset` function which takes a path to a dataset and a label (either \"pos\" or \"neg\"), reads the contents of each file in the given directory, and adds these contents into a datastructure that is a list of tuples, where each tuple contains both the text of the file, and the label 'pos' or 'neg'. An example is below. The actual data should be read off disk instead of defined in code.\n",
    "\n",
    "```\n",
    "contents_labels = [('this is the text from one of the files', 'pos'), ('this is another text', 'pos')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(dataset_path, label):\n",
    "    contents_labels = []\n",
    "    files = os.listdir(dataset_path)\n",
    "    for fn in files:\n",
    "        path = os.path.join(dataset_path, fn)\n",
    "        with open(path) as f:\n",
    "            s = f.read()\n",
    "            contents_labels.append((s, label))\n",
    "    return contents_labels   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the function you defined above to read each dataset into its own variable. You should have four variables in total: `train_pos`, `train_neg`, `test_pos`, and `test_neg`, each one of which is a list of tuples, containing the relative text and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = read_dataset(dataset_train_pos_path, \"pos\")\n",
    "train_neg = read_dataset(dataset_train_neg_path, \"neg\")\n",
    "\n",
    "test_pos = read_dataset(dataset_test_pos_path, \"pos\")\n",
    "test_neg = read_dataset(dataset_test_neg_path, \"neg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Combine the train_pos and train_neg datasets. Do the same for the test_pos and test_neg ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_pos + train_neg\n",
    "test = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Use the `random.shuffle` function to shuffle the train and test datasets separately. This gives us datasets where the training data is mixed up, instead of feeding all the positive and then all the negative examples to the classifier in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train)\n",
    "random.shuffle(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Split each of the train and test datasets back into data and labels respectively. You should have four variables again called `train_data`, `y_train`, `test_data`, and `y_test` where the `y` prefix indicates that the respective array contains labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, y_train = zip(*train)\n",
    "test_data, y_test = zip(*test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Import `TfidfVectorizer` from sklearn, initialise an instance of it, fit the vectorizer on the training data, and vectorize both the training and testing data into `X_train` and `X_test` variables respectively. Time how long this takes and print out the shape of the training vectors at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of our vectors:\n",
      "(25000, 74849)\n",
      "- - -\n",
      "CPU times: user 12.5 s, sys: 441 ms, total: 12.9 s\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data)\n",
    "X_test = vectorizer.transform(test_data)\n",
    "print(\"The dimensions of our vectors:\")\n",
    "print(X_train.shape)\n",
    "print(\"- - -\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Again timing your execution time, import `LinearSVC` from `sklearn` and initialise and instance of it. Fit the SVM on the training data and training labels, and then generate predictions on the test data (`X_test`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 750 ms, sys: 75.1 ms, total: 825 ms\n",
      "Wall time: 923 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm_classifier = LinearSVC()\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "predictions = svm_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Import `accuracy_score` and `classification_report` from sklearn and calculate the results of your predictions using each. How accurate was the classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8772\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.87      0.89      0.88     12500\n",
      "         pos       0.89      0.87      0.88     12500\n",
      "\n",
      "    accuracy                           0.88     25000\n",
      "   macro avg       0.88      0.88      0.88     25000\n",
      "weighted avg       0.88      0.88      0.88     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"Accuracy: {}\\n\".format(accuracy_score(y_test, predictions)))\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how your classifier performs on data in different topics. Create two restaurant reviews. For example:\n",
    "```\n",
    "good_review = \"The restaurant was really great! I ate wonderful food and had a very good time\"\n",
    "bad_review = \"The restuarant was awful. The staff were rude and the food was horrible. I hated it\"\n",
    "```\n",
    "\n",
    "Now vectorize each using the same vectorizer and generate predictions for whether each one is negative or positive. Did you classifier guess correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos' 'neg']\n"
     ]
    }
   ],
   "source": [
    "good_review = \"The restaurant was really great! I ate wonderful food and had a very good time\"\n",
    "bad_review = \"The restuarant was awful. The staff were rude and the food was horrible. I hated it\"\n",
    "\n",
    "restuarant_reviews = [good_review, bad_review]\n",
    "vectors = vectorizer.transform(restuarant_reviews)\n",
    "print(svm_classifier.predict(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
