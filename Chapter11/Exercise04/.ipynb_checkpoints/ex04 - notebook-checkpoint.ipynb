{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kevin/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10f4b45f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import module\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "\n",
    "# make game\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# seed the experiment\n",
    "env.seed(9)\n",
    "torch.manual_seed(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define util function\n",
    "def to_torch_tensor(np_arr):\n",
    "    return torch.from_numpy(np_arr).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our policy\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(Policy, self).__init__()\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.fc1 = nn.Linear(self.observation_space, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3 = nn.Linear(24, self.action_space)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "# define our agent\n",
    "class Agent:\n",
    "    def __init__(self, policy):\n",
    "        MEMORY_SIZE = 1000000\n",
    "        GAMMA = 0.95\n",
    "        BATCH_SIZE = 20\n",
    "        EXPLORATION_MAX = 1.0\n",
    "        EXPLORATION_MIN = 0.01\n",
    "        EXPLORATION_DECAY = 0.995\n",
    "\n",
    "        self.policy = policy\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-2)\n",
    "        self.loss_fn = nn.MSELoss(reduction='mean')\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        self.gamma = GAMMA\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.exploration_min = EXPLORATION_MIN\n",
    "        self.exploration_decay = EXPLORATION_DECAY\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.policy.action_space)\n",
    "        q_values = self.policy(to_torch_tensor(state))\n",
    "        return int(q_values.max(0)[-1])\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        print(\"[ Experience replay ] starts\")\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, state_next, done in batch:\n",
    "            if not done:\n",
    "                # Q function (bellman eqution): q value = reward at current step + gamma * q value of next step by taking an optimal action\n",
    "                q_value_to_update = (reward + self.gamma * torch.max(self.policy(to_torch_tensor(state_next))))\n",
    "                # remove this tensor from the autograph\n",
    "                q_value_to_update = q_value_to_update.clone().detach()\n",
    "            else:\n",
    "                q_value_to_update = reward\n",
    "            q_values_hat = self.policy(to_torch_tensor(state))\n",
    "            # generate target\n",
    "            q_values_target = q_values_hat.clone().detach()\n",
    "            q_values_target[action] = q_value_to_update\n",
    "            # train policy            \n",
    "            policy_loss = self.loss_fn(q_values_target, q_values_hat)\n",
    "            policy_loss.backward()    \n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        # the more policy gets replayed, the less the agent explores\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        self.exploration_rate = max(self.exploration_min, self.exploration_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 1 ] state=[-0.00551277  0.02101743  0.00884103  0.02545213]\n",
      "[ episode 1 ][ timestamp 1 ] state=[-0.00551277  0.02101743  0.00884103  0.02545213], action=1, reward=1.0, next_state=[-0.00509242  0.21601149  0.00935007 -0.26442829]\n",
      "[ episode 1 ][ timestamp 2 ] state=[-0.00509242  0.21601149  0.00935007 -0.26442829], action=0, reward=1.0, next_state=[-0.00077219  0.02075734  0.0040615   0.03118901]\n",
      "[ episode 1 ][ timestamp 3 ] state=[-0.00077219  0.02075734  0.0040615   0.03118901], action=1, reward=1.0, next_state=[-0.00035704  0.21582081  0.00468528 -0.2602097 ]\n",
      "[ episode 1 ][ timestamp 4 ] state=[-0.00035704  0.21582081  0.00468528 -0.2602097 ], action=1, reward=1.0, next_state=[ 3.95937136e-03  4.10875563e-01 -5.18910048e-04 -5.51411155e-01]\n",
      "[ episode 1 ][ timestamp 5 ] state=[ 3.95937136e-03  4.10875563e-01 -5.18910048e-04 -5.51411155e-01], action=0, reward=1.0, next_state=[ 0.01217688  0.2157609  -0.01154713 -0.25889176]\n",
      "[ episode 1 ][ timestamp 6 ] state=[ 0.01217688  0.2157609  -0.01154713 -0.25889176], action=1, reward=1.0, next_state=[ 0.0164921   0.41104578 -0.01672497 -0.55519434]\n",
      "[ episode 1 ][ timestamp 7 ] state=[ 0.0164921   0.41104578 -0.01672497 -0.55519434], action=0, reward=1.0, next_state=[ 0.02471302  0.21616261 -0.02782886 -0.26782737]\n",
      "[ episode 1 ][ timestamp 8 ] state=[ 0.02471302  0.21616261 -0.02782886 -0.26782737], action=0, reward=1.0, next_state=[ 0.02903627  0.02144864 -0.0331854   0.01594987]\n",
      "[ episode 1 ][ timestamp 9 ] state=[ 0.02903627  0.02144864 -0.0331854   0.01594987], action=1, reward=1.0, next_state=[ 0.02946524  0.21703041 -0.03286641 -0.28701597]\n",
      "[ episode 1 ][ timestamp 10 ] state=[ 0.02946524  0.21703041 -0.03286641 -0.28701597], action=0, reward=1.0, next_state=[ 0.03380585  0.0223922  -0.03860672 -0.00487733]\n",
      "[ episode 1 ][ timestamp 11 ] state=[ 0.03380585  0.0223922  -0.03860672 -0.00487733], action=1, reward=1.0, next_state=[ 0.03425369  0.21804595 -0.03870427 -0.30948683]\n",
      "[ episode 1 ][ timestamp 12 ] state=[ 0.03425369  0.21804595 -0.03870427 -0.30948683], action=1, reward=1.0, next_state=[ 0.03861461  0.41369738 -0.04489401 -0.61412038]\n",
      "[ episode 1 ][ timestamp 13 ] state=[ 0.03861461  0.41369738 -0.04489401 -0.61412038], action=0, reward=1.0, next_state=[ 0.04688856  0.21923055 -0.05717642 -0.33590846]\n",
      "[ episode 1 ][ timestamp 14 ] state=[ 0.04688856  0.21923055 -0.05717642 -0.33590846], action=1, reward=1.0, next_state=[ 0.05127317  0.41511761 -0.06389458 -0.64605961]\n",
      "[ episode 1 ][ timestamp 15 ] state=[ 0.05127317  0.41511761 -0.06389458 -0.64605961], action=0, reward=1.0, next_state=[ 0.05957552  0.22094144 -0.07681578 -0.37416191]\n",
      "[ episode 1 ][ timestamp 16 ] state=[ 0.05957552  0.22094144 -0.07681578 -0.37416191], action=1, reward=1.0, next_state=[ 0.06399435  0.41706572 -0.08429902 -0.69004245]\n",
      "[ episode 1 ][ timestamp 17 ] state=[ 0.06399435  0.41706572 -0.08429902 -0.69004245], action=0, reward=1.0, next_state=[ 0.07233567  0.22320847 -0.09809986 -0.42504374]\n",
      "[ episode 1 ][ timestamp 18 ] state=[ 0.07233567  0.22320847 -0.09809986 -0.42504374], action=0, reward=1.0, next_state=[ 0.07679984  0.02960303 -0.10660074 -0.16482697]\n",
      "[ episode 1 ][ timestamp 19 ] state=[ 0.07679984  0.02960303 -0.10660074 -0.16482697], action=0, reward=1.0, next_state=[ 0.0773919  -0.16384424 -0.10989728  0.0924155 ]\n",
      "[ episode 1 ][ timestamp 20 ] state=[ 0.0773919  -0.16384424 -0.10989728  0.0924155 ], action=1, reward=1.0, next_state=[ 0.07411501  0.03266726 -0.10804897 -0.23281834]\n",
      "[ Experience replay ] starts\n",
      "[ episode 1 ][ timestamp 21 ] state=[ 0.07411501  0.03266726 -0.10804897 -0.23281834], action=0, reward=1.0, next_state=[ 0.07476836 -0.16075825 -0.11270533  0.02392334]\n",
      "[ Experience replay ] starts\n",
      "[ episode 1 ][ timestamp 22 ] state=[ 0.07476836 -0.16075825 -0.11270533  0.02392334], action=0, reward=1.0, next_state=[ 0.07155319 -0.35409862 -0.11222687  0.27902865]\n",
      "[ Experience replay ] starts\n",
      "[ episode 1 ][ timestamp 23 ] state=[ 0.07155319 -0.35409862 -0.11222687  0.27902865], action=1, reward=1.0, next_state=[ 0.06447122 -0.15756952 -0.1066463  -0.04683599]\n",
      "[ Experience replay ] starts\n",
      "[ episode 1 ][ timestamp 24 ] state=[ 0.06447122 -0.15756952 -0.1066463  -0.04683599], action=1, reward=1.0, next_state=[ 0.06131983  0.03890721 -0.10758301 -0.37117132]\n",
      "[ Experience replay ] starts\n",
      "[ episode 1 ][ timestamp 25 ] state=[ 0.06131983  0.03890721 -0.10758301 -0.37117132], action=1, reward=1.0, next_state=[ 0.06209797  0.23538    -0.11500644 -0.69574508]\n",
      "[ Experience replay ] starts\n",
      "[ episode 1 ][ timestamp 26 ] state=[ 0.06209797  0.23538    -0.11500644 -0.69574508], action=1, reward=1.0, next_state=[ 0.06680557  0.43189327 -0.12892134 -1.02230515]\n",
      "[ Experience replay ] starts\n",
      "[ episode 1 ][ timestamp 27 ] state=[ 0.06680557  0.43189327 -0.12892134 -1.02230515], action=0, reward=1.0, next_state=[ 0.07544344  0.23870238 -0.14936745 -0.77272169]\n",
      "[ Experience replay ] starts\n",
      "[ episode 1 ][ timestamp 28 ] state=[ 0.07544344  0.23870238 -0.14936745 -0.77272169], action=0, reward=1.0, next_state=[ 0.08021749  0.04591656 -0.16482188 -0.53051376]\n",
      "[ Experience replay ] starts\n",
      "[ episode 1 ][ timestamp 29 ] state=[ 0.08021749  0.04591656 -0.16482188 -0.53051376], action=0, reward=1.0, next_state=[ 0.08113582 -0.14655011 -0.17543215 -0.29396486]\n",
      "[ Experience replay ] starts\n",
      "[ episode 1 ][ timestamp 30 ] state=[ 0.08113582 -0.14655011 -0.17543215 -0.29396486], action=1, reward=1.0, next_state=[ 0.07820482  0.05058238 -0.18131145 -0.63643788]\n",
      "[ Experience replay ] starts\n",
      "[ episode 1 ][ timestamp 31 ] state=[ 0.07820482  0.05058238 -0.18131145 -0.63643788], action=0, reward=1.0, next_state=[ 0.07921646 -0.14160991 -0.19404021 -0.40588903]\n",
      "[ Experience replay ] starts\n",
      "[ episode 1 ][ timestamp 32 ] state=[ 0.07921646 -0.14160991 -0.19404021 -0.40588903], action=1, reward=1.0, next_state=[ 0.07638426  0.05565785 -0.20215799 -0.75292806]\n",
      "[ Experience replay ] starts\n",
      "[ episode 1 ][ timestamp 33 ] state=[ 0.07638426  0.05565785 -0.20215799 -0.75292806], action=1, reward=-1.0, next_state=[ 0.07749742  0.2529078  -0.21721655 -1.1018081 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 1: Exploration_rate=0.9322301194154049. Score=33.\n",
      "[ episode 2 ] state=[ 0.04526127 -0.00615054 -0.03334226  0.00441591]\n",
      "[ episode 2 ][ timestamp 1 ] state=[ 0.04526127 -0.00615054 -0.03334226  0.00441591], action=0, reward=1.0, next_state=[ 0.04513826 -0.20077884 -0.03325395  0.28639529]\n",
      "[ Experience replay ] starts\n",
      "[ episode 2 ][ timestamp 2 ] state=[ 0.04513826 -0.20077884 -0.03325395  0.28639529], action=0, reward=1.0, next_state=[ 0.04112268 -0.39541115 -0.02752604  0.56840749]\n",
      "[ Experience replay ] starts\n",
      "[ episode 2 ][ timestamp 3 ] state=[ 0.04112268 -0.39541115 -0.02752604  0.56840749], action=0, reward=1.0, next_state=[ 0.03321446 -0.59013642 -0.01615789  0.85229311]\n",
      "[ Experience replay ] starts\n",
      "[ episode 2 ][ timestamp 4 ] state=[ 0.03321446 -0.59013642 -0.01615789  0.85229311], action=1, reward=1.0, next_state=[ 0.02141173 -0.39479796  0.00088797  0.55457346]\n",
      "[ Experience replay ] starts\n",
      "[ episode 2 ][ timestamp 5 ] state=[ 0.02141173 -0.39479796  0.00088797  0.55457346], action=1, reward=1.0, next_state=[ 0.01351577 -0.19968849  0.01197944  0.26217043]\n",
      "[ Experience replay ] starts\n",
      "[ episode 2 ][ timestamp 6 ] state=[ 0.01351577 -0.19968849  0.01197944  0.26217043], action=0, reward=1.0, next_state=[ 0.009522   -0.39497938  0.01722285  0.55860761]\n",
      "[ Experience replay ] starts\n",
      "[ episode 2 ][ timestamp 7 ] state=[ 0.009522   -0.39497938  0.01722285  0.55860761], action=0, reward=1.0, next_state=[ 0.00162241 -0.5903388   0.028395    0.85666655]\n",
      "[ Experience replay ] starts\n",
      "[ episode 2 ][ timestamp 8 ] state=[ 0.00162241 -0.5903388   0.028395    0.85666655], action=1, reward=1.0, next_state=[-0.01018436 -0.39561501  0.04552833  0.57304562]\n",
      "[ Experience replay ] starts\n",
      "[ episode 2 ][ timestamp 9 ] state=[-0.01018436 -0.39561501  0.04552833  0.57304562], action=1, reward=1.0, next_state=[-0.01809666 -0.20115999  0.05698925  0.29504605]\n",
      "[ Experience replay ] starts\n",
      "[ episode 2 ][ timestamp 10 ] state=[-0.01809666 -0.20115999  0.05698925  0.29504605], action=0, reward=1.0, next_state=[-0.02211986 -0.39704615  0.06289017  0.60514403]\n",
      "[ Experience replay ] starts\n",
      "[ episode 2 ][ timestamp 11 ] state=[-0.02211986 -0.39704615  0.06289017  0.60514403], action=0, reward=1.0, next_state=[-0.03006079 -0.59298857  0.07499305  0.91695415]\n",
      "[ Experience replay ] starts\n",
      "[ episode 2 ][ timestamp 12 ] state=[-0.03006079 -0.59298857  0.07499305  0.91695415], action=1, reward=1.0, next_state=[-0.04192056 -0.39895634  0.09333213  0.64875114]\n",
      "[ Experience replay ] starts\n",
      "[ episode 2 ][ timestamp 13 ] state=[-0.04192056 -0.39895634  0.09333213  0.64875114], action=1, reward=1.0, next_state=[-0.04989968 -0.20524999  0.10630715  0.38685603]\n",
      "[ Experience replay ] starts\n",
      "[ episode 2 ][ timestamp 14 ] state=[-0.04989968 -0.20524999  0.10630715  0.38685603], action=1, reward=1.0, next_state=[-0.05400468 -0.01178506  0.11404427  0.12949236]\n",
      "[ Experience replay ] starts\n",
      "[ episode 2 ][ timestamp 15 ] state=[-0.05400468 -0.01178506  0.11404427  0.12949236], action=0, reward=1.0, next_state=[-0.05424039 -0.20834032  0.11663412  0.4558664 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 2 ][ timestamp 16 ] state=[-0.05424039 -0.20834032  0.11663412  0.4558664 ], action=1, reward=1.0, next_state=[-0.05840719 -0.01504389  0.12575145  0.20210438]\n",
      "[ Experience replay ] starts\n",
      "[ episode 2 ][ timestamp 17 ] state=[-0.05840719 -0.01504389  0.12575145  0.20210438], action=0, reward=1.0, next_state=[-0.05870807 -0.21171898  0.12979354  0.53166107]\n",
      "[ Experience replay ] starts\n",
      "[ episode 2 ][ timestamp 18 ] state=[-0.05870807 -0.21171898  0.12979354  0.53166107], action=1, reward=1.0, next_state=[-0.06294245 -0.01863859  0.14042676  0.28252885]\n",
      "[ Experience replay ] starts\n",
      "[ episode 2 ][ timestamp 19 ] state=[-0.06294245 -0.01863859  0.14042676  0.28252885], action=0, reward=1.0, next_state=[-0.06331522 -0.21545507  0.14607733  0.61599739]\n",
      "[ Experience replay ] starts\n",
      "[ episode 2 ][ timestamp 20 ] state=[-0.06331522 -0.21545507  0.14607733  0.61599739], action=0, reward=1.0, next_state=[-0.06762432 -0.41228329  0.15839728  0.95088944]\n",
      "[ Experience replay ] starts\n",
      "[ episode 2 ][ timestamp 21 ] state=[-0.06762432 -0.41228329  0.15839728  0.95088944], action=0, reward=1.0, next_state=[-0.07586999 -0.60914155  0.17741507  1.28885455]\n",
      "[ Experience replay ] starts\n",
      "[ episode 2 ][ timestamp 22 ] state=[-0.07586999 -0.60914155  0.17741507  1.28885455], action=0, reward=1.0, next_state=[-0.08805282 -0.80602012  0.20319216  1.63142369]\n",
      "[ Experience replay ] starts\n",
      "[ episode 2 ][ timestamp 23 ] state=[-0.08805282 -0.80602012  0.20319216  1.63142369], action=1, reward=-1.0, next_state=[-0.10417322 -0.6137815   0.23582064  1.40832631]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 2: Exploration_rate=0.8307187014821328. Score=23.\n",
      "[ episode 3 ] state=[-0.00796233  0.00449342 -0.04318868  0.00635115]\n",
      "[ episode 3 ][ timestamp 1 ] state=[-0.00796233  0.00449342 -0.04318868  0.00635115], action=0, reward=1.0, next_state=[-0.00787246 -0.18998339 -0.04306165  0.28510082]\n",
      "[ Experience replay ] starts\n",
      "[ episode 3 ][ timestamp 2 ] state=[-0.00787246 -0.18998339 -0.04306165  0.28510082], action=0, reward=1.0, next_state=[-0.01167213 -0.38446557 -0.03735964  0.56389746]\n",
      "[ Experience replay ] starts\n",
      "[ episode 3 ][ timestamp 3 ] state=[-0.01167213 -0.38446557 -0.03735964  0.56389746], action=0, reward=1.0, next_state=[-0.01936144 -0.57904396 -0.02608169  0.8445802 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 3 ][ timestamp 4 ] state=[-0.01936144 -0.57904396 -0.02608169  0.8445802 ], action=0, reward=1.0, next_state=[-0.03094232 -0.77380049 -0.00919008  1.12894848]\n",
      "[ Experience replay ] starts\n",
      "[ episode 3 ][ timestamp 5 ] state=[-0.03094232 -0.77380049 -0.00919008  1.12894848], action=0, reward=1.0, next_state=[-0.04641833 -0.96880087  0.01338889  1.41873486]\n",
      "[ Experience replay ] starts\n",
      "[ episode 3 ][ timestamp 6 ] state=[-0.04641833 -0.96880087  0.01338889  1.41873486], action=0, reward=1.0, next_state=[-0.06579435 -1.16408596  0.04176358  1.71557246]\n",
      "[ Experience replay ] starts\n",
      "[ episode 3 ][ timestamp 7 ] state=[-0.06579435 -1.16408596  0.04176358  1.71557246], action=1, reward=1.0, next_state=[-0.08907607 -0.9694672   0.07607503  1.4361738 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 3 ][ timestamp 8 ] state=[-0.08907607 -0.9694672   0.07607503  1.4361738 ], action=0, reward=1.0, next_state=[-0.10846541 -1.16544021  0.10479851  1.75162758]\n",
      "[ Experience replay ] starts\n",
      "[ episode 3 ][ timestamp 9 ] state=[-0.10846541 -1.16544021  0.10479851  1.75162758], action=0, reward=1.0, next_state=[-0.13177422 -1.36158397  0.13983106  2.07498345]\n",
      "[ Experience replay ] starts\n",
      "[ episode 3 ][ timestamp 10 ] state=[-0.13177422 -1.36158397  0.13983106  2.07498345], action=0, reward=1.0, next_state=[-0.1590059  -1.55782088  0.18133073  2.40744227]\n",
      "[ Experience replay ] starts\n",
      "[ episode 3 ][ timestamp 11 ] state=[-0.1590059  -1.55782088  0.18133073  2.40744227], action=1, reward=-1.0, next_state=[-0.19016231 -1.36468274  0.22947957  2.17550449]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 3: Exploration_rate=0.7861544476842928. Score=11.\n",
      "[ episode 4 ] state=[ 0.03516278  0.02257556 -0.00252319  0.04904582]\n",
      "[ episode 4 ][ timestamp 1 ] state=[ 0.03516278  0.02257556 -0.00252319  0.04904582], action=0, reward=1.0, next_state=[ 0.03561429 -0.17251012 -0.00154227  0.34093159]\n",
      "[ Experience replay ] starts\n",
      "[ episode 4 ][ timestamp 2 ] state=[ 0.03561429 -0.17251012 -0.00154227  0.34093159], action=1, reward=1.0, next_state=[0.03216409 0.02263375 0.00527636 0.04776272]\n",
      "[ Experience replay ] starts\n",
      "[ episode 4 ][ timestamp 3 ] state=[0.03216409 0.02263375 0.00527636 0.04776272], action=1, reward=1.0, next_state=[ 0.03261676  0.21767964  0.00623162 -0.2432508 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 4 ][ timestamp 4 ] state=[ 0.03261676  0.21767964  0.00623162 -0.2432508 ], action=1, reward=1.0, next_state=[ 0.03697036  0.41271203  0.0013666  -0.53396162]\n",
      "[ Experience replay ] starts\n",
      "[ episode 4 ][ timestamp 5 ] state=[ 0.03697036  0.41271203  0.0013666  -0.53396162], action=0, reward=1.0, next_state=[ 0.0452246   0.21757089 -0.00931263 -0.2408484 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 4 ][ timestamp 6 ] state=[ 0.0452246   0.21757089 -0.00931263 -0.2408484 ], action=1, reward=1.0, next_state=[ 0.04957601  0.41282462 -0.0141296  -0.53645418]\n",
      "[ Experience replay ] starts\n",
      "[ episode 4 ][ timestamp 7 ] state=[ 0.04957601  0.41282462 -0.0141296  -0.53645418], action=1, reward=1.0, next_state=[ 0.05783251  0.60814236 -0.02485868 -0.83355551]\n",
      "[ Experience replay ] starts\n",
      "[ episode 4 ][ timestamp 8 ] state=[ 0.05783251  0.60814236 -0.02485868 -0.83355551], action=0, reward=1.0, next_state=[ 0.06999535  0.41336873 -0.04152979 -0.54879303]\n",
      "[ Experience replay ] starts\n",
      "[ episode 4 ][ timestamp 9 ] state=[ 0.06999535  0.41336873 -0.04152979 -0.54879303], action=1, reward=1.0, next_state=[ 0.07826273  0.60904872 -0.05250565 -0.85426617]\n",
      "[ Experience replay ] starts\n",
      "[ episode 4 ][ timestamp 10 ] state=[ 0.07826273  0.60904872 -0.05250565 -0.85426617], action=1, reward=1.0, next_state=[ 0.0904437   0.80484547 -0.06959098 -1.16298613]\n",
      "[ Experience replay ] starts\n",
      "[ episode 4 ][ timestamp 11 ] state=[ 0.0904437   0.80484547 -0.06959098 -1.16298613], action=1, reward=1.0, next_state=[ 0.10654061  1.00080116 -0.0928507  -1.47665143]\n",
      "[ Experience replay ] starts\n",
      "[ episode 4 ][ timestamp 12 ] state=[ 0.10654061  1.00080116 -0.0928507  -1.47665143], action=1, reward=1.0, next_state=[ 0.12655664  1.19692648 -0.12238373 -1.79683109]\n",
      "[ Experience replay ] starts\n",
      "[ episode 4 ][ timestamp 13 ] state=[ 0.12655664  1.19692648 -0.12238373 -1.79683109], action=0, reward=1.0, next_state=[ 0.15049516  1.00336875 -0.15832035 -1.54455716]\n",
      "[ Experience replay ] starts\n",
      "[ episode 4 ][ timestamp 14 ] state=[ 0.15049516  1.00336875 -0.15832035 -1.54455716], action=1, reward=1.0, next_state=[ 0.17056254  1.19999892 -0.18921149 -1.88216566]\n",
      "[ Experience replay ] starts\n",
      "[ episode 4 ][ timestamp 15 ] state=[ 0.17056254  1.19999892 -0.18921149 -1.88216566], action=1, reward=-1.0, next_state=[ 0.19456252  1.39661117 -0.22685481 -2.22711742]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 4: Exploration_rate=0.7292124703704616. Score=15.\n",
      "[ episode 5 ] state=[-0.0392785   0.00260751 -0.02165696 -0.00071208]\n",
      "[ episode 5 ][ timestamp 1 ] state=[-0.0392785   0.00260751 -0.02165696 -0.00071208], action=0, reward=1.0, next_state=[-0.03922635 -0.19219726 -0.0216712   0.2850599 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 5 ][ timestamp 2 ] state=[-0.03922635 -0.19219726 -0.0216712   0.2850599 ], action=0, reward=1.0, next_state=[-0.04307029 -0.38700354 -0.01597     0.57082987]\n",
      "[ Experience replay ] starts\n",
      "[ episode 5 ][ timestamp 3 ] state=[-0.04307029 -0.38700354 -0.01597     0.57082987], action=0, reward=1.0, next_state=[-0.05081036 -0.58189793 -0.0045534   0.85843921]\n",
      "[ Experience replay ] starts\n",
      "[ episode 5 ][ timestamp 4 ] state=[-0.05081036 -0.58189793 -0.0045534   0.85843921], action=1, reward=1.0, next_state=[-0.06244832 -0.38671425  0.01261538  0.56432802]\n",
      "[ Experience replay ] starts\n",
      "[ episode 5 ][ timestamp 5 ] state=[-0.06244832 -0.38671425  0.01261538  0.56432802], action=0, reward=1.0, next_state=[-0.07018261 -0.58201091  0.02390194  0.86095853]\n",
      "[ Experience replay ] starts\n",
      "[ episode 5 ][ timestamp 6 ] state=[-0.07018261 -0.58201091  0.02390194  0.86095853], action=0, reward=1.0, next_state=[-0.08182283 -0.77745007  0.04112111  1.16106003]\n",
      "[ Experience replay ] starts\n",
      "[ episode 5 ][ timestamp 7 ] state=[-0.08182283 -0.77745007  0.04112111  1.16106003], action=0, reward=1.0, next_state=[-0.09737183 -0.97308283  0.06434231  1.46634731]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 5 ][ timestamp 8 ] state=[-0.09737183 -0.97308283  0.06434231  1.46634731], action=0, reward=1.0, next_state=[-0.11683348 -1.16893088  0.09366926  1.77841508]\n",
      "[ Experience replay ] starts\n",
      "[ episode 5 ][ timestamp 9 ] state=[-0.11683348 -1.16893088  0.09366926  1.77841508], action=1, reward=1.0, next_state=[-0.1402121  -0.97497999  0.12923756  1.5162626 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 5 ][ timestamp 10 ] state=[-0.1402121  -0.97497999  0.12923756  1.5162626 ], action=1, reward=1.0, next_state=[-0.1597117  -0.78163683  0.15956281  1.26655661]\n",
      "[ Experience replay ] starts\n",
      "[ episode 5 ][ timestamp 11 ] state=[-0.1597117  -0.78163683  0.15956281  1.26655661], action=1, reward=1.0, next_state=[-0.17534444 -0.588872    0.18489395  1.02779512]\n",
      "[ Experience replay ] starts\n",
      "[ episode 5 ][ timestamp 12 ] state=[-0.17534444 -0.588872    0.18489395  1.02779512], action=1, reward=1.0, next_state=[-0.18712188 -0.39662764  0.20544985  0.7983932 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 5 ][ timestamp 13 ] state=[-0.18712188 -0.39662764  0.20544985  0.7983932 ], action=1, reward=-1.0, next_state=[-0.19505443 -0.20482715  0.22141771  0.57672123]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 5: Exploration_rate=0.6832098777212641. Score=13.\n",
      "[ episode 6 ] state=[ 0.02139753  0.00545731 -0.0183568  -0.03246379]\n",
      "[ episode 6 ][ timestamp 1 ] state=[ 0.02139753  0.00545731 -0.0183568  -0.03246379], action=1, reward=1.0, next_state=[ 0.02150668  0.20083763 -0.01900608 -0.33088149]\n",
      "[ Experience replay ] starts\n",
      "[ episode 6 ][ timestamp 2 ] state=[ 0.02150668  0.20083763 -0.01900608 -0.33088149], action=0, reward=1.0, next_state=[ 0.02552343  0.00599131 -0.02562371 -0.04425225]\n",
      "[ Experience replay ] starts\n",
      "[ episode 6 ][ timestamp 3 ] state=[ 0.02552343  0.00599131 -0.02562371 -0.04425225], action=0, reward=1.0, next_state=[ 0.02564326 -0.18875402 -0.02650875  0.2402373 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 6 ][ timestamp 4 ] state=[ 0.02564326 -0.18875402 -0.02650875  0.2402373 ], action=0, reward=1.0, next_state=[ 0.02186818 -0.38348746 -0.02170401  0.52444217]\n",
      "[ Experience replay ] starts\n",
      "[ episode 6 ][ timestamp 5 ] state=[ 0.02186818 -0.38348746 -0.02170401  0.52444217], action=1, reward=1.0, next_state=[ 0.01419843 -0.18806689 -0.01121516  0.22499989]\n",
      "[ Experience replay ] starts\n",
      "[ episode 6 ][ timestamp 6 ] state=[ 0.01419843 -0.18806689 -0.01121516  0.22499989], action=1, reward=1.0, next_state=[ 0.01043709  0.00721353 -0.00671517 -0.07119952]\n",
      "[ Experience replay ] starts\n",
      "[ episode 6 ][ timestamp 7 ] state=[ 0.01043709  0.00721353 -0.00671517 -0.07119952], action=0, reward=1.0, next_state=[ 0.01058136 -0.1878115  -0.00813916  0.2193572 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 6 ][ timestamp 8 ] state=[ 0.01058136 -0.1878115  -0.00813916  0.2193572 ], action=1, reward=1.0, next_state=[ 0.00682513  0.00742584 -0.00375201 -0.07588201]\n",
      "[ Experience replay ] starts\n",
      "[ episode 6 ][ timestamp 9 ] state=[ 0.00682513  0.00742584 -0.00375201 -0.07588201], action=1, reward=1.0, next_state=[ 0.00697365  0.20260138 -0.00526965 -0.36974634]\n",
      "[ Experience replay ] starts\n",
      "[ episode 6 ][ timestamp 10 ] state=[ 0.00697365  0.20260138 -0.00526965 -0.36974634], action=1, reward=1.0, next_state=[ 0.01102567  0.3977978  -0.01266458 -0.66408618]\n",
      "[ Experience replay ] starts\n",
      "[ episode 6 ][ timestamp 11 ] state=[ 0.01102567  0.3977978  -0.01266458 -0.66408618], action=0, reward=1.0, next_state=[ 0.01898163  0.2028543  -0.0259463  -0.37541767]\n",
      "[ Experience replay ] starts\n",
      "[ episode 6 ][ timestamp 12 ] state=[ 0.01898163  0.2028543  -0.0259463  -0.37541767], action=1, reward=1.0, next_state=[ 0.02303872  0.398335   -0.03345466 -0.67616738]\n",
      "[ Experience replay ] starts\n",
      "[ episode 6 ][ timestamp 13 ] state=[ 0.02303872  0.398335   -0.03345466 -0.67616738], action=0, reward=1.0, next_state=[ 0.03100542  0.20369349 -0.046978   -0.39420232]\n",
      "[ Experience replay ] starts\n",
      "[ episode 6 ][ timestamp 14 ] state=[ 0.03100542  0.20369349 -0.046978   -0.39420232], action=0, reward=1.0, next_state=[ 0.03507928  0.00926854 -0.05486205 -0.11669309]\n",
      "[ Experience replay ] starts\n",
      "[ episode 6 ][ timestamp 15 ] state=[ 0.03507928  0.00926854 -0.05486205 -0.11669309], action=1, reward=1.0, next_state=[ 0.03526466  0.20513189 -0.05719591 -0.42616745]\n",
      "[ Experience replay ] starts\n",
      "[ episode 6 ][ timestamp 16 ] state=[ 0.03526466  0.20513189 -0.05719591 -0.42616745], action=0, reward=1.0, next_state=[ 0.03936729  0.01086475 -0.06571926 -0.15204968]\n",
      "[ Experience replay ] starts\n",
      "[ episode 6 ][ timestamp 17 ] state=[ 0.03936729  0.01086475 -0.06571926 -0.15204968], action=1, reward=1.0, next_state=[ 0.03958459  0.20686317 -0.06876025 -0.4647202 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 6 ][ timestamp 18 ] state=[ 0.03958459  0.20686317 -0.06876025 -0.4647202 ], action=1, reward=1.0, next_state=[ 0.04372185  0.40288595 -0.07805466 -0.77825914]\n",
      "[ Experience replay ] starts\n",
      "[ episode 6 ][ timestamp 19 ] state=[ 0.04372185  0.40288595 -0.07805466 -0.77825914], action=1, reward=1.0, next_state=[ 0.05177957  0.59898946 -0.09361984 -1.09444356]\n",
      "[ Experience replay ] starts\n",
      "[ episode 6 ][ timestamp 20 ] state=[ 0.05177957  0.59898946 -0.09361984 -1.09444356], action=1, reward=1.0, next_state=[ 0.06375936  0.79521151 -0.11550871 -1.41497175]\n",
      "[ Experience replay ] starts\n",
      "[ episode 6 ][ timestamp 21 ] state=[ 0.06375936  0.79521151 -0.11550871 -1.41497175], action=0, reward=1.0, next_state=[ 0.07966359  0.60169438 -0.14380815 -1.16051448]\n",
      "[ Experience replay ] starts\n",
      "[ episode 6 ][ timestamp 22 ] state=[ 0.07966359  0.60169438 -0.14380815 -1.16051448], action=1, reward=1.0, next_state=[ 0.09169748  0.79836657 -0.16701844 -1.49461152]\n",
      "[ Experience replay ] starts\n",
      "[ episode 6 ][ timestamp 23 ] state=[ 0.09169748  0.79836657 -0.16701844 -1.49461152], action=1, reward=1.0, next_state=[ 0.10766481  0.99507941 -0.19691067 -1.83445029]\n",
      "[ Experience replay ] starts\n",
      "[ episode 6 ][ timestamp 24 ] state=[ 0.10766481  0.99507941 -0.19691067 -1.83445029], action=1, reward=-1.0, next_state=[ 0.1275664   1.19175977 -0.23359967 -2.18128811]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 6: Exploration_rate=0.6057704364907278. Score=24.\n",
      "[ episode 7 ] state=[-0.02530876 -0.02150658  0.00726012 -0.00122606]\n",
      "[ episode 7 ][ timestamp 1 ] state=[-0.02530876 -0.02150658  0.00726012 -0.00122606], action=1, reward=1.0, next_state=[-0.02573889  0.1735105   0.0072356  -0.29160952]\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 2 ] state=[-0.02573889  0.1735105   0.0072356  -0.29160952], action=1, reward=1.0, next_state=[-0.02226868  0.36852854  0.00140341 -0.58200167]\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 3 ] state=[-0.02226868  0.36852854  0.00140341 -0.58200167], action=0, reward=1.0, next_state=[-0.01489811  0.17338695 -0.01023662 -0.28887698]\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 4 ] state=[-0.01489811  0.17338695 -0.01023662 -0.28887698], action=0, reward=1.0, next_state=[-0.01143037 -0.02158754 -0.01601416  0.00055992]\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 5 ] state=[-0.01143037 -0.02158754 -0.01601416  0.00055992], action=1, reward=1.0, next_state=[-0.01186212  0.17376037 -0.01600296 -0.29713233]\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 6 ] state=[-0.01186212  0.17376037 -0.01600296 -0.29713233], action=1, reward=1.0, next_state=[-0.00838691  0.36910675 -0.02194561 -0.59481905]\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 7 ] state=[-0.00838691  0.36910675 -0.02194561 -0.59481905], action=1, reward=1.0, next_state=[-0.00100478  0.56452887 -0.03384199 -0.89433314]\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 8 ] state=[-0.00100478  0.56452887 -0.03384199 -0.89433314], action=0, reward=1.0, next_state=[ 0.0102858   0.3698818  -0.05172865 -0.61247735]\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 9 ] state=[ 0.0102858   0.3698818  -0.05172865 -0.61247735], action=1, reward=1.0, next_state=[ 0.01768344  0.56568707 -0.0639782  -0.92099383]\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 10 ] state=[ 0.01768344  0.56568707 -0.0639782  -0.92099383], action=1, reward=1.0, next_state=[ 0.02899718  0.76161255 -0.08239808 -1.23307754]\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 11 ] state=[ 0.02899718  0.76161255 -0.08239808 -1.23307754], action=1, reward=1.0, next_state=[ 0.04422943  0.95769159 -0.10705963 -1.55039584]\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 12 ] state=[ 0.04422943  0.95769159 -0.10705963 -1.55039584], action=1, reward=1.0, next_state=[ 0.06338326  1.15392269 -0.13806754 -1.87447269]\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 13 ] state=[ 0.06338326  1.15392269 -0.13806754 -1.87447269], action=1, reward=1.0, next_state=[ 0.08646171  1.35025577 -0.175557   -2.2066328 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 7 ][ timestamp 14 ] state=[ 0.08646171  1.35025577 -0.175557   -2.2066328 ], action=1, reward=-1.0, next_state=[ 0.11346683  1.54657615 -0.21968965 -2.54793607]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 7: Exploration_rate=0.5647174463480732. Score=14.\n",
      "[ episode 8 ] state=[-0.00883346  0.00371395 -0.00354461  0.02637272]\n",
      "[ episode 8 ][ timestamp 1 ] state=[-0.00883346  0.00371395 -0.00354461  0.02637272], action=0, reward=1.0, next_state=[-0.00875918 -0.19135699 -0.00301715  0.31793518]\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 2 ] state=[-0.00875918 -0.19135699 -0.00301715  0.31793518], action=1, reward=1.0, next_state=[-0.01258632  0.0038078   0.00334155  0.02430228]\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 3 ] state=[-0.01258632  0.0038078   0.00334155  0.02430228], action=1, reward=1.0, next_state=[-0.01251016  0.19888167  0.0038276  -0.26732448]\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 4 ] state=[-0.01251016  0.19888167  0.0038276  -0.26732448], action=0, reward=1.0, next_state=[-0.00853253  0.0037053  -0.00151889  0.02656324]\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 5 ] state=[-0.00853253  0.0037053  -0.00151889  0.02656324], action=1, reward=1.0, next_state=[-0.00845842  0.198849   -0.00098763 -0.26659853]\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 6 ] state=[-0.00845842  0.198849   -0.00098763 -0.26659853], action=1, reward=1.0, next_state=[-0.00448144  0.39398504 -0.0063196  -0.5595928 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 7 ] state=[-0.00448144  0.39398504 -0.0063196  -0.5595928 ], action=0, reward=1.0, next_state=[ 0.00339826  0.19895235 -0.01751146 -0.26890757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 8 ] state=[ 0.00339826  0.19895235 -0.01751146 -0.26890757], action=1, reward=1.0, next_state=[ 0.0073773   0.39431978 -0.02288961 -0.56706187]\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 9 ] state=[ 0.0073773   0.39431978 -0.02288961 -0.56706187], action=1, reward=1.0, next_state=[ 0.0152637   0.58975521 -0.03423084 -0.86686719]\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 10 ] state=[ 0.0152637   0.58975521 -0.03423084 -0.86686719], action=1, reward=1.0, next_state=[ 0.0270588   0.78532585 -0.05156819 -1.1701132 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 11 ] state=[ 0.0270588   0.78532585 -0.05156819 -1.1701132 ], action=1, reward=1.0, next_state=[ 0.04276532  0.98107914 -0.07497045 -1.47850713]\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 12 ] state=[ 0.04276532  0.98107914 -0.07497045 -1.47850713], action=0, reward=1.0, next_state=[ 0.0623869   0.78694831 -0.10454059 -1.21014951]\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 13 ] state=[ 0.0623869   0.78694831 -0.10454059 -1.21014951], action=1, reward=1.0, next_state=[ 0.07812587  0.98325313 -0.12874358 -1.53367816]\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 14 ] state=[ 0.07812587  0.98325313 -0.12874358 -1.53367816], action=1, reward=1.0, next_state=[ 0.09779093  1.17966959 -0.15941715 -1.86361068]\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 15 ] state=[ 0.09779093  1.17966959 -0.15941715 -1.86361068], action=1, reward=1.0, next_state=[ 0.12138432  1.37613914 -0.19668936 -2.20124852]\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 16 ] state=[ 0.12138432  1.37613914 -0.19668936 -2.20124852], action=1, reward=-1.0, next_state=[ 0.14890711  1.57253707 -0.24071433 -2.54761981]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 8: Exploration_rate=0.5211953074858876. Score=16.\n",
      "[ episode 9 ] state=[-0.0166116  -0.0488663  -0.00318727 -0.03194849]\n",
      "[ episode 9 ][ timestamp 1 ] state=[-0.0166116  -0.0488663  -0.00318727 -0.03194849], action=1, reward=1.0, next_state=[-0.01758893  0.14630121 -0.00382624 -0.32563533]\n",
      "[ Experience replay ] starts\n",
      "[ episode 9 ][ timestamp 2 ] state=[-0.01758893  0.14630121 -0.00382624 -0.32563533], action=1, reward=1.0, next_state=[-0.0146629   0.34147743 -0.01033895 -0.61952243]\n",
      "[ Experience replay ] starts\n",
      "[ episode 9 ][ timestamp 3 ] state=[-0.0146629   0.34147743 -0.01033895 -0.61952243], action=1, reward=1.0, next_state=[-0.00783336  0.53674225 -0.0227294  -0.9154436 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 9 ][ timestamp 4 ] state=[-0.00783336  0.53674225 -0.0227294  -0.9154436 ], action=1, reward=1.0, next_state=[ 0.00290149  0.73216409 -0.04103827 -1.21518252]\n",
      "[ Experience replay ] starts\n",
      "[ episode 9 ][ timestamp 5 ] state=[ 0.00290149  0.73216409 -0.04103827 -1.21518252], action=1, reward=1.0, next_state=[ 0.01754477  0.92779073 -0.06534192 -1.52043728]\n",
      "[ Experience replay ] starts\n",
      "[ episode 9 ][ timestamp 6 ] state=[ 0.01754477  0.92779073 -0.06534192 -1.52043728], action=1, reward=1.0, next_state=[ 0.03610059  1.12363876 -0.09575067 -1.83277926]\n",
      "[ Experience replay ] starts\n",
      "[ episode 9 ][ timestamp 7 ] state=[ 0.03610059  1.12363876 -0.09575067 -1.83277926], action=1, reward=1.0, next_state=[ 0.05857336  1.31968111 -0.13240625 -2.15360351]\n",
      "[ Experience replay ] starts\n",
      "[ episode 9 ][ timestamp 8 ] state=[ 0.05857336  1.31968111 -0.13240625 -2.15360351], action=1, reward=1.0, next_state=[ 0.08496698  1.51583232 -0.17547832 -2.48406877]\n",
      "[ Experience replay ] starts\n",
      "[ episode 9 ][ timestamp 9 ] state=[ 0.08496698  1.51583232 -0.17547832 -2.48406877], action=1, reward=-1.0, next_state=[ 0.11528363  1.71193124 -0.2251597  -2.82502623]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 9: Exploration_rate=0.4982051627146237. Score=9.\n",
      "[ episode 10 ] state=[-0.00982436  0.02553941 -0.00842842  0.04125779]\n",
      "[ episode 10 ][ timestamp 1 ] state=[-0.00982436  0.02553941 -0.00842842  0.04125779], action=1, reward=1.0, next_state=[-0.00931357  0.2207812  -0.00760326 -0.25407242]\n",
      "[ Experience replay ] starts\n",
      "[ episode 10 ][ timestamp 2 ] state=[-0.00931357  0.2207812  -0.00760326 -0.25407242], action=1, reward=1.0, next_state=[-0.00489794  0.41601089 -0.01268471 -0.54914382]\n",
      "[ Experience replay ] starts\n",
      "[ episode 10 ][ timestamp 3 ] state=[-0.00489794  0.41601089 -0.01268471 -0.54914382], action=1, reward=1.0, next_state=[ 0.00342227  0.61130871 -0.02366758 -0.84579619]\n",
      "[ Experience replay ] starts\n",
      "[ episode 10 ][ timestamp 4 ] state=[ 0.00342227  0.61130871 -0.02366758 -0.84579619], action=1, reward=1.0, next_state=[ 0.01564845  0.80674543 -0.04058351 -1.14582679]\n",
      "[ Experience replay ] starts\n",
      "[ episode 10 ][ timestamp 5 ] state=[ 0.01564845  0.80674543 -0.04058351 -1.14582679], action=1, reward=1.0, next_state=[ 0.03178336  1.00237324 -0.06350004 -1.45095516]\n",
      "[ Experience replay ] starts\n",
      "[ episode 10 ][ timestamp 6 ] state=[ 0.03178336  1.00237324 -0.06350004 -1.45095516], action=1, reward=1.0, next_state=[ 0.05183082  1.19821539 -0.09251915 -1.76278279]\n",
      "[ Experience replay ] starts\n",
      "[ episode 10 ][ timestamp 7 ] state=[ 0.05183082  1.19821539 -0.09251915 -1.76278279], action=1, reward=1.0, next_state=[ 0.07579513  1.39425412 -0.1277748  -2.08274509]\n",
      "[ Experience replay ] starts\n",
      "[ episode 10 ][ timestamp 8 ] state=[ 0.07579513  1.39425412 -0.1277748  -2.08274509], action=0, reward=1.0, next_state=[ 0.10368021  1.20063554 -0.16942971 -1.83214847]\n",
      "[ Experience replay ] starts\n",
      "[ episode 10 ][ timestamp 9 ] state=[ 0.10368021  1.20063554 -0.16942971 -1.83214847], action=1, reward=1.0, next_state=[ 0.12769292  1.39718001 -0.20607267 -2.17231807]\n",
      "[ Experience replay ] starts\n",
      "[ episode 10 ][ timestamp 10 ] state=[ 0.12769292  1.39718001 -0.20607267 -2.17231807], action=1, reward=-1.0, next_state=[ 0.15563652  1.59363052 -0.24951904 -2.52091658]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 10: Exploration_rate=0.4738479773082268. Score=10.\n",
      "[ episode 11 ] state=[-0.04084854  0.00633337  0.04368708  0.00604724]\n",
      "[ episode 11 ][ timestamp 1 ] state=[-0.04084854  0.00633337  0.04368708  0.00604724], action=1, reward=1.0, next_state=[-0.04072187  0.20080244  0.04380803 -0.27253813]\n",
      "[ Experience replay ] starts\n",
      "[ episode 11 ][ timestamp 2 ] state=[-0.04072187  0.20080244  0.04380803 -0.27253813], action=1, reward=1.0, next_state=[-0.03670582  0.39527281  0.03835726 -0.55108837]\n",
      "[ Experience replay ] starts\n",
      "[ episode 11 ][ timestamp 3 ] state=[-0.03670582  0.39527281  0.03835726 -0.55108837], action=0, reward=1.0, next_state=[-0.02880037  0.19963371  0.0273355  -0.24657131]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 11 ][ timestamp 4 ] state=[-0.02880037  0.19963371  0.0273355  -0.24657131], action=1, reward=1.0, next_state=[-0.02480769  0.3943548   0.02240407 -0.53050819]\n",
      "[ Experience replay ] starts\n",
      "[ episode 11 ][ timestamp 5 ] state=[-0.02480769  0.3943548   0.02240407 -0.53050819], action=1, reward=1.0, next_state=[-0.0169206   0.58915455  0.01179391 -0.81604824]\n",
      "[ Experience replay ] starts\n",
      "[ episode 11 ][ timestamp 6 ] state=[-0.0169206   0.58915455  0.01179391 -0.81604824], action=1, reward=1.0, next_state=[-0.00513751  0.78411306 -0.00452706 -1.10499833]\n",
      "[ Experience replay ] starts\n",
      "[ episode 11 ][ timestamp 7 ] state=[-0.00513751  0.78411306 -0.00452706 -1.10499833], action=1, reward=1.0, next_state=[ 0.01054476  0.97929425 -0.02662703 -1.39909807]\n",
      "[ Experience replay ] starts\n",
      "[ episode 11 ][ timestamp 8 ] state=[ 0.01054476  0.97929425 -0.02662703 -1.39909807], action=1, reward=1.0, next_state=[ 0.03013064  1.17473691 -0.05460899 -1.69998556]\n",
      "[ Experience replay ] starts\n",
      "[ episode 11 ][ timestamp 9 ] state=[ 0.03013064  1.17473691 -0.05460899 -1.69998556], action=1, reward=1.0, next_state=[ 0.05362538  1.37044392 -0.0886087  -2.00915553]\n",
      "[ Experience replay ] starts\n",
      "[ episode 11 ][ timestamp 10 ] state=[ 0.05362538  1.37044392 -0.0886087  -2.00915553], action=1, reward=1.0, next_state=[ 0.08103426  1.56636924 -0.12879181 -2.32790741]\n",
      "[ Experience replay ] starts\n",
      "[ episode 11 ][ timestamp 11 ] state=[ 0.08103426  1.56636924 -0.12879181 -2.32790741], action=1, reward=1.0, next_state=[ 0.11236164  1.76240224 -0.17534996 -2.65728174]\n",
      "[ Experience replay ] starts\n",
      "[ episode 11 ][ timestamp 12 ] state=[ 0.11236164  1.76240224 -0.17534996 -2.65728174], action=1, reward=-1.0, next_state=[ 0.14760969  1.9583496  -0.22849559 -2.99798475]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 11: Exploration_rate=0.446186062443672. Score=12.\n",
      "[ episode 12 ] state=[ 0.02016282 -0.04751182  0.04800106 -0.02684788]\n",
      "[ episode 12 ][ timestamp 1 ] state=[ 0.02016282 -0.04751182  0.04800106 -0.02684788], action=0, reward=1.0, next_state=[ 0.01921258 -0.24328811  0.04746411  0.28058519]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 2 ] state=[ 0.01921258 -0.24328811  0.04746411  0.28058519], action=0, reward=1.0, next_state=[ 0.01434682 -0.43905385  0.05307581  0.5878523 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 3 ] state=[ 0.01434682 -0.43905385  0.05307581  0.5878523 ], action=0, reward=1.0, next_state=[ 0.00556574 -0.63487735  0.06483286  0.89677087]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 4 ] state=[ 0.00556574 -0.63487735  0.06483286  0.89677087], action=1, reward=1.0, next_state=[-0.00713181 -0.4406914   0.08276827  0.62515141]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 5 ] state=[-0.00713181 -0.4406914   0.08276827  0.62515141], action=1, reward=1.0, next_state=[-0.01594563 -0.24681649  0.0952713   0.3596407 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 6 ] state=[-0.01594563 -0.24681649  0.0952713   0.3596407 ], action=1, reward=1.0, next_state=[-0.02088196 -0.05316883  0.10246412  0.09845388]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 7 ] state=[-0.02088196 -0.05316883  0.10246412  0.09845388], action=1, reward=1.0, next_state=[-0.02194534  0.14034672  0.10443319 -0.16022525]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 8 ] state=[-0.02194534  0.14034672  0.10443319 -0.16022525], action=0, reward=1.0, next_state=[-0.01913841 -0.05610328  0.10122869  0.16349188]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 9 ] state=[-0.01913841 -0.05610328  0.10122869  0.16349188], action=1, reward=1.0, next_state=[-0.02026047  0.13743482  0.10449853 -0.09561868]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 10 ] state=[-0.02026047  0.13743482  0.10449853 -0.09561868], action=0, reward=1.0, next_state=[-0.01751178 -0.05901758  0.10258615  0.22811913]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 11 ] state=[-0.01751178 -0.05901758  0.10258615  0.22811913], action=0, reward=1.0, next_state=[-0.01869213 -0.25544454  0.10714854  0.551318  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 12 ] state=[-0.01869213 -0.25544454  0.10714854  0.551318  ], action=1, reward=1.0, next_state=[-0.02380102 -0.06197771  0.1181749   0.29422345]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 13 ] state=[-0.02380102 -0.06197771  0.1181749   0.29422345], action=0, reward=1.0, next_state=[-0.02504057 -0.25856881  0.12405936  0.62171603]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 14 ] state=[-0.02504057 -0.25856881  0.12405936  0.62171603], action=1, reward=1.0, next_state=[-0.03021195 -0.06537764  0.13649369  0.37053639]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 15 ] state=[-0.03021195 -0.06537764  0.13649369  0.37053639], action=1, reward=1.0, next_state=[-0.0315195   0.12756784  0.14390441  0.12381464]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 16 ] state=[-0.0315195   0.12756784  0.14390441  0.12381464], action=1, reward=1.0, next_state=[-0.02896814  0.32036639  0.14638071 -0.12023189]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 17 ] state=[-0.02896814  0.32036639  0.14638071 -0.12023189], action=0, reward=1.0, next_state=[-0.02256082  0.12348352  0.14397607  0.21481645]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 18 ] state=[-0.02256082  0.12348352  0.14397607  0.21481645], action=1, reward=1.0, next_state=[-0.02009115  0.3162851   0.1482724  -0.02921077]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 19 ] state=[-0.02009115  0.3162851   0.1482724  -0.02921077], action=1, reward=1.0, next_state=[-0.01376544  0.50900402  0.14768818 -0.27168476]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 20 ] state=[-0.01376544  0.50900402  0.14768818 -0.27168476], action=1, reward=1.0, next_state=[-0.00358536  0.70174378  0.14225449 -0.51438449]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 21 ] state=[-0.00358536  0.70174378  0.14225449 -0.51438449], action=1, reward=1.0, next_state=[ 0.01044951  0.89460596  0.1319668  -0.75907366]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 22 ] state=[ 0.01044951  0.89460596  0.1319668  -0.75907366], action=1, reward=1.0, next_state=[ 0.02834163  1.08768656  0.11678532 -1.00749059]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 23 ] state=[ 0.02834163  1.08768656  0.11678532 -1.00749059], action=1, reward=1.0, next_state=[ 0.05009536  1.2810721   0.09663551 -1.2613361 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 24 ] state=[ 0.05009536  1.2810721   0.09663551 -1.2613361 ], action=1, reward=1.0, next_state=[ 0.0757168   1.47483454  0.07140879 -1.5222571 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 25 ] state=[ 0.0757168   1.47483454  0.07140879 -1.5222571 ], action=1, reward=1.0, next_state=[ 0.1052135   1.66902483  0.04096365 -1.79182384]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 26 ] state=[ 0.1052135   1.66902483  0.04096365 -1.79182384], action=1, reward=1.0, next_state=[ 0.13859399  1.86366435  0.00512717 -2.07149825]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 27 ] state=[ 0.13859399  1.86366435  0.00512717 -2.07149825], action=1, reward=1.0, next_state=[ 0.17586728  2.05873386 -0.03630279 -2.36259129]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 28 ] state=[ 0.17586728  2.05873386 -0.03630279 -2.36259129], action=1, reward=1.0, next_state=[ 0.21704196  2.25415951 -0.08355462 -2.66620729]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 29 ] state=[ 0.21704196  2.25415951 -0.08355462 -2.66620729], action=1, reward=1.0, next_state=[ 0.26212515  2.44979568 -0.13687877 -2.98317427]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 30 ] state=[ 0.26212515  2.44979568 -0.13687877 -2.98317427], action=1, reward=1.0, next_state=[ 0.31112106  2.64540504 -0.19654225 -3.31396074]\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 31 ] state=[ 0.31112106  2.64540504 -0.19654225 -3.31396074], action=1, reward=-1.0, next_state=[ 0.36402916  2.84063659 -0.26282147 -3.6585822 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 12: Exploration_rate=0.3819719776053028. Score=31.\n",
      "[ episode 13 ] state=[ 0.01278416  0.03839105  0.00546557 -0.03038043]\n",
      "[ episode 13 ][ timestamp 1 ] state=[ 0.01278416  0.03839105  0.00546557 -0.03038043], action=1, reward=1.0, next_state=[ 0.01355198  0.23343419  0.00485796 -0.32133391]\n",
      "[ Experience replay ] starts\n",
      "[ episode 13 ][ timestamp 2 ] state=[ 0.01355198  0.23343419  0.00485796 -0.32133391], action=0, reward=1.0, next_state=[ 0.01822067  0.0382434  -0.00156871 -0.02712294]\n",
      "[ Experience replay ] starts\n",
      "[ episode 13 ][ timestamp 3 ] state=[ 0.01822067  0.0382434  -0.00156871 -0.02712294], action=1, reward=1.0, next_state=[ 0.01898554  0.23338781 -0.00211117 -0.3203004 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 13 ][ timestamp 4 ] state=[ 0.01898554  0.23338781 -0.00211117 -0.3203004 ], action=1, reward=1.0, next_state=[ 0.02365329  0.42853977 -0.00851718 -0.61364836]\n",
      "[ Experience replay ] starts\n",
      "[ episode 13 ][ timestamp 5 ] state=[ 0.02365329  0.42853977 -0.00851718 -0.61364836], action=1, reward=1.0, next_state=[ 0.03222409  0.62377969 -0.02079015 -0.90900165]\n",
      "[ Experience replay ] starts\n",
      "[ episode 13 ][ timestamp 6 ] state=[ 0.03222409  0.62377969 -0.02079015 -0.90900165], action=1, reward=1.0, next_state=[ 0.04469968  0.81917678 -0.03897018 -1.20814581]\n",
      "[ Experience replay ] starts\n",
      "[ episode 13 ][ timestamp 7 ] state=[ 0.04469968  0.81917678 -0.03897018 -1.20814581], action=1, reward=1.0, next_state=[ 0.06108322  1.01477985 -0.0631331  -1.51278198]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 13 ][ timestamp 8 ] state=[ 0.06108322  1.01477985 -0.0631331  -1.51278198], action=1, reward=1.0, next_state=[ 0.08137881  1.21060692 -0.09338874 -1.82448619]\n",
      "[ Experience replay ] starts\n",
      "[ episode 13 ][ timestamp 9 ] state=[ 0.08137881  1.21060692 -0.09338874 -1.82448619], action=1, reward=1.0, next_state=[ 0.10559095  1.40663285 -0.12987846 -2.14466019]\n",
      "[ Experience replay ] starts\n",
      "[ episode 13 ][ timestamp 10 ] state=[ 0.10559095  1.40663285 -0.12987846 -2.14466019], action=1, reward=1.0, next_state=[ 0.13372361  1.60277467 -0.17277166 -2.47447195]\n",
      "[ Experience replay ] starts\n",
      "[ episode 13 ][ timestamp 11 ] state=[ 0.13372361  1.60277467 -0.17277166 -2.47447195], action=1, reward=-1.0, next_state=[ 0.1657791   1.79887438 -0.2222611  -2.81478478]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 13: Exploration_rate=0.3614809303671764. Score=11.\n",
      "[ episode 14 ] state=[-0.0375153  -0.00623601 -0.00406604 -0.04050475]\n",
      "[ episode 14 ][ timestamp 1 ] state=[-0.0375153  -0.00623601 -0.00406604 -0.04050475], action=1, reward=1.0, next_state=[-0.03764002  0.18894401 -0.00487613 -0.33446778]\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 2 ] state=[-0.03764002  0.18894401 -0.00487613 -0.33446778], action=1, reward=1.0, next_state=[-0.03386114  0.38413502 -0.01156549 -0.62868439]\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 3 ] state=[-0.03386114  0.38413502 -0.01156549 -0.62868439], action=1, reward=1.0, next_state=[-0.02617844  0.57941645 -0.02413918 -0.92498712]\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 4 ] state=[-0.02617844  0.57941645 -0.02413918 -0.92498712], action=0, reward=1.0, next_state=[-0.01459011  0.38462871 -0.04263892 -0.63998687]\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 5 ] state=[-0.01459011  0.38462871 -0.04263892 -0.63998687], action=0, reward=1.0, next_state=[-0.00689754  0.19012635 -0.05543866 -0.36103054]\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 6 ] state=[-0.00689754  0.19012635 -0.05543866 -0.36103054], action=1, reward=1.0, next_state=[-0.00309501  0.3859907  -0.06265927 -0.67066631]\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 7 ] state=[-0.00309501  0.3859907  -0.06265927 -0.67066631], action=0, reward=1.0, next_state=[ 0.0046248   0.19179326 -0.07607259 -0.39835158]\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 8 ] state=[ 0.0046248   0.19179326 -0.07607259 -0.39835158], action=0, reward=1.0, next_state=[ 0.00846067 -0.00217169 -0.08403962 -0.13058938]\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 9 ] state=[ 0.00846067 -0.00217169 -0.08403962 -0.13058938], action=0, reward=1.0, next_state=[ 0.00841723 -0.19599553 -0.08665141  0.13444171]\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 10 ] state=[ 0.00841723 -0.19599553 -0.08665141  0.13444171], action=0, reward=1.0, next_state=[ 0.00449732 -0.38977631 -0.08396258  0.39857868]\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 11 ] state=[ 0.00449732 -0.38977631 -0.08396258  0.39857868], action=0, reward=1.0, next_state=[-0.00329821 -0.58361301 -0.075991    0.66365346]\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 12 ] state=[-0.00329821 -0.58361301 -0.075991    0.66365346], action=0, reward=1.0, next_state=[-0.01497047 -0.77760015 -0.06271793  0.93147455]\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 13 ] state=[-0.01497047 -0.77760015 -0.06271793  0.93147455], action=0, reward=1.0, next_state=[-0.03052247 -0.97182222 -0.04408844  1.20380787]\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 14 ] state=[-0.03052247 -0.97182222 -0.04408844  1.20380787], action=0, reward=1.0, next_state=[-0.04995891 -1.16634736 -0.02001229  1.48235423]\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 15 ] state=[-0.04995891 -1.16634736 -0.02001229  1.48235423], action=0, reward=1.0, next_state=[-0.07328586 -1.36121957  0.0096348   1.7687208 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 16 ] state=[-0.07328586 -1.36121957  0.0096348   1.7687208 ], action=0, reward=1.0, next_state=[-0.10051025 -1.55644896  0.04500921  2.06438388]\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 17 ] state=[-0.10051025 -1.55644896  0.04500921  2.06438388], action=0, reward=1.0, next_state=[-0.13163923 -1.75199949  0.08629689  2.37064086]\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 18 ] state=[-0.13163923 -1.75199949  0.08629689  2.37064086], action=1, reward=1.0, next_state=[-0.16667922 -1.55774205  0.13370971  2.10567883]\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 19 ] state=[-0.16667922 -1.55774205  0.13370971  2.10567883], action=1, reward=1.0, next_state=[-0.19783406 -1.36418994  0.17582329  1.85713571]\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 20 ] state=[-0.19783406 -1.36418994  0.17582329  1.85713571], action=1, reward=-1.0, next_state=[-0.22511786 -1.17138099  0.212966    1.62380725]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 14: Exploration_rate=0.326999438029567. Score=20.\n",
      "[ episode 15 ] state=[ 0.01713211 -0.02542854  0.00360869  0.04617124]\n",
      "[ episode 15 ][ timestamp 1 ] state=[ 0.01713211 -0.02542854  0.00360869  0.04617124], action=0, reward=1.0, next_state=[ 0.01662354 -0.22060205  0.00453211  0.33999055]\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 2 ] state=[ 0.01662354 -0.22060205  0.00453211  0.33999055], action=0, reward=1.0, next_state=[ 0.0122115  -0.41578819  0.01133192  0.6340992 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 3 ] state=[ 0.0122115  -0.41578819  0.01133192  0.6340992 ], action=0, reward=1.0, next_state=[ 0.00389574 -0.61106636  0.02401391  0.93032917]\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 4 ] state=[ 0.00389574 -0.61106636  0.02401391  0.93032917], action=0, reward=1.0, next_state=[-0.00832559 -0.80650406  0.04262049  1.23046059]\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 5 ] state=[-0.00832559 -0.80650406  0.04262049  1.23046059], action=0, reward=1.0, next_state=[-0.02445567 -1.00214758  0.0672297   1.53618601]\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 6 ] state=[-0.02445567 -1.00214758  0.0672297   1.53618601], action=1, reward=1.0, next_state=[-0.04449862 -0.80789638  0.09795342  1.2652181 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 7 ] state=[-0.04449862 -0.80789638  0.09795342  1.2652181 ], action=0, reward=1.0, next_state=[-0.06065655 -1.0041241   0.12325778  1.586901  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 8 ] state=[-0.06065655 -1.0041241   0.12325778  1.586901  ], action=1, reward=1.0, next_state=[-0.08073903 -0.81066391  0.1549958   1.33505838]\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 9 ] state=[-0.08073903 -0.81066391  0.1549958   1.33505838], action=1, reward=1.0, next_state=[-0.09695231 -0.61779727  0.18169697  1.09461303]\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 10 ] state=[-0.09695231 -0.61779727  0.18169697  1.09461303], action=1, reward=1.0, next_state=[-0.10930826 -0.42547242  0.20358923  0.86400015]\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 11 ] state=[-0.10930826 -0.42547242  0.20358923  0.86400015], action=1, reward=-1.0, next_state=[-0.1178177  -0.23361684  0.22086923  0.64160292]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 15: Exploration_rate=0.30945741577570285. Score=11.\n",
      "[ episode 16 ] state=[-0.00043132 -0.00893639  0.04828571 -0.01982985]\n",
      "[ episode 16 ][ timestamp 1 ] state=[-0.00043132 -0.00893639  0.04828571 -0.01982985], action=0, reward=1.0, next_state=[-0.00061005 -0.20471636  0.04788911  0.28768831]\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 2 ] state=[-0.00061005 -0.20471636  0.04788911  0.28768831], action=0, reward=1.0, next_state=[-0.00470438 -0.40048737  0.05364288  0.59508218]\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 3 ] state=[-0.00470438 -0.40048737  0.05364288  0.59508218], action=1, reward=1.0, next_state=[-0.01271413 -0.20615561  0.06554452  0.31976729]\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 4 ] state=[-0.01271413 -0.20615561  0.06554452  0.31976729], action=1, reward=1.0, next_state=[-0.01683724 -0.01202538  0.07193986  0.04845351]\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 5 ] state=[-0.01683724 -0.01202538  0.07193986  0.04845351], action=0, reward=1.0, next_state=[-0.01707775 -0.20810121  0.07290893  0.36293859]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 16 ][ timestamp 6 ] state=[-0.01707775 -0.20810121  0.07290893  0.36293859], action=0, reward=1.0, next_state=[-0.02123977 -0.40417959  0.08016771  0.67769102]\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 7 ] state=[-0.02123977 -0.40417959  0.08016771  0.67769102], action=1, reward=1.0, next_state=[-0.02932336 -0.21025755  0.09372153  0.41128628]\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 8 ] state=[-0.02932336 -0.21025755  0.09372153  0.41128628], action=0, reward=1.0, next_state=[-0.03352851 -0.4065745   0.10194725  0.73198316]\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 9 ] state=[-0.03352851 -0.4065745   0.10194725  0.73198316], action=1, reward=1.0, next_state=[-0.04166    -0.21299802  0.11658692  0.47304664]\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 10 ] state=[-0.04166    -0.21299802  0.11658692  0.47304664], action=1, reward=1.0, next_state=[-0.04591996 -0.01969897  0.12604785  0.21926535]\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 11 ] state=[-0.04591996 -0.01969897  0.12604785  0.21926535], action=0, reward=1.0, next_state=[-0.04631394 -0.21637621  0.13043316  0.54890072]\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 12 ] state=[-0.04631394 -0.21637621  0.13043316  0.54890072], action=1, reward=1.0, next_state=[-0.05064147 -0.02330447  0.14141117  0.29999184]\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 13 ] state=[-0.05064147 -0.02330447  0.14141117  0.29999184], action=1, reward=1.0, next_state=[-0.05110756  0.16954844  0.14741101  0.05503651]\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 14 ] state=[-0.05110756  0.16954844  0.14741101  0.05503651], action=0, reward=1.0, next_state=[-0.04771659 -0.02734588  0.14851174  0.39035694]\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 15 ] state=[-0.04771659 -0.02734588  0.14851174  0.39035694], action=1, reward=1.0, next_state=[-0.0482635   0.16539061  0.15631888  0.14793669]\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 16 ] state=[-0.0482635   0.16539061  0.15631888  0.14793669], action=0, reward=1.0, next_state=[-0.04495569 -0.03158419  0.15927761  0.48556715]\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 17 ] state=[-0.04495569 -0.03158419  0.15927761  0.48556715], action=0, reward=1.0, next_state=[-0.04558738 -0.22855294  0.16898895  0.82391034]\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 18 ] state=[-0.04558738 -0.22855294  0.16898895  0.82391034], action=1, reward=1.0, next_state=[-0.05015843 -0.03609562  0.18546716  0.58878321]\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 19 ] state=[-0.05015843 -0.03609562  0.18546716  0.58878321], action=1, reward=1.0, next_state=[-0.05088035  0.15601141  0.19724282  0.35977985]\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 20 ] state=[-0.05088035  0.15601141  0.19724282  0.35977985], action=0, reward=1.0, next_state=[-0.04776012 -0.04128734  0.20443842  0.70760383]\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 21 ] state=[-0.04776012 -0.04128734  0.20443842  0.70760383], action=1, reward=-1.0, next_state=[-0.04858587  0.15050444  0.2185905   0.4855943 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 16: Exploration_rate=0.27853872940185365. Score=21.\n",
      "[ episode 17 ] state=[ 0.01971887  0.01740424  0.0294541  -0.04498197]\n",
      "[ episode 17 ][ timestamp 1 ] state=[ 0.01971887  0.01740424  0.0294541  -0.04498197], action=0, reward=1.0, next_state=[ 0.02006696 -0.17812741  0.02855446  0.25684655]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 2 ] state=[ 0.02006696 -0.17812741  0.02855446  0.25684655], action=0, reward=1.0, next_state=[ 0.01650441 -0.37364516  0.03369139  0.55839748]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 3 ] state=[ 0.01650441 -0.37364516  0.03369139  0.55839748], action=1, reward=1.0, next_state=[ 0.0090315  -0.17901194  0.04485934  0.27651673]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 4 ] state=[ 0.0090315  -0.17901194  0.04485934  0.27651673], action=1, reward=1.0, next_state=[ 0.00545127  0.01544226  0.05038967 -0.00168691]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 5 ] state=[ 0.00545127  0.01544226  0.05038967 -0.00168691], action=0, reward=1.0, next_state=[ 0.00576011 -0.18036478  0.05035593  0.30645914]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 6 ] state=[ 0.00576011 -0.18036478  0.05035593  0.30645914], action=1, reward=1.0, next_state=[0.00215282 0.01400479 0.05648512 0.03007274]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 7 ] state=[0.00215282 0.01400479 0.05648512 0.03007274], action=0, reward=1.0, next_state=[ 0.00243291 -0.18187979  0.05708657  0.34002879]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 8 ] state=[ 0.00243291 -0.18187979  0.05708657  0.34002879], action=1, reward=1.0, next_state=[-0.00120468  0.0123854   0.06388715  0.06588004]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 9 ] state=[-0.00120468  0.0123854   0.06388715  0.06588004], action=0, reward=1.0, next_state=[-0.00095698 -0.18359157  0.06520475  0.37801583]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 10 ] state=[-0.00095698 -0.18359157  0.06520475  0.37801583], action=1, reward=1.0, next_state=[-0.00462881  0.01054667  0.07276506  0.10658392]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 11 ] state=[-0.00462881  0.01054667  0.07276506  0.10658392], action=0, reward=1.0, next_state=[-0.00441787 -0.18553852  0.07489674  0.42130744]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 12 ] state=[-0.00441787 -0.18553852  0.07489674  0.42130744], action=1, reward=1.0, next_state=[-0.00812865  0.0084468   0.08332289  0.15314426]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 13 ] state=[-0.00812865  0.0084468   0.08332289  0.15314426], action=0, reward=1.0, next_state=[-0.00795971 -0.18776325  0.08638578  0.47090685]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 14 ] state=[-0.00795971 -0.18776325  0.08638578  0.47090685], action=0, reward=1.0, next_state=[-0.01171497 -0.3839924   0.09580391  0.78951883]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 15 ] state=[-0.01171497 -0.3839924   0.09580391  0.78951883], action=1, reward=1.0, next_state=[-0.01939482 -0.19030753  0.11159429  0.52844708]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 16 ] state=[-0.01939482 -0.19030753  0.11159429  0.52844708], action=1, reward=1.0, next_state=[-0.02320097  0.0030821   0.12216323  0.27290768]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 17 ] state=[-0.02320097  0.0030821   0.12216323  0.27290768], action=1, reward=1.0, next_state=[-0.02313933  0.19626837  0.12762139  0.02111461]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 18 ] state=[-0.02313933  0.19626837  0.12762139  0.02111461], action=1, reward=1.0, next_state=[-0.01921396  0.38935105  0.12804368 -0.2287351 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 19 ] state=[-0.01921396  0.38935105  0.12804368 -0.2287351 ], action=0, reward=1.0, next_state=[-0.01142694  0.19265397  0.12346898  0.10143722]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 20 ] state=[-0.01142694  0.19265397  0.12346898  0.10143722], action=0, reward=1.0, next_state=[-0.00757386 -0.00400128  0.12549772  0.43038223]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 21 ] state=[-0.00757386 -0.00400128  0.12549772  0.43038223], action=1, reward=1.0, next_state=[-0.00765389  0.18914089  0.13410537  0.17974697]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 22 ] state=[-0.00765389  0.18914089  0.13410537  0.17974697], action=1, reward=1.0, next_state=[-0.00387107  0.38211435  0.1377003  -0.06780534]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 23 ] state=[-0.00387107  0.38211435  0.1377003  -0.06780534], action=0, reward=1.0, next_state=[0.00377122 0.18531438 0.1363442  0.2649564 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 24 ] state=[0.00377122 0.18531438 0.1363442  0.2649564 ], action=0, reward=1.0, next_state=[ 0.0074775  -0.01146349  0.14164333  0.59734503]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 25 ] state=[ 0.0074775  -0.01146349  0.14164333  0.59734503], action=1, reward=1.0, next_state=[0.00724823 0.18142203 0.15359023 0.35241831]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 26 ] state=[0.00724823 0.18142203 0.15359023 0.35241831], action=1, reward=1.0, next_state=[0.01087667 0.37406437 0.16063859 0.11183462]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 17 ][ timestamp 27 ] state=[0.01087667 0.37406437 0.16063859 0.11183462], action=0, reward=1.0, next_state=[0.01835796 0.17704882 0.16287529 0.45057808]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 28 ] state=[0.01835796 0.17704882 0.16287529 0.45057808], action=1, reward=1.0, next_state=[0.02189894 0.36953776 0.17188685 0.21333991]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 29 ] state=[0.02189894 0.36953776 0.17188685 0.21333991], action=0, reward=1.0, next_state=[0.02928969 0.1724285  0.17615364 0.55493309]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 30 ] state=[0.02928969 0.1724285  0.17615364 0.55493309], action=1, reward=1.0, next_state=[0.03273826 0.36469669 0.18725231 0.32251558]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 31 ] state=[0.03273826 0.36469669 0.18725231 0.32251558], action=1, reward=1.0, next_state=[0.0400322  0.55672745 0.19370262 0.09423566]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 32 ] state=[0.0400322  0.55672745 0.19370262 0.09423566], action=0, reward=1.0, next_state=[0.05116675 0.35943307 0.19558733 0.44123571]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 33 ] state=[0.05116675 0.35943307 0.19558733 0.44123571], action=1, reward=1.0, next_state=[0.05835541 0.5513276  0.20441205 0.21601875]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 34 ] state=[0.05835541 0.5513276  0.20441205 0.21601875], action=0, reward=1.0, next_state=[0.06938196 0.35395938 0.20873242 0.56558692]\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 35 ] state=[0.06938196 0.35395938 0.20873242 0.56558692], action=1, reward=-1.0, next_state=[0.07646115 0.54563643 0.22004416 0.34523474]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 17: Exploration_rate=0.23371867538818816. Score=35.\n",
      "[ episode 18 ] state=[-0.03816452  0.03167719 -0.00492155 -0.03666001]\n",
      "[ episode 18 ][ timestamp 1 ] state=[-0.03816452  0.03167719 -0.00492155 -0.03666001], action=0, reward=1.0, next_state=[-0.03753098 -0.16337384 -0.00565475  0.25446606]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 2 ] state=[-0.03753098 -0.16337384 -0.00565475  0.25446606], action=1, reward=1.0, next_state=[-0.04079846  0.03182839 -0.00056543 -0.0399951 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 3 ] state=[-0.04079846  0.03182839 -0.00056543 -0.0399951 ], action=0, reward=1.0, next_state=[-0.04016189 -0.16328545 -0.00136533  0.25250938]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 4 ] state=[-0.04016189 -0.16328545 -0.00136533  0.25250938], action=1, reward=1.0, next_state=[-0.0434276   0.03185597  0.00368485 -0.04060389]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 5 ] state=[-0.0434276   0.03185597  0.00368485 -0.04060389], action=1, reward=1.0, next_state=[-0.04279048  0.22692489  0.00287278 -0.33212193]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 6 ] state=[-0.04279048  0.22692489  0.00287278 -0.33212193], action=0, reward=1.0, next_state=[-0.03825198  0.03176216 -0.00376966 -0.03853446]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 7 ] state=[-0.03825198  0.03176216 -0.00376966 -0.03853446], action=0, reward=1.0, next_state=[-0.03761674 -0.16330553 -0.00454035  0.25295672]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 8 ] state=[-0.03761674 -0.16330553 -0.00454035  0.25295672], action=1, reward=1.0, next_state=[-0.04088285  0.03188096  0.00051878 -0.04115485]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 9 ] state=[-0.04088285  0.03188096  0.00051878 -0.04115485], action=1, reward=1.0, next_state=[-4.02452291e-02  2.26995468e-01 -3.04315377e-04 -3.33674055e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 10 ] state=[-4.02452291e-02  2.26995468e-01 -3.04315377e-04 -3.33674055e-01], action=0, reward=1.0, next_state=[-0.03570532  0.03187785 -0.0069778  -0.04108711]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 11 ] state=[-0.03570532  0.03187785 -0.0069778  -0.04108711], action=1, reward=1.0, next_state=[-0.03506776  0.22709916 -0.00779954 -0.3359634 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 12 ] state=[-0.03506776  0.22709916 -0.00779954 -0.3359634 ], action=0, reward=1.0, next_state=[-0.03052578  0.03208907 -0.01451881 -0.04575021]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 13 ] state=[-0.03052578  0.03208907 -0.01451881 -0.04575021], action=1, reward=1.0, next_state=[-0.029884    0.22741617 -0.01543381 -0.34297836]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 14 ] state=[-0.029884    0.22741617 -0.01543381 -0.34297836], action=0, reward=1.0, next_state=[-0.02533567  0.03251716 -0.02229338 -0.05520201]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 15 ] state=[-0.02533567  0.03251716 -0.02229338 -0.05520201], action=1, reward=1.0, next_state=[-0.02468533  0.22795155 -0.02339742 -0.35483447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 16 ] state=[-0.02468533  0.22795155 -0.02339742 -0.35483447], action=1, reward=1.0, next_state=[-0.0201263   0.42339823 -0.03049411 -0.65480246]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 17 ] state=[-0.0201263   0.42339823 -0.03049411 -0.65480246], action=0, reward=1.0, next_state=[-0.01165834  0.22871383 -0.04359016 -0.3718755 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 18 ] state=[-0.01165834  0.22871383 -0.04359016 -0.3718755 ], action=0, reward=1.0, next_state=[-0.00708406  0.03423737 -0.05102767 -0.09324936]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 19 ] state=[-0.00708406  0.03423737 -0.05102767 -0.09324936], action=0, reward=1.0, next_state=[-0.00639931 -0.16011747 -0.05289265  0.18290781]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 20 ] state=[-0.00639931 -0.16011747 -0.05289265  0.18290781], action=1, reward=1.0, next_state=[-0.00960166  0.03571984 -0.0492345  -0.12598054]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 21 ] state=[-0.00960166  0.03571984 -0.0492345  -0.12598054], action=1, reward=1.0, next_state=[-0.00888726  0.23151129 -0.05175411 -0.43378092]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 22 ] state=[-0.00888726  0.23151129 -0.05175411 -0.43378092], action=0, reward=1.0, next_state=[-0.00425704  0.0371588  -0.06042973 -0.15785145]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 23 ] state=[-0.00425704  0.0371588  -0.06042973 -0.15785145], action=0, reward=1.0, next_state=[-0.00351386 -0.15704824 -0.06358676  0.11517186]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 24 ] state=[-0.00351386 -0.15704824 -0.06358676  0.11517186], action=1, reward=1.0, next_state=[-0.00665483  0.03892445 -0.06128332 -0.19687502]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 25 ] state=[-0.00665483  0.03892445 -0.06128332 -0.19687502], action=1, reward=1.0, next_state=[-0.00587634  0.23486701 -0.06522082 -0.50824312]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 26 ] state=[-0.00587634  0.23486701 -0.06522082 -0.50824312], action=0, reward=1.0, next_state=[-0.001179    0.04072168 -0.07538568 -0.23680563]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 27 ] state=[-0.001179    0.04072168 -0.07538568 -0.23680563], action=1, reward=1.0, next_state=[-3.64564682e-04  2.36835168e-01 -8.01217942e-02 -5.52282776e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 28 ] state=[-3.64564682e-04  2.36835168e-01 -8.01217942e-02 -5.52282776e-01], action=0, reward=1.0, next_state=[ 0.00437214  0.04292444 -0.09116745 -0.28588041]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 29 ] state=[ 0.00437214  0.04292444 -0.09116745 -0.28588041], action=0, reward=1.0, next_state=[ 0.00523063 -0.15078705 -0.09688506 -0.02328598]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 30 ] state=[ 0.00523063 -0.15078705 -0.09688506 -0.02328598], action=0, reward=1.0, next_state=[ 0.00221489 -0.34439569 -0.09735078  0.23732538]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 31 ] state=[ 0.00221489 -0.34439569 -0.09735078  0.23732538], action=1, reward=1.0, next_state=[-0.00467303 -0.14802748 -0.09260427 -0.08440823]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 32 ] state=[-0.00467303 -0.14802748 -0.09260427 -0.08440823], action=0, reward=1.0, next_state=[-0.00763358 -0.34170838 -0.09429243  0.17768157]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 33 ] state=[-0.00763358 -0.34170838 -0.09429243  0.17768157], action=1, reward=1.0, next_state=[-0.01446774 -0.14537239 -0.0907388  -0.14319507]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 34 ] state=[-0.01446774 -0.14537239 -0.0907388  -0.14319507], action=1, reward=1.0, next_state=[-0.01737519  0.050924   -0.0936027  -0.46306896]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 35 ] state=[-0.01737519  0.050924   -0.0936027  -0.46306896], action=0, reward=1.0, next_state=[-0.01635671 -0.14275912 -0.10286408 -0.20129508]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 36 ] state=[-0.01635671 -0.14275912 -0.10286408 -0.20129508], action=1, reward=1.0, next_state=[-0.01921189  0.05367206 -0.10688999 -0.52457313]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 18 ][ timestamp 37 ] state=[-0.01921189  0.05367206 -0.10688999 -0.52457313], action=0, reward=1.0, next_state=[-0.01813845 -0.1397961  -0.11738145 -0.26739301]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 38 ] state=[-0.01813845 -0.1397961  -0.11738145 -0.26739301], action=0, reward=1.0, next_state=[-0.02093438 -0.33306426 -0.12272931 -0.01391662]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 39 ] state=[-0.02093438 -0.33306426 -0.12272931 -0.01391662], action=1, reward=1.0, next_state=[-0.02759566 -0.13641548 -0.12300764 -0.34266296]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 40 ] state=[-0.02759566 -0.13641548 -0.12300764 -0.34266296], action=1, reward=1.0, next_state=[-0.03032397  0.06022217 -0.1298609  -0.6714639 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 41 ] state=[-0.03032397  0.06022217 -0.1298609  -0.6714639 ], action=0, reward=1.0, next_state=[-0.02911953 -0.13287827 -0.14329018 -0.422324  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 42 ] state=[-0.02911953 -0.13287827 -0.14329018 -0.422324  ], action=0, reward=1.0, next_state=[-0.03177709 -0.32571037 -0.15173666 -0.1780235 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 43 ] state=[-0.03177709 -0.32571037 -0.15173666 -0.1780235 ], action=1, reward=1.0, next_state=[-0.0382913  -0.12877945 -0.15529713 -0.5144654 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 44 ] state=[-0.0382913  -0.12877945 -0.15529713 -0.5144654 ], action=0, reward=1.0, next_state=[-0.04086689 -0.32141258 -0.16558644 -0.27446709]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 45 ] state=[-0.04086689 -0.32141258 -0.16558644 -0.27446709], action=1, reward=1.0, next_state=[-0.04729514 -0.12436289 -0.17107578 -0.61445896]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 46 ] state=[-0.04729514 -0.12436289 -0.17107578 -0.61445896], action=0, reward=1.0, next_state=[-0.0497824  -0.31673371 -0.18336496 -0.38016632]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 47 ] state=[-0.0497824  -0.31673371 -0.18336496 -0.38016632], action=0, reward=1.0, next_state=[-0.05611707 -0.50884285 -0.19096828 -0.15044118]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 48 ] state=[-0.05611707 -0.50884285 -0.19096828 -0.15044118], action=1, reward=1.0, next_state=[-0.06629393 -0.31157246 -0.19397711 -0.4967715 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 49 ] state=[-0.06629393 -0.31157246 -0.19397711 -0.4967715 ], action=1, reward=1.0, next_state=[-0.07252538 -0.11432056 -0.20391254 -0.84377257]\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 50 ] state=[-0.07252538 -0.11432056 -0.20391254 -0.84377257], action=0, reward=-1.0, next_state=[-0.07481179 -0.30616353 -0.22078799 -0.62150578]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 18: Exploration_rate=0.18190617987607657. Score=50.\n",
      "[ episode 19 ] state=[0.03387186 0.02571307 0.02149082 0.00112801]\n",
      "[ episode 19 ][ timestamp 1 ] state=[0.03387186 0.02571307 0.02149082 0.00112801], action=0, reward=1.0, next_state=[ 0.03438612 -0.16971039  0.02151338  0.30051333]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 2 ] state=[ 0.03438612 -0.16971039  0.02151338  0.30051333], action=1, reward=1.0, next_state=[0.03099191 0.02509842 0.02752365 0.01469218]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 3 ] state=[0.03099191 0.02509842 0.02752365 0.01469218], action=1, reward=1.0, next_state=[ 0.03149388  0.21981506  0.02781749 -0.26918122]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 4 ] state=[ 0.03149388  0.21981506  0.02781749 -0.26918122], action=0, reward=1.0, next_state=[0.03589018 0.0243074  0.02243387 0.0321441 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 5 ] state=[0.03589018 0.0243074  0.02243387 0.0321441 ], action=1, reward=1.0, next_state=[ 0.03637633  0.21910058  0.02307675 -0.25337714]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 6 ] state=[ 0.03637633  0.21910058  0.02307675 -0.25337714], action=0, reward=1.0, next_state=[0.04075834 0.02365685 0.01800921 0.04649436]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 7 ] state=[0.04075834 0.02365685 0.01800921 0.04649436], action=1, reward=1.0, next_state=[ 0.04123148  0.21851599  0.0189391  -0.24045254]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 8 ] state=[ 0.04123148  0.21851599  0.0189391  -0.24045254], action=1, reward=1.0, next_state=[ 0.0456018   0.41336235  0.01413005 -0.52710189]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 9 ] state=[ 0.0456018   0.41336235  0.01413005 -0.52710189], action=0, reward=1.0, next_state=[ 0.05386905  0.21804446  0.00358801 -0.23000022]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 10 ] state=[ 0.05386905  0.21804446  0.00358801 -0.23000022], action=0, reward=1.0, next_state=[ 0.05822993  0.02287142 -0.001012    0.06381233]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 11 ] state=[ 0.05822993  0.02287142 -0.001012    0.06381233], action=1, reward=1.0, next_state=[ 0.05868736  0.21800787  0.00026425 -0.22918972]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 12 ] state=[ 0.05868736  0.21800787  0.00026425 -0.22918972], action=0, reward=1.0, next_state=[ 0.06304752  0.02288214 -0.00431954  0.06357655]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 13 ] state=[ 0.06304752  0.02288214 -0.00431954  0.06357655], action=1, reward=1.0, next_state=[ 0.06350516  0.21806576 -0.00304801 -0.23046608]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 14 ] state=[ 0.06350516  0.21806576 -0.00304801 -0.23046608], action=0, reward=1.0, next_state=[ 0.06786648  0.02298749 -0.00765734  0.06125384]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 15 ] state=[ 0.06786648  0.02298749 -0.00765734  0.06125384], action=1, reward=1.0, next_state=[ 0.06832623  0.21821839 -0.00643226 -0.23383516]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 16 ] state=[ 0.06832623  0.21821839 -0.00643226 -0.23383516], action=0, reward=1.0, next_state=[ 0.0726906   0.02318893 -0.01110896  0.05681191]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 17 ] state=[ 0.0726906   0.02318893 -0.01110896  0.05681191], action=1, reward=1.0, next_state=[ 0.07315437  0.21846839 -0.00997272 -0.23935517]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 18 ] state=[ 0.07315437  0.21846839 -0.00997272 -0.23935517], action=0, reward=1.0, next_state=[ 0.07752374  0.02349032 -0.01475983  0.05016546]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 19 ] state=[ 0.07752374  0.02349032 -0.01475983  0.05016546], action=1, reward=1.0, next_state=[ 0.07799355  0.21882076 -0.01375652 -0.24713753]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 20 ] state=[ 0.07799355  0.21882076 -0.01375652 -0.24713753], action=0, reward=1.0, next_state=[ 0.08236996  0.02389796 -0.01869927  0.04117473]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 21 ] state=[ 0.08236996  0.02389796 -0.01869927  0.04117473], action=0, reward=1.0, next_state=[ 0.08284792 -0.17095093 -0.01787577  0.3278997 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 22 ] state=[ 0.08284792 -0.17095093 -0.01787577  0.3278997 ], action=1, reward=1.0, next_state=[ 0.0794289   0.02442089 -0.01131778  0.02963359]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 23 ] state=[ 0.0794289   0.02442089 -0.01131778  0.02963359], action=1, reward=1.0, next_state=[ 0.07991732  0.2197033  -0.01072511 -0.26659862]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 24 ] state=[ 0.07991732  0.2197033  -0.01072511 -0.26659862], action=0, reward=1.0, next_state=[ 0.08431139  0.02473605 -0.01605708  0.02268232]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 25 ] state=[ 0.08431139  0.02473605 -0.01605708  0.02268232], action=0, reward=1.0, next_state=[ 0.08480611 -0.17015199 -0.01560343  0.31025612]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 26 ] state=[ 0.08480611 -0.17015199 -0.01560343  0.31025612], action=1, reward=1.0, next_state=[ 0.08140307  0.02518875 -0.00939831  0.01269344]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 27 ] state=[ 0.08140307  0.02518875 -0.00939831  0.01269344], action=1, reward=1.0, next_state=[ 0.08190684  0.22044422 -0.00914444 -0.28293988]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 28 ] state=[ 0.08190684  0.22044422 -0.00914444 -0.28293988], action=0, reward=1.0, next_state=[ 0.08631573  0.02545388 -0.01480324  0.00684496]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 29 ] state=[ 0.08631573  0.02545388 -0.01480324  0.00684496], action=1, reward=1.0, next_state=[ 0.08682481  0.22078497 -0.01466634 -0.29047156]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 30 ] state=[ 0.08682481  0.22078497 -0.01466634 -0.29047156], action=0, reward=1.0, next_state=[ 0.09124051  0.02587519 -0.02047577 -0.00245008]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 31 ] state=[ 0.09124051  0.02587519 -0.02047577 -0.00245008], action=1, reward=1.0, next_state=[ 0.09175801  0.22128472 -0.02052477 -0.30152238]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 32 ] state=[ 0.09175801  0.22128472 -0.02052477 -0.30152238], action=0, reward=1.0, next_state=[ 0.0961837   0.02646122 -0.02655522 -0.01538255]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 33 ] state=[ 0.0961837   0.02646122 -0.02655522 -0.01538255], action=1, reward=1.0, next_state=[ 0.09671293  0.22195374 -0.02686287 -0.31632427]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 34 ] state=[ 0.09671293  0.22195374 -0.02686287 -0.31632427], action=0, reward=1.0, next_state=[ 0.101152    0.02722452 -0.03318936 -0.03223255]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 19 ][ timestamp 35 ] state=[ 0.101152    0.02722452 -0.03318936 -0.03223255], action=1, reward=1.0, next_state=[ 0.10169649  0.22280631 -0.03383401 -0.33519955]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 36 ] state=[ 0.10169649  0.22280631 -0.03383401 -0.33519955], action=0, reward=1.0, next_state=[ 0.10615262  0.02818181 -0.040538   -0.05337517]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 37 ] state=[ 0.10615262  0.02818181 -0.040538   -0.05337517], action=1, reward=1.0, next_state=[ 0.10671626  0.22386087 -0.0416055  -0.35856753]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 38 ] state=[ 0.10671626  0.22386087 -0.0416055  -0.35856753], action=0, reward=1.0, next_state=[ 0.11119347  0.02935431 -0.04877685 -0.07928868]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 39 ] state=[ 0.11119347  0.02935431 -0.04877685 -0.07928868], action=1, reward=1.0, next_state=[ 0.11178056  0.22514033 -0.05036263 -0.38695313]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 40 ] state=[ 0.11178056  0.22514033 -0.05036263 -0.38695313], action=0, reward=1.0, next_state=[ 0.11628337  0.03076812 -0.05810169 -0.11056484]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 41 ] state=[ 0.11628337  0.03076812 -0.05810169 -0.11056484], action=1, reward=1.0, next_state=[ 0.11689873  0.22667244 -0.06031299 -0.42099774]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 42 ] state=[ 0.11689873  0.22667244 -0.06031299 -0.42099774], action=0, reward=1.0, next_state=[ 0.12143218  0.03245457 -0.06873294 -0.14792193]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 43 ] state=[ 0.12143218  0.03245457 -0.06873294 -0.14792193], action=1, reward=1.0, next_state=[ 0.12208127  0.22849003 -0.07169138 -0.46147238]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 44 ] state=[ 0.12208127  0.22849003 -0.07169138 -0.46147238], action=1, reward=1.0, next_state=[ 0.12665107  0.42454813 -0.08092083 -0.77586332]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 45 ] state=[ 0.12665107  0.42454813 -0.08092083 -0.77586332], action=0, reward=1.0, next_state=[ 0.13514203  0.23062689 -0.09643809 -0.50969807]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 46 ] state=[ 0.13514203  0.23062689 -0.09643809 -0.50969807], action=1, reward=1.0, next_state=[ 0.13975457  0.42696574 -0.10663206 -0.83114677]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 47 ] state=[ 0.13975457  0.42696574 -0.10663206 -0.83114677], action=0, reward=1.0, next_state=[ 0.14829388  0.23345015 -0.12325499 -0.57381253]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 48 ] state=[ 0.14829388  0.23345015 -0.12325499 -0.57381253], action=0, reward=1.0, next_state=[ 0.15296289  0.04025218 -0.13473124 -0.32235935]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 49 ] state=[ 0.15296289  0.04025218 -0.13473124 -0.32235935], action=1, reward=1.0, next_state=[ 0.15376793  0.2370097  -0.14117843 -0.6543122 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 50 ] state=[ 0.15376793  0.2370097  -0.14117843 -0.6543122 ], action=0, reward=1.0, next_state=[ 0.15850813  0.04410635 -0.15426467 -0.40920472]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 51 ] state=[ 0.15850813  0.04410635 -0.15426467 -0.40920472], action=0, reward=1.0, next_state=[ 0.15939025 -0.14853047 -0.16244877 -0.16885503]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 52 ] state=[ 0.15939025 -0.14853047 -0.16244877 -0.16885503], action=1, reward=1.0, next_state=[ 0.15641964  0.04849884 -0.16582587 -0.50805808]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 53 ] state=[ 0.15641964  0.04849884 -0.16582587 -0.50805808], action=1, reward=1.0, next_state=[ 0.15738962  0.24552125 -0.17598703 -0.84806735]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 54 ] state=[ 0.15738962  0.24552125 -0.17598703 -0.84806735], action=0, reward=1.0, next_state=[ 0.16230004  0.05317997 -0.19294838 -0.61548525]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 55 ] state=[ 0.16230004  0.05317997 -0.19294838 -0.61548525], action=0, reward=1.0, next_state=[ 0.16336364 -0.13879735 -0.20525808 -0.38923851]\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 56 ] state=[ 0.16336364 -0.13879735 -0.20525808 -0.38923851], action=1, reward=-1.0, next_state=[ 0.1605877   0.05855629 -0.21304285 -0.73897785]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 19: Exploration_rate=0.13738520790976036. Score=56.\n",
      "[ episode 20 ] state=[-0.04166013  0.02156697  0.04292098 -0.0303552 ]\n",
      "[ episode 20 ][ timestamp 1 ] state=[-0.04166013  0.02156697  0.04292098 -0.0303552 ], action=1, reward=1.0, next_state=[-0.0412288   0.21604797  0.04231387 -0.30919314]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 2 ] state=[-0.0412288   0.21604797  0.04231387 -0.30919314], action=1, reward=1.0, next_state=[-0.03690784  0.41054228  0.03613001 -0.5882369 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 3 ] state=[-0.03690784  0.41054228  0.03613001 -0.5882369 ], action=0, reward=1.0, next_state=[-0.02869699  0.21493349  0.02436527 -0.28439529]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 4 ] state=[-0.02869699  0.21493349  0.02436527 -0.28439529], action=1, reward=1.0, next_state=[-0.02439832  0.40969961  0.01867737 -0.56929508]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 5 ] state=[-0.02439832  0.40969961  0.01867737 -0.56929508], action=0, reward=1.0, next_state=[-0.01620433  0.21432076  0.00729146 -0.27078708]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 6 ] state=[-0.01620433  0.21432076  0.00729146 -0.27078708], action=0, reward=1.0, next_state=[-0.01191791  0.01909552  0.00187572  0.02418666]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 7 ] state=[-0.01191791  0.01909552  0.00187572  0.02418666], action=1, reward=1.0, next_state=[-0.011536    0.21419052  0.00235946 -0.26790386]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 8 ] state=[-0.011536    0.21419052  0.00235946 -0.26790386], action=1, reward=1.0, next_state=[-0.00725219  0.40927872 -0.00299862 -0.55984167]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 9 ] state=[-0.00725219  0.40927872 -0.00299862 -0.55984167], action=0, reward=1.0, next_state=[ 0.00093338  0.21419898 -0.01419546 -0.26810497]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 10 ] state=[ 0.00093338  0.21419898 -0.01419546 -0.26810497], action=1, reward=1.0, next_state=[ 0.00521736  0.40952062 -0.01955755 -0.56523123]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 11 ] state=[ 0.00521736  0.40952062 -0.01955755 -0.56523123], action=0, reward=1.0, next_state=[ 0.01340777  0.21467843 -0.03086218 -0.2787734 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 12 ] state=[ 0.01340777  0.21467843 -0.03086218 -0.2787734 ], action=1, reward=1.0, next_state=[ 0.01770134  0.41022675 -0.03643765 -0.58102824]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 13 ] state=[ 0.01770134  0.41022675 -0.03643765 -0.58102824], action=0, reward=1.0, next_state=[ 0.02590588  0.2156338  -0.04805821 -0.30004286]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 14 ] state=[ 0.02590588  0.2156338  -0.04805821 -0.30004286], action=1, reward=1.0, next_state=[ 0.03021855  0.41140664 -0.05405907 -0.60748674]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 15 ] state=[ 0.03021855  0.41140664 -0.05405907 -0.60748674], action=0, reward=1.0, next_state=[ 0.03844669  0.21708052 -0.0662088  -0.33230901]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 16 ] state=[ 0.03844669  0.21708052 -0.0662088  -0.33230901], action=1, reward=1.0, next_state=[ 0.0427883   0.41307933 -0.07285498 -0.64511424]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 17 ] state=[ 0.0427883   0.41307933 -0.07285498 -0.64511424], action=0, reward=1.0, next_state=[ 0.05104988  0.21904421 -0.08575727 -0.37623407]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 18 ] state=[ 0.05104988  0.21904421 -0.08575727 -0.37623407], action=0, reward=1.0, next_state=[ 0.05543077  0.02523832 -0.09328195 -0.11177532]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 19 ] state=[ 0.05543077  0.02523832 -0.09328195 -0.11177532], action=1, reward=1.0, next_state=[ 0.05593553  0.22156456 -0.09551746 -0.43236948]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 20 ] state=[ 0.05593553  0.22156456 -0.09551746 -0.43236948], action=0, reward=1.0, next_state=[ 0.06036683  0.02791564 -0.10416485 -0.17125962]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 21 ] state=[ 0.06036683  0.02791564 -0.10416485 -0.17125962], action=1, reward=1.0, next_state=[ 0.06092514  0.22436231 -0.10759004 -0.49490155]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 22 ] state=[ 0.06092514  0.22436231 -0.10759004 -0.49490155], action=1, reward=1.0, next_state=[ 0.06541238  0.42082395 -0.11748807 -0.81946053]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 20 ][ timestamp 23 ] state=[ 0.06541238  0.42082395 -0.11748807 -0.81946053], action=0, reward=1.0, next_state=[ 0.07382886  0.22748907 -0.13387728 -0.5659195 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 24 ] state=[ 0.07382886  0.22748907 -0.13387728 -0.5659195 ], action=0, reward=1.0, next_state=[ 0.07837865  0.03447417 -0.14519567 -0.3182303 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 25 ] state=[ 0.07837865  0.03447417 -0.14519567 -0.3182303 ], action=0, reward=1.0, next_state=[ 0.07906813 -0.1583136  -0.15156028 -0.07462923]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 26 ] state=[ 0.07906813 -0.1583136  -0.15156028 -0.07462923], action=1, reward=1.0, next_state=[ 0.07590186  0.03861953 -0.15305286 -0.41103098]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 27 ] state=[ 0.07590186  0.03861953 -0.15305286 -0.41103098], action=0, reward=1.0, next_state=[ 0.07667425 -0.15403902 -0.16127348 -0.17024343]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 28 ] state=[ 0.07667425 -0.15403902 -0.16127348 -0.17024343], action=1, reward=1.0, next_state=[ 0.07359347  0.04297969 -0.16467835 -0.50914573]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 29 ] state=[ 0.07359347  0.04297969 -0.16467835 -0.50914573], action=0, reward=1.0, next_state=[ 0.07445306 -0.14948599 -0.17486126 -0.27254988]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 30 ] state=[ 0.07445306 -0.14948599 -0.17486126 -0.27254988], action=0, reward=1.0, next_state=[ 0.07146334 -0.34173783 -0.18031226 -0.03971731]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 31 ] state=[ 0.07146334 -0.34173783 -0.18031226 -0.03971731], action=1, reward=1.0, next_state=[ 0.06462858 -0.14454994 -0.18110661 -0.38342886]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 32 ] state=[ 0.06462858 -0.14454994 -0.18110661 -0.38342886], action=1, reward=1.0, next_state=[ 0.06173759  0.05261904 -0.18877518 -0.72730002]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 33 ] state=[ 0.06173759  0.05261904 -0.18877518 -0.72730002], action=0, reward=1.0, next_state=[ 0.06278997 -0.13946138 -0.20332119 -0.49946878]\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 34 ] state=[ 0.06278997 -0.13946138 -0.20332119 -0.49946878], action=1, reward=-1.0, next_state=[ 0.06000074  0.05785897 -0.21331056 -0.84871793]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 20: Exploration_rate=0.11585765144771248. Score=34.\n",
      "[ episode 21 ] state=[-0.02014701 -0.00057976  0.02066312  0.00078902]\n",
      "[ episode 21 ][ timestamp 1 ] state=[-0.02014701 -0.00057976  0.02066312  0.00078902], action=1, reward=1.0, next_state=[-0.02015861  0.19423985  0.0206789  -0.28530349]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 2 ] state=[-0.02015861  0.19423985  0.0206789  -0.28530349], action=0, reward=1.0, next_state=[-0.01627381 -0.00117082  0.01497283  0.01382902]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 3 ] state=[-0.01627381 -0.00117082  0.01497283  0.01382902], action=1, reward=1.0, next_state=[-0.01629723  0.19373323  0.01524941 -0.27409245]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 4 ] state=[-0.01629723  0.19373323  0.01524941 -0.27409245], action=1, reward=1.0, next_state=[-0.01242256  0.38863432  0.00976756 -0.56192693]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 5 ] state=[-0.01242256  0.38863432  0.00976756 -0.56192693], action=0, reward=1.0, next_state=[-0.00464988  0.19337667 -0.00147098 -0.26618281]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 6 ] state=[-0.00464988  0.19337667 -0.00147098 -0.26618281], action=1, reward=1.0, next_state=[-0.00078234  0.38851959 -0.00679464 -0.55932933]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 7 ] state=[-0.00078234  0.38851959 -0.00679464 -0.55932933], action=0, reward=1.0, next_state=[ 0.00698805  0.19349366 -0.01798122 -0.26879481]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 8 ] state=[ 0.00698805  0.19349366 -0.01798122 -0.26879481], action=0, reward=1.0, next_state=[ 0.01085792 -0.00136713 -0.02335712  0.01816293]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 9 ] state=[ 0.01085792 -0.00136713 -0.02335712  0.01816293], action=1, reward=1.0, next_state=[ 0.01083058  0.19408187 -0.02299386 -0.28179697]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 10 ] state=[ 0.01083058  0.19408187 -0.02299386 -0.28179697], action=0, reward=1.0, next_state=[ 0.01471222 -0.00070468 -0.0286298   0.00354602]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 11 ] state=[ 0.01471222 -0.00070468 -0.0286298   0.00354602], action=1, reward=1.0, next_state=[ 0.01469812  0.19481592 -0.02855888 -0.2980307 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 12 ] state=[ 0.01469812  0.19481592 -0.02855888 -0.2980307 ], action=0, reward=1.0, next_state=[ 0.01859444  0.00011246 -0.03451949 -0.01448978]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 13 ] state=[ 0.01859444  0.00011246 -0.03451949 -0.01448978], action=1, reward=1.0, next_state=[ 0.01859669  0.19571203 -0.03480929 -0.31786105]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 14 ] state=[ 0.01859669  0.19571203 -0.03480929 -0.31786105], action=0, reward=1.0, next_state=[ 0.02251093  0.0011027  -0.04116651 -0.03635577]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 15 ] state=[ 0.02251093  0.0011027  -0.04116651 -0.03635577], action=1, reward=1.0, next_state=[ 0.02253299  0.19679007 -0.04189363 -0.34173767]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 16 ] state=[ 0.02253299  0.19679007 -0.04189363 -0.34173767], action=0, reward=1.0, next_state=[ 0.02646879  0.00228842 -0.04872838 -0.06255431]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 17 ] state=[ 0.02646879  0.00228842 -0.04872838 -0.06255431], action=1, reward=1.0, next_state=[ 0.02651456  0.19807393 -0.04997947 -0.37020445]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 18 ] state=[ 0.02651456  0.19807393 -0.04997947 -0.37020445], action=0, reward=1.0, next_state=[ 0.03047604  0.00369638 -0.05738356 -0.09369006]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 19 ] state=[ 0.03047604  0.00369638 -0.05738356 -0.09369006], action=0, reward=1.0, next_state=[ 0.03054996 -0.19055815 -0.05925736  0.18035062]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 20 ] state=[ 0.03054996 -0.19055815 -0.05925736  0.18035062], action=1, reward=1.0, next_state=[ 0.0267388   0.0053595  -0.05565034 -0.1304215 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 21 ] state=[ 0.0267388   0.0053595  -0.05565034 -0.1304215 ], action=0, reward=1.0, next_state=[ 0.02684599 -0.18892294 -0.05825877  0.14419824]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 22 ] state=[ 0.02684599 -0.18892294 -0.05825877  0.14419824], action=1, reward=1.0, next_state=[ 0.02306753  0.00698286 -0.05537481 -0.1662803 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 23 ] state=[ 0.02306753  0.00698286 -0.05537481 -0.1662803 ], action=1, reward=1.0, next_state=[ 0.02320719  0.20285195 -0.05870042 -0.47590547]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 24 ] state=[ 0.02320719  0.20285195 -0.05870042 -0.47590547], action=0, reward=1.0, next_state=[ 0.02726423  0.00860587 -0.06821852 -0.20228621]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 25 ] state=[ 0.02726423  0.00860587 -0.06821852 -0.20228621], action=0, reward=1.0, next_state=[ 0.02743634 -0.18547749 -0.07226425  0.06812099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 26 ] state=[ 0.02743634 -0.18547749 -0.07226425  0.06812099], action=1, reward=1.0, next_state=[ 0.02372679  0.01060211 -0.07090183 -0.24645798]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 27 ] state=[ 0.02372679  0.01060211 -0.07090183 -0.24645798], action=0, reward=1.0, next_state=[ 0.02393884 -0.18343935 -0.07583099  0.02304524]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 28 ] state=[ 0.02393884 -0.18343935 -0.07583099  0.02304524], action=1, reward=1.0, next_state=[ 0.02027005  0.01268356 -0.07537008 -0.29256665]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 29 ] state=[ 0.02027005  0.01268356 -0.07537008 -0.29256665], action=1, reward=1.0, next_state=[ 0.02052372  0.20879469 -0.08122142 -0.60803604]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 30 ] state=[ 0.02052372  0.20879469 -0.08122142 -0.60803604], action=0, reward=1.0, next_state=[ 0.02469962  0.01489658 -0.09338214 -0.34200055]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 31 ] state=[ 0.02469962  0.01489658 -0.09338214 -0.34200055], action=1, reward=1.0, next_state=[ 0.02499755  0.21121446 -0.10022215 -0.66260881]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 32 ] state=[ 0.02499755  0.21121446 -0.10022215 -0.66260881], action=0, reward=1.0, next_state=[ 0.02922184  0.01761919 -0.11347432 -0.40308911]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 33 ] state=[ 0.02922184  0.01761919 -0.11347432 -0.40308911], action=1, reward=1.0, next_state=[ 0.02957422  0.21415227 -0.12153611 -0.72928269]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 21 ][ timestamp 34 ] state=[ 0.02957422  0.21415227 -0.12153611 -0.72928269], action=0, reward=1.0, next_state=[ 0.03385726  0.02090114 -0.13612176 -0.47718797]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 35 ] state=[ 0.03385726  0.02090114 -0.13612176 -0.47718797], action=0, reward=1.0, next_state=[ 0.03427529 -0.17206279 -0.14566552 -0.23031585]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 36 ] state=[ 0.03427529 -0.17206279 -0.14566552 -0.23031585], action=1, reward=1.0, next_state=[ 0.03083403  0.02480778 -0.15027184 -0.56516865]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 37 ] state=[ 0.03083403  0.02480778 -0.15027184 -0.56516865], action=0, reward=1.0, next_state=[ 0.03133019 -0.16792194 -0.16157521 -0.32334587]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 38 ] state=[ 0.03133019 -0.16792194 -0.16157521 -0.32334587], action=1, reward=1.0, next_state=[ 0.02797175  0.02908763 -0.16804213 -0.66230788]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 39 ] state=[ 0.02797175  0.02908763 -0.16804213 -0.66230788], action=1, reward=1.0, next_state=[ 0.0285535   0.22609961 -0.18128829 -1.00283542]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 40 ] state=[ 0.0285535   0.22609961 -0.18128829 -1.00283542], action=0, reward=1.0, next_state=[ 0.03307549  0.0338015  -0.20134499 -0.77212255]\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 41 ] state=[ 0.03307549  0.0338015  -0.20134499 -0.77212255], action=0, reward=-1.0, next_state=[ 0.03375152 -0.15806511 -0.21678744 -0.54893291]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 21: Exploration_rate=0.0943346041173244. Score=41.\n",
      "[ episode 22 ] state=[-0.01978792 -0.00914173  0.04344534 -0.00079638]\n",
      "[ episode 22 ][ timestamp 1 ] state=[-0.01978792 -0.00914173  0.04344534 -0.00079638], action=1, reward=1.0, next_state=[-0.01997075  0.1853311   0.04342941 -0.27946145]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 2 ] state=[-0.01997075  0.1853311   0.04342941 -0.27946145], action=1, reward=1.0, next_state=[-0.01626413  0.37980748  0.03784019 -0.55813672]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 3 ] state=[-0.01626413  0.37980748  0.03784019 -0.55813672], action=0, reward=1.0, next_state=[-0.00866798  0.18417534  0.02667745 -0.25377623]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 4 ] state=[-0.00866798  0.18417534  0.02667745 -0.25377623], action=0, reward=1.0, next_state=[-0.00498447 -0.01131717  0.02160193  0.04720044]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 5 ] state=[-0.00498447 -0.01131717  0.02160193  0.04720044], action=1, reward=1.0, next_state=[-0.00521082  0.18348847  0.02254594 -0.23858938]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 6 ] state=[-0.00521082  0.18348847  0.02254594 -0.23858938], action=0, reward=1.0, next_state=[-0.00154105 -0.01194819  0.01777415  0.06111906]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 7 ] state=[-0.00154105 -0.01194819  0.01777415  0.06111906], action=0, reward=1.0, next_state=[-0.00178001 -0.20732041  0.01899653  0.35935643]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 8 ] state=[-0.00178001 -0.20732041  0.01899653  0.35935643], action=1, reward=1.0, next_state=[-0.00592642 -0.01247359  0.02618366  0.07272356]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 9 ] state=[-0.00592642 -0.01247359  0.02618366  0.07272356], action=1, reward=1.0, next_state=[-0.00617589  0.18226339  0.02763813 -0.21158466]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 10 ] state=[-0.00617589  0.18226339  0.02763813 -0.21158466], action=0, reward=1.0, next_state=[-0.00253062 -0.0132426   0.02340644  0.0896869 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 11 ] state=[-0.00253062 -0.0132426   0.02340644  0.0896869 ], action=1, reward=1.0, next_state=[-0.00279547  0.18153617  0.02520017 -0.19552036]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 12 ] state=[-0.00279547  0.18153617  0.02520017 -0.19552036], action=0, reward=1.0, next_state=[ 0.00083525 -0.01393701  0.02128977  0.10500438]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 13 ] state=[ 0.00083525 -0.01393701  0.02128977  0.10500438], action=1, reward=1.0, next_state=[ 0.00055651  0.18087347  0.02338985 -0.1808864 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 14 ] state=[ 0.00055651  0.18087347  0.02338985 -0.1808864 ], action=1, reward=1.0, next_state=[ 0.00417398  0.37565305  0.01977213 -0.46609987]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 15 ] state=[ 0.00417398  0.37565305  0.01977213 -0.46609987], action=1, reward=1.0, next_state=[ 0.01168704  0.57049013  0.01045013 -0.75248574]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 16 ] state=[ 0.01168704  0.57049013  0.01045013 -0.75248574], action=0, reward=1.0, next_state=[ 0.02309684  0.37522566 -0.00459959 -0.45653273]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 17 ] state=[ 0.02309684  0.37522566 -0.00459959 -0.45653273], action=0, reward=1.0, next_state=[ 0.03060135  0.18016904 -0.01373024 -0.16530317]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 18 ] state=[ 0.03060135  0.18016904 -0.01373024 -0.16530317], action=1, reward=1.0, next_state=[ 0.03420474  0.37548481 -0.0170363  -0.46228579]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 19 ] state=[ 0.03420474  0.37548481 -0.0170363  -0.46228579], action=0, reward=1.0, next_state=[ 0.04171443  0.18060773 -0.02628202 -0.17502101]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 20 ] state=[ 0.04171443  0.18060773 -0.02628202 -0.17502101], action=1, reward=1.0, next_state=[ 0.04532659  0.37609577 -0.02978244 -0.47587782]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 21 ] state=[ 0.04532659  0.37609577 -0.02978244 -0.47587782], action=0, reward=1.0, next_state=[ 0.0528485   0.18140674 -0.0393     -0.19272853]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 22 ] state=[ 0.0528485   0.18140674 -0.0393     -0.19272853], action=1, reward=1.0, next_state=[ 0.05647664  0.3770682  -0.04315457 -0.49754533]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 23 ] state=[ 0.05647664  0.3770682  -0.04315457 -0.49754533], action=0, reward=1.0, next_state=[ 0.064018    0.18258045 -0.05310547 -0.21876881]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 24 ] state=[ 0.064018    0.18258045 -0.05310547 -0.21876881], action=1, reward=1.0, next_state=[ 0.06766961  0.37841973 -0.05748085 -0.52771927]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 25 ] state=[ 0.06766961  0.37841973 -0.05748085 -0.52771927], action=0, reward=1.0, next_state=[ 0.075238    0.18415163 -0.06803524 -0.25368846]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 26 ] state=[ 0.075238    0.18415163 -0.06803524 -0.25368846], action=1, reward=1.0, next_state=[ 0.07892104  0.38017574 -0.073109   -0.5670313 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 27 ] state=[ 0.07892104  0.38017574 -0.073109   -0.5670313 ], action=0, reward=1.0, next_state=[ 0.08652455  0.18615138 -0.08444963 -0.2982471 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 28 ] state=[ 0.08652455  0.18615138 -0.08444963 -0.2982471 ], action=0, reward=1.0, next_state=[ 0.09024758 -0.00767161 -0.09041457 -0.03334742]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 29 ] state=[ 0.09024758 -0.00767161 -0.09041457 -0.03334742], action=1, reward=1.0, next_state=[ 0.09009415  0.18862277 -0.09108152 -0.35313199]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 30 ] state=[ 0.09009415  0.18862277 -0.09108152 -0.35313199], action=0, reward=1.0, next_state=[ 0.0938666  -0.00509395 -0.09814416 -0.09050232]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 31 ] state=[ 0.0938666  -0.00509395 -0.09814416 -0.09050232], action=1, reward=1.0, next_state=[ 0.09376472  0.19128776 -0.09995421 -0.4124654 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 32 ] state=[ 0.09376472  0.19128776 -0.09995421 -0.4124654 ], action=0, reward=1.0, next_state=[ 0.09759048 -0.00228575 -0.10820352 -0.15289203]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 33 ] state=[ 0.09759048 -0.00228575 -0.10820352 -0.15289203], action=1, reward=1.0, next_state=[ 0.09754476  0.19420577 -0.11126136 -0.4776554 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 34 ] state=[ 0.09754476  0.19420577 -0.11126136 -0.4776554 ], action=0, reward=1.0, next_state=[ 0.10142888  0.00081607 -0.12081446 -0.22200788]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 35 ] state=[ 0.10142888  0.00081607 -0.12081446 -0.22200788], action=1, reward=1.0, next_state=[ 0.1014452   0.1974391  -0.12525462 -0.55022569]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 22 ][ timestamp 36 ] state=[ 0.1014452   0.1974391  -0.12525462 -0.55022569], action=0, reward=1.0, next_state=[ 0.10539398  0.00427843 -0.13625914 -0.2994832 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 37 ] state=[ 0.10539398  0.00427843 -0.13625914 -0.2994832 ], action=1, reward=1.0, next_state=[ 0.10547955  0.20105288 -0.1422488  -0.63184538]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 38 ] state=[ 0.10547955  0.20105288 -0.1422488  -0.63184538], action=1, reward=1.0, next_state=[ 0.10950061  0.39784286 -0.15488571 -0.96572914]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 39 ] state=[ 0.10950061  0.39784286 -0.15488571 -0.96572914], action=0, reward=1.0, next_state=[ 0.11745747  0.2051021  -0.17420029 -0.72543345]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 40 ] state=[ 0.11745747  0.2051021  -0.17420029 -0.72543345], action=1, reward=1.0, next_state=[ 0.12155951  0.40215    -0.18870896 -1.0674882 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 41 ] state=[ 0.12155951  0.40215    -0.18870896 -1.0674882 ], action=0, reward=-1.0, next_state=[ 0.12960251  0.20995694 -0.21005872 -0.83946829]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 22: Exploration_rate=0.07680992513462537. Score=41.\n",
      "[ episode 23 ] state=[-0.04716767 -0.04133847  0.00636357 -0.03126362]\n",
      "[ episode 23 ][ timestamp 1 ] state=[-0.04716767 -0.04133847  0.00636357 -0.03126362], action=1, reward=1.0, next_state=[-0.04799443  0.15369165  0.0057383  -0.321932  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 2 ] state=[-0.04799443  0.15369165  0.0057383  -0.321932  ], action=1, reward=1.0, next_state=[-0.0449206   0.34873142 -0.00070034 -0.61279978]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 3 ] state=[-0.0449206   0.34873142 -0.00070034 -0.61279978], action=1, reward=1.0, next_state=[-0.03794597  0.54386315 -0.01295633 -0.9057032 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 4 ] state=[-0.03794597  0.54386315 -0.01295633 -0.9057032 ], action=0, reward=1.0, next_state=[-0.02706871  0.34891902 -0.0310704  -0.6171206 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 5 ] state=[-0.02706871  0.34891902 -0.0310704  -0.6171206 ], action=0, reward=1.0, next_state=[-0.02009033  0.15424458 -0.04341281 -0.33438311]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 6 ] state=[-0.02009033  0.15424458 -0.04341281 -0.33438311], action=0, reward=1.0, next_state=[-0.01700544 -0.04023348 -0.05010047 -0.05570023]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 7 ] state=[-0.01700544 -0.04023348 -0.05010047 -0.05570023], action=0, reward=1.0, next_state=[-0.01781011 -0.2346026  -0.05121448  0.22076425]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 8 ] state=[-0.01781011 -0.2346026  -0.05121448  0.22076425], action=1, reward=1.0, next_state=[-0.02250216 -0.03878743 -0.04679919 -0.08762387]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 9 ] state=[-0.02250216 -0.03878743 -0.04679919 -0.08762387], action=1, reward=1.0, next_state=[-0.02327791  0.15697301 -0.04855167 -0.39469697]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 10 ] state=[-0.02327791  0.15697301 -0.04855167 -0.39469697], action=0, reward=1.0, next_state=[-0.02013845 -0.0374276  -0.05644561 -0.11770825]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 11 ] state=[-0.02013845 -0.0374276  -0.05644561 -0.11770825], action=0, reward=1.0, next_state=[-0.020887   -0.23169728 -0.05879977  0.15664597]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 12 ] state=[-0.020887   -0.23169728 -0.05879977  0.15664597], action=1, reward=1.0, next_state=[-0.02552095 -0.03578491 -0.05566685 -0.1539919 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 13 ] state=[-0.02552095 -0.03578491 -0.05566685 -0.1539919 ], action=1, reward=1.0, next_state=[-0.02623664  0.16008809 -0.05874669 -0.46370388]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 14 ] state=[-0.02623664  0.16008809 -0.05874669 -0.46370388], action=1, reward=1.0, next_state=[-0.02303488  0.35598888 -0.06802077 -0.77430975]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 15 ] state=[-0.02303488  0.35598888 -0.06802077 -0.77430975], action=0, reward=1.0, next_state=[-0.0159151   0.16186531 -0.08350696 -0.50378044]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 16 ] state=[-0.0159151   0.16186531 -0.08350696 -0.50378044], action=0, reward=1.0, next_state=[-0.0126778  -0.03198656 -0.09358257 -0.23853843]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 17 ] state=[-0.0126778  -0.03198656 -0.09358257 -0.23853843], action=0, reward=1.0, next_state=[-0.01331753 -0.22565567 -0.09835334  0.02322095]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 18 ] state=[-0.01331753 -0.22565567 -0.09835334  0.02322095], action=1, reward=1.0, next_state=[-0.01783064 -0.02927087 -0.09788892 -0.2988019 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 19 ] state=[-0.01783064 -0.02927087 -0.09788892 -0.2988019 ], action=0, reward=1.0, next_state=[-0.01841606 -0.22287111 -0.10386496 -0.03852518]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 20 ] state=[-0.01841606 -0.22287111 -0.10386496 -0.03852518], action=1, reward=1.0, next_state=[-0.02287348 -0.02642493 -0.10463546 -0.36208786]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 21 ] state=[-0.02287348 -0.02642493 -0.10463546 -0.36208786], action=1, reward=1.0, next_state=[-0.02340198  0.17001656 -0.11187722 -0.68584522]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 22 ] state=[-0.02340198  0.17001656 -0.11187722 -0.68584522], action=0, reward=1.0, next_state=[-0.02000165 -0.02338911 -0.12559413 -0.43037374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 23 ] state=[-0.02000165 -0.02338911 -0.12559413 -0.43037374], action=1, reward=1.0, next_state=[-0.02046943  0.17326674 -0.1342016  -0.75986172]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 24 ] state=[-0.02046943  0.17326674 -0.1342016  -0.75986172], action=0, reward=1.0, next_state=[-0.0170041  -0.01977609 -0.14939883 -0.51223805]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 25 ] state=[-0.0170041  -0.01977609 -0.14939883 -0.51223805], action=1, reward=1.0, next_state=[-0.01739962  0.17709947 -0.1596436  -0.84802186]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 26 ] state=[-0.01739962  0.17709947 -0.1596436  -0.84802186], action=0, reward=1.0, next_state=[-0.01385763 -0.01552702 -0.17660403 -0.60949239]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 27 ] state=[-0.01385763 -0.01552702 -0.17660403 -0.60949239], action=0, reward=1.0, next_state=[-0.01416817 -0.20779797 -0.18879388 -0.37722394]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 28 ] state=[-0.01416817 -0.20779797 -0.18879388 -0.37722394], action=1, reward=1.0, next_state=[-0.01832413 -0.01056668 -0.19633836 -0.72299033]\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 29 ] state=[-0.01832413 -0.01056668 -0.19633836 -0.72299033], action=0, reward=-1.0, next_state=[-0.01853546 -0.20250966 -0.21079817 -0.49796075]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 23: Exploration_rate=0.06641813604822402. Score=29.\n",
      "[ episode 24 ] state=[-0.00203755 -0.01872867  0.04701859  0.03601537]\n",
      "[ episode 24 ][ timestamp 1 ] state=[-0.00203755 -0.01872867  0.04701859  0.03601537], action=1, reward=1.0, next_state=[-0.00241213  0.17568859  0.0477389  -0.24146986]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 2 ] state=[-0.00241213  0.17568859  0.0477389  -0.24146986], action=1, reward=1.0, next_state=[ 0.00110164  0.37009726  0.0429095  -0.51872072]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 3 ] state=[ 0.00110164  0.37009726  0.0429095  -0.51872072], action=0, reward=1.0, next_state=[ 0.00850359  0.17439829  0.03253509 -0.21283094]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 4 ] state=[ 0.00850359  0.17439829  0.03253509 -0.21283094], action=0, reward=1.0, next_state=[ 0.01199155 -0.02117336  0.02827847  0.0899349 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 5 ] state=[ 0.01199155 -0.02117336  0.02827847  0.0899349 ], action=1, reward=1.0, next_state=[ 0.01156809  0.17353209  0.03007716 -0.19369374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 6 ] state=[ 0.01156809  0.17353209  0.03007716 -0.19369374], action=1, reward=1.0, next_state=[ 0.01503873  0.36821117  0.02620329 -0.47673893]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 7 ] state=[ 0.01503873  0.36821117  0.02620329 -0.47673893], action=0, reward=1.0, next_state=[ 0.02240295  0.17272922  0.01666851 -0.17591379]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 8 ] state=[ 0.02240295  0.17272922  0.01666851 -0.17591379], action=1, reward=1.0, next_state=[ 0.02585754  0.36760871  0.01315024 -0.4632921 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 24 ][ timestamp 9 ] state=[ 0.02585754  0.36760871  0.01315024 -0.4632921 ], action=0, reward=1.0, next_state=[ 0.03320971  0.17230341  0.00388439 -0.16649342]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 10 ] state=[ 0.03320971  0.17230341  0.00388439 -0.16649342], action=1, reward=1.0, next_state=[ 0.03665578  0.36736954  0.00055452 -0.4579484 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 11 ] state=[ 0.03665578  0.36736954  0.00055452 -0.4579484 ], action=0, reward=1.0, next_state=[ 0.04400317  0.17223976 -0.00860444 -0.16509074]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 12 ] state=[ 0.04400317  0.17223976 -0.00860444 -0.16509074], action=1, reward=1.0, next_state=[ 0.04744797  0.36748382 -0.01190626 -0.46047566]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 13 ] state=[ 0.04744797  0.36748382 -0.01190626 -0.46047566], action=0, reward=1.0, next_state=[ 0.05479764  0.17253216 -0.02111577 -0.17156926]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 14 ] state=[ 0.05479764  0.17253216 -0.02111577 -0.17156926], action=1, reward=1.0, next_state=[ 0.05824829  0.36794987 -0.02454716 -0.47083806]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 15 ] state=[ 0.05824829  0.36794987 -0.02454716 -0.47083806], action=0, reward=1.0, next_state=[ 0.06560728  0.1731831  -0.03396392 -0.18599205]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 16 ] state=[ 0.06560728  0.1731831  -0.03396392 -0.18599205], action=1, reward=1.0, next_state=[ 0.06907094  0.36877412 -0.03768376 -0.48919286]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 17 ] state=[ 0.06907094  0.36877412 -0.03768376 -0.48919286], action=1, reward=1.0, next_state=[ 0.07644643  0.56440689 -0.04746762 -0.79351007]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 18 ] state=[ 0.07644643  0.56440689 -0.04746762 -0.79351007], action=0, reward=1.0, next_state=[ 0.08773456  0.36996755 -0.06333782 -0.51612983]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 19 ] state=[ 0.08773456  0.36996755 -0.06333782 -0.51612983], action=1, reward=1.0, next_state=[ 0.09513392  0.56592154 -0.07366041 -0.8280803 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 20 ] state=[ 0.09513392  0.56592154 -0.07366041 -0.8280803 ], action=0, reward=1.0, next_state=[ 0.10645235  0.37187983 -0.09022202 -0.5594436 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 21 ] state=[ 0.10645235  0.37187983 -0.09022202 -0.5594436 ], action=0, reward=1.0, next_state=[ 0.11388994  0.17813236 -0.10141089 -0.29649372]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 22 ] state=[ 0.11388994  0.17813236 -0.10141089 -0.29649372], action=1, reward=1.0, next_state=[ 0.11745259  0.37454277 -0.10734077 -0.61935941]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 23 ] state=[ 0.11745259  0.37454277 -0.10734077 -0.61935941], action=0, reward=1.0, next_state=[ 0.12494345  0.18107083 -0.11972795 -0.36231942]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 24 ] state=[ 0.12494345  0.18107083 -0.11972795 -0.36231942], action=0, reward=1.0, next_state=[ 0.12856486 -0.01216399 -0.12697434 -0.10965817]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 25 ] state=[ 0.12856486 -0.01216399 -0.12697434 -0.10965817], action=1, reward=1.0, next_state=[ 0.12832158  0.18452717 -0.12916751 -0.43954997]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 26 ] state=[ 0.12832158  0.18452717 -0.12916751 -0.43954997], action=0, reward=1.0, next_state=[ 0.13201213 -0.00855274 -0.13795851 -0.19021253]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 27 ] state=[ 0.13201213 -0.00855274 -0.13795851 -0.19021253], action=1, reward=1.0, next_state=[ 0.13184107  0.18824555 -0.14176276 -0.52303649]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 28 ] state=[ 0.13184107  0.18824555 -0.14176276 -0.52303649], action=0, reward=1.0, next_state=[ 0.13560598 -0.00462644 -0.15222349 -0.27816951]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 29 ] state=[ 0.13560598 -0.00462644 -0.15222349 -0.27816951], action=0, reward=1.0, next_state=[ 0.13551345 -0.19728616 -0.15778688 -0.03710277]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 30 ] state=[ 0.13551345 -0.19728616 -0.15778688 -0.03710277], action=1, reward=1.0, next_state=[ 1.31567731e-01 -2.94785943e-04 -1.58528932e-01 -3.75116213e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 31 ] state=[ 1.31567731e-01 -2.94785943e-04 -1.58528932e-01 -3.75116213e-01], action=1, reward=1.0, next_state=[ 0.13156184  0.19668194 -0.16603126 -0.71328887]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 32 ] state=[ 0.13156184  0.19668194 -0.16603126 -0.71328887], action=0, reward=1.0, next_state=[ 0.13549547  0.00420025 -0.18029703 -0.47712596]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 33 ] state=[ 0.13549547  0.00420025 -0.18029703 -0.47712596], action=1, reward=1.0, next_state=[ 0.13557948  0.20134856 -0.18983955 -0.8207755 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 34 ] state=[ 0.13557948  0.20134856 -0.18983955 -0.8207755 ], action=1, reward=1.0, next_state=[ 0.13960645  0.39849047 -0.20625506 -1.16665395]\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 35 ] state=[ 0.13960645  0.39849047 -0.20625506 -1.16665395], action=0, reward=-1.0, next_state=[ 0.14757626  0.2065604  -0.22958814 -0.94507082]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 24: Exploration_rate=0.05573070148010834. Score=35.\n",
      "[ episode 25 ] state=[ 0.04008623 -0.04576434  0.02428739  0.01729052]\n",
      "[ episode 25 ][ timestamp 1 ] state=[ 0.04008623 -0.04576434  0.02428739  0.01729052], action=1, reward=1.0, next_state=[ 0.03917095  0.14900103  0.0246332  -0.26763159]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 2 ] state=[ 0.03917095  0.14900103  0.0246332  -0.26763159], action=1, reward=1.0, next_state=[ 0.04215097  0.34376292  0.01928056 -0.55244437]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 3 ] state=[ 0.04215097  0.34376292  0.01928056 -0.55244437], action=0, reward=1.0, next_state=[ 0.04902623  0.14837558  0.00823168 -0.2537497 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 4 ] state=[ 0.04902623  0.14837558  0.00823168 -0.2537497 ], action=1, reward=1.0, next_state=[ 0.05199374  0.34337904  0.00315668 -0.54382489]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 5 ] state=[ 0.05199374  0.34337904  0.00315668 -0.54382489], action=0, reward=1.0, next_state=[ 0.05886132  0.14821287 -0.00771981 -0.25014903]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 6 ] state=[ 0.05886132  0.14821287 -0.00771981 -0.25014903], action=1, reward=1.0, next_state=[ 0.06182558  0.3434442  -0.0127228  -0.54525691]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 7 ] state=[ 0.06182558  0.3434442  -0.0127228  -0.54525691], action=0, reward=1.0, next_state=[ 0.06869446  0.14850332 -0.02362793 -0.25660964]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 8 ] state=[ 0.06869446  0.14850332 -0.02362793 -0.25660964], action=1, reward=1.0, next_state=[ 0.07166453  0.3439545  -0.02876013 -0.55665055]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 9 ] state=[ 0.07166453  0.3439545  -0.02876013 -0.55665055], action=0, reward=1.0, next_state=[ 0.07854362  0.14924787 -0.03989314 -0.2731657 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 10 ] state=[ 0.07854362  0.14924787 -0.03989314 -0.2731657 ], action=1, reward=1.0, next_state=[ 0.08152857  0.34491567 -0.04535645 -0.57815935]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 11 ] state=[ 0.08152857  0.34491567 -0.04535645 -0.57815935], action=0, reward=1.0, next_state=[ 0.08842689  0.15045776 -0.05691964 -0.3001027 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 12 ] state=[ 0.08842689  0.15045776 -0.05691964 -0.3001027 ], action=0, reward=1.0, next_state=[ 0.09143604 -0.04380863 -0.06292169 -0.02590036]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 13 ] state=[ 0.09143604 -0.04380863 -0.06292169 -0.02590036], action=1, reward=1.0, next_state=[ 0.09055987  0.15215659 -0.0634397  -0.33775327]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 14 ] state=[ 0.09055987  0.15215659 -0.0634397  -0.33775327], action=0, reward=1.0, next_state=[ 0.093603   -0.04200793 -0.07019476 -0.06573112]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 15 ] state=[ 0.093603   -0.04200793 -0.07019476 -0.06573112], action=0, reward=1.0, next_state=[ 0.09276284 -0.23605694 -0.07150939  0.20400526]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 16 ] state=[ 0.09276284 -0.23605694 -0.07150939  0.20400526], action=1, reward=1.0, next_state=[ 0.0880417  -0.03998907 -0.06742928 -0.11035075]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 25 ][ timestamp 17 ] state=[ 0.0880417  -0.03998907 -0.06742928 -0.11035075], action=1, reward=1.0, next_state=[ 0.08724192  0.15603107 -0.0696363  -0.42352197]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 18 ] state=[ 0.08724192  0.15603107 -0.0696363  -0.42352197], action=0, reward=1.0, next_state=[ 0.09036254 -0.03803885 -0.07810674 -0.15357916]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 19 ] state=[ 0.09036254 -0.03803885 -0.07810674 -0.15357916], action=1, reward=1.0, next_state=[ 0.08960177  0.15810954 -0.08117832 -0.46984476]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 20 ] state=[ 0.08960177  0.15810954 -0.08117832 -0.46984476], action=0, reward=1.0, next_state=[ 0.09276396 -0.03577748 -0.09057521 -0.2038122 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 21 ] state=[ 0.09276396 -0.03577748 -0.09057521 -0.2038122 ], action=1, reward=1.0, next_state=[ 0.09204841  0.16051518 -0.09465146 -0.52363697]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 22 ] state=[ 0.09204841  0.16051518 -0.09465146 -0.52363697], action=0, reward=1.0, next_state=[ 0.09525871 -0.03315612 -0.1051242  -0.26221634]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 23 ] state=[ 0.09525871 -0.03315612 -0.1051242  -0.26221634], action=1, reward=1.0, next_state=[ 0.09459559  0.16329707 -0.11036853 -0.58611898]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 24 ] state=[ 0.09459559  0.16329707 -0.11036853 -0.58611898], action=0, reward=1.0, next_state=[ 0.09786153 -0.03012008 -0.1220909  -0.33014102]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 25 ] state=[ 0.09786153 -0.03012008 -0.1220909  -0.33014102], action=1, reward=1.0, next_state=[ 0.09725913  0.16650914 -0.12869373 -0.65869495]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 26 ] state=[ 0.09725913  0.16650914 -0.12869373 -0.65869495], action=1, reward=1.0, next_state=[ 0.10058931  0.3631651  -0.14186762 -0.98897109]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 27 ] state=[ 0.10058931  0.3631651  -0.14186762 -0.98897109], action=0, reward=1.0, next_state=[ 0.10785261  0.17019791 -0.16164705 -0.74399753]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 28 ] state=[ 0.10785261  0.17019791 -0.16164705 -0.74399753], action=0, reward=1.0, next_state=[ 0.11125657 -0.0223679  -0.176527   -0.50623193]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 29 ] state=[ 0.11125657 -0.0223679  -0.176527   -0.50623193], action=0, reward=1.0, next_state=[ 0.11080921 -0.21462054 -0.18665164 -0.27396432]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 30 ] state=[ 0.11080921 -0.21462054 -0.18665164 -0.27396432], action=1, reward=1.0, next_state=[ 0.1065168  -0.01739402 -0.19213092 -0.6192232 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 31 ] state=[ 0.1065168  -0.01739402 -0.19213092 -0.6192232 ], action=0, reward=1.0, next_state=[ 0.10616892 -0.20938704 -0.20451539 -0.3926724 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 32 ] state=[ 0.10616892 -0.20938704 -0.20451539 -0.3926724 ], action=1, reward=-1.0, next_state=[ 0.10198118 -0.0120393  -0.21236883 -0.74223402]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 25: Exploration_rate=0.047471515157588115. Score=32.\n",
      "[ episode 26 ] state=[ 0.01611798 -0.01455908 -0.00213678 -0.01430272]\n",
      "[ episode 26 ][ timestamp 1 ] state=[ 0.01611798 -0.01455908 -0.00213678 -0.01430272], action=1, reward=1.0, next_state=[ 0.0158268   0.18059345 -0.00242284 -0.30765906]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 2 ] state=[ 0.0158268   0.18059345 -0.00242284 -0.30765906], action=1, reward=1.0, next_state=[ 0.01943867  0.37574984 -0.00857602 -0.6011051 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 3 ] state=[ 0.01943867  0.37574984 -0.00857602 -0.6011051 ], action=0, reward=1.0, next_state=[ 0.02695367  0.1807489  -0.02059812 -0.31113577]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 4 ] state=[ 0.02695367  0.1807489  -0.02059812 -0.31113577], action=0, reward=1.0, next_state=[ 0.03056865 -0.01407362 -0.02682084 -0.02501939]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 5 ] state=[ 0.03056865 -0.01407362 -0.02682084 -0.02501939], action=1, reward=1.0, next_state=[ 0.03028717  0.18142249 -0.02732122 -0.32604248]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 6 ] state=[ 0.03028717  0.18142249 -0.02732122 -0.32604248], action=0, reward=1.0, next_state=[ 0.03391562 -0.01330003 -0.03384207 -0.04209914]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 7 ] state=[ 0.03391562 -0.01330003 -0.03384207 -0.04209914], action=0, reward=1.0, next_state=[ 0.03364962 -0.20792076 -0.03468406  0.23971713]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 8 ] state=[ 0.03364962 -0.20792076 -0.03468406  0.23971713], action=1, reward=1.0, next_state=[ 0.02949121 -0.01232095 -0.02988971 -0.06370121]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 9 ] state=[ 0.02949121 -0.01232095 -0.02988971 -0.06370121], action=1, reward=1.0, next_state=[ 0.02924479  0.18321652 -0.03116374 -0.36566266]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 10 ] state=[ 0.02924479  0.18321652 -0.03116374 -0.36566266], action=0, reward=1.0, next_state=[ 0.03290912 -0.01144903 -0.03847699 -0.08296678]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 11 ] state=[ 0.03290912 -0.01144903 -0.03847699 -0.08296678], action=1, reward=1.0, next_state=[ 0.03268014  0.18420275 -0.04013633 -0.38753668]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 12 ] state=[ 0.03268014  0.18420275 -0.04013633 -0.38753668], action=1, reward=1.0, next_state=[ 0.03636419  0.37987077 -0.04788706 -0.69259924]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 13 ] state=[ 0.03636419  0.37987077 -0.04788706 -0.69259924], action=0, reward=1.0, next_state=[ 0.04396161  0.18544474 -0.06173904 -0.41536793]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 14 ] state=[ 0.04396161  0.18544474 -0.06173904 -0.41536793], action=0, reward=1.0, next_state=[ 0.0476705  -0.00875033 -0.0700464  -0.14277007]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 15 ] state=[ 0.0476705  -0.00875033 -0.0700464  -0.14277007], action=1, reward=1.0, next_state=[ 0.0474955   0.18730125 -0.0729018  -0.45670309]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 16 ] state=[ 0.0474955   0.18730125 -0.0729018  -0.45670309], action=0, reward=1.0, next_state=[ 0.05124152 -0.00671839 -0.08203587 -0.1878608 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 17 ] state=[ 0.05124152 -0.00671839 -0.08203587 -0.1878608 ], action=0, reward=1.0, next_state=[ 0.05110715 -0.20057667 -0.08579308  0.07785719]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 18 ] state=[ 0.05110715 -0.20057667 -0.08579308  0.07785719], action=1, reward=1.0, next_state=[ 0.04709562 -0.00433629 -0.08423594 -0.24061297]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 19 ] state=[ 0.04709562 -0.00433629 -0.08423594 -0.24061297], action=0, reward=1.0, next_state=[ 0.04700889 -0.19816024 -0.0890482   0.02435598]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 20 ] state=[ 0.04700889 -0.19816024 -0.0890482   0.02435598], action=1, reward=1.0, next_state=[ 0.04304569 -0.00188158 -0.08856108 -0.29504105]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 21 ] state=[ 0.04304569 -0.00188158 -0.08856108 -0.29504105], action=0, reward=1.0, next_state=[ 0.04300806 -0.19563667 -0.0944619  -0.03155032]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 22 ] state=[ 0.04300806 -0.19563667 -0.0944619  -0.03155032], action=1, reward=1.0, next_state=[ 0.03909532  0.00070405 -0.0950929  -0.35247894]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 23 ] state=[ 0.03909532  0.00070405 -0.0950929  -0.35247894], action=1, reward=1.0, next_state=[ 0.03910941  0.19704054 -0.10214248 -0.67356832]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 24 ] state=[ 0.03910941  0.19704054 -0.10214248 -0.67356832], action=1, reward=1.0, next_state=[ 0.04305022  0.39342267 -0.11561385 -0.99658389]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 25 ] state=[ 0.04305022  0.39342267 -0.11561385 -0.99658389], action=0, reward=1.0, next_state=[ 0.05091867  0.20002059 -0.13554553 -0.74233226]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 26 ][ timestamp 26 ] state=[ 0.05091867  0.20002059 -0.13554553 -0.74233226], action=0, reward=1.0, next_state=[ 0.05491908  0.00700408 -0.15039217 -0.49519156]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 27 ] state=[ 0.05491908  0.00700408 -0.15039217 -0.49519156], action=1, reward=1.0, next_state=[ 0.05505916  0.20389127 -0.160296   -0.83123757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 28 ] state=[ 0.05505916  0.20389127 -0.160296   -0.83123757], action=0, reward=1.0, next_state=[ 0.05913699  0.01128046 -0.17692076 -0.59295071]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 29 ] state=[ 0.05913699  0.01128046 -0.17692076 -0.59295071], action=1, reward=1.0, next_state=[ 0.0593626   0.20838    -0.18877977 -0.9357288 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 30 ] state=[ 0.0593626   0.20838    -0.18877977 -0.9357288 ], action=0, reward=1.0, next_state=[ 0.0635302   0.01623638 -0.20749435 -0.70780599]\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 31 ] state=[ 0.0635302   0.01623638 -0.20749435 -0.70780599], action=1, reward=-1.0, next_state=[ 0.06385493  0.21353605 -0.22165047 -1.05797397]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 26: Exploration_rate=0.04063952250178857. Score=31.\n",
      "[ episode 27 ] state=[ 0.00761592 -0.01799296  0.03406501  0.00724279]\n",
      "[ episode 27 ][ timestamp 1 ] state=[ 0.00761592 -0.01799296  0.03406501  0.00724279], action=1, reward=1.0, next_state=[ 0.00725607  0.17662432  0.03420987 -0.27450058]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 2 ] state=[ 0.00725607  0.17662432  0.03420987 -0.27450058], action=0, reward=1.0, next_state=[ 0.01078855 -0.01896862  0.02871986  0.02877289]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 3 ] state=[ 0.01078855 -0.01896862  0.02871986  0.02877289], action=1, reward=1.0, next_state=[ 0.01040918  0.17572995  0.02929531 -0.25471205]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 4 ] state=[ 0.01040918  0.17572995  0.02929531 -0.25471205], action=0, reward=1.0, next_state=[ 0.01392378 -0.01979777  0.02420107  0.04706528]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 5 ] state=[ 0.01392378 -0.01979777  0.02420107  0.04706528], action=1, reward=1.0, next_state=[ 0.01352782  0.17496895  0.02514238 -0.23788482]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 6 ] state=[ 0.01352782  0.17496895  0.02514238 -0.23788482], action=1, reward=1.0, next_state=[ 0.0170272   0.36972285  0.02038468 -0.52253227]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 7 ] state=[ 0.0170272   0.36972285  0.02038468 -0.52253227], action=1, reward=1.0, next_state=[ 0.02442166  0.56455205  0.00993404 -0.80872266]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 8 ] state=[ 0.02442166  0.56455205  0.00993404 -0.80872266], action=0, reward=1.0, next_state=[ 0.0357127   0.36929539 -0.00624042 -0.51293156]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 9 ] state=[ 0.0357127   0.36929539 -0.00624042 -0.51293156], action=1, reward=1.0, next_state=[ 0.04309861  0.56450467 -0.01649905 -0.80757447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 10 ] state=[ 0.04309861  0.56450467 -0.01649905 -0.80757447], action=0, reward=1.0, next_state=[ 0.0543887   0.36961268 -0.03265054 -0.52012677]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 11 ] state=[ 0.0543887   0.36961268 -0.03265054 -0.52012677], action=0, reward=1.0, next_state=[ 0.06178096  0.17496522 -0.04305307 -0.23790874]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 12 ] state=[ 0.06178096  0.17496522 -0.04305307 -0.23790874], action=0, reward=1.0, next_state=[ 0.06528026 -0.01951607 -0.04781125  0.04088917]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 13 ] state=[ 0.06528026 -0.01951607 -0.04781125  0.04088917], action=1, reward=1.0, next_state=[ 0.06488994  0.17625772 -0.04699346 -0.26648709]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 14 ] state=[ 0.06488994  0.17625772 -0.04699346 -0.26648709], action=1, reward=1.0, next_state=[ 0.06841509  0.37201777 -0.05232321 -0.57361398]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 15 ] state=[ 0.06841509  0.37201777 -0.05232321 -0.57361398], action=0, reward=1.0, next_state=[ 0.07585545  0.17766695 -0.06379549 -0.29786272]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 16 ] state=[ 0.07585545  0.17766695 -0.06379549 -0.29786272], action=0, reward=1.0, next_state=[ 0.07940879 -0.01649033 -0.06975274 -0.02596239]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 17 ] state=[ 0.07940879 -0.01649033 -0.06975274 -0.02596239], action=0, reward=1.0, next_state=[ 0.07907898 -0.21054624 -0.07027199  0.24392296]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 18 ] state=[ 0.07907898 -0.21054624 -0.07027199  0.24392296], action=1, reward=1.0, next_state=[ 0.07486806 -0.0144946  -0.06539353 -0.07007167]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 19 ] state=[ 0.07486806 -0.0144946  -0.06539353 -0.07007167], action=1, reward=1.0, next_state=[ 0.07457816  0.18150096 -0.06679496 -0.38264864]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 20 ] state=[ 0.07457816  0.18150096 -0.06679496 -0.38264864], action=1, reward=1.0, next_state=[ 0.07820818  0.37750458 -0.07444794 -0.69562156]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 21 ] state=[ 0.07820818  0.37750458 -0.07444794 -0.69562156], action=0, reward=1.0, next_state=[ 0.08575827  0.1834898  -0.08836037 -0.427273  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 22 ] state=[ 0.08575827  0.1834898  -0.08836037 -0.427273  ], action=1, reward=1.0, next_state=[ 0.08942807  0.37974484 -0.09690583 -0.74645126]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 23 ] state=[ 0.08942807  0.37974484 -0.09690583 -0.74645126], action=1, reward=1.0, next_state=[ 0.09702297  0.57606081 -0.11183485 -1.06798938]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 24 ] state=[ 0.09702297  0.57606081 -0.11183485 -1.06798938], action=0, reward=1.0, next_state=[ 0.10854418  0.38258153 -0.13319464 -0.81239441]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 25 ] state=[ 0.10854418  0.38258153 -0.13319464 -0.81239441], action=0, reward=1.0, next_state=[ 0.11619581  0.18951092 -0.14944253 -0.56439715]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 26 ] state=[ 0.11619581  0.18951092 -0.14944253 -0.56439715], action=1, reward=1.0, next_state=[ 0.11998603  0.38637873 -0.16073047 -0.90018026]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 27 ] state=[ 0.11998603  0.38637873 -0.16073047 -0.90018026], action=0, reward=1.0, next_state=[ 0.12771361  0.19375687 -0.17873408 -0.66202317]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 28 ] state=[ 0.12771361  0.19375687 -0.17873408 -0.66202317], action=0, reward=1.0, next_state=[ 0.13158874  0.00151257 -0.19197454 -0.43051902]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 29 ] state=[ 0.13158874  0.00151257 -0.19197454 -0.43051902], action=1, reward=1.0, next_state=[ 0.131619    0.19876066 -0.20058492 -0.77705029]\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 30 ] state=[ 0.131619    0.19876066 -0.20058492 -0.77705029], action=0, reward=-1.0, next_state=[ 0.13559421  0.00687884 -0.21612593 -0.55357565]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 27: Exploration_rate=0.03496560272750046. Score=30.\n",
      "[ episode 28 ] state=[-0.0144156  -0.0183929   0.00747609 -0.03979776]\n",
      "[ episode 28 ][ timestamp 1 ] state=[-0.0144156  -0.0183929   0.00747609 -0.03979776], action=1, reward=1.0, next_state=[-0.01478345  0.17662105  0.00668014 -0.33011256]\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 2 ] state=[-0.01478345  0.17662105  0.00668014 -0.33011256], action=0, reward=1.0, next_state=[-1.12510324e-02 -1.85953507e-02  7.78883356e-05 -3.53305419e-02]\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 3 ] state=[-1.12510324e-02 -1.85953507e-02  7.78883356e-05 -3.53305419e-02], action=1, reward=1.0, next_state=[-0.01162294  0.17652548 -0.00062872 -0.32798889]\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 4 ] state=[-0.01162294  0.17652548 -0.00062872 -0.32798889], action=1, reward=1.0, next_state=[-0.00809243  0.37165638 -0.0071885  -0.62087002]\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 5 ] state=[-0.00809243  0.37165638 -0.0071885  -0.62087002], action=1, reward=1.0, next_state=[-6.59302188e-04  5.66877980e-01 -1.96059008e-02 -9.15808259e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 6 ] state=[-6.59302188e-04  5.66877980e-01 -1.96059008e-02 -9.15808259e-01], action=0, reward=1.0, next_state=[ 0.01067826  0.37202657 -0.03792207 -0.62935109]\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 7 ] state=[ 0.01067826  0.37202657 -0.03792207 -0.62935109], action=0, reward=1.0, next_state=[ 0.01811879  0.17745377 -0.05050909 -0.34884813]\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 8 ] state=[ 0.01811879  0.17745377 -0.05050909 -0.34884813], action=0, reward=1.0, next_state=[ 0.02166786 -0.01691478 -0.05748605 -0.07251049]\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 9 ] state=[ 0.02166786 -0.01691478 -0.05748605 -0.07251049], action=1, reward=1.0, next_state=[ 0.02132957  0.17898218 -0.05893626 -0.38276212]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 28 ][ timestamp 10 ] state=[ 0.02132957  0.17898218 -0.05893626 -0.38276212], action=0, reward=1.0, next_state=[ 0.02490921 -0.0152556  -0.0665915  -0.10922854]\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 11 ] state=[ 0.02490921 -0.0152556  -0.0665915  -0.10922854], action=1, reward=1.0, next_state=[ 0.0246041   0.18075427 -0.06877607 -0.42215513]\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 12 ] state=[ 0.0246041   0.18075427 -0.06877607 -0.42215513], action=1, reward=1.0, next_state=[ 0.02821919  0.37677977 -0.07721918 -0.73570247]\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 13 ] state=[ 0.02821919  0.37677977 -0.07721918 -0.73570247], action=0, reward=1.0, next_state=[ 0.03575478  0.18280459 -0.09193323 -0.46828662]\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 14 ] state=[ 0.03575478  0.18280459 -0.09193323 -0.46828662], action=0, reward=1.0, next_state=[ 0.03941087 -0.01090651 -0.10129896 -0.20593732]\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 15 ] state=[ 0.03941087 -0.01090651 -0.10129896 -0.20593732], action=1, reward=1.0, next_state=[ 0.03919274  0.18550715 -0.1054177  -0.52877846]\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 16 ] state=[ 0.03919274  0.18550715 -0.1054177  -0.52877846], action=0, reward=1.0, next_state=[ 0.04290289 -0.00798609 -0.11599327 -0.27108525]\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 17 ] state=[ 0.04290289 -0.00798609 -0.11599327 -0.27108525], action=1, reward=1.0, next_state=[ 0.04274316  0.18858355 -0.12141498 -0.59798398]\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 18 ] state=[ 0.04274316  0.18858355 -0.12141498 -0.59798398], action=0, reward=1.0, next_state=[ 0.04651483 -0.00464904 -0.13337466 -0.34587726]\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 19 ] state=[ 0.04651483 -0.00464904 -0.13337466 -0.34587726], action=0, reward=1.0, next_state=[ 0.04642185 -0.1976466  -0.1402922  -0.09804799]\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 20 ] state=[ 0.04642185 -0.1976466  -0.1402922  -0.09804799], action=1, reward=1.0, next_state=[ 0.04246892 -0.00082186 -0.14225316 -0.43149519]\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 21 ] state=[ 0.04246892 -0.00082186 -0.14225316 -0.43149519], action=1, reward=1.0, next_state=[ 0.04245248  0.19599757 -0.15088307 -0.76542378]\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 22 ] state=[ 0.04245248  0.19599757 -0.15088307 -0.76542378], action=0, reward=1.0, next_state=[ 0.04637244  0.00323956 -0.16619154 -0.52376323]\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 23 ] state=[ 0.04637244  0.00323956 -0.16619154 -0.52376323], action=0, reward=1.0, next_state=[ 0.04643723 -0.18920153 -0.17666681 -0.28771449]\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 24 ] state=[ 0.04643723 -0.18920153 -0.17666681 -0.28771449], action=1, reward=1.0, next_state=[ 0.0426532   0.00794194 -0.1824211  -0.63049715]\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 25 ] state=[ 0.0426532   0.00794194 -0.1824211  -0.63049715], action=1, reward=1.0, next_state=[ 0.04281204  0.20507735 -0.19503104 -0.97462862]\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 26 ] state=[ 0.04281204  0.20507735 -0.19503104 -0.97462862], action=0, reward=-1.0, next_state=[ 0.04691358  0.0130302  -0.21452361 -0.74899556]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 28: Exploration_rate=0.030693125720441184. Score=26.\n",
      "[ episode 29 ] state=[-0.01387775  0.02180851  0.04635044 -0.0242206 ]\n",
      "[ episode 29 ][ timestamp 1 ] state=[-0.01387775  0.02180851  0.04635044 -0.0242206 ], action=1, reward=1.0, next_state=[-0.01344158  0.21623617  0.04586603 -0.30192671]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 2 ] state=[-0.01344158  0.21623617  0.04586603 -0.30192671], action=0, reward=1.0, next_state=[-0.00911686  0.02049154  0.0398275   0.00486133]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 3 ] state=[-0.00911686  0.02049154  0.0398275   0.00486133], action=1, reward=1.0, next_state=[-0.00870703  0.21502034  0.03992472 -0.27499429]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 4 ] state=[-0.00870703  0.21502034  0.03992472 -0.27499429], action=1, reward=1.0, next_state=[-0.00440662  0.40955059  0.03442484 -0.55482239]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 5 ] state=[-0.00440662  0.40955059  0.03442484 -0.55482239], action=1, reward=1.0, next_state=[ 0.00378439  0.6041727   0.02332839 -0.83646368]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 6 ] state=[ 0.00378439  0.6041727   0.02332839 -0.83646368], action=0, reward=1.0, next_state=[ 0.01586784  0.40874001  0.00659912 -0.53653649]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 7 ] state=[ 0.01586784  0.40874001  0.00659912 -0.53653649], action=0, reward=1.0, next_state=[ 0.02404264  0.2135259  -0.00413161 -0.24178156]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 8 ] state=[ 0.02404264  0.2135259  -0.00413161 -0.24178156], action=1, reward=1.0, next_state=[ 0.02831316  0.40870662 -0.00896725 -0.53576484]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 9 ] state=[ 0.02831316  0.40870662 -0.00896725 -0.53576484], action=0, reward=1.0, next_state=[ 0.03648729  0.2137119  -0.01968254 -0.24592086]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 10 ] state=[ 0.03648729  0.2137119  -0.01968254 -0.24592086], action=1, reward=1.0, next_state=[ 0.04076153  0.40910936 -0.02460096 -0.54474657]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 11 ] state=[ 0.04076153  0.40910936 -0.02460096 -0.54474657], action=0, reward=1.0, next_state=[ 0.04894372  0.21434158 -0.03549589 -0.25991525]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 12 ] state=[ 0.04894372  0.21434158 -0.03549589 -0.25991525], action=0, reward=1.0, next_state=[ 0.05323055  0.01974386 -0.0406942   0.02136386]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 13 ] state=[ 0.05323055  0.01974386 -0.0406942   0.02136386], action=0, reward=1.0, next_state=[ 0.05362543 -0.17477159 -0.04026692  0.30093467]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 14 ] state=[ 0.05362543 -0.17477159 -0.04026692  0.30093467], action=0, reward=1.0, next_state=[ 0.05013    -0.36929716 -0.03424823  0.58065124]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 15 ] state=[ 0.05013    -0.36929716 -0.03424823  0.58065124], action=1, reward=1.0, next_state=[ 0.04274405 -0.17371247 -0.0226352   0.27737923]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 16 ] state=[ 0.04274405 -0.17371247 -0.0226352   0.27737923], action=1, reward=1.0, next_state=[ 0.0392698   0.02172497 -0.01708762 -0.02235601]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 17 ] state=[ 0.0392698   0.02172497 -0.01708762 -0.02235601], action=1, reward=1.0, next_state=[ 0.0397043   0.21708775 -0.01753474 -0.32038092]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 18 ] state=[ 0.0397043   0.21708775 -0.01753474 -0.32038092], action=0, reward=1.0, next_state=[ 0.04404606  0.02221985 -0.02394235 -0.03327895]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 19 ] state=[ 0.04404606  0.02221985 -0.02394235 -0.03327895], action=0, reward=1.0, next_state=[ 0.04449045 -0.17255072 -0.02460793  0.2517548 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 20 ] state=[ 0.04449045 -0.17255072 -0.02460793  0.2517548 ], action=1, reward=1.0, next_state=[ 0.04103944  0.02291382 -0.01957284 -0.04858725]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 21 ] state=[ 0.04103944  0.02291382 -0.01957284 -0.04858725], action=0, reward=1.0, next_state=[ 0.04149772 -0.17192208 -0.02054458  0.23785658]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 22 ] state=[ 0.04149772 -0.17192208 -0.02054458  0.23785658], action=1, reward=1.0, next_state=[ 0.03805927  0.02348726 -0.01578745 -0.06123526]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 23 ] state=[ 0.03805927  0.02348726 -0.01578745 -0.06123526], action=0, reward=1.0, next_state=[ 0.03852902 -0.17140482 -0.01701216  0.22642511]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 24 ] state=[ 0.03852902 -0.17140482 -0.01701216  0.22642511], action=1, reward=1.0, next_state=[ 0.03510092  0.02395607 -0.01248365 -0.07157516]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 29 ][ timestamp 25 ] state=[ 0.03510092  0.02395607 -0.01248365 -0.07157516], action=1, reward=1.0, next_state=[ 0.03558004  0.21925475 -0.01391516 -0.36817045]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 26 ] state=[ 0.03558004  0.21925475 -0.01391516 -0.36817045], action=0, reward=1.0, next_state=[ 0.03996514  0.02433326 -0.02127857 -0.07990744]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 27 ] state=[ 0.03996514  0.02433326 -0.02127857 -0.07990744], action=1, reward=1.0, next_state=[ 0.0404518   0.21975368 -0.02287671 -0.37922714]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 28 ] state=[ 0.0404518   0.21975368 -0.02287671 -0.37922714], action=1, reward=1.0, next_state=[ 0.04484688  0.41519291 -0.03046126 -0.67903444]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 29 ] state=[ 0.04484688  0.41519291 -0.03046126 -0.67903444], action=0, reward=1.0, next_state=[ 0.05315074  0.22050706 -0.04404195 -0.39609537]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 30 ] state=[ 0.05315074  0.22050706 -0.04404195 -0.39609537], action=1, reward=1.0, next_state=[ 0.05756088  0.41622532 -0.05196385 -0.70233223]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 31 ] state=[ 0.05756088  0.41622532 -0.05196385 -0.70233223], action=0, reward=1.0, next_state=[ 0.06588538  0.22186064 -0.0660105  -0.42644924]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 32 ] state=[ 0.06588538  0.22186064 -0.0660105  -0.42644924], action=1, reward=1.0, next_state=[ 0.0703226   0.41785243 -0.07453948 -0.73918964]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 33 ] state=[ 0.0703226   0.41785243 -0.07453948 -0.73918964], action=0, reward=1.0, next_state=[ 0.07867964  0.22383456 -0.08932328 -0.47086528]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 34 ] state=[ 0.07867964  0.22383456 -0.08932328 -0.47086528], action=1, reward=1.0, next_state=[ 0.08315634  0.42009716 -0.09874058 -0.79031167]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 35 ] state=[ 0.08315634  0.42009716 -0.09874058 -0.79031167], action=0, reward=1.0, next_state=[ 0.09155828  0.22645973 -0.11454681 -0.53025288]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 36 ] state=[ 0.09155828  0.22645973 -0.11454681 -0.53025288], action=1, reward=1.0, next_state=[ 0.09608747  0.42299091 -0.12515187 -0.85672091]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 37 ] state=[ 0.09608747  0.42299091 -0.12515187 -0.85672091], action=0, reward=1.0, next_state=[ 0.10454729  0.22977603 -0.14228629 -0.60586406]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 38 ] state=[ 0.10454729  0.22977603 -0.14228629 -0.60586406], action=0, reward=1.0, next_state=[ 0.10914281  0.03690026 -0.15440357 -0.36116526]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 39 ] state=[ 0.10914281  0.03690026 -0.15440357 -0.36116526], action=1, reward=1.0, next_state=[ 0.10988082  0.23384116 -0.16162688 -0.69827672]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 40 ] state=[ 0.10988082  0.23384116 -0.16162688 -0.69827672], action=0, reward=1.0, next_state=[ 0.11455764  0.04128533 -0.17559241 -0.46051909]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 41 ] state=[ 0.11455764  0.04128533 -0.17559241 -0.46051909], action=1, reward=1.0, next_state=[ 0.11538335  0.2383978  -0.18480279 -0.80300066]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 42 ] state=[ 0.11538335  0.2383978  -0.18480279 -0.80300066], action=0, reward=1.0, next_state=[ 0.1201513   0.04622545 -0.20086281 -0.57367374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 43 ] state=[ 0.1201513   0.04622545 -0.20086281 -0.57367374], action=1, reward=-1.0, next_state=[ 0.12107581  0.24351252 -0.21233628 -0.92231197]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 29: Exploration_rate=0.024741930939380097. Score=43.\n",
      "[ episode 30 ] state=[-0.01245591 -0.00097128 -0.04308935  0.01573372]\n",
      "[ episode 30 ][ timestamp 1 ] state=[-0.01245591 -0.00097128 -0.04308935  0.01573372], action=1, reward=1.0, next_state=[-0.01247534  0.19474129 -0.04277467 -0.29022699]\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 2 ] state=[-0.01247534  0.19474129 -0.04277467 -0.29022699], action=1, reward=1.0, next_state=[-0.00858051  0.39044625 -0.04857921 -0.59608782]\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 3 ] state=[-0.00858051  0.39044625 -0.04857921 -0.59608782], action=0, reward=1.0, next_state=[-0.00077159  0.19603661 -0.06050097 -0.31909406]\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 4 ] state=[-0.00077159  0.19603661 -0.06050097 -0.31909406], action=1, reward=1.0, next_state=[ 0.00314915  0.3919657  -0.06688285 -0.63022642]\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 5 ] state=[ 0.00314915  0.3919657  -0.06688285 -0.63022642], action=0, reward=1.0, next_state=[ 0.01098846  0.19783764 -0.07948738 -0.35933428]\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 6 ] state=[ 0.01098846  0.19783764 -0.07948738 -0.35933428], action=0, reward=1.0, next_state=[ 0.01494521  0.00393031 -0.08667406 -0.09273636]\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 7 ] state=[ 0.01494521  0.00393031 -0.08667406 -0.09273636], action=0, reward=1.0, next_state=[ 0.01502382 -0.1898493  -0.08852879  0.17139165]\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 8 ] state=[ 0.01502382 -0.1898493  -0.08852879  0.17139165], action=1, reward=1.0, next_state=[ 0.01122683  0.00642087 -0.08510096 -0.14785416]\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 9 ] state=[ 0.01122683  0.00642087 -0.08510096 -0.14785416], action=0, reward=1.0, next_state=[ 0.01135525 -0.18738586 -0.08805804  0.11681438]\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 10 ] state=[ 0.01135525 -0.18738586 -0.08805804  0.11681438], action=0, reward=1.0, next_state=[ 0.00760753 -0.38114297 -0.08572175  0.38046833]\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 11 ] state=[ 0.00760753 -0.38114297 -0.08572175  0.38046833], action=1, reward=1.0, next_state=[-1.53265827e-05 -1.84914992e-01 -7.81123861e-02  6.20358103e-02]\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 12 ] state=[-1.53265827e-05 -1.84914992e-01 -7.81123861e-02  6.20358103e-02], action=1, reward=1.0, next_state=[-0.00371363  0.01123497 -0.07687167 -0.25423367]\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 13 ] state=[-0.00371363  0.01123497 -0.07687167 -0.25423367], action=1, reward=1.0, next_state=[-0.00348893  0.20736555 -0.08195634 -0.57013875]\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 14 ] state=[-0.00348893  0.20736555 -0.08195634 -0.57013875], action=1, reward=1.0, next_state=[ 6.58383965e-04  4.03535463e-01 -9.33591183e-02 -8.87474139e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 15 ] state=[ 6.58383965e-04  4.03535463e-01 -9.33591183e-02 -8.87474139e-01], action=0, reward=1.0, next_state=[ 0.00872909  0.20979622 -0.1111086  -0.62553854]\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 16 ] state=[ 0.00872909  0.20979622 -0.1111086  -0.62553854], action=1, reward=1.0, next_state=[ 0.01292502  0.40627946 -0.12361937 -0.95104483]\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 17 ] state=[ 0.01292502  0.40627946 -0.12361937 -0.95104483], action=1, reward=1.0, next_state=[ 0.02105061  0.60282882 -0.14264027 -1.27987063]\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 18 ] state=[ 0.02105061  0.60282882 -0.14264027 -1.27987063], action=0, reward=1.0, next_state=[ 0.03310718  0.40978318 -0.16823768 -1.03503716]\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 19 ] state=[ 0.03310718  0.40978318 -0.16823768 -1.03503716], action=1, reward=1.0, next_state=[ 0.04130285  0.60669365 -0.18893842 -1.37546159]\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 20 ] state=[ 0.04130285  0.60669365 -0.18893842 -1.37546159], action=0, reward=-1.0, next_state=[ 0.05343672  0.41436708 -0.21644766 -1.14732365]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 30: Exploration_rate=0.022381810029994047. Score=20.\n",
      "[ episode 31 ] state=[-0.02638457 -0.04115002  0.0433485   0.00769043]\n",
      "[ episode 31 ][ timestamp 1 ] state=[-0.02638457 -0.04115002  0.0433485   0.00769043], action=0, reward=1.0, next_state=[-0.02720757 -0.23686597  0.04350231  0.31372905]\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 2 ] state=[-0.02720757 -0.23686597  0.04350231  0.31372905], action=0, reward=1.0, next_state=[-0.03194489 -0.43257977  0.04977689  0.61980764]\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 3 ] state=[-0.03194489 -0.43257977  0.04977689  0.61980764], action=0, reward=1.0, next_state=[-0.04059648 -0.6283603   0.06217305  0.92774306]\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 4 ] state=[-0.04059648 -0.6283603   0.06217305  0.92774306], action=1, reward=1.0, next_state=[-0.05316369 -0.4341304   0.08072791  0.65522822]\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 5 ] state=[-0.05316369 -0.4341304   0.08072791  0.65522822], action=1, reward=1.0, next_state=[-0.0618463  -0.24021967  0.09383247  0.38901764]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 31 ][ timestamp 6 ] state=[-0.0618463  -0.24021967  0.09383247  0.38901764], action=1, reward=1.0, next_state=[-0.06665069 -0.0465461   0.10161282  0.12733153]\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 7 ] state=[-0.06665069 -0.0465461   0.10161282  0.12733153], action=0, reward=1.0, next_state=[-0.06758161 -0.24296583  0.10415946  0.45026417]\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 8 ] state=[-0.06758161 -0.24296583  0.10415946  0.45026417], action=1, reward=1.0, next_state=[-0.07244093 -0.04945939  0.11316474  0.19214517]\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 9 ] state=[-0.07244093 -0.04945939  0.11316474  0.19214517], action=1, reward=1.0, next_state=[-0.07343012  0.14387711  0.11700764 -0.06280516]\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 10 ] state=[-0.07343012  0.14387711  0.11700764 -0.06280516], action=0, reward=1.0, next_state=[-0.07055257 -0.05271114  0.11575154  0.26438274]\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 11 ] state=[-0.07055257 -0.05271114  0.11575154  0.26438274], action=0, reward=1.0, next_state=[-0.0716068  -0.24927862  0.12103919  0.59121591]\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 12 ] state=[-0.0716068  -0.24927862  0.12103919  0.59121591], action=1, reward=1.0, next_state=[-0.07659237 -0.05604061  0.13286351  0.33897827]\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 13 ] state=[-0.07659237 -0.05604061  0.13286351  0.33897827], action=0, reward=1.0, next_state=[-0.07771318 -0.25277819  0.13964308  0.67043081]\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 14 ] state=[-0.07771318 -0.25277819  0.13964308  0.67043081], action=1, reward=1.0, next_state=[-0.08276874 -0.05984546  0.15305169  0.42477056]\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 15 ] state=[-0.08276874 -0.05984546  0.15305169  0.42477056], action=0, reward=1.0, next_state=[-0.08396565 -0.25676654  0.1615471   0.76152103]\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 16 ] state=[-0.08396565 -0.25676654  0.1615471   0.76152103], action=0, reward=1.0, next_state=[-0.08910098 -0.45370142  0.17677752  1.10036562]\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 17 ] state=[-0.08910098 -0.45370142  0.17677752  1.10036562], action=1, reward=1.0, next_state=[-0.09817501 -0.26128989  0.19878484  0.86794862]\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 18 ] state=[-0.09817501 -0.26128989  0.19878484  0.86794862], action=0, reward=-1.0, next_state=[-0.10340081 -0.45848038  0.21614381  1.21596812]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 31: Exploration_rate=0.020450816818411825. Score=18.\n",
      "[ episode 32 ] state=[-0.00978935 -0.03555889 -0.01909043 -0.03236013]\n",
      "[ episode 32 ][ timestamp 1 ] state=[-0.00978935 -0.03555889 -0.01909043 -0.03236013], action=0, reward=1.0, next_state=[-0.01050053 -0.23040195 -0.01973763  0.25423896]\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 2 ] state=[-0.01050053 -0.23040195 -0.01973763  0.25423896], action=1, reward=1.0, next_state=[-0.01510857 -0.03500381 -0.01465285 -0.04460364]\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 3 ] state=[-0.01510857 -0.03500381 -0.01465285 -0.04460364], action=0, reward=1.0, next_state=[-0.01580864 -0.22991262 -0.01554492  0.2434204 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 4 ] state=[-0.01580864 -0.22991262 -0.01554492  0.2434204 ], action=0, reward=1.0, next_state=[-0.0204069  -0.42480912 -0.01067652  0.53115981]\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 5 ] state=[-0.0204069  -0.42480912 -0.01067652  0.53115981], action=0, reward=1.0, next_state=[-2.89030784e-02 -6.19779281e-01 -5.33188860e-05  8.20459546e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 6 ] state=[-2.89030784e-02 -6.19779281e-01 -5.33188860e-05  8.20459546e-01], action=0, reward=1.0, next_state=[-0.04129866 -0.8149005   0.01635587  1.1131257 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 7 ] state=[-0.04129866 -0.8149005   0.01635587  1.1131257 ], action=0, reward=1.0, next_state=[-0.05759667 -1.01023339  0.03861839  1.41089425]\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 8 ] state=[-0.05759667 -1.01023339  0.03861839  1.41089425], action=1, reward=1.0, next_state=[-0.07780134 -0.81561097  0.06683627  1.13052928]\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 9 ] state=[-0.07780134 -0.81561097  0.06683627  1.13052928], action=1, reward=1.0, next_state=[-0.09411356 -0.6214248   0.08944686  0.8595356 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 10 ] state=[-0.09411356 -0.6214248   0.08944686  0.8595356 ], action=1, reward=1.0, next_state=[-0.10654206 -0.42762758  0.10663757  0.59626421]\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 11 ] state=[-0.10654206 -0.42762758  0.10663757  0.59626421], action=1, reward=1.0, next_state=[-0.11509461 -0.23414686  0.11856285  0.33898376]\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 12 ] state=[-0.11509461 -0.23414686  0.11856285  0.33898376], action=1, reward=1.0, next_state=[-0.11977755 -0.04089397  0.12534253  0.08591535]\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 13 ] state=[-0.11977755 -0.04089397  0.12534253  0.08591535], action=0, reward=1.0, next_state=[-0.12059543 -0.23756894  0.12706084  0.41536769]\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 14 ] state=[-0.12059543 -0.23756894  0.12706084  0.41536769], action=1, reward=1.0, next_state=[-0.1253468  -0.04445525  0.13536819  0.16528774]\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 15 ] state=[-0.1253468  -0.04445525  0.13536819  0.16528774], action=0, reward=1.0, next_state=[-0.12623591 -0.24122912  0.13867394  0.49742514]\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 16 ] state=[-0.12623591 -0.24122912  0.13867394  0.49742514], action=0, reward=1.0, next_state=[-0.13106049 -0.43800597  0.14862245  0.83039648]\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 17 ] state=[-0.13106049 -0.43800597  0.14862245  0.83039648], action=1, reward=1.0, next_state=[-0.13982061 -0.24519396  0.16523038  0.58790113]\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 18 ] state=[-0.13982061 -0.24519396  0.16523038  0.58790113], action=0, reward=1.0, next_state=[-0.14472449 -0.44219723  0.1769884   0.92773839]\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 19 ] state=[-0.14472449 -0.44219723  0.1769884   0.92773839], action=1, reward=1.0, next_state=[-0.15356844 -0.24984932  0.19554317  0.69548705]\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 20 ] state=[-0.15356844 -0.24984932  0.19554317  0.69548705], action=0, reward=-1.0, next_state=[-0.15856542 -0.44706823  0.20945291  1.04280161]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 32: Exploration_rate=0.018500023224111744. Score=20.\n",
      "[ episode 33 ] state=[-0.03803954  0.01609746 -0.00118585  0.0388231 ]\n",
      "[ episode 33 ][ timestamp 1 ] state=[-0.03803954  0.01609746 -0.00118585  0.0388231 ], action=0, reward=1.0, next_state=[-0.03771759 -0.17900746 -0.00040939  0.33113164]\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 2 ] state=[-0.03771759 -0.17900746 -0.00040939  0.33113164], action=0, reward=1.0, next_state=[-0.04129774 -0.37412359  0.00621324  0.62368544]\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 3 ] state=[-0.04129774 -0.37412359  0.00621324  0.62368544], action=0, reward=1.0, next_state=[-0.04878021 -0.56933173  0.01868695  0.91831869]\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 4 ] state=[-0.04878021 -0.56933173  0.01868695  0.91831869], action=1, reward=1.0, next_state=[-0.06016685 -0.37446732  0.03705332  0.63156675]\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 5 ] state=[-0.06016685 -0.37446732  0.03705332  0.63156675], action=1, reward=1.0, next_state=[-0.06765619 -0.17988141  0.04968466  0.3507794 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 6 ] state=[-0.06765619 -0.17988141  0.04968466  0.3507794 ], action=1, reward=1.0, next_state=[-0.07125382  0.01450005  0.05670025  0.07416831]\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 7 ] state=[-0.07125382  0.01450005  0.05670025  0.07416831], action=1, reward=1.0, next_state=[-0.07096382  0.20876523  0.05818361 -0.20010024]\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 8 ] state=[-0.07096382  0.20876523  0.05818361 -0.20010024], action=0, reward=1.0, next_state=[-0.06678851  0.01286147  0.05418161  0.11035448]\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 9 ] state=[-0.06678851  0.01286147  0.05418161  0.11035448], action=1, reward=1.0, next_state=[-0.06653129  0.20716684  0.0563887  -0.16475428]\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 10 ] state=[-0.06653129  0.20716684  0.0563887  -0.16475428], action=0, reward=1.0, next_state=[-0.06238795  0.01128492  0.05309361  0.14517108]\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 11 ] state=[-0.06238795  0.01128492  0.05309361  0.14517108], action=1, reward=1.0, next_state=[-0.06216225  0.20560791  0.05599703 -0.13030047]\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 12 ] state=[-0.06216225  0.20560791  0.05599703 -0.13030047], action=1, reward=1.0, next_state=[-0.05805009  0.39988486  0.05339102 -0.40480459]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 33 ][ timestamp 13 ] state=[-0.05805009  0.39988486  0.05339102 -0.40480459], action=0, reward=1.0, next_state=[-0.05005239  0.204048    0.04529493 -0.09577839]\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 14 ] state=[-0.05005239  0.204048    0.04529493 -0.09577839], action=0, reward=1.0, next_state=[-0.04597143  0.00830711  0.04337936  0.21084396]\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 15 ] state=[-0.04597143  0.00830711  0.04337936  0.21084396], action=0, reward=1.0, next_state=[-0.04580529 -0.18740737  0.04759624  0.51688905]\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 16 ] state=[-0.04580529 -0.18740737  0.04759624  0.51688905], action=0, reward=1.0, next_state=[-0.04955344 -0.38316607  0.05793402  0.82418256]\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 17 ] state=[-0.04955344 -0.38316607  0.05793402  0.82418256], action=1, reward=1.0, next_state=[-0.05721676 -0.18888241  0.07441768  0.55026907]\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 18 ] state=[-0.05721676 -0.18888241  0.07441768  0.55026907], action=0, reward=1.0, next_state=[-0.06099441 -0.38496641  0.08542306  0.86543962]\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 19 ] state=[-0.06099441 -0.38496641  0.08542306  0.86543962], action=0, reward=1.0, next_state=[-0.06869374 -0.58114066  0.10273185  1.18371188]\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 20 ] state=[-0.06869374 -0.58114066  0.10273185  1.18371188], action=1, reward=1.0, next_state=[-0.08031655 -0.38749052  0.12640609  0.9249182 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 21 ] state=[-0.08031655 -0.38749052  0.12640609  0.9249182 ], action=0, reward=1.0, next_state=[-0.08806636 -0.58407224  0.14490445  1.2545026 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 22 ] state=[-0.08806636 -0.58407224  0.14490445  1.2545026 ], action=0, reward=1.0, next_state=[-0.09974781 -0.78072164  0.1699945   1.58883826]\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 23 ] state=[-0.09974781 -0.78072164  0.1699945   1.58883826], action=0, reward=1.0, next_state=[-0.11536224 -0.97740631  0.20177127  1.92935067]\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 24 ] state=[-0.11536224 -0.97740631  0.20177127  1.92935067], action=0, reward=-1.0, next_state=[-0.13491036 -1.17403842  0.24035828  2.27723433]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 33: Exploration_rate=0.0164031105360144. Score=24.\n",
      "[ episode 34 ] state=[ 0.00347535  0.01928308 -0.01471377  0.028973  ]\n",
      "[ episode 34 ][ timestamp 1 ] state=[ 0.00347535  0.01928308 -0.01471377  0.028973  ], action=0, reward=1.0, next_state=[ 0.00386101 -0.17562481 -0.01413431  0.31697749]\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 2 ] state=[ 0.00386101 -0.17562481 -0.01413431  0.31697749], action=1, reward=1.0, next_state=[ 0.00034851  0.01969558 -0.00779476  0.01987083]\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 3 ] state=[ 0.00034851  0.01969558 -0.00779476  0.01987083], action=0, reward=1.0, next_state=[ 0.00074242 -0.17531372 -0.00739734  0.31008426]\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 4 ] state=[ 0.00074242 -0.17531372 -0.00739734  0.31008426], action=0, reward=1.0, next_state=[-0.00276385 -0.3703295  -0.00119566  0.60042513]\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 5 ] state=[-0.00276385 -0.3703295  -0.00119566  0.60042513], action=0, reward=1.0, next_state=[-0.01017044 -0.56543471  0.01081285  0.8927312 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 6 ] state=[-0.01017044 -0.56543471  0.01081285  0.8927312 ], action=0, reward=1.0, next_state=[-0.02147914 -0.76070164  0.02866747  1.18879339]\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 7 ] state=[-0.02147914 -0.76070164  0.02866747  1.18879339], action=0, reward=1.0, next_state=[-0.03669317 -0.95618323  0.05244334  1.49032238]\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 8 ] state=[-0.03669317 -0.95618323  0.05244334  1.49032238], action=0, reward=1.0, next_state=[-0.05581683 -1.15190298  0.08224979  1.79890965]\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 9 ] state=[-0.05581683 -1.15190298  0.08224979  1.79890965], action=1, reward=1.0, next_state=[-0.07885489 -0.95779178  0.11822798  1.53288136]\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 10 ] state=[-0.07885489 -0.95779178  0.11822798  1.53288136], action=1, reward=1.0, next_state=[-0.09801073 -0.76427624  0.14888561  1.2793125 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 11 ] state=[-0.09801073 -0.76427624  0.14888561  1.2793125 ], action=1, reward=1.0, next_state=[-0.11329625 -0.57133191  0.17447186  1.03670864]\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 12 ] state=[-0.11329625 -0.57133191  0.17447186  1.03670864], action=0, reward=1.0, next_state=[-0.12472289 -0.7682893   0.19520603  1.3786944 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 13 ] state=[-0.12472289 -0.7682893   0.19520603  1.3786944 ], action=1, reward=-1.0, next_state=[-0.14008868 -0.57606579  0.22277992  1.15286206]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 34: Exploration_rate=0.015368315270123408. Score=13.\n",
      "[ episode 35 ] state=[0.04333278 0.04405373 0.03978121 0.00656529]\n",
      "[ episode 35 ][ timestamp 1 ] state=[0.04333278 0.04405373 0.03978121 0.00656529], action=1, reward=1.0, next_state=[ 0.04421385  0.23858325  0.03991252 -0.27330554]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 2 ] state=[ 0.04421385  0.23858325  0.03991252 -0.27330554], action=1, reward=1.0, next_state=[ 0.04898551  0.43311365  0.03444641 -0.55313759]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 3 ] state=[ 0.04898551  0.43311365  0.03444641 -0.55313759], action=0, reward=1.0, next_state=[ 0.05764779  0.23752533  0.02338366 -0.24980391]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 4 ] state=[ 0.05764779  0.23752533  0.02338366 -0.24980391], action=0, reward=1.0, next_state=[0.06239829 0.04207739 0.01838758 0.05016203]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 5 ] state=[0.06239829 0.04207739 0.01838758 0.05016203], action=1, reward=1.0, next_state=[ 0.06323984  0.23693092  0.01939082 -0.23666322]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 6 ] state=[ 0.06323984  0.23693092  0.01939082 -0.23666322], action=0, reward=1.0, next_state=[0.06797846 0.04153738 0.01465756 0.06207254]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 7 ] state=[0.06797846 0.04153738 0.01465756 0.06207254], action=1, reward=1.0, next_state=[ 0.06880921  0.23644614  0.01589901 -0.22595003]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 8 ] state=[ 0.06880921  0.23644614  0.01589901 -0.22595003], action=1, reward=1.0, next_state=[ 0.07353813  0.4313373   0.01138001 -0.51357572]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 9 ] state=[ 0.07353813  0.4313373   0.01138001 -0.51357572], action=0, reward=1.0, next_state=[ 0.08216488  0.23605694  0.00110849 -0.21732849]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 10 ] state=[ 0.08216488  0.23605694  0.00110849 -0.21732849], action=1, reward=1.0, next_state=[ 0.08688602  0.43116303 -0.00323808 -0.50966155]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 11 ] state=[ 0.08688602  0.43116303 -0.00323808 -0.50966155], action=0, reward=1.0, next_state=[ 0.09550928  0.23608684 -0.01343131 -0.2180008 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 12 ] state=[ 0.09550928  0.23608684 -0.01343131 -0.2180008 ], action=1, reward=1.0, next_state=[ 0.10023101  0.43139819 -0.01779133 -0.51489009]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 13 ] state=[ 0.10023101  0.43139819 -0.01779133 -0.51489009], action=0, reward=1.0, next_state=[ 0.10885898  0.23653125 -0.02808913 -0.22786631]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 14 ] state=[ 0.10885898  0.23653125 -0.02808913 -0.22786631], action=0, reward=1.0, next_state=[ 0.1135896   0.04182175 -0.03264645  0.05582562]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 15 ] state=[ 0.1135896   0.04182175 -0.03264645  0.05582562], action=1, reward=1.0, next_state=[ 0.11442604  0.23739622 -0.03152994 -0.24697612]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 16 ] state=[ 0.11442604  0.23739622 -0.03152994 -0.24697612], action=0, reward=1.0, next_state=[ 0.11917396  0.04273843 -0.03646946  0.03559717]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 17 ] state=[ 0.11917396  0.04273843 -0.03646946  0.03559717], action=1, reward=1.0, next_state=[ 0.12002873  0.23836387 -0.03575752 -0.26836552]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 18 ] state=[ 0.12002873  0.23836387 -0.03575752 -0.26836552], action=1, reward=1.0, next_state=[ 0.12479601  0.43397739 -0.04112483 -0.57210871]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 19 ] state=[ 0.12479601  0.43397739 -0.04112483 -0.57210871], action=0, reward=1.0, next_state=[ 0.13347556  0.2394555  -0.052567   -0.29265986]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 20 ] state=[ 0.13347556  0.2394555  -0.052567   -0.29265986], action=0, reward=1.0, next_state=[ 0.13826467  0.04512091 -0.0584202  -0.01700823]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 35 ][ timestamp 21 ] state=[ 0.13826467  0.04512091 -0.0584202  -0.01700823], action=1, reward=1.0, next_state=[ 0.13916708  0.24102991 -0.05876037 -0.32753616]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 22 ] state=[ 0.13916708  0.24102991 -0.05876037 -0.32753616], action=0, reward=1.0, next_state=[ 0.14398768  0.0467916  -0.06531109 -0.05394716]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 23 ] state=[ 0.14398768  0.0467916  -0.06531109 -0.05394716], action=1, reward=1.0, next_state=[ 0.14492351  0.24278627 -0.06639003 -0.36650019]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 24 ] state=[ 0.14492351  0.24278627 -0.06639003 -0.36650019], action=0, reward=1.0, next_state=[ 0.14977924  0.04866747 -0.07372004 -0.0954678 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 25 ] state=[ 0.14977924  0.04866747 -0.07372004 -0.0954678 ], action=1, reward=1.0, next_state=[ 0.15075259  0.24476436 -0.07562939 -0.41046826]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 26 ] state=[ 0.15075259  0.24476436 -0.07562939 -0.41046826], action=0, reward=1.0, next_state=[ 0.15564788  0.05079152 -0.08383876 -0.14255458]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 27 ] state=[ 0.15564788  0.05079152 -0.08383876 -0.14255458], action=1, reward=1.0, next_state=[ 0.15666371  0.24700788 -0.08668985 -0.46046505]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 28 ] state=[ 0.15666371  0.24700788 -0.08668985 -0.46046505], action=1, reward=1.0, next_state=[ 0.16160386  0.44324132 -0.09589915 -0.77916477]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 29 ] state=[ 0.16160386  0.44324132 -0.09589915 -0.77916477], action=0, reward=1.0, next_state=[ 0.17046869  0.24955951 -0.11148245 -0.5181281 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 30 ] state=[ 0.17046869  0.24955951 -0.11148245 -0.5181281 ], action=0, reward=1.0, next_state=[ 0.17545988  0.05616917 -0.12184501 -0.26255135]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 31 ] state=[ 0.17545988  0.05616917 -0.12184501 -0.26255135], action=1, reward=1.0, next_state=[ 0.17658326  0.25280059 -0.12709603 -0.59104563]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 32 ] state=[ 0.17658326  0.25280059 -0.12709603 -0.59104563], action=0, reward=1.0, next_state=[ 0.18163928  0.05966566 -0.13891695 -0.34094565]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 33 ] state=[ 0.18163928  0.05966566 -0.13891695 -0.34094565], action=1, reward=1.0, next_state=[ 0.18283259  0.25646255 -0.14573586 -0.67400758]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 34 ] state=[ 0.18283259  0.25646255 -0.14573586 -0.67400758], action=1, reward=1.0, next_state=[ 0.18796184  0.45327703 -0.15921601 -1.00879459]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 35 ] state=[ 0.18796184  0.45327703 -0.15921601 -1.00879459], action=0, reward=1.0, next_state=[ 0.19702738  0.26059705 -0.1793919  -0.77004216]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 36 ] state=[ 0.19702738  0.26059705 -0.1793919  -0.77004216], action=0, reward=1.0, next_state=[ 0.20223932  0.06833766 -0.19479275 -0.53873983]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 37 ] state=[ 0.20223932  0.06833766 -0.19479275 -0.53873983], action=1, reward=1.0, next_state=[ 0.20360607  0.26558753 -0.20556754 -0.88592657]\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 38 ] state=[ 0.20360607  0.26558753 -0.20556754 -0.88592657], action=1, reward=-1.0, next_state=[ 0.20891782  0.46281766 -0.22328608 -1.23555494]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 35: Exploration_rate=0.012702913170639124. Score=38.\n",
      "[ episode 36 ] state=[-0.02017533  0.01596358  0.02347703 -0.02193699]\n",
      "[ episode 36 ][ timestamp 1 ] state=[-0.02017533  0.01596358  0.02347703 -0.02193699], action=1, reward=1.0, next_state=[-0.01985606  0.21074112  0.02303829 -0.30712117]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 2 ] state=[-0.01985606  0.21074112  0.02303829 -0.30712117], action=1, reward=1.0, next_state=[-0.01564124  0.40552734  0.01689587 -0.59245031]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 3 ] state=[-0.01564124  0.40552734  0.01689587 -0.59245031], action=1, reward=1.0, next_state=[-0.00753069  0.60040874  0.00504686 -0.87976354]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 4 ] state=[-0.00753069  0.60040874  0.00504686 -0.87976354], action=0, reward=1.0, next_state=[ 0.00447748  0.40521858 -0.01254841 -0.58549827]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 5 ] state=[ 0.00447748  0.40521858 -0.01254841 -0.58549827], action=0, reward=1.0, next_state=[ 0.01258185  0.21027463 -0.02425837 -0.29679448]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 6 ] state=[ 0.01258185  0.21027463 -0.02425837 -0.29679448], action=1, reward=1.0, next_state=[ 0.01678735  0.40573384 -0.03019426 -0.59702831]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 7 ] state=[ 0.01678735  0.40573384 -0.03019426 -0.59702831], action=0, reward=1.0, next_state=[ 0.02490202  0.21104714 -0.04213483 -0.31400714]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 8 ] state=[ 0.02490202  0.21104714 -0.04213483 -0.31400714], action=0, reward=1.0, next_state=[ 0.02912297  0.01654995 -0.04841497 -0.03490426]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 9 ] state=[ 0.02912297  0.01654995 -0.04841497 -0.03490426], action=0, reward=1.0, next_state=[ 0.02945397 -0.17784549 -0.04911306  0.24211877]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 10 ] state=[ 0.02945397 -0.17784549 -0.04911306  0.24211877], action=1, reward=1.0, next_state=[ 0.02589706  0.01794235 -0.04427068 -0.06564229]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 11 ] state=[ 0.02589706  0.01794235 -0.04427068 -0.06564229], action=0, reward=1.0, next_state=[ 0.0262559  -0.17651784 -0.04558353  0.21275087]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 12 ] state=[ 0.0262559  -0.17651784 -0.04558353  0.21275087], action=0, reward=1.0, next_state=[ 0.02272555 -0.37095943 -0.04132851  0.49071338]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 13 ] state=[ 0.02272555 -0.37095943 -0.04132851  0.49071338], action=1, reward=1.0, next_state=[ 0.01530636 -0.17527959 -0.03151424  0.18529713]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 14 ] state=[ 0.01530636 -0.17527959 -0.03151424  0.18529713], action=1, reward=1.0, next_state=[ 0.01180077  0.02027876 -0.0278083  -0.1171584 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 15 ] state=[ 0.01180077  0.02027876 -0.0278083  -0.1171584 ], action=1, reward=1.0, next_state=[ 0.01220634  0.21578788 -0.03015147 -0.41848329]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 16 ] state=[ 0.01220634  0.21578788 -0.03015147 -0.41848329], action=0, reward=1.0, next_state=[ 0.0165221   0.02110588 -0.03852113 -0.1354562 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 17 ] state=[ 0.0165221   0.02110588 -0.03852113 -0.1354562 ], action=0, reward=1.0, next_state=[ 0.01694422 -0.17344374 -0.04123026  0.14482933]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 18 ] state=[ 0.01694422 -0.17344374 -0.04123026  0.14482933], action=1, reward=1.0, next_state=[ 0.01347534  0.02224367 -0.03833367 -0.16057059]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 19 ] state=[ 0.01347534  0.02224367 -0.03833367 -0.16057059], action=1, reward=1.0, next_state=[ 0.01392021  0.21789285 -0.04154508 -0.46509611]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 20 ] state=[ 0.01392021  0.21789285 -0.04154508 -0.46509611], action=1, reward=1.0, next_state=[ 0.01827807  0.41357647 -0.05084701 -0.770579  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 21 ] state=[ 0.01827807  0.41357647 -0.05084701 -0.770579  ], action=1, reward=1.0, next_state=[ 0.0265496   0.60935994 -0.06625859 -1.07881723]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 22 ] state=[ 0.0265496   0.60935994 -0.06625859 -1.07881723], action=0, reward=1.0, next_state=[ 0.0387368   0.41517256 -0.08783493 -0.8076411 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 23 ] state=[ 0.0387368   0.41517256 -0.08783493 -0.8076411 ], action=0, reward=1.0, next_state=[ 0.04704025  0.22135713 -0.10398775 -0.54382896]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 24 ] state=[ 0.04704025  0.22135713 -0.10398775 -0.54382896], action=0, reward=1.0, next_state=[ 0.05146739  0.0278384  -0.11486433 -0.28563624]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 25 ] state=[ 0.05146739  0.0278384  -0.11486433 -0.28563624], action=0, reward=1.0, next_state=[ 0.05202416 -0.165474   -0.12057706 -0.03127434]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 36 ][ timestamp 26 ] state=[ 0.05202416 -0.165474   -0.12057706 -0.03127434], action=0, reward=1.0, next_state=[ 0.04871468 -0.35867898 -0.12120254  0.22106514]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 27 ] state=[ 0.04871468 -0.35867898 -0.12120254  0.22106514], action=0, reward=1.0, next_state=[ 0.0415411  -0.5518788  -0.11678124  0.47319251]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 28 ] state=[ 0.0415411  -0.5518788  -0.11678124  0.47319251], action=0, reward=1.0, next_state=[ 0.03050353 -0.74517456 -0.10731739  0.7269056 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 29 ] state=[ 0.03050353 -0.74517456 -0.10731739  0.7269056 ], action=0, reward=1.0, next_state=[ 0.01560003 -0.938662   -0.09277928  0.98397628]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 30 ] state=[ 0.01560003 -0.938662   -0.09277928  0.98397628], action=1, reward=1.0, next_state=[-0.00317321 -0.74242784 -0.07309975  0.66365302]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 31 ] state=[-0.00317321 -0.74242784 -0.07309975  0.66365302], action=0, reward=1.0, next_state=[-0.01802176 -0.93646081 -0.05982669  0.93245302]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 32 ] state=[-0.01802176 -0.93646081 -0.05982669  0.93245302], action=1, reward=1.0, next_state=[-0.03675098 -0.74058486 -0.04117763  0.62158619]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 33 ] state=[-0.03675098 -0.74058486 -0.04117763  0.62158619], action=1, reward=1.0, next_state=[-0.05156268 -0.54491281 -0.02874591  0.31622411]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 34 ] state=[-0.05156268 -0.54491281 -0.02874591  0.31622411], action=1, reward=1.0, next_state=[-0.06246093 -0.34939345 -0.02242143  0.01461611]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 35 ] state=[-0.06246093 -0.34939345 -0.02242143  0.01461611], action=1, reward=1.0, next_state=[-0.0694488  -0.15395725 -0.0221291  -0.28505587]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 36 ] state=[-0.0694488  -0.15395725 -0.0221291  -0.28505587], action=0, reward=1.0, next_state=[-0.07252795 -0.34875671 -0.02783022  0.00056636]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 37 ] state=[-0.07252795 -0.34875671 -0.02783022  0.00056636], action=1, reward=1.0, next_state=[-0.07950308 -0.15324692 -0.02781889 -0.30076579]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 38 ] state=[-0.07950308 -0.15324692 -0.02781889 -0.30076579], action=1, reward=1.0, next_state=[-0.08256802  0.04226027 -0.03383421 -0.6020908 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 39 ] state=[-0.08256802  0.04226027 -0.03383421 -0.6020908 ], action=1, reward=1.0, next_state=[-0.08172281  0.23783874 -0.04587603 -0.90523597]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 40 ] state=[-0.08172281  0.23783874 -0.04587603 -0.90523597], action=0, reward=1.0, next_state=[-0.07696604  0.04336706 -0.06398074 -0.62731818]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 41 ] state=[-0.07696604  0.04336706 -0.06398074 -0.62731818], action=0, reward=1.0, next_state=[-0.0760987  -0.15080628 -0.07652711 -0.35545162]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 42 ] state=[-0.0760987  -0.15080628 -0.07652711 -0.35545162], action=0, reward=1.0, next_state=[-0.07911482 -0.34476147 -0.08363614 -0.08784733]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 43 ] state=[-0.07911482 -0.34476147 -0.08363614 -0.08784733], action=0, reward=1.0, next_state=[-0.08601005 -0.5385912  -0.08539309  0.1773206 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 44 ] state=[-0.08601005 -0.5385912  -0.08539309  0.1773206 ], action=1, reward=1.0, next_state=[-0.09678188 -0.3423576  -0.08184668 -0.14103233]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 45 ] state=[-0.09678188 -0.3423576  -0.08184668 -0.14103233], action=0, reward=1.0, next_state=[-0.10362903 -0.53621776 -0.08466732  0.1247484 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 46 ] state=[-0.10362903 -0.53621776 -0.08466732  0.1247484 ], action=1, reward=1.0, next_state=[-0.11435338 -0.33999131 -0.08217235 -0.19339936]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 47 ] state=[-0.11435338 -0.33999131 -0.08217235 -0.19339936], action=1, reward=1.0, next_state=[-0.12115321 -0.1437959  -0.08604034 -0.51083096]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 48 ] state=[-0.12115321 -0.1437959  -0.08604034 -0.51083096], action=0, reward=1.0, next_state=[-0.12402913 -0.33760714 -0.09625696 -0.24645417]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 49 ] state=[-0.12402913 -0.33760714 -0.09625696 -0.24645417], action=1, reward=1.0, next_state=[-0.13078127 -0.14125164 -0.10118604 -0.56787986]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 50 ] state=[-0.13078127 -0.14125164 -0.10118604 -0.56787986], action=1, reward=1.0, next_state=[-0.1336063   0.05513318 -0.11254364 -0.8906483 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 51 ] state=[-0.1336063   0.05513318 -0.11254364 -0.8906483 ], action=1, reward=1.0, next_state=[-0.13250364  0.25158736 -0.13035661 -1.21648333]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 52 ] state=[-0.13250364  0.25158736 -0.13035661 -1.21648333], action=1, reward=1.0, next_state=[-0.12747189  0.44812705 -0.15468627 -1.54700798]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 53 ] state=[-0.12747189  0.44812705 -0.15468627 -1.54700798], action=1, reward=1.0, next_state=[-0.11850935  0.64473099 -0.18562643 -1.88368931]\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 54 ] state=[-0.11850935  0.64473099 -0.18562643 -1.88368931], action=0, reward=-1.0, next_state=[-0.10561473  0.4520518  -0.22330022 -1.65389692]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 36: Exploration_rate=0.01. Score=54.\n",
      "[ episode 37 ] state=[ 0.00499993  0.03408254  0.00703154 -0.03385436]\n",
      "[ episode 37 ][ timestamp 1 ] state=[ 0.00499993  0.03408254  0.00703154 -0.03385436], action=1, reward=1.0, next_state=[ 0.00568159  0.22910296  0.00635445 -0.32431049]\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 2 ] state=[ 0.00568159  0.22910296  0.00635445 -0.32431049], action=0, reward=1.0, next_state=[ 0.01026364  0.0338911  -0.00013176 -0.02963043]\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 3 ] state=[ 0.01026364  0.0338911  -0.00013176 -0.02963043], action=0, reward=1.0, next_state=[ 0.01094147 -0.16122896 -0.00072437  0.26301092]\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 4 ] state=[ 0.01094147 -0.16122896 -0.00072437  0.26301092], action=0, reward=1.0, next_state=[ 0.00771689 -0.35634056  0.00453585  0.55546529]\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 5 ] state=[ 0.00771689 -0.35634056  0.00453585  0.55546529], action=1, reward=1.0, next_state=[ 0.00059008 -0.16128259  0.01564516  0.26421488]\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 6 ] state=[ 0.00059008 -0.16128259  0.01564516  0.26421488], action=1, reward=1.0, next_state=[-0.00263558  0.0336126   0.02092946 -0.02349264]\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 7 ] state=[-0.00263558  0.0336126   0.02092946 -0.02349264], action=0, reward=1.0, next_state=[-0.00196332 -0.16180315  0.0204596   0.2757196 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 8 ] state=[-0.00196332 -0.16180315  0.0204596   0.2757196 ], action=0, reward=1.0, next_state=[-0.00519939 -0.35721094  0.025974    0.57478464]\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 9 ] state=[-0.00519939 -0.35721094  0.025974    0.57478464], action=0, reward=1.0, next_state=[-0.01234361 -0.55268721  0.03746969  0.87553564]\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 10 ] state=[-0.01234361 -0.55268721  0.03746969  0.87553564], action=1, reward=1.0, next_state=[-0.02339735 -0.35809409  0.0549804   0.59486435]\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 11 ] state=[-0.02339735 -0.35809409  0.0549804   0.59486435], action=0, reward=1.0, next_state=[-0.03055923 -0.5539407   0.06687769  0.90434647]\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 12 ] state=[-0.03055923 -0.5539407   0.06687769  0.90434647], action=1, reward=1.0, next_state=[-0.04163805 -0.35978514  0.08496462  0.63341156]\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 13 ] state=[-0.04163805 -0.35978514  0.08496462  0.63341156], action=1, reward=1.0, next_state=[-0.04883375 -0.16594478  0.09763285  0.36864944]\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 14 ] state=[-0.04883375 -0.16594478  0.09763285  0.36864944], action=1, reward=1.0, next_state=[-0.05215264  0.02766417  0.10500584  0.10827753]\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 15 ] state=[-0.05215264  0.02766417  0.10500584  0.10827753], action=1, reward=1.0, next_state=[-0.05159936  0.22113691  0.10717139 -0.14951808]\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 16 ] state=[-0.05159936  0.22113691  0.10717139 -0.14951808], action=0, reward=1.0, next_state=[-0.04717662  0.02465653  0.10418103  0.17495968]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 37 ][ timestamp 17 ] state=[-0.04717662  0.02465653  0.10418103  0.17495968], action=0, reward=1.0, next_state=[-0.04668349 -0.17179019  0.10768022  0.49860591]\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 18 ] state=[-0.04668349 -0.17179019  0.10768022  0.49860591], action=0, reward=1.0, next_state=[-0.0501193  -0.36825242  0.11765234  0.82318926]\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 19 ] state=[-0.0501193  -0.36825242  0.11765234  0.82318926], action=1, reward=1.0, next_state=[-0.05748434 -0.17491955  0.13411612  0.56970479]\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 20 ] state=[-0.05748434 -0.17491955  0.13411612  0.56970479], action=1, reward=1.0, next_state=[-0.06098273  0.01809179  0.14551022  0.32209971]\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 21 ] state=[-0.06098273  0.01809179  0.14551022  0.32209971], action=0, reward=1.0, next_state=[-0.0606209  -0.17877014  0.15195221  0.65690116]\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 22 ] state=[-0.0606209  -0.17877014  0.15195221  0.65690116], action=1, reward=1.0, next_state=[-0.0641963   0.01394662  0.16509024  0.41565914]\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 23 ] state=[-0.0641963   0.01394662  0.16509024  0.41565914], action=0, reward=1.0, next_state=[-0.06391737 -0.18308311  0.17340342  0.75550172]\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 24 ] state=[-0.06391737 -0.18308311  0.17340342  0.75550172], action=0, reward=1.0, next_state=[-0.06757903 -0.38011711  0.18851345  1.09734593]\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 25 ] state=[-0.06757903 -0.38011711  0.18851345  1.09734593], action=1, reward=-1.0, next_state=[-0.07518137 -0.18790887  0.21046037  0.86923661]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 37: Exploration_rate=0.01. Score=25.\n",
      "[ episode 38 ] state=[-0.00721901 -0.003781   -0.01409408  0.04046496]\n",
      "[ episode 38 ][ timestamp 1 ] state=[-0.00721901 -0.003781   -0.01409408  0.04046496], action=0, reward=1.0, next_state=[-0.00729463 -0.19869804 -0.01328478  0.32866796]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 2 ] state=[-0.00729463 -0.19869804 -0.01328478  0.32866796], action=1, reward=1.0, next_state=[-0.01126859 -0.00338951 -0.00671142  0.03182541]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 3 ] state=[-0.01126859 -0.00338951 -0.00671142  0.03182541], action=0, reward=1.0, next_state=[-0.01133638 -0.19841457 -0.00607491  0.32238328]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 4 ] state=[-0.01133638 -0.19841457 -0.00607491  0.32238328], action=1, reward=1.0, next_state=[-0.01530467 -0.00320664  0.00037276  0.02779077]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 5 ] state=[-0.01530467 -0.00320664  0.00037276  0.02779077], action=1, reward=1.0, next_state=[-0.0153688   0.19190996  0.00092857 -0.26477452]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 6 ] state=[-0.0153688   0.19190996  0.00092857 -0.26477452], action=0, reward=1.0, next_state=[-0.0115306  -0.00322523 -0.00436692  0.02820114]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 7 ] state=[-0.0115306  -0.00322523 -0.00436692  0.02820114], action=1, reward=1.0, next_state=[-0.01159511  0.19195907 -0.0038029  -0.26585639]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 8 ] state=[-0.01159511  0.19195907 -0.0038029  -0.26585639], action=1, reward=1.0, next_state=[-0.00775593  0.38713509 -0.00912002 -0.55973636]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 9 ] state=[-0.00775593  0.38713509 -0.00912002 -0.55973636], action=0, reward=1.0, next_state=[-1.32251896e-05  1.92142327e-01 -2.03147513e-02 -2.69940622e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 10 ] state=[-1.32251896e-05  1.92142327e-01 -2.03147513e-02 -2.69940622e-01], action=0, reward=1.0, next_state=[ 0.00382962 -0.00268392 -0.02571356  0.01626633]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 11 ] state=[ 0.00382962 -0.00268392 -0.02571356  0.01626633], action=1, reward=1.0, next_state=[ 0.00377594  0.19279717 -0.02538824 -0.28441734]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 12 ] state=[ 0.00377594  0.19279717 -0.02538824 -0.28441734], action=0, reward=1.0, next_state=[ 0.00763189 -0.00195365 -0.03107658  0.00015141]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 13 ] state=[ 0.00763189 -0.00195365 -0.03107658  0.00015141], action=1, reward=1.0, next_state=[ 0.00759281  0.19359988 -0.03107356 -0.3021723 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 14 ] state=[ 0.00759281  0.19359988 -0.03107356 -0.3021723 ], action=0, reward=1.0, next_state=[ 0.01146481 -0.00106573 -0.037117   -0.01944899]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 15 ] state=[ 0.01146481 -0.00106573 -0.037117   -0.01944899], action=1, reward=1.0, next_state=[ 0.0114435   0.19456832 -0.03750598 -0.32360784]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 16 ] state=[ 0.0114435   0.19456832 -0.03750598 -0.32360784], action=1, reward=1.0, next_state=[ 0.01533486  0.3902037  -0.04397814 -0.62787871]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 17 ] state=[ 0.01533486  0.3902037  -0.04397814 -0.62787871], action=0, reward=1.0, next_state=[ 0.02313894  0.19572224 -0.05653571 -0.34936399]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 18 ] state=[ 0.02313894  0.19572224 -0.05653571 -0.34936399], action=1, reward=1.0, next_state=[ 0.02705338  0.39160078 -0.06352299 -0.65932501]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 19 ] state=[ 0.02705338  0.39160078 -0.06352299 -0.65932501], action=0, reward=1.0, next_state=[ 0.0348854   0.19741772 -0.07670949 -0.3873011 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 20 ] state=[ 0.0348854   0.19741772 -0.07670949 -0.3873011 ], action=0, reward=1.0, next_state=[ 0.03883375  0.00346372 -0.08445551 -0.11975613]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 21 ] state=[ 0.03883375  0.00346372 -0.08445551 -0.11975613], action=0, reward=1.0, next_state=[ 0.03890303 -0.19035303 -0.08685064  0.14513237]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 22 ] state=[ 0.03890303 -0.19035303 -0.08685064  0.14513237], action=1, reward=1.0, next_state=[ 0.03509597  0.0058984  -0.08394799 -0.17363723]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 23 ] state=[ 0.03509597  0.0058984  -0.08394799 -0.17363723], action=0, reward=1.0, next_state=[ 0.03521393 -0.18792802 -0.08742073  0.09142681]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 24 ] state=[ 0.03521393 -0.18792802 -0.08742073  0.09142681], action=0, reward=1.0, next_state=[ 0.03145537 -0.38169524 -0.0855922   0.35529874]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 25 ] state=[ 0.03145537 -0.38169524 -0.0855922   0.35529874], action=0, reward=1.0, next_state=[ 0.02382147 -0.57550255 -0.07848622  0.61981209]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 26 ] state=[ 0.02382147 -0.57550255 -0.07848622  0.61981209], action=0, reward=1.0, next_state=[ 0.01231142 -0.76944566 -0.06608998  0.88677992]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 27 ] state=[ 0.01231142 -0.76944566 -0.06608998  0.88677992], action=0, reward=1.0, next_state=[-0.0030775  -0.96361124 -0.04835438  1.15797614]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 28 ] state=[-0.0030775  -0.96361124 -0.04835438  1.15797614], action=1, reward=1.0, next_state=[-0.02234972 -0.76789359 -0.02519486  0.85053216]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 29 ] state=[-0.02234972 -0.76789359 -0.02519486  0.85053216], action=1, reward=1.0, next_state=[-0.03770759 -0.57243731 -0.00818422  0.55003429]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 30 ] state=[-0.03770759 -0.57243731 -0.00818422  0.55003429], action=1, reward=1.0, next_state=[-0.04915634 -0.37720137  0.00281647  0.25478405]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 31 ] state=[-0.04915634 -0.37720137  0.00281647  0.25478405], action=1, reward=1.0, next_state=[-0.05670037 -0.18211974  0.00791215 -0.03700919]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 32 ] state=[-0.05670037 -0.18211974  0.00791215 -0.03700919], action=0, reward=1.0, next_state=[-0.06034276 -0.37735426  0.00717197  0.25815956]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 33 ] state=[-0.06034276 -0.37735426  0.00717197  0.25815956], action=0, reward=1.0, next_state=[-0.06788985 -0.57257786  0.01233516  0.55309598]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 34 ] state=[-0.06788985 -0.57257786  0.01233516  0.55309598], action=0, reward=1.0, next_state=[-0.0793414  -0.76787084  0.02339708  0.84963961]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 35 ] state=[-0.0793414  -0.76787084  0.02339708  0.84963961], action=0, reward=1.0, next_state=[-0.09469882 -0.96330392  0.04038987  1.1495871 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 36 ] state=[-0.09469882 -0.96330392  0.04038987  1.1495871 ], action=0, reward=1.0, next_state=[-0.1139649  -1.15892909  0.06338161  1.45465693]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 37 ] state=[-0.1139649  -1.15892909  0.06338161  1.45465693], action=1, reward=1.0, next_state=[-0.13714348 -0.96463996  0.09247475  1.18243015]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 38 ][ timestamp 38 ] state=[-0.13714348 -0.96463996  0.09247475  1.18243015], action=1, reward=1.0, next_state=[-0.15643628 -0.77083165  0.11612335  0.92010867]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 39 ] state=[-0.15643628 -0.77083165  0.11612335  0.92010867], action=0, reward=1.0, next_state=[-0.17185291 -0.96731536  0.13452553  1.24691291]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 40 ] state=[-0.17185291 -0.96731536  0.13452553  1.24691291], action=0, reward=1.0, next_state=[-0.19119922 -1.16388125  0.15946378  1.57852913]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 41 ] state=[-0.19119922 -1.16388125  0.15946378  1.57852913], action=1, reward=1.0, next_state=[-0.21447684 -0.97097755  0.19103437  1.33952867]\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 42 ] state=[-0.21447684 -0.97097755  0.19103437  1.33952867], action=1, reward=-1.0, next_state=[-0.2338964  -0.77870396  0.21782494  1.11218806]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 38: Exploration_rate=0.01. Score=42.\n",
      "[ episode 39 ] state=[ 0.00920935 -0.02673088  0.00030564 -0.03450336]\n",
      "[ episode 39 ][ timestamp 1 ] state=[ 0.00920935 -0.02673088  0.00030564 -0.03450336], action=1, reward=1.0, next_state=[ 0.00867473  0.16838669 -0.00038443 -0.32708984]\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 2 ] state=[ 0.00867473  0.16838669 -0.00038443 -0.32708984], action=0, reward=1.0, next_state=[ 0.01204246 -0.02672979 -0.00692622 -0.03452816]\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 3 ] state=[ 0.01204246 -0.02672979 -0.00692622 -0.03452816], action=0, reward=1.0, next_state=[ 0.01150787 -0.22175174 -0.00761679  0.25596144]\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 4 ] state=[ 0.01150787 -0.22175174 -0.00761679  0.25596144], action=0, reward=1.0, next_state=[ 0.00707283 -0.41676411 -0.00249756  0.54623221]\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 5 ] state=[ 0.00707283 -0.41676411 -0.00249756  0.54623221], action=0, reward=1.0, next_state=[-0.00126245 -0.61185089  0.00842709  0.83812717]\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 6 ] state=[-0.00126245 -0.61185089  0.00842709  0.83812717], action=0, reward=1.0, next_state=[-0.01349947 -0.8070869   0.02518963  1.13344833]\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 7 ] state=[-0.01349947 -0.8070869   0.02518963  1.13344833], action=0, reward=1.0, next_state=[-0.0296412  -1.00252931  0.0478586   1.43392391]\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 8 ] state=[-0.0296412  -1.00252931  0.0478586   1.43392391], action=0, reward=1.0, next_state=[-0.04969179 -1.19820784  0.07653707  1.74117069]\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 9 ] state=[-0.04969179 -1.19820784  0.07653707  1.74117069], action=0, reward=1.0, next_state=[-0.07365595 -1.3941132   0.11136049  2.05664839]\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 10 ] state=[-0.07365595 -1.3941132   0.11136049  2.05664839], action=0, reward=1.0, next_state=[-0.10153821 -1.59018328  0.15249346  2.38160412]\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 11 ] state=[-0.10153821 -1.59018328  0.15249346  2.38160412], action=0, reward=1.0, next_state=[-0.13334188 -1.78628678  0.20012554  2.71700533]\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 12 ] state=[-0.13334188 -1.78628678  0.20012554  2.71700533], action=1, reward=-1.0, next_state=[-0.16906761 -1.59308584  0.25446564  2.49143284]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 39: Exploration_rate=0.01. Score=12.\n",
      "[ episode 40 ] state=[-0.03524177  0.04891932  0.03192115 -0.03746048]\n",
      "[ episode 40 ][ timestamp 1 ] state=[-0.03524177  0.04891932  0.03192115 -0.03746048], action=0, reward=1.0, next_state=[-0.03426338 -0.14664549  0.03117194  0.26512052]\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 2 ] state=[-0.03426338 -0.14664549  0.03117194  0.26512052], action=0, reward=1.0, next_state=[-0.03719629 -0.34219817  0.03647435  0.5674701 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 3 ] state=[-0.03719629 -0.34219817  0.03647435  0.5674701 ], action=0, reward=1.0, next_state=[-0.04404026 -0.53781226  0.04782375  0.87141717]\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 4 ] state=[-0.04404026 -0.53781226  0.04782375  0.87141717], action=0, reward=1.0, next_state=[-0.0547965  -0.73355088  0.0652521   1.17874423]\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 5 ] state=[-0.0547965  -0.73355088  0.0652521   1.17874423], action=0, reward=1.0, next_state=[-0.06946752 -0.92945665  0.08882698  1.49114801]\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 6 ] state=[-0.06946752 -0.92945665  0.08882698  1.49114801], action=0, reward=1.0, next_state=[-0.08805665 -1.12554047  0.11864994  1.81019495]\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 7 ] state=[-0.08805665 -1.12554047  0.11864994  1.81019495], action=1, reward=1.0, next_state=[-0.11056746 -0.9319243   0.15485384  1.55661385]\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 8 ] state=[-0.11056746 -0.9319243   0.15485384  1.55661385], action=1, reward=1.0, next_state=[-0.12920595 -0.73895912  0.18598612  1.31597489]\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 9 ] state=[-0.12920595 -0.73895912  0.18598612  1.31597489], action=0, reward=-1.0, next_state=[-0.14398513 -0.93588218  0.21230562  1.6606306 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 40: Exploration_rate=0.01. Score=9.\n",
      "[ episode 41 ] state=[-0.00803828  0.03108819 -0.04626699 -0.0476733 ]\n",
      "[ episode 41 ][ timestamp 1 ] state=[-0.00803828  0.03108819 -0.04626699 -0.0476733 ], action=0, reward=1.0, next_state=[-0.00741652 -0.16334085 -0.04722046  0.23006051]\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 2 ] state=[-0.00741652 -0.16334085 -0.04722046  0.23006051], action=0, reward=1.0, next_state=[-0.01068333 -0.35775733 -0.04261925  0.50748252]\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 3 ] state=[-0.01068333 -0.35775733 -0.04261925  0.50748252], action=0, reward=1.0, next_state=[-0.01783848 -0.55225367 -0.0324696   0.78643584]\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 4 ] state=[-0.01783848 -0.55225367 -0.0324696   0.78643584], action=1, reward=1.0, next_state=[-0.02888355 -0.35670106 -0.01674088  0.48371714]\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 5 ] state=[-0.02888355 -0.35670106 -0.01674088  0.48371714], action=0, reward=1.0, next_state=[-0.03601757 -0.55158279 -0.00706654  0.77107718]\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 6 ] state=[-0.03601757 -0.55158279 -0.00706654  0.77107718], action=0, reward=1.0, next_state=[-0.04704923 -0.74660679  0.008355    1.06152833]\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 7 ] state=[-0.04704923 -0.74660679  0.008355    1.06152833], action=0, reward=1.0, next_state=[-0.06198136 -0.94183837  0.02958557  1.35682183]\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 8 ] state=[-0.06198136 -0.94183837  0.02958557  1.35682183], action=0, reward=1.0, next_state=[-0.08081813 -1.13731873  0.05672201  1.65861094]\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 9 ] state=[-0.08081813 -1.13731873  0.05672201  1.65861094], action=0, reward=1.0, next_state=[-0.10356451 -1.33305423  0.08989422  1.96840932]\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 10 ] state=[-0.10356451 -1.33305423  0.08989422  1.96840932], action=1, reward=1.0, next_state=[-0.13022559 -1.13898964  0.12926241  1.70488115]\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 11 ] state=[-0.13022559 -1.13898964  0.12926241  1.70488115], action=1, reward=1.0, next_state=[-0.15300538 -0.94557055  0.16336003  1.45507039]\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 12 ] state=[-0.15300538 -0.94557055  0.16336003  1.45507039], action=1, reward=1.0, next_state=[-0.1719168  -0.75278713  0.19246144  1.21755974]\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 13 ] state=[-0.1719168  -0.75278713  0.19246144  1.21755974], action=1, reward=-1.0, next_state=[-0.18697254 -0.56059557  0.21681264  0.9908302 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 41: Exploration_rate=0.01. Score=13.\n",
      "[ episode 42 ] state=[ 0.01031851 -0.02167815 -0.00010867  0.0408918 ]\n",
      "[ episode 42 ][ timestamp 1 ] state=[ 0.01031851 -0.02167815 -0.00010867  0.0408918 ], action=0, reward=1.0, next_state=[ 0.00988495 -0.21679854  0.00070917  0.33354043]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 42 ][ timestamp 2 ] state=[ 0.00988495 -0.21679854  0.00070917  0.33354043], action=0, reward=1.0, next_state=[ 0.00554898 -0.41193058  0.00737998  0.62644691]\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 3 ] state=[ 0.00554898 -0.41193058  0.00737998  0.62644691], action=1, reward=1.0, next_state=[-0.00268963 -0.21691242  0.01990892  0.33609733]\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 4 ] state=[-0.00268963 -0.21691242  0.01990892  0.33609733], action=0, reward=1.0, next_state=[-0.00702788 -0.41231196  0.02663086  0.63499138]\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 5 ] state=[-0.00702788 -0.41231196  0.02663086  0.63499138], action=1, reward=1.0, next_state=[-0.01527412 -0.21757138  0.03933069  0.35081264]\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 6 ] state=[-0.01527412 -0.21757138  0.03933069  0.35081264], action=1, reward=1.0, next_state=[-0.01962555 -0.02303019  0.04634694  0.07078678]\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 7 ] state=[-0.01962555 -0.02303019  0.04634694  0.07078678], action=0, reward=1.0, next_state=[-0.02008615 -0.2187849   0.04776268  0.37772466]\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 8 ] state=[-0.02008615 -0.2187849   0.04776268  0.37772466], action=0, reward=1.0, next_state=[-0.02446185 -0.4145515   0.05531717  0.68507656]\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 9 ] state=[-0.02446185 -0.4145515   0.05531717  0.68507656], action=0, reward=1.0, next_state=[-0.03275288 -0.61039604  0.0690187   0.99464898]\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 10 ] state=[-0.03275288 -0.61039604  0.0690187   0.99464898], action=0, reward=1.0, next_state=[-0.0449608  -0.80636992  0.08891168  1.30818531]\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 11 ] state=[-0.0449608  -0.80636992  0.08891168  1.30818531], action=1, reward=1.0, next_state=[-0.0610882  -0.61247999  0.11507539  1.04460483]\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 12 ] state=[-0.0610882  -0.61247999  0.11507539  1.04460483], action=1, reward=1.0, next_state=[-0.0733378  -0.41905821  0.13596749  0.79014861]\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 13 ] state=[-0.0733378  -0.41905821  0.13596749  0.79014861], action=1, reward=1.0, next_state=[-0.08171896 -0.22603922  0.15177046  0.54314366]\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 14 ] state=[-0.08171896 -0.22603922  0.15177046  0.54314366], action=1, reward=1.0, next_state=[-0.08623975 -0.03333939  0.16263333  0.30186597]\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 15 ] state=[-0.08623975 -0.03333939  0.16263333  0.30186597], action=1, reward=1.0, next_state=[-0.08690653  0.15913623  0.16867065  0.06456601]\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 16 ] state=[-0.08690653  0.15913623  0.16867065  0.06456601], action=0, reward=1.0, next_state=[-0.08372381 -0.037952    0.16996197  0.40535735]\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 17 ] state=[-0.08372381 -0.037952    0.16996197  0.40535735], action=1, reward=1.0, next_state=[-0.08448285  0.15440356  0.17806912  0.17071003]\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 18 ] state=[-0.08448285  0.15440356  0.17806912  0.17071003], action=0, reward=1.0, next_state=[-0.08139478 -0.04276085  0.18148332  0.51385625]\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 19 ] state=[-0.08139478 -0.04276085  0.18148332  0.51385625], action=1, reward=1.0, next_state=[-0.08224999  0.14940358  0.19176044  0.28340714]\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 20 ] state=[-0.08224999  0.14940358  0.19176044  0.28340714], action=0, reward=1.0, next_state=[-0.07926192 -0.04786234  0.19742859  0.62991495]\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 21 ] state=[-0.07926192 -0.04786234  0.19742859  0.62991495], action=1, reward=-1.0, next_state=[-0.08021917  0.1440369   0.21002688  0.40532545]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 42: Exploration_rate=0.01. Score=21.\n",
      "[ episode 43 ] state=[ 0.03391424 -0.02913143  0.01825021  0.00096216]\n",
      "[ episode 43 ][ timestamp 1 ] state=[ 0.03391424 -0.02913143  0.01825021  0.00096216], action=1, reward=1.0, next_state=[ 0.03333161  0.1657241   0.01826945 -0.28590719]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 2 ] state=[ 0.03333161  0.1657241   0.01826945 -0.28590719], action=1, reward=1.0, next_state=[ 0.0366461   0.3605808   0.01255131 -0.57277254]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 3 ] state=[ 0.0366461   0.3605808   0.01255131 -0.57277254], action=0, reward=1.0, next_state=[ 0.04385771  0.16528513  0.00109586 -0.27616212]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 4 ] state=[ 0.04385771  0.16528513  0.00109586 -0.27616212], action=0, reward=1.0, next_state=[ 0.04716341 -0.02985244 -0.00442738  0.01686624]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 5 ] state=[ 0.04716341 -0.02985244 -0.00442738  0.01686624], action=1, reward=1.0, next_state=[ 0.04656637  0.16533272 -0.00409006 -0.27721028]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 6 ] state=[ 0.04656637  0.16533272 -0.00409006 -0.27721028], action=1, reward=1.0, next_state=[ 0.04987302  0.36051279 -0.00963426 -0.5711804 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 7 ] state=[ 0.04987302  0.36051279 -0.00963426 -0.5711804 ], action=0, reward=1.0, next_state=[ 0.05708328  0.16552725 -0.02105787 -0.28154811]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 8 ] state=[ 0.05708328  0.16552725 -0.02105787 -0.28154811], action=0, reward=1.0, next_state=[ 0.06039382 -0.02928809 -0.02668883  0.00441957]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 9 ] state=[ 0.06039382 -0.02928809 -0.02668883  0.00441957], action=1, reward=1.0, next_state=[ 0.05980806  0.16620625 -0.02660044 -0.29656309]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 10 ] state=[ 0.05980806  0.16620625 -0.02660044 -0.29656309], action=0, reward=1.0, next_state=[ 0.06313218 -0.0285266  -0.0325317  -0.01238677]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 11 ] state=[ 0.06313218 -0.0285266  -0.0325317  -0.01238677], action=0, reward=1.0, next_state=[ 0.06256165 -0.22316726 -0.03277944  0.26985712]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 12 ] state=[ 0.06256165 -0.22316726 -0.03277944  0.26985712], action=1, reward=1.0, next_state=[ 0.05809831 -0.02759324 -0.0273823  -0.03298175]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 13 ] state=[ 0.05809831 -0.02759324 -0.0273823  -0.03298175], action=1, reward=1.0, next_state=[ 0.05754644  0.16791046 -0.02804193 -0.33417677]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 14 ] state=[ 0.05754644  0.16791046 -0.02804193 -0.33417677], action=0, reward=1.0, next_state=[ 0.06090465 -0.02680139 -0.03472547 -0.05046706]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 15 ] state=[ 0.06090465 -0.02680139 -0.03472547 -0.05046706], action=1, reward=1.0, next_state=[ 0.06036862  0.16880083 -0.03573481 -0.35390075]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 16 ] state=[ 0.06036862  0.16880083 -0.03573481 -0.35390075], action=1, reward=1.0, next_state=[ 0.06374464  0.3644122  -0.04281282 -0.65763428]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 17 ] state=[ 0.06374464  0.3644122  -0.04281282 -0.65763428], action=0, reward=1.0, next_state=[ 0.07103288  0.16991151 -0.05596551 -0.37873371]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 18 ] state=[ 0.07103288  0.16991151 -0.05596551 -0.37873371], action=0, reward=1.0, next_state=[ 0.07443111 -0.02437283 -0.06354018 -0.10420873]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 19 ] state=[ 0.07443111 -0.02437283 -0.06354018 -0.10420873], action=1, reward=1.0, next_state=[ 0.07394366  0.17159944 -0.06562436 -0.41624218]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 20 ] state=[ 0.07394366  0.17159944 -0.06562436 -0.41624218], action=1, reward=1.0, next_state=[ 0.07737565  0.36758708 -0.0739492  -0.72887057]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 21 ] state=[ 0.07737565  0.36758708 -0.0739492  -0.72887057], action=0, reward=1.0, next_state=[ 0.08472739  0.17356101 -0.08852661 -0.46034812]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 22 ] state=[ 0.08472739  0.17356101 -0.08852661 -0.46034812], action=1, reward=1.0, next_state=[ 0.08819861  0.36981542 -0.09773357 -0.7795698 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 43 ][ timestamp 23 ] state=[ 0.08819861  0.36981542 -0.09773357 -0.7795698 ], action=0, reward=1.0, next_state=[ 0.09559492  0.17616326 -0.11332497 -0.51916571]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 24 ] state=[ 0.09559492  0.17616326 -0.11332497 -0.51916571], action=0, reward=1.0, next_state=[ 0.09911818 -0.01719614 -0.12370829 -0.26423332]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 25 ] state=[ 0.09911818 -0.01719614 -0.12370829 -0.26423332], action=1, reward=1.0, next_state=[ 0.09877426  0.17945444 -0.12899295 -0.59323248]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 26 ] state=[ 0.09877426  0.17945444 -0.12899295 -0.59323248], action=1, reward=1.0, next_state=[ 0.10236335  0.3761236  -0.1408576  -0.92360415]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 27 ] state=[ 0.10236335  0.3761236  -0.1408576  -0.92360415], action=0, reward=1.0, next_state=[ 0.10988582  0.18315659 -0.15932968 -0.6782957 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 28 ] state=[ 0.10988582  0.18315659 -0.15932968 -0.6782957 ], action=1, reward=1.0, next_state=[ 0.11354895  0.38009115 -0.1728956  -1.01660093]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 29 ] state=[ 0.11354895  0.38009115 -0.1728956  -1.01660093], action=0, reward=1.0, next_state=[ 0.12115077  0.18764296 -0.19322762 -0.78281096]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 30 ] state=[ 0.12115077  0.18764296 -0.19322762 -0.78281096], action=0, reward=1.0, next_state=[ 0.12490363 -0.00437301 -0.20888384 -0.55660333]\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 31 ] state=[ 0.12490363 -0.00437301 -0.20888384 -0.55660333], action=0, reward=-1.0, next_state=[ 0.12481617 -0.19604523 -0.2200159  -0.3363108 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 43: Exploration_rate=0.01. Score=31.\n",
      "[ episode 44 ] state=[ 0.03138339  0.00498829 -0.03730645  0.03172011]\n",
      "[ episode 44 ][ timestamp 1 ] state=[ 0.03138339  0.00498829 -0.03730645  0.03172011], action=0, reward=1.0, next_state=[ 0.03148316 -0.18957936 -0.03667205  0.31240297]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 2 ] state=[ 0.03148316 -0.18957936 -0.03667205  0.31240297], action=1, reward=1.0, next_state=[ 0.02769157  0.00604532 -0.03042399  0.00838407]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 3 ] state=[ 0.02769157  0.00604532 -0.03042399  0.00838407], action=1, reward=1.0, next_state=[ 0.02781248  0.20159009 -0.03025631 -0.29374061]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 4 ] state=[ 0.02781248  0.20159009 -0.03025631 -0.29374061], action=1, reward=1.0, next_state=[ 0.03184428  0.39713005 -0.03613112 -0.59581031]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 5 ] state=[ 0.03184428  0.39713005 -0.03613112 -0.59581031], action=0, reward=1.0, next_state=[ 0.03978688  0.20253189 -0.04804732 -0.31472382]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 6 ] state=[ 0.03978688  0.20253189 -0.04804732 -0.31472382], action=1, reward=1.0, next_state=[ 0.04383752  0.39830417 -0.0543418  -0.62216381]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 7 ] state=[ 0.04383752  0.39830417 -0.0543418  -0.62216381], action=0, reward=1.0, next_state=[ 0.0518036   0.20398146 -0.06678508 -0.34707865]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 8 ] state=[ 0.0518036   0.20398146 -0.06678508 -0.34707865], action=0, reward=1.0, next_state=[ 0.05588323  0.00986987 -0.07372665 -0.07618059]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 9 ] state=[ 0.05588323  0.00986987 -0.07372665 -0.07618059], action=0, reward=1.0, next_state=[ 0.05608063 -0.18412196 -0.07525026  0.19236067]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 10 ] state=[ 0.05608063 -0.18412196 -0.07525026  0.19236067], action=1, reward=1.0, next_state=[ 0.05239819  0.0119913  -0.07140305 -0.12307944]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 11 ] state=[ 0.05239819  0.0119913  -0.07140305 -0.12307944], action=1, reward=1.0, next_state=[ 0.05263801  0.20805973 -0.07386464 -0.43740733]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 12 ] state=[ 0.05263801  0.20805973 -0.07386464 -0.43740733], action=0, reward=1.0, next_state=[ 0.05679921  0.01405679 -0.08261278 -0.16889288]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 13 ] state=[ 0.05679921  0.01405679 -0.08261278 -0.16889288], action=1, reward=1.0, next_state=[ 0.05708034  0.21025809 -0.08599064 -0.48645166]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 14 ] state=[ 0.05708034  0.21025809 -0.08599064 -0.48645166], action=0, reward=1.0, next_state=[ 0.06128551  0.01644808 -0.09571968 -0.22206091]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 15 ] state=[ 0.06128551  0.01644808 -0.09571968 -0.22206091], action=1, reward=1.0, next_state=[ 0.06161447  0.21279859 -0.10016089 -0.54333708]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 16 ] state=[ 0.06161447  0.21279859 -0.10016089 -0.54333708], action=0, reward=1.0, next_state=[ 0.06587044  0.01921633 -0.11102764 -0.28381711]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 17 ] state=[ 0.06587044  0.01921633 -0.11102764 -0.28381711], action=1, reward=1.0, next_state=[ 0.06625477  0.21573229 -0.11670398 -0.60935116]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 18 ] state=[ 0.06625477  0.21573229 -0.11670398 -0.60935116], action=0, reward=1.0, next_state=[ 0.07056941  0.02241849 -0.128891   -0.35558603]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 19 ] state=[ 0.07056941  0.02241849 -0.128891   -0.35558603], action=1, reward=1.0, next_state=[ 0.07101778  0.21911488 -0.13600272 -0.68597236]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 20 ] state=[ 0.07101778  0.21911488 -0.13600272 -0.68597236], action=0, reward=1.0, next_state=[ 0.07540008  0.02611681 -0.14972217 -0.43901014]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 21 ] state=[ 0.07540008  0.02611681 -0.14972217 -0.43901014], action=0, reward=1.0, next_state=[ 0.07592242 -0.16660415 -0.15850237 -0.19701684]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 22 ] state=[ 0.07592242 -0.16660415 -0.15850237 -0.19701684], action=0, reward=1.0, next_state=[ 0.07259033 -0.35914581 -0.16244271  0.04177051]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 23 ] state=[ 0.07259033 -0.35914581 -0.16244271  0.04177051], action=1, reward=1.0, next_state=[ 0.06540742 -0.16211234 -0.1616073  -0.29743724]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 24 ] state=[ 0.06540742 -0.16211234 -0.1616073  -0.29743724], action=1, reward=1.0, next_state=[ 0.06216517  0.03490004 -0.16755604 -0.63641118]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 25 ] state=[ 0.06216517  0.03490004 -0.16755604 -0.63641118], action=0, reward=1.0, next_state=[ 0.06286317 -0.1575379  -0.18028427 -0.40082812]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 26 ] state=[ 0.06286317 -0.1575379  -0.18028427 -0.40082812], action=1, reward=1.0, next_state=[ 0.05971241  0.03962199 -0.18830083 -0.74449175]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 27 ] state=[ 0.05971241  0.03962199 -0.18830083 -0.74449175], action=1, reward=1.0, next_state=[ 0.06050485  0.23677437 -0.20319066 -1.0900268 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 28 ] state=[ 0.06050485  0.23677437 -0.20319066 -1.0900268 ], action=0, reward=-1.0, next_state=[ 0.06524034  0.04482494 -0.2249912  -0.86735376]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 44: Exploration_rate=0.01. Score=28.\n",
      "[ episode 45 ] state=[ 0.04823659  0.04315728 -0.00840204 -0.01898929]\n",
      "[ episode 45 ][ timestamp 1 ] state=[ 0.04823659  0.04315728 -0.00840204 -0.01898929], action=0, reward=1.0, next_state=[ 0.04909973 -0.15184317 -0.00878182  0.2710309 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 2 ] state=[ 0.04909973 -0.15184317 -0.00878182  0.2710309 ], action=1, reward=1.0, next_state=[ 0.04606287  0.04340299 -0.0033612  -0.02440886]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 3 ] state=[ 0.04606287  0.04340299 -0.0033612  -0.02440886], action=1, reward=1.0, next_state=[ 0.04693093  0.23857298 -0.00384938 -0.31815039]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 4 ] state=[ 0.04693093  0.23857298 -0.00384938 -0.31815039], action=0, reward=1.0, next_state=[ 0.05170239  0.04350606 -0.01021239 -0.0266839 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 5 ] state=[ 0.05170239  0.04350606 -0.01021239 -0.0266839 ], action=1, reward=1.0, next_state=[ 0.05257251  0.23877297 -0.01074607 -0.32257137]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 6 ] state=[ 0.05257251  0.23877297 -0.01074607 -0.32257137], action=0, reward=1.0, next_state=[ 0.05734797  0.04380568 -0.01719749 -0.0332966 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 7 ] state=[ 0.05734797  0.04380568 -0.01719749 -0.0332966 ], action=1, reward=1.0, next_state=[ 0.05822408  0.23916997 -0.01786343 -0.33135553]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 45 ][ timestamp 8 ] state=[ 0.05822408  0.23916997 -0.01786343 -0.33135553], action=1, reward=1.0, next_state=[ 0.06300748  0.43454158 -0.02449054 -0.62961775]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 9 ] state=[ 0.06300748  0.43454158 -0.02449054 -0.62961775], action=0, reward=1.0, next_state=[ 0.07169831  0.23976979 -0.03708289 -0.34474718]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 10 ] state=[ 0.07169831  0.23976979 -0.03708289 -0.34474718], action=1, reward=1.0, next_state=[ 0.07649371  0.43539911 -0.04397784 -0.64888928]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 11 ] state=[ 0.07649371  0.43539911 -0.04397784 -0.64888928], action=0, reward=1.0, next_state=[ 0.08520169  0.24091649 -0.05695562 -0.37037274]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 12 ] state=[ 0.08520169  0.24091649 -0.05695562 -0.37037274], action=1, reward=1.0, next_state=[ 0.09002002  0.43679944 -0.06436308 -0.68045662]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 13 ] state=[ 0.09002002  0.43679944 -0.06436308 -0.68045662], action=1, reward=1.0, next_state=[ 0.09875601  0.63275354 -0.07797221 -0.99268883]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 14 ] state=[ 0.09875601  0.63275354 -0.07797221 -0.99268883], action=0, reward=1.0, next_state=[ 0.11141108  0.43875653 -0.09782598 -0.72547807]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 15 ] state=[ 0.11141108  0.43875653 -0.09782598 -0.72547807], action=0, reward=1.0, next_state=[ 0.12018621  0.24511363 -0.11233555 -0.46511745]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 16 ] state=[ 0.12018621  0.24511363 -0.11233555 -0.46511745], action=0, reward=1.0, next_state=[ 0.12508848  0.05174336 -0.12163789 -0.20984749]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 17 ] state=[ 0.12508848  0.05174336 -0.12163789 -0.20984749], action=1, reward=1.0, next_state=[ 0.12612335  0.24837557 -0.12583484 -0.53828993]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 18 ] state=[ 0.12612335  0.24837557 -0.12583484 -0.53828993], action=1, reward=1.0, next_state=[ 0.13109086  0.44502108 -0.13660064 -0.86782383]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 19 ] state=[ 0.13109086  0.44502108 -0.13660064 -0.86782383], action=0, reward=1.0, next_state=[ 0.13999128  0.25199575 -0.15395712 -0.62101881]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 20 ] state=[ 0.13999128  0.25199575 -0.15395712 -0.62101881], action=0, reward=1.0, next_state=[ 0.1450312   0.05932088 -0.1663775  -0.38050974]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 21 ] state=[ 0.1450312   0.05932088 -0.1663775  -0.38050974], action=1, reward=1.0, next_state=[ 0.14621762  0.25636631 -0.17398769 -0.72068604]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 22 ] state=[ 0.14621762  0.25636631 -0.17398769 -0.72068604], action=0, reward=1.0, next_state=[ 0.15134494  0.06402358 -0.18840141 -0.48742255]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 23 ] state=[ 0.15134494  0.06402358 -0.18840141 -0.48742255], action=0, reward=1.0, next_state=[ 0.15262541 -0.12801044 -0.19814986 -0.25953153]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 24 ] state=[ 0.15262541 -0.12801044 -0.19814986 -0.25953153], action=0, reward=1.0, next_state=[ 0.15006521 -0.31983345 -0.20334049 -0.03530286]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 25 ] state=[ 0.15006521 -0.31983345 -0.20334049 -0.03530286], action=0, reward=1.0, next_state=[ 0.14366854 -0.51154711 -0.20404655  0.18697196]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 26 ] state=[ 0.14366854 -0.51154711 -0.20404655  0.18697196], action=1, reward=1.0, next_state=[ 0.13343759 -0.31417927 -0.20030711 -0.16251237]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 27 ] state=[ 0.13343759 -0.31417927 -0.20030711 -0.16251237], action=1, reward=1.0, next_state=[ 0.12715401 -0.11683762 -0.20355736 -0.51110349]\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 28 ] state=[ 0.12715401 -0.11683762 -0.20355736 -0.51110349], action=1, reward=-1.0, next_state=[ 0.12481726  0.08048211 -0.21377943 -0.86040561]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 45: Exploration_rate=0.01. Score=28.\n",
      "[ episode 46 ] state=[-0.01691799 -0.0469115  -0.01121607 -0.00313979]\n",
      "[ episode 46 ][ timestamp 1 ] state=[-0.01691799 -0.0469115  -0.01121607 -0.00313979], action=0, reward=1.0, next_state=[-0.01785622 -0.24187081 -0.01127887  0.28598333]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 2 ] state=[-0.01785622 -0.24187081 -0.01127887  0.28598333], action=1, reward=1.0, next_state=[-0.02269364 -0.04658984 -0.0055592  -0.01023542]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 3 ] state=[-0.02269364 -0.04658984 -0.0055592  -0.01023542], action=0, reward=1.0, next_state=[-0.02362544 -0.24163162 -0.00576391  0.28068835]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 4 ] state=[-0.02362544 -0.24163162 -0.00576391  0.28068835], action=1, reward=1.0, next_state=[-0.02845807 -0.04642793 -0.00015014 -0.01380691]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 5 ] state=[-0.02845807 -0.04642793 -0.00015014 -0.01380691], action=0, reward=1.0, next_state=[-0.02938663 -0.24154772 -0.00042628  0.27882864]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 6 ] state=[-0.02938663 -0.24154772 -0.00042628  0.27882864], action=1, reward=1.0, next_state=[-0.03421758 -0.04641969  0.00515029 -0.0139887 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 7 ] state=[-0.03421758 -0.04641969  0.00515029 -0.0139887 ], action=0, reward=1.0, next_state=[-0.03514597 -0.24161513  0.00487052  0.28031474]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 8 ] state=[-0.03514597 -0.24161513  0.00487052  0.28031474], action=0, reward=1.0, next_state=[-0.03997828 -0.43680622  0.01047681  0.57452983]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 9 ] state=[-0.03997828 -0.43680622  0.01047681  0.57452983], action=1, reward=1.0, next_state=[-0.0487144  -0.2418327   0.02196741  0.28516573]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 10 ] state=[-0.0487144  -0.2418327   0.02196741  0.28516573], action=1, reward=1.0, next_state=[-0.05355106 -0.04703083  0.02767073 -0.00050868]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 11 ] state=[-0.05355106 -0.04703083  0.02767073 -0.00050868], action=1, reward=1.0, next_state=[-0.05449167  0.14768358  0.02766055 -0.28433433]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 12 ] state=[-0.05449167  0.14768358  0.02766055 -0.28433433], action=1, reward=1.0, next_state=[-0.051538    0.34240033  0.02197387 -0.56816655]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 13 ] state=[-0.051538    0.34240033  0.02197387 -0.56816655], action=1, reward=1.0, next_state=[-0.04468999  0.53720728  0.01061053 -0.85384664]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 14 ] state=[-0.04468999  0.53720728  0.01061053 -0.85384664], action=0, reward=1.0, next_state=[-0.03394585  0.34194233 -0.0064664  -0.55784626]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 15 ] state=[-0.03394585  0.34194233 -0.0064664  -0.55784626], action=0, reward=1.0, next_state=[-0.027107    0.14691174 -0.01762332 -0.26720761]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 16 ] state=[-0.027107    0.14691174 -0.01762332 -0.26720761], action=1, reward=1.0, next_state=[-0.02416877  0.34228072 -0.02296748 -0.56539656]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 17 ] state=[-0.02416877  0.34228072 -0.02296748 -0.56539656], action=0, reward=1.0, next_state=[-0.01732315  0.14748839 -0.03427541 -0.28003698]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 18 ] state=[-0.01732315  0.14748839 -0.03427541 -0.28003698], action=1, reward=1.0, next_state=[-0.01437338  0.34308209 -0.03987615 -0.5833302 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 19 ] state=[-0.01437338  0.34308209 -0.03987615 -0.5833302 ], action=1, reward=1.0, next_state=[-0.00751174  0.53873933 -0.05154275 -0.88830323]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 20 ] state=[-0.00751174  0.53873933 -0.05154275 -0.88830323], action=0, reward=1.0, next_state=[ 0.00326304  0.34435336 -0.06930882 -0.61225836]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 21 ] state=[ 0.00326304  0.34435336 -0.06930882 -0.61225836], action=0, reward=1.0, next_state=[ 0.01015011  0.150265   -0.08155398 -0.34218529]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 46 ][ timestamp 22 ] state=[ 0.01015011  0.150265   -0.08155398 -0.34218529], action=0, reward=1.0, next_state=[ 0.01315541 -0.04360769 -0.08839769 -0.07629311]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 23 ] state=[ 0.01315541 -0.04360769 -0.08839769 -0.07629311], action=0, reward=1.0, next_state=[ 0.01228326 -0.23735849 -0.08992355  0.18724325]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 24 ] state=[ 0.01228326 -0.23735849 -0.08992355  0.18724325], action=1, reward=1.0, next_state=[ 0.00753609 -0.04107274 -0.08617869 -0.13239768]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 25 ] state=[ 0.00753609 -0.04107274 -0.08617869 -0.13239768], action=1, reward=1.0, next_state=[ 0.00671463  0.15517118 -0.08882664 -0.45097632]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 26 ] state=[ 0.00671463  0.15517118 -0.08882664 -0.45097632], action=0, reward=1.0, next_state=[ 0.00981806 -0.03858959 -0.09784617 -0.18756172]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 27 ] state=[ 0.00981806 -0.03858959 -0.09784617 -0.18756172], action=1, reward=1.0, next_state=[ 0.00904626  0.1577862  -0.1015974  -0.50943736]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 28 ] state=[ 0.00904626  0.1577862  -0.1015974  -0.50943736], action=0, reward=1.0, next_state=[ 0.01220199 -0.0357687  -0.11178615 -0.25042041]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 29 ] state=[ 0.01220199 -0.0357687  -0.11178615 -0.25042041], action=0, reward=1.0, next_state=[ 0.01148661 -0.22913156 -0.11679456  0.00501682]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 30 ] state=[ 0.01148661 -0.22913156 -0.11679456  0.00501682], action=1, reward=1.0, next_state=[ 0.00690398 -0.03254512 -0.11669422 -0.32211349]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 31 ] state=[ 0.00690398 -0.03254512 -0.11669422 -0.32211349], action=1, reward=1.0, next_state=[ 0.00625308  0.16402848 -0.12313649 -0.64919882]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 32 ] state=[ 0.00625308  0.16402848 -0.12313649 -0.64919882], action=1, reward=1.0, next_state=[ 0.00953365  0.36063107 -0.13612047 -0.97798048]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 33 ] state=[ 0.00953365  0.36063107 -0.13612047 -0.97798048], action=1, reward=1.0, next_state=[ 0.01674627  0.55728956 -0.15568008 -1.31013549]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 34 ] state=[ 0.01674627  0.55728956 -0.15568008 -1.31013549], action=1, reward=1.0, next_state=[ 0.02789206  0.7540026  -0.18188278 -1.64722186]\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 35 ] state=[ 0.02789206  0.7540026  -0.18188278 -1.64722186], action=1, reward=-1.0, next_state=[ 0.04297212  0.95072609 -0.21482722 -1.99061882]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 46: Exploration_rate=0.01. Score=35.\n",
      "[ episode 47 ] state=[ 0.04772537 -0.01466121  0.00476804  0.02696535]\n",
      "[ episode 47 ][ timestamp 1 ] state=[ 0.04772537 -0.01466121  0.00476804  0.02696535], action=1, reward=1.0, next_state=[ 0.04743215  0.18039204  0.00530735 -0.2642094 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 2 ] state=[ 0.04743215  0.18039204  0.00530735 -0.2642094 ], action=0, reward=1.0, next_state=[ 5.10399891e-02 -1.48052634e-02  2.31585968e-05  3.01427779e-02]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 3 ] state=[ 5.10399891e-02 -1.48052634e-02  2.31585968e-05  3.01427779e-02], action=0, reward=1.0, next_state=[ 0.05074388 -0.20992755  0.00062601  0.32283301]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 4 ] state=[ 0.05074388 -0.20992755  0.00062601  0.32283301], action=1, reward=1.0, next_state=[ 0.04654533 -0.01481452  0.00708267  0.03034757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 5 ] state=[ 0.04654533 -0.01481452  0.00708267  0.03034757], action=0, reward=1.0, next_state=[ 0.04624904 -0.21003732  0.00768963  0.32525672]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 6 ] state=[ 0.04624904 -0.21003732  0.00768963  0.32525672], action=1, reward=1.0, next_state=[ 0.0420483  -0.01502569  0.01419476  0.03500865]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 7 ] state=[ 0.0420483  -0.01502569  0.01419476  0.03500865], action=0, reward=1.0, next_state=[ 0.04174778 -0.21034829  0.01489493  0.33213616]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 8 ] state=[ 0.04174778 -0.21034829  0.01489493  0.33213616], action=1, reward=1.0, next_state=[ 0.03754082 -0.01544149  0.02153766  0.04418733]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 9 ] state=[ 0.03754082 -0.01544149  0.02153766  0.04418733], action=0, reward=1.0, next_state=[ 0.03723199 -0.21086555  0.0224214   0.34358702]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 10 ] state=[ 0.03723199 -0.21086555  0.0224214   0.34358702], action=1, reward=1.0, next_state=[ 0.03301468 -0.01606963  0.02929314  0.05805792]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 11 ] state=[ 0.03301468 -0.01606963  0.02929314  0.05805792], action=0, reward=1.0, next_state=[ 0.03269328 -0.21159908  0.0304543   0.35983722]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 12 ] state=[ 0.03269328 -0.21159908  0.0304543   0.35983722], action=1, reward=1.0, next_state=[ 0.0284613  -0.01692298  0.03765105  0.07691065]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 13 ] state=[ 0.0284613  -0.01692298  0.03765105  0.07691065], action=0, reward=1.0, next_state=[ 0.02812284 -0.21256389  0.03918926  0.38123083]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 14 ] state=[ 0.02812284 -0.21256389  0.03918926  0.38123083], action=0, reward=1.0, next_state=[ 0.02387156 -0.40821976  0.04681388  0.686008  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 15 ] state=[ 0.02387156 -0.40821976  0.04681388  0.686008  ], action=1, reward=1.0, next_state=[ 0.01570717 -0.21377788  0.06053404  0.40842297]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 16 ] state=[ 0.01570717 -0.21377788  0.06053404  0.40842297], action=1, reward=1.0, next_state=[ 0.01143161 -0.01956412  0.0687025   0.13542206]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 17 ] state=[ 0.01143161 -0.01956412  0.0687025   0.13542206], action=1, reward=1.0, next_state=[ 0.01104033  0.17450994  0.07141094 -0.13481963]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 18 ] state=[ 0.01104033  0.17450994  0.07141094 -0.13481963], action=1, reward=1.0, next_state=[ 0.01453053  0.36854022  0.06871454 -0.40414629]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 19 ] state=[ 0.01453053  0.36854022  0.06871454 -0.40414629], action=1, reward=1.0, next_state=[ 0.02190133  0.56262379  0.06063162 -0.67439844]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 20 ] state=[ 0.02190133  0.56262379  0.06063162 -0.67439844], action=0, reward=1.0, next_state=[ 0.03315381  0.36671395  0.04714365 -0.36325888]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 21 ] state=[ 0.03315381  0.36671395  0.04714365 -0.36325888], action=0, reward=1.0, next_state=[ 0.04048809  0.17095477  0.03987847 -0.05609126]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 22 ] state=[ 0.04048809  0.17095477  0.03987847 -0.05609126], action=1, reward=1.0, next_state=[ 0.04390718  0.36548291  0.03875665 -0.33593032]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 23 ] state=[ 0.04390718  0.36548291  0.03875665 -0.33593032], action=1, reward=1.0, next_state=[ 0.05121684  0.56003248  0.03203804 -0.61614394]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 24 ] state=[ 0.05121684  0.56003248  0.03203804 -0.61614394], action=0, reward=1.0, next_state=[ 0.06241749  0.36447792  0.01971516 -0.31354505]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 25 ] state=[ 0.06241749  0.36447792  0.01971516 -0.31354505], action=1, reward=1.0, next_state=[ 0.06970705  0.55931355  0.01344426 -0.59994582]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 26 ] state=[ 0.06970705  0.55931355  0.01344426 -0.59994582], action=1, reward=1.0, next_state=[ 0.08089332  0.75424486  0.00144534 -0.88836386]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 27 ] state=[ 0.08089332  0.75424486  0.00144534 -0.88836386], action=0, reward=1.0, next_state=[ 0.09597822  0.55910332 -0.01632193 -0.59522693]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 28 ] state=[ 0.09597822  0.55910332 -0.01632193 -0.59522693], action=0, reward=1.0, next_state=[ 0.10716028  0.36421356 -0.02822647 -0.30772967]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 47 ][ timestamp 29 ] state=[ 0.10716028  0.36421356 -0.02822647 -0.30772967], action=0, reward=1.0, next_state=[ 0.11444455  0.16950495 -0.03438107 -0.02408057]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 30 ] state=[ 0.11444455  0.16950495 -0.03438107 -0.02408057], action=1, reward=1.0, next_state=[ 0.11783465  0.36510265 -0.03486268 -0.32740978]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 31 ] state=[ 0.11783465  0.36510265 -0.03486268 -0.32740978], action=0, reward=1.0, next_state=[ 0.12513671  0.17049393 -0.04141087 -0.04592163]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 32 ] state=[ 0.12513671  0.17049393 -0.04141087 -0.04592163], action=1, reward=1.0, next_state=[ 0.12854659  0.36618447 -0.04232931 -0.35137711]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 33 ] state=[ 0.12854659  0.36618447 -0.04232931 -0.35137711], action=0, reward=1.0, next_state=[ 0.13587027  0.17168925 -0.04935685 -0.0723367 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 34 ] state=[ 0.13587027  0.17168925 -0.04935685 -0.0723367 ], action=0, reward=1.0, next_state=[ 0.13930406 -0.02269163 -0.05080358  0.20437453]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 35 ] state=[ 0.13930406 -0.02269163 -0.05080358  0.20437453], action=1, reward=1.0, next_state=[ 0.13885023  0.17311865 -0.04671609 -0.10389176]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 36 ] state=[ 0.13885023  0.17311865 -0.04671609 -0.10389176], action=0, reward=1.0, next_state=[ 0.1423126  -0.02130377 -0.04879393  0.17369416]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 37 ] state=[ 0.1423126  -0.02130377 -0.04879393  0.17369416], action=0, reward=1.0, next_state=[ 0.14188652 -0.21569465 -0.04532004  0.45059371]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 38 ] state=[ 0.14188652 -0.21569465 -0.04532004  0.45059371], action=0, reward=1.0, next_state=[ 0.13757263 -0.41014731 -0.03630817  0.72865368]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 39 ] state=[ 0.13757263 -0.41014731 -0.03630817  0.72865368], action=0, reward=1.0, next_state=[ 0.12936969 -0.60474905 -0.0217351   1.00969164]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 40 ] state=[ 0.12936969 -0.60474905 -0.0217351   1.00969164], action=1, reward=1.0, next_state=[ 0.1172747  -0.40934385 -0.00154126  0.71026346]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 41 ] state=[ 0.1172747  -0.40934385 -0.00154126  0.71026346], action=1, reward=1.0, next_state=[ 0.10908783 -0.21420059  0.01266401  0.41709579]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 42 ] state=[ 0.10908783 -0.21420059  0.01266401  0.41709579], action=0, reward=1.0, next_state=[ 0.10480382 -0.4094997   0.02100592  0.71374408]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 43 ] state=[ 0.10480382 -0.4094997   0.02100592  0.71374408], action=1, reward=1.0, next_state=[ 0.09661382 -0.21467476  0.0352808   0.42774644]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 44 ] state=[ 0.09661382 -0.21467476  0.0352808   0.42774644], action=1, reward=1.0, next_state=[ 0.09232033 -0.02006979  0.04383573  0.14639104]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 45 ] state=[ 0.09232033 -0.02006979  0.04383573  0.14639104], action=1, reward=1.0, next_state=[ 0.09191893  0.17439789  0.04676355 -0.13214668]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 46 ] state=[ 0.09191893  0.17439789  0.04676355 -0.13214668], action=0, reward=1.0, next_state=[ 0.09540689 -0.02136165  0.04412062  0.17491508]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 47 ] state=[ 0.09540689 -0.02136165  0.04412062  0.17491508], action=0, reward=1.0, next_state=[ 0.09497966 -0.21708635  0.04761892  0.48118369]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 48 ] state=[ 0.09497966 -0.21708635  0.04761892  0.48118369], action=1, reward=1.0, next_state=[ 0.09063793 -0.02266778  0.0572426   0.20388108]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 49 ] state=[ 0.09063793 -0.02266778  0.0572426   0.20388108], action=0, reward=1.0, next_state=[ 0.09018457 -0.21855964  0.06132022  0.51405773]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 50 ] state=[ 0.09018457 -0.21855964  0.06132022  0.51405773], action=0, reward=1.0, next_state=[ 0.08581338 -0.41448918  0.07160137  0.8254165 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 51 ] state=[ 0.08581338 -0.41448918  0.07160137  0.8254165 ], action=0, reward=1.0, next_state=[ 0.0775236  -0.61051353  0.0881097   1.13973245]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 52 ] state=[ 0.0775236  -0.61051353  0.0881097   1.13973245], action=1, reward=1.0, next_state=[ 0.06531333 -0.41664699  0.11090435  0.87593145]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 53 ] state=[ 0.06531333 -0.41664699  0.11090435  0.87593145], action=1, reward=1.0, next_state=[ 0.05698039 -0.22319311  0.12842298  0.62007245]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 54 ] state=[ 0.05698039 -0.22319311  0.12842298  0.62007245], action=0, reward=1.0, next_state=[ 0.05251652 -0.41985259  0.14082443  0.95028512]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 55 ] state=[ 0.05251652 -0.41985259  0.14082443  0.95028512], action=1, reward=1.0, next_state=[ 0.04411947 -0.22687819  0.15983013  0.70495469]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 56 ] state=[ 0.04411947 -0.22687819  0.15983013  0.70495469], action=0, reward=1.0, next_state=[ 0.03958191 -0.42381135  0.17392923  1.04337962]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 57 ] state=[ 0.03958191 -0.42381135  0.17392923  1.04337962], action=1, reward=1.0, next_state=[ 0.03110568 -0.23137169  0.19479682  0.80995306]\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 58 ] state=[ 0.03110568 -0.23137169  0.19479682  0.80995306], action=1, reward=-1.0, next_state=[ 0.02647825 -0.03937576  0.21099588  0.58431475]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 47: Exploration_rate=0.01. Score=58.\n",
      "[ episode 48 ] state=[-0.03646513 -0.0295532   0.04321229  0.03916854]\n",
      "[ episode 48 ][ timestamp 1 ] state=[-0.03646513 -0.0295532   0.04321229  0.03916854], action=0, reward=1.0, next_state=[-0.0370562  -0.22526732  0.04399566  0.34516612]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 2 ] state=[-0.0370562  -0.22526732  0.04399566  0.34516612], action=1, reward=1.0, next_state=[-0.04156154 -0.03079793  0.05089898  0.06667486]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 3 ] state=[-0.04156154 -0.03079793  0.05089898  0.06667486], action=1, reward=1.0, next_state=[-0.0421775   0.16355873  0.05223248 -0.20952472]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 4 ] state=[-0.0421775   0.16355873  0.05223248 -0.20952472], action=1, reward=1.0, next_state=[-0.03890633  0.35789643  0.04804199 -0.48528435]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 5 ] state=[-0.03890633  0.35789643  0.04804199 -0.48528435], action=0, reward=1.0, next_state=[-0.0317484   0.16213061  0.0383363  -0.17785552]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 6 ] state=[-0.0317484   0.16213061  0.0383363  -0.17785552], action=1, reward=1.0, next_state=[-0.02850579  0.35668357  0.03477919 -0.45820243]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 7 ] state=[-0.02850579  0.35668357  0.03477919 -0.45820243], action=0, reward=1.0, next_state=[-0.02137212  0.16108767  0.02561514 -0.15476298]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 8 ] state=[-0.02137212  0.16108767  0.02561514 -0.15476298], action=0, reward=1.0, next_state=[-0.01815036 -0.0343915   0.02251988  0.14588961]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 9 ] state=[-0.01815036 -0.0343915   0.02251988  0.14588961], action=1, reward=1.0, next_state=[-0.01883819  0.16040083  0.02543767 -0.13960452]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 10 ] state=[-0.01883819  0.16040083  0.02543767 -0.13960452], action=1, reward=1.0, next_state=[-0.01563018  0.35514939  0.02264558 -0.42415498]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 11 ] state=[-0.01563018  0.35514939  0.02264558 -0.42415498], action=1, reward=1.0, next_state=[-0.00852719  0.54994335  0.01416248 -0.70961377]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 12 ] state=[-0.00852719  0.54994335  0.01416248 -0.70961377], action=0, reward=1.0, next_state=[ 2.47167842e-03  3.54628139e-01 -2.97921181e-05 -4.12506700e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 13 ] state=[ 2.47167842e-03  3.54628139e-01 -2.97921181e-05 -4.12506700e-01], action=1, reward=1.0, next_state=[ 0.00956424  0.54975051 -0.00827993 -0.70519902]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 48 ][ timestamp 14 ] state=[ 0.00956424  0.54975051 -0.00827993 -0.70519902], action=0, reward=1.0, next_state=[ 0.02055925  0.35474426 -0.02238391 -0.41513394]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 15 ] state=[ 0.02055925  0.35474426 -0.02238391 -0.41513394], action=0, reward=1.0, next_state=[ 0.02765414  0.1599466  -0.03068659 -0.12959096]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 16 ] state=[ 0.02765414  0.1599466  -0.03068659 -0.12959096], action=0, reward=1.0, next_state=[ 0.03085307 -0.03472263 -0.0332784   0.15325497]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 17 ] state=[ 0.03085307 -0.03472263 -0.0332784   0.15325497], action=1, reward=1.0, next_state=[ 0.03015862  0.16085962 -0.03021331 -0.14973803]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 18 ] state=[ 0.03015862  0.16085962 -0.03021331 -0.14973803], action=1, reward=1.0, next_state=[ 0.03337581  0.3564009  -0.03320807 -0.45179743]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 19 ] state=[ 0.03337581  0.3564009  -0.03320807 -0.45179743], action=0, reward=1.0, next_state=[ 0.04050383  0.16176394 -0.04224401 -0.16976433]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 20 ] state=[ 0.04050383  0.16176394 -0.04224401 -0.16976433], action=1, reward=1.0, next_state=[ 0.04373911  0.35746428 -0.0456393  -0.47546901]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 21 ] state=[ 0.04373911  0.35746428 -0.0456393  -0.47546901], action=0, reward=1.0, next_state=[ 0.05088839  0.16301551 -0.05514868 -0.19751286]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 22 ] state=[ 0.05088839  0.16301551 -0.05514868 -0.19751286], action=1, reward=1.0, next_state=[ 0.0541487   0.35888113 -0.05909894 -0.50707013]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 23 ] state=[ 0.0541487   0.35888113 -0.05909894 -0.50707013], action=0, reward=1.0, next_state=[ 0.06132632  0.16463954 -0.06924034 -0.23358138]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 24 ] state=[ 0.06132632  0.16463954 -0.06924034 -0.23358138], action=1, reward=1.0, next_state=[ 0.06461911  0.36067898 -0.07391197 -0.54727634]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 25 ] state=[ 0.06461911  0.36067898 -0.07391197 -0.54727634], action=0, reward=1.0, next_state=[ 0.07183269  0.16666901 -0.0848575  -0.27876627]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 26 ] state=[ 0.07183269  0.16666901 -0.0848575  -0.27876627], action=0, reward=1.0, next_state=[ 0.07516607 -0.02714634 -0.09043282 -0.0140075 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 27 ] state=[ 0.07516607 -0.02714634 -0.09043282 -0.0140075 ], action=0, reward=1.0, next_state=[ 0.07462315 -0.22086283 -0.09071297  0.24882884]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 28 ] state=[ 0.07462315 -0.22086283 -0.09071297  0.24882884], action=1, reward=1.0, next_state=[ 0.07020589 -0.02457038 -0.08573639 -0.07103226]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 29 ] state=[ 0.07020589 -0.02457038 -0.08573639 -0.07103226], action=0, reward=1.0, next_state=[ 0.06971448 -0.21836521 -0.08715704  0.1934166 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 30 ] state=[ 0.06971448 -0.21836521 -0.08715704  0.1934166 ], action=1, reward=1.0, next_state=[ 0.06534718 -0.0221116  -0.08328871 -0.12543815]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 31 ] state=[ 0.06534718 -0.0221116  -0.08328871 -0.12543815], action=1, reward=1.0, next_state=[ 0.06490495  0.17409867 -0.08579747 -0.4431919 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 32 ] state=[ 0.06490495  0.17409867 -0.08579747 -0.4431919 ], action=0, reward=1.0, next_state=[ 0.06838692 -0.01971113 -0.09466131 -0.17874006]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 33 ] state=[ 0.06838692 -0.01971113 -0.09466131 -0.17874006], action=1, reward=1.0, next_state=[ 0.0679927   0.17662901 -0.09823611 -0.49972062]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 34 ] state=[ 0.0679927   0.17662901 -0.09823611 -0.49972062], action=0, reward=1.0, next_state=[ 0.07152528 -0.01698076 -0.10823052 -0.23954112]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 35 ] state=[ 0.07152528 -0.01698076 -0.10823052 -0.23954112], action=0, reward=1.0, next_state=[ 0.07118566 -0.21040353 -0.11302134  0.01713771]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 36 ] state=[ 0.07118566 -0.21040353 -0.11302134  0.01713771], action=1, reward=1.0, next_state=[ 0.06697759 -0.01385741 -0.11267859 -0.30895807]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 37 ] state=[ 0.06697759 -0.01385741 -0.11267859 -0.30895807], action=1, reward=1.0, next_state=[ 0.06670044  0.18267459 -0.11885775 -0.63494405]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 38 ] state=[ 0.06670044  0.18267459 -0.11885775 -0.63494405], action=1, reward=1.0, next_state=[ 0.07035394  0.37923626 -0.13155663 -0.96256832]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 39 ] state=[ 0.07035394  0.37923626 -0.13155663 -0.96256832], action=0, reward=1.0, next_state=[ 0.07793866  0.18610392 -0.150808   -0.71393931]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 40 ] state=[ 0.07793866  0.18610392 -0.150808   -0.71393931], action=0, reward=1.0, next_state=[ 0.08166074 -0.00664424 -0.16508679 -0.47226828]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 41 ] state=[ 0.08166074 -0.00664424 -0.16508679 -0.47226828], action=1, reward=1.0, next_state=[ 0.08152785  0.19037742 -0.17453215 -0.81209809]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 42 ] state=[ 0.08152785  0.19037742 -0.17453215 -0.81209809], action=0, reward=1.0, next_state=[ 0.0853354  -0.00197933 -0.19077411 -0.57899876]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 43 ] state=[ 0.0853354  -0.00197933 -0.19077411 -0.57899876], action=0, reward=1.0, next_state=[ 0.08529582 -0.19398807 -0.20235409 -0.35195886]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 44 ] state=[ 0.08529582 -0.19398807 -0.20235409 -0.35195886], action=0, reward=1.0, next_state=[ 0.08141605 -0.3857441  -0.20939327 -0.12928057]\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 45 ] state=[ 0.08141605 -0.3857441  -0.20939327 -0.12928057], action=1, reward=-1.0, next_state=[ 0.07370117 -0.18833368 -0.21197888 -0.48004095]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 48: Exploration_rate=0.01. Score=45.\n",
      "[ episode 49 ] state=[-0.02308932  0.04350422 -0.03366331 -0.02491041]\n",
      "[ episode 49 ][ timestamp 1 ] state=[-0.02308932  0.04350422 -0.03366331 -0.02491041], action=0, reward=1.0, next_state=[-0.02221923 -0.1511192  -0.03416152  0.25696418]\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 2 ] state=[-0.02221923 -0.1511192  -0.03416152  0.25696418], action=1, reward=1.0, next_state=[-0.02524162  0.0444734  -0.02902223 -0.04629507]\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 3 ] state=[-0.02524162  0.0444734  -0.02902223 -0.04629507], action=0, reward=1.0, next_state=[-0.02435215 -0.15022063 -0.02994813  0.23709165]\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 4 ] state=[-0.02435215 -0.15022063 -0.02994813  0.23709165], action=0, reward=1.0, next_state=[-0.02735656 -0.34490221 -0.0252063   0.52017963]\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 5 ] state=[-0.02735656 -0.34490221 -0.0252063   0.52017963], action=1, reward=1.0, next_state=[-0.03425461 -0.14943465 -0.01480271  0.21966157]\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 6 ] state=[-0.03425461 -0.14943465 -0.01480271  0.21966157], action=1, reward=1.0, next_state=[-0.0372433   0.04589573 -0.01040948 -0.07765374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 7 ] state=[-0.0372433   0.04589573 -0.01040948 -0.07765374], action=0, reward=1.0, next_state=[-0.03632538 -0.14907546 -0.01196255  0.21172687]\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 8 ] state=[-0.03632538 -0.14907546 -0.01196255  0.21172687], action=1, reward=1.0, next_state=[-0.03930689  0.04621547 -0.00772801 -0.08470547]\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 9 ] state=[-0.03930689  0.04621547 -0.00772801 -0.08470547], action=1, reward=1.0, next_state=[-0.03838258  0.24144734 -0.00942212 -0.37981654]\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 10 ] state=[-0.03838258  0.24144734 -0.00942212 -0.37981654], action=0, reward=1.0, next_state=[-0.03355364  0.04646045 -0.01701845 -0.09011925]\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 11 ] state=[-0.03355364  0.04646045 -0.01701845 -0.09011925], action=0, reward=1.0, next_state=[-0.03262443 -0.14841349 -0.01882084  0.19714614]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 49 ][ timestamp 12 ] state=[-0.03262443 -0.14841349 -0.01882084  0.19714614], action=0, reward=1.0, next_state=[-0.0355927  -0.34326125 -0.01487792  0.48383301]\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 13 ] state=[-0.0355927  -0.34326125 -0.01487792  0.48383301], action=0, reward=1.0, next_state=[-0.04245792 -0.5381701  -0.00520126  0.77178999]\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 14 ] state=[-0.04245792 -0.5381701  -0.00520126  0.77178999], action=1, reward=1.0, next_state=[-0.05322133 -0.34297697  0.01023454  0.47747509]\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 15 ] state=[-0.05322133 -0.34297697  0.01023454  0.47747509], action=0, reward=1.0, next_state=[-0.06008086 -0.53824191  0.01978405  0.77336607]\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 16 ] state=[-0.06008086 -0.53824191  0.01978405  0.77336607], action=1, reward=1.0, next_state=[-0.0708457  -0.34339766  0.03525137  0.48697301]\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 17 ] state=[-0.0708457  -0.34339766  0.03525137  0.48697301], action=0, reward=1.0, next_state=[-0.07771366 -0.53899881  0.04499083  0.79055422]\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 18 ] state=[-0.07771366 -0.53899881  0.04499083  0.79055422], action=0, reward=1.0, next_state=[-0.08849363 -0.73470874  0.06080191  1.09704489]\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 19 ] state=[-0.08849363 -0.73470874  0.06080191  1.09704489], action=0, reward=1.0, next_state=[-0.10318781 -0.93057627  0.08274281  1.40816802]\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 20 ] state=[-0.10318781 -0.93057627  0.08274281  1.40816802], action=0, reward=1.0, next_state=[-0.12179933 -1.12662159  0.11090617  1.72552856]\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 21 ] state=[-0.12179933 -1.12662159  0.11090617  1.72552856], action=1, reward=1.0, next_state=[-0.14433176 -0.93292929  0.14541674  1.46931474]\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 22 ] state=[-0.14433176 -0.93292929  0.14541674  1.46931474], action=0, reward=1.0, next_state=[-0.16299035 -1.12950024  0.17480304  1.80366114]\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 23 ] state=[-0.16299035 -1.12950024  0.17480304  1.80366114], action=1, reward=-1.0, next_state=[-0.18558036 -0.93670916  0.21087626  1.57001225]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 49: Exploration_rate=0.01. Score=23.\n",
      "[ episode 50 ] state=[ 0.04479809 -0.03983746  0.00483124  0.02906465]\n",
      "[ episode 50 ][ timestamp 1 ] state=[ 0.04479809 -0.03983746  0.00483124  0.02906465], action=0, reward=1.0, next_state=[ 0.04400134 -0.23502836  0.00541253  0.32326796]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 2 ] state=[ 0.04400134 -0.23502836  0.00541253  0.32326796], action=1, reward=1.0, next_state=[ 0.03930077 -0.03998389  0.01187789  0.03229683]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 3 ] state=[ 0.03930077 -0.03998389  0.01187789  0.03229683], action=0, reward=1.0, next_state=[ 0.03850109 -0.23527415  0.01252382  0.32870356]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 4 ] state=[ 0.03850109 -0.23527415  0.01252382  0.32870356], action=1, reward=1.0, next_state=[ 0.03379561 -0.0403327   0.0190979   0.03999624]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 5 ] state=[ 0.03379561 -0.0403327   0.0190979   0.03999624], action=1, reward=1.0, next_state=[ 0.03298896  0.15451025  0.01989782 -0.24660046]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 6 ] state=[ 0.03298896  0.15451025  0.01989782 -0.24660046], action=0, reward=1.0, next_state=[ 0.03607916 -0.04089015  0.01496581  0.05229169]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 7 ] state=[ 0.03607916 -0.04089015  0.01496581  0.05229169], action=0, reward=1.0, next_state=[ 0.03526136 -0.23622346  0.01601164  0.34965863]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 8 ] state=[ 0.03526136 -0.23622346  0.01601164  0.34965863], action=0, reward=1.0, next_state=[ 0.03053689 -0.43156942  0.02300482  0.64734724]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 9 ] state=[ 0.03053689 -0.43156942  0.02300482  0.64734724], action=1, reward=1.0, next_state=[ 0.0219055  -0.23677542  0.03595176  0.36199636]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 10 ] state=[ 0.0219055  -0.23677542  0.03595176  0.36199636], action=1, reward=1.0, next_state=[ 0.01716999 -0.04218242  0.04319169  0.08086303]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 11 ] state=[ 0.01716999 -0.04218242  0.04319169  0.08086303], action=0, reward=1.0, next_state=[ 0.01632634 -0.23789606  0.04480895  0.38685411]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 12 ] state=[ 0.01632634 -0.23789606  0.04480895  0.38685411], action=1, reward=1.0, next_state=[ 0.01156842 -0.04343789  0.05254603  0.10862905]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 13 ] state=[ 0.01156842 -0.04343789  0.05254603  0.10862905], action=1, reward=1.0, next_state=[ 0.01069966  0.15089324  0.05471861 -0.16702389]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 14 ] state=[ 0.01069966  0.15089324  0.05471861 -0.16702389], action=0, reward=1.0, next_state=[ 0.01371753 -0.04496753  0.05137814  0.14240681]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 15 ] state=[ 0.01371753 -0.04496753  0.05137814  0.14240681], action=0, reward=1.0, next_state=[ 0.01281818 -0.24078622  0.05422627  0.45084577]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 16 ] state=[ 0.01281818 -0.24078622  0.05422627  0.45084577], action=1, reward=1.0, next_state=[ 0.00800245 -0.04647145  0.06324319  0.17573676]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 17 ] state=[ 0.00800245 -0.04647145  0.06324319  0.17573676], action=1, reward=1.0, next_state=[ 0.00707303  0.14769108  0.06675792 -0.09634368]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 18 ] state=[ 0.00707303  0.14769108  0.06675792 -0.09634368], action=0, reward=1.0, next_state=[ 0.01002685 -0.04832101  0.06483105  0.21663179]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 19 ] state=[ 0.01002685 -0.04832101  0.06483105  0.21663179], action=1, reward=1.0, next_state=[ 0.00906043  0.14581712  0.06916368 -0.05491665]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 20 ] state=[ 0.00906043  0.14581712  0.06916368 -0.05491665], action=0, reward=1.0, next_state=[ 0.01197677 -0.05022487  0.06806535  0.25876118]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 21 ] state=[ 0.01197677 -0.05022487  0.06806535  0.25876118], action=1, reward=1.0, next_state=[ 0.01097227  0.14386269  0.07324057 -0.01170026]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 22 ] state=[ 0.01097227  0.14386269  0.07324057 -0.01170026], action=0, reward=1.0, next_state=[ 0.01384953 -0.05222905  0.07300657  0.30316228]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 23 ] state=[ 0.01384953 -0.05222905  0.07300657  0.30316228], action=1, reward=1.0, next_state=[0.01280494 0.14178061 0.07906981 0.03436786]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 24 ] state=[0.01280494 0.14178061 0.07906981 0.03436786], action=0, reward=1.0, next_state=[ 0.01564056 -0.05438096  0.07975717  0.35091319]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 25 ] state=[ 0.01564056 -0.05438096  0.07975717  0.35091319], action=0, reward=1.0, next_state=[ 0.01455294 -0.25054122  0.08677544  0.66764197]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 26 ] state=[ 0.01455294 -0.25054122  0.08677544  0.66764197], action=1, reward=1.0, next_state=[ 0.00954211 -0.05672635  0.10012828  0.40349352]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 27 ] state=[ 0.00954211 -0.05672635  0.10012828  0.40349352], action=1, reward=1.0, next_state=[0.00840759 0.13684355 0.10819815 0.14398151]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 28 ] state=[0.00840759 0.13684355 0.10819815 0.14398151], action=0, reward=1.0, next_state=[ 0.01114446 -0.0596482   0.11107778  0.46874381]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 29 ] state=[ 0.01114446 -0.0596482   0.11107778  0.46874381], action=1, reward=1.0, next_state=[0.00995149 0.13374369 0.12045265 0.21303348]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 30 ] state=[0.00995149 0.13374369 0.12045265 0.21303348], action=0, reward=1.0, next_state=[ 0.01262637 -0.06287602  0.12471332  0.5411536 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 31 ] state=[ 0.01262637 -0.06287602  0.12471332  0.5411536 ], action=1, reward=1.0, next_state=[0.01136885 0.13029269 0.13553639 0.29022169]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 32 ] state=[0.01136885 0.13029269 0.13553639 0.29022169], action=1, reward=1.0, next_state=[0.0139747  0.32324789 0.14134083 0.04316908]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 33 ] state=[0.0139747  0.32324789 0.14134083 0.04316908], action=0, reward=1.0, next_state=[0.02043966 0.12641187 0.14220421 0.37689483]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 34 ] state=[0.02043966 0.12641187 0.14220421 0.37689483], action=1, reward=1.0, next_state=[0.0229679  0.319258   0.14974211 0.13221279]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 35 ] state=[0.0229679  0.319258   0.14974211 0.13221279], action=0, reward=1.0, next_state=[0.02935306 0.1223437  0.15238636 0.46813876]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 36 ] state=[0.02935306 0.1223437  0.15238636 0.46813876], action=0, reward=1.0, next_state=[ 0.03179993 -0.07456557  0.16174914  0.80470828]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 50 ][ timestamp 37 ] state=[ 0.03179993 -0.07456557  0.16174914  0.80470828], action=1, reward=1.0, next_state=[0.03030862 0.11801319 0.1778433  0.56695785]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 38 ] state=[0.03030862 0.11801319 0.1778433  0.56695785], action=1, reward=1.0, next_state=[0.03266888 0.3102533  0.18918246 0.33515659]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 39 ] state=[0.03266888 0.3102533  0.18918246 0.33515659], action=1, reward=1.0, next_state=[0.03887395 0.50225016 0.19588559 0.1075881 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 40 ] state=[0.03887395 0.50225016 0.19588559 0.1075881 ], action=0, reward=1.0, next_state=[0.04891895 0.30493936 0.19803735 0.45511691]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 41 ] state=[0.04891895 0.30493936 0.19803735 0.45511691], action=0, reward=1.0, next_state=[0.05501774 0.10764949 0.20713969 0.8031107 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 42 ] state=[0.05501774 0.10764949 0.20713969 0.8031107 ], action=1, reward=-1.0, next_state=[0.05717073 0.29942091 0.22320191 0.58206728]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 50: Exploration_rate=0.01. Score=42.\n",
      "[ episode 51 ] state=[0.02209279 0.03323408 0.01676797 0.0412674 ]\n",
      "[ episode 51 ][ timestamp 1 ] state=[0.02209279 0.03323408 0.01676797 0.0412674 ], action=1, reward=1.0, next_state=[ 0.02275747  0.22811162  0.01759332 -0.24607826]\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 2 ] state=[ 0.02275747  0.22811162  0.01759332 -0.24607826], action=0, reward=1.0, next_state=[0.02731971 0.03274287 0.01267175 0.05210168]\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 3 ] state=[0.02731971 0.03274287 0.01267175 0.05210168], action=1, reward=1.0, next_state=[ 0.02797456  0.22768085  0.01371379 -0.23655643]\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 4 ] state=[ 0.02797456  0.22768085  0.01371379 -0.23655643], action=0, reward=1.0, next_state=[0.03252818 0.03236568 0.00898266 0.0604205 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 5 ] state=[0.03252818 0.03236568 0.00898266 0.0604205 ], action=1, reward=1.0, next_state=[ 0.03317549  0.2273577   0.01019107 -0.22941486]\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 6 ] state=[ 0.03317549  0.2273577   0.01019107 -0.22941486], action=0, reward=1.0, next_state=[0.03772265 0.03209161 0.00560277 0.06646519]\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 7 ] state=[0.03772265 0.03209161 0.00560277 0.06646519], action=1, reward=1.0, next_state=[ 0.03836448  0.22713279  0.00693207 -0.22444478]\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 8 ] state=[ 0.03836448  0.22713279  0.00693207 -0.22444478], action=0, reward=1.0, next_state=[0.04290714 0.03191245 0.00244318 0.0704167 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 9 ] state=[0.04290714 0.03191245 0.00244318 0.0704167 ], action=1, reward=1.0, next_state=[ 0.04354538  0.22699929  0.00385151 -0.22149439]\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 10 ] state=[ 0.04354538  0.22699929  0.00385151 -0.22149439], action=0, reward=1.0, next_state=[ 0.04808537  0.0318225  -0.00057838  0.07240097]\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 11 ] state=[ 0.04808537  0.0318225  -0.00057838  0.07240097], action=1, reward=1.0, next_state=[ 0.04872182  0.22695274  0.00086964 -0.22046438]\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 12 ] state=[ 0.04872182  0.22695274  0.00086964 -0.22046438], action=1, reward=1.0, next_state=[ 0.05326087  0.42206225 -0.00353964 -0.51287286]\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 13 ] state=[ 0.05326087  0.42206225 -0.00353964 -0.51287286], action=1, reward=1.0, next_state=[ 0.06170212  0.61723387 -0.0137971  -0.80666912]\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 14 ] state=[ 0.06170212  0.61723387 -0.0137971  -0.80666912], action=1, reward=1.0, next_state=[ 0.0740468   0.81254219 -0.02993048 -1.10365993]\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 15 ] state=[ 0.0740468   0.81254219 -0.02993048 -1.10365993], action=1, reward=1.0, next_state=[ 0.09029764  1.00804476 -0.05200368 -1.40558069]\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 16 ] state=[ 0.09029764  1.00804476 -0.05200368 -1.40558069], action=0, reward=1.0, next_state=[ 0.11045854  0.81360552 -0.0801153  -1.12959832]\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 17 ] state=[ 0.11045854  0.81360552 -0.0801153  -1.12959832], action=1, reward=1.0, next_state=[ 0.12673065  1.00968002 -0.10270726 -1.44629541]\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 18 ] state=[ 0.12673065  1.00968002 -0.10270726 -1.44629541], action=0, reward=1.0, next_state=[ 0.14692425  0.81596047 -0.13163317 -1.18739024]\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 19 ] state=[ 0.14692425  0.81596047 -0.13163317 -1.18739024], action=0, reward=1.0, next_state=[ 0.16324346  0.62276759 -0.15538098 -0.93869642]\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 20 ] state=[ 0.16324346  0.62276759 -0.15538098 -0.93869642], action=1, reward=1.0, next_state=[ 0.17569881  0.81960426 -0.1741549  -1.27589279]\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 21 ] state=[ 0.17569881  0.81960426 -0.1741549  -1.27589279], action=1, reward=1.0, next_state=[ 0.19209089  1.01646597 -0.19967276 -1.61766167]\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 22 ] state=[ 0.19209089  1.01646597 -0.19967276 -1.61766167], action=1, reward=-1.0, next_state=[ 0.21242021  1.21330299 -0.23202599 -1.96536542]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 51: Exploration_rate=0.01. Score=22.\n",
      "[ episode 52 ] state=[ 0.0167798  -0.03774758 -0.00289243  0.00777961]\n",
      "[ episode 52 ][ timestamp 1 ] state=[ 0.0167798  -0.03774758 -0.00289243  0.00777961], action=1, reward=1.0, next_state=[ 0.01602485  0.15741574 -0.00273684 -0.28581451]\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 2 ] state=[ 0.01602485  0.15741574 -0.00273684 -0.28581451], action=0, reward=1.0, next_state=[ 0.01917317 -0.03766708 -0.00845313  0.00600398]\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 3 ] state=[ 0.01917317 -0.03766708 -0.00845313  0.00600398], action=1, reward=1.0, next_state=[ 0.01841982  0.15757508 -0.00833305 -0.28933398]\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 4 ] state=[ 0.01841982  0.15757508 -0.00833305 -0.28933398], action=1, reward=1.0, next_state=[ 0.02157133  0.35281486 -0.01411973 -0.58463337]\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 5 ] state=[ 0.02157133  0.35281486 -0.01411973 -0.58463337], action=0, reward=1.0, next_state=[ 0.02862762  0.15789351 -0.0258124  -0.29643156]\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 6 ] state=[ 0.02862762  0.15789351 -0.0258124  -0.29643156], action=0, reward=1.0, next_state=[ 0.03178549 -0.03685113 -0.03174103 -0.0119999 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 7 ] state=[ 0.03178549 -0.03685113 -0.03174103 -0.0119999 ], action=0, reward=1.0, next_state=[ 0.03104847 -0.23150384 -0.03198103  0.27050179]\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 8 ] state=[ 0.03104847 -0.23150384 -0.03198103  0.27050179], action=0, reward=1.0, next_state=[ 0.02641839 -0.42615517 -0.02657099  0.55292866]\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 9 ] state=[ 0.02641839 -0.42615517 -0.02657099  0.55292866], action=0, reward=1.0, next_state=[ 0.01789529 -0.6208941  -0.01551242  0.837123  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 10 ] state=[ 0.01789529 -0.6208941  -0.01551242  0.837123  ], action=0, reward=1.0, next_state=[ 0.00547741 -0.81580079  0.00123004  1.12488739]\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 11 ] state=[ 0.00547741 -0.81580079  0.00123004  1.12488739], action=0, reward=1.0, next_state=[-0.01083861 -1.01093884  0.02372779  1.41795588]\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 12 ] state=[-0.01083861 -1.01093884  0.02372779  1.41795588], action=0, reward=1.0, next_state=[-0.03105738 -1.20634637  0.05208691  1.71795997]\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 13 ] state=[-0.03105738 -1.20634637  0.05208691  1.71795997], action=0, reward=1.0, next_state=[-0.05518431 -1.40202525  0.08644611  2.02638685]\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 14 ] state=[-0.05518431 -1.40202525  0.08644611  2.02638685], action=0, reward=1.0, next_state=[-0.08322482 -1.59792806  0.12697384  2.34452729]\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 15 ] state=[-0.08322482 -1.59792806  0.12697384  2.34452729], action=1, reward=1.0, next_state=[-0.11518338 -1.40415586  0.17386439  2.09343897]\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 16 ] state=[-0.11518338 -1.40415586  0.17386439  2.09343897], action=1, reward=-1.0, next_state=[-0.14326649 -1.21116039  0.21573317  1.85916927]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 52: Exploration_rate=0.01. Score=16.\n",
      "[ episode 53 ] state=[ 0.00614851 -0.02761336  0.03959487  0.01187264]\n",
      "[ episode 53 ][ timestamp 1 ] state=[ 0.00614851 -0.02761336  0.03959487  0.01187264], action=1, reward=1.0, next_state=[ 0.00559624  0.16691904  0.03983232 -0.2680594 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 2 ] state=[ 0.00559624  0.16691904  0.03983232 -0.2680594 ], action=1, reward=1.0, next_state=[ 0.00893462  0.36145056  0.03447113 -0.54791762]\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 3 ] state=[ 0.00893462  0.36145056  0.03447113 -0.54791762], action=1, reward=1.0, next_state=[ 0.01616363  0.55607172  0.02351278 -0.82954342]\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 4 ] state=[ 0.01616363  0.55607172  0.02351278 -0.82954342], action=1, reward=1.0, next_state=[ 0.02728506  0.75086449  0.00692191 -1.1147397 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 53 ][ timestamp 5 ] state=[ 0.02728506  0.75086449  0.00692191 -1.1147397 ], action=1, reward=1.0, next_state=[ 0.04230235  0.94589488 -0.01537288 -1.40524325]\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 6 ] state=[ 0.04230235  0.94589488 -0.01537288 -1.40524325], action=1, reward=1.0, next_state=[ 0.06122025  1.14120427 -0.04347775 -1.70269217]\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 7 ] state=[ 0.06122025  1.14120427 -0.04347775 -1.70269217], action=0, reward=1.0, next_state=[ 0.08404434  0.94660903 -0.07753159 -1.42385359]\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 8 ] state=[ 0.08404434  0.94660903 -0.07753159 -1.42385359], action=0, reward=1.0, next_state=[ 0.10297652  0.75252655 -0.10600866 -1.15637587]\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 9 ] state=[ 0.10297652  0.75252655 -0.10600866 -1.15637587], action=0, reward=1.0, next_state=[ 0.11802705  0.5589341  -0.12913618 -0.89872555]\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 10 ] state=[ 0.11802705  0.5589341  -0.12913618 -0.89872555], action=0, reward=1.0, next_state=[ 0.12920573  0.36577654 -0.14711069 -0.6492623 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 11 ] state=[ 0.12920573  0.36577654 -0.14711069 -0.6492623 ], action=0, reward=1.0, next_state=[ 0.13652126  0.17297689 -0.16009594 -0.40628126]\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 12 ] state=[ 0.13652126  0.17297689 -0.16009594 -0.40628126], action=0, reward=1.0, next_state=[ 0.1399808  -0.01955575 -0.16822156 -0.16804286]\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 13 ] state=[ 0.1399808  -0.01955575 -0.16822156 -0.16804286], action=1, reward=1.0, next_state=[ 0.13958968  0.17752463 -0.17158242 -0.50871468]\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 14 ] state=[ 0.13958968  0.17752463 -0.17158242 -0.50871468], action=0, reward=1.0, next_state=[ 0.14314018 -0.01481741 -0.18175671 -0.27463627]\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 15 ] state=[ 0.14314018 -0.01481741 -0.18175671 -0.27463627], action=1, reward=1.0, next_state=[ 0.14284383  0.18236945 -0.18724944 -0.61868711]\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 16 ] state=[ 0.14284383  0.18236945 -0.18724944 -0.61868711], action=0, reward=1.0, next_state=[ 0.14649122 -0.00971187 -0.19962318 -0.39033171]\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 17 ] state=[ 0.14649122 -0.00971187 -0.19962318 -0.39033171], action=0, reward=1.0, next_state=[ 0.14629698 -0.20152396 -0.20742982 -0.16662746]\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 18 ] state=[ 0.14629698 -0.20152396 -0.20742982 -0.16662746], action=1, reward=-1.0, next_state=[ 0.1422665  -0.00412994 -0.21076236 -0.5169193 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 53: Exploration_rate=0.01. Score=18.\n",
      "[ episode 54 ] state=[-1.74675948e-02 -2.42826825e-03  1.02452170e-06  6.13554275e-03]\n",
      "[ episode 54 ][ timestamp 1 ] state=[-1.74675948e-02 -2.42826825e-03  1.02452170e-06  6.13554275e-03], action=1, reward=1.0, next_state=[-1.75161601e-02  1.92693668e-01  1.23735377e-04 -2.86547061e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 2 ] state=[-1.75161601e-02  1.92693668e-01  1.23735377e-04 -2.86547061e-01], action=0, reward=1.0, next_state=[-0.01366229 -0.00243005 -0.00560721  0.00617489]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 3 ] state=[-0.01366229 -0.00243005 -0.00560721  0.00617489], action=1, reward=1.0, next_state=[-0.01371089  0.19277187 -0.00548371 -0.28827189]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 4 ] state=[-0.01371089  0.19277187 -0.00548371 -0.28827189], action=1, reward=1.0, next_state=[-0.00985545  0.38797159 -0.01124915 -0.58267927]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 5 ] state=[-0.00985545  0.38797159 -0.01124915 -0.58267927], action=0, reward=1.0, next_state=[-0.00209602  0.19300903 -0.02290273 -0.29356112]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 6 ] state=[-0.00209602  0.19300903 -0.02290273 -0.29356112], action=1, reward=1.0, next_state=[ 0.00176416  0.3884499  -0.02877395 -0.59337836]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 7 ] state=[ 0.00176416  0.3884499  -0.02877395 -0.59337836], action=0, reward=1.0, next_state=[ 0.00953316  0.19374229 -0.04064152 -0.30989621]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 8 ] state=[ 0.00953316  0.19374229 -0.04064152 -0.30989621], action=0, reward=1.0, next_state=[ 0.01340801 -0.00077775 -0.04683945 -0.0303024 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 9 ] state=[ 0.01340801 -0.00077775 -0.04683945 -0.0303024 ], action=1, reward=1.0, next_state=[ 0.01339245  0.19498352 -0.04744549 -0.33738801]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 10 ] state=[ 0.01339245  0.19498352 -0.04744549 -0.33738801], action=1, reward=1.0, next_state=[ 0.01729212  0.3907474  -0.05419325 -0.64464713]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 11 ] state=[ 0.01729212  0.3907474  -0.05419325 -0.64464713], action=0, reward=1.0, next_state=[ 0.02510707  0.1964209  -0.0670862  -0.36951033]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 12 ] state=[ 0.02510707  0.1964209  -0.0670862  -0.36951033], action=1, reward=1.0, next_state=[ 0.02903549  0.3924287  -0.0744764  -0.68256922]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 13 ] state=[ 0.02903549  0.3924287  -0.0744764  -0.68256922], action=0, reward=1.0, next_state=[ 0.03688406  0.19841568 -0.08812779 -0.41423225]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 14 ] state=[ 0.03688406  0.19841568 -0.08812779 -0.41423225], action=0, reward=1.0, next_state=[ 0.04085237  0.00464617 -0.09641243 -0.15058199]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 15 ] state=[ 0.04085237  0.00464617 -0.09641243 -0.15058199], action=1, reward=1.0, next_state=[ 0.0409453   0.20100699 -0.09942407 -0.47205671]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 16 ] state=[ 0.0409453   0.20100699 -0.09942407 -0.47205671], action=0, reward=1.0, next_state=[ 0.04496544  0.00741953 -0.10886521 -0.21229211]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 17 ] state=[ 0.04496544  0.00741953 -0.10886521 -0.21229211], action=1, reward=1.0, next_state=[ 0.04511383  0.20391596 -0.11311105 -0.53723506]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 18 ] state=[ 0.04511383  0.20391596 -0.11311105 -0.53723506], action=1, reward=1.0, next_state=[ 0.04919215  0.40043132 -0.12385575 -0.86330821]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 19 ] state=[ 0.04919215  0.40043132 -0.12385575 -0.86330821], action=0, reward=1.0, next_state=[ 0.05720077  0.2071935  -0.14112191 -0.61199244]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 20 ] state=[ 0.05720077  0.2071935  -0.14112191 -0.61199244], action=0, reward=1.0, next_state=[ 0.06134464  0.01429651 -0.15336176 -0.36687566]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 21 ] state=[ 0.06134464  0.01429651 -0.15336176 -0.36687566], action=0, reward=1.0, next_state=[ 0.06163057 -0.17835144 -0.16069928 -0.12620718]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 22 ] state=[ 0.06163057 -0.17835144 -0.16069928 -0.12620718], action=1, reward=1.0, next_state=[ 0.05806355  0.01866413 -0.16322342 -0.46496541]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 23 ] state=[ 0.05806355  0.01866413 -0.16322342 -0.46496541], action=1, reward=1.0, next_state=[ 0.05843683  0.21567067 -0.17252273 -0.80432237]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 24 ] state=[ 0.05843683  0.21567067 -0.17252273 -0.80432237], action=0, reward=1.0, next_state=[ 0.06275024  0.02328065 -0.18860917 -0.57049188]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 25 ] state=[ 0.06275024  0.02328065 -0.18860917 -0.57049188], action=0, reward=1.0, next_state=[ 0.06321585 -0.16876561 -0.20001901 -0.34265403]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 26 ] state=[ 0.06321585 -0.16876561 -0.20001901 -0.34265403], action=1, reward=1.0, next_state=[ 0.05984054  0.02855627 -0.20687209 -0.69115005]\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 27 ] state=[ 0.05984054  0.02855627 -0.20687209 -0.69115005], action=0, reward=-1.0, next_state=[ 0.06041167 -0.16318657 -0.22069509 -0.47005576]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 54: Exploration_rate=0.01. Score=27.\n",
      "[ episode 55 ] state=[-0.01243509 -0.01131522 -0.04438431  0.00068926]\n",
      "[ episode 55 ][ timestamp 1 ] state=[-0.01243509 -0.01131522 -0.04438431  0.00068926], action=1, reward=1.0, next_state=[-0.01266139  0.18441424 -0.04437052 -0.30566049]\n",
      "[ Experience replay ] starts\n",
      "[ episode 55 ][ timestamp 2 ] state=[-0.01266139  0.18441424 -0.04437052 -0.30566049], action=1, reward=1.0, next_state=[-0.00897311  0.38013947 -0.05048373 -0.61200004]\n",
      "[ Experience replay ] starts\n",
      "[ episode 55 ][ timestamp 3 ] state=[-0.00897311  0.38013947 -0.05048373 -0.61200004], action=1, reward=1.0, next_state=[-0.00137032  0.57592928 -0.06272373 -0.92014651]\n",
      "[ Experience replay ] starts\n",
      "[ episode 55 ][ timestamp 4 ] state=[-0.00137032  0.57592928 -0.06272373 -0.92014651], action=1, reward=1.0, next_state=[ 0.01014827  0.77184033 -0.08112666 -1.23186388]\n",
      "[ Experience replay ] starts\n",
      "[ episode 55 ][ timestamp 5 ] state=[ 0.01014827  0.77184033 -0.08112666 -1.23186388], action=1, reward=1.0, next_state=[ 0.02558508  0.96790647 -0.10576394 -1.54882089]\n",
      "[ Experience replay ] starts\n",
      "[ episode 55 ][ timestamp 6 ] state=[ 0.02558508  0.96790647 -0.10576394 -1.54882089], action=1, reward=1.0, next_state=[ 0.0449432   1.16412687 -0.13674036 -1.8725435 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 55 ][ timestamp 7 ] state=[ 0.0449432   1.16412687 -0.13674036 -1.8725435 ], action=1, reward=1.0, next_state=[ 0.06822574  1.36045228 -0.17419123 -2.20435924]\n",
      "[ Experience replay ] starts\n",
      "[ episode 55 ][ timestamp 8 ] state=[ 0.06822574  1.36045228 -0.17419123 -2.20435924], action=1, reward=-1.0, next_state=[ 0.09543479  1.55676907 -0.21827841 -2.54533177]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 55: Exploration_rate=0.01. Score=8.\n",
      "[ episode 56 ] state=[-0.0447301   0.04940131 -0.00751866 -0.02033787]\n",
      "[ episode 56 ][ timestamp 1 ] state=[-0.0447301   0.04940131 -0.00751866 -0.02033787], action=1, reward=1.0, next_state=[-0.04374207  0.24463027 -0.00792542 -0.31538351]\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 2 ] state=[-0.04374207  0.24463027 -0.00792542 -0.31538351], action=1, reward=1.0, next_state=[-0.03884947  0.43986422 -0.01423309 -0.61055527]\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 3 ] state=[-0.03884947  0.43986422 -0.01423309 -0.61055527], action=1, reward=1.0, next_state=[-0.03005218  0.63518219 -0.02644419 -0.90768695]\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 4 ] state=[-0.03005218  0.63518219 -0.02644419 -0.90768695], action=1, reward=1.0, next_state=[-0.01734854  0.83065196 -0.04459793 -1.20856278]\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 5 ] state=[-0.01734854  0.83065196 -0.04459793 -1.20856278], action=1, reward=1.0, next_state=[-7.35501559e-04  1.02632068e+00 -6.87691892e-02 -1.51488146e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 6 ] state=[-7.35501559e-04  1.02632068e+00 -6.87691892e-02 -1.51488146e+00], action=1, reward=1.0, next_state=[ 0.01979091  1.22220425 -0.09906682 -1.82821452]\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 7 ] state=[ 0.01979091  1.22220425 -0.09906682 -1.82821452], action=1, reward=1.0, next_state=[ 0.044235    1.41827483 -0.13563111 -2.14995639]\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 8 ] state=[ 0.044235    1.41827483 -0.13563111 -2.14995639], action=1, reward=1.0, next_state=[ 0.07260049  1.61444596 -0.17863024 -2.48126409]\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 9 ] state=[ 0.07260049  1.61444596 -0.17863024 -2.48126409], action=1, reward=-1.0, next_state=[ 0.10488941  1.81055531 -0.22825552 -2.82298582]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 56: Exploration_rate=0.01. Score=9.\n",
      "[ episode 57 ] state=[0.03637915 0.04089196 0.01805071 0.0214861 ]\n",
      "[ episode 57 ][ timestamp 1 ] state=[0.03637915 0.04089196 0.01805071 0.0214861 ], action=1, reward=1.0, next_state=[ 0.03719699  0.23575045  0.01848043 -0.26544741]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 2 ] state=[ 0.03719699  0.23575045  0.01848043 -0.26544741], action=1, reward=1.0, next_state=[ 0.041912    0.43060383  0.01317148 -0.55224463]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 3 ] state=[ 0.041912    0.43060383  0.01317148 -0.55224463], action=1, reward=1.0, next_state=[ 0.05052407  0.62553835  0.00212659 -0.84074874]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 4 ] state=[ 0.05052407  0.62553835  0.00212659 -0.84074874], action=0, reward=1.0, next_state=[ 0.06303484  0.43038743 -0.01468838 -0.54739781]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 5 ] state=[ 0.06303484  0.43038743 -0.01468838 -0.54739781], action=1, reward=1.0, next_state=[ 0.07164259  0.62571263 -0.02563634 -0.84467223]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 6 ] state=[ 0.07164259  0.62571263 -0.02563634 -0.84467223], action=0, reward=1.0, next_state=[ 0.08415684  0.4309497  -0.04252979 -0.56016009]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 7 ] state=[ 0.08415684  0.4309497  -0.04252979 -0.56016009], action=0, reward=1.0, next_state=[ 0.09277584  0.23644966 -0.05373299 -0.28117384]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 8 ] state=[ 0.09277584  0.23644966 -0.05373299 -0.28117384], action=1, reward=1.0, next_state=[ 0.09750483  0.43229526 -0.05935646 -0.59030815]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 9 ] state=[ 0.09750483  0.43229526 -0.05935646 -0.59030815], action=0, reward=1.0, next_state=[ 0.10615074  0.23805242 -0.07116263 -0.31689756]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 10 ] state=[ 0.10615074  0.23805242 -0.07116263 -0.31689756], action=0, reward=1.0, next_state=[ 0.11091178  0.04401242 -0.07750058 -0.04747838]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 11 ] state=[ 0.11091178  0.04401242 -0.07750058 -0.04747838], action=0, reward=1.0, next_state=[ 0.11179203 -0.14991762 -0.07845015  0.21978114]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 12 ] state=[ 0.11179203 -0.14991762 -0.07845015  0.21978114], action=0, reward=1.0, next_state=[ 0.10879368 -0.34383564 -0.07405452  0.48672284]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 13 ] state=[ 0.10879368 -0.34383564 -0.07405452  0.48672284], action=0, reward=1.0, next_state=[ 0.10191697 -0.53783881 -0.06432007  0.75517788]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 14 ] state=[ 0.10191697 -0.53783881 -0.06432007  0.75517788], action=1, reward=1.0, next_state=[ 0.09116019 -0.34189195 -0.04921651  0.44296831]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 15 ] state=[ 0.09116019 -0.34189195 -0.04921651  0.44296831], action=1, reward=1.0, next_state=[ 0.08432235 -0.14610939 -0.04035714  0.13518626]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 16 ] state=[ 0.08432235 -0.14610939 -0.04035714  0.13518626], action=1, reward=1.0, next_state=[ 0.08140016  0.04956668 -0.03765342 -0.16995064]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 17 ] state=[ 0.08140016  0.04956668 -0.03765342 -0.16995064], action=0, reward=1.0, next_state=[ 0.0823915  -0.14499666 -0.04105243  0.11062002]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 18 ] state=[ 0.0823915  -0.14499666 -0.04105243  0.11062002], action=0, reward=1.0, next_state=[ 0.07949156 -0.33950704 -0.03884003  0.39007374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 19 ] state=[ 0.07949156 -0.33950704 -0.03884003  0.39007374], action=0, reward=1.0, next_state=[ 0.07270142 -0.53405683 -0.03103855  0.67026224]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 20 ] state=[ 0.07270142 -0.53405683 -0.03103855  0.67026224], action=0, reward=1.0, next_state=[ 0.06202029 -0.72873381 -0.01763331  0.95301319]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 21 ] state=[ 0.06202029 -0.72873381 -0.01763331  0.95301319], action=1, reward=1.0, next_state=[ 0.04744561 -0.53337909  0.00142695  0.65484274]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 22 ] state=[ 0.04744561 -0.53337909  0.00142695  0.65484274], action=1, reward=1.0, next_state=[ 0.03677803 -0.33827703  0.01452381  0.36260948]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 23 ] state=[ 0.03677803 -0.33827703  0.01452381  0.36260948], action=0, reward=1.0, next_state=[ 0.03001249 -0.53360237  0.021776    0.65983644]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 24 ] state=[ 0.03001249 -0.53360237  0.021776    0.65983644], action=1, reward=1.0, next_state=[ 0.01934044 -0.33879013  0.03497273  0.374089  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 25 ] state=[ 0.01934044 -0.33879013  0.03497273  0.374089  ], action=0, reward=1.0, next_state=[ 0.01256464 -0.53439096  0.04245451  0.67759072]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 26 ] state=[ 0.01256464 -0.53439096  0.04245451  0.67759072], action=1, reward=1.0, next_state=[ 0.00187682 -0.33988376  0.05600632  0.39857069]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 27 ] state=[ 0.00187682 -0.33988376  0.05600632  0.39857069], action=1, reward=1.0, next_state=[-0.00492086 -0.14559922  0.06397774  0.12405806]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 28 ] state=[-0.00492086 -0.14559922  0.06397774  0.12405806], action=0, reward=1.0, next_state=[-0.00783284 -0.34157662  0.0664589   0.43621938]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 29 ] state=[-0.00783284 -0.34157662  0.0664589   0.43621938], action=1, reward=1.0, next_state=[-0.01466437 -0.1474553   0.07518328  0.16520474]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 57 ][ timestamp 30 ] state=[-0.01466437 -0.1474553   0.07518328  0.16520474], action=0, reward=1.0, next_state=[-0.01761348 -0.34356847  0.07848738  0.48062656]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 31 ] state=[-0.01761348 -0.34356847  0.07848738  0.48062656], action=1, reward=1.0, next_state=[-0.02448485 -0.14963709  0.08809991  0.21367663]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 32 ] state=[-0.02448485 -0.14963709  0.08809991  0.21367663], action=1, reward=1.0, next_state=[-0.02747759  0.04412207  0.09237344 -0.04996705]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 33 ] state=[-0.02747759  0.04412207  0.09237344 -0.04996705], action=0, reward=1.0, next_state=[-0.02659515 -0.15219468  0.0913741   0.2703718 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 34 ] state=[-0.02659515 -0.15219468  0.0913741   0.2703718 ], action=1, reward=1.0, next_state=[-0.02963904  0.04151261  0.09678154  0.00784962]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 35 ] state=[-0.02963904  0.04151261  0.09678154  0.00784962], action=0, reward=1.0, next_state=[-0.02880879 -0.15485451  0.09693853  0.32943127]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 36 ] state=[-0.02880879 -0.15485451  0.09693853  0.32943127], action=0, reward=1.0, next_state=[-0.03190588 -0.35121317  0.10352716  0.65104176]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 37 ] state=[-0.03190588 -0.35121317  0.10352716  0.65104176], action=1, reward=1.0, next_state=[-0.03893014 -0.15767387  0.11654799  0.39266981]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 38 ] state=[-0.03893014 -0.15767387  0.11654799  0.39266981], action=1, reward=1.0, next_state=[-0.04208362  0.03561796  0.12440139  0.13888661]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 39 ] state=[-0.04208362  0.03561796  0.12440139  0.13888661], action=1, reward=1.0, next_state=[-0.04137126  0.22875891  0.12717912 -0.11210623]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 40 ] state=[-0.04137126  0.22875891  0.12717912 -0.11210623], action=0, reward=1.0, next_state=[-0.03679608  0.03206572  0.12493699  0.21784066]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 41 ] state=[-0.03679608  0.03206572  0.12493699  0.21784066], action=0, reward=1.0, next_state=[-0.03615477 -0.1646002   0.12929381  0.54717617]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 42 ] state=[-0.03615477 -0.1646002   0.12929381  0.54717617], action=1, reward=1.0, next_state=[-0.03944677  0.02849087  0.14023733  0.29786366]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 43 ] state=[-0.03944677  0.02849087  0.14023733  0.29786366], action=1, reward=1.0, next_state=[-0.03887696  0.22136425  0.1461946   0.05248856]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 44 ] state=[-0.03887696  0.22136425  0.1461946   0.05248856], action=0, reward=1.0, next_state=[-0.03444967  0.0244815   0.14724438  0.38749061]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 45 ] state=[-0.03444967  0.0244815   0.14724438  0.38749061], action=1, reward=1.0, next_state=[-0.03396004  0.21724004  0.15499419  0.14461511]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 46 ] state=[-0.03396004  0.21724004  0.15499419  0.14461511], action=1, reward=1.0, next_state=[-0.02961524  0.40984188  0.15788649 -0.09543834]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 47 ] state=[-0.02961524  0.40984188  0.15788649 -0.09543834], action=0, reward=1.0, next_state=[-0.0214184   0.21285078  0.15597772  0.242599  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 48 ] state=[-0.0214184   0.21285078  0.15597772  0.242599  ], action=1, reward=1.0, next_state=[-0.01716139  0.40544074  0.1608297   0.00289283]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 49 ] state=[-0.01716139  0.40544074  0.1608297   0.00289283], action=0, reward=1.0, next_state=[-0.00905257  0.20842151  0.16088756  0.34168815]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 50 ] state=[-0.00905257  0.20842151  0.16088756  0.34168815], action=1, reward=1.0, next_state=[-0.00488414  0.40093247  0.16772132  0.10374813]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 51 ] state=[-0.00488414  0.40093247  0.16772132  0.10374813], action=0, reward=1.0, next_state=[0.00313451 0.20385362 0.16979629 0.44429741]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 52 ] state=[0.00313451 0.20385362 0.16979629 0.44429741], action=1, reward=1.0, next_state=[0.00721158 0.39621761 0.17868223 0.20958154]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 53 ] state=[0.00721158 0.39621761 0.17868223 0.20958154], action=0, reward=1.0, next_state=[0.01513593 0.19905063 0.18287386 0.55287677]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 54 ] state=[0.01513593 0.19905063 0.18287386 0.55287677], action=1, reward=1.0, next_state=[0.01911695 0.39119725 0.1939314  0.32292861]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 55 ] state=[0.01911695 0.39119725 0.1939314  0.32292861], action=1, reward=1.0, next_state=[0.02694089 0.58310523 0.20038997 0.09712196]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 56 ] state=[0.02694089 0.58310523 0.20038997 0.09712196], action=0, reward=1.0, next_state=[0.03860299 0.38575968 0.20233241 0.44573782]\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 57 ] state=[0.03860299 0.38575968 0.20233241 0.44573782], action=1, reward=-1.0, next_state=[0.04631819 0.57753074 0.21124717 0.22302996]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 57: Exploration_rate=0.01. Score=57.\n",
      "[ episode 58 ] state=[ 0.00290774  0.02724752 -0.01130739 -0.03319322]\n",
      "[ episode 58 ][ timestamp 1 ] state=[ 0.00290774  0.02724752 -0.01130739 -0.03319322], action=1, reward=1.0, next_state=[ 0.00345269  0.22252979 -0.01197125 -0.32942219]\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 2 ] state=[ 0.00345269  0.22252979 -0.01197125 -0.32942219], action=1, reward=1.0, next_state=[ 0.00790329  0.41782009 -0.01855969 -0.62585612]\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 3 ] state=[ 0.00790329  0.41782009 -0.01855969 -0.62585612], action=0, reward=1.0, next_state=[ 0.01625969  0.22296207 -0.03107682 -0.33907567]\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 4 ] state=[ 0.01625969  0.22296207 -0.03107682 -0.33907567], action=1, reward=1.0, next_state=[ 0.02071893  0.41851212 -0.03785833 -0.64139423]\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 5 ] state=[ 0.02071893  0.41851212 -0.03785833 -0.64139423], action=0, reward=1.0, next_state=[ 0.02908917  0.2239378  -0.05068621 -0.36086957]\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 6 ] state=[ 0.02908917  0.2239378  -0.05068621 -0.36086957], action=1, reward=1.0, next_state=[ 0.03356793  0.41974221 -0.05790361 -0.66909435]\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 7 ] state=[ 0.03356793  0.41974221 -0.05790361 -0.66909435], action=0, reward=1.0, next_state=[ 0.04196277  0.22547116 -0.07128549 -0.3951903 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 8 ] state=[ 0.04196277  0.22547116 -0.07128549 -0.3951903 ], action=1, reward=1.0, next_state=[ 0.04647219  0.42152836 -0.0791893  -0.70946939]\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 9 ] state=[ 0.04647219  0.42152836 -0.0791893  -0.70946939], action=0, reward=1.0, next_state=[ 0.05490276  0.22758731 -0.09337869 -0.44272682]\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 10 ] state=[ 0.05490276  0.22758731 -0.09337869 -0.44272682], action=1, reward=1.0, next_state=[ 0.05945451  0.42389796 -0.10223322 -0.76332337]\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 11 ] state=[ 0.05945451  0.42389796 -0.10223322 -0.76332337], action=1, reward=1.0, next_state=[ 0.06793247  0.62026823 -0.11749969 -1.08634505]\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 12 ] state=[ 0.06793247  0.62026823 -0.11749969 -1.08634505], action=0, reward=1.0, next_state=[ 0.08033783  0.42687542 -0.13922659 -0.83272153]\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 13 ] state=[ 0.08033783  0.42687542 -0.13922659 -0.83272153], action=0, reward=1.0, next_state=[ 0.08887534  0.23390254 -0.15588102 -0.58686362]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 58 ][ timestamp 14 ] state=[ 0.08887534  0.23390254 -0.15588102 -0.58686362], action=1, reward=1.0, next_state=[ 0.09355339  0.43082455 -0.16761829 -0.92430881]\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 15 ] state=[ 0.09355339  0.43082455 -0.16761829 -0.92430881], action=0, reward=1.0, next_state=[ 0.10216988  0.23831473 -0.18610447 -0.68864048]\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 16 ] state=[ 0.10216988  0.23831473 -0.18610447 -0.68864048], action=0, reward=1.0, next_state=[ 0.10693618  0.04619626 -0.19987728 -0.45983831]\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 17 ] state=[ 0.10693618  0.04619626 -0.19987728 -0.45983831], action=0, reward=1.0, next_state=[ 0.1078601  -0.14562255 -0.20907405 -0.23621191]\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 18 ] state=[ 0.1078601  -0.14562255 -0.20907405 -0.23621191], action=0, reward=-1.0, next_state=[ 0.10494765 -0.33723999 -0.21379828 -0.01606582]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 58: Exploration_rate=0.01. Score=18.\n",
      "[ episode 59 ] state=[ 0.00533227 -0.04442931 -0.02703673 -0.03291908]\n",
      "[ episode 59 ][ timestamp 1 ] state=[ 0.00533227 -0.04442931 -0.02703673 -0.03291908], action=1, reward=1.0, next_state=[ 0.00444369  0.15106971 -0.02769511 -0.33400827]\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 2 ] state=[ 0.00444369  0.15106971 -0.02769511 -0.33400827], action=1, reward=1.0, next_state=[ 0.00746508  0.34657466 -0.03437527 -0.63529456]\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 3 ] state=[ 0.00746508  0.34657466 -0.03437527 -0.63529456], action=0, reward=1.0, next_state=[ 0.01439657  0.15194861 -0.04708117 -0.35363228]\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 4 ] state=[ 0.01439657  0.15194861 -0.04708117 -0.35363228], action=1, reward=1.0, next_state=[ 0.01743555  0.3477073  -0.05415381 -0.66078169]\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 5 ] state=[ 0.01743555  0.3477073  -0.05415381 -0.66078169], action=0, reward=1.0, next_state=[ 0.02438969  0.15337908 -0.06736944 -0.38563012]\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 6 ] state=[ 0.02438969  0.15337908 -0.06736944 -0.38563012], action=1, reward=1.0, next_state=[ 0.02745727  0.34938952 -0.07508205 -0.69877046]\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 7 ] state=[ 0.02745727  0.34938952 -0.07508205 -0.69877046], action=0, reward=1.0, next_state=[ 0.03444506  0.15538446 -0.08905746 -0.43063611]\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 8 ] state=[ 0.03444506  0.15538446 -0.08905746 -0.43063611], action=0, reward=1.0, next_state=[ 0.03755275 -0.03837097 -0.09767018 -0.16730304]\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 9 ] state=[ 0.03755275 -0.03837097 -0.09767018 -0.16730304], action=1, reward=1.0, next_state=[ 0.03678533  0.15800353 -0.10101624 -0.48913032]\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 10 ] state=[ 0.03678533  0.15800353 -0.10101624 -0.48913032], action=0, reward=1.0, next_state=[ 0.0399454  -0.03555904 -0.11079885 -0.22991487]\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 11 ] state=[ 0.0399454  -0.03555904 -0.11079885 -0.22991487], action=0, reward=1.0, next_state=[ 0.03923422 -0.22893765 -0.11539714  0.02586612]\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 12 ] state=[ 0.03923422 -0.22893765 -0.11539714  0.02586612], action=1, reward=1.0, next_state=[ 0.03465547 -0.03236615 -0.11487982 -0.30088159]\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 13 ] state=[ 0.03465547 -0.03236615 -0.11487982 -0.30088159], action=1, reward=1.0, next_state=[ 0.03400815  0.16418979 -0.12089745 -0.62747255]\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 14 ] state=[ 0.03400815  0.16418979 -0.12089745 -0.62747255], action=1, reward=1.0, next_state=[ 0.03729194  0.36077319 -0.1334469  -0.95565263]\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 15 ] state=[ 0.03729194  0.36077319 -0.1334469  -0.95565263], action=0, reward=1.0, next_state=[ 0.04450741  0.167674   -0.15255996 -0.70769611]\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 16 ] state=[ 0.04450741  0.167674   -0.15255996 -0.70769611], action=0, reward=1.0, next_state=[ 0.04786089 -0.0250424  -0.16671388 -0.46665787]\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 17 ] state=[ 0.04786089 -0.0250424  -0.16671388 -0.46665787], action=1, reward=1.0, next_state=[ 0.04736004  0.17199415 -0.17604704 -0.8069021 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 18 ] state=[ 0.04736004  0.17199415 -0.17604704 -0.8069021 ], action=0, reward=1.0, next_state=[ 0.05079992 -0.02033446 -0.19218508 -0.57435911]\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 19 ] state=[ 0.05079992 -0.02033446 -0.19218508 -0.57435911], action=1, reward=1.0, next_state=[ 0.05039323  0.17688839 -0.20367226 -0.92090207]\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 20 ] state=[ 0.05039323  0.17688839 -0.20367226 -0.92090207], action=1, reward=-1.0, next_state=[ 0.053931    0.37409349 -0.2220903  -1.27006201]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 59: Exploration_rate=0.01. Score=20.\n",
      "[ episode 60 ] state=[-0.02636951  0.00219667 -0.00438661  0.02289945]\n",
      "[ episode 60 ][ timestamp 1 ] state=[-0.02636951  0.00219667 -0.00438661  0.02289945], action=1, reward=1.0, next_state=[-0.02632557  0.19738125 -0.00392862 -0.27116427]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 2 ] state=[-0.02632557  0.19738125 -0.00392862 -0.27116427], action=1, reward=1.0, next_state=[-0.02237795  0.39255904 -0.0093519  -0.5650837 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 3 ] state=[-0.02237795  0.39255904 -0.0093519  -0.5650837 ], action=0, reward=1.0, next_state=[-0.01452677  0.19756954 -0.02065358 -0.27536166]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 4 ] state=[-0.01452677  0.19756954 -0.02065358 -0.27536166], action=0, reward=1.0, next_state=[-0.01057538  0.00274825 -0.02616081  0.01073622]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 5 ] state=[-0.01057538  0.00274825 -0.02616081  0.01073622], action=1, reward=1.0, next_state=[-0.01052041  0.19823543 -0.02594609 -0.29008461]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 6 ] state=[-0.01052041  0.19823543 -0.02594609 -0.29008461], action=1, reward=1.0, next_state=[-0.0065557   0.39371756 -0.03174778 -0.59083641]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 7 ] state=[-0.0065557   0.39371756 -0.03174778 -0.59083641], action=0, reward=1.0, next_state=[ 0.00131865  0.19905415 -0.04356451 -0.30832072]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 8 ] state=[ 0.00131865  0.19905415 -0.04356451 -0.30832072], action=0, reward=1.0, next_state=[ 0.00529973  0.00457914 -0.04973092 -0.02968888]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 9 ] state=[ 0.00529973  0.00457914 -0.04973092 -0.02968888], action=0, reward=1.0, next_state=[ 0.00539131 -0.18979567 -0.0503247   0.246898  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 10 ] state=[ 0.00539131 -0.18979567 -0.0503247   0.246898  ], action=0, reward=1.0, next_state=[ 0.0015954  -0.38416411 -0.04538674  0.52329233]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 11 ] state=[ 0.0015954  -0.38416411 -0.04538674  0.52329233], action=1, reward=1.0, next_state=[-0.00608788 -0.18843374 -0.03492089  0.21666001]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 12 ] state=[-0.00608788 -0.18843374 -0.03492089  0.21666001], action=1, reward=1.0, next_state=[-0.00985656  0.00716957 -0.03058769 -0.08683074]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 13 ] state=[-0.00985656  0.00716957 -0.03058769 -0.08683074], action=1, reward=1.0, next_state=[-0.00971317  0.20271631 -0.03232431 -0.38900503]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 14 ] state=[-0.00971317  0.20271631 -0.03232431 -0.38900503], action=1, reward=1.0, next_state=[-0.00565884  0.3982818  -0.04010441 -0.69170171]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 15 ] state=[-0.00565884  0.3982818  -0.04010441 -0.69170171], action=0, reward=1.0, next_state=[ 0.0023068   0.20373856 -0.05393844 -0.41190902]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 16 ] state=[ 0.0023068   0.20373856 -0.05393844 -0.41190902], action=0, reward=1.0, next_state=[ 0.00638157  0.00942107 -0.06217662 -0.1367069 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 60 ][ timestamp 17 ] state=[ 0.00638157  0.00942107 -0.06217662 -0.1367069 ], action=1, reward=1.0, next_state=[ 0.00656999  0.20537593 -0.06491076 -0.44833937]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 18 ] state=[ 0.00656999  0.20537593 -0.06491076 -0.44833937], action=0, reward=1.0, next_state=[ 0.01067751  0.01122934 -0.07387755 -0.17680313]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 19 ] state=[ 0.01067751  0.01122934 -0.07387755 -0.17680313], action=1, reward=1.0, next_state=[ 0.01090209  0.20732653 -0.07741361 -0.49184682]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 20 ] state=[ 0.01090209  0.20732653 -0.07741361 -0.49184682], action=0, reward=1.0, next_state=[ 0.01504862  0.01337699 -0.08725055 -0.2245307 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 21 ] state=[ 0.01504862  0.01337699 -0.08725055 -0.2245307 ], action=0, reward=1.0, next_state=[ 0.01531616 -0.18039662 -0.09174116  0.03940495]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 22 ] state=[ 0.01531616 -0.18039662 -0.09174116  0.03940495], action=1, reward=1.0, next_state=[ 0.01170823  0.01591297 -0.09095306 -0.28075522]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 23 ] state=[ 0.01170823  0.01591297 -0.09095306 -0.28075522], action=0, reward=1.0, next_state=[ 0.01202649 -0.17780182 -0.09656817 -0.01808743]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 24 ] state=[ 0.01202649 -0.17780182 -0.09656817 -0.01808743], action=1, reward=1.0, next_state=[ 0.00847046  0.01856287 -0.09692991 -0.33960907]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 25 ] state=[ 0.00847046  0.01856287 -0.09692991 -0.33960907], action=0, reward=1.0, next_state=[ 0.00884171 -0.17505586 -0.1037221  -0.07899706]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 26 ] state=[ 0.00884171 -0.17505586 -0.1037221  -0.07899706], action=1, reward=1.0, next_state=[ 0.0053406   0.02138826 -0.10530204 -0.40251925]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 27 ] state=[ 0.0053406   0.02138826 -0.10530204 -0.40251925], action=1, reward=1.0, next_state=[ 0.00576836  0.21783384 -0.11335242 -0.72645703]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 28 ] state=[ 0.00576836  0.21783384 -0.11335242 -0.72645703], action=0, reward=1.0, next_state=[ 0.01012504  0.02444644 -0.12788156 -0.47149182]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 29 ] state=[ 0.01012504  0.02444644 -0.12788156 -0.47149182], action=1, reward=1.0, next_state=[ 0.01061397  0.22112075 -0.1373114  -0.80158909]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 30 ] state=[ 0.01061397  0.22112075 -0.1373114  -0.80158909], action=0, reward=1.0, next_state=[ 0.01503638  0.02812217 -0.15334318 -0.55505891]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 31 ] state=[ 0.01503638  0.02812217 -0.15334318 -0.55505891], action=0, reward=1.0, next_state=[ 0.01559882 -0.16455193 -0.16444436 -0.31434546]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 32 ] state=[ 0.01559882 -0.16455193 -0.16444436 -0.31434546], action=1, reward=1.0, next_state=[ 0.01230779  0.03248396 -0.17073127 -0.65404115]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 33 ] state=[ 0.01230779  0.03248396 -0.17073127 -0.65404115], action=0, reward=1.0, next_state=[ 0.01295747 -0.15990131 -0.18381209 -0.41961043]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 34 ] state=[ 0.01295747 -0.15990131 -0.18381209 -0.41961043], action=1, reward=1.0, next_state=[ 0.00975944  0.03728438 -0.1922043  -0.76414326]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 35 ] state=[ 0.00975944  0.03728438 -0.1922043  -0.76414326], action=1, reward=1.0, next_state=[ 0.01050513  0.23446016 -0.20748717 -1.11062138]\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 36 ] state=[ 0.01050513  0.23446016 -0.20748717 -1.11062138], action=0, reward=-1.0, next_state=[ 0.01519433  0.04257713 -0.22969959 -0.88953467]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 60: Exploration_rate=0.01. Score=36.\n",
      "[ episode 61 ] state=[ 0.01692514 -0.01240243  0.03264803  0.00164978]\n",
      "[ episode 61 ][ timestamp 1 ] state=[ 0.01692514 -0.01240243  0.03264803  0.00164978], action=0, reward=1.0, next_state=[ 0.01667709 -0.20797702  0.03268103  0.30445215]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 2 ] state=[ 0.01667709 -0.20797702  0.03268103  0.30445215], action=1, reward=1.0, next_state=[ 0.01251755 -0.01333568  0.03877007  0.02225255]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 3 ] state=[ 0.01251755 -0.01333568  0.03877007  0.02225255], action=1, reward=1.0, next_state=[ 0.01225084  0.18120944  0.03921512 -0.25795029]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 4 ] state=[ 0.01225084  0.18120944  0.03921512 -0.25795029], action=0, reward=1.0, next_state=[ 0.01587503 -0.01444979  0.03405611  0.0468392 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 5 ] state=[ 0.01587503 -0.01444979  0.03405611  0.0468392 ], action=1, reward=1.0, next_state=[ 0.01558603  0.18016769  0.0349929  -0.23490718]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 6 ] state=[ 0.01558603  0.18016769  0.0349929  -0.23490718], action=1, reward=1.0, next_state=[ 0.01918939  0.37477266  0.03029475 -0.51635011]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 7 ] state=[ 0.01918939  0.37477266  0.03029475 -0.51635011], action=0, reward=1.0, next_state=[ 0.02668484  0.17923751  0.01996775 -0.21427668]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 8 ] state=[ 0.02668484  0.17923751  0.01996775 -0.21427668], action=1, reward=1.0, next_state=[ 0.03026959  0.37406838  0.01568222 -0.50059459]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 9 ] state=[ 0.03026959  0.37406838  0.01568222 -0.50059459], action=1, reward=1.0, next_state=[ 0.03775096  0.56896579  0.00567033 -0.78829438]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 10 ] state=[ 0.03775096  0.56896579  0.00567033 -0.78829438], action=0, reward=1.0, next_state=[ 0.04913027  0.37376641 -0.01009556 -0.49383296]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 11 ] state=[ 0.04913027  0.37376641 -0.01009556 -0.49383296], action=0, reward=1.0, next_state=[ 0.0566056   0.17878829 -0.01997222 -0.20434872]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 12 ] state=[ 0.0566056   0.17878829 -0.01997222 -0.20434872], action=1, reward=1.0, next_state=[ 0.06018137  0.37419008 -0.0240592  -0.50326439]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 13 ] state=[ 0.06018137  0.37419008 -0.0240592  -0.50326439], action=1, reward=1.0, next_state=[ 0.06766517  0.56964272 -0.03412448 -0.80343123]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 14 ] state=[ 0.06766517  0.56964272 -0.03412448 -0.80343123], action=0, reward=1.0, next_state=[ 0.07905802  0.37500488 -0.05019311 -0.52167508]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 15 ] state=[ 0.07905802  0.37500488 -0.05019311 -0.52167508], action=0, reward=1.0, next_state=[ 0.08655812  0.18062404 -0.06062661 -0.24522162]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 16 ] state=[ 0.08655812  0.18062404 -0.06062661 -0.24522162], action=0, reward=1.0, next_state=[ 0.0901706  -0.01358195 -0.06553104  0.02773887]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 17 ] state=[ 0.0901706  -0.01358195 -0.06553104  0.02773887], action=0, reward=1.0, next_state=[ 0.08989896 -0.20770592 -0.06497626  0.29904748]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 18 ] state=[ 0.08989896 -0.20770592 -0.06497626  0.29904748], action=1, reward=1.0, next_state=[ 0.08574484 -0.01172086 -0.05899531 -0.01339934]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 19 ] state=[ 0.08574484 -0.01172086 -0.05899531 -0.01339934], action=1, reward=1.0, next_state=[ 0.08551043  0.18419536 -0.0592633  -0.32409698]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 20 ] state=[ 0.08551043  0.18419536 -0.0592633  -0.32409698], action=0, reward=1.0, next_state=[ 0.08919433 -0.01003487 -0.06574524 -0.05067632]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 21 ] state=[ 0.08919433 -0.01003487 -0.06574524 -0.05067632], action=1, reward=1.0, next_state=[ 0.08899364  0.18596519 -0.06675877 -0.36335641]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 61 ][ timestamp 22 ] state=[ 0.08899364  0.18596519 -0.06675877 -0.36335641], action=0, reward=1.0, next_state=[ 0.09271294 -0.00814757 -0.0740259  -0.09244836]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 23 ] state=[ 0.09271294 -0.00814757 -0.0740259  -0.09244836], action=1, reward=1.0, next_state=[ 0.09254999  0.18795304 -0.07587486 -0.40753744]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 24 ] state=[ 0.09254999  0.18795304 -0.07587486 -0.40753744], action=0, reward=1.0, next_state=[ 0.09630905 -0.00601566 -0.08402561 -0.13970731]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 25 ] state=[ 0.09630905 -0.00601566 -0.08402561 -0.13970731], action=1, reward=1.0, next_state=[ 0.09618874  0.19020295 -0.08681976 -0.45767129]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 26 ] state=[ 0.09618874  0.19020295 -0.08681976 -0.45767129], action=0, reward=1.0, next_state=[ 0.0999928  -0.00359123 -0.09597318 -0.19356785]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 27 ] state=[ 0.0999928  -0.00359123 -0.09597318 -0.19356785], action=1, reward=1.0, next_state=[ 0.09992097  0.19276325 -0.09984454 -0.51491699]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 28 ] state=[ 0.09992097  0.19276325 -0.09984454 -0.51491699], action=1, reward=1.0, next_state=[ 0.10377624  0.38913907 -0.11014288 -0.83731924]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 29 ] state=[ 0.10377624  0.38913907 -0.11014288 -0.83731924], action=1, reward=1.0, next_state=[ 0.11155902  0.58557903 -0.12688926 -1.16251023]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 30 ] state=[ 0.11155902  0.58557903 -0.12688926 -1.16251023], action=1, reward=1.0, next_state=[ 0.1232706   0.78210415 -0.15013947 -1.49213334]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 31 ] state=[ 0.1232706   0.78210415 -0.15013947 -1.49213334], action=0, reward=1.0, next_state=[ 0.13891268  0.58909427 -0.17998214 -1.24985085]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 32 ] state=[ 0.13891268  0.58909427 -0.17998214 -1.24985085], action=0, reward=1.0, next_state=[ 0.15069457  0.3966765  -0.20497915 -1.01851594]\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 33 ] state=[ 0.15069457  0.3966765  -0.20497915 -1.01851594], action=0, reward=-1.0, next_state=[ 0.1586281   0.20478831 -0.22534947 -0.7965521 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 61: Exploration_rate=0.01. Score=33.\n",
      "[ episode 62 ] state=[ 0.03334722  0.03549241 -0.04297203  0.04451563]\n",
      "[ episode 62 ][ timestamp 1 ] state=[ 0.03334722  0.03549241 -0.04297203  0.04451563], action=1, reward=1.0, next_state=[ 0.03405707  0.23120337 -0.04208172 -0.2614097 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 2 ] state=[ 0.03405707  0.23120337 -0.04208172 -0.2614097 ], action=0, reward=1.0, next_state=[ 0.03868114  0.0367066  -0.04730991  0.01770879]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 3 ] state=[ 0.03868114  0.0367066  -0.04730991  0.01770879], action=1, reward=1.0, next_state=[ 0.03941527  0.23247398 -0.04695574 -0.28951763]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 4 ] state=[ 0.03941527  0.23247398 -0.04695574 -0.28951763], action=0, reward=1.0, next_state=[ 0.04406475  0.03805196 -0.05274609 -0.01200596]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 5 ] state=[ 0.04406475  0.03805196 -0.05274609 -0.01200596], action=0, reward=1.0, next_state=[ 0.04482579 -0.15627542 -0.05298621  0.26357956]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 6 ] state=[ 0.04482579 -0.15627542 -0.05298621  0.26357956], action=0, reward=1.0, next_state=[ 0.04170028 -0.3506026  -0.04771462  0.53909059]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 7 ] state=[ 0.04170028 -0.3506026  -0.04771462  0.53909059], action=1, reward=1.0, next_state=[ 0.03468823 -0.1548435  -0.0369328   0.23176336]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 8 ] state=[ 0.03468823 -0.1548435  -0.0369328   0.23176336], action=0, reward=1.0, next_state=[ 0.03159136 -0.34941878 -0.03229754  0.51257148]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 9 ] state=[ 0.03159136 -0.34941878 -0.03229754  0.51257148], action=1, reward=1.0, next_state=[ 0.02460298 -0.15385716 -0.02204611  0.2098882 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 10 ] state=[ 0.02460298 -0.15385716 -0.02204611  0.2098882 ], action=0, reward=1.0, next_state=[ 0.02152584 -0.34865706 -0.01784834  0.49553601]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 11 ] state=[ 0.02152584 -0.34865706 -0.01784834  0.49553601], action=0, reward=1.0, next_state=[ 0.0145527  -0.54352283 -0.00793762  0.78254098]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 12 ] state=[ 0.0145527  -0.54352283 -0.00793762  0.78254098], action=1, reward=1.0, next_state=[ 0.00368224 -0.34829269  0.0077132   0.48737135]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 13 ] state=[ 0.00368224 -0.34829269  0.0077132   0.48737135], action=1, reward=1.0, next_state=[-0.00328361 -0.15328041  0.01746062  0.1971293 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 14 ] state=[-0.00328361 -0.15328041  0.01746062  0.1971293 ], action=0, reward=1.0, next_state=[-0.00634922 -0.3486477   0.02140321  0.49526873]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 15 ] state=[-0.00634922 -0.3486477   0.02140321  0.49526873], action=0, reward=1.0, next_state=[-0.01332217 -0.54406484  0.03130858  0.79461936]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 16 ] state=[-0.01332217 -0.54406484  0.03130858  0.79461936], action=1, reward=1.0, next_state=[-0.02420347 -0.34938628  0.04720097  0.51194785]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 17 ] state=[-0.02420347 -0.34938628  0.04720097  0.51194785], action=1, reward=1.0, next_state=[-0.0311912  -0.15495987  0.05743993  0.23450498]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 18 ] state=[-0.0311912  -0.15495987  0.05743993  0.23450498], action=1, reward=1.0, next_state=[-0.03429039  0.03929636  0.06213003 -0.03952075]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 19 ] state=[-0.03429039  0.03929636  0.06213003 -0.03952075], action=1, reward=1.0, next_state=[-0.03350447  0.23347488  0.06133961 -0.31197207]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 20 ] state=[-0.03350447  0.23347488  0.06133961 -0.31197207], action=0, reward=1.0, next_state=[-0.02883497  0.03753514  0.05510017 -0.00059266]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 21 ] state=[-0.02883497  0.03753514  0.05510017 -0.00059266], action=1, reward=1.0, next_state=[-0.02808427  0.23182535  0.05508832 -0.27539444]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 22 ] state=[-0.02808427  0.23182535  0.05508832 -0.27539444], action=0, reward=1.0, next_state=[-0.02344776  0.03596247  0.04958043  0.03414197]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 23 ] state=[-0.02344776  0.03596247  0.04958043  0.03414197], action=0, reward=1.0, next_state=[-0.02272851 -0.15983413  0.05026327  0.34204664]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 24 ] state=[-0.02272851 -0.15983413  0.05026327  0.34204664], action=0, reward=1.0, next_state=[-0.02592519 -0.35563381  0.0571042   0.65014642]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 25 ] state=[-0.02592519 -0.35563381  0.0571042   0.65014642], action=1, reward=1.0, next_state=[-0.03303787 -0.16135181  0.07010713  0.37597795]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 26 ] state=[-0.03303787 -0.16135181  0.07010713  0.37597795], action=0, reward=1.0, next_state=[-0.0362649  -0.35739586  0.07762669  0.68991627]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 27 ] state=[-0.0362649  -0.35739586  0.07762669  0.68991627], action=1, reward=1.0, next_state=[-0.04341282 -0.16343206  0.09142501  0.42264607]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 28 ] state=[-0.04341282 -0.16343206  0.09142501  0.42264607], action=1, reward=1.0, next_state=[-0.04668146  0.03028378  0.09987794  0.16012737]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 29 ] state=[-0.04668146  0.03028378  0.09987794  0.16012737], action=1, reward=1.0, next_state=[-0.04607579  0.22384452  0.10308048 -0.09945146]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 30 ] state=[-0.04607579  0.22384452  0.10308048 -0.09945146], action=0, reward=1.0, next_state=[-0.0415989   0.02740787  0.10109145  0.22389147]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 31 ] state=[-0.0415989   0.02740787  0.10109145  0.22389147], action=1, reward=1.0, next_state=[-0.04105074  0.22095059  0.10556928 -0.03527014]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 32 ] state=[-0.04105074  0.22095059  0.10556928 -0.03527014], action=0, reward=1.0, next_state=[-0.03663173  0.02448559  0.10486388  0.28876645]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 62 ][ timestamp 33 ] state=[-0.03663173  0.02448559  0.10486388  0.28876645], action=0, reward=1.0, next_state=[-0.03614202 -0.17196326  0.11063921  0.61259454]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 34 ] state=[-0.03614202 -0.17196326  0.11063921  0.61259454], action=1, reward=1.0, next_state=[-0.03958128  0.02145277  0.1228911   0.356706  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 35 ] state=[-0.03958128  0.02145277  0.1228911   0.356706  ], action=0, reward=1.0, next_state=[-0.03915223 -0.17518251  0.13002522  0.68547362]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 36 ] state=[-0.03915223 -0.17518251  0.13002522  0.68547362], action=1, reward=1.0, next_state=[-0.04265588  0.01791753  0.14373469  0.43638838]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 37 ] state=[-0.04265588  0.01791753  0.14373469  0.43638838], action=1, reward=1.0, next_state=[-0.04229753  0.21074352  0.15246246  0.1922447 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 38 ] state=[-0.04229753  0.21074352  0.15246246  0.1922447 ], action=0, reward=1.0, next_state=[-0.03808266  0.0138066   0.15630735  0.5288739 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 39 ] state=[-0.03808266  0.0138066   0.15630735  0.5288739 ], action=1, reward=1.0, next_state=[-0.03780652  0.20642415  0.16688483  0.2892374 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 40 ] state=[-0.03780652  0.20642415  0.16688483  0.2892374 ], action=1, reward=1.0, next_state=[-0.03367804  0.39882202  0.17266958  0.05348677]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 41 ] state=[-0.03367804  0.39882202  0.17266958  0.05348677], action=0, reward=1.0, next_state=[-0.0257016   0.20169893  0.17373932  0.39528744]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 42 ] state=[-0.0257016   0.20169893  0.17373932  0.39528744], action=1, reward=1.0, next_state=[-0.02166762  0.39398495  0.18164506  0.1620234 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 43 ] state=[-0.02166762  0.39398495  0.18164506  0.1620234 ], action=0, reward=1.0, next_state=[-0.01378792  0.19679036  0.18488553  0.50605933]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 44 ] state=[-0.01378792  0.19679036  0.18488553  0.50605933], action=1, reward=1.0, next_state=[-0.00985211  0.3888917   0.19500672  0.27686542]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 45 ] state=[-0.00985211  0.3888917   0.19500672  0.27686542], action=1, reward=1.0, next_state=[-0.00207428  0.58077464  0.20054403  0.05146562]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 46 ] state=[-0.00207428  0.58077464  0.20054403  0.05146562], action=1, reward=1.0, next_state=[ 0.00954121  0.77254071  0.20157334 -0.17185299]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 47 ] state=[ 0.00954121  0.77254071  0.20157334 -0.17185299], action=1, reward=1.0, next_state=[ 0.02499203  0.96429283  0.19813628 -0.39479546]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 48 ] state=[ 0.02499203  0.96429283  0.19813628 -0.39479546], action=0, reward=1.0, next_state=[ 0.04427788  0.76699241  0.19024037 -0.04676339]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 49 ] state=[ 0.04427788  0.76699241  0.19024037 -0.04676339], action=1, reward=1.0, next_state=[ 0.05961773  0.95894957  0.1893051  -0.2739105 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 50 ] state=[ 0.05961773  0.95894957  0.1893051  -0.2739105 ], action=0, reward=1.0, next_state=[0.07879672 0.76170194 0.18382689 0.07199913]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 51 ] state=[0.07879672 0.76170194 0.18382689 0.07199913], action=1, reward=1.0, next_state=[ 0.09403076  0.95377786  0.18526688 -0.15751918]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 52 ] state=[ 0.09403076  0.95377786  0.18526688 -0.15751918], action=0, reward=1.0, next_state=[0.11310632 0.75655344 0.18211649 0.18741223]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 53 ] state=[0.11310632 0.75655344 0.18211649 0.18741223], action=0, reward=1.0, next_state=[0.12823739 0.55935654 0.18586474 0.53156267]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 54 ] state=[0.12823739 0.55935654 0.18586474 0.53156267], action=0, reward=1.0, next_state=[0.13942452 0.36217364 0.19649599 0.876573  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 55 ] state=[0.13942452 0.36217364 0.19649599 0.876573  ], action=0, reward=-1.0, next_state=[0.14666799 0.16500189 0.21402745 1.22403805]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 62: Exploration_rate=0.01. Score=55.\n",
      "[ episode 63 ] state=[ 0.04985646  0.02878747  0.02877438 -0.00208835]\n",
      "[ episode 63 ][ timestamp 1 ] state=[ 0.04985646  0.02878747  0.02877438 -0.00208835], action=0, reward=1.0, next_state=[ 0.05043221 -0.16673508  0.02873261  0.29953257]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 2 ] state=[ 0.05043221 -0.16673508  0.02873261  0.29953257], action=0, reward=1.0, next_state=[ 0.04709751 -0.36225455  0.03472326  0.60113694]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 3 ] state=[ 0.04709751 -0.36225455  0.03472326  0.60113694], action=1, reward=1.0, next_state=[ 0.03985242 -0.1676351   0.046746    0.31959034]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 4 ] state=[ 0.03985242 -0.1676351   0.046746    0.31959034], action=1, reward=1.0, next_state=[0.03649972 0.02679101 0.05313781 0.04200807]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 5 ] state=[0.03649972 0.02679101 0.05313781 0.04200807], action=0, reward=1.0, next_state=[ 0.03703554 -0.16905106  0.05397797  0.35097171]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 6 ] state=[ 0.03703554 -0.16905106  0.05397797  0.35097171], action=0, reward=1.0, next_state=[ 0.03365452 -0.36489744  0.0609974   0.66017522]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 7 ] state=[ 0.03365452 -0.36489744  0.0609974   0.66017522], action=1, reward=1.0, next_state=[ 0.02635657 -0.170675    0.07420091  0.3873055 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 8 ] state=[ 0.02635657 -0.170675    0.07420091  0.3873055 ], action=0, reward=1.0, next_state=[ 0.02294307 -0.3667675   0.08194701  0.70242994]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 9 ] state=[ 0.02294307 -0.3667675   0.08194701  0.70242994], action=1, reward=1.0, next_state=[ 0.01560772 -0.17287119  0.09599561  0.43662695]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 10 ] state=[ 0.01560772 -0.17287119  0.09599561  0.43662695], action=0, reward=1.0, next_state=[ 0.0121503  -0.36921161  0.10472815  0.75796102]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 11 ] state=[ 0.0121503  -0.36921161  0.10472815  0.75796102], action=1, reward=1.0, next_state=[ 0.00476606 -0.17567681  0.11988737  0.49998321]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 12 ] state=[ 0.00476606 -0.17567681  0.11988737  0.49998321], action=1, reward=1.0, next_state=[0.00125253 0.01756911 0.12988704 0.24735749]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 13 ] state=[0.00125253 0.01756911 0.12988704 0.24735749], action=1, reward=1.0, next_state=[ 0.00160391  0.21061993  0.13483419 -0.0017    ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 14 ] state=[ 0.00160391  0.21061993  0.13483419 -0.0017    ], action=0, reward=1.0, next_state=[0.00581631 0.01384779 0.13480019 0.33030049]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 15 ] state=[0.00581631 0.01384779 0.13480019 0.33030049], action=1, reward=1.0, next_state=[0.00609326 0.20681913 0.1414062  0.08298072]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 16 ] state=[0.00609326 0.20681913 0.1414062  0.08298072], action=0, reward=1.0, next_state=[0.01022965 0.00998315 0.14306581 0.41672271]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 17 ] state=[0.01022965 0.00998315 0.14306581 0.41672271], action=1, reward=1.0, next_state=[0.01042931 0.20281854 0.15140027 0.17234279]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 18 ] state=[0.01042931 0.20281854 0.15140027 0.17234279], action=1, reward=1.0, next_state=[ 0.01448568  0.3954859   0.15484712 -0.06901051]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 19 ] state=[ 0.01448568  0.3954859   0.15484712 -0.06901051], action=1, reward=1.0, next_state=[ 0.0223954   0.58808794  0.15346691 -0.30911354]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 20 ] state=[ 0.0223954   0.58808794  0.15346691 -0.30911354], action=1, reward=1.0, next_state=[ 0.03415716  0.78072822  0.14728464 -0.54973544]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 21 ] state=[ 0.03415716  0.78072822  0.14728464 -0.54973544], action=1, reward=1.0, next_state=[ 0.04977172  0.97350778  0.13628993 -0.79262872]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 22 ] state=[ 0.04977172  0.97350778  0.13628993 -0.79262872], action=0, reward=1.0, next_state=[ 0.06924188  0.77680433  0.12043736 -0.4603643 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 23 ] state=[ 0.06924188  0.77680433  0.12043736 -0.4603643 ], action=1, reward=1.0, next_state=[ 0.08477796  0.97003643  0.11123007 -0.71278981]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 63 ][ timestamp 24 ] state=[ 0.08477796  0.97003643  0.11123007 -0.71278981], action=1, reward=1.0, next_state=[ 0.10417869  1.16345694  0.09697427 -0.96849341]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 25 ] state=[ 0.10417869  1.16345694  0.09697427 -0.96849341], action=1, reward=1.0, next_state=[ 0.12744783  1.35715258  0.07760441 -1.22920603]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 26 ] state=[ 0.12744783  1.35715258  0.07760441 -1.22920603], action=1, reward=1.0, next_state=[ 0.15459088  1.55119498  0.05302029 -1.49660081]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 27 ] state=[ 0.15459088  1.55119498  0.05302029 -1.49660081], action=1, reward=1.0, next_state=[ 0.18561478  1.74563382  0.02308827 -1.77226855]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 28 ] state=[ 0.18561478  1.74563382  0.02308827 -1.77226855], action=0, reward=1.0, next_state=[ 0.22052746  1.55025923 -0.0123571  -1.47249742]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 29 ] state=[ 0.22052746  1.55025923 -0.0123571  -1.47249742], action=0, reward=1.0, next_state=[ 0.25153264  1.35529052 -0.04180705 -1.18369958]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 30 ] state=[ 0.25153264  1.35529052 -0.04180705 -1.18369958], action=0, reward=1.0, next_state=[ 0.27863845  1.16073517 -0.06548104 -0.90440926]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 31 ] state=[ 0.27863845  1.16073517 -0.06548104 -0.90440926], action=1, reward=1.0, next_state=[ 0.30185316  1.35667993 -0.08356923 -1.21693417]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 32 ] state=[ 0.30185316  1.35667993 -0.08356923 -1.21693417], action=1, reward=1.0, next_state=[ 0.32898676  1.55277424 -0.10790791 -1.53458988]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 33 ] state=[ 0.32898676  1.55277424 -0.10790791 -1.53458988], action=0, reward=1.0, next_state=[ 0.36004224  1.35910479 -0.13859971 -1.27743879]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 34 ] state=[ 0.36004224  1.35910479 -0.13859971 -1.27743879], action=0, reward=1.0, next_state=[ 0.38722434  1.16599495 -0.16414848 -1.03116978]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 35 ] state=[ 0.38722434  1.16599495 -0.16414848 -1.03116978], action=0, reward=1.0, next_state=[ 0.41054424  0.97339188 -0.18477188 -0.79419189]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 36 ] state=[ 0.41054424  0.97339188 -0.18477188 -0.79419189], action=1, reward=1.0, next_state=[ 0.43001207  1.17050395 -0.20065572 -1.13884154]\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 37 ] state=[ 0.43001207  1.17050395 -0.20065572 -1.13884154], action=0, reward=-1.0, next_state=[ 0.45342215  0.97848901 -0.22343255 -0.91519569]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 63: Exploration_rate=0.01. Score=37.\n",
      "[ episode 64 ] state=[ 0.04899117  0.04704899 -0.03408528  0.03670891]\n",
      "[ episode 64 ][ timestamp 1 ] state=[ 0.04899117  0.04704899 -0.03408528  0.03670891], action=0, reward=1.0, next_state=[ 0.04993215 -0.14756802 -0.0333511   0.31844572]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 2 ] state=[ 0.04993215 -0.14756802 -0.0333511   0.31844572], action=0, reward=1.0, next_state=[ 0.04698079 -0.34219948 -0.02698219  0.60042716]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 3 ] state=[ 0.04698079 -0.34219948 -0.02698219  0.60042716], action=0, reward=1.0, next_state=[ 0.0401368  -0.53693378 -0.01497364  0.88449047]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 4 ] state=[ 0.0401368  -0.53693378 -0.01497364  0.88449047], action=1, reward=1.0, next_state=[ 0.02939813 -0.34161175  0.00271617  0.58713819]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 5 ] state=[ 0.02939813 -0.34161175  0.00271617  0.58713819], action=0, reward=1.0, next_state=[ 0.02256589 -0.53677163  0.01445893  0.88067549]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 6 ] state=[ 0.02256589 -0.53677163  0.01445893  0.88067549], action=1, reward=1.0, next_state=[ 0.01183046 -0.34184905  0.03207244  0.59257296]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 7 ] state=[ 0.01183046 -0.34184905  0.03207244  0.59257296], action=0, reward=1.0, next_state=[ 0.00499348 -0.53740496  0.0439239   0.89518364]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 8 ] state=[ 0.00499348 -0.53740496  0.0439239   0.89518364], action=1, reward=1.0, next_state=[-0.00575462 -0.34290524  0.06182757  0.61662493]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 9 ] state=[-0.00575462 -0.34290524  0.06182757  0.61662493], action=1, reward=1.0, next_state=[-0.01261273 -0.14869906  0.07416007  0.344038  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 10 ] state=[-0.01261273 -0.14869906  0.07416007  0.344038  ], action=0, reward=1.0, next_state=[-0.01558671 -0.34479336  0.08104083  0.65915405]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 11 ] state=[-0.01558671 -0.34479336  0.08104083  0.65915405], action=1, reward=1.0, next_state=[-0.02248257 -0.15088723  0.09422391  0.3930494 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 12 ] state=[-0.02248257 -0.15088723  0.09422391  0.3930494 ], action=1, reward=1.0, next_state=[-0.02550032  0.04278014  0.1020849   0.1314978 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 13 ] state=[-0.02550032  0.04278014  0.1020849   0.1314978 ], action=1, reward=1.0, next_state=[-0.02464472  0.23630286  0.10471486 -0.12731416]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 14 ] state=[-0.02464472  0.23630286  0.10471486 -0.12731416], action=1, reward=1.0, next_state=[-0.01991866  0.42978098  0.10216857 -0.38521172]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 15 ] state=[-0.01991866  0.42978098  0.10216857 -0.38521172], action=0, reward=1.0, next_state=[-0.01132304  0.23336822  0.09446434 -0.06214359]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 16 ] state=[-0.01132304  0.23336822  0.09446434 -0.06214359], action=1, reward=1.0, next_state=[-0.00665568  0.42701775  0.09322147 -0.32359159]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 17 ] state=[-0.00665568  0.42701775  0.09322147 -0.32359159], action=0, reward=1.0, next_state=[ 0.00188468  0.23070058  0.08674963 -0.00302701]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 18 ] state=[ 0.00188468  0.23070058  0.08674963 -0.00302701], action=1, reward=1.0, next_state=[ 0.00649869  0.42447821  0.08668909 -0.26712802]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 19 ] state=[ 0.00649869  0.42447821  0.08668909 -0.26712802], action=0, reward=1.0, next_state=[0.01498826 0.2282329  0.08134653 0.05158923]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 20 ] state=[0.01498826 0.2282329  0.08134653 0.05158923], action=0, reward=1.0, next_state=[0.01955291 0.03204445 0.08237832 0.36878828]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 21 ] state=[0.01955291 0.03204445 0.08237832 0.36878828], action=0, reward=1.0, next_state=[ 0.0201938  -0.16414547  0.08975408  0.68626703]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 22 ] state=[ 0.0201938  -0.16414547  0.08975408  0.68626703], action=1, reward=1.0, next_state=[0.01691089 0.02962345 0.10347942 0.42313587]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 23 ] state=[0.01691089 0.02962345 0.10347942 0.42313587], action=1, reward=1.0, next_state=[0.01750336 0.22313889 0.11194214 0.16478414]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 24 ] state=[0.01750336 0.22313889 0.11194214 0.16478414], action=1, reward=1.0, next_state=[ 0.02196614  0.41649522  0.11523782 -0.09059273]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 25 ] state=[ 0.02196614  0.41649522  0.11523782 -0.09059273], action=0, reward=1.0, next_state=[0.03029604 0.21992626 0.11342597 0.23611006]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 26 ] state=[0.03029604 0.21992626 0.11342597 0.23611006], action=1, reward=1.0, next_state=[ 0.03469457  0.41326037  0.11814817 -0.01875183]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 27 ] state=[ 0.03469457  0.41326037  0.11814817 -0.01875183], action=1, reward=1.0, next_state=[ 0.04295978  0.6065072   0.11777313 -0.27194647]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 28 ] state=[ 0.04295978  0.6065072   0.11777313 -0.27194647], action=1, reward=1.0, next_state=[ 0.05508992  0.79976893  0.11233421 -0.5252856 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 29 ] state=[ 0.05508992  0.79976893  0.11233421 -0.5252856 ], action=0, reward=1.0, next_state=[ 0.0710853   0.60326032  0.10182849 -0.19942369]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 30 ] state=[ 0.0710853   0.60326032  0.10182849 -0.19942369], action=0, reward=1.0, next_state=[0.08315051 0.40684053 0.09784002 0.12356567]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 31 ] state=[0.08315051 0.40684053 0.09784002 0.12356567], action=0, reward=1.0, next_state=[0.09128732 0.21046291 0.10031133 0.44544242]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 32 ] state=[0.09128732 0.21046291 0.10031133 0.44544242], action=1, reward=1.0, next_state=[0.09549657 0.40403322 0.10922018 0.18598865]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 33 ] state=[0.09549657 0.40403322 0.10922018 0.18598865], action=0, reward=1.0, next_state=[0.10357724 0.20753184 0.11293995 0.51103135]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 64 ][ timestamp 34 ] state=[0.10357724 0.20753184 0.11293995 0.51103135], action=1, reward=1.0, next_state=[0.10772788 0.40089681 0.12316058 0.25596557]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 35 ] state=[0.10772788 0.40089681 0.12316058 0.25596557], action=0, reward=1.0, next_state=[0.11574581 0.20425137 0.12827989 0.58481718]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 36 ] state=[0.11574581 0.20425137 0.12827989 0.58481718], action=1, reward=1.0, next_state=[0.11983084 0.39736515 0.13997624 0.33513755]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 37 ] state=[0.11983084 0.39736515 0.13997624 0.33513755], action=1, reward=1.0, next_state=[0.12777814 0.59024632 0.14667899 0.08966432]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 38 ] state=[0.12777814 0.59024632 0.14667899 0.08966432], action=1, reward=1.0, next_state=[ 0.13958307  0.78299461  0.14847227 -0.15338433]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 39 ] state=[ 0.13958307  0.78299461  0.14847227 -0.15338433], action=0, reward=1.0, next_state=[0.15524296 0.58609317 0.14540459 0.18220909]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 40 ] state=[0.15524296 0.58609317 0.14540459 0.18220909], action=1, reward=1.0, next_state=[ 0.16696482  0.77886764  0.14904877 -0.06130273]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 41 ] state=[ 0.16696482  0.77886764  0.14904877 -0.06130273], action=0, reward=1.0, next_state=[0.18254218 0.58195789 0.14782271 0.27444539]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 42 ] state=[0.18254218 0.58195789 0.14782271 0.27444539], action=0, reward=1.0, next_state=[0.19418133 0.38507003 0.15331162 0.60985809]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 43 ] state=[0.19418133 0.38507003 0.15331162 0.60985809], action=1, reward=1.0, next_state=[0.20188274 0.57775418 0.16550878 0.36911918]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 44 ] state=[0.20188274 0.57775418 0.16550878 0.36911918], action=1, reward=1.0, next_state=[0.21343782 0.7701853  0.17289117 0.13285467]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 45 ] state=[0.21343782 0.7701853  0.17289117 0.13285467], action=0, reward=1.0, next_state=[0.22884153 0.57306278 0.17554826 0.47470741]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 46 ] state=[0.22884153 0.57306278 0.17554826 0.47470741], action=1, reward=1.0, next_state=[0.24030278 0.76532781 0.18504241 0.24208878]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 47 ] state=[0.24030278 0.76532781 0.18504241 0.24208878], action=0, reward=1.0, next_state=[0.25560934 0.56811126 0.18988419 0.58695595]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 48 ] state=[0.25560934 0.56811126 0.18988419 0.58695595], action=0, reward=1.0, next_state=[0.26697156 0.37090857 0.2016233  0.93293432]\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 49 ] state=[0.26697156 0.37090857 0.2016233  0.93293432], action=1, reward=-1.0, next_state=[0.27438973 0.56282356 0.22028199 0.70976977]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 64: Exploration_rate=0.01. Score=49.\n",
      "[ episode 65 ] state=[-0.01780628  0.02553079 -0.0044031   0.00808813]\n",
      "[ episode 65 ][ timestamp 1 ] state=[-0.01780628  0.02553079 -0.0044031   0.00808813], action=0, reward=1.0, next_state=[-0.01729566 -0.16952773 -0.00424134  0.29937858]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 2 ] state=[-0.01729566 -0.16952773 -0.00424134  0.29937858], action=0, reward=1.0, next_state=[-0.02068622 -0.36458897  0.00174623  0.59072086]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 3 ] state=[-0.02068622 -0.36458897  0.00174623  0.59072086], action=1, reward=1.0, next_state=[-0.027978   -0.16949152  0.01356065  0.29858851]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 4 ] state=[-0.027978   -0.16949152  0.01356065  0.29858851], action=0, reward=1.0, next_state=[-0.03136783 -0.36480411  0.01953242  0.59551718]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 5 ] state=[-0.03136783 -0.36480411  0.01953242  0.59551718], action=1, reward=1.0, next_state=[-0.03866391 -0.1699609   0.03144276  0.30905027]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 6 ] state=[-0.03866391 -0.1699609   0.03144276  0.30905027], action=1, reward=1.0, next_state=[-0.04206313  0.02469927  0.03762377  0.02644699]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 7 ] state=[-0.04206313  0.02469927  0.03762377  0.02644699], action=0, reward=1.0, next_state=[-0.04156914 -0.17094147  0.03815271  0.3307592 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 8 ] state=[-0.04156914 -0.17094147  0.03815271  0.3307592 ], action=1, reward=1.0, next_state=[-0.04498797  0.0236172   0.04476789  0.05034774]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 9 ] state=[-0.04498797  0.0236172   0.04476789  0.05034774], action=1, reward=1.0, next_state=[-0.04451563  0.21806959  0.04577485 -0.22788123]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 10 ] state=[-0.04451563  0.21806959  0.04577485 -0.22788123], action=0, reward=1.0, next_state=[-0.04015424  0.02232438  0.04121722  0.07888212]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 11 ] state=[-0.04015424  0.02232438  0.04121722  0.07888212], action=1, reward=1.0, next_state=[-0.03970775  0.21683197  0.04279486 -0.20051704]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 12 ] state=[-0.03970775  0.21683197  0.04279486 -0.20051704], action=0, reward=1.0, next_state=[-0.03537111  0.02112492  0.03878452  0.10535262]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 13 ] state=[-0.03537111  0.02112492  0.03878452  0.10535262], action=0, reward=1.0, next_state=[-0.03494861 -0.17453076  0.04089157  0.41001522]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 14 ] state=[-0.03494861 -0.17453076  0.04089157  0.41001522], action=1, reward=1.0, next_state=[-0.03843923  0.01998832  0.04909188  0.13049928]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 15 ] state=[-0.03843923  0.01998832  0.04909188  0.13049928], action=1, reward=1.0, next_state=[-0.03803946  0.21437392  0.05170186 -0.14630061]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 16 ] state=[-0.03803946  0.21437392  0.05170186 -0.14630061], action=0, reward=1.0, next_state=[-0.03375198  0.01855115  0.04877585  0.16223461]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 17 ] state=[-0.03375198  0.01855115  0.04877585  0.16223461], action=1, reward=1.0, next_state=[-0.03338096  0.21294214  0.05202054 -0.11467066]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 18 ] state=[-0.03338096  0.21294214  0.05202054 -0.11467066], action=1, reward=1.0, next_state=[-0.02912212  0.4072816   0.04972713 -0.39049838]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 19 ] state=[-0.02912212  0.4072816   0.04972713 -0.39049838], action=1, reward=1.0, next_state=[-0.02097648  0.60166382  0.04191716 -0.66709753]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 20 ] state=[-0.02097648  0.60166382  0.04191716 -0.66709753], action=1, reward=1.0, next_state=[-0.00894321  0.79617852  0.02857521 -0.94629325]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 21 ] state=[-0.00894321  0.79617852  0.02857521 -0.94629325], action=0, reward=1.0, next_state=[ 0.00698036  0.60068362  0.00964935 -0.64477065]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 22 ] state=[ 0.00698036  0.60068362  0.00964935 -0.64477065], action=1, reward=1.0, next_state=[ 0.01899404  0.79566978 -0.00324606 -0.9343994 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 23 ] state=[ 0.01899404  0.79566978 -0.00324606 -0.9343994 ], action=0, reward=1.0, next_state=[ 0.03490743  0.60059177 -0.02193405 -0.64273827]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 24 ] state=[ 0.03490743  0.60059177 -0.02193405 -0.64273827], action=0, reward=1.0, next_state=[ 0.04691927  0.4057823  -0.03478882 -0.35704245]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 25 ] state=[ 0.04691927  0.4057823  -0.03478882 -0.35704245], action=0, reward=1.0, next_state=[ 0.05503491  0.21117177 -0.04192967 -0.07552913]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 26 ] state=[ 0.05503491  0.21117177 -0.04192967 -0.07552913], action=1, reward=1.0, next_state=[ 0.05925835  0.40686896 -0.04344025 -0.38114062]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 27 ] state=[ 0.05925835  0.40686896 -0.04344025 -0.38114062], action=0, reward=1.0, next_state=[ 0.06739573  0.2123899  -0.05106306 -0.10246465]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 28 ] state=[ 0.06739573  0.2123899  -0.05106306 -0.10246465], action=0, reward=1.0, next_state=[ 0.07164353  0.01803552 -0.05311236  0.1736809 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 29 ] state=[ 0.07164353  0.01803552 -0.05311236  0.1736809 ], action=1, reward=1.0, next_state=[ 0.07200424  0.2138758  -0.04963874 -0.13527297]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 30 ] state=[ 0.07200424  0.2138758  -0.04963874 -0.13527297], action=0, reward=1.0, next_state=[ 0.07628175  0.0194987  -0.0523442   0.14134575]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 65 ][ timestamp 31 ] state=[ 0.07628175  0.0194987  -0.0523442   0.14134575], action=1, reward=1.0, next_state=[ 0.07667173  0.21532973 -0.04951728 -0.16738064]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 32 ] state=[ 0.07667173  0.21532973 -0.04951728 -0.16738064], action=0, reward=1.0, next_state=[ 0.08097832  0.02095026 -0.05286489  0.10927905]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 33 ] state=[ 0.08097832  0.02095026 -0.05286489  0.10927905], action=0, reward=1.0, next_state=[ 0.08139733 -0.17337585 -0.05067931  0.38482597]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 34 ] state=[ 0.08139733 -0.17337585 -0.05067931  0.38482597], action=1, reward=1.0, next_state=[ 0.07792981  0.02242758 -0.04298279  0.07660456]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 35 ] state=[ 0.07792981  0.02242758 -0.04298279  0.07660456], action=1, reward=1.0, next_state=[ 0.07837836  0.21813853 -0.0414507  -0.22932376]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 36 ] state=[ 0.07837836  0.21813853 -0.0414507  -0.22932376], action=0, reward=1.0, next_state=[ 0.08274113  0.02363267 -0.04603718  0.0500014 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 37 ] state=[ 0.08274113  0.02363267 -0.04603718  0.0500014 ], action=1, reward=1.0, next_state=[ 0.08321378  0.21938348 -0.04503715 -0.25684386]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 38 ] state=[ 0.08321378  0.21938348 -0.04503715 -0.25684386], action=0, reward=1.0, next_state=[ 0.08760145  0.0249325  -0.05017403  0.02130041]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 39 ] state=[ 0.08760145  0.0249325  -0.05017403  0.02130041], action=0, reward=1.0, next_state=[ 0.0881001  -0.16943534 -0.04974802  0.29774029]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 40 ] state=[ 0.0881001  -0.16943534 -0.04974802  0.29774029], action=1, reward=1.0, next_state=[ 0.0847114   0.02635917 -0.04379321 -0.01020801]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 41 ] state=[ 0.0847114   0.02635917 -0.04379321 -0.01020801], action=0, reward=1.0, next_state=[ 0.08523858 -0.16810826 -0.04399737  0.26834237]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 42 ] state=[ 0.08523858 -0.16810826 -0.04399737  0.26834237], action=1, reward=1.0, next_state=[ 0.08187642  0.02761307 -0.03863053 -0.03788658]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 43 ] state=[ 0.08187642  0.02761307 -0.03863053 -0.03788658], action=0, reward=1.0, next_state=[ 0.08242868 -0.16693424 -0.03938826  0.24236211]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 44 ] state=[ 0.08242868 -0.16693424 -0.03938826  0.24236211], action=1, reward=1.0, next_state=[ 0.07908999  0.02872756 -0.03454102 -0.0624801 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 45 ] state=[ 0.07908999  0.02872756 -0.03454102 -0.0624801 ], action=0, reward=1.0, next_state=[ 0.07966454 -0.16588256 -0.03579062  0.21910793]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 46 ] state=[ 0.07966454 -0.16588256 -0.03579062  0.21910793], action=1, reward=1.0, next_state=[ 0.07634689  0.02973223 -0.03140846 -0.08464655]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 47 ] state=[ 0.07634689  0.02973223 -0.03140846 -0.08464655], action=0, reward=1.0, next_state=[ 0.07694154 -0.16492574 -0.03310139  0.19796384]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 48 ] state=[ 0.07694154 -0.16492574 -0.03310139  0.19796384], action=1, reward=1.0, next_state=[ 0.07364302  0.03065364 -0.02914211 -0.10497457]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 49 ] state=[ 0.07364302  0.03065364 -0.02914211 -0.10497457], action=0, reward=1.0, next_state=[ 0.0742561  -0.16403882 -0.0312416   0.17837356]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 50 ] state=[ 0.0742561  -0.16403882 -0.0312416   0.17837356], action=0, reward=1.0, next_state=[ 0.07097532 -0.35870009 -0.02767413  0.46103943]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 51 ] state=[ 0.07097532 -0.35870009 -0.02767413  0.46103943], action=0, reward=1.0, next_state=[ 0.06380132 -0.55342018 -0.01845334  0.74487258]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 52 ] state=[ 0.06380132 -0.55342018 -0.01845334  0.74487258], action=0, reward=1.0, next_state=[ 0.05273291 -0.74828268 -0.00355589  1.03169159]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 53 ] state=[ 0.05273291 -0.74828268 -0.00355589  1.03169159], action=1, reward=1.0, next_state=[ 0.03776726 -0.5531136   0.01707794  0.7378944 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 54 ] state=[ 0.03776726 -0.5531136   0.01707794  0.7378944 ], action=0, reward=1.0, next_state=[ 0.02670499 -0.74846719  0.03183583  1.03590271]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 55 ] state=[ 0.02670499 -0.74846719  0.03183583  1.03590271], action=1, reward=1.0, next_state=[ 0.01173564 -0.55378261  0.05255388  0.75338197]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 56 ] state=[ 0.01173564 -0.55378261  0.05255388  0.75338197], action=1, reward=1.0, next_state=[ 0.00065999 -0.35942313  0.06762152  0.47768899]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 57 ] state=[ 0.00065999 -0.35942313  0.06762152  0.47768899], action=1, reward=1.0, next_state=[-0.00652847 -0.16531782  0.0771753   0.20706203]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 58 ] state=[-0.00652847 -0.16531782  0.0771753   0.20706203], action=1, reward=1.0, next_state=[-0.00983483  0.02862059  0.08131654 -0.06031266]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 59 ] state=[-0.00983483  0.02862059  0.08131654 -0.06031266], action=1, reward=1.0, next_state=[-0.00926242  0.22248817  0.08011029 -0.32627239]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 60 ] state=[-0.00926242  0.22248817  0.08011029 -0.32627239], action=1, reward=1.0, next_state=[-0.00481265  0.41638359  0.07358484 -0.59265551]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 61 ] state=[-0.00481265  0.41638359  0.07358484 -0.59265551], action=0, reward=1.0, next_state=[ 0.00351502  0.22031287  0.06173173 -0.27773089]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 62 ] state=[ 0.00351502  0.22031287  0.06173173 -0.27773089], action=0, reward=1.0, next_state=[0.00792128 0.02436705 0.05617711 0.03376558]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 63 ] state=[0.00792128 0.02436705 0.05617711 0.03376558], action=1, reward=1.0, next_state=[ 0.00840862  0.21864028  0.05685242 -0.24067718]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 64 ] state=[ 0.00840862  0.21864028  0.05685242 -0.24067718], action=0, reward=1.0, next_state=[0.01278142 0.02275423 0.05203888 0.06938278]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 65 ] state=[0.01278142 0.02275423 0.05203888 0.06938278], action=0, reward=1.0, next_state=[ 0.01323651 -0.17307368  0.05342654  0.37801952]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 66 ] state=[ 0.01323651 -0.17307368  0.05342654  0.37801952], action=0, reward=1.0, next_state=[ 0.00977503 -0.36891208  0.06098693  0.6870579 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 67 ] state=[ 0.00977503 -0.36891208  0.06098693  0.6870579 ], action=0, reward=1.0, next_state=[ 0.00239679 -0.56482518  0.07472808  0.99830026]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 68 ] state=[ 0.00239679 -0.56482518  0.07472808  0.99830026], action=0, reward=1.0, next_state=[-0.00889971 -0.76086232  0.09469409  1.31348491]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 69 ] state=[-0.00889971 -0.76086232  0.09469409  1.31348491], action=0, reward=1.0, next_state=[-0.02411696 -0.95704673  0.12096379  1.63424161]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 70 ] state=[-0.02411696 -0.95704673  0.12096379  1.63424161], action=1, reward=1.0, next_state=[-0.04325789 -0.76353446  0.15364862  1.38157095]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 71 ] state=[-0.04325789 -0.76353446  0.15364862  1.38157095], action=1, reward=1.0, next_state=[-0.05852858 -0.57062716  0.18128004  1.14061404]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 72 ] state=[-0.05852858 -0.57062716  0.18128004  1.14061404], action=0, reward=1.0, next_state=[-0.06994112 -0.76759506  0.20409232  1.48422943]\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 73 ] state=[-0.06994112 -0.76759506  0.20409232  1.48422943], action=1, reward=-1.0, next_state=[-0.08529302 -0.57546139  0.23377691  1.26159788]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 65: Exploration_rate=0.01. Score=73.\n",
      "[ episode 66 ] state=[ 0.01081877 -0.03793902  0.00575893 -0.03352795]\n",
      "[ episode 66 ][ timestamp 1 ] state=[ 0.01081877 -0.03793902  0.00575893 -0.03352795], action=1, reward=1.0, next_state=[ 0.01005999  0.15709988  0.00508838 -0.32438832]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 2 ] state=[ 0.01005999  0.15709988  0.00508838 -0.32438832], action=0, reward=1.0, next_state=[ 0.01320199 -0.03809415 -0.00139939 -0.03010509]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 3 ] state=[ 0.01320199 -0.03809415 -0.00139939 -0.03010509], action=1, reward=1.0, next_state=[ 0.0124401   0.15704784 -0.00200149 -0.32322921]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 4 ] state=[ 0.0124401   0.15704784 -0.00200149 -0.32322921], action=0, reward=1.0, next_state=[ 0.01558106 -0.03804556 -0.00846608 -0.03117814]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 66 ][ timestamp 5 ] state=[ 0.01558106 -0.03804556 -0.00846608 -0.03117814], action=0, reward=1.0, next_state=[ 0.01482015 -0.23304508 -0.00908964  0.25882167]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 6 ] state=[ 0.01482015 -0.23304508 -0.00908964  0.25882167], action=1, reward=1.0, next_state=[ 0.01015925 -0.03779456 -0.00391321 -0.03671434]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 7 ] state=[ 0.01015925 -0.03779456 -0.00391321 -0.03671434], action=1, reward=1.0, next_state=[ 0.00940336  0.15738329 -0.00464749 -0.33062935]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 8 ] state=[ 0.00940336  0.15738329 -0.00464749 -0.33062935], action=0, reward=1.0, next_state=[ 0.01255102 -0.03767219 -0.01126008 -0.03941564]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 9 ] state=[ 0.01255102 -0.03767219 -0.01126008 -0.03941564], action=0, reward=1.0, next_state=[ 0.01179758 -0.23263088 -0.01204839  0.24969346]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 10 ] state=[ 0.01179758 -0.23263088 -0.01204839  0.24969346], action=1, reward=1.0, next_state=[ 0.00714496 -0.03733896 -0.00705452 -0.0467653 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 11 ] state=[ 0.00714496 -0.03733896 -0.00705452 -0.0467653 ], action=1, reward=1.0, next_state=[ 0.00639818  0.15788343 -0.00798983 -0.34166561]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 12 ] state=[ 0.00639818  0.15788343 -0.00798983 -0.34166561], action=0, reward=1.0, next_state=[ 0.00955585 -0.03712394 -0.01482314 -0.05151288]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 13 ] state=[ 0.00955585 -0.03712394 -0.01482314 -0.05151288], action=0, reward=1.0, next_state=[ 0.00881337 -0.23203024 -0.0158534   0.23645661]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 14 ] state=[ 0.00881337 -0.23203024 -0.0158534   0.23645661], action=1, reward=1.0, next_state=[ 0.00417277 -0.03668542 -0.01112427 -0.0611845 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 15 ] state=[ 0.00417277 -0.03668542 -0.01112427 -0.0611845 ], action=1, reward=1.0, next_state=[ 0.00343906  0.15859424 -0.01234796 -0.35735635]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 16 ] state=[ 0.00343906  0.15859424 -0.01234796 -0.35735635], action=0, reward=1.0, next_state=[ 0.00661094 -0.03635    -0.01949508 -0.06859248]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 17 ] state=[ 0.00661094 -0.03635    -0.01949508 -0.06859248], action=1, reward=1.0, next_state=[ 0.00588394  0.15904595 -0.02086693 -0.36736189]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 18 ] state=[ 0.00588394  0.15904595 -0.02086693 -0.36736189], action=1, reward=1.0, next_state=[ 0.00906486  0.3544581  -0.02821417 -0.66655074]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 19 ] state=[ 0.00906486  0.3544581  -0.02821417 -0.66655074], action=1, reward=1.0, next_state=[ 0.01615402  0.54996086 -0.04154519 -0.96798203]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 20 ] state=[ 0.01615402  0.54996086 -0.04154519 -0.96798203], action=0, reward=1.0, next_state=[ 0.02715324  0.35542063 -0.06090483 -0.68863425]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 21 ] state=[ 0.02715324  0.35542063 -0.06090483 -0.68863425], action=0, reward=1.0, next_state=[ 0.03426165  0.16119447 -0.07467751 -0.41573015]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 22 ] state=[ 0.03426165  0.16119447 -0.07467751 -0.41573015], action=1, reward=1.0, next_state=[ 0.03748554  0.35729098 -0.08299212 -0.7309899 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 23 ] state=[ 0.03748554  0.35729098 -0.08299212 -0.7309899 ], action=1, reward=1.0, next_state=[ 0.04463136  0.55345585 -0.09761191 -1.04859613]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 24 ] state=[ 0.04463136  0.55345585 -0.09761191 -1.04859613], action=0, reward=1.0, next_state=[ 0.05570048  0.35975499 -0.11858384 -0.78808029]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 25 ] state=[ 0.05570048  0.35975499 -0.11858384 -0.78808029], action=0, reward=1.0, next_state=[ 0.06289558  0.1664441  -0.13434544 -0.53493233]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 26 ] state=[ 0.06289558  0.1664441  -0.13434544 -0.53493233], action=0, reward=1.0, next_state=[ 0.06622446 -0.02655828 -0.14504409 -0.28741627]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 27 ] state=[ 0.06622446 -0.02655828 -0.14504409 -0.28741627], action=1, reward=1.0, next_state=[ 0.0656933   0.1703022  -0.15079241 -0.6220999 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 28 ] state=[ 0.0656933   0.1703022  -0.15079241 -0.6220999 ], action=0, reward=1.0, next_state=[ 0.06909934 -0.02242828 -0.16323441 -0.38044989]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 29 ] state=[ 0.06909934 -0.02242828 -0.16323441 -0.38044989], action=1, reward=1.0, next_state=[ 0.06865077  0.17458966 -0.17084341 -0.71982638]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 30 ] state=[ 0.06865077  0.17458966 -0.17084341 -0.71982638], action=0, reward=1.0, next_state=[ 0.07214257 -0.01780858 -0.18523994 -0.48541449]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 31 ] state=[ 0.07214257 -0.01780858 -0.18523994 -0.48541449], action=0, reward=1.0, next_state=[ 0.0717864  -0.20989981 -0.19494823 -0.25635668]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 32 ] state=[ 0.0717864  -0.20989981 -0.19494823 -0.25635668], action=0, reward=1.0, next_state=[ 0.0675884  -0.40178177 -0.20007536 -0.0309382 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 33 ] state=[ 0.0675884  -0.40178177 -0.20007536 -0.0309382 ], action=0, reward=1.0, next_state=[ 0.05955276 -0.59355624 -0.20069412  0.19255462]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 34 ] state=[ 0.05955276 -0.59355624 -0.20069412  0.19255462], action=0, reward=1.0, next_state=[ 0.04768164 -0.78532618 -0.19684303  0.41582708]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 35 ] state=[ 0.04768164 -0.78532618 -0.19684303  0.41582708], action=0, reward=1.0, next_state=[ 0.03197512 -0.97719349 -0.18852649  0.64057143]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 36 ] state=[ 0.03197512 -0.97719349 -0.18852649  0.64057143], action=1, reward=1.0, next_state=[ 0.01243125 -0.78001318 -0.17571506  0.29494255]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 37 ] state=[ 0.01243125 -0.78001318 -0.17571506  0.29494255], action=1, reward=1.0, next_state=[-0.00316902 -0.5828784  -0.16981621 -0.04760112]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 38 ] state=[-0.00316902 -0.5828784  -0.16981621 -0.04760112], action=1, reward=1.0, next_state=[-0.01482658 -0.38577987 -0.17076823 -0.38868262]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 39 ] state=[-0.01482658 -0.38577987 -0.17076823 -0.38868262], action=1, reward=1.0, next_state=[-0.02254218 -0.1886977  -0.17854189 -0.7299681 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 40 ] state=[-0.02254218 -0.1886977  -0.17854189 -0.7299681 ], action=1, reward=1.0, next_state=[-0.02631614  0.00838359 -0.19314125 -1.07310361]\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 41 ] state=[-0.02631614  0.00838359 -0.19314125 -1.07310361], action=1, reward=-1.0, next_state=[-0.02614846  0.20546018 -0.21460332 -1.41965303]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 66: Exploration_rate=0.01. Score=41.\n",
      "[ episode 67 ] state=[-0.04848494  0.03431363  0.01201304  0.02812302]\n",
      "[ episode 67 ][ timestamp 1 ] state=[-0.04848494  0.03431363  0.01201304  0.02812302], action=1, reward=1.0, next_state=[-0.04779867  0.22926126  0.0125755  -0.26074558]\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 2 ] state=[-0.04779867  0.22926126  0.0125755  -0.26074558], action=1, reward=1.0, next_state=[-0.04321344  0.42420146  0.00736059 -0.54943566]\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 3 ] state=[-0.04321344  0.42420146  0.00736059 -0.54943566], action=1, reward=1.0, next_state=[-0.03472941  0.61921925 -0.00362812 -0.83979042]\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 4 ] state=[-0.03472941  0.61921925 -0.00362812 -0.83979042], action=1, reward=1.0, next_state=[-0.02234503  0.81439055 -0.02042393 -1.13361211]\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 5 ] state=[-0.02234503  0.81439055 -0.02042393 -1.13361211], action=1, reward=1.0, next_state=[-0.00605722  1.00977376 -0.04309617 -1.43263002]\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 6 ] state=[-0.00605722  1.00977376 -0.04309617 -1.43263002], action=1, reward=1.0, next_state=[ 0.01413826  1.20540017 -0.07174877 -1.73846354]\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 7 ] state=[ 0.01413826  1.20540017 -0.07174877 -1.73846354], action=1, reward=1.0, next_state=[ 0.03824626  1.40126256 -0.10651804 -2.05257727]\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 8 ] state=[ 0.03824626  1.40126256 -0.10651804 -2.05257727], action=1, reward=1.0, next_state=[ 0.06627151  1.59730147 -0.14756959 -2.37622613]\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 9 ] state=[ 0.06627151  1.59730147 -0.14756959 -2.37622613], action=1, reward=1.0, next_state=[ 0.09821754  1.79338903 -0.19509411 -2.71038883]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 67 ][ timestamp 10 ] state=[ 0.09821754  1.79338903 -0.19509411 -2.71038883], action=0, reward=-1.0, next_state=[ 0.13408532  1.60013658 -0.24930189 -2.48300382]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 67: Exploration_rate=0.01. Score=10.\n",
      "[ episode 68 ] state=[ 0.01233761 -0.04146722 -0.01014843 -0.01164619]\n",
      "[ episode 68 ][ timestamp 1 ] state=[ 0.01233761 -0.04146722 -0.01014843 -0.01164619], action=0, reward=1.0, next_state=[ 0.01150827 -0.23644217 -0.01038136  0.27781759]\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 2 ] state=[ 0.01150827 -0.23644217 -0.01038136  0.27781759], action=0, reward=1.0, next_state=[ 0.00677943 -0.43141449 -0.004825    0.56720825]\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 3 ] state=[ 0.00677943 -0.43141449 -0.004825    0.56720825], action=0, reward=1.0, next_state=[-0.00184886 -0.62646843  0.00651916  0.8583672 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 4 ] state=[-0.00184886 -0.62646843  0.00651916  0.8583672 ], action=0, reward=1.0, next_state=[-0.01437823 -0.82167858  0.02368651  1.15309282]\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 5 ] state=[-0.01437823 -0.82167858  0.02368651  1.15309282], action=1, reward=1.0, next_state=[-0.0308118  -0.62687347  0.04674836  0.86793031]\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 6 ] state=[-0.0308118  -0.62687347  0.04674836  0.86793031], action=1, reward=1.0, next_state=[-0.04334927 -0.43241771  0.06410697  0.59030435]\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 7 ] state=[-0.04334927 -0.43241771  0.06410697  0.59030435], action=1, reward=1.0, next_state=[-0.05199763 -0.23824916  0.07591306  0.31848435]\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 8 ] state=[-0.05199763 -0.23824916  0.07591306  0.31848435], action=1, reward=1.0, next_state=[-0.05676261 -0.04428586  0.08228274  0.05067433]\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 9 ] state=[-0.05676261 -0.04428586  0.08228274  0.05067433], action=0, reward=1.0, next_state=[-0.05764833 -0.24048537  0.08329623  0.36814172]\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 10 ] state=[-0.05764833 -0.24048537  0.08329623  0.36814172], action=0, reward=1.0, next_state=[-0.06245803 -0.43668601  0.09065906  0.68588309]\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 11 ] state=[-0.06245803 -0.43668601  0.09065906  0.68588309], action=0, reward=1.0, next_state=[-0.07119175 -0.63294178  0.10437672  1.00567506]\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 12 ] state=[-0.07119175 -0.63294178  0.10437672  1.00567506], action=1, reward=1.0, next_state=[-0.08385059 -0.43935685  0.12449023  0.74750905]\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 13 ] state=[-0.08385059 -0.43935685  0.12449023  0.74750905], action=0, reward=1.0, next_state=[-0.09263773 -0.6359563   0.13944041  1.0766317 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 14 ] state=[-0.09263773 -0.6359563   0.13944041  1.0766317 ], action=1, reward=1.0, next_state=[-0.10535685 -0.44292398  0.16097304  0.83075636]\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 15 ] state=[-0.10535685 -0.44292398  0.16097304  0.83075636], action=1, reward=1.0, next_state=[-0.11421533 -0.25032505  0.17758817  0.59271485]\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 16 ] state=[-0.11421533 -0.25032505  0.17758817  0.59271485], action=1, reward=1.0, next_state=[-0.11922183 -0.05807522  0.18944247  0.36081239]\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 17 ] state=[-0.11922183 -0.05807522  0.18944247  0.36081239], action=0, reward=1.0, next_state=[-0.12038334 -0.25531381  0.19665871  0.70674075]\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 18 ] state=[-0.12038334 -0.25531381  0.19665871  0.70674075], action=1, reward=-1.0, next_state=[-0.12548961 -0.06338104  0.21079353  0.48183657]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 68: Exploration_rate=0.01. Score=18.\n",
      "[ episode 69 ] state=[-0.00460194 -0.04715056 -0.01054589  0.04444543]\n",
      "[ episode 69 ][ timestamp 1 ] state=[-0.00460194 -0.04715056 -0.01054589  0.04444543], action=1, reward=1.0, next_state=[-0.00554495  0.14812102 -0.00965698 -0.25154608]\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 2 ] state=[-0.00554495  0.14812102 -0.00965698 -0.25154608], action=1, reward=1.0, next_state=[-0.00258253  0.34337953 -0.01468791 -0.5472593 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 3 ] state=[-0.00258253  0.34337953 -0.01468791 -0.5472593 ], action=0, reward=1.0, next_state=[ 0.00428506  0.14846698 -0.02563309 -0.2592401 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 4 ] state=[ 0.00428506  0.14846698 -0.02563309 -0.2592401 ], action=1, reward=1.0, next_state=[ 0.0072544   0.34394531 -0.03081789 -0.55989657]\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 5 ] state=[ 0.0072544   0.34394531 -0.03081789 -0.55989657], action=0, reward=1.0, next_state=[ 0.01413331  0.14926915 -0.04201582 -0.27708002]\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 6 ] state=[ 0.01413331  0.14926915 -0.04201582 -0.27708002], action=0, reward=1.0, next_state=[ 0.01711869 -0.04522898 -0.04755742  0.00206068]\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 7 ] state=[ 0.01711869 -0.04522898 -0.04755742  0.00206068], action=0, reward=1.0, next_state=[ 0.01621411 -0.23963777 -0.04751621  0.27936754]\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 8 ] state=[ 0.01621411 -0.23963777 -0.04751621  0.27936754], action=1, reward=1.0, next_state=[ 0.01142136 -0.04387132 -0.04192886 -0.0279152 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 9 ] state=[ 0.01142136 -0.04387132 -0.04192886 -0.0279152 ], action=1, reward=1.0, next_state=[ 0.01054393  0.15182605 -0.04248716 -0.33352675]\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 10 ] state=[ 0.01054393  0.15182605 -0.04248716 -0.33352675], action=0, reward=1.0, next_state=[ 0.01358045 -0.04266624 -0.0491577  -0.05453906]\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 11 ] state=[ 0.01358045 -0.04266624 -0.0491577  -0.05453906], action=0, reward=1.0, next_state=[ 0.01272713 -0.23705013 -0.05024848  0.22223802]\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 12 ] state=[ 0.01272713 -0.23705013 -0.05024848  0.22223802], action=1, reward=1.0, next_state=[ 0.00798612 -0.04124732 -0.04580372 -0.08586233]\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 13 ] state=[ 0.00798612 -0.04124732 -0.04580372 -0.08586233], action=1, reward=1.0, next_state=[ 0.00716118  0.15450025 -0.04752097 -0.39263731]\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 14 ] state=[ 0.00716118  0.15450025 -0.04752097 -0.39263731], action=1, reward=1.0, next_state=[ 0.01025118  0.35026323 -0.05537371 -0.69991619]\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 15 ] state=[ 0.01025118  0.35026323 -0.05537371 -0.69991619], action=1, reward=1.0, next_state=[ 0.01725645  0.54610735 -0.06937204 -1.00950366]\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 16 ] state=[ 0.01725645  0.54610735 -0.06937204 -1.00950366], action=1, reward=1.0, next_state=[ 0.02817859  0.74208319 -0.08956211 -1.32313938]\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 17 ] state=[ 0.02817859  0.74208319 -0.08956211 -1.32313938], action=0, reward=1.0, next_state=[ 0.04302026  0.54819956 -0.1160249  -1.05977564]\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 18 ] state=[ 0.04302026  0.54819956 -0.1160249  -1.05977564], action=1, reward=1.0, next_state=[ 0.05398425  0.7446511  -0.13722041 -1.38650657]\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 19 ] state=[ 0.05398425  0.7446511  -0.13722041 -1.38650657], action=1, reward=1.0, next_state=[ 0.06887727  0.94119074 -0.16495054 -1.71876115]\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 20 ] state=[ 0.06887727  0.94119074 -0.16495054 -1.71876115], action=0, reward=1.0, next_state=[ 0.08770109  0.74829905 -0.19932576 -1.48162678]\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 21 ] state=[ 0.08770109  0.74829905 -0.19932576 -1.48162678], action=0, reward=-1.0, next_state=[ 0.10266707  0.55608795 -0.2289583  -1.25723322]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 69: Exploration_rate=0.01. Score=21.\n",
      "[ episode 70 ] state=[ 0.01618143 -0.04584759 -0.00849073 -0.00562426]\n",
      "[ episode 70 ][ timestamp 1 ] state=[ 0.01618143 -0.04584759 -0.00849073 -0.00562426], action=1, reward=1.0, next_state=[ 0.01526448  0.14939509 -0.00860322 -0.30097398]\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 2 ] state=[ 0.01526448  0.14939509 -0.00860322 -0.30097398], action=1, reward=1.0, next_state=[ 0.01825239  0.3446386  -0.01462269 -0.59635772]\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 3 ] state=[ 0.01825239  0.3446386  -0.01462269 -0.59635772], action=1, reward=1.0, next_state=[ 0.02514516  0.53996211 -0.02654985 -0.89361057]\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 4 ] state=[ 0.02514516  0.53996211 -0.02654985 -0.89361057], action=1, reward=1.0, next_state=[ 0.0359444   0.73543388 -0.04442206 -1.19451963]\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 5 ] state=[ 0.0359444   0.73543388 -0.04442206 -1.19451963], action=1, reward=1.0, next_state=[ 0.05065308  0.93110202 -0.06831245 -1.50078809]\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 6 ] state=[ 0.05065308  0.93110202 -0.06831245 -1.50078809], action=0, reward=1.0, next_state=[ 0.06927512  0.73687295 -0.09832821 -1.23019226]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 70 ][ timestamp 7 ] state=[ 0.06927512  0.73687295 -0.09832821 -1.23019226], action=0, reward=1.0, next_state=[ 0.08401258  0.54314376 -0.12293206 -0.96986408]\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 8 ] state=[ 0.08401258  0.54314376 -0.12293206 -0.96986408], action=0, reward=1.0, next_state=[ 0.09487545  0.34986725 -0.14232934 -0.71818824]\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 9 ] state=[ 0.09487545  0.34986725 -0.14232934 -0.71818824], action=0, reward=1.0, next_state=[ 0.1018728   0.15697167 -0.15669311 -0.47347433]\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 10 ] state=[ 0.1018728   0.15697167 -0.15669311 -0.47347433], action=1, reward=1.0, next_state=[ 0.10501223  0.35391915 -0.16616259 -0.81115576]\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 11 ] state=[ 0.10501223  0.35391915 -0.16616259 -0.81115576], action=0, reward=1.0, next_state=[ 0.11209061  0.16141576 -0.18238571 -0.57500509]\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 12 ] state=[ 0.11209061  0.16141576 -0.18238571 -0.57500509], action=0, reward=1.0, next_state=[ 0.11531893 -0.03074409 -0.19388581 -0.34487073]\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 13 ] state=[ 0.11531893 -0.03074409 -0.19388581 -0.34487073], action=0, reward=1.0, next_state=[ 0.11470405 -0.22265566 -0.20078322 -0.11904312]\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 14 ] state=[ 0.11470405 -0.22265566 -0.20078322 -0.11904312], action=0, reward=1.0, next_state=[ 0.11025093 -0.41441951 -0.20316409  0.1041896 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 15 ] state=[ 0.11025093 -0.41441951 -0.20316409  0.1041896 ], action=1, reward=1.0, next_state=[ 0.10196254 -0.21705346 -0.2010803  -0.24509084]\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 16 ] state=[ 0.10196254 -0.21705346 -0.2010803  -0.24509084], action=1, reward=1.0, next_state=[ 0.09762147 -0.01971255 -0.20598211 -0.59385801]\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 17 ] state=[ 0.09762147 -0.01971255 -0.20598211 -0.59385801], action=0, reward=-1.0, next_state=[ 0.09722722 -0.21144677 -0.21785927 -0.37246782]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 70: Exploration_rate=0.01. Score=17.\n",
      "[ episode 71 ] state=[-0.01748718 -0.04908238  0.03700404 -0.04420474]\n",
      "[ episode 71 ][ timestamp 1 ] state=[-0.01748718 -0.04908238  0.03700404 -0.04420474], action=1, reward=1.0, next_state=[-0.01846883  0.14548995  0.03611995 -0.32498673]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 2 ] state=[-0.01846883  0.14548995  0.03611995 -0.32498673], action=1, reward=1.0, next_state=[-0.01555903  0.34007949  0.02962021 -0.6060637 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 3 ] state=[-0.01555903  0.34007949  0.02962021 -0.6060637 ], action=1, reward=1.0, next_state=[-0.00875744  0.53477501  0.01749894 -0.88927181]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 4 ] state=[-0.00875744  0.53477501  0.01749894 -0.88927181], action=1, reward=1.0, next_state=[ 1.93806059e-03  7.29655188e-01 -2.86496042e-04 -1.17640289e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 5 ] state=[ 1.93806059e-03  7.29655188e-01 -2.86496042e-04 -1.17640289e+00], action=0, reward=1.0, next_state=[ 0.01653116  0.53453696 -0.02381455 -0.88380979]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 6 ] state=[ 0.01653116  0.53453696 -0.02381455 -0.88380979], action=0, reward=1.0, next_state=[ 0.0272219   0.33974635 -0.04149075 -0.59870755]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 7 ] state=[ 0.0272219   0.33974635 -0.04149075 -0.59870755], action=0, reward=1.0, next_state=[ 0.03401683  0.14522874 -0.0534649  -0.31937701]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 8 ] state=[ 0.03401683  0.14522874 -0.0534649  -0.31937701], action=1, reward=1.0, next_state=[ 0.03692141  0.34106975 -0.05985244 -0.62842997]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 9 ] state=[ 0.03692141  0.34106975 -0.05985244 -0.62842997], action=0, reward=1.0, next_state=[ 0.0437428   0.14683193 -0.07242104 -0.35518106]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 10 ] state=[ 0.0437428   0.14683193 -0.07242104 -0.35518106], action=0, reward=1.0, next_state=[ 0.04667944 -0.0471896  -0.07952466 -0.08618481]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 11 ] state=[ 0.04667944 -0.0471896  -0.07952466 -0.08618481], action=1, reward=1.0, next_state=[ 0.04573565  0.14897689 -0.08124836 -0.40286021]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 12 ] state=[ 0.04573565  0.14897689 -0.08124836 -0.40286021], action=0, reward=1.0, next_state=[ 0.04871518 -0.04490437 -0.08930556 -0.13685844]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 13 ] state=[ 0.04871518 -0.04490437 -0.08930556 -0.13685844], action=1, reward=1.0, next_state=[ 0.0478171   0.15137568 -0.09204273 -0.45632618]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 14 ] state=[ 0.0478171   0.15137568 -0.09204273 -0.45632618], action=0, reward=1.0, next_state=[ 0.05084461 -0.04233262 -0.10116925 -0.19401603]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 15 ] state=[ 0.05084461 -0.04233262 -0.10116925 -0.19401603], action=0, reward=1.0, next_state=[ 0.04999796 -0.23587279 -0.10504958  0.06511675]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 16 ] state=[ 0.04999796 -0.23587279 -0.10504958  0.06511675], action=0, reward=1.0, next_state=[ 0.0452805  -0.42934402 -0.10374724  0.32289599]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 17 ] state=[ 0.0452805  -0.42934402 -0.10374724  0.32289599], action=1, reward=1.0, next_state=[ 0.03669362 -0.23290952 -0.09728932 -0.00061844]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 18 ] state=[ 0.03669362 -0.23290952 -0.09728932 -0.00061844], action=1, reward=1.0, next_state=[ 0.03203543 -0.03653666 -0.09730169 -0.32234275]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 19 ] state=[ 0.03203543 -0.03653666 -0.09730169 -0.32234275], action=1, reward=1.0, next_state=[ 0.0313047   0.1598265  -0.10374854 -0.64405585]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 20 ] state=[ 0.0313047   0.1598265  -0.10374854 -0.64405585], action=0, reward=1.0, next_state=[ 0.03450123 -0.03370823 -0.11662966 -0.38576211]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 21 ] state=[ 0.03450123 -0.03370823 -0.11662966 -0.38576211], action=0, reward=1.0, next_state=[ 0.03382706 -0.22699805 -0.1243449  -0.1320085 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 22 ] state=[ 0.03382706 -0.22699805 -0.1243449  -0.1320085 ], action=1, reward=1.0, next_state=[ 0.0292871  -0.0303346  -0.12698507 -0.46118933]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 23 ] state=[ 0.0292871  -0.0303346  -0.12698507 -0.46118933], action=1, reward=1.0, next_state=[ 0.02868041  0.16633191 -0.13620886 -0.79104717]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 24 ] state=[ 0.02868041  0.16633191 -0.13620886 -0.79104717], action=0, reward=1.0, next_state=[ 0.03200705 -0.02668317 -0.1520298  -0.54412783]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 25 ] state=[ 0.03200705 -0.02668317 -0.1520298  -0.54412783], action=1, reward=1.0, next_state=[ 0.03147339  0.17021149 -0.16291236 -0.88058805]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 26 ] state=[ 0.03147339  0.17021149 -0.16291236 -0.88058805], action=0, reward=1.0, next_state=[ 0.03487762 -0.02236719 -0.18052412 -0.64322955]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 27 ] state=[ 0.03487762 -0.02236719 -0.18052412 -0.64322955], action=0, reward=1.0, next_state=[ 0.03443027 -0.21457514 -0.19338871 -0.41238905]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 28 ] state=[ 0.03443027 -0.21457514 -0.19338871 -0.41238905], action=0, reward=1.0, next_state=[ 0.03013877 -0.40650543 -0.20163649 -0.18636294]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 29 ] state=[ 0.03013877 -0.40650543 -0.20163649 -0.18636294], action=0, reward=1.0, next_state=[ 0.02200866 -0.5982574  -0.20536375  0.03655746]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 30 ] state=[ 0.02200866 -0.5982574  -0.20536375  0.03655746], action=0, reward=1.0, next_state=[ 0.01004351 -0.78993328 -0.2046326   0.25807628]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 31 ] state=[ 0.01004351 -0.78993328 -0.2046326   0.25807628], action=1, reward=1.0, next_state=[-0.00575515 -0.59256738 -0.19947108 -0.09153871]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 32 ] state=[-0.00575515 -0.59256738 -0.19947108 -0.09153871], action=0, reward=1.0, next_state=[-0.0176065  -0.78435453 -0.20130185  0.13218137]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 33 ] state=[-0.0176065  -0.78435453 -0.20130185  0.13218137], action=1, reward=1.0, next_state=[-0.03329359 -0.58700368 -0.19865822 -0.21665113]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 71 ][ timestamp 34 ] state=[-0.03329359 -0.58700368 -0.19865822 -0.21665113], action=1, reward=1.0, next_state=[-0.04503366 -0.38967844 -0.20299125 -0.56483968]\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 35 ] state=[-0.04503366 -0.38967844 -0.20299125 -0.56483968], action=0, reward=-1.0, next_state=[-0.05282723 -0.58146093 -0.21428804 -0.3423429 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 71: Exploration_rate=0.01. Score=35.\n",
      "[ episode 72 ] state=[ 0.02549114  0.00114762 -0.01028129 -0.00561367]\n",
      "[ episode 72 ][ timestamp 1 ] state=[ 0.02549114  0.00114762 -0.01028129 -0.00561367], action=1, reward=1.0, next_state=[ 0.02551409  0.19641549 -0.01039356 -0.30152265]\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 2 ] state=[ 0.02551409  0.19641549 -0.01039356 -0.30152265], action=0, reward=1.0, next_state=[ 0.0294424   0.00144321 -0.01642401 -0.01213568]\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 3 ] state=[ 0.0294424   0.00144321 -0.01642401 -0.01213568], action=1, reward=1.0, next_state=[ 0.02947126  0.19679681 -0.01666673 -0.30995499]\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 4 ] state=[ 0.02947126  0.19679681 -0.01666673 -0.30995499], action=0, reward=1.0, next_state=[ 0.0334072   0.00191623 -0.02286583 -0.02257452]\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 5 ] state=[ 0.0334072   0.00191623 -0.02286583 -0.02257452], action=1, reward=1.0, next_state=[ 0.03344552  0.19735851 -0.02331732 -0.32238327]\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 6 ] state=[ 0.03344552  0.19735851 -0.02331732 -0.32238327], action=1, reward=1.0, next_state=[ 0.03739269  0.39280461 -0.02976498 -0.62232739]\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 7 ] state=[ 0.03739269  0.39280461 -0.02976498 -0.62232739], action=1, reward=1.0, next_state=[ 0.04524879  0.58832927 -0.04221153 -0.92423408]\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 8 ] state=[ 0.04524879  0.58832927 -0.04221153 -0.92423408], action=0, reward=1.0, next_state=[ 0.05701537  0.39380215 -0.06069621 -0.64510983]\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 9 ] state=[ 0.05701537  0.39380215 -0.06069621 -0.64510983], action=1, reward=1.0, next_state=[ 0.06489141  0.58971506 -0.07359841 -0.95627179]\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 10 ] state=[ 0.06489141  0.58971506 -0.07359841 -0.95627179], action=0, reward=1.0, next_state=[ 0.07668572  0.39565599 -0.09272384 -0.68758959]\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 11 ] state=[ 0.07668572  0.39565599 -0.09272384 -0.68758959], action=0, reward=1.0, next_state=[ 0.08459883  0.20193504 -0.10647564 -0.4254782 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 12 ] state=[ 0.08459883  0.20193504 -0.10647564 -0.4254782 ], action=1, reward=1.0, next_state=[ 0.08863754  0.39839137 -0.1149852  -0.74973857]\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 13 ] state=[ 0.08863754  0.39839137 -0.1149852  -0.74973857], action=0, reward=1.0, next_state=[ 0.09660536  0.2050273  -0.12997997 -0.495339  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 14 ] state=[ 0.09660536  0.2050273  -0.12997997 -0.495339  ], action=0, reward=1.0, next_state=[ 0.10070591  0.01195483 -0.13988675 -0.2462799 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 15 ] state=[ 0.10070591  0.01195483 -0.13988675 -0.2462799 ], action=1, reward=1.0, next_state=[ 0.10094501  0.20876868 -0.14481235 -0.57960959]\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 16 ] state=[ 0.10094501  0.20876868 -0.14481235 -0.57960959], action=0, reward=1.0, next_state=[ 0.10512038  0.01594115 -0.15640454 -0.33582198]\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 17 ] state=[ 0.10512038  0.01594115 -0.15640454 -0.33582198], action=1, reward=1.0, next_state=[ 0.1054392   0.21290295 -0.16312098 -0.67345411]\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 18 ] state=[ 0.1054392   0.21290295 -0.16312098 -0.67345411], action=1, reward=1.0, next_state=[ 0.10969726  0.40987106 -0.17659006 -1.01272939]\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 19 ] state=[ 0.10969726  0.40987106 -0.17659006 -1.01272939], action=0, reward=1.0, next_state=[ 0.11789468  0.217488   -0.19684465 -0.78029064]\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 20 ] state=[ 0.11789468  0.217488   -0.19684465 -0.78029064], action=1, reward=-1.0, next_state=[ 0.12224444  0.41469252 -0.21245046 -1.12788429]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 72: Exploration_rate=0.01. Score=20.\n",
      "[ episode 73 ] state=[-0.01042265 -0.01743934 -0.01469836 -0.00998034]\n",
      "[ episode 73 ][ timestamp 1 ] state=[-0.01042265 -0.01743934 -0.01469836 -0.00998034], action=1, reward=1.0, next_state=[-0.01077143  0.17789029 -0.01489796 -0.30726429]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 2 ] state=[-0.01077143  0.17789029 -0.01489796 -0.30726429], action=1, reward=1.0, next_state=[-0.00721363  0.37322133 -0.02104325 -0.60460817]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 3 ] state=[-0.00721363  0.37322133 -0.02104325 -0.60460817], action=0, reward=1.0, next_state=[ 2.50797481e-04  1.78399884e-01 -3.31354134e-02 -3.18626966e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 4 ] state=[ 2.50797481e-04  1.78399884e-01 -3.31354134e-02 -3.18626966e-01], action=0, reward=1.0, next_state=[ 0.0038188  -0.01623486 -0.03950795 -0.03657515]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 5 ] state=[ 0.0038188  -0.01623486 -0.03950795 -0.03657515], action=0, reward=1.0, next_state=[ 0.0034941  -0.21076864 -0.04023946  0.24338551]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 6 ] state=[ 0.0034941  -0.21076864 -0.04023946  0.24338551], action=1, reward=1.0, next_state=[-0.00072127 -0.01509571 -0.03537175 -0.06171349]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 7 ] state=[-0.00072127 -0.01509571 -0.03537175 -0.06171349], action=1, reward=1.0, next_state=[-0.00102319  0.18051507 -0.03660602 -0.36534325]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 8 ] state=[-0.00102319  0.18051507 -0.03660602 -0.36534325], action=0, reward=1.0, next_state=[ 0.00258711 -0.01406807 -0.04391288 -0.08442385]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 9 ] state=[ 0.00258711 -0.01406807 -0.04391288 -0.08442385], action=1, reward=1.0, next_state=[ 0.00230575  0.18165495 -0.04560136 -0.39063159]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 10 ] state=[ 0.00230575  0.18165495 -0.04560136 -0.39063159], action=1, reward=1.0, next_state=[ 0.00593885  0.37739343 -0.05341399 -0.69733625]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 11 ] state=[ 0.00593885  0.37739343 -0.05341399 -0.69733625], action=0, reward=1.0, next_state=[ 0.01348672  0.18305126 -0.06736071 -0.42193499]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 12 ] state=[ 0.01348672  0.18305126 -0.06736071 -0.42193499], action=0, reward=1.0, next_state=[ 0.01714774 -0.0110549  -0.07579941 -0.15122514]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 13 ] state=[ 0.01714774 -0.0110549  -0.07579941 -0.15122514], action=0, reward=1.0, next_state=[ 0.01692665 -0.20501424 -0.07882392  0.11661478]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 14 ] state=[ 0.01692665 -0.20501424 -0.07882392  0.11661478], action=1, reward=1.0, next_state=[ 0.01282636 -0.00885655 -0.07649162 -0.19985839]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 15 ] state=[ 0.01282636 -0.00885655 -0.07649162 -0.19985839], action=1, reward=1.0, next_state=[ 0.01264923  0.18727135 -0.08048879 -0.51565662]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 16 ] state=[ 0.01264923  0.18727135 -0.08048879 -0.51565662], action=1, reward=1.0, next_state=[ 0.01639466  0.383429   -0.09080192 -0.83257868]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 17 ] state=[ 0.01639466  0.383429   -0.09080192 -0.83257868], action=0, reward=1.0, next_state=[ 0.02406324  0.1896574  -0.10745349 -0.56977778]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 18 ] state=[ 0.02406324  0.1896574  -0.10745349 -0.56977778], action=0, reward=1.0, next_state=[ 0.02785638 -0.0038065  -0.11884905 -0.31278623]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 19 ] state=[ 0.02785638 -0.0038065  -0.11884905 -0.31278623], action=1, reward=1.0, next_state=[ 0.02778025  0.19279036 -0.12510477 -0.64046068]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 20 ] state=[ 0.02778025  0.19279036 -0.12510477 -0.64046068], action=1, reward=1.0, next_state=[ 0.03163606  0.3894139  -0.13791399 -0.96977588]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 21 ] state=[ 0.03163606  0.3894139  -0.13791399 -0.96977588], action=0, reward=1.0, next_state=[ 0.03942434  0.19638566 -0.15730951 -0.72340104]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 22 ] state=[ 0.03942434  0.19638566 -0.15730951 -0.72340104], action=0, reward=1.0, next_state=[ 0.04335205  0.00374868 -0.17177753 -0.48407197]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 73 ][ timestamp 23 ] state=[ 0.04335205  0.00374868 -0.17177753 -0.48407197], action=0, reward=1.0, next_state=[ 0.04342703 -0.18858579 -0.18145897 -0.2500709 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 24 ] state=[ 0.04342703 -0.18858579 -0.18145897 -0.2500709 ], action=1, reward=1.0, next_state=[ 0.03965531  0.00860089 -0.18646038 -0.59405128]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 25 ] state=[ 0.03965531  0.00860089 -0.18646038 -0.59405128], action=0, reward=1.0, next_state=[ 0.03982733 -0.18348929 -0.19834141 -0.36541264]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 26 ] state=[ 0.03982733 -0.18348929 -0.19834141 -0.36541264], action=1, reward=1.0, next_state=[ 0.03615754  0.01381695 -0.20564966 -0.71350043]\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 27 ] state=[ 0.03615754  0.01381695 -0.20564966 -0.71350043], action=1, reward=-1.0, next_state=[ 0.03643388  0.2111024  -0.21991967 -1.06322869]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 73: Exploration_rate=0.01. Score=27.\n",
      "[ episode 74 ] state=[ 0.01684989 -0.03914854  0.0286399   0.03665196]\n",
      "[ episode 74 ][ timestamp 1 ] state=[ 0.01684989 -0.03914854  0.0286399   0.03665196], action=1, reward=1.0, next_state=[ 0.01606692  0.15555126  0.02937294 -0.24685898]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 2 ] state=[ 0.01606692  0.15555126  0.02937294 -0.24685898], action=1, reward=1.0, next_state=[ 0.01917795  0.35024166  0.02443576 -0.53013421]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 3 ] state=[ 0.01917795  0.35024166  0.02443576 -0.53013421], action=0, reward=1.0, next_state=[ 0.02618278  0.15478464  0.01383308 -0.22985281]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 4 ] state=[ 0.02618278  0.15478464  0.01383308 -0.22985281], action=0, reward=1.0, next_state=[ 0.02927847 -0.04053223  0.00923602  0.06716126]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 5 ] state=[ 0.02927847 -0.04053223  0.00923602  0.06716126], action=1, reward=1.0, next_state=[ 0.02846783  0.1544561   0.01057925 -0.2225934 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 6 ] state=[ 0.02846783  0.1544561   0.01057925 -0.2225934 ], action=1, reward=1.0, next_state=[ 0.03155695  0.34942525  0.00612738 -0.51192053]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 7 ] state=[ 0.03155695  0.34942525  0.00612738 -0.51192053], action=1, reward=1.0, next_state=[ 0.03854545  0.54446036 -0.00411103 -0.80266626]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 8 ] state=[ 0.03854545  0.54446036 -0.00411103 -0.80266626], action=0, reward=1.0, next_state=[ 0.04943466  0.34939503 -0.02016436 -0.51127937]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 9 ] state=[ 0.04943466  0.34939503 -0.02016436 -0.51127937], action=1, reward=1.0, next_state=[ 0.05642256  0.54479513 -0.03038994 -0.81024786]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 10 ] state=[ 0.05642256  0.54479513 -0.03038994 -0.81024786], action=0, reward=1.0, next_state=[ 0.06731846  0.35010244 -0.0465949  -0.52727693]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 11 ] state=[ 0.06731846  0.35010244 -0.0465949  -0.52727693], action=0, reward=1.0, next_state=[ 0.07432051  0.15566599 -0.05714044 -0.24963275]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 12 ] state=[ 0.07432051  0.15566599 -0.05714044 -0.24963275], action=1, reward=1.0, next_state=[ 0.07743383  0.35155542 -0.06213309 -0.55977748]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 13 ] state=[ 0.07743383  0.35155542 -0.06213309 -0.55977748], action=0, reward=1.0, next_state=[ 0.08446494  0.15735806 -0.07332864 -0.28729892]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 14 ] state=[ 0.08446494  0.15735806 -0.07332864 -0.28729892], action=0, reward=1.0, next_state=[ 0.0876121  -0.03664573 -0.07907462 -0.01861458]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 15 ] state=[ 0.0876121  -0.03664573 -0.07907462 -0.01861458], action=1, reward=1.0, next_state=[ 0.08687919  0.15951596 -0.07944691 -0.33516139]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 16 ] state=[ 0.08687919  0.15951596 -0.07944691 -0.33516139], action=1, reward=1.0, next_state=[ 0.09006951  0.35567341 -0.08615014 -0.6518023 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 17 ] state=[ 0.09006951  0.35567341 -0.08615014 -0.6518023 ], action=0, reward=1.0, next_state=[ 0.09718298  0.16185021 -0.09918619 -0.38744254]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 18 ] state=[ 0.09718298  0.16185021 -0.09918619 -0.38744254], action=0, reward=1.0, next_state=[ 0.10041998 -0.03173419 -0.10693504 -0.12760608]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 19 ] state=[ 0.10041998 -0.03173419 -0.10693504 -0.12760608], action=0, reward=1.0, next_state=[ 0.0997853  -0.22517466 -0.10948716  0.12951818]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 20 ] state=[ 0.0997853  -0.22517466 -0.10948716  0.12951818], action=1, reward=1.0, next_state=[ 0.0952818  -0.02866848 -0.1068968  -0.1956011 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 21 ] state=[ 0.0952818  -0.02866848 -0.1068968  -0.1956011 ], action=1, reward=1.0, next_state=[ 0.09470843  0.16780724 -0.11080882 -0.52000029]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 22 ] state=[ 0.09470843  0.16780724 -0.11080882 -0.52000029], action=1, reward=1.0, next_state=[ 0.09806458  0.36430039 -0.12120882 -0.84544353]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 23 ] state=[ 0.09806458  0.36430039 -0.12120882 -0.84544353], action=1, reward=1.0, next_state=[ 0.10535059  0.5608491  -0.13811769 -1.17365175]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 24 ] state=[ 0.10535059  0.5608491  -0.13811769 -1.17365175], action=0, reward=1.0, next_state=[ 0.11656757  0.36776565 -0.16159073 -0.92726232]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 25 ] state=[ 0.11656757  0.36776565 -0.16159073 -0.92726232], action=0, reward=1.0, next_state=[ 0.12392288  0.17515087 -0.18013598 -0.68940525]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 26 ] state=[ 0.12392288  0.17515087 -0.18013598 -0.68940525], action=1, reward=1.0, next_state=[ 0.1274259   0.37225469 -0.19392408 -1.0329511 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 27 ] state=[ 0.1274259   0.37225469 -0.19392408 -1.0329511 ], action=0, reward=-1.0, next_state=[ 0.13487099  0.18016606 -0.2145831  -0.80687604]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 74: Exploration_rate=0.01. Score=27.\n",
      "[ episode 75 ] state=[-0.02256048  0.02000099  0.01666236 -0.03602144]\n",
      "[ episode 75 ][ timestamp 1 ] state=[-0.02256048  0.02000099  0.01666236 -0.03602144], action=1, reward=1.0, next_state=[-0.02216046  0.21488008  0.01594194 -0.32340099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 2 ] state=[-0.02216046  0.21488008  0.01594194 -0.32340099], action=1, reward=1.0, next_state=[-0.01786286  0.40977144  0.00947392 -0.61101416]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 3 ] state=[-0.01786286  0.40977144  0.00947392 -0.61101416], action=0, reward=1.0, next_state=[-0.00966743  0.21451836 -0.00274637 -0.31536239]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 4 ] state=[-0.00966743  0.21451836 -0.00274637 -0.31536239], action=0, reward=1.0, next_state=[-0.00537707  0.01943564 -0.00905361 -0.02354684]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 5 ] state=[-0.00537707  0.01943564 -0.00905361 -0.02354684], action=1, reward=1.0, next_state=[-0.00498835  0.21468625 -0.00952455 -0.31907248]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 6 ] state=[-0.00498835  0.21468625 -0.00952455 -0.31907248], action=0, reward=1.0, next_state=[-0.00069463  0.01970124 -0.015906   -0.0294084 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 7 ] state=[-0.00069463  0.01970124 -0.015906   -0.0294084 ], action=1, reward=1.0, next_state=[-3.00602783e-04  2.15047635e-01 -1.64941693e-02 -3.27067097e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 8 ] state=[-3.00602783e-04  2.15047635e-01 -1.64941693e-02 -3.27067097e-01], action=0, reward=1.0, next_state=[ 0.00400035  0.02016435 -0.02303551 -0.039631  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 9 ] state=[ 0.00400035  0.02016435 -0.02303551 -0.039631  ], action=1, reward=1.0, next_state=[ 0.00440364  0.21560892 -0.02382813 -0.33949192]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 10 ] state=[ 0.00440364  0.21560892 -0.02382813 -0.33949192], action=0, reward=1.0, next_state=[ 0.00871582  0.02083399 -0.03061797 -0.05441726]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 11 ] state=[ 0.00871582  0.02083399 -0.03061797 -0.05441726], action=1, reward=1.0, next_state=[ 0.0091325   0.21638127 -0.03170631 -0.35660099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 12 ] state=[ 0.0091325   0.21638127 -0.03170631 -0.35660099], action=0, reward=1.0, next_state=[ 0.01346012  0.02172411 -0.03883833 -0.0740821 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 13 ] state=[ 0.01346012  0.02172411 -0.03883833 -0.0740821 ], action=0, reward=1.0, next_state=[ 0.0138946  -0.17282015 -0.04031998  0.20609863]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 14 ] state=[ 0.0138946  -0.17282015 -0.04031998  0.20609863], action=0, reward=1.0, next_state=[ 0.0104382  -0.36734302 -0.036198    0.48579494]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 75 ][ timestamp 15 ] state=[ 0.0104382  -0.36734302 -0.036198    0.48579494], action=1, reward=1.0, next_state=[ 0.00309134 -0.17172947 -0.02648211  0.18192693]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 16 ] state=[ 0.00309134 -0.17172947 -0.02648211  0.18192693], action=1, reward=1.0, next_state=[-0.00034325  0.02376121 -0.02284357 -0.1189911 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 17 ] state=[-0.00034325  0.02376121 -0.02284357 -0.1189911 ], action=0, reward=1.0, next_state=[ 1.31974133e-04 -1.71026124e-01 -2.52233886e-02  1.66398244e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 18 ] state=[ 1.31974133e-04 -1.71026124e-01 -2.52233886e-02  1.66398244e-01], action=0, reward=1.0, next_state=[-0.00328855 -0.3657781  -0.02189542  0.4510184 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 19 ] state=[-0.00328855 -0.3657781  -0.02189542  0.4510184 ], action=0, reward=1.0, next_state=[-0.01060411 -0.56058366 -0.01287506  0.73671995]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 20 ] state=[-0.01060411 -0.56058366 -0.01287506  0.73671995], action=1, reward=1.0, next_state=[-0.02181578 -0.36528626  0.00185934  0.44001298]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 21 ] state=[-0.02181578 -0.36528626  0.00185934  0.44001298], action=0, reward=1.0, next_state=[-0.02912151 -0.56043448  0.0106596   0.73328144]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 22 ] state=[-0.02912151 -0.56043448  0.0106596   0.73328144], action=0, reward=1.0, next_state=[-0.0403302  -0.75570208  0.02532523  1.02930007]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 23 ] state=[-0.0403302  -0.75570208  0.02532523  1.02930007], action=1, reward=1.0, next_state=[-0.05544424 -0.56092614  0.04591123  0.74467466]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 24 ] state=[-0.05544424 -0.56092614  0.04591123  0.74467466], action=0, reward=1.0, next_state=[-0.06666276 -0.7566506   0.06080473  1.05144516]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 25 ] state=[-0.06666276 -0.7566506   0.06080473  1.05144516], action=0, reward=1.0, next_state=[-0.08179577 -0.95252397  0.08183363  1.36257781]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 26 ] state=[-0.08179577 -0.95252397  0.08183363  1.36257781], action=1, reward=1.0, next_state=[-0.10084625 -0.75851719  0.10908519  1.09657375]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 27 ] state=[-0.10084625 -0.75851719  0.10908519  1.09657375], action=1, reward=1.0, next_state=[-0.1160166  -0.56498742  0.13101666  0.84001205]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 28 ] state=[-0.1160166  -0.56498742  0.13101666  0.84001205], action=1, reward=1.0, next_state=[-0.12731635 -0.37187424  0.1478169   0.59123367]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 29 ] state=[-0.12731635 -0.37187424  0.1478169   0.59123367], action=1, reward=1.0, next_state=[-0.13475383 -0.1790972   0.15964158  0.34852155]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 30 ] state=[-0.13475383 -0.1790972   0.15964158  0.34852155], action=1, reward=1.0, next_state=[-0.13833578  0.01343681  0.16661201  0.11012836]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 31 ] state=[-0.13833578  0.01343681  0.16661201  0.11012836], action=1, reward=1.0, next_state=[-0.13806704  0.20582805  0.16881457 -0.12570463]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 32 ] state=[-0.13806704  0.20582805  0.16881457 -0.12570463], action=0, reward=1.0, next_state=[-0.13395048  0.00874047  0.16630048  0.21512032]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 33 ] state=[-0.13395048  0.00874047  0.16630048  0.21512032], action=1, reward=1.0, next_state=[-0.13377567  0.20114286  0.17060289 -0.02083437]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 34 ] state=[-0.13377567  0.20114286  0.17060289 -0.02083437], action=0, reward=1.0, next_state=[-0.12975281  0.0040372   0.1701862   0.32044622]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 35 ] state=[-0.12975281  0.0040372   0.1701862   0.32044622], action=1, reward=1.0, next_state=[-0.12967207  0.19637855  0.17659512  0.08589582]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 36 ] state=[-0.12967207  0.19637855  0.17659512  0.08589582], action=1, reward=1.0, next_state=[-0.1257445   0.3885874   0.17831304 -0.14628395]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 37 ] state=[-0.1257445   0.3885874   0.17831304 -0.14628395], action=1, reward=1.0, next_state=[-0.11797275  0.58076726  0.17538736 -0.37783636]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 38 ] state=[-0.11797275  0.58076726  0.17538736 -0.37783636], action=1, reward=1.0, next_state=[-0.1063574   0.77302116  0.16783063 -0.61049324]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 39 ] state=[-0.1063574   0.77302116  0.16783063 -0.61049324], action=1, reward=1.0, next_state=[-0.09089698  0.96544898  0.15562077 -0.84596849]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 40 ] state=[-0.09089698  0.96544898  0.15562077 -0.84596849], action=0, reward=1.0, next_state=[-0.071588    0.76858531  0.1387014  -0.50867343]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 41 ] state=[-0.071588    0.76858531  0.1387014  -0.50867343], action=0, reward=1.0, next_state=[-0.05621629  0.57180972  0.12852793 -0.17569709]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 42 ] state=[-0.05621629  0.57180972  0.12852793 -0.17569709], action=0, reward=1.0, next_state=[-0.0447801   0.37510504  0.12501399  0.15460945]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 43 ] state=[-0.0447801   0.37510504  0.12501399  0.15460945], action=1, reward=1.0, next_state=[-0.037278    0.56823593  0.12810618 -0.09616762]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 44 ] state=[-0.037278    0.56823593  0.12810618 -0.09616762], action=1, reward=1.0, next_state=[-0.02591328  0.76131126  0.12618283 -0.34584712]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 45 ] state=[-0.02591328  0.76131126  0.12618283 -0.34584712], action=1, reward=1.0, next_state=[-0.01068706  0.95443359  0.11926588 -0.59622812]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 46 ] state=[-0.01068706  0.95443359  0.11926588 -0.59622812], action=0, reward=1.0, next_state=[ 0.00840162  0.75786226  0.10734132 -0.2684846 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 47 ] state=[ 0.00840162  0.75786226  0.10734132 -0.2684846 ], action=1, reward=1.0, next_state=[ 0.02355886  0.95130165  0.10197163 -0.52547589]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 48 ] state=[ 0.02355886  0.95130165  0.10197163 -0.52547589], action=1, reward=1.0, next_state=[ 0.04258489  1.14485197  0.09146211 -0.7843655 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 49 ] state=[ 0.04258489  1.14485197  0.09146211 -0.7843655 ], action=1, reward=1.0, next_state=[ 0.06548193  1.33860608  0.0757748  -1.04692952]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 50 ] state=[ 0.06548193  1.33860608  0.0757748  -1.04692952], action=1, reward=1.0, next_state=[ 0.09225406  1.53264504  0.05483621 -1.31489629]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 51 ] state=[ 0.09225406  1.53264504  0.05483621 -1.31489629], action=0, reward=1.0, next_state=[ 0.12290696  1.33687372  0.02853829 -1.00556694]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 52 ] state=[ 0.12290696  1.33687372  0.02853829 -1.00556694], action=0, reward=1.0, next_state=[ 0.14964443  1.14138251  0.00842695 -0.7040604 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 53 ] state=[ 0.14964443  1.14138251  0.00842695 -0.7040604 ], action=0, reward=1.0, next_state=[ 0.17247208  0.94614479 -0.00565426 -0.40873674]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 54 ] state=[ 0.17247208  0.94614479 -0.00565426 -0.40873674], action=1, reward=1.0, next_state=[ 0.19139498  1.14134646 -0.013829   -0.7031969 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 55 ] state=[ 0.19139498  1.14134646 -0.013829   -0.7031969 ], action=0, reward=1.0, next_state=[ 0.21422191  0.94641886 -0.02789293 -0.41489907]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 56 ] state=[ 0.21422191  0.94641886 -0.02789293 -0.41489907], action=1, reward=1.0, next_state=[ 0.23315028  1.14192482 -0.03619092 -0.71624339]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 57 ] state=[ 0.23315028  1.14192482 -0.03619092 -0.71624339], action=1, reward=1.0, next_state=[ 0.25598878  1.3375285  -0.05051578 -1.02009459]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 58 ] state=[ 0.25598878  1.3375285  -0.05051578 -1.02009459], action=0, reward=1.0, next_state=[ 0.28273935  1.1431148  -0.07091768 -0.74369136]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 75 ][ timestamp 59 ] state=[ 0.28273935  1.1431148  -0.07091768 -0.74369136], action=1, reward=1.0, next_state=[ 0.30560165  1.33914016 -0.0857915  -1.05782263]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 60 ] state=[ 0.30560165  1.33914016 -0.0857915  -1.05782263], action=1, reward=1.0, next_state=[ 0.33238445  1.53528754 -0.10694796 -1.37615337]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 61 ] state=[ 0.33238445  1.53528754 -0.10694796 -1.37615337], action=0, reward=1.0, next_state=[ 0.3630902   1.34165192 -0.13447102 -1.11874225]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 62 ] state=[ 0.3630902   1.34165192 -0.13447102 -1.11874225], action=0, reward=1.0, next_state=[ 0.38992324  1.14852561 -0.15684587 -0.87108342]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 63 ] state=[ 0.38992324  1.14852561 -0.15684587 -0.87108342], action=0, reward=1.0, next_state=[ 0.41289375  0.95584471 -0.17426754 -0.6315337 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 64 ] state=[ 0.41289375  0.95584471 -0.17426754 -0.6315337 ], action=0, reward=1.0, next_state=[ 0.43201064  0.7635273  -0.18689821 -0.39840262]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 65 ] state=[ 0.43201064  0.7635273  -0.18689821 -0.39840262], action=0, reward=1.0, next_state=[ 0.44728119  0.57148015 -0.19486626 -0.1699773 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 66 ] state=[ 0.44728119  0.57148015 -0.19486626 -0.1699773 ], action=1, reward=1.0, next_state=[ 0.45871079  0.76877981 -0.19826581 -0.5172543 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 67 ] state=[ 0.45871079  0.76877981 -0.19826581 -0.5172543 ], action=0, reward=1.0, next_state=[ 0.47408639  0.57692058 -0.20861089 -0.29301236]\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 68 ] state=[ 0.47408639  0.57692058 -0.20861089 -0.29301236], action=0, reward=-1.0, next_state=[ 0.4856248   0.38528843 -0.21447114 -0.07268387]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 75: Exploration_rate=0.01. Score=68.\n",
      "[ episode 76 ] state=[-0.00824003 -0.03882844  0.01938159  0.02631033]\n",
      "[ episode 76 ][ timestamp 1 ] state=[-0.00824003 -0.03882844  0.01938159  0.02631033], action=1, reward=1.0, next_state=[-0.0090166   0.15601028  0.0199078  -0.26019503]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 2 ] state=[-0.0090166   0.15601028  0.0199078  -0.26019503], action=0, reward=1.0, next_state=[-0.0058964  -0.03939013  0.0147039   0.0387    ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 3 ] state=[-0.0058964  -0.03939013  0.0147039   0.0387    ], action=1, reward=1.0, next_state=[-0.0066842   0.15551792  0.0154779  -0.24930767]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 4 ] state=[-0.0066842   0.15551792  0.0154779  -0.24930767], action=0, reward=1.0, next_state=[-0.00357384 -0.03982161  0.01049174  0.04821685]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 5 ] state=[-0.00357384 -0.03982161  0.01049174  0.04821685], action=1, reward=1.0, next_state=[-0.00437027  0.15514834  0.01145608 -0.24113746]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 6 ] state=[-0.00437027  0.15514834  0.01145608 -0.24113746], action=0, reward=1.0, next_state=[-0.00126731 -0.04013537  0.00663333  0.05513689]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 7 ] state=[-0.00126731 -0.04013537  0.00663333  0.05513689], action=1, reward=1.0, next_state=[-0.00207001  0.15489084  0.00773607 -0.23544581]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 8 ] state=[-0.00207001  0.15489084  0.00773607 -0.23544581], action=0, reward=1.0, next_state=[ 0.0010278  -0.04034078  0.00302715  0.05966724]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 9 ] state=[ 0.0010278  -0.04034078  0.00302715  0.05966724], action=1, reward=1.0, next_state=[ 2.20988712e-04  1.54737642e-01  4.22049606e-03 -2.32059066e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 10 ] state=[ 2.20988712e-04  1.54737642e-01  4.22049606e-03 -2.32059066e-01], action=1, reward=1.0, next_state=[ 3.31574155e-03  3.49799033e-01 -4.20685255e-04 -5.23407724e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 11 ] state=[ 3.31574155e-03  3.49799033e-01 -4.20685255e-04 -5.23407724e-01], action=0, reward=1.0, next_state=[ 0.01031172  0.15468301 -0.01088884 -0.23085739]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 12 ] state=[ 0.01031172  0.15468301 -0.01088884 -0.23085739], action=0, reward=1.0, next_state=[ 0.01340538 -0.04028167 -0.01550599  0.05837103]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 13 ] state=[ 0.01340538 -0.04028167 -0.01550599  0.05837103], action=1, reward=1.0, next_state=[ 0.01259975  0.15505913 -0.01433857 -0.23916353]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 14 ] state=[ 0.01259975  0.15505913 -0.01433857 -0.23916353], action=0, reward=1.0, next_state=[ 0.01570093 -0.03985508 -0.01912184  0.04896234]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 15 ] state=[ 0.01570093 -0.03985508 -0.01912184  0.04896234], action=1, reward=1.0, next_state=[ 0.01490383  0.15553577 -0.01814259 -0.24969183]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 16 ] state=[ 0.01490383  0.15553577 -0.01814259 -0.24969183], action=1, reward=1.0, next_state=[ 0.01801455  0.35091205 -0.02313643 -0.54804164]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 17 ] state=[ 0.01801455  0.35091205 -0.02313643 -0.54804164], action=0, reward=1.0, next_state=[ 0.02503279  0.15612264 -0.03409726 -0.26273723]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 18 ] state=[ 0.02503279  0.15612264 -0.03409726 -0.26273723], action=1, reward=1.0, next_state=[ 0.02815524  0.35171429 -0.039352   -0.56597682]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 19 ] state=[ 0.02815524  0.35171429 -0.039352   -0.56597682], action=0, reward=1.0, next_state=[ 0.03518952  0.15716586 -0.05067154 -0.28594661]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 20 ] state=[ 0.03518952  0.15716586 -0.05067154 -0.28594661], action=0, reward=1.0, next_state=[ 0.03833284 -0.03719819 -0.05639047 -0.0096658 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 21 ] state=[ 0.03833284 -0.03719819 -0.05639047 -0.0096658 ], action=0, reward=1.0, next_state=[ 0.03758888 -0.23146798 -0.05658379  0.26470568]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 22 ] state=[ 0.03758888 -0.23146798 -0.05658379  0.26470568], action=0, reward=1.0, next_state=[ 0.03295952 -0.42573857 -0.05128968  0.53901843]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 23 ] state=[ 0.03295952 -0.42573857 -0.05128968  0.53901843], action=1, reward=1.0, next_state=[ 0.02444475 -0.22993453 -0.04050931  0.23062604]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 24 ] state=[ 0.02444475 -0.22993453 -0.04050931  0.23062604], action=0, reward=1.0, next_state=[ 0.01984606 -0.42445491 -0.03589679  0.51026076]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 25 ] state=[ 0.01984606 -0.42445491 -0.03589679  0.51026076], action=1, reward=1.0, next_state=[ 0.01135696 -0.22884614 -0.02569157  0.20648524]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 26 ] state=[ 0.01135696 -0.22884614 -0.02569157  0.20648524], action=1, reward=1.0, next_state=[ 0.00678004 -0.03336641 -0.02156187 -0.09419009]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 27 ] state=[ 0.00678004 -0.03336641 -0.02156187 -0.09419009], action=1, reward=1.0, next_state=[ 0.00611271  0.16205784 -0.02344567 -0.39359702]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 28 ] state=[ 0.00611271  0.16205784 -0.02344567 -0.39359702], action=1, reward=1.0, next_state=[ 0.00935386  0.35750451 -0.03131761 -0.69357885]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 29 ] state=[ 0.00935386  0.35750451 -0.03131761 -0.69357885], action=1, reward=1.0, next_state=[ 0.01650395  0.55304659 -0.04518919 -0.99595401]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 30 ] state=[ 0.01650395  0.55304659 -0.04518919 -0.99595401], action=1, reward=1.0, next_state=[ 0.02756489  0.7487428  -0.06510827 -1.30247975]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 31 ] state=[ 0.02756489  0.7487428  -0.06510827 -1.30247975], action=1, reward=1.0, next_state=[ 0.04253974  0.94462751 -0.09115786 -1.61481256]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 76 ][ timestamp 32 ] state=[ 0.04253974  0.94462751 -0.09115786 -1.61481256], action=1, reward=1.0, next_state=[ 0.06143229  1.14069909 -0.12345411 -1.93446211]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 33 ] state=[ 0.06143229  1.14069909 -0.12345411 -1.93446211], action=1, reward=1.0, next_state=[ 0.08424627  1.33690635 -0.16214335 -2.26273645]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 34 ] state=[ 0.08424627  1.33690635 -0.16214335 -2.26273645], action=1, reward=1.0, next_state=[ 0.1109844   1.53313257 -0.20739808 -2.60067663]\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 35 ] state=[ 0.1109844   1.53313257 -0.20739808 -2.60067663], action=1, reward=-1.0, next_state=[ 0.14164705  1.72917738 -0.25941162 -2.94898084]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 76: Exploration_rate=0.01. Score=35.\n",
      "[ episode 77 ] state=[-0.04399404  0.0191766  -0.03907563  0.01551216]\n",
      "[ episode 77 ][ timestamp 1 ] state=[-0.04399404  0.0191766  -0.03907563  0.01551216], action=1, reward=1.0, next_state=[-0.0436105   0.21483652 -0.03876538 -0.289239  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 2 ] state=[-0.0436105   0.21483652 -0.03876538 -0.289239  ], action=1, reward=1.0, next_state=[-0.03931377  0.4104892  -0.04455016 -0.5938917 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 3 ] state=[-0.03931377  0.4104892  -0.04455016 -0.5938917 ], action=1, reward=1.0, next_state=[-0.03110399  0.60620549 -0.056428   -0.90026827]\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 4 ] state=[-0.03110399  0.60620549 -0.056428   -0.90026827], action=0, reward=1.0, next_state=[-0.01897988  0.41189171 -0.07443336 -0.62584255]\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 5 ] state=[-0.01897988  0.41189171 -0.07443336 -0.62584255], action=0, reward=1.0, next_state=[-0.01074205  0.2178834  -0.08695021 -0.35749905]\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 6 ] state=[-0.01074205  0.2178834  -0.08695021 -0.35749905], action=0, reward=1.0, next_state=[-0.00638438  0.02409826 -0.09410019 -0.09345064]\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 7 ] state=[-0.00638438  0.02409826 -0.09410019 -0.09345064], action=0, reward=1.0, next_state=[-0.00590241 -0.1695578  -0.09596921  0.16812367]\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 8 ] state=[-0.00590241 -0.1695578  -0.09596921  0.16812367], action=0, reward=1.0, next_state=[-0.00929357 -0.36318441 -0.09260673  0.42905545]\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 9 ] state=[-0.00929357 -0.36318441 -0.09260673  0.42905545], action=0, reward=1.0, next_state=[-0.01655726 -0.55688123 -0.08402562  0.69116824]\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 10 ] state=[-0.01655726 -0.55688123 -0.08402562  0.69116824], action=0, reward=1.0, next_state=[-0.02769488 -0.75074299 -0.07020226  0.95626047]\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 11 ] state=[-0.02769488 -0.75074299 -0.07020226  0.95626047], action=0, reward=1.0, next_state=[-0.04270974 -0.94485414 -0.05107705  1.22608748]\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 12 ] state=[-0.04270974 -0.94485414 -0.05107705  1.22608748], action=0, reward=1.0, next_state=[-0.06160682 -1.13928264 -0.0265553   1.50233976]\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 13 ] state=[-0.06160682 -1.13928264 -0.0265553   1.50233976], action=0, reward=1.0, next_state=[-0.08439248 -1.33407235  0.00349149  1.78661497]\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 14 ] state=[-0.08439248 -1.33407235  0.00349149  1.78661497], action=0, reward=1.0, next_state=[-0.11107392 -1.52923333  0.03922379  2.08038115]\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 15 ] state=[-0.11107392 -1.52923333  0.03922379  2.08038115], action=1, reward=1.0, next_state=[-0.14165859 -1.33452965  0.08083142  1.80007911]\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 16 ] state=[-0.14165859 -1.33452965  0.08083142  1.80007911], action=1, reward=1.0, next_state=[-0.16834918 -1.14039926  0.116833    1.53357286]\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 17 ] state=[-0.16834918 -1.14039926  0.116833    1.53357286], action=1, reward=1.0, next_state=[-0.19115717 -0.94686262  0.14750446  1.27951778]\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 18 ] state=[-0.19115717 -0.94686262  0.14750446  1.27951778], action=1, reward=1.0, next_state=[-0.21009442 -0.75389578  0.17309481  1.03641992]\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 19 ] state=[-0.21009442 -0.75389578  0.17309481  1.03641992], action=1, reward=1.0, next_state=[-0.22517234 -0.5614442   0.19382321  0.80269254]\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 20 ] state=[-0.22517234 -0.5614442   0.19382321  0.80269254], action=1, reward=-1.0, next_state=[-0.23640122 -0.36943302  0.20987706  0.57669678]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 77: Exploration_rate=0.01. Score=20.\n",
      "[ episode 78 ] state=[-0.02550602 -0.01763589 -0.02416274  0.03924912]\n",
      "[ episode 78 ][ timestamp 1 ] state=[-0.02550602 -0.01763589 -0.02416274  0.03924912], action=1, reward=1.0, next_state=[-0.02585874  0.17782407 -0.02337776 -0.26095839]\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 2 ] state=[-0.02585874  0.17782407 -0.02337776 -0.26095839], action=1, reward=1.0, next_state=[-0.02230226  0.3732718  -0.02859692 -0.56092232]\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 3 ] state=[-0.02230226  0.3732718  -0.02859692 -0.56092232], action=0, reward=1.0, next_state=[-0.01483682  0.17856262 -0.03981537 -0.27738431]\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 4 ] state=[-0.01483682  0.17856262 -0.03981537 -0.27738431], action=1, reward=1.0, next_state=[-0.01126557  0.37422931 -0.04536306 -0.58235436]\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 5 ] state=[-0.01126557  0.37422931 -0.04536306 -0.58235436], action=1, reward=1.0, next_state=[-0.00378099  0.56995648 -0.05701014 -0.88897526]\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 6 ] state=[-0.00378099  0.56995648 -0.05701014 -0.88897526], action=1, reward=1.0, next_state=[ 0.00761814  0.76580382 -0.07478965 -1.1990209 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 7 ] state=[ 0.00761814  0.76580382 -0.07478965 -1.1990209 ], action=0, reward=1.0, next_state=[ 0.02293422  0.57172495 -0.09877007 -0.93068406]\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 8 ] state=[ 0.02293422  0.57172495 -0.09877007 -0.93068406], action=0, reward=1.0, next_state=[ 0.03436872  0.37806477 -0.11738375 -0.67060079]\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 9 ] state=[ 0.03436872  0.37806477 -0.11738375 -0.67060079], action=0, reward=1.0, next_state=[ 0.04193001  0.18475347 -0.13079576 -0.4170609 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 10 ] state=[ 0.04193001  0.18475347 -0.13079576 -0.4170609 ], action=1, reward=1.0, next_state=[ 0.04562508  0.38146294 -0.13913698 -0.74794919]\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 11 ] state=[ 0.04562508  0.38146294 -0.13913698 -0.74794919], action=0, reward=1.0, next_state=[ 0.05325434  0.18850664 -0.15409597 -0.50208623]\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 12 ] state=[ 0.05325434  0.18850664 -0.15409597 -0.50208623], action=1, reward=1.0, next_state=[ 0.05702448  0.38542654 -0.16413769 -0.83909117]\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 13 ] state=[ 0.05702448  0.38542654 -0.16413769 -0.83909117], action=1, reward=1.0, next_state=[ 0.06473301  0.58236342 -0.18091951 -1.17856623]\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 14 ] state=[ 0.06473301  0.58236342 -0.18091951 -1.17856623], action=0, reward=1.0, next_state=[ 0.07638027  0.38999176 -0.20449084 -0.94761903]\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 15 ] state=[ 0.07638027  0.38999176 -0.20449084 -0.94761903], action=1, reward=-1.0, next_state=[ 0.08418011  0.58719241 -0.22344322 -1.29695902]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 78: Exploration_rate=0.01. Score=15.\n",
      "[ episode 79 ] state=[-0.04532565  0.00163806  0.03301772 -0.01301467]\n",
      "[ episode 79 ][ timestamp 1 ] state=[-0.04532565  0.00163806  0.03301772 -0.01301467], action=0, reward=1.0, next_state=[-0.04529289 -0.19394147  0.03275743  0.28990017]\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 2 ] state=[-0.04529289 -0.19394147  0.03275743  0.28990017], action=0, reward=1.0, next_state=[-0.04917172 -0.38951484  0.03855543  0.5927318 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 3 ] state=[-0.04917172 -0.38951484  0.03855543  0.5927318 ], action=0, reward=1.0, next_state=[-0.05696202 -0.5851547   0.05041006  0.897306  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 4 ] state=[-0.05696202 -0.5851547   0.05041006  0.897306  ], action=1, reward=1.0, next_state=[-0.06866511 -0.39075102  0.06835618  0.62088518]\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 5 ] state=[-0.06866511 -0.39075102  0.06835618  0.62088518], action=1, reward=1.0, next_state=[-0.07648013 -0.19664692  0.08077389  0.35049006]\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 6 ] state=[-0.07648013 -0.19664692  0.08077389  0.35049006], action=1, reward=1.0, next_state=[-0.08041307 -0.00276104  0.08778369  0.08433118]\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 7 ] state=[-0.08041307 -0.00276104  0.08778369  0.08433118], action=1, reward=1.0, next_state=[-0.08046829  0.19100006  0.08947031 -0.17941608]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 79 ][ timestamp 8 ] state=[-0.08046829  0.19100006  0.08947031 -0.17941608], action=1, reward=1.0, next_state=[-0.07664829  0.38473535  0.08588199 -0.44258746]\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 9 ] state=[-0.07664829  0.38473535  0.08588199 -0.44258746], action=0, reward=1.0, next_state=[-0.06895358  0.1885098   0.07703024 -0.12411568]\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 10 ] state=[-0.06895358  0.1885098   0.07703024 -0.12411568], action=0, reward=1.0, next_state=[-0.06518339 -0.00762637  0.07454793  0.19184065]\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 11 ] state=[-0.06518339 -0.00762637  0.07454793  0.19184065], action=0, reward=1.0, next_state=[-0.06533591 -0.20373122  0.07838474  0.50707773]\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 12 ] state=[-0.06533591 -0.20373122  0.07838474  0.50707773], action=0, reward=1.0, next_state=[-0.06941054 -0.39986508  0.0885263   0.8233967 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 13 ] state=[-0.06941054 -0.39986508  0.0885263   0.8233967 ], action=1, reward=1.0, next_state=[-0.07740784 -0.20605847  0.10499423  0.55981793]\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 14 ] state=[-0.07740784 -0.20605847  0.10499423  0.55981793], action=0, reward=1.0, next_state=[-0.08152901 -0.40248525  0.11619059  0.88364718]\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 15 ] state=[-0.08152901 -0.40248525  0.11619059  0.88364718], action=1, reward=1.0, next_state=[-0.08957871 -0.20911653  0.13386353  0.62963301]\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 16 ] state=[-0.08957871 -0.20911653  0.13386353  0.62963301], action=1, reward=1.0, next_state=[-0.09376104 -0.01609148  0.14645619  0.3819242 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 17 ] state=[-0.09376104 -0.01609148  0.14645619  0.3819242 ], action=0, reward=1.0, next_state=[-0.09408287 -0.21295639  0.15409468  0.71696459]\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 18 ] state=[-0.09408287 -0.21295639  0.15409468  0.71696459], action=1, reward=1.0, next_state=[-0.098342   -0.02026474  0.16843397  0.47647671]\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 19 ] state=[-0.098342   -0.02026474  0.16843397  0.47647671], action=0, reward=1.0, next_state=[-0.0987473  -0.21731453  0.1779635   0.81715434]\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 20 ] state=[-0.0987473  -0.21731453  0.1779635   0.81715434], action=1, reward=1.0, next_state=[-0.10309359 -0.0250169   0.19430659  0.58530909]\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 21 ] state=[-0.10309359 -0.0250169   0.19430659  0.58530909], action=1, reward=1.0, next_state=[-0.10359393  0.16692893  0.20601277  0.3595758 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 22 ] state=[-0.10359393  0.16692893  0.20601277  0.3595758 ], action=1, reward=-1.0, next_state=[-0.10025535  0.35861814  0.21320429  0.13826232]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 79: Exploration_rate=0.01. Score=22.\n",
      "[ episode 80 ] state=[ 0.04893084  0.04597589 -0.00307883 -0.01730199]\n",
      "[ episode 80 ][ timestamp 1 ] state=[ 0.04893084  0.04597589 -0.00307883 -0.01730199], action=1, reward=1.0, next_state=[ 0.04985036  0.24114186 -0.00342487 -0.31095473]\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 2 ] state=[ 0.04985036  0.24114186 -0.00342487 -0.31095473], action=1, reward=1.0, next_state=[ 0.05467319  0.43631243 -0.00964397 -0.60471579]\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 3 ] state=[ 0.05467319  0.43631243 -0.00964397 -0.60471579], action=1, reward=1.0, next_state=[ 0.06339944  0.63156792 -0.02173828 -0.90042068]\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 4 ] state=[ 0.06339944  0.63156792 -0.02173828 -0.90042068], action=0, reward=1.0, next_state=[ 0.0760308   0.43674717 -0.0397467  -0.61464915]\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 5 ] state=[ 0.0760308   0.43674717 -0.0397467  -0.61464915], action=1, reward=1.0, next_state=[ 0.08476574  0.63240129 -0.05203968 -0.91958101]\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 6 ] state=[ 0.08476574  0.63240129 -0.05203968 -0.91958101], action=0, reward=1.0, next_state=[ 0.09741377  0.43801989 -0.0704313  -0.64369638]\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 7 ] state=[ 0.09741377  0.43801989 -0.0704313  -0.64369638], action=0, reward=1.0, next_state=[ 0.10617417  0.24394657 -0.08330523 -0.37399782]\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 8 ] state=[ 0.10617417  0.24394657 -0.08330523 -0.37399782], action=1, reward=1.0, next_state=[ 0.1110531   0.44014697 -0.09078519 -0.69174124]\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 9 ] state=[ 0.1110531   0.44014697 -0.09078519 -0.69174124], action=0, reward=1.0, next_state=[ 0.11985604  0.24639407 -0.10462001 -0.42896294]\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 10 ] state=[ 0.11985604  0.24639407 -0.10462001 -0.42896294], action=0, reward=1.0, next_state=[ 0.12478392  0.05289725 -0.11319927 -0.17100689]\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 11 ] state=[ 0.12478392  0.05289725 -0.11319927 -0.17100689], action=0, reward=1.0, next_state=[ 0.12584187 -0.14043781 -0.11661941  0.08393007]\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 12 ] state=[ 0.12584187 -0.14043781 -0.11661941  0.08393007], action=1, reward=1.0, next_state=[ 0.12303311  0.05614597 -0.11494081 -0.24315114]\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 13 ] state=[ 0.12303311  0.05614597 -0.11494081 -0.24315114], action=0, reward=1.0, next_state=[ 0.12415603 -0.13716261 -0.11980383  0.01118019]\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 14 ] state=[ 0.12415603 -0.13716261 -0.11980383  0.01118019], action=1, reward=1.0, next_state=[ 0.12141278  0.05945559 -0.11958022 -0.31677123]\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 15 ] state=[ 0.12141278  0.05945559 -0.11958022 -0.31677123], action=1, reward=1.0, next_state=[ 0.12260189  0.25605979 -0.12591565 -0.6446444 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 16 ] state=[ 0.12260189  0.25605979 -0.12591565 -0.6446444 ], action=0, reward=1.0, next_state=[ 0.12772308  0.06289662 -0.13880854 -0.39411498]\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 17 ] state=[ 0.12772308  0.06289662 -0.13880854 -0.39411498], action=1, reward=1.0, next_state=[ 0.12898102  0.25968718 -0.14669084 -0.72714038]\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 18 ] state=[ 0.12898102  0.25968718 -0.14669084 -0.72714038], action=0, reward=1.0, next_state=[ 0.13417476  0.06686497 -0.16123364 -0.48398599]\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 19 ] state=[ 0.13417476  0.06686497 -0.16123364 -0.48398599], action=1, reward=1.0, next_state=[ 0.13551206  0.26385123 -0.17091336 -0.82283058]\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 20 ] state=[ 0.13551206  0.26385123 -0.17091336 -0.82283058], action=1, reward=1.0, next_state=[ 0.14078908  0.46084754 -0.18736998 -1.1640239 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 21 ] state=[ 0.14078908  0.46084754 -0.18736998 -1.1640239 ], action=0, reward=-1.0, next_state=[ 0.15000603  0.26859218 -0.21065045 -0.93545328]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 80: Exploration_rate=0.01. Score=21.\n",
      "[ episode 81 ] state=[-0.01291827  0.04731144 -0.04755432  0.02834216]\n",
      "[ episode 81 ][ timestamp 1 ] state=[-0.01291827  0.04731144 -0.04755432  0.02834216], action=0, reward=1.0, next_state=[-0.01197204 -0.14709743 -0.04698748  0.3056501 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 2 ] state=[-0.01197204 -0.14709743 -0.04698748  0.3056501 ], action=1, reward=1.0, next_state=[-0.01491399  0.04866151 -0.04087448 -0.00147346]\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 3 ] state=[-0.01491399  0.04866151 -0.04087448 -0.00147346], action=0, reward=1.0, next_state=[-0.01394076 -0.14585112 -0.04090395  0.27803805]\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 4 ] state=[-0.01394076 -0.14585112 -0.04090395  0.27803805], action=1, reward=1.0, next_state=[-0.01685778  0.04982977 -0.03534318 -0.02726018]\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 5 ] state=[-0.01685778  0.04982977 -0.03534318 -0.02726018], action=1, reward=1.0, next_state=[-0.01586118  0.24544027 -0.03588839 -0.33088143]\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 6 ] state=[-0.01586118  0.24544027 -0.03588839 -0.33088143], action=0, reward=1.0, next_state=[-0.01095238  0.05084707 -0.04250602 -0.0497285 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 7 ] state=[-0.01095238  0.05084707 -0.04250602 -0.0497285 ], action=0, reward=1.0, next_state=[-0.00993544 -0.14364042 -0.04350059  0.22924622]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 81 ][ timestamp 8 ] state=[-0.00993544 -0.14364042 -0.04350059  0.22924622], action=1, reward=1.0, next_state=[-0.01280824  0.05207529 -0.03891566 -0.07683477]\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 9 ] state=[-0.01280824  0.05207529 -0.03891566 -0.07683477], action=1, reward=1.0, next_state=[-0.01176674  0.24773289 -0.04045236 -0.38153728]\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 10 ] state=[-0.01176674  0.24773289 -0.04045236 -0.38153728], action=1, reward=1.0, next_state=[-0.00681208  0.4434052  -0.0480831  -0.68669537]\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 11 ] state=[-0.00681208  0.4434052  -0.0480831  -0.68669537], action=0, reward=1.0, next_state=[ 0.00205602  0.24898252 -0.06181701 -0.4095294 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 12 ] state=[ 0.00205602  0.24898252 -0.06181701 -0.4095294 ], action=1, reward=1.0, next_state=[ 0.00703567  0.44492394 -0.0700076  -0.72104276]\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 13 ] state=[ 0.00703567  0.44492394 -0.0700076  -0.72104276], action=1, reward=1.0, next_state=[ 0.01593415  0.64094096 -0.08442845 -1.0349135 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 14 ] state=[ 0.01593415  0.64094096 -0.08442845 -1.0349135 ], action=0, reward=1.0, next_state=[ 0.02875297  0.44703687 -0.10512672 -0.76988587]\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 15 ] state=[ 0.02875297  0.44703687 -0.10512672 -0.76988587], action=1, reward=1.0, next_state=[ 0.03769371  0.64343649 -0.12052444 -1.09370927]\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 16 ] state=[ 0.03769371  0.64343649 -0.12052444 -1.09370927], action=1, reward=1.0, next_state=[ 0.05056244  0.8399221  -0.14239863 -1.4216481 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 17 ] state=[ 0.05056244  0.8399221  -0.14239863 -1.4216481 ], action=0, reward=1.0, next_state=[ 0.06736088  0.64681961 -0.17083159 -1.17664996]\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 18 ] state=[ 0.06736088  0.64681961 -0.17083159 -1.17664996], action=0, reward=1.0, next_state=[ 0.08029727  0.45427778 -0.19436459 -0.9420218 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 19 ] state=[ 0.08029727  0.45427778 -0.19436459 -0.9420218 ], action=0, reward=-1.0, next_state=[ 0.08938283  0.26223062 -0.21320502 -0.71615934]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 81: Exploration_rate=0.01. Score=19.\n",
      "[ episode 82 ] state=[ 0.01354198  0.04655105 -0.00500404  0.00519953]\n",
      "[ episode 82 ][ timestamp 1 ] state=[ 0.01354198  0.04655105 -0.00500404  0.00519953], action=0, reward=1.0, next_state=[ 0.014473   -0.14849878 -0.00490005  0.29629943]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 2 ] state=[ 0.014473   -0.14849878 -0.00490005  0.29629943], action=1, reward=1.0, next_state=[0.01150302 0.04669268 0.00102594 0.00207514]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 3 ] state=[0.01150302 0.04669268 0.00102594 0.00207514], action=0, reward=1.0, next_state=[ 0.01243687 -0.14844397  0.00106744  0.29508159]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 4 ] state=[ 0.01243687 -0.14844397  0.00106744  0.29508159], action=1, reward=1.0, next_state=[0.00946799 0.04666275 0.00696907 0.0027355 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 5 ] state=[0.00946799 0.04666275 0.00696907 0.0027355 ], action=1, reward=1.0, next_state=[ 0.01040125  0.24168406  0.00702378 -0.28774047]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 6 ] state=[ 0.01040125  0.24168406  0.00702378 -0.28774047], action=1, reward=1.0, next_state=[ 0.01523493  0.43670515  0.00126897 -0.57819991]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 7 ] state=[ 0.01523493  0.43670515  0.00126897 -0.57819991], action=1, reward=1.0, next_state=[ 0.02396903  0.63180929 -0.01029503 -0.87048282]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 8 ] state=[ 0.02396903  0.63180929 -0.01029503 -0.87048282], action=0, reward=1.0, next_state=[ 0.03660522  0.43682888 -0.02770468 -0.58105438]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 9 ] state=[ 0.03660522  0.43682888 -0.02770468 -0.58105438], action=1, reward=1.0, next_state=[ 0.0453418   0.63232785 -0.03932577 -0.88233444]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 10 ] state=[ 0.0453418   0.63232785 -0.03932577 -0.88233444], action=0, reward=1.0, next_state=[ 0.05798835  0.43776145 -0.05697246 -0.60226928]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 11 ] state=[ 0.05798835  0.43776145 -0.05697246 -0.60226928], action=0, reward=1.0, next_state=[ 0.06674358  0.24348074 -0.06901785 -0.32806189]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 12 ] state=[ 0.06674358  0.24348074 -0.06901785 -0.32806189], action=0, reward=1.0, next_state=[ 0.0716132   0.04940576 -0.07557908 -0.05791764]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 13 ] state=[ 0.0716132   0.04940576 -0.07557908 -0.05791764], action=1, reward=1.0, next_state=[ 0.07260131  0.24552544 -0.07673744 -0.37345646]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 14 ] state=[ 0.07260131  0.24552544 -0.07673744 -0.37345646], action=1, reward=1.0, next_state=[ 0.07751182  0.44164884 -0.08420657 -0.68931447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 15 ] state=[ 0.07751182  0.44164884 -0.08420657 -0.68931447], action=0, reward=1.0, next_state=[ 0.0863448   0.24779018 -0.09799285 -0.42428432]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 16 ] state=[ 0.0863448   0.24779018 -0.09799285 -0.42428432], action=0, reward=1.0, next_state=[ 0.0913006   0.05418303 -0.10647854 -0.16403063]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 17 ] state=[ 0.0913006   0.05418303 -0.10647854 -0.16403063], action=1, reward=1.0, next_state=[ 0.09238426  0.25065536 -0.10975915 -0.48831562]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 18 ] state=[ 0.09238426  0.25065536 -0.10975915 -0.48831562], action=0, reward=1.0, next_state=[ 0.09739737  0.05723923 -0.11952547 -0.23214168]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 19 ] state=[ 0.09739737  0.05723923 -0.11952547 -0.23214168], action=0, reward=1.0, next_state=[ 0.09854215 -0.13599007 -0.1241683   0.02057745]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 20 ] state=[ 0.09854215 -0.13599007 -0.1241683   0.02057745], action=0, reward=1.0, next_state=[ 0.09582235 -0.32913279 -0.12375675  0.27164928]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 21 ] state=[ 0.09582235 -0.32913279 -0.12375675  0.27164928], action=1, reward=1.0, next_state=[ 0.0892397  -0.1324822  -0.11832377 -0.05736229]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 22 ] state=[ 0.0892397  -0.1324822  -0.11832377 -0.05736229], action=0, reward=1.0, next_state=[ 0.08659005 -0.32572634 -0.11947101  0.19577109]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 23 ] state=[ 0.08659005 -0.32572634 -0.11947101  0.19577109], action=1, reward=1.0, next_state=[ 0.08007553 -0.12911608 -0.11555559 -0.13208308]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 24 ] state=[ 0.08007553 -0.12911608 -0.11555559 -0.13208308], action=0, reward=1.0, next_state=[ 0.07749321 -0.32240945 -0.11819725  0.12202555]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 25 ] state=[ 0.07749321 -0.32240945 -0.11819725  0.12202555], action=1, reward=1.0, next_state=[ 0.07104502 -0.12580987 -0.11575674 -0.20548539]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 26 ] state=[ 0.07104502 -0.12580987 -0.11575674 -0.20548539], action=1, reward=1.0, next_state=[ 0.06852882  0.07076078 -0.11986645 -0.53232462]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 27 ] state=[ 0.06852882  0.07076078 -0.11986645 -0.53232462], action=0, reward=1.0, next_state=[ 0.06994403 -0.12248939 -0.13051294 -0.27968574]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 28 ] state=[ 0.06994403 -0.12248939 -0.13051294 -0.27968574], action=0, reward=1.0, next_state=[ 0.06749425 -0.31553147 -0.13610665 -0.03084724]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 29 ] state=[ 0.06749425 -0.31553147 -0.13610665 -0.03084724], action=1, reward=1.0, next_state=[ 0.06118362 -0.11874679 -0.1367236  -0.36318632]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 30 ] state=[ 0.06118362 -0.11874679 -0.1367236  -0.36318632], action=0, reward=1.0, next_state=[ 0.05880868 -0.31168755 -0.14398733 -0.11654763]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 31 ] state=[ 0.05880868 -0.31168755 -0.14398733 -0.11654763], action=0, reward=1.0, next_state=[ 0.05257493 -0.50448438 -0.14631828  0.1274688 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 32 ] state=[ 0.05257493 -0.50448438 -0.14631828  0.1274688 ], action=0, reward=1.0, next_state=[ 0.04248524 -0.69724024 -0.1437689   0.37064882]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 33 ] state=[ 0.04248524 -0.69724024 -0.1437689   0.37064882], action=1, reward=1.0, next_state=[ 0.02854044 -0.50039958 -0.13635593  0.03631144]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 34 ] state=[ 0.02854044 -0.50039958 -0.13635593  0.03631144], action=1, reward=1.0, next_state=[ 0.01853245 -0.30361248 -0.1356297  -0.29609386]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 35 ] state=[ 0.01853245 -0.30361248 -0.1356297  -0.29609386], action=0, reward=1.0, next_state=[ 0.0124602  -0.49656651 -0.14155157 -0.04907383]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 82 ][ timestamp 36 ] state=[ 0.0124602  -0.49656651 -0.14155157 -0.04907383], action=1, reward=1.0, next_state=[ 0.00252887 -0.2997285  -0.14253305 -0.38285509]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 37 ] state=[ 0.00252887 -0.2997285  -0.14253305 -0.38285509], action=0, reward=1.0, next_state=[-0.0034657  -0.49256948 -0.15019015 -0.13828989]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 38 ] state=[-0.0034657  -0.49256948 -0.15019015 -0.13828989], action=1, reward=1.0, next_state=[-0.01331709 -0.2956512  -0.15295595 -0.47433223]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 39 ] state=[-0.01331709 -0.2956512  -0.15295595 -0.47433223], action=0, reward=1.0, next_state=[-0.01923012 -0.48831978 -0.1624426  -0.23349737]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 40 ] state=[-0.01923012 -0.48831978 -0.1624426  -0.23349737], action=1, reward=1.0, next_state=[-0.02899651 -0.29129462 -0.16711254 -0.57269279]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 41 ] state=[-0.02899651 -0.29129462 -0.16711254 -0.57269279], action=0, reward=1.0, next_state=[-0.0348224  -0.48372795 -0.1785664  -0.33696665]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 42 ] state=[-0.0348224  -0.48372795 -0.1785664  -0.33696665], action=0, reward=1.0, next_state=[-0.04449696 -0.67591911 -0.18530573 -0.10548383]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 43 ] state=[-0.04449696 -0.67591911 -0.18530573 -0.10548383], action=0, reward=1.0, next_state=[-0.05801535 -0.86796889 -0.18741541  0.12349036]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 44 ] state=[-0.05801535 -0.86796889 -0.18741541  0.12349036], action=0, reward=1.0, next_state=[-0.07537472 -1.05998056 -0.1849456   0.35168628]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 45 ] state=[-0.07537472 -1.05998056 -0.1849456   0.35168628], action=1, reward=1.0, next_state=[-0.09657433 -0.86277644 -0.17791188  0.00686014]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 46 ] state=[-0.09657433 -0.86277644 -0.17791188  0.00686014], action=1, reward=1.0, next_state=[-0.11382986 -0.66560833 -0.17777467 -0.33625427]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 47 ] state=[-0.11382986 -0.66560833 -0.17777467 -0.33625427], action=0, reward=1.0, next_state=[-0.12714203 -0.85781381 -0.18449976 -0.10448078]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 48 ] state=[-0.12714203 -0.85781381 -0.18449976 -0.10448078], action=0, reward=1.0, next_state=[-0.14429831 -1.04987838 -0.18658937  0.12479082]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 49 ] state=[-0.14429831 -1.04987838 -0.18658937  0.12479082], action=0, reward=1.0, next_state=[-0.16529587 -1.24190533 -0.18409356  0.35329211]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 50 ] state=[-0.16529587 -1.24190533 -0.18409356  0.35329211], action=1, reward=1.0, next_state=[-0.19013398 -1.04470826 -0.17702772  0.00867637]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 51 ] state=[-0.19013398 -1.04470826 -0.17702772  0.00867637], action=1, reward=1.0, next_state=[-0.21102815 -0.84754759 -0.17685419 -0.33421736]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 52 ] state=[-0.21102815 -0.84754759 -0.17685419 -0.33421736], action=0, reward=1.0, next_state=[-0.2279791  -1.03976958 -0.18353853 -0.1021063 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 53 ] state=[-0.2279791  -1.03976958 -0.18353853 -0.1021063 ], action=0, reward=1.0, next_state=[-0.24877449 -1.23185173 -0.18558066  0.12751973]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 54 ] state=[-0.24877449 -1.23185173 -0.18558066  0.12751973], action=1, reward=1.0, next_state=[-0.27341152 -1.03462323 -0.18303027 -0.21749125]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 55 ] state=[-0.27341152 -1.03462323 -0.18303027 -0.21749125], action=0, reward=1.0, next_state=[-0.29410399 -1.22672125 -0.18738009  0.01233181]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 56 ] state=[-0.29410399 -1.22672125 -0.18738009  0.01233181], action=0, reward=1.0, next_state=[-0.31863841 -1.41873084 -0.18713346  0.24053675]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 57 ] state=[-0.31863841 -1.41873084 -0.18713346  0.24053675], action=0, reward=1.0, next_state=[-0.34701303 -1.61075542 -0.18232272  0.46884827]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 58 ] state=[-0.34701303 -1.61075542 -0.18232272  0.46884827], action=0, reward=1.0, next_state=[-0.37922814 -1.80289686 -0.17294575  0.69897696]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 59 ] state=[-0.37922814 -1.80289686 -0.17294575  0.69897696], action=0, reward=1.0, next_state=[-0.41528608 -1.9952529  -0.15896622  0.93261374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 60 ] state=[-0.41528608 -1.9952529  -0.15896622  0.93261374], action=1, reward=1.0, next_state=[-0.45519113 -1.79838464 -0.14031394  0.59449521]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 61 ] state=[-0.45519113 -1.79838464 -0.14031394  0.59449521], action=1, reward=1.0, next_state=[-0.49115883 -1.60160653 -0.12842404  0.26111184]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 62 ] state=[-0.49115883 -1.60160653 -0.12842404  0.26111184], action=1, reward=1.0, next_state=[-0.52319096 -1.40490756 -0.1232018  -0.06915984]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 63 ] state=[-0.52319096 -1.40490756 -0.1232018  -0.06915984], action=1, reward=1.0, next_state=[-0.55128911 -1.20825443 -0.124585   -0.39803344]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 64 ] state=[-0.55128911 -1.20825443 -0.124585   -0.39803344], action=0, reward=1.0, next_state=[-0.5754542  -1.40140905 -0.13254567 -0.14708044]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 65 ] state=[-0.5754542  -1.40140905 -0.13254567 -0.14708044], action=1, reward=1.0, next_state=[-0.60348238 -1.20466261 -0.13548727 -0.47846593]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 66 ] state=[-0.60348238 -1.20466261 -0.13548727 -0.47846593], action=0, reward=1.0, next_state=[-0.62757563 -1.39763772 -0.14505659 -0.23136751]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 67 ] state=[-0.62757563 -1.39763772 -0.14505659 -0.23136751], action=0, reward=1.0, next_state=[-0.65552838 -1.59042122 -0.14968394  0.0122735 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 68 ] state=[-0.65552838 -1.59042122 -0.14968394  0.0122735 ], action=1, reward=1.0, next_state=[-0.68733681 -1.39350494 -0.14943847 -0.32364104]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 69 ] state=[-0.68733681 -1.39350494 -0.14943847 -0.32364104], action=0, reward=1.0, next_state=[-0.71520691 -1.58621815 -0.15591129 -0.08156454]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 70 ] state=[-0.71520691 -1.58621815 -0.15591129 -0.08156454], action=0, reward=1.0, next_state=[-0.74693127 -1.77880139 -0.15754258  0.15815397]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 71 ] state=[-0.74693127 -1.77880139 -0.15754258  0.15815397], action=1, reward=1.0, next_state=[-0.7825073  -1.58181587 -0.1543795  -0.17979121]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 72 ] state=[-0.7825073  -1.58181587 -0.1543795  -0.17979121], action=0, reward=1.0, next_state=[-0.81414362 -1.77443042 -0.15797533  0.060487  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 73 ] state=[-0.81414362 -1.77443042 -0.15797533  0.060487  ], action=0, reward=1.0, next_state=[-0.84963222 -1.96697615 -0.15676559  0.29945737]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 74 ] state=[-0.84963222 -1.96697615 -0.15676559  0.29945737], action=1, reward=1.0, next_state=[-0.88897175 -1.77000756 -0.15077644 -0.03827303]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 75 ] state=[-0.88897175 -1.77000756 -0.15077644 -0.03827303], action=0, reward=1.0, next_state=[-0.9243719  -1.96268192 -0.1515419   0.20329908]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 76 ] state=[-0.9243719  -1.96268192 -0.1515419   0.20329908], action=1, reward=1.0, next_state=[-0.96362554 -1.76575423 -0.14747592 -0.1330901 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 77 ] state=[-0.96362554 -1.76575423 -0.14747592 -0.1330901 ], action=1, reward=1.0, next_state=[-0.99894062 -1.5688614  -0.15013772 -0.4684244 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 82 ][ timestamp 78 ] state=[-0.99894062 -1.5688614  -0.15013772 -0.4684244 ], action=0, reward=1.0, next_state=[-1.03031785 -1.76157891 -0.15950621 -0.22657495]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 79 ] state=[-1.03031785 -1.76157891 -0.15950621 -0.22657495], action=1, reward=1.0, next_state=[-1.06554943 -1.5645796  -0.16403771 -0.56501902]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 80 ] state=[-1.06554943 -1.5645796  -0.16403771 -0.56501902], action=0, reward=1.0, next_state=[-1.09684102 -1.75706625 -0.17533809 -0.32817607]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 81 ] state=[-1.09684102 -1.75706625 -0.17533809 -0.32817607], action=0, reward=1.0, next_state=[-1.13198234 -1.94931509 -0.18190161 -0.09550992]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 82 ] state=[-1.13198234 -1.94931509 -0.18190161 -0.09550992], action=1, reward=1.0, next_state=[-1.17096865 -1.75211538 -0.18381181 -0.43961388]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 83 ] state=[-1.17096865 -1.75211538 -0.18381181 -0.43961388], action=0, reward=1.0, next_state=[-1.20601095 -1.94422503 -0.19260409 -0.21004066]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 84 ] state=[-1.20601095 -1.94422503 -0.19260409 -0.21004066], action=1, reward=1.0, next_state=[-1.24489545 -1.7469458  -0.1968049  -0.55676385]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 85 ] state=[-1.24489545 -1.7469458  -0.1968049  -0.55676385], action=0, reward=1.0, next_state=[-1.27983437 -1.93883988 -0.20794018 -0.33196697]\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 86 ] state=[-1.27983437 -1.93883988 -0.20794018 -0.33196697], action=1, reward=-1.0, next_state=[-1.31861117 -1.74145871 -0.21457952 -0.6823556 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 82: Exploration_rate=0.01. Score=86.\n",
      "[ episode 83 ] state=[-0.00134041 -0.02637524  0.02580936  0.03886123]\n",
      "[ episode 83 ][ timestamp 1 ] state=[-0.00134041 -0.02637524  0.02580936  0.03886123], action=0, reward=1.0, next_state=[-0.00186791 -0.22185761  0.02658658  0.33957423]\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 2 ] state=[-0.00186791 -0.22185761  0.02658658  0.33957423], action=0, reward=1.0, next_state=[-0.00630506 -0.41734757  0.03337807  0.64052108]\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 3 ] state=[-0.00630506 -0.41734757  0.03337807  0.64052108], action=1, reward=1.0, next_state=[-0.01465201 -0.22270646  0.04618849  0.35853337]\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 4 ] state=[-0.01465201 -0.22270646  0.04618849  0.35853337], action=1, reward=1.0, next_state=[-0.01910614 -0.02827052  0.05335916  0.08076509]\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 5 ] state=[-0.01910614 -0.02827052  0.05335916  0.08076509], action=1, reward=1.0, next_state=[-0.01967155  0.16604753  0.05497446 -0.19461699]\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 6 ] state=[-0.01967155  0.16604753  0.05497446 -0.19461699], action=1, reward=1.0, next_state=[-0.0163506   0.36034177  0.05108212 -0.4694637 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 7 ] state=[-0.0163506   0.36034177  0.05108212 -0.4694637 ], action=1, reward=1.0, next_state=[-0.00914377  0.5547063   0.04169285 -0.74561859]\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 8 ] state=[-0.00914377  0.5547063   0.04169285 -0.74561859], action=0, reward=1.0, next_state=[ 0.00195036  0.35903459  0.02678047 -0.44011194]\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 9 ] state=[ 0.00195036  0.35903459  0.02678047 -0.44011194], action=1, reward=1.0, next_state=[ 0.00913105  0.55376749  0.01797824 -0.72423404]\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 10 ] state=[ 0.00913105  0.55376749  0.01797824 -0.72423404], action=1, reward=1.0, next_state=[ 0.0202064   0.74863626  0.00349355 -1.01120463]\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 11 ] state=[ 0.0202064   0.74863626  0.00349355 -1.01120463], action=1, reward=1.0, next_state=[ 0.03517913  0.94371142 -0.01673054 -1.30278849]\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 12 ] state=[ 0.03517913  0.94371142 -0.01673054 -1.30278849], action=1, reward=1.0, next_state=[ 0.05405335  1.13904156 -0.04278631 -1.60066125]\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 13 ] state=[ 0.05405335  1.13904156 -0.04278631 -1.60066125], action=1, reward=1.0, next_state=[ 0.07683418  1.33464328 -0.07479953 -1.90637064]\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 14 ] state=[ 0.07683418  1.33464328 -0.07479953 -1.90637064], action=1, reward=1.0, next_state=[ 0.10352705  1.53048898 -0.11292695 -2.22128832]\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 15 ] state=[ 0.10352705  1.53048898 -0.11292695 -2.22128832], action=1, reward=1.0, next_state=[ 0.13413683  1.72649217 -0.15735271 -2.54655045]\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 16 ] state=[ 0.13413683  1.72649217 -0.15735271 -2.54655045], action=1, reward=1.0, next_state=[ 0.16866667  1.92249013 -0.20828372 -2.88298625]\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 17 ] state=[ 0.16866667  1.92249013 -0.20828372 -2.88298625], action=1, reward=-1.0, next_state=[ 0.20711648  2.11822437 -0.26594345 -3.23103571]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 83: Exploration_rate=0.01. Score=17.\n",
      "[ episode 84 ] state=[-0.02852417 -0.03008178 -0.04984408  0.00440854]\n",
      "[ episode 84 ][ timestamp 1 ] state=[-0.02852417 -0.03008178 -0.04984408  0.00440854], action=1, reward=1.0, next_state=[-0.02912581  0.16571826 -0.04975591 -0.30357485]\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 2 ] state=[-0.02912581  0.16571826 -0.04975591 -0.30357485], action=1, reward=1.0, next_state=[-0.02581144  0.3615127  -0.05582741 -0.61152525]\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 3 ] state=[-0.02581144  0.3615127  -0.05582741 -0.61152525], action=1, reward=1.0, next_state=[-0.01858119  0.55736866 -0.06805791 -0.92125622]\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 4 ] state=[-0.01858119  0.55736866 -0.06805791 -0.92125622], action=1, reward=1.0, next_state=[-0.00743381  0.75334102 -0.08648304 -1.23452782]\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 5 ] state=[-0.00743381  0.75334102 -0.08648304 -1.23452782], action=1, reward=1.0, next_state=[ 0.00763301  0.9494616  -0.1111736  -1.55300357]\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 6 ] state=[ 0.00763301  0.9494616  -0.1111736  -1.55300357], action=1, reward=1.0, next_state=[ 0.02662224  1.14572702 -0.14223367 -1.87820201]\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 7 ] state=[ 0.02662224  1.14572702 -0.14223367 -1.87820201], action=1, reward=1.0, next_state=[ 0.04953678  1.34208484 -0.17979771 -2.2114403 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 8 ] state=[ 0.04953678  1.34208484 -0.17979771 -2.2114403 ], action=0, reward=-1.0, next_state=[ 0.07637848  1.14908463 -0.22402651 -1.97918294]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 84: Exploration_rate=0.01. Score=8.\n",
      "[ episode 85 ] state=[ 0.00339141  0.02911781  0.00592788 -0.033841  ]\n",
      "[ episode 85 ][ timestamp 1 ] state=[ 0.00339141  0.02911781  0.00592788 -0.033841  ], action=1, reward=1.0, next_state=[ 0.00397377  0.22415425  0.00525106 -0.32464773]\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 2 ] state=[ 0.00397377  0.22415425  0.00525106 -0.32464773], action=1, reward=1.0, next_state=[ 0.00845685  0.41920104 -0.00124189 -0.61567008]\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 3 ] state=[ 0.00845685  0.41920104 -0.00124189 -0.61567008], action=1, reward=1.0, next_state=[ 0.01684087  0.61434032 -0.01355529 -0.90874389]\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 4 ] state=[ 0.01684087  0.61434032 -0.01355529 -0.90874389], action=1, reward=1.0, next_state=[ 0.02912768  0.80964311 -0.03173017 -1.20565629]\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 5 ] state=[ 0.02912768  0.80964311 -0.03173017 -1.20565629], action=1, reward=1.0, next_state=[ 0.04532054  1.00516042 -0.0558433  -1.50811174]\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 6 ] state=[ 0.04532054  1.00516042 -0.0558433  -1.50811174], action=1, reward=1.0, next_state=[ 0.06542375  1.20091311 -0.08600553 -1.81769246]\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 7 ] state=[ 0.06542375  1.20091311 -0.08600553 -1.81769246], action=1, reward=1.0, next_state=[ 0.08944201  1.39687974 -0.12235938 -2.13581037]\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 8 ] state=[ 0.08944201  1.39687974 -0.12235938 -2.13581037], action=1, reward=1.0, next_state=[ 0.11737961  1.59298209 -0.16507559 -2.46364859]\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 9 ] state=[ 0.11737961  1.59298209 -0.16507559 -2.46364859], action=1, reward=-1.0, next_state=[ 0.14923925  1.78906819 -0.21434856 -2.80209143]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 85: Exploration_rate=0.01. Score=9.\n",
      "[ episode 86 ] state=[ 0.01864208 -0.03335847 -0.00171924 -0.04238407]\n",
      "[ episode 86 ][ timestamp 1 ] state=[ 0.01864208 -0.03335847 -0.00171924 -0.04238407], action=1, reward=1.0, next_state=[ 0.01797491  0.16178809 -0.00256692 -0.33560894]\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 2 ] state=[ 0.01797491  0.16178809 -0.00256692 -0.33560894], action=1, reward=1.0, next_state=[ 0.02121067  0.35694648 -0.0092791  -0.62910023]\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 3 ] state=[ 0.02121067  0.35694648 -0.0092791  -0.62910023], action=1, reward=1.0, next_state=[ 0.0283496   0.55219668 -0.02186111 -0.92469095]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 86 ][ timestamp 4 ] state=[ 0.0283496   0.55219668 -0.02186111 -0.92469095], action=1, reward=1.0, next_state=[ 0.03939353  0.74760699 -0.04035492 -1.22416302]\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 5 ] state=[ 0.03939353  0.74760699 -0.04035492 -1.22416302], action=1, reward=1.0, next_state=[ 0.05434567  0.94322478 -0.06483819 -1.52921194]\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 6 ] state=[ 0.05434567  0.94322478 -0.06483819 -1.52921194], action=1, reward=1.0, next_state=[ 0.07321017  1.13906602 -0.09542242 -1.84140561]\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 7 ] state=[ 0.07321017  1.13906602 -0.09542242 -1.84140561], action=0, reward=1.0, next_state=[ 0.09599149  0.94511792 -0.13225054 -1.57981857]\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 8 ] state=[ 0.09599149  0.94511792 -0.13225054 -1.57981857], action=1, reward=1.0, next_state=[ 0.11489385  1.14154349 -0.16384691 -1.91065246]\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 9 ] state=[ 0.11489385  1.14154349 -0.16384691 -1.91065246], action=1, reward=1.0, next_state=[ 0.13772472  1.33800999 -0.20205996 -2.24936108]\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 10 ] state=[ 0.13772472  1.33800999 -0.20205996 -2.24936108], action=1, reward=-1.0, next_state=[ 0.16448492  1.5343822  -0.24704718 -2.59692888]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 86: Exploration_rate=0.01. Score=10.\n",
      "[ episode 87 ] state=[ 0.00266921  0.03011514  0.02598831 -0.01127348]\n",
      "[ episode 87 ][ timestamp 1 ] state=[ 0.00266921  0.03011514  0.02598831 -0.01127348], action=1, reward=1.0, next_state=[ 0.00327151  0.22485493  0.02576284 -0.29564483]\n",
      "[ Experience replay ] starts\n",
      "[ episode 87 ][ timestamp 2 ] state=[ 0.00327151  0.22485493  0.02576284 -0.29564483], action=1, reward=1.0, next_state=[ 0.00776861  0.41960031  0.01984994 -0.58009252]\n",
      "[ Experience replay ] starts\n",
      "[ episode 87 ][ timestamp 3 ] state=[ 0.00776861  0.41960031  0.01984994 -0.58009252], action=1, reward=1.0, next_state=[ 0.01616062  0.61443856  0.00824809 -0.86645682]\n",
      "[ Experience replay ] starts\n",
      "[ episode 87 ][ timestamp 4 ] state=[ 0.01616062  0.61443856  0.00824809 -0.86645682], action=1, reward=1.0, next_state=[ 0.02844939  0.80944729 -0.00908105 -1.15653506]\n",
      "[ Experience replay ] starts\n",
      "[ episode 87 ][ timestamp 5 ] state=[ 0.02844939  0.80944729 -0.00908105 -1.15653506], action=1, reward=1.0, next_state=[ 0.04463834  1.00468644 -0.03221175 -1.45205151]\n",
      "[ Experience replay ] starts\n",
      "[ episode 87 ][ timestamp 6 ] state=[ 0.04463834  1.00468644 -0.03221175 -1.45205151], action=1, reward=1.0, next_state=[ 0.06473206  1.20018895 -0.06125278 -1.75462176]\n",
      "[ Experience replay ] starts\n",
      "[ episode 87 ][ timestamp 7 ] state=[ 0.06473206  1.20018895 -0.06125278 -1.75462176], action=1, reward=1.0, next_state=[ 0.08873584  1.39594962 -0.09634521 -2.06570914]\n",
      "[ Experience replay ] starts\n",
      "[ episode 87 ][ timestamp 8 ] state=[ 0.08873584  1.39594962 -0.09634521 -2.06570914], action=1, reward=1.0, next_state=[ 0.11665484  1.59191161 -0.1376594  -2.38657062]\n",
      "[ Experience replay ] starts\n",
      "[ episode 87 ][ timestamp 9 ] state=[ 0.11665484  1.59191161 -0.1376594  -2.38657062], action=1, reward=1.0, next_state=[ 0.14849307  1.78795031 -0.18539081 -2.71819101]\n",
      "[ Experience replay ] starts\n",
      "[ episode 87 ][ timestamp 10 ] state=[ 0.14849307  1.78795031 -0.18539081 -2.71819101], action=1, reward=-1.0, next_state=[ 0.18425207  1.98385476 -0.23975463 -3.06120545]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 87: Exploration_rate=0.01. Score=10.\n",
      "[ episode 88 ] state=[-0.01097904 -0.00337663  0.01189213  0.01249944]\n",
      "[ episode 88 ][ timestamp 1 ] state=[-0.01097904 -0.00337663  0.01189213  0.01249944], action=1, reward=1.0, next_state=[-0.01104657  0.19157277  0.01214212 -0.27640778]\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 2 ] state=[-0.01104657  0.19157277  0.01214212 -0.27640778], action=1, reward=1.0, next_state=[-0.00721511  0.38651941  0.00661397 -0.56523648]\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 3 ] state=[-0.00721511  0.38651941  0.00661397 -0.56523648], action=1, reward=1.0, next_state=[ 5.15274896e-04  5.81547944e-01 -4.69076359e-03 -8.55828393e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 4 ] state=[ 5.15274896e-04  5.81547944e-01 -4.69076359e-03 -8.55828393e-01], action=1, reward=1.0, next_state=[ 0.01214623  0.7767335  -0.02180733 -1.14998259]\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 5 ] state=[ 0.01214623  0.7767335  -0.02180733 -1.14998259], action=1, reward=1.0, next_state=[ 0.0276809   0.97213317 -0.04480698 -1.44942325]\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 6 ] state=[ 0.0276809   0.97213317 -0.04480698 -1.44942325], action=1, reward=1.0, next_state=[ 0.04712357  1.16777633 -0.07379545 -1.7557623 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 7 ] state=[ 0.04712357  1.16777633 -0.07379545 -1.7557623 ], action=1, reward=1.0, next_state=[ 0.07047909  1.36365313 -0.10891069 -2.07045401]\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 8 ] state=[ 0.07047909  1.36365313 -0.10891069 -2.07045401], action=1, reward=1.0, next_state=[ 0.09775216  1.55970063 -0.15031977 -2.39473939]\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 9 ] state=[ 0.09775216  1.55970063 -0.15031977 -2.39473939], action=1, reward=1.0, next_state=[ 0.12894617  1.75578643 -0.19821456 -2.72957904]\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 10 ] state=[ 0.12894617  1.75578643 -0.19821456 -2.72957904], action=1, reward=-1.0, next_state=[ 0.1640619   1.95168996 -0.25280614 -3.07557481]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 88: Exploration_rate=0.01. Score=10.\n",
      "[ episode 89 ] state=[-0.04053788  0.00023133  0.00093916  0.03366997]\n",
      "[ episode 89 ][ timestamp 1 ] state=[-0.04053788  0.00023133  0.00093916  0.03366997], action=1, reward=1.0, next_state=[-0.04053325  0.19533981  0.00161256 -0.2587165 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 2 ] state=[-0.04053325  0.19533981  0.00161256 -0.2587165 ], action=1, reward=1.0, next_state=[-0.03662646  0.3904387  -0.00356177 -0.55089037]\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 3 ] state=[-0.03662646  0.3904387  -0.00356177 -0.55089037], action=1, reward=1.0, next_state=[-0.02881768  0.58561049 -0.01457958 -0.84469336]\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 4 ] state=[-0.02881768  0.58561049 -0.01457958 -0.84469336], action=1, reward=1.0, next_state=[-0.01710547  0.78092832 -0.03147345 -1.14192521]\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 5 ] state=[-0.01710547  0.78092832 -0.03147345 -1.14192521], action=1, reward=1.0, next_state=[-0.00148691  0.97644715 -0.05431195 -1.44430988]\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 6 ] state=[-0.00148691  0.97644715 -0.05431195 -1.44430988], action=1, reward=1.0, next_state=[ 0.01804203  1.1721938  -0.08319815 -1.75345676]\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 7 ] state=[ 0.01804203  1.1721938  -0.08319815 -1.75345676], action=1, reward=1.0, next_state=[ 0.04148591  1.36815514 -0.11826729 -2.07081408]\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 8 ] state=[ 0.04148591  1.36815514 -0.11826729 -2.07081408], action=1, reward=1.0, next_state=[ 0.06884901  1.56426403 -0.15968357 -2.39761214]\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 9 ] state=[ 0.06884901  1.56426403 -0.15968357 -2.39761214], action=1, reward=1.0, next_state=[ 0.10013429  1.76038277 -0.20763581 -2.73479533]\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 10 ] state=[ 0.10013429  1.76038277 -0.20763581 -2.73479533], action=1, reward=-1.0, next_state=[ 0.13534195  1.95628432 -0.26233172 -3.08294324]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 89: Exploration_rate=0.01. Score=10.\n",
      "[ episode 90 ] state=[ 0.01860113 -0.02003506 -0.01781635  0.03360138]\n",
      "[ episode 90 ][ timestamp 1 ] state=[ 0.01860113 -0.02003506 -0.01781635  0.03360138], action=1, reward=1.0, next_state=[ 0.01820043  0.17533779 -0.01714432 -0.26464912]\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 2 ] state=[ 0.01820043  0.17533779 -0.01714432 -0.26464912], action=1, reward=1.0, next_state=[ 0.02170719  0.3707002  -0.0224373  -0.56268984]\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 3 ] state=[ 0.02170719  0.3707002  -0.0224373  -0.56268984], action=1, reward=1.0, next_state=[ 0.02912119  0.5661297  -0.0336911  -0.86235632]\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 4 ] state=[ 0.02912119  0.5661297  -0.0336911  -0.86235632], action=1, reward=1.0, next_state=[ 0.04044378  0.76169379 -0.05093822 -1.16543929]\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 5 ] state=[ 0.04044378  0.76169379 -0.05093822 -1.16543929], action=1, reward=1.0, next_state=[ 0.05567766  0.9574404  -0.07424701 -1.47364773]\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 6 ] state=[ 0.05567766  0.9574404  -0.07424701 -1.47364773], action=1, reward=1.0, next_state=[ 0.07482647  1.1533872  -0.10371996 -1.78856674]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 90 ][ timestamp 7 ] state=[ 0.07482647  1.1533872  -0.10371996 -1.78856674], action=1, reward=1.0, next_state=[ 0.09789421  1.34950905 -0.1394913  -2.11160758]\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 8 ] state=[ 0.09789421  1.34950905 -0.1394913  -2.11160758], action=1, reward=1.0, next_state=[ 0.12488439  1.54572327 -0.18172345 -2.44394769]\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 9 ] state=[ 0.12488439  1.54572327 -0.18172345 -2.44394769], action=1, reward=-1.0, next_state=[ 0.15579886  1.74187251 -0.23060241 -2.7864599 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 90: Exploration_rate=0.01. Score=9.\n",
      "[ episode 91 ] state=[ 0.01318458 -0.0299232  -0.00424924 -0.02295416]\n",
      "[ episode 91 ][ timestamp 1 ] state=[ 0.01318458 -0.0299232  -0.00424924 -0.02295416], action=1, reward=1.0, next_state=[ 0.01258611  0.16525943 -0.00470832 -0.31697473]\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 2 ] state=[ 0.01258611  0.16525943 -0.00470832 -0.31697473], action=1, reward=1.0, next_state=[ 0.0158913   0.36044812 -0.01104782 -0.61113877]\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 3 ] state=[ 0.0158913   0.36044812 -0.01104782 -0.61113877], action=1, reward=1.0, next_state=[ 0.02310026  0.55572273 -0.02327059 -0.90728081]\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 4 ] state=[ 0.02310026  0.55572273 -0.02327059 -0.90728081], action=1, reward=1.0, next_state=[ 0.03421472  0.75115187 -0.04141621 -1.20718608]\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 5 ] state=[ 0.03421472  0.75115187 -0.04141621 -1.20718608], action=1, reward=1.0, next_state=[ 0.04923775  0.9467837  -0.06555993 -1.51255508]\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 6 ] state=[ 0.04923775  0.9467837  -0.06555993 -1.51255508], action=1, reward=1.0, next_state=[ 0.06817343  1.14263546 -0.09581103 -1.8249624 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 7 ] state=[ 0.06817343  1.14263546 -0.09581103 -1.8249624 ], action=1, reward=1.0, next_state=[ 0.09102614  1.33868097 -0.13231028 -2.14580733]\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 8 ] state=[ 0.09102614  1.33868097 -0.13231028 -2.14580733], action=1, reward=1.0, next_state=[ 0.11779976  1.53483595 -0.17522643 -2.47625396]\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 9 ] state=[ 0.11779976  1.53483595 -0.17522643 -2.47625396], action=1, reward=-1.0, next_state=[ 0.14849648  1.73094079 -0.22475151 -2.81716017]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 91: Exploration_rate=0.01. Score=9.\n",
      "[ episode 92 ] state=[ 0.04217235  0.00942058 -0.00576082  0.019292  ]\n",
      "[ episode 92 ][ timestamp 1 ] state=[ 0.04217235  0.00942058 -0.00576082  0.019292  ], action=1, reward=1.0, next_state=[ 0.04236076  0.20462467 -0.00537498 -0.27520295]\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 2 ] state=[ 0.04236076  0.20462467 -0.00537498 -0.27520295], action=1, reward=1.0, next_state=[ 0.04645325  0.39982289 -0.01087904 -0.56957629]\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 3 ] state=[ 0.04645325  0.39982289 -0.01087904 -0.56957629], action=1, reward=1.0, next_state=[ 0.05444971  0.59509572 -0.02227056 -0.86566657]\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 4 ] state=[ 0.05444971  0.59509572 -0.02227056 -0.86566657], action=1, reward=1.0, next_state=[ 0.06635162  0.79051359 -0.0395839  -1.16526769]\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 5 ] state=[ 0.06635162  0.79051359 -0.0395839  -1.16526769], action=1, reward=1.0, next_state=[ 0.0821619   0.9861278  -0.06288925 -1.47009378]\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 6 ] state=[ 0.0821619   0.9861278  -0.06288925 -1.47009378], action=1, reward=1.0, next_state=[ 0.10188445  1.18196019 -0.09229112 -1.78173891]\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 7 ] state=[ 0.10188445  1.18196019 -0.09229112 -1.78173891], action=1, reward=1.0, next_state=[ 0.12552366  1.37799096 -0.1279259  -2.10162876]\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 8 ] state=[ 0.12552366  1.37799096 -0.1279259  -2.10162876], action=1, reward=1.0, next_state=[ 0.15308347  1.57414426 -0.16995848 -2.43096216]\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 9 ] state=[ 0.15308347  1.57414426 -0.16995848 -2.43096216], action=1, reward=-1.0, next_state=[ 0.18456636  1.77027126 -0.21857772 -2.77064149]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 92: Exploration_rate=0.01. Score=9.\n",
      "[ episode 93 ] state=[-0.04258865  0.00430705 -0.01717628 -0.00979547]\n",
      "[ episode 93 ][ timestamp 1 ] state=[-0.04258865  0.00430705 -0.01717628 -0.00979547], action=1, reward=1.0, next_state=[-0.04250251  0.19967107 -0.01737219 -0.30784785]\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 2 ] state=[-0.04250251  0.19967107 -0.01737219 -0.30784785], action=1, reward=1.0, next_state=[-0.03850909  0.39503619 -0.02352915 -0.60595848]\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 3 ] state=[-0.03850909  0.39503619 -0.02352915 -0.60595848], action=1, reward=1.0, next_state=[-0.03060836  0.59047912 -0.03564832 -0.90595865]\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 4 ] state=[-0.03060836  0.59047912 -0.03564832 -0.90595865], action=1, reward=1.0, next_state=[-0.01879878  0.78606517 -0.05376749 -1.20962972]\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 5 ] state=[-0.01879878  0.78606517 -0.05376749 -1.20962972], action=1, reward=1.0, next_state=[-0.00307748  0.98183865 -0.07796008 -1.51866559]\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 6 ] state=[-0.00307748  0.98183865 -0.07796008 -1.51866559], action=1, reward=1.0, next_state=[ 0.01655929  1.17781192 -0.10833339 -1.8346297 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 7 ] state=[ 0.01655929  1.17781192 -0.10833339 -1.8346297 ], action=0, reward=1.0, next_state=[ 0.04011553  0.9840422  -0.14502599 -1.57746679]\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 8 ] state=[ 0.04011553  0.9840422  -0.14502599 -1.57746679], action=0, reward=1.0, next_state=[ 0.05979638  0.79091539 -0.17657532 -1.33330602]\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 9 ] state=[ 0.05979638  0.79091539 -0.17657532 -1.33330602], action=0, reward=1.0, next_state=[ 0.07561468  0.59840347 -0.20324144 -1.10067198]\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 10 ] state=[ 0.07561468  0.59840347 -0.20324144 -1.10067198], action=0, reward=-1.0, next_state=[ 0.08758275  0.40645035 -0.22525488 -0.87801109]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 93: Exploration_rate=0.01. Score=10.\n",
      "[ episode 94 ] state=[ 0.00413333 -0.03544184 -0.04756482  0.01108778]\n",
      "[ episode 94 ][ timestamp 1 ] state=[ 0.00413333 -0.03544184 -0.04756482  0.01108778], action=1, reward=1.0, next_state=[ 0.0034245   0.16032884 -0.04734307 -0.2962149 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 2 ] state=[ 0.0034245   0.16032884 -0.04734307 -0.2962149 ], action=0, reward=1.0, next_state=[ 0.00663107 -0.03408734 -0.05326737 -0.01883104]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 3 ] state=[ 0.00663107 -0.03408734 -0.05326737 -0.01883104], action=0, reward=1.0, next_state=[ 0.00594933 -0.22840652 -0.05364399  0.2565811 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 4 ] state=[ 0.00594933 -0.22840652 -0.05364399  0.2565811 ], action=0, reward=1.0, next_state=[ 0.0013812  -0.42272318 -0.04851236  0.53187303]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 5 ] state=[ 0.0013812  -0.42272318 -0.04851236  0.53187303], action=0, reward=1.0, next_state=[-0.00707327 -0.61713042 -0.0378749   0.80888377]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 6 ] state=[-0.00707327 -0.61713042 -0.0378749   0.80888377], action=0, reward=1.0, next_state=[-0.01941588 -0.81171346 -0.02169723  1.08941645]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 7 ] state=[-0.01941588 -0.81171346 -0.02169723  1.08941645], action=1, reward=1.0, next_state=[-3.56501452e-02 -6.16312289e-01  9.11007974e-05  7.90005198e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 8 ] state=[-3.56501452e-02 -6.16312289e-01  9.11007974e-05  7.90005198e-01], action=1, reward=1.0, next_state=[-0.04797639 -0.42119159  0.0158912   0.49735093]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 9 ] state=[-0.04797639 -0.42119159  0.0158912   0.49735093], action=1, reward=1.0, next_state=[-0.05640022 -0.22629727  0.02583822  0.20971818]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 94 ][ timestamp 10 ] state=[-0.05640022 -0.22629727  0.02583822  0.20971818], action=1, reward=1.0, next_state=[-0.06092617 -0.03155411  0.03003259 -0.07470345]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 11 ] state=[-0.06092617 -0.03155411  0.03003259 -0.07470345], action=1, reward=1.0, next_state=[-0.06155725  0.1631247   0.02853852 -0.35776174]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 12 ] state=[-0.06155725  0.1631247   0.02853852 -0.35776174], action=1, reward=1.0, next_state=[-0.05829476  0.35782956  0.02138328 -0.64131091]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 13 ] state=[-0.05829476  0.35782956  0.02138328 -0.64131091], action=1, reward=1.0, next_state=[-0.05113817  0.552647    0.00855706 -0.92718406]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 14 ] state=[-0.05113817  0.552647    0.00855706 -0.92718406], action=0, reward=1.0, next_state=[-0.04008523  0.35741055 -0.00998662 -0.63182437]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 15 ] state=[-0.04008523  0.35741055 -0.00998662 -0.63182437], action=0, reward=1.0, next_state=[-0.03293701  0.16242935 -0.0226231  -0.34230316]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 16 ] state=[-0.03293701  0.16242935 -0.0226231  -0.34230316], action=0, reward=1.0, next_state=[-0.02968843 -0.03236356 -0.02946917 -0.0568392 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 17 ] state=[-0.02968843 -0.03236356 -0.02946917 -0.0568392 ], action=1, reward=1.0, next_state=[-0.0303357   0.16316827 -0.03060595 -0.35867227]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 18 ] state=[-0.0303357   0.16316827 -0.03060595 -0.35867227], action=0, reward=1.0, next_state=[-0.02707233 -0.03150553 -0.0377794  -0.07579508]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 19 ] state=[-0.02707233 -0.03150553 -0.0377794  -0.07579508], action=1, reward=1.0, next_state=[-0.02770244  0.16413709 -0.0392953  -0.3801541 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 20 ] state=[-0.02770244  0.16413709 -0.0392953  -0.3801541 ], action=1, reward=1.0, next_state=[-0.0244197   0.35979438 -0.04689838 -0.68496332]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 21 ] state=[-0.0244197   0.35979438 -0.04689838 -0.68496332], action=1, reward=1.0, next_state=[-0.01722381  0.555535   -0.06059765 -0.99203448]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 22 ] state=[-0.01722381  0.555535   -0.06059765 -0.99203448], action=1, reward=1.0, next_state=[-0.00611311  0.75141317 -0.08043834 -1.30311726]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 23 ] state=[-0.00611311  0.75141317 -0.08043834 -1.30311726], action=1, reward=1.0, next_state=[ 0.00891515  0.94745802 -0.10650068 -1.61985707]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 24 ] state=[ 0.00891515  0.94745802 -0.10650068 -1.61985707], action=0, reward=1.0, next_state=[ 0.02786431  0.75373997 -0.13889782 -1.36217839]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 25 ] state=[ 0.02786431  0.75373997 -0.13889782 -1.36217839], action=0, reward=1.0, next_state=[ 0.04293911  0.56060475 -0.16614139 -1.11597042]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 26 ] state=[ 0.04293911  0.56060475 -0.16614139 -1.11597042], action=0, reward=1.0, next_state=[ 0.0541512  0.3680064 -0.1884608 -0.8796721]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 27 ] state=[ 0.0541512  0.3680064 -0.1884608 -0.8796721], action=0, reward=1.0, next_state=[ 0.06151133  0.1758757  -0.20605424 -0.65165899]\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 28 ] state=[ 0.06151133  0.1758757  -0.20605424 -0.65165899], action=1, reward=-1.0, next_state=[ 0.06502885  0.37318072 -0.21908742 -1.00150796]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 94: Exploration_rate=0.01. Score=28.\n",
      "[ episode 95 ] state=[ 0.02665077 -0.02753811 -0.01615269 -0.02103089]\n",
      "[ episode 95 ][ timestamp 1 ] state=[ 0.02665077 -0.02753811 -0.01615269 -0.02103089], action=1, reward=1.0, next_state=[ 0.02610001  0.16781172 -0.0165733  -0.31876609]\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 2 ] state=[ 0.02610001  0.16781172 -0.0165733  -0.31876609], action=1, reward=1.0, next_state=[ 0.02945624  0.36316574 -0.02294863 -0.61662922]\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 3 ] state=[ 0.02945624  0.36316574 -0.02294863 -0.61662922], action=1, reward=1.0, next_state=[ 0.03671956  0.55860065 -0.03528121 -0.9164507 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 4 ] state=[ 0.03671956  0.55860065 -0.03528121 -0.9164507 ], action=1, reward=1.0, next_state=[ 0.04789157  0.75418146 -0.05361022 -1.22000986]\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 5 ] state=[ 0.04789157  0.75418146 -0.05361022 -1.22000986], action=1, reward=1.0, next_state=[ 0.0629752   0.94995184 -0.07801042 -1.5289974 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 6 ] state=[ 0.0629752   0.94995184 -0.07801042 -1.5289974 ], action=1, reward=1.0, next_state=[ 0.08197424  1.14592321 -0.10859037 -1.84497226]\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 7 ] state=[ 0.08197424  1.14592321 -0.10859037 -1.84497226], action=1, reward=1.0, next_state=[ 0.1048927   1.34206181 -0.14548981 -2.1693101 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 8 ] state=[ 0.1048927   1.34206181 -0.14548981 -2.1693101 ], action=1, reward=1.0, next_state=[ 0.13173394  1.53827362 -0.18887602 -2.5031416 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 9 ] state=[ 0.13173394  1.53827362 -0.18887602 -2.5031416 ], action=0, reward=-1.0, next_state=[ 0.16249941  1.34514688 -0.23893885 -2.27380337]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 95: Exploration_rate=0.01. Score=9.\n",
      "[ episode 96 ] state=[-0.0026789  -0.04852906 -0.02199837 -0.03008976]\n",
      "[ episode 96 ][ timestamp 1 ] state=[-0.0026789  -0.04852906 -0.02199837 -0.03008976], action=1, reward=1.0, next_state=[-0.00364948  0.14690134 -0.02260017 -0.32963143]\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 2 ] state=[-0.00364948  0.14690134 -0.02260017 -0.32963143], action=1, reward=1.0, next_state=[-0.00071145  0.3423376  -0.02919279 -0.62935484]\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 3 ] state=[-0.00071145  0.3423376  -0.02919279 -0.62935484], action=1, reward=1.0, next_state=[ 0.0061353   0.53785451 -0.04177989 -0.93108671]\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 4 ] state=[ 0.0061353   0.53785451 -0.04177989 -0.93108671], action=1, reward=1.0, next_state=[ 0.01689239  0.73351465 -0.06040162 -1.23660052]\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 5 ] state=[ 0.01689239  0.73351465 -0.06040162 -1.23660052], action=1, reward=1.0, next_state=[ 0.03156268  0.92935847 -0.08513364 -1.54757781]\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 6 ] state=[ 0.03156268  0.92935847 -0.08513364 -1.54757781], action=1, reward=1.0, next_state=[ 0.05014985  1.12539308 -0.11608519 -1.86556383]\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 7 ] state=[ 0.05014985  1.12539308 -0.11608519 -1.86556383], action=0, reward=1.0, next_state=[ 0.07265771  0.93171788 -0.15339647 -1.61105872]\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 8 ] state=[ 0.07265771  0.93171788 -0.15339647 -1.61105872], action=0, reward=1.0, next_state=[ 0.09129207  0.73870432 -0.18561764 -1.36985989]\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 9 ] state=[ 0.09129207  0.73870432 -0.18561764 -1.36985989], action=0, reward=-1.0, next_state=[ 0.10606616  0.54632525 -0.21301484 -1.14050694]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 96: Exploration_rate=0.01. Score=9.\n",
      "[ episode 97 ] state=[ 0.02008229 -0.01007599  0.02765294 -0.01163051]\n",
      "[ episode 97 ][ timestamp 1 ] state=[ 0.02008229 -0.01007599  0.02765294 -0.01163051], action=1, reward=1.0, next_state=[ 0.01988077  0.1846387   0.02742033 -0.29546194]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 2 ] state=[ 0.01988077  0.1846387   0.02742033 -0.29546194], action=1, reward=1.0, next_state=[ 0.02357354  0.37935922  0.0215111  -0.57937236]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 3 ] state=[ 0.02357354  0.37935922  0.0215111  -0.57937236], action=1, reward=1.0, next_state=[ 0.03116073  0.57417322  0.00992365 -0.86520197]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 4 ] state=[ 0.03116073  0.57417322  0.00992365 -0.86520197], action=0, reward=1.0, next_state=[ 0.04264419  0.37891761 -0.00738039 -0.56941548]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 5 ] state=[ 0.04264419  0.37891761 -0.00738039 -0.56941548], action=0, reward=1.0, next_state=[ 0.05022254  0.18389994 -0.0187687  -0.27906676]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 6 ] state=[ 0.05022254  0.18389994 -0.0187687  -0.27906676], action=0, reward=1.0, next_state=[ 0.05390054 -0.01094931 -0.02435004  0.00763796]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 7 ] state=[ 0.05390054 -0.01094931 -0.02435004  0.00763796], action=0, reward=1.0, next_state=[ 0.05368156 -0.20571373 -0.02419728  0.29253979]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 8 ] state=[ 0.05368156 -0.20571373 -0.02419728  0.29253979], action=1, reward=1.0, next_state=[ 0.04956728 -0.01025529 -0.01834648 -0.00767536]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 97 ][ timestamp 9 ] state=[ 0.04956728 -0.01025529 -0.01834648 -0.00767536], action=1, reward=1.0, next_state=[ 0.04936218  0.18512491 -0.01849999 -0.30608989]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 10 ] state=[ 0.04936218  0.18512491 -0.01849999 -0.30608989], action=1, reward=1.0, next_state=[ 0.05306467  0.38050553 -0.02462179 -0.60454936]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 11 ] state=[ 0.05306467  0.38050553 -0.02462179 -0.60454936], action=1, reward=1.0, next_state=[ 0.06067479  0.57596301 -0.03671277 -0.90488478]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 12 ] state=[ 0.06067479  0.57596301 -0.03671277 -0.90488478], action=0, reward=1.0, next_state=[ 0.07219405  0.38135696 -0.05481047 -0.62396354]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 13 ] state=[ 0.07219405  0.38135696 -0.05481047 -0.62396354], action=0, reward=1.0, next_state=[ 0.07982118  0.18704136 -0.06728974 -0.34903406]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 14 ] state=[ 0.07982118  0.18704136 -0.06728974 -0.34903406], action=0, reward=1.0, next_state=[ 0.08356201 -0.00706225 -0.07427042 -0.07830582]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 15 ] state=[ 0.08356201 -0.00706225 -0.07427042 -0.07830582], action=0, reward=1.0, next_state=[ 0.08342077 -0.20104526 -0.07583654  0.1900511 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 16 ] state=[ 0.08342077 -0.20104526 -0.07583654  0.1900511 ], action=1, reward=1.0, next_state=[ 0.07939986 -0.00492491 -0.07203552 -0.12555846]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 17 ] state=[ 0.07939986 -0.00492491 -0.07203552 -0.12555846], action=0, reward=1.0, next_state=[ 0.07930136 -0.1989449  -0.07454668  0.14355663]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 18 ] state=[ 0.07930136 -0.1989449  -0.07454668  0.14355663], action=0, reward=1.0, next_state=[ 0.07532247 -0.39292447 -0.07167555  0.41182143]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 19 ] state=[ 0.07532247 -0.39292447 -0.07167555  0.41182143], action=1, reward=1.0, next_state=[ 0.06746398 -0.19686352 -0.06343912  0.09743055]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 20 ] state=[ 0.06746398 -0.19686352 -0.06343912  0.09743055], action=0, reward=1.0, next_state=[ 0.06352671 -0.39102159 -0.06149051  0.36944321]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 21 ] state=[ 0.06352671 -0.39102159 -0.06149051  0.36944321], action=1, reward=1.0, next_state=[ 0.05570627 -0.19508232 -0.05410165  0.05802296]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 22 ] state=[ 0.05570627 -0.19508232 -0.05410165  0.05802296], action=1, reward=1.0, next_state=[ 0.05180463  0.00077193 -0.05294119 -0.2512267 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 23 ] state=[ 0.05180463  0.00077193 -0.05294119 -0.2512267 ], action=0, reward=1.0, next_state=[ 0.05182007 -0.19355563 -0.05796572  0.0242988 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 24 ] state=[ 0.05182007 -0.19355563 -0.05796572  0.0242988 ], action=0, reward=1.0, next_state=[ 0.04794895 -0.38780046 -0.05747975  0.2981443 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 25 ] state=[ 0.04794895 -0.38780046 -0.05747975  0.2981443 ], action=0, reward=1.0, next_state=[ 0.04019294 -0.58205795 -0.05151686  0.57215957]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 26 ] state=[ 0.04019294 -0.58205795 -0.05151686  0.57215957], action=0, reward=1.0, next_state=[ 0.02855179 -0.77642112 -0.04007367  0.84817827]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 27 ] state=[ 0.02855179 -0.77642112 -0.04007367  0.84817827], action=0, reward=1.0, next_state=[ 0.01302336 -0.97097424 -0.0231101   1.12799515]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 28 ] state=[ 0.01302336 -0.97097424 -0.0231101   1.12799515], action=1, reward=1.0, next_state=[-6.39612148e-03 -7.75557292e-01 -5.50200468e-04  8.28154243e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 29 ] state=[-6.39612148e-03 -7.75557292e-01 -5.50200468e-04  8.28154243e-01], action=1, reward=1.0, next_state=[-0.02190727 -0.58042782  0.01601288  0.53529832]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 30 ] state=[-0.02190727 -0.58042782  0.01601288  0.53529832], action=1, reward=1.0, next_state=[-0.03351582 -0.38553466  0.02671885  0.24770365]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 31 ] state=[-0.03351582 -0.38553466  0.02671885  0.24770365], action=1, reward=1.0, next_state=[-0.04122652 -0.19080428  0.03167292 -0.03643325]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 32 ] state=[-0.04122652 -0.19080428  0.03167292 -0.03643325], action=1, reward=1.0, next_state=[-0.0450426   0.00384949  0.03094426 -0.31895719]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 33 ] state=[-0.0450426   0.00384949  0.03094426 -0.31895719], action=1, reward=1.0, next_state=[-0.04496561  0.19851738  0.02456511 -0.60172307]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 34 ] state=[-0.04496561  0.19851738  0.02456511 -0.60172307], action=0, reward=1.0, next_state=[-0.04099527  0.00306057  0.01253065 -0.3014049 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 35 ] state=[-0.04099527  0.00306057  0.01253065 -0.3014049 ], action=0, reward=1.0, next_state=[-0.04093405 -0.19223771  0.00650256 -0.00479655]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 36 ] state=[-0.04093405 -0.19223771  0.00650256 -0.00479655], action=1, reward=1.0, next_state=[-0.04477881  0.00279038  0.00640662 -0.29542077]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 37 ] state=[-0.04477881  0.00279038  0.00640662 -0.29542077], action=0, reward=1.0, next_state=[-0.044723   -0.19242232  0.00049821 -0.0007242 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 38 ] state=[-0.044723   -0.19242232  0.00049821 -0.0007242 ], action=0, reward=1.0, next_state=[-0.04857145 -0.38755141  0.00048372  0.29211587]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 39 ] state=[-0.04857145 -0.38755141  0.00048372  0.29211587], action=0, reward=1.0, next_state=[-0.05632248 -0.58268025  0.00632604  0.58495132]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 40 ] state=[-0.05632248 -0.58268025  0.00632604  0.58495132], action=1, reward=1.0, next_state=[-0.06797608 -0.38764748  0.01802507  0.29426787]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 41 ] state=[-0.06797608 -0.38764748  0.01802507  0.29426787], action=0, reward=1.0, next_state=[-0.07572903 -0.58302172  0.02391043  0.59258069]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 42 ] state=[-0.07572903 -0.58302172  0.02391043  0.59258069], action=1, reward=1.0, next_state=[-0.08738946 -0.3882425   0.03576204  0.30752438]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 43 ] state=[-0.08738946 -0.3882425   0.03576204  0.30752438], action=0, reward=1.0, next_state=[-0.09515431 -0.5838553   0.04191253  0.61126776]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 44 ] state=[-0.09515431 -0.5838553   0.04191253  0.61126776], action=1, reward=1.0, next_state=[-0.10683142 -0.38934345  0.05413788  0.33207489]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 45 ] state=[-0.10683142 -0.38934345  0.05413788  0.33207489], action=1, reward=1.0, next_state=[-0.11461829 -0.1950322   0.06077938  0.05694382]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 46 ] state=[-0.11461829 -0.1950322   0.06077938  0.05694382], action=1, reward=1.0, next_state=[-0.11851893 -0.00083201  0.06191826 -0.21596045]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 47 ] state=[-0.11851893 -0.00083201  0.06191826 -0.21596045], action=1, reward=1.0, next_state=[-0.11853557  0.19335262  0.05759905 -0.48848688]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 48 ] state=[-0.11853557  0.19335262  0.05759905 -0.48848688], action=0, reward=1.0, next_state=[-0.11466852 -0.00253266  0.04782931 -0.17822148]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 49 ] state=[-0.11466852 -0.00253266  0.04782931 -0.17822148], action=1, reward=1.0, next_state=[-0.11471917  0.19187336  0.04426488 -0.45544057]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 50 ] state=[-0.11471917  0.19187336  0.04426488 -0.45544057], action=1, reward=1.0, next_state=[-0.11088171  0.38634241  0.03515607 -0.73384878]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 51 ] state=[-0.11088171  0.38634241  0.03515607 -0.73384878], action=1, reward=1.0, next_state=[-0.10315486  0.58096146  0.02047909 -1.01526322]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 52 ] state=[-0.10315486  0.58096146  0.02047909 -1.01526322], action=1, reward=1.0, next_state=[-9.15356295e-02  7.75804411e-01  1.73829533e-04 -1.30144592e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 53 ] state=[-9.15356295e-02  7.75804411e-01  1.73829533e-04 -1.30144592e+00], action=0, reward=1.0, next_state=[-0.07601954  0.58068025 -0.02585509 -1.00870859]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 97 ][ timestamp 54 ] state=[-0.07601954  0.58068025 -0.02585509 -1.00870859], action=0, reward=1.0, next_state=[-0.06440594  0.3859128  -0.04602926 -0.7242556 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 55 ] state=[-0.06440594  0.3859128  -0.04602926 -0.7242556 ], action=0, reward=1.0, next_state=[-0.05668768  0.19145662 -0.06051437 -0.44640809]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 56 ] state=[-0.05668768  0.19145662 -0.06051437 -0.44640809], action=1, reward=1.0, next_state=[-0.05285855  0.38738013 -0.06944253 -0.75753578]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 57 ] state=[-0.05285855  0.38738013 -0.06944253 -0.75753578], action=1, reward=1.0, next_state=[-0.04511095  0.58338689 -0.08459325 -1.07123702]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 58 ] state=[-0.04511095  0.58338689 -0.08459325 -1.07123702], action=0, reward=1.0, next_state=[-0.03344321  0.38947905 -0.10601799 -0.8062561 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 59 ] state=[-0.03344321  0.38947905 -0.10601799 -0.8062561 ], action=1, reward=1.0, next_state=[-0.02565363  0.58588205 -0.12214311 -1.13031744]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 60 ] state=[-0.02565363  0.58588205 -0.12214311 -1.13031744], action=0, reward=1.0, next_state=[-0.01393599  0.39255257 -0.14474946 -0.87830458]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 61 ] state=[-0.01393599  0.39255257 -0.14474946 -0.87830458], action=0, reward=1.0, next_state=[-0.00608493  0.19966276 -0.16231555 -0.6344036 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 62 ] state=[-0.00608493  0.19966276 -0.16231555 -0.6344036 ], action=1, reward=1.0, next_state=[-0.00209168  0.39663204 -0.17500362 -0.97348549]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 63 ] state=[-0.00209168  0.39663204 -0.17500362 -0.97348549], action=0, reward=1.0, next_state=[ 0.00584096  0.20423476 -0.19447333 -0.74048645]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 64 ] state=[ 0.00584096  0.20423476 -0.19447333 -0.74048645], action=0, reward=1.0, next_state=[ 0.00992566  0.01225328 -0.20928306 -0.51475808]\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 65 ] state=[ 0.00992566  0.01225328 -0.20928306 -0.51475808], action=0, reward=-1.0, next_state=[ 0.01017072 -0.17940252 -0.21957823 -0.29462829]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 97: Exploration_rate=0.01. Score=65.\n",
      "[ episode 98 ] state=[0.04041957 0.03237287 0.03930133 0.00073584]\n",
      "[ episode 98 ][ timestamp 1 ] state=[0.04041957 0.03237287 0.03930133 0.00073584], action=1, reward=1.0, next_state=[ 0.04106703  0.22690979  0.03931605 -0.27929259]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 2 ] state=[ 0.04106703  0.22690979  0.03931605 -0.27929259], action=1, reward=1.0, next_state=[ 0.04560522  0.42144947  0.03373019 -0.55932067]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 3 ] state=[ 0.04560522  0.42144947  0.03373019 -0.55932067], action=0, reward=1.0, next_state=[ 0.05403421  0.22587072  0.02254378 -0.25620461]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 4 ] state=[ 0.05403421  0.22587072  0.02254378 -0.25620461], action=1, reward=1.0, next_state=[ 0.05855163  0.42066367  0.01741969 -0.54169248]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 5 ] state=[ 0.05855163  0.42066367  0.01741969 -0.54169248], action=1, reward=1.0, next_state=[ 0.0669649   0.61553651  0.00658584 -0.82883626]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 6 ] state=[ 0.0669649   0.61553651  0.00658584 -0.82883626], action=0, reward=1.0, next_state=[ 0.07927563  0.42032514 -0.00999089 -0.53408934]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 7 ] state=[ 0.07927563  0.42032514 -0.00999089 -0.53408934], action=1, reward=1.0, next_state=[ 0.08768213  0.61558616 -0.02067267 -0.82990352]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 8 ] state=[ 0.08768213  0.61558616 -0.02067267 -0.82990352], action=0, reward=1.0, next_state=[ 0.09999386  0.4207528  -0.03727074 -0.54379326]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 9 ] state=[ 0.09999386  0.4207528  -0.03727074 -0.54379326], action=0, reward=1.0, next_state=[ 0.10840891  0.22617389 -0.04814661 -0.26308265]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 10 ] state=[ 0.10840891  0.22617389 -0.04814661 -0.26308265], action=0, reward=1.0, next_state=[ 0.11293239  0.03177107 -0.05340826  0.01403403]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 11 ] state=[ 0.11293239  0.03177107 -0.05340826  0.01403403], action=0, reward=1.0, next_state=[ 0.11356781 -0.16254588 -0.05312758  0.28939928]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 12 ] state=[ 0.11356781 -0.16254588 -0.05312758  0.28939928], action=0, reward=1.0, next_state=[ 0.11031689 -0.35687158 -0.04733959  0.56486439]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 13 ] state=[ 0.11031689 -0.35687158 -0.04733959  0.56486439], action=0, reward=1.0, next_state=[ 0.10317946 -0.5512985  -0.03604231  0.8422654 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 14 ] state=[ 0.10317946 -0.5512985  -0.03604231  0.8422654 ], action=1, reward=1.0, next_state=[ 0.09215349 -0.35570362 -0.019197    0.53846948]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 15 ] state=[ 0.09215349 -0.35570362 -0.019197    0.53846948], action=0, reward=1.0, next_state=[ 0.08503942 -0.5505505  -0.00842761  0.82504238]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 16 ] state=[ 0.08503942 -0.5505505  -0.00842761  0.82504238], action=1, reward=1.0, next_state=[ 0.07402841 -0.3553143   0.00807324  0.5297208 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 17 ] state=[ 0.07402841 -0.3553143   0.00807324  0.5297208 ], action=1, reward=1.0, next_state=[ 0.06692212 -0.16030685  0.01866765  0.23959266]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 18 ] state=[ 0.06692212 -0.16030685  0.01866765  0.23959266], action=1, reward=1.0, next_state=[ 0.06371599  0.03454352  0.02345951 -0.047144  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 19 ] state=[ 0.06371599  0.03454352  0.02345951 -0.047144  ], action=1, reward=1.0, next_state=[ 0.06440686  0.22932136  0.02251663 -0.33233391]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 20 ] state=[ 0.06440686  0.22932136  0.02251663 -0.33233391], action=1, reward=1.0, next_state=[ 0.06899328  0.4241157   0.01586995 -0.61783202]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 21 ] state=[ 0.06899328  0.4241157   0.01586995 -0.61783202], action=0, reward=1.0, next_state=[ 0.0774756   0.2287757   0.00351331 -0.32019334]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 22 ] state=[ 0.0774756   0.2287757   0.00351331 -0.32019334], action=0, reward=1.0, next_state=[ 0.08205111  0.03360389 -0.00289056 -0.02640453]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 23 ] state=[ 0.08205111  0.03360389 -0.00289056 -0.02640453], action=0, reward=1.0, next_state=[ 0.08272319 -0.16147649 -0.00341865  0.265365  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 24 ] state=[ 0.08272319 -0.16147649 -0.00341865  0.265365  ], action=1, reward=1.0, next_state=[ 0.07949366  0.03369409  0.00188865 -0.02839424]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 25 ] state=[ 0.07949366  0.03369409  0.00188865 -0.02839424], action=0, reward=1.0, next_state=[ 0.08016754 -0.1614549   0.00132077  0.26488398]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 26 ] state=[ 0.08016754 -0.1614549   0.00132077  0.26488398], action=1, reward=1.0, next_state=[ 0.07693844  0.03364818  0.00661845 -0.02738207]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 27 ] state=[ 0.07693844  0.03364818  0.00661845 -0.02738207], action=1, reward=1.0, next_state=[ 0.07761141  0.22867459  0.0060708  -0.31796948]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 28 ] state=[ 0.07761141  0.22867459  0.0060708  -0.31796948], action=1, reward=1.0, next_state=[ 8.21848994e-02  4.23709555e-01 -2.88584866e-04 -6.08731727e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 29 ] state=[ 8.21848994e-02  4.23709555e-01 -2.88584866e-04 -6.08731727e-01], action=0, reward=1.0, next_state=[ 0.09065909  0.22859164 -0.01246322 -0.31613971]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 98 ][ timestamp 30 ] state=[ 0.09065909  0.22859164 -0.01246322 -0.31613971], action=1, reward=1.0, next_state=[ 0.09523092  0.42388888 -0.01878601 -0.61272691]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 31 ] state=[ 0.09523092  0.42388888 -0.01878601 -0.61272691], action=0, reward=1.0, next_state=[ 0.1037087   0.22903443 -0.03104055 -0.32601958]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 32 ] state=[ 0.1037087   0.22903443 -0.03104055 -0.32601958], action=0, reward=1.0, next_state=[ 0.10828939  0.03436786 -0.03756094 -0.04328484]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 33 ] state=[ 0.10828939  0.03436786 -0.03756094 -0.04328484], action=1, reward=1.0, next_state=[ 0.10897675  0.23000773 -0.03842664 -0.34757798]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 34 ] state=[ 0.10897675  0.23000773 -0.03842664 -0.34757798], action=1, reward=1.0, next_state=[ 0.1135769   0.42565458 -0.0453782  -0.65212626]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 35 ] state=[ 0.1135769   0.42565458 -0.0453782  -0.65212626], action=0, reward=1.0, next_state=[ 0.12208999  0.23119298 -0.05842072 -0.37407075]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 36 ] state=[ 0.12208999  0.23119298 -0.05842072 -0.37407075], action=0, reward=1.0, next_state=[ 0.12671385  0.03694745 -0.06590214 -0.10036545]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 37 ] state=[ 0.12671385  0.03694745 -0.06590214 -0.10036545], action=0, reward=1.0, next_state=[ 0.1274528  -0.15717115 -0.06790945  0.17081917]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 38 ] state=[ 0.1274528  -0.15717115 -0.06790945  0.17081917], action=1, reward=1.0, next_state=[ 0.12430938  0.03885375 -0.06449307 -0.14249048]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 39 ] state=[ 0.12430938  0.03885375 -0.06449307 -0.14249048], action=1, reward=1.0, next_state=[ 0.12508645  0.23483723 -0.06734288 -0.45480235]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 40 ] state=[ 0.12508645  0.23483723 -0.06734288 -0.45480235], action=0, reward=1.0, next_state=[ 0.1297832   0.04072889 -0.07643892 -0.18408366]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 41 ] state=[ 0.1297832   0.04072889 -0.07643892 -0.18408366], action=0, reward=1.0, next_state=[ 0.13059778 -0.15322085 -0.0801206   0.08354027]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 42 ] state=[ 0.13059778 -0.15322085 -0.0801206   0.08354027], action=0, reward=1.0, next_state=[ 0.12753336 -0.34710833 -0.07844979  0.34990827]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 43 ] state=[ 0.12753336 -0.34710833 -0.07844979  0.34990827], action=1, reward=1.0, next_state=[ 0.12059119 -0.15096342 -0.07145162  0.03355521]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 44 ] state=[ 0.12059119 -0.15096342 -0.07145162  0.03355521], action=0, reward=1.0, next_state=[ 0.11757192 -0.34499185 -0.07078052  0.30286633]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 45 ] state=[ 0.11757192 -0.34499185 -0.07078052  0.30286633], action=1, reward=1.0, next_state=[ 0.11067209 -0.14893627 -0.06472319 -0.01127279]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 46 ] state=[ 0.11067209 -0.14893627 -0.06472319 -0.01127279], action=0, reward=1.0, next_state=[ 0.10769336 -0.34307317 -0.06494865  0.26030749]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 47 ] state=[ 0.10769336 -0.34307317 -0.06494865  0.26030749], action=0, reward=1.0, next_state=[ 0.1008319  -0.53721073 -0.0597425   0.53181837]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 48 ] state=[ 0.1008319  -0.53721073 -0.0597425   0.53181837], action=1, reward=1.0, next_state=[ 0.09008768 -0.3413016  -0.04910613  0.22092509]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 49 ] state=[ 0.09008768 -0.3413016  -0.04910613  0.22092509], action=1, reward=1.0, next_state=[ 0.08326165 -0.14551338 -0.04468763 -0.08683461]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 50 ] state=[ 0.08326165 -0.14551338 -0.04468763 -0.08683461], action=1, reward=1.0, next_state=[ 0.08035138  0.05021969 -0.04642432 -0.39327491]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 51 ] state=[ 0.08035138  0.05021969 -0.04642432 -0.39327491], action=1, reward=1.0, next_state=[ 0.08135578  0.24596863 -0.05428982 -0.70022581]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 52 ] state=[ 0.08135578  0.24596863 -0.05428982 -0.70022581], action=0, reward=1.0, next_state=[ 0.08627515  0.05163966 -0.06829434 -0.42511518]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 53 ] state=[ 0.08627515  0.05163966 -0.06829434 -0.42511518], action=0, reward=1.0, next_state=[ 0.08730794 -0.14245179 -0.07679664 -0.15471962]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 54 ] state=[ 0.08730794 -0.14245179 -0.07679664 -0.15471962], action=1, reward=1.0, next_state=[ 0.08445891  0.05368094 -0.07989103 -0.47060763]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 55 ] state=[ 0.08445891  0.05368094 -0.07989103 -0.47060763], action=1, reward=1.0, next_state=[ 0.08553253  0.24983512 -0.08930319 -0.78736341]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 56 ] state=[ 0.08553253  0.24983512 -0.08930319 -0.78736341], action=0, reward=1.0, next_state=[ 0.09052923  0.05604596 -0.10505045 -0.52405825]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 57 ] state=[ 0.09052923  0.05604596 -0.10505045 -0.52405825], action=0, reward=1.0, next_state=[ 0.09165015 -0.1374529  -0.11553162 -0.26623808]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 58 ] state=[ 0.09165015 -0.1374529  -0.11553162 -0.26623808], action=1, reward=1.0, next_state=[ 0.08890109  0.05911214 -0.12085638 -0.59301087]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 59 ] state=[ 0.08890109  0.05911214 -0.12085638 -0.59301087], action=0, reward=1.0, next_state=[ 0.09008333 -0.13412922 -0.1327166  -0.3407085 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 60 ] state=[ 0.09008333 -0.13412922 -0.1327166  -0.3407085 ], action=0, reward=1.0, next_state=[ 0.08740075 -0.32713778 -0.13953077 -0.09264584]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 61 ] state=[ 0.08740075 -0.32713778 -0.13953077 -0.09264584], action=1, reward=1.0, next_state=[ 0.08085799 -0.13032035 -0.14138368 -0.42589186]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 62 ] state=[ 0.08085799 -0.13032035 -0.14138368 -0.42589186], action=0, reward=1.0, next_state=[ 0.07825159 -0.32318629 -0.14990152 -0.18090805]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 63 ] state=[ 0.07825159 -0.32318629 -0.14990152 -0.18090805], action=0, reward=1.0, next_state=[ 0.07178786 -0.5158808  -0.15351968  0.06098616]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 64 ] state=[ 0.07178786 -0.5158808  -0.15351968  0.06098616], action=1, reward=1.0, next_state=[ 0.06147024 -0.31892912 -0.15229996 -0.27592453]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 65 ] state=[ 0.06147024 -0.31892912 -0.15229996 -0.27592453], action=1, reward=1.0, next_state=[ 0.05509166 -0.12199951 -0.15781845 -0.61250298]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 66 ] state=[ 0.05509166 -0.12199951 -0.15781845 -0.61250298], action=0, reward=1.0, next_state=[ 0.05265167 -0.31460493 -0.17006851 -0.37339152]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 67 ] state=[ 0.05265167 -0.31460493 -0.17006851 -0.37339152], action=0, reward=1.0, next_state=[ 0.04635957 -0.50695446 -0.17753634 -0.13878917]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 68 ] state=[ 0.04635957 -0.50695446 -0.17753634 -0.13878917], action=0, reward=1.0, next_state=[ 0.03622048 -0.69914815 -0.18031212  0.09304802]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 69 ] state=[ 0.03622048 -0.69914815 -0.18031212  0.09304802], action=1, reward=1.0, next_state=[ 0.02223752 -0.5019615  -0.17845116 -0.25066167]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 70 ] state=[ 0.02223752 -0.5019615  -0.17845116 -0.25066167], action=1, reward=1.0, next_state=[ 0.01219829 -0.30479972 -0.1834644  -0.59389452]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 71 ] state=[ 0.01219829 -0.30479972 -0.1834644  -0.59389452], action=0, reward=1.0, next_state=[ 0.0061023  -0.496944   -0.19534229 -0.36415152]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 98 ][ timestamp 72 ] state=[ 0.0061023  -0.496944   -0.19534229 -0.36415152], action=1, reward=1.0, next_state=[-0.00383658 -0.29966026 -0.20262532 -0.7115151 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 73 ] state=[-0.00383658 -0.29966026 -0.20262532 -0.7115151 ], action=0, reward=-1.0, next_state=[-0.00982979 -0.49148611 -0.21685562 -0.48882802]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 98: Exploration_rate=0.01. Score=73.\n",
      "[ episode 99 ] state=[-0.00680511 -0.01631518  0.0374257   0.02566912]\n",
      "[ episode 99 ][ timestamp 1 ] state=[-0.00680511 -0.01631518  0.0374257   0.02566912], action=0, reward=1.0, next_state=[-0.00713141 -0.2119533   0.03793908  0.3299214 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 2 ] state=[-0.00713141 -0.2119533   0.03793908  0.3299214 ], action=1, reward=1.0, next_state=[-0.01137048 -0.01739138  0.04453751  0.04943995]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 3 ] state=[-0.01137048 -0.01739138  0.04453751  0.04943995], action=1, reward=1.0, next_state=[-0.01171831  0.17706458  0.04552631 -0.22886506]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 4 ] state=[-0.01171831  0.17706458  0.04552631 -0.22886506], action=1, reward=1.0, next_state=[-0.00817702  0.37150738  0.04094901 -0.50684693]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 5 ] state=[-0.00817702  0.37150738  0.04094901 -0.50684693], action=0, reward=1.0, next_state=[-0.00074687  0.17583307  0.03081207 -0.20154587]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 6 ] state=[-0.00074687  0.17583307  0.03081207 -0.20154587], action=1, reward=1.0, next_state=[ 0.00276979  0.37050111  0.02678115 -0.48435202]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 7 ] state=[ 0.00276979  0.37050111  0.02678115 -0.48435202], action=1, reward=1.0, next_state=[ 0.01017982  0.56523508  0.01709411 -0.76847551]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 8 ] state=[ 0.01017982  0.56523508  0.01709411 -0.76847551], action=0, reward=1.0, next_state=[ 0.02148452  0.36988204  0.0017246  -0.47046334]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 9 ] state=[ 0.02148452  0.36988204  0.0017246  -0.47046334], action=1, reward=1.0, next_state=[ 0.02888216  0.56497959 -0.00768466 -0.7626022 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 10 ] state=[ 0.02888216  0.56497959 -0.00768466 -0.7626022 ], action=1, reward=1.0, next_state=[ 0.04018175  0.76020655 -0.02293671 -1.05769325]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 11 ] state=[ 0.04018175  0.76020655 -0.02293671 -1.05769325], action=0, reward=1.0, next_state=[ 0.05538588  0.56539589 -0.04409057 -0.77229693]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 12 ] state=[ 0.05538588  0.56539589 -0.04409057 -0.77229693], action=0, reward=1.0, next_state=[ 0.0666938   0.37090744 -0.05953651 -0.4938062 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 13 ] state=[ 0.0666938   0.37090744 -0.05953651 -0.4938062 ], action=0, reward=1.0, next_state=[ 0.07411195  0.17667348 -0.06941264 -0.22046486]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 14 ] state=[ 0.07411195  0.17667348 -0.06941264 -0.22046486], action=0, reward=1.0, next_state=[ 0.07764542 -0.01739117 -0.07382193  0.04954019]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 15 ] state=[ 0.07764542 -0.01739117 -0.07382193  0.04954019], action=1, reward=1.0, next_state=[ 0.07729759  0.17870742 -0.07283113 -0.26549049]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 16 ] state=[ 0.07729759  0.17870742 -0.07283113 -0.26549049], action=1, reward=1.0, next_state=[ 0.08087174  0.37478921 -0.07814094 -0.58022689]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 17 ] state=[ 0.08087174  0.37478921 -0.07814094 -0.58022689], action=0, reward=1.0, next_state=[ 0.08836753  0.1808442  -0.08974548 -0.31314715]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 18 ] state=[ 0.08836753  0.1808442  -0.08974548 -0.31314715], action=0, reward=1.0, next_state=[ 0.09198441 -0.01289228 -0.09600842 -0.05006172]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 19 ] state=[ 0.09198441 -0.01289228 -0.09600842 -0.05006172], action=0, reward=1.0, next_state=[ 0.09172656 -0.20651582 -0.09700965  0.21085293]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 20 ] state=[ 0.09172656 -0.20651582 -0.09700965  0.21085293], action=1, reward=1.0, next_state=[ 0.08759625 -0.01015031 -0.0927926  -0.11078656]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 21 ] state=[ 0.08759625 -0.01015031 -0.0927926  -0.11078656], action=0, reward=1.0, next_state=[ 0.08739324 -0.20382854 -0.09500833  0.15123905]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 22 ] state=[ 0.08739324 -0.20382854 -0.09500833  0.15123905], action=1, reward=1.0, next_state=[ 0.08331667 -0.00748363 -0.09198355 -0.16984053]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 23 ] state=[ 0.08331667 -0.00748363 -0.09198355 -0.16984053], action=1, reward=1.0, next_state=[ 0.083167    0.1888263  -0.09538036 -0.49006561]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 24 ] state=[ 0.083167    0.1888263  -0.09538036 -0.49006561], action=0, reward=1.0, next_state=[ 0.08694352 -0.00482984 -0.10518167 -0.22890106]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 25 ] state=[ 0.08694352 -0.00482984 -0.10518167 -0.22890106], action=0, reward=1.0, next_state=[ 0.08684693 -0.19830376 -0.10975969  0.02883955]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 26 ] state=[ 0.08684693 -0.19830376 -0.10975969  0.02883955], action=1, reward=1.0, next_state=[ 0.08288085 -0.00179292 -0.1091829  -0.29635753]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 27 ] state=[ 0.08288085 -0.00179292 -0.1091829  -0.29635753], action=1, reward=1.0, next_state=[ 0.08284499  0.19470241 -0.11511005 -0.62138149]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 28 ] state=[ 0.08284499  0.19470241 -0.11511005 -0.62138149], action=0, reward=1.0, next_state=[ 0.08673904  0.00136012 -0.12753768 -0.36705499]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 29 ] state=[ 0.08673904  0.00136012 -0.12753768 -0.36705499], action=0, reward=1.0, next_state=[ 0.08676625 -0.19174066 -0.13487878 -0.11715085]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 30 ] state=[ 0.08676625 -0.19174066 -0.13487878 -0.11715085], action=0, reward=1.0, next_state=[ 0.08293143 -0.38469813 -0.1372218   0.13012235]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 31 ] state=[ 0.08293143 -0.38469813 -0.1372218   0.13012235], action=1, reward=1.0, next_state=[ 0.07523747 -0.18790453 -0.13461935 -0.20250993]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 32 ] state=[ 0.07523747 -0.18790453 -0.13461935 -0.20250993], action=0, reward=1.0, next_state=[ 0.07147938 -0.38087012 -0.13866955  0.044861  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 33 ] state=[ 0.07147938 -0.38087012 -0.13866955  0.044861  ], action=1, reward=1.0, next_state=[ 0.06386198 -0.18406026 -0.13777233 -0.28815827]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 34 ] state=[ 0.06386198 -0.18406026 -0.13777233 -0.28815827], action=0, reward=1.0, next_state=[ 0.06018077 -0.37697614 -0.14353549 -0.04190348]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 35 ] state=[ 0.06018077 -0.37697614 -0.14353549 -0.04190348], action=0, reward=1.0, next_state=[ 0.05264125 -0.56977933 -0.14437356  0.20227256]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 36 ] state=[ 0.05264125 -0.56977933 -0.14437356  0.20227256], action=1, reward=1.0, next_state=[ 0.04124566 -0.3729194  -0.14032811 -0.13224372]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 37 ] state=[ 0.04124566 -0.3729194  -0.14032811 -0.13224372], action=1, reward=1.0, next_state=[ 0.03378727 -0.17609539 -0.14297299 -0.46569882]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 38 ] state=[ 0.03378727 -0.17609539 -0.14297299 -0.46569882], action=0, reward=1.0, next_state=[ 0.03026537 -0.3689384  -0.15228696 -0.22127673]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 39 ] state=[ 0.03026537 -0.3689384  -0.15228696 -0.22127673], action=1, reward=1.0, next_state=[ 0.0228866  -0.17200489 -0.1567125  -0.55785776]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 40 ] state=[ 0.0228866  -0.17200489 -0.1567125  -0.55785776], action=0, reward=1.0, next_state=[ 0.0194465  -0.36462005 -0.16786965 -0.31836068]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 41 ] state=[ 0.0194465  -0.36462005 -0.16786965 -0.31836068], action=0, reward=1.0, next_state=[ 0.0121541  -0.55700303 -0.17423687 -0.08296492]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 99 ][ timestamp 42 ] state=[ 0.0121541  -0.55700303 -0.17423687 -0.08296492], action=0, reward=1.0, next_state=[ 0.00101404 -0.74925485 -0.17589616  0.15007966]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 43 ] state=[ 0.00101404 -0.74925485 -0.17589616  0.15007966], action=1, reward=1.0, next_state=[-0.01397106 -0.55210757 -0.17289457 -0.19252552]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 44 ] state=[-0.01397106 -0.55210757 -0.17289457 -0.19252552], action=1, reward=1.0, next_state=[-0.02501321 -0.35498827 -0.17674508 -0.53437431]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 45 ] state=[-0.02501321 -0.35498827 -0.17674508 -0.53437431], action=0, reward=1.0, next_state=[-0.03211298 -0.547242   -0.18743257 -0.30217929]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 46 ] state=[-0.03211298 -0.547242   -0.18743257 -0.30217929], action=0, reward=1.0, next_state=[-0.04305782 -0.73926715 -0.19347615 -0.07396938]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 47 ] state=[-0.04305782 -0.73926715 -0.19347615 -0.07396938], action=1, reward=1.0, next_state=[-0.05784316 -0.54197389 -0.19495554 -0.42091532]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 48 ] state=[-0.05784316 -0.54197389 -0.19495554 -0.42091532], action=1, reward=1.0, next_state=[-0.06868264 -0.3447015  -0.20337385 -0.76817285]\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 49 ] state=[-0.06868264 -0.3447015  -0.20337385 -0.76817285], action=0, reward=-1.0, next_state=[-0.07557667 -0.53653022 -0.2187373  -0.54574054]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 99: Exploration_rate=0.01. Score=49.\n",
      "[ episode 100 ] state=[ 0.02512704 -0.04404248 -0.03578977 -0.00153519]\n",
      "[ episode 100 ][ timestamp 1 ] state=[ 0.02512704 -0.04404248 -0.03578977 -0.00153519], action=1, reward=1.0, next_state=[ 0.02424619  0.15157399 -0.03582048 -0.30529193]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 2 ] state=[ 0.02424619  0.15157399 -0.03582048 -0.30529193], action=0, reward=1.0, next_state=[ 0.02727767 -0.04301968 -0.04192632 -0.02411764]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 3 ] state=[ 0.02727767 -0.04301968 -0.04192632 -0.02411764], action=0, reward=1.0, next_state=[ 0.02641727 -0.23751607 -0.04240867  0.25504784]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 4 ] state=[ 0.02641727 -0.23751607 -0.04240867  0.25504784], action=0, reward=1.0, next_state=[ 0.02166695 -0.43200766 -0.03730771  0.53405851]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 5 ] state=[ 0.02166695 -0.43200766 -0.03730771  0.53405851], action=1, reward=1.0, next_state=[ 0.0130268  -0.23638145 -0.02662654  0.22985746]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 6 ] state=[ 0.0130268  -0.23638145 -0.02662654  0.22985746], action=1, reward=1.0, next_state=[ 0.00829917 -0.04088933 -0.02202939 -0.07110406]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 7 ] state=[ 0.00829917 -0.04088933 -0.02202939 -0.07110406], action=1, reward=1.0, next_state=[ 0.00748138  0.15454141 -0.02345147 -0.37065515]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 8 ] state=[ 0.00748138  0.15454141 -0.02345147 -0.37065515], action=0, reward=1.0, next_state=[ 0.01057221 -0.04023965 -0.03086458 -0.08545801]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 9 ] state=[ 0.01057221 -0.04023965 -0.03086458 -0.08545801], action=0, reward=1.0, next_state=[ 0.00976742 -0.23490589 -0.03257374  0.19732954]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 10 ] state=[ 0.00976742 -0.23490589 -0.03257374  0.19732954], action=1, reward=1.0, next_state=[ 0.0050693  -0.03933353 -0.02862715 -0.10544837]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 11 ] state=[ 0.0050693  -0.03933353 -0.02862715 -0.10544837], action=1, reward=1.0, next_state=[ 0.00428263  0.15618672 -0.03073611 -0.40702381]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 12 ] state=[ 0.00428263  0.15618672 -0.03073611 -0.40702381], action=0, reward=1.0, next_state=[ 0.00740636 -0.03848622 -0.03887659 -0.12418731]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 13 ] state=[ 0.00740636 -0.03848622 -0.03887659 -0.12418731], action=1, reward=1.0, next_state=[ 0.00663664  0.1571705  -0.04136034 -0.42887748]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 14 ] state=[ 0.00663664  0.1571705  -0.04136034 -0.42887748], action=1, reward=1.0, next_state=[ 0.00978005  0.35285305 -0.04993789 -0.73430674]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 15 ] state=[ 0.00978005  0.35285305 -0.04993789 -0.73430674], action=0, reward=1.0, next_state=[ 0.01683711  0.15845527 -0.06462402 -0.45774923]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 16 ] state=[ 0.01683711  0.15845527 -0.06462402 -0.45774923], action=0, reward=1.0, next_state=[ 0.02000622 -0.03569641 -0.073779   -0.18611586]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 17 ] state=[ 0.02000622 -0.03569641 -0.073779   -0.18611586], action=0, reward=1.0, next_state=[ 0.01929229 -0.22968946 -0.07750132  0.08241075]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 18 ] state=[ 0.01929229 -0.22968946 -0.07750132  0.08241075], action=1, reward=1.0, next_state=[ 0.0146985  -0.03354702 -0.07585311 -0.23368235]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 19 ] state=[ 0.0146985  -0.03354702 -0.07585311 -0.23368235], action=0, reward=1.0, next_state=[ 0.01402756 -0.22750783 -0.08052675  0.03414284]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 20 ] state=[ 0.01402756 -0.22750783 -0.08052675  0.03414284], action=0, reward=1.0, next_state=[ 0.0094774  -0.42138819 -0.0798439   0.30037168]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 21 ] state=[ 0.0094774  -0.42138819 -0.0798439   0.30037168], action=1, reward=1.0, next_state=[ 0.00104964 -0.22522435 -0.07383646 -0.01638583]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 22 ] state=[ 0.00104964 -0.22522435 -0.07383646 -0.01638583], action=1, reward=1.0, next_state=[-0.00345485 -0.02912543 -0.07416418 -0.33142096]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 23 ] state=[-0.00345485 -0.02912543 -0.07416418 -0.33142096], action=0, reward=1.0, next_state=[-0.00403736 -0.22311766 -0.0807926  -0.0630168 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 24 ] state=[-0.00403736 -0.22311766 -0.0807926  -0.0630168 ], action=0, reward=1.0, next_state=[-0.00849971 -0.41699388 -0.08205294  0.20312172]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 25 ] state=[-0.00849971 -0.41699388 -0.08205294  0.20312172], action=1, reward=1.0, next_state=[-0.01683959 -0.22080018 -0.0779905  -0.11427521]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 26 ] state=[-0.01683959 -0.22080018 -0.0779905  -0.11427521], action=1, reward=1.0, next_state=[-0.02125559 -0.02465237 -0.08027601 -0.43050854]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 27 ] state=[-0.02125559 -0.02465237 -0.08027601 -0.43050854], action=0, reward=1.0, next_state=[-0.02174864 -0.21855126 -0.08888618 -0.16417266]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 28 ] state=[-0.02174864 -0.21855126 -0.08888618 -0.16417266], action=0, reward=1.0, next_state=[-0.02611966 -0.41229578 -0.09216963  0.09919869]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 29 ] state=[-0.02611966 -0.41229578 -0.09216963  0.09919869], action=1, reward=1.0, next_state=[-0.03436558 -0.21598202 -0.09018566 -0.22108155]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 30 ] state=[-0.03436558 -0.21598202 -0.09018566 -0.22108155], action=0, reward=1.0, next_state=[-0.03868522 -0.40970689 -0.09460729  0.04184617]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 31 ] state=[-0.03868522 -0.40970689 -0.09460729  0.04184617], action=0, reward=1.0, next_state=[-0.04687936 -0.60335384 -0.09377036  0.30324455]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 32 ] state=[-0.04687936 -0.60335384 -0.09377036  0.30324455], action=1, reward=1.0, next_state=[-0.05894643 -0.40702929 -0.08770547 -0.01747663]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 33 ] state=[-0.05894643 -0.40702929 -0.08770547 -0.01747663], action=1, reward=1.0, next_state=[-0.06708702 -0.21076617 -0.088055   -0.33649213]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 34 ] state=[-0.06708702 -0.21076617 -0.088055   -0.33649213], action=1, reward=1.0, next_state=[-0.07130234 -0.01450867 -0.09478485 -0.65559255]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 35 ] state=[-0.07130234 -0.01450867 -0.09478485 -0.65559255], action=0, reward=1.0, next_state=[-0.07159252 -0.20819213 -0.1078967  -0.39419647]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 100 ][ timestamp 36 ] state=[-0.07159252 -0.20819213 -0.1078967  -0.39419647], action=1, reward=1.0, next_state=[-0.07575636 -0.01171781 -0.11578063 -0.71885427]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 37 ] state=[-0.07575636 -0.01171781 -0.11578063 -0.71885427], action=0, reward=1.0, next_state=[-0.07599072 -0.20506354 -0.13015771 -0.46474089]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 38 ] state=[-0.07599072 -0.20506354 -0.13015771 -0.46474089], action=0, reward=1.0, next_state=[-0.08009199 -0.39812922 -0.13945253 -0.21575036]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 39 ] state=[-0.08009199 -0.39812922 -0.13945253 -0.21575036], action=0, reward=1.0, next_state=[-0.08805457 -0.59101067 -0.14376754  0.02989686]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 40 ] state=[-0.08805457 -0.59101067 -0.14376754  0.02989686], action=0, reward=1.0, next_state=[-0.09987478 -0.78380963 -0.1431696   0.27398949]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 41 ] state=[-0.09987478 -0.78380963 -0.1431696   0.27398949], action=1, reward=1.0, next_state=[-0.11555098 -0.58696602 -0.13768981 -0.06020318]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 42 ] state=[-0.11555098 -0.58696602 -0.13768981 -0.06020318], action=0, reward=1.0, next_state=[-0.1272903  -0.77987274 -0.13889387  0.18606529]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 43 ] state=[-0.1272903  -0.77987274 -0.13889387  0.18606529], action=0, reward=1.0, next_state=[-0.14288775 -0.97276241 -0.13517257  0.4319098 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 44 ] state=[-0.14288775 -0.97276241 -0.13517257  0.4319098 ], action=1, reward=1.0, next_state=[-0.162343   -0.77601139 -0.12653437  0.09985556]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 45 ] state=[-0.162343   -0.77601139 -0.12653437  0.09985556], action=1, reward=1.0, next_state=[-0.17786323 -0.57932448 -0.12453726 -0.229918  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 46 ] state=[-0.17786323 -0.57932448 -0.12453726 -0.229918  ], action=1, reward=1.0, next_state=[-0.18944972 -0.38266333 -0.12913562 -0.55914448]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 47 ] state=[-0.18944972 -0.38266333 -0.12913562 -0.55914448], action=1, reward=1.0, next_state=[-0.19710298 -0.18598783 -0.14031851 -0.88956176]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 48 ] state=[-0.19710298 -0.18598783 -0.14031851 -0.88956176], action=1, reward=1.0, next_state=[-0.20082274  0.01073065 -0.15810975 -1.22285772]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 49 ] state=[-0.20082274  0.01073065 -0.15810975 -1.22285772], action=0, reward=1.0, next_state=[-0.20060813 -0.1820414  -0.1825669  -0.98359724]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 50 ] state=[-0.20060813 -0.1820414  -0.1825669  -0.98359724], action=0, reward=1.0, next_state=[-0.20424896 -0.3743105  -0.20223885 -0.75336358]\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 51 ] state=[-0.20424896 -0.3743105  -0.20223885 -0.75336358], action=0, reward=-1.0, next_state=[-0.21173517 -0.56615536 -0.21730612 -0.53051492]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 100: Exploration_rate=0.01. Score=51.\n",
      "[ episode 101 ] state=[-0.04983246  0.00198069 -0.01596287  0.02663866]\n",
      "[ episode 101 ][ timestamp 1 ] state=[-0.04983246  0.00198069 -0.01596287  0.02663866], action=0, reward=1.0, next_state=[-0.04979284 -0.19290874 -0.01543009  0.31424268]\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 2 ] state=[-0.04979284 -0.19290874 -0.01543009  0.31424268], action=1, reward=1.0, next_state=[-0.05365102  0.00242957 -0.00914524  0.01673382]\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 3 ] state=[-0.05365102  0.00242957 -0.00914524  0.01673382], action=1, reward=1.0, next_state=[-0.05360243  0.19768148 -0.00881056 -0.27882045]\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 4 ] state=[-0.05360243  0.19768148 -0.00881056 -0.27882045], action=1, reward=1.0, next_state=[-0.0496488   0.392928   -0.01438697 -0.57426914]\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 5 ] state=[-0.0496488   0.392928   -0.01438697 -0.57426914], action=1, reward=1.0, next_state=[-0.04179024  0.58824867 -0.02587236 -0.87144944]\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 6 ] state=[-0.04179024  0.58824867 -0.02587236 -0.87144944], action=1, reward=1.0, next_state=[-0.03002526  0.78371276 -0.04330135 -1.17215309]\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 7 ] state=[-0.03002526  0.78371276 -0.04330135 -1.17215309], action=0, reward=1.0, next_state=[-0.01435101  0.58917969 -0.06674441 -0.89335361]\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 8 ] state=[-0.01435101  0.58917969 -0.06674441 -0.89335361], action=0, reward=1.0, next_state=[-0.00256742  0.39502337 -0.08461148 -0.62237589]\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 9 ] state=[-0.00256742  0.39502337 -0.08461148 -0.62237589], action=0, reward=1.0, next_state=[ 0.00533305  0.20117845 -0.097059   -0.35749481]\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 10 ] state=[ 0.00533305  0.20117845 -0.097059   -0.35749481], action=1, reward=1.0, next_state=[ 0.00935662  0.39753664 -0.10420889 -0.6791364 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 11 ] state=[ 0.00935662  0.39753664 -0.10420889 -0.6791364 ], action=0, reward=1.0, next_state=[ 0.01730735  0.20400482 -0.11779162 -0.42099548]\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 12 ] state=[ 0.01730735  0.20400482 -0.11779162 -0.42099548], action=0, reward=1.0, next_state=[ 0.02138745  0.01073158 -0.12621153 -0.16764524]\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 13 ] state=[ 0.02138745  0.01073158 -0.12621153 -0.16764524], action=1, reward=1.0, next_state=[ 0.02160208  0.20741295 -0.12956444 -0.49732841]\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 14 ] state=[ 0.02160208  0.20741295 -0.12956444 -0.49732841], action=1, reward=1.0, next_state=[ 0.02575034  0.40410089 -0.139511   -0.82787291]\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 15 ] state=[ 0.02575034  0.40410089 -0.139511   -0.82787291], action=1, reward=1.0, next_state=[ 0.03383236  0.60082644 -0.15606846 -1.1609775 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 16 ] state=[ 0.03383236  0.60082644 -0.15606846 -1.1609775 ], action=0, reward=1.0, next_state=[ 0.04584889  0.40804298 -0.17928801 -0.92101502]\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 17 ] state=[ 0.04584889  0.40804298 -0.17928801 -0.92101502], action=0, reward=1.0, next_state=[ 0.05400975  0.21573743 -0.19770831 -0.68960916]\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 18 ] state=[ 0.05400975  0.21573743 -0.19770831 -0.68960916], action=0, reward=-1.0, next_state=[ 0.0583245   0.02382823 -0.2115005  -0.46510146]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 101: Exploration_rate=0.01. Score=18.\n",
      "[ episode 102 ] state=[-0.03926217  0.0174265   0.01290433 -0.0249585 ]\n",
      "[ episode 102 ][ timestamp 1 ] state=[-0.03926217  0.0174265   0.01290433 -0.0249585 ], action=1, reward=1.0, next_state=[-0.03891364  0.21236104  0.01240516 -0.3135422 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 2 ] state=[-0.03891364  0.21236104  0.01240516 -0.3135422 ], action=0, reward=1.0, next_state=[-0.03466642  0.01706459  0.00613431 -0.01697304]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 3 ] state=[-0.03466642  0.01706459  0.00613431 -0.01697304], action=1, reward=1.0, next_state=[-0.03432513  0.21209803  0.00579485 -0.30771422]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 4 ] state=[-0.03432513  0.21209803  0.00579485 -0.30771422], action=0, reward=1.0, next_state=[-0.03008317  0.01689399 -0.00035943 -0.0132094 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 5 ] state=[-0.03008317  0.01689399 -0.00035943 -0.0132094 ], action=0, reward=1.0, next_state=[-0.02974529 -0.1782228  -0.00062362  0.2793601 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 6 ] state=[-0.02974529 -0.1782228  -0.00062362  0.2793601 ], action=1, reward=1.0, next_state=[-0.03330975  0.01690804  0.00496358 -0.01351945]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 7 ] state=[-0.03330975  0.01690804  0.00496358 -0.01351945], action=0, reward=1.0, next_state=[-0.03297159 -0.17828475  0.00469319  0.28072541]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 8 ] state=[-0.03297159 -0.17828475  0.00469319  0.28072541], action=0, reward=1.0, next_state=[-0.03653728 -0.37347333  0.0103077   0.57488485]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 9 ] state=[-0.03653728 -0.37347333  0.0103077   0.57488485], action=1, reward=1.0, next_state=[-0.04400675 -0.17849739  0.0218054   0.28546689]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 10 ] state=[-0.04400675 -0.17849739  0.0218054   0.28546689], action=1, reward=1.0, next_state=[-0.04757669  0.0163069   0.02751474 -0.0002598 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 11 ] state=[-0.04757669  0.0163069   0.02751474 -0.0002598 ], action=0, reward=1.0, next_state=[-0.04725056 -0.17919863  0.02750954  0.3009758 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 102 ][ timestamp 12 ] state=[-0.04725056 -0.17919863  0.02750954  0.3009758 ], action=1, reward=1.0, next_state=[-0.05083453  0.01552065  0.03352906  0.01709419]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 13 ] state=[-0.05083453  0.01552065  0.03352906  0.01709419], action=1, reward=1.0, next_state=[-0.05052412  0.21014611  0.03387094 -0.26482422]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 14 ] state=[-0.05052412  0.21014611  0.03387094 -0.26482422], action=0, reward=1.0, next_state=[-0.04632119  0.0145575   0.02857446  0.03834657]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 15 ] state=[-0.04632119  0.0145575   0.02857446  0.03834657], action=1, reward=1.0, next_state=[-0.04603004  0.20925829  0.02934139 -0.24518564]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 16 ] state=[-0.04603004  0.20925829  0.02934139 -0.24518564], action=1, reward=1.0, next_state=[-0.04184488  0.40394914  0.02443768 -0.52847109]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 17 ] state=[-0.04184488  0.40394914  0.02443768 -0.52847109], action=1, reward=1.0, next_state=[-0.0337659   0.59871891  0.01386825 -0.81335455]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 18 ] state=[-0.0337659   0.59871891  0.01386825 -0.81335455], action=0, reward=1.0, next_state=[-0.02179152  0.40340979 -0.00239884 -0.51634191]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 19 ] state=[-0.02179152  0.40340979 -0.00239884 -0.51634191], action=1, reward=1.0, next_state=[-0.01372332  0.59856544 -0.01272568 -0.80977979]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 20 ] state=[-0.01372332  0.59856544 -0.01272568 -0.80977979], action=1, reward=1.0, next_state=[-0.00175201  0.79385942 -0.02892127 -1.10643829]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 21 ] state=[-0.00175201  0.79385942 -0.02892127 -1.10643829], action=0, reward=1.0, next_state=[ 0.01412518  0.59912939 -0.05105004 -0.82296707]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 22 ] state=[ 0.01412518  0.59912939 -0.05105004 -0.82296707], action=0, reward=1.0, next_state=[ 0.02610776  0.40474162 -0.06750938 -0.54676747]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 23 ] state=[ 0.02610776  0.40474162 -0.06750938 -0.54676747], action=0, reward=1.0, next_state=[ 0.0342026   0.21062987 -0.07844473 -0.27609578]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 24 ] state=[ 0.0342026   0.21062987 -0.07844473 -0.27609578], action=0, reward=1.0, next_state=[ 0.03841519  0.01670963 -0.08396664 -0.00914904]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 25 ] state=[ 0.03841519  0.01670963 -0.08396664 -0.00914904], action=1, reward=1.0, next_state=[ 0.03874939  0.21292914 -0.08414962 -0.32709854]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 26 ] state=[ 0.03874939  0.21292914 -0.08414962 -0.32709854], action=0, reward=1.0, next_state=[ 0.04300797  0.01909974 -0.09069159 -0.06209404]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 27 ] state=[ 0.04300797  0.01909974 -0.09069159 -0.06209404], action=1, reward=1.0, next_state=[ 0.04338996  0.21539707 -0.09193348 -0.38195675]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 28 ] state=[ 0.04338996  0.21539707 -0.09193348 -0.38195675], action=1, reward=1.0, next_state=[ 0.04769791  0.41169595 -0.09957261 -0.70215202]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 29 ] state=[ 0.04769791  0.41169595 -0.09957261 -0.70215202], action=1, reward=1.0, next_state=[ 0.05593182  0.6080467  -0.11361565 -1.02444529]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 30 ] state=[ 0.05593182  0.6080467  -0.11361565 -1.02444529], action=0, reward=1.0, next_state=[ 0.06809276  0.41460603 -0.13410456 -0.76948622]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 31 ] state=[ 0.06809276  0.41460603 -0.13410456 -0.76948622], action=0, reward=1.0, next_state=[ 0.07638488  0.22155963 -0.14949428 -0.52182521]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 32 ] state=[ 0.07638488  0.22155963 -0.14949428 -0.52182521], action=0, reward=1.0, next_state=[ 0.08081607  0.02882311 -0.15993079 -0.27973275]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 33 ] state=[ 0.08081607  0.02882311 -0.15993079 -0.27973275], action=1, reward=1.0, next_state=[ 0.08139253  0.22582207 -0.16552544 -0.6182796 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 34 ] state=[ 0.08139253  0.22582207 -0.16552544 -0.6182796 ], action=0, reward=1.0, next_state=[ 0.08590897  0.03335178 -0.17789103 -0.38196276]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 35 ] state=[ 0.08590897  0.03335178 -0.17789103 -0.38196276], action=0, reward=1.0, next_state=[ 0.08657601 -0.15885724 -0.18553029 -0.15022365]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 36 ] state=[ 0.08657601 -0.15885724 -0.18553029 -0.15022365], action=0, reward=1.0, next_state=[ 0.08339887 -0.35090494 -0.18853476  0.07867066]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 37 ] state=[ 0.08339887 -0.35090494 -0.18853476  0.07867066], action=1, reward=1.0, next_state=[ 0.07638077 -0.15365086 -0.18696135 -0.26706884]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 38 ] state=[ 0.07638077 -0.15365086 -0.18696135 -0.26706884], action=0, reward=1.0, next_state=[ 0.07330775 -0.34568105 -0.19230272 -0.03869014]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 39 ] state=[ 0.07330775 -0.34568105 -0.19230272 -0.03869014], action=0, reward=1.0, next_state=[ 0.06639413 -0.53759993 -0.19307653  0.18769246]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 40 ] state=[ 0.06639413 -0.53759993 -0.19307653  0.18769246], action=0, reward=1.0, next_state=[ 0.05564213 -0.72951077 -0.18932268  0.41379729]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 41 ] state=[ 0.05564213 -0.72951077 -0.18932268  0.41379729], action=0, reward=1.0, next_state=[ 0.04105191 -0.92151584 -0.18104673  0.64132979]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 42 ] state=[ 0.04105191 -0.92151584 -0.18104673  0.64132979], action=1, reward=1.0, next_state=[ 0.0226216  -0.72439366 -0.16822014  0.29754182]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 43 ] state=[ 0.0226216  -0.72439366 -0.16822014  0.29754182], action=0, reward=1.0, next_state=[ 0.00813372 -0.91676822 -0.1622693   0.53280662]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 44 ] state=[ 0.00813372 -0.91676822 -0.1622693   0.53280662], action=0, reward=1.0, next_state=[-0.01020164 -1.10928071 -0.15161317  0.77028377]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 45 ] state=[-0.01020164 -1.10928071 -0.15161317  0.77028377], action=1, reward=1.0, next_state=[-0.03238725 -0.91243356 -0.13620749  0.43399647]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 46 ] state=[-0.03238725 -0.91243356 -0.13620749  0.43399647], action=0, reward=1.0, next_state=[-0.05063593 -1.10539079 -0.12752756  0.6808303 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 47 ] state=[-0.05063593 -1.10539079 -0.12752756  0.6808303 ], action=0, reward=1.0, next_state=[-0.07274374 -1.2985325  -0.11391096  0.93079865]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 48 ] state=[-0.07274374 -1.2985325  -0.11391096  0.93079865], action=1, reward=1.0, next_state=[-0.09871439 -1.10207279 -0.09529498  0.60460148]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 49 ] state=[-0.09871439 -1.10207279 -0.09529498  0.60460148], action=0, reward=1.0, next_state=[-0.12075585 -1.29574197 -0.08320295  0.86581285]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 50 ] state=[-0.12075585 -1.29574197 -0.08320295  0.86581285], action=1, reward=1.0, next_state=[-0.14667069 -1.09959217 -0.0658867   0.54817254]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 51 ] state=[-0.14667069 -1.09959217 -0.0658867   0.54817254], action=1, reward=1.0, next_state=[-0.16866253 -0.9036095  -0.05492325  0.23547971]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 52 ] state=[-0.16866253 -0.9036095  -0.05492325  0.23547971], action=0, reward=1.0, next_state=[-0.18673472 -1.09790549 -0.05021365  0.51034489]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 53 ] state=[-0.18673472 -1.09790549 -0.05021365  0.51034489], action=0, reward=1.0, next_state=[-0.20869283 -1.29228543 -0.04000675  0.7867907 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 54 ] state=[-0.20869283 -1.29228543 -0.04000675  0.7867907 ], action=0, reward=1.0, next_state=[-0.23453854 -1.48683562 -0.02427094  1.06662363]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 55 ] state=[-0.23453854 -1.48683562 -0.02427094  1.06662363], action=1, reward=1.0, next_state=[-0.26427525 -1.29140109 -0.00293847  0.76642321]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 102 ][ timestamp 56 ] state=[-0.26427525 -1.29140109 -0.00293847  0.76642321], action=0, reward=1.0, next_state=[-0.29010327 -1.48648246  0.01239     1.05818009]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 57 ] state=[-0.29010327 -1.48648246  0.01239     1.05818009], action=1, reward=1.0, next_state=[-0.31983292 -1.29152683  0.0335536   0.76941167]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 58 ] state=[-0.31983292 -1.29152683  0.0335536   0.76941167], action=0, reward=1.0, next_state=[-0.34566346 -1.48709415  0.04894183  1.07246043]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 59 ] state=[-0.34566346 -1.48709415  0.04894183  1.07246043], action=0, reward=1.0, next_state=[-0.37540534 -1.6828277   0.07039104  1.38009236]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 60 ] state=[-0.37540534 -1.6828277   0.07039104  1.38009236], action=0, reward=1.0, next_state=[-0.4090619  -1.87875423  0.09799289  1.69393222]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 61 ] state=[-0.4090619  -1.87875423  0.09799289  1.69393222], action=1, reward=1.0, next_state=[-0.44663698 -1.68489055  0.13187153  1.43329561]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 62 ] state=[-0.44663698 -1.68489055  0.13187153  1.43329561], action=1, reward=1.0, next_state=[-0.48033479 -1.49161895  0.16053744  1.18456328]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 63 ] state=[-0.48033479 -1.49161895  0.16053744  1.18456328], action=1, reward=1.0, next_state=[-0.51016717 -1.29890147  0.18422871  0.94619968]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 64 ] state=[-0.51016717 -1.29890147  0.18422871  0.94619968], action=1, reward=1.0, next_state=[-0.5361452  -1.10667425  0.2031527   0.71659557]\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 65 ] state=[-0.5361452  -1.10667425  0.2031527   0.71659557], action=1, reward=-1.0, next_state=[-0.55827868 -0.91485656  0.21748461  0.49410293]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 102: Exploration_rate=0.01. Score=65.\n",
      "[ episode 103 ] state=[0.00402546 0.03698163 0.00632199 0.03507671]\n",
      "[ episode 103 ][ timestamp 1 ] state=[0.00402546 0.03698163 0.00632199 0.03507671], action=0, reward=1.0, next_state=[ 0.0047651  -0.15823041  0.00702352  0.32974757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 2 ] state=[ 0.0047651  -0.15823041  0.00702352  0.32974757], action=1, reward=1.0, next_state=[0.00160049 0.03679086 0.01361848 0.03928778]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 3 ] state=[0.00160049 0.03679086 0.01361848 0.03928778], action=0, reward=1.0, next_state=[ 0.00233631 -0.1585237   0.01440423  0.33623617]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 4 ] state=[ 0.00233631 -0.1585237   0.01440423  0.33623617], action=0, reward=1.0, next_state=[-0.00083417 -0.35384765  0.02112895  0.63342639]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 5 ] state=[-0.00083417 -0.35384765  0.02112895  0.63342639], action=1, reward=1.0, next_state=[-0.00791112 -0.15902672  0.03379748  0.34747168]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 6 ] state=[-0.00791112 -0.15902672  0.03379748  0.34747168], action=1, reward=1.0, next_state=[-0.01109165  0.03559861  0.04074692  0.06563496]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 7 ] state=[-0.01109165  0.03559861  0.04074692  0.06563496], action=1, reward=1.0, next_state=[-0.01037968  0.23011339  0.04205961 -0.21391875]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 8 ] state=[-0.01037968  0.23011339  0.04205961 -0.21391875], action=0, reward=1.0, next_state=[-0.00577741  0.03441615  0.03778124  0.09172939]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 9 ] state=[-0.00577741  0.03441615  0.03778124  0.09172939], action=0, reward=1.0, next_state=[-0.00508909 -0.16122639  0.03961583  0.39608882]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 10 ] state=[-0.00508909 -0.16122639  0.03961583  0.39608882], action=1, reward=1.0, next_state=[-0.00831362  0.03331174  0.0475376   0.11615459]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 11 ] state=[-0.00831362  0.03331174  0.0475376   0.11615459], action=0, reward=1.0, next_state=[-0.00764738 -0.16245797  0.0498607   0.4234482 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 12 ] state=[-0.00764738 -0.16245797  0.0498607   0.4234482 ], action=1, reward=1.0, next_state=[-0.01089654  0.03192348  0.05832966  0.14689136]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 13 ] state=[-0.01089654  0.03192348  0.05832966  0.14689136], action=0, reward=1.0, next_state=[-0.01025807 -0.16398316  0.06126749  0.45739076]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 14 ] state=[-0.01025807 -0.16398316  0.06126749  0.45739076], action=1, reward=1.0, next_state=[-0.01353774  0.03022154  0.0704153   0.18463165]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 15 ] state=[-0.01353774  0.03022154  0.0704153   0.18463165], action=1, reward=1.0, next_state=[-0.01293331  0.22426902  0.07410794 -0.08503327]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 16 ] state=[-0.01293331  0.22426902  0.07410794 -0.08503327], action=1, reward=1.0, next_state=[-0.00844793  0.41825476  0.07240727 -0.35344541]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 17 ] state=[-0.00844793  0.41825476  0.07240727 -0.35344541], action=1, reward=1.0, next_state=[-8.28312465e-05  6.12276424e-01  6.53383614e-02 -6.22446192e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 18 ] state=[-8.28312465e-05  6.12276424e-01  6.53383614e-02 -6.22446192e-01], action=0, reward=1.0, next_state=[ 0.0121627   0.4163059   0.05288944 -0.30992183]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 19 ] state=[ 0.0121627   0.4163059   0.05288944 -0.30992183], action=0, reward=1.0, next_state=[ 0.02048882  0.22047185  0.046691   -0.00103927]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 20 ] state=[ 0.02048882  0.22047185  0.046691   -0.00103927], action=1, reward=1.0, next_state=[ 0.02489825  0.41489416  0.04667022 -0.27863274]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 21 ] state=[ 0.02489825  0.41489416  0.04667022 -0.27863274], action=1, reward=1.0, next_state=[ 0.03319614  0.60932034  0.04109756 -0.55623839]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 22 ] state=[ 0.03319614  0.60932034  0.04109756 -0.55623839], action=0, reward=1.0, next_state=[ 0.04538254  0.41364622  0.02997279 -0.25089577]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 23 ] state=[ 0.04538254  0.41364622  0.02997279 -0.25089577], action=0, reward=1.0, next_state=[0.05365547 0.21810936 0.02495488 0.05108846]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 24 ] state=[0.05365547 0.21810936 0.02495488 0.05108846], action=0, reward=1.0, next_state=[0.05801765 0.02263864 0.02597665 0.35153923]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 25 ] state=[0.05801765 0.02263864 0.02597665 0.35153923], action=0, reward=1.0, next_state=[ 0.05847043 -0.1728429   0.03300743  0.65229889]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 26 ] state=[ 0.05847043 -0.1728429   0.03300743  0.65229889], action=1, reward=1.0, next_state=[0.05501357 0.0218042  0.04605341 0.37018969]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 27 ] state=[0.05501357 0.0218042  0.04605341 0.37018969], action=1, reward=1.0, next_state=[0.05544965 0.21624262 0.0534572  0.09237622]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 28 ] state=[0.05544965 0.21624262 0.0534572  0.09237622], action=0, reward=1.0, next_state=[0.05977451 0.02039683 0.05530473 0.40143419]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 29 ] state=[0.05977451 0.02039683 0.05530473 0.40143419], action=0, reward=1.0, next_state=[ 0.06018244 -0.17546418  0.06333341  0.71102782]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 30 ] state=[ 0.06018244 -0.17546418  0.06333341  0.71102782], action=0, reward=1.0, next_state=[ 0.05667316 -0.37140335  0.07755397  1.0229549 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 31 ] state=[ 0.05667316 -0.37140335  0.07755397  1.0229549 ], action=1, reward=1.0, next_state=[ 0.04924509 -0.17739529  0.09801307  0.75559556]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 32 ] state=[ 0.04924509 -0.17739529  0.09801307  0.75559556], action=1, reward=1.0, next_state=[0.04569719 0.01624882 0.11312498 0.4952932 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 33 ] state=[0.04569719 0.01624882 0.11312498 0.4952932 ], action=0, reward=1.0, next_state=[ 0.04602216 -0.18027146  0.12303084  0.82137727]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 34 ] state=[ 0.04602216 -0.18027146  0.12303084  0.82137727], action=0, reward=1.0, next_state=[ 0.04241673 -0.3768427   0.13945839  1.15008528]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 35 ] state=[ 0.04241673 -0.3768427   0.13945839  1.15008528], action=1, reward=1.0, next_state=[ 0.03487988 -0.18378853  0.16246009  0.90418344]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 103 ][ timestamp 36 ] state=[ 0.03487988 -0.18378853  0.16246009  0.90418344], action=1, reward=1.0, next_state=[0.03120411 0.00880461 0.18054376 0.66665115]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 37 ] state=[0.03120411 0.00880461 0.18054376 0.66665115], action=0, reward=1.0, next_state=[ 0.0313802  -0.1883078   0.19387678  1.010306  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 38 ] state=[ 0.0313802  -0.1883078   0.19387678  1.010306  ], action=1, reward=-1.0, next_state=[0.02761404 0.00377298 0.2140829  0.78422623]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 103: Exploration_rate=0.01. Score=38.\n",
      "[ episode 104 ] state=[ 0.04166393  0.01334501 -0.01966819  0.0349914 ]\n",
      "[ episode 104 ][ timestamp 1 ] state=[ 0.04166393  0.01334501 -0.01966819  0.0349914 ], action=1, reward=1.0, next_state=[ 0.04193083  0.2087434  -0.01896836 -0.26383158]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 2 ] state=[ 0.04193083  0.2087434  -0.01896836 -0.26383158], action=1, reward=1.0, next_state=[ 0.0461057   0.40413089 -0.02424499 -0.56243645]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 3 ] state=[ 0.0461057   0.40413089 -0.02424499 -0.56243645], action=0, reward=1.0, next_state=[ 0.05418832  0.20935741 -0.03549372 -0.27748942]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 4 ] state=[ 0.05418832  0.20935741 -0.03549372 -0.27748942], action=1, reward=1.0, next_state=[ 0.05837547  0.40496727 -0.04104351 -0.58115238]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 5 ] state=[ 0.05837547  0.40496727 -0.04104351 -0.58115238], action=0, reward=1.0, next_state=[ 0.06647481  0.21044373 -0.05266655 -0.3016762 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 6 ] state=[ 0.06647481  0.21044373 -0.05266655 -0.3016762 ], action=0, reward=1.0, next_state=[ 0.07068369  0.01611043 -0.05870008 -0.02605724]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 7 ] state=[ 0.07068369  0.01611043 -0.05870008 -0.02605724], action=0, reward=1.0, next_state=[ 0.0710059  -0.17812274 -0.05922122  0.24754279]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 8 ] state=[ 0.0710059  -0.17812274 -0.05922122  0.24754279], action=0, reward=1.0, next_state=[ 0.06744344 -0.37235111 -0.05427037  0.52097373]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 9 ] state=[ 0.06744344 -0.37235111 -0.05427037  0.52097373], action=1, reward=1.0, next_state=[ 0.05999642 -0.17650889 -0.04385089  0.21169526]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 10 ] state=[ 0.05999642 -0.17650889 -0.04385089  0.21169526], action=0, reward=1.0, next_state=[ 0.05646624 -0.37097734 -0.03961699  0.49022948]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 11 ] state=[ 0.05646624 -0.37097734 -0.03961699  0.49022948], action=1, reward=1.0, next_state=[ 0.04904669 -0.17531956 -0.0298124   0.18532875]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 12 ] state=[ 0.04904669 -0.17531956 -0.0298124   0.18532875], action=1, reward=1.0, next_state=[ 0.0455403   0.02021598 -0.02610582 -0.11660778]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 13 ] state=[ 0.0455403   0.02021598 -0.02610582 -0.11660778], action=1, reward=1.0, next_state=[ 0.04594462  0.21570207 -0.02843798 -0.41741123]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 14 ] state=[ 0.04594462  0.21570207 -0.02843798 -0.41741123], action=0, reward=1.0, next_state=[ 0.05025866  0.02099442 -0.0367862  -0.13382749]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 15 ] state=[ 0.05025866  0.02099442 -0.0367862  -0.13382749], action=0, reward=1.0, next_state=[ 0.05067855 -0.17358182 -0.03946275  0.14702671]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 16 ] state=[ 0.05067855 -0.17358182 -0.03946275  0.14702671], action=1, reward=1.0, next_state=[ 0.04720692  0.02208238 -0.03652222 -0.15784012]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 17 ] state=[ 0.04720692  0.02208238 -0.03652222 -0.15784012], action=1, reward=1.0, next_state=[ 0.04764856  0.21770768 -0.03967902 -0.46181753]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 18 ] state=[ 0.04764856  0.21770768 -0.03967902 -0.46181753], action=0, reward=1.0, next_state=[ 0.05200272  0.02316833 -0.04891537 -0.18190077]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 19 ] state=[ 0.05200272  0.02316833 -0.04891537 -0.18190077], action=0, reward=1.0, next_state=[ 0.05246608 -0.1712208  -0.05255339  0.09495877]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 20 ] state=[ 0.05246608 -0.1712208  -0.05255339  0.09495877], action=1, reward=1.0, next_state=[ 0.04904167  0.02461346 -0.05065421 -0.21383064]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 21 ] state=[ 0.04904167  0.02461346 -0.05065421 -0.21383064], action=1, reward=1.0, next_state=[ 0.04953394  0.22042163 -0.05493082 -0.52205215]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 22 ] state=[ 0.04953394  0.22042163 -0.05493082 -0.52205215], action=1, reward=1.0, next_state=[ 0.05394237  0.41627199 -0.06537187 -0.83152612]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 23 ] state=[ 0.05394237  0.41627199 -0.06537187 -0.83152612], action=1, reward=1.0, next_state=[ 0.06226781  0.61222355 -0.08200239 -1.14403128]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 24 ] state=[ 0.06226781  0.61222355 -0.08200239 -1.14403128], action=1, reward=1.0, next_state=[ 0.07451228  0.80831542 -0.10488301 -1.46126237]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 25 ] state=[ 0.07451228  0.80831542 -0.10488301 -1.46126237], action=1, reward=1.0, next_state=[ 0.09067859  1.00455507 -0.13410826 -1.78478339]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 26 ] state=[ 0.09067859  1.00455507 -0.13410826 -1.78478339], action=0, reward=1.0, next_state=[ 0.11076969  0.81117089 -0.16980393 -1.53662148]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 27 ] state=[ 0.11076969  0.81117089 -0.16980393 -1.53662148], action=0, reward=1.0, next_state=[ 0.12699311  0.61845104 -0.20053636 -1.30138208]\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 28 ] state=[ 0.12699311  0.61845104 -0.20053636 -1.30138208], action=0, reward=-1.0, next_state=[ 0.13936213  0.42635717 -0.226564   -1.07757897]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 104: Exploration_rate=0.01. Score=28.\n",
      "[ episode 105 ] state=[-0.04683196 -0.00377198  0.04594766 -0.04679963]\n",
      "[ episode 105 ][ timestamp 1 ] state=[-0.04683196 -0.00377198  0.04594766 -0.04679963], action=1, reward=1.0, next_state=[-0.0469074   0.19066202  0.04501167 -0.32463896]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 2 ] state=[-0.0469074   0.19066202  0.04501167 -0.32463896], action=1, reward=1.0, next_state=[-0.04309416  0.38511513  0.03851889 -0.60279423]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 3 ] state=[-0.04309416  0.38511513  0.03851889 -0.60279423], action=0, reward=1.0, next_state=[-0.03539186  0.18947619  0.02646301 -0.29823174]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 4 ] state=[-0.03539186  0.18947619  0.02646301 -0.29823174], action=0, reward=1.0, next_state=[-0.03160233 -0.00601279  0.02049837  0.00267828]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 5 ] state=[-0.03160233 -0.00601279  0.02049837  0.00267828], action=1, reward=1.0, next_state=[-0.03172259  0.18880928  0.02055194 -0.28346733]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 6 ] state=[-0.03172259  0.18880928  0.02055194 -0.28346733], action=1, reward=1.0, next_state=[-0.0279464   0.38363216  0.01488259 -0.5695981 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 7 ] state=[-0.0279464   0.38363216  0.01488259 -0.5695981 ], action=0, reward=1.0, next_state=[-0.02027376  0.18830468  0.00349063 -0.272264  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 8 ] state=[-0.02027376  0.18830468  0.00349063 -0.272264  ], action=0, reward=1.0, next_state=[-0.01650767 -0.00686691 -0.00195465  0.02151784]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 9 ] state=[-0.01650767 -0.00686691 -0.00195465  0.02151784], action=1, reward=1.0, next_state=[-0.016645    0.18828302 -0.0015243  -0.27178116]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 105 ][ timestamp 10 ] state=[-0.016645    0.18828302 -0.0015243  -0.27178116], action=0, reward=1.0, next_state=[-0.01287934 -0.00681715 -0.00695992  0.0204206 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 11 ] state=[-0.01287934 -0.00681715 -0.00695992  0.0204206 ], action=0, reward=1.0, next_state=[-0.01301569 -0.2018386  -0.00655151  0.31089949]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 12 ] state=[-0.01301569 -0.2018386  -0.00655151  0.31089949], action=0, reward=1.0, next_state=[-1.70524588e-02 -3.96866600e-01 -3.33517542e-04  6.01509088e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 13 ] state=[-1.70524588e-02 -3.96866600e-01 -3.33517542e-04  6.01509088e-01], action=1, reward=1.0, next_state=[-0.02498979 -0.20173998  0.01169666  0.30872113]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 14 ] state=[-0.02498979 -0.20173998  0.01169666  0.30872113], action=0, reward=1.0, next_state=[-0.02902459 -0.39702663  0.01787109  0.60506979]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 15 ] state=[-0.02902459 -0.39702663  0.01787109  0.60506979], action=1, reward=1.0, next_state=[-0.03696512 -0.20215909  0.02997248  0.31806898]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 16 ] state=[-0.03696512 -0.20215909  0.02997248  0.31806898], action=1, reward=1.0, next_state=[-0.0410083  -0.00747656  0.03633386  0.03498695]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 17 ] state=[-0.0410083  -0.00747656  0.03633386  0.03498695], action=1, reward=1.0, next_state=[-0.04115784  0.18710602  0.0370336  -0.24601449]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 18 ] state=[-0.04115784  0.18710602  0.0370336  -0.24601449], action=1, reward=1.0, next_state=[-0.03741572  0.38168001  0.03211331 -0.52678996]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 19 ] state=[-0.03741572  0.38168001  0.03211331 -0.52678996], action=1, reward=1.0, next_state=[-0.02978212  0.57633574  0.02157751 -0.80918332]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 20 ] state=[-0.02978212  0.57633574  0.02157751 -0.80918332], action=0, reward=1.0, next_state=[-0.0182554   0.38092487  0.00539385 -0.50979195]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 21 ] state=[-0.0182554   0.38092487  0.00539385 -0.50979195], action=0, reward=1.0, next_state=[-0.0106369   0.18572734 -0.00480199 -0.21541414]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 22 ] state=[-0.0106369   0.18572734 -0.00480199 -0.21541414], action=0, reward=1.0, next_state=[-0.00692236 -0.00932563 -0.00911028  0.07575016]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 23 ] state=[-0.00692236 -0.00932563 -0.00911028  0.07575016], action=1, reward=1.0, next_state=[-0.00710887  0.18592573 -0.00759527 -0.21979311]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 24 ] state=[-0.00710887  0.18592573 -0.00759527 -0.21979311], action=1, reward=1.0, next_state=[-0.00339035  0.38115543 -0.01199114 -0.51486219]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 25 ] state=[-0.00339035  0.38115543 -0.01199114 -0.51486219], action=1, reward=1.0, next_state=[ 0.00423275  0.57644418 -0.02228838 -0.81129957]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 26 ] state=[ 0.00423275  0.57644418 -0.02228838 -0.81129957], action=1, reward=1.0, next_state=[ 0.01576164  0.77186426 -0.03851437 -1.11090912]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 27 ] state=[ 0.01576164  0.77186426 -0.03851437 -1.11090912], action=1, reward=1.0, next_state=[ 0.03119892  0.96747043 -0.06073255 -1.41542122]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 28 ] state=[ 0.03119892  0.96747043 -0.06073255 -1.41542122], action=0, reward=1.0, next_state=[ 0.05054833  0.77315108 -0.08904098 -1.14232398]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 29 ] state=[ 0.05054833  0.77315108 -0.08904098 -1.14232398], action=0, reward=1.0, next_state=[ 0.06601135  0.57929832 -0.11188746 -0.87884024]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 30 ] state=[ 0.06601135  0.57929832 -0.11188746 -0.87884024], action=0, reward=1.0, next_state=[ 0.07759732  0.38585996 -0.12946426 -0.62332334]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 31 ] state=[ 0.07759732  0.38585996 -0.12946426 -0.62332334], action=1, reward=1.0, next_state=[ 0.08531452  0.58252915 -0.14193073 -0.95381456]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 32 ] state=[ 0.08531452  0.58252915 -0.14193073 -0.95381456], action=0, reward=1.0, next_state=[ 0.0969651   0.38957243 -0.16100702 -0.7088775 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 33 ] state=[ 0.0969651   0.38957243 -0.16100702 -0.7088775 ], action=1, reward=1.0, next_state=[ 0.10475655  0.58651482 -0.17518457 -1.04760211]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 34 ] state=[ 0.10475655  0.58651482 -0.17518457 -1.04760211], action=1, reward=1.0, next_state=[ 0.11648685  0.78347369 -0.19613661 -1.38975978]\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 35 ] state=[ 0.11648685  0.78347369 -0.19613661 -1.38975978], action=1, reward=-1.0, next_state=[ 0.13215632  0.98042241 -0.22393181 -1.73681381]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 105: Exploration_rate=0.01. Score=35.\n",
      "[ episode 106 ] state=[ 0.00723614 -0.00069807 -0.00064443  0.02730625]\n",
      "[ episode 106 ][ timestamp 1 ] state=[ 0.00723614 -0.00069807 -0.00064443  0.02730625], action=1, reward=1.0, next_state=[ 7.22218017e-03  1.94433115e-01 -9.83058179e-05 -2.65579928e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 2 ] state=[ 7.22218017e-03  1.94433115e-01 -9.83058179e-05 -2.65579928e-01], action=1, reward=1.0, next_state=[ 0.01111084  0.38955647 -0.0054099  -0.55829386]\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 3 ] state=[ 0.01111084  0.38955647 -0.0054099  -0.55829386], action=1, reward=1.0, next_state=[ 0.01890197  0.58475394 -0.01657578 -0.85267629]\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 4 ] state=[ 0.01890197  0.58475394 -0.01657578 -0.85267629], action=1, reward=1.0, next_state=[ 0.03059705  0.78009789 -0.03362931 -1.15052501]\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 5 ] state=[ 0.03059705  0.78009789 -0.03362931 -1.15052501], action=0, reward=1.0, next_state=[ 0.04619901  0.58543055 -0.05663981 -0.86857425]\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 6 ] state=[ 0.04619901  0.58543055 -0.05663981 -0.86857425], action=1, reward=1.0, next_state=[ 0.05790762  0.78127547 -0.07401129 -1.17851375]\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 7 ] state=[ 0.05790762  0.78127547 -0.07401129 -1.17851375], action=1, reward=1.0, next_state=[ 0.07353313  0.97727637 -0.09758157 -1.49344971]\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 8 ] state=[ 0.07353313  0.97727637 -0.09758157 -1.49344971], action=1, reward=1.0, next_state=[ 0.09307866  1.17344067 -0.12745056 -1.81493982]\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 9 ] state=[ 0.09307866  1.17344067 -0.12745056 -1.81493982], action=1, reward=1.0, next_state=[ 0.11654747  1.36973021 -0.16374936 -2.14435511]\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 10 ] state=[ 0.11654747  1.36973021 -0.16374936 -2.14435511], action=0, reward=1.0, next_state=[ 0.14394207  1.17655921 -0.20663646 -1.90640214]\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 11 ] state=[ 0.14394207  1.17655921 -0.20663646 -1.90640214], action=0, reward=-1.0, next_state=[ 0.16747326  0.98418215 -0.2447645  -1.68429505]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 106: Exploration_rate=0.01. Score=11.\n",
      "[ episode 107 ] state=[ 0.02620085 -0.03297713 -0.03391855 -0.00128624]\n",
      "[ episode 107 ][ timestamp 1 ] state=[ 0.02620085 -0.03297713 -0.03391855 -0.00128624], action=0, reward=1.0, next_state=[ 0.02554131 -0.22759664 -0.03394427  0.28050496]\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 2 ] state=[ 0.02554131 -0.22759664 -0.03394427  0.28050496], action=0, reward=1.0, next_state=[ 0.02098938 -0.42221836 -0.02833417  0.56229167]\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 3 ] state=[ 0.02098938 -0.42221836 -0.02833417  0.56229167], action=0, reward=1.0, next_state=[ 0.01254501 -0.61693147 -0.01708834  0.84591498]\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 4 ] state=[ 0.01254501 -0.61693147 -0.01708834  0.84591498], action=0, reward=1.0, next_state=[ 2.06380685e-04 -8.11816167e-01 -1.70039775e-04  1.13317562e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 5 ] state=[ 2.06380685e-04 -8.11816167e-01 -1.70039775e-04  1.13317562e+00], action=1, reward=1.0, next_state=[-0.01602994 -0.61669199  0.02249347  0.84043936]\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 6 ] state=[-0.01602994 -0.61669199  0.02249347  0.84043936], action=1, reward=1.0, next_state=[-0.02836378 -0.42188423  0.03930226  0.55491417]\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 7 ] state=[-0.02836378 -0.42188423  0.03930226  0.55491417], action=0, reward=1.0, next_state=[-0.03680147 -0.61753535  0.05040054  0.8597161 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 8 ] state=[-0.03680147 -0.61753535  0.05040054  0.8597161 ], action=0, reward=1.0, next_state=[-0.04915217 -0.81330621  0.06759487  1.16781098]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 107 ][ timestamp 9 ] state=[-0.04915217 -0.81330621  0.06759487  1.16781098], action=0, reward=1.0, next_state=[-0.0654183  -1.00923935  0.09095108  1.48089728]\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 10 ] state=[-0.0654183  -1.00923935  0.09095108  1.48089728], action=0, reward=1.0, next_state=[-0.08560309 -1.20534579  0.12056903  1.80054389]\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 11 ] state=[-0.08560309 -1.20534579  0.12056903  1.80054389], action=0, reward=1.0, next_state=[-0.10971    -1.40159201  0.15657991  2.12813769]\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 12 ] state=[-0.10971    -1.40159201  0.15657991  2.12813769], action=0, reward=1.0, next_state=[-0.13774184 -1.59788473  0.19914266  2.46482132]\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 13 ] state=[-0.13774184 -1.59788473  0.19914266  2.46482132], action=0, reward=-1.0, next_state=[-0.16969954 -1.79405344  0.24843909  2.81142067]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 107: Exploration_rate=0.01. Score=13.\n",
      "[ episode 108 ] state=[-0.00260107 -0.0261397  -0.01105928 -0.03555457]\n",
      "[ episode 108 ][ timestamp 1 ] state=[-0.00260107 -0.0261397  -0.01105928 -0.03555457], action=0, reward=1.0, next_state=[-0.00312386 -0.22110133 -0.01177038  0.25361862]\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 2 ] state=[-0.00312386 -0.22110133 -0.01177038  0.25361862], action=0, reward=1.0, next_state=[-0.00754589 -0.41605325 -0.006698    0.54256584]\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 3 ] state=[-0.00754589 -0.41605325 -0.006698    0.54256584], action=0, reward=1.0, next_state=[-0.01586695 -0.61108043  0.00415331  0.83313085]\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 4 ] state=[-0.01586695 -0.61108043  0.00415331  0.83313085], action=1, reward=1.0, next_state=[-0.02808856 -0.41601548  0.02081593  0.54175701]\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 5 ] state=[-0.02808856 -0.41601548  0.02081593  0.54175701], action=0, reward=1.0, next_state=[-0.03640887 -0.61142372  0.03165107  0.84092532]\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 6 ] state=[-0.03640887 -0.61142372  0.03165107  0.84092532], action=0, reward=1.0, next_state=[-0.04863735 -0.80696313  0.04846958  1.14339139]\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 7 ] state=[-0.04863735 -0.80696313  0.04846958  1.14339139], action=1, reward=1.0, next_state=[-0.06477661 -0.61250682  0.0713374   0.86629396]\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 8 ] state=[-0.06477661 -0.61250682  0.0713374   0.86629396], action=0, reward=1.0, next_state=[-0.07702675 -0.80852333  0.08866328  1.18052631]\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 9 ] state=[-0.07702675 -0.80852333  0.08866328  1.18052631], action=0, reward=1.0, next_state=[-0.09319721 -1.00467726  0.11227381  1.49963433]\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 10 ] state=[-0.09319721 -1.00467726  0.11227381  1.49963433], action=1, reward=1.0, next_state=[-0.11329076 -0.81108398  0.1422665   1.24401192]\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 11 ] state=[-0.11329076 -0.81108398  0.1422665   1.24401192], action=1, reward=1.0, next_state=[-0.12951244 -0.61804481  0.16714674  0.99906392]\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 12 ] state=[-0.12951244 -0.61804481  0.16714674  0.99906392], action=1, reward=1.0, next_state=[-0.14187333 -0.42550353  0.18712801  0.76318968]\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 13 ] state=[-0.14187333 -0.42550353  0.18712801  0.76318968], action=1, reward=1.0, next_state=[-0.1503834  -0.23338387  0.20239181  0.53473618]\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 14 ] state=[-0.1503834  -0.23338387  0.20239181  0.53473618], action=1, reward=-1.0, next_state=[-0.15505108 -0.04159685  0.21308653  0.3120254 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 108: Exploration_rate=0.01. Score=14.\n",
      "[ episode 109 ] state=[0.04280532 0.02265422 0.00315982 0.03869043]\n",
      "[ episode 109 ][ timestamp 1 ] state=[0.04280532 0.02265422 0.00315982 0.03869043], action=0, reward=1.0, next_state=[ 0.0432584  -0.1725129   0.00393363  0.33236863]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 2 ] state=[ 0.0432584  -0.1725129   0.00393363  0.33236863], action=1, reward=1.0, next_state=[0.03980814 0.02255284 0.01058101 0.04092877]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 3 ] state=[0.03980814 0.02255284 0.01058101 0.04092877], action=1, reward=1.0, next_state=[ 0.0402592   0.21752147  0.01139958 -0.24839706]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 4 ] state=[ 0.0402592   0.21752147  0.01139958 -0.24839706], action=1, reward=1.0, next_state=[ 0.04460963  0.41247879  0.00643164 -0.53746262]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 5 ] state=[ 0.04460963  0.41247879  0.00643164 -0.53746262], action=0, reward=1.0, next_state=[ 0.0528592   0.217267   -0.00431761 -0.24276011]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 6 ] state=[ 0.0528592   0.217267   -0.00431761 -0.24276011], action=0, reward=1.0, next_state=[ 0.05720454  0.02220699 -0.00917281  0.04855781]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 7 ] state=[ 0.05720454  0.02220699 -0.00917281  0.04855781], action=1, reward=1.0, next_state=[ 0.05764868  0.21745926 -0.00820166 -0.24700505]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 8 ] state=[ 0.05764868  0.21745926 -0.00820166 -0.24700505], action=0, reward=1.0, next_state=[ 0.06199787  0.0224554  -0.01314176  0.04307965]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 9 ] state=[ 0.06199787  0.0224554  -0.01314176  0.04307965], action=0, reward=1.0, next_state=[ 0.06244698 -0.17247566 -0.01228017  0.33158742]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 10 ] state=[ 0.06244698 -0.17247566 -0.01228017  0.33158742], action=1, reward=1.0, next_state=[ 0.05899746  0.02281891 -0.00564842  0.03505737]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 11 ] state=[ 0.05899746  0.02281891 -0.00564842  0.03505737], action=1, reward=1.0, next_state=[ 0.05945384  0.21802141 -0.00494727 -0.25940233]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 12 ] state=[ 0.05945384  0.21802141 -0.00494727 -0.25940233], action=0, reward=1.0, next_state=[ 0.06381427  0.02297043 -0.01013532  0.03171606]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 13 ] state=[ 0.06381427  0.02297043 -0.01013532  0.03171606], action=0, reward=1.0, next_state=[ 0.06427368 -0.17200472 -0.009501    0.32118404]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 14 ] state=[ 0.06427368 -0.17200472 -0.009501    0.32118404], action=1, reward=1.0, next_state=[ 0.06083358  0.02325124 -0.00307732  0.02552007]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 15 ] state=[ 0.06083358  0.02325124 -0.00307732  0.02552007], action=1, reward=1.0, next_state=[ 0.06129861  0.21841718 -0.00256691 -0.26813219]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 16 ] state=[ 0.06129861  0.21841718 -0.00256691 -0.26813219], action=0, reward=1.0, next_state=[ 0.06566695  0.02333196 -0.00792956  0.02374001]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 17 ] state=[ 0.06566695  0.02333196 -0.00792956  0.02374001], action=1, reward=1.0, next_state=[ 0.06613359  0.21856672 -0.00745476 -0.2714342 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 18 ] state=[ 0.06613359  0.21856672 -0.00745476 -0.2714342 ], action=0, reward=1.0, next_state=[ 0.07050493  0.02355194 -0.01288344  0.01888817]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 19 ] state=[ 0.07050493  0.02355194 -0.01288344  0.01888817], action=0, reward=1.0, next_state=[ 0.07097597 -0.1713829  -0.01250568  0.30747854]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 20 ] state=[ 0.07097597 -0.1713829  -0.01250568  0.30747854], action=0, reward=1.0, next_state=[ 0.06754831 -0.36632445 -0.00635611  0.59619142]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 21 ] state=[ 0.06754831 -0.36632445 -0.00635611  0.59619142], action=1, reward=1.0, next_state=[ 0.06022182 -0.17111412  0.00556772  0.30151316]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 22 ] state=[ 0.06022182 -0.17111412  0.00556772  0.30151316], action=1, reward=1.0, next_state=[0.05679954 0.02392803 0.01159798 0.01059137]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 23 ] state=[0.05679954 0.02392803 0.01159798 0.01059137], action=0, reward=1.0, next_state=[ 0.0572781  -0.17135831  0.01180981  0.30691092]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 24 ] state=[ 0.0572781  -0.17135831  0.01180981  0.30691092], action=1, reward=1.0, next_state=[0.05385093 0.02359338 0.01794803 0.01797577]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 25 ] state=[0.05385093 0.02359338 0.01794803 0.01797577], action=1, reward=1.0, next_state=[ 0.0543228   0.2184534   0.01830755 -0.26899074]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 26 ] state=[ 0.0543228   0.2184534   0.01830755 -0.26899074], action=0, reward=1.0, next_state=[0.05869187 0.02307503 0.01292773 0.02940981]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 27 ] state=[0.05869187 0.02307503 0.01292773 0.02940981], action=1, reward=1.0, next_state=[ 0.05915337  0.21800923  0.01351593 -0.2591664 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 109 ][ timestamp 28 ] state=[ 0.05915337  0.21800923  0.01351593 -0.2591664 ], action=1, reward=1.0, next_state=[ 0.06351355  0.41293565  0.0083326  -0.54755576]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 29 ] state=[ 0.06351355  0.41293565  0.0083326  -0.54755576], action=0, reward=1.0, next_state=[ 0.07177226  0.21769763 -0.00261852 -0.25225914]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 30 ] state=[ 0.07177226  0.21769763 -0.00261852 -0.25225914], action=0, reward=1.0, next_state=[ 0.07612622  0.02261317 -0.0076637   0.03959671]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 31 ] state=[ 0.07612622  0.02261317 -0.0076637   0.03959671], action=1, reward=1.0, next_state=[ 0.07657848  0.21784417 -0.00687177 -0.25549431]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 32 ] state=[ 0.07657848  0.21784417 -0.00687177 -0.25549431], action=1, reward=1.0, next_state=[ 0.08093536  0.41306356 -0.01198165 -0.55033676]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 33 ] state=[ 0.08093536  0.41306356 -0.01198165 -0.55033676], action=1, reward=1.0, next_state=[ 0.08919663  0.60835173 -0.02298839 -0.84677052]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 34 ] state=[ 0.08919663  0.60835173 -0.02298839 -0.84677052], action=0, reward=1.0, next_state=[ 0.10136367  0.41355081 -0.0399238  -0.56140432]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 35 ] state=[ 0.10136367  0.41355081 -0.0399238  -0.56140432], action=0, reward=1.0, next_state=[ 0.10963469  0.21901122 -0.05115188 -0.28156194]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 36 ] state=[ 0.10963469  0.21901122 -0.05115188 -0.28156194], action=1, reward=1.0, next_state=[ 0.11401491  0.41482407 -0.05678312 -0.58992914]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 37 ] state=[ 0.11401491  0.41482407 -0.05678312 -0.58992914], action=0, reward=1.0, next_state=[ 0.12231139  0.22054124 -0.0685817  -0.31565986]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 38 ] state=[ 0.12231139  0.22054124 -0.0685817  -0.31565986], action=0, reward=1.0, next_state=[ 0.12672222  0.0264598  -0.0748949  -0.04536929]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 39 ] state=[ 0.12672222  0.0264598  -0.0748949  -0.04536929], action=0, reward=1.0, next_state=[ 0.12725141 -0.16751275 -0.07580229  0.22277536]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 40 ] state=[ 0.12725141 -0.16751275 -0.07580229  0.22277536], action=1, reward=1.0, next_state=[ 0.12390116  0.02860619 -0.07134678 -0.09282282]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 41 ] state=[ 0.12390116  0.02860619 -0.07134678 -0.09282282], action=0, reward=1.0, next_state=[ 0.12447328 -0.16542446 -0.07320324  0.17652455]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 42 ] state=[ 0.12447328 -0.16542446 -0.07320324  0.17652455], action=1, reward=1.0, next_state=[ 0.12116479  0.03066461 -0.06967275 -0.13832386]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 43 ] state=[ 0.12116479  0.03066461 -0.06967275 -0.13832386], action=1, reward=1.0, next_state=[ 0.12177808  0.22671172 -0.07243922 -0.45214828]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 44 ] state=[ 0.12177808  0.22671172 -0.07243922 -0.45214828], action=1, reward=1.0, next_state=[ 0.12631232  0.42277934 -0.08148219 -0.76675692]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 45 ] state=[ 0.12631232  0.42277934 -0.08148219 -0.76675692], action=0, reward=1.0, next_state=[ 0.1347679   0.22886811 -0.09681733 -0.50078438]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 46 ] state=[ 0.1347679   0.22886811 -0.09681733 -0.50078438], action=0, reward=1.0, next_state=[ 0.13934527  0.03523474 -0.10683301 -0.24011439]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 47 ] state=[ 0.13934527  0.03523474 -0.10683301 -0.24011439], action=0, reward=1.0, next_state=[ 0.14004996 -0.15821176 -0.1116353   0.01705184]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 48 ] state=[ 0.14004996 -0.15821176 -0.1116353   0.01705184], action=1, reward=1.0, next_state=[ 0.13688573  0.03831943 -0.11129427 -0.30866256]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 49 ] state=[ 0.13688573  0.03831943 -0.11129427 -0.30866256], action=0, reward=1.0, next_state=[ 0.13765212 -0.15505533 -0.11746752 -0.05304799]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 50 ] state=[ 0.13765212 -0.15505533 -0.11746752 -0.05304799], action=0, reward=1.0, next_state=[ 0.13455101 -0.34831417 -0.11852848  0.20038647]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 51 ] state=[ 0.13455101 -0.34831417 -0.11852848  0.20038647], action=0, reward=1.0, next_state=[ 0.12758473 -0.54155904 -0.11452075  0.45345414]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 52 ] state=[ 0.12758473 -0.54155904 -0.11452075  0.45345414], action=0, reward=1.0, next_state=[ 0.11675354 -0.73489112 -0.10545166  0.70795713]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 53 ] state=[ 0.11675354 -0.73489112 -0.10545166  0.70795713], action=0, reward=1.0, next_state=[ 0.10205572 -0.92840652 -0.09129252  0.96567244]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 54 ] state=[ 0.10205572 -0.92840652 -0.09129252  0.96567244], action=1, reward=1.0, next_state=[ 0.08348759 -0.73218484 -0.07197907  0.64576285]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 55 ] state=[ 0.08348759 -0.73218484 -0.07197907  0.64576285], action=1, reward=1.0, next_state=[ 0.0688439  -0.53613761 -0.05906382  0.3313099 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 56 ] state=[ 0.0688439  -0.53613761 -0.05906382  0.3313099 ], action=1, reward=1.0, next_state=[ 0.05812114 -0.34022684 -0.05243762  0.0206015 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 57 ] state=[ 0.05812114 -0.34022684 -0.05243762  0.0206015 ], action=0, reward=1.0, next_state=[ 0.05131661 -0.53455909 -0.05202559  0.29628961]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 58 ] state=[ 0.05131661 -0.53455909 -0.05202559  0.29628961], action=0, reward=1.0, next_state=[ 0.04062542 -0.72890227 -0.0460998   0.57212132]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 59 ] state=[ 0.04062542 -0.72890227 -0.0460998   0.57212132], action=1, reward=1.0, next_state=[ 0.02604738 -0.53316525 -0.03465737  0.26527918]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 60 ] state=[ 0.02604738 -0.53316525 -0.03465737  0.26527918], action=1, reward=1.0, next_state=[ 0.01538407 -0.33756622 -0.02935179 -0.0381304 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 61 ] state=[ 0.01538407 -0.33756622 -0.02935179 -0.0381304 ], action=1, reward=1.0, next_state=[ 0.00863275 -0.14203592 -0.03011439 -0.3399277 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 62 ] state=[ 0.00863275 -0.14203592 -0.03011439 -0.3399277 ], action=0, reward=1.0, next_state=[ 0.00579203 -0.33671673 -0.03691295 -0.05689118]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 63 ] state=[ 0.00579203 -0.33671673 -0.03691295 -0.05689118], action=1, reward=1.0, next_state=[-0.0009423  -0.14108549 -0.03805077 -0.36098809]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 64 ] state=[-0.0009423  -0.14108549 -0.03805077 -0.36098809], action=0, reward=1.0, next_state=[-0.00376401 -0.3356465  -0.04527053 -0.08054206]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 65 ] state=[-0.00376401 -0.3356465  -0.04527053 -0.08054206], action=0, reward=1.0, next_state=[-0.01047694 -0.53009124 -0.04688137  0.19752124]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 66 ] state=[-0.01047694 -0.53009124 -0.04688137  0.19752124], action=1, reward=1.0, next_state=[-0.02107877 -0.33433117 -0.04293095 -0.10957431]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 67 ] state=[-0.02107877 -0.33433117 -0.04293095 -0.10957431], action=0, reward=1.0, next_state=[-0.02776539 -0.52881247 -0.04512244  0.16926103]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 68 ] state=[-0.02776539 -0.52881247 -0.04512244  0.16926103], action=0, reward=1.0, next_state=[-0.03834164 -0.72326049 -0.04173722  0.44737469]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 69 ] state=[-0.03834164 -0.72326049 -0.04173722  0.44737469], action=1, reward=1.0, next_state=[-0.05280685 -0.52757374 -0.03278972  0.14183301]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 109 ][ timestamp 70 ] state=[-0.05280685 -0.52757374 -0.03278972  0.14183301], action=0, reward=1.0, next_state=[-0.06335833 -0.72221111 -0.02995306  0.42399368]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 71 ] state=[-0.06335833 -0.72221111 -0.02995306  0.42399368], action=0, reward=1.0, next_state=[-0.07780255 -0.91689622 -0.02147319  0.70708547]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 72 ] state=[-0.07780255 -0.91689622 -0.02147319  0.70708547], action=1, reward=1.0, next_state=[-0.09614047 -0.72148347 -0.00733148  0.4077213 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 73 ] state=[-0.09614047 -0.72148347 -0.00733148  0.4077213 ], action=0, reward=1.0, next_state=[-1.10570141e-01 -9.16500707e-01  8.22947335e-04  6.98083848e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 74 ] state=[-1.10570141e-01 -9.16500707e-01  8.22947335e-04  6.98083848e-01], action=0, reward=1.0, next_state=[-0.12890016 -1.11163406  0.01478462  0.99102572]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 75 ] state=[-0.12890016 -1.11163406  0.01478462  0.99102572], action=1, reward=1.0, next_state=[-0.15113284 -0.91671306  0.03460514  0.7030227 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 76 ] state=[-0.15113284 -0.91671306  0.03460514  0.7030227 ], action=1, reward=1.0, next_state=[-0.1694671  -0.72208737  0.04866559  0.42143082]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 77 ] state=[-0.1694671  -0.72208737  0.04866559  0.42143082], action=1, reward=1.0, next_state=[-0.18390885 -0.52768748  0.05709421  0.14447827]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 78 ] state=[-0.18390885 -0.52768748  0.05709421  0.14447827], action=1, reward=1.0, next_state=[-0.1944626  -0.33342769  0.05998377 -0.12966004]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 79 ] state=[-0.1944626  -0.33342769  0.05998377 -0.12966004], action=0, reward=1.0, next_state=[-0.20113115 -0.52935533  0.05739057  0.18132752]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 80 ] state=[-0.20113115 -0.52935533  0.05739057  0.18132752], action=0, reward=1.0, next_state=[-0.21171826 -0.72524954  0.06101712  0.49154863]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 81 ] state=[-0.21171826 -0.72524954  0.06101712  0.49154863], action=1, reward=1.0, next_state=[-0.22622325 -0.53103896  0.0708481   0.21870279]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 82 ] state=[-0.22622325 -0.53103896  0.0708481   0.21870279], action=0, reward=1.0, next_state=[-0.23684403 -0.72709839  0.07522215  0.53286608]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 83 ] state=[-0.23684403 -0.72709839  0.07522215  0.53286608], action=1, reward=1.0, next_state=[-0.25138599 -0.53311052  0.08587947  0.26480159]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 84 ] state=[-0.25138599 -0.53311052  0.08587947  0.26480159], action=0, reward=1.0, next_state=[-0.2620482  -0.72934655  0.09117551  0.58328837]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 85 ] state=[-0.2620482  -0.72934655  0.09117551  0.58328837], action=1, reward=1.0, next_state=[-0.27663513 -0.53561225  0.10284127  0.32066244]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 86 ] state=[-0.27663513 -0.53561225  0.10284127  0.32066244], action=1, reward=1.0, next_state=[-0.28734738 -0.34209375  0.10925452  0.06210044]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 87 ] state=[-0.28734738 -0.34209375  0.10925452  0.06210044], action=1, reward=1.0, next_state=[-0.29418925 -0.14869406  0.11049653 -0.19421245]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 88 ] state=[-0.29418925 -0.14869406  0.11049653 -0.19421245], action=1, reward=1.0, next_state=[-0.29716314  0.04468807  0.10661228 -0.45009672]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 89 ] state=[-0.29716314  0.04468807  0.10661228 -0.45009672], action=0, reward=1.0, next_state=[-0.29626937 -0.1517675   0.09761035 -0.12580182]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 90 ] state=[-0.29626937 -0.1517675   0.09761035 -0.12580182], action=1, reward=1.0, next_state=[-0.29930472  0.04183042  0.09509431 -0.38616448]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 91 ] state=[-0.29930472  0.04183042  0.09509431 -0.38616448], action=1, reward=1.0, next_state=[-0.29846812  0.23548285  0.08737102 -0.64741512]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 92 ] state=[-0.29846812  0.23548285  0.08737102 -0.64741512], action=0, reward=1.0, next_state=[-0.29375846  0.03925924  0.07442272 -0.32854802]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 93 ] state=[-0.29375846  0.03925924  0.07442272 -0.32854802], action=1, reward=1.0, next_state=[-0.29297327  0.23324716  0.06785176 -0.59686434]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 94 ] state=[-0.29297327  0.23324716  0.06785176 -0.59686434], action=0, reward=1.0, next_state=[-0.28830833  0.03724459  0.05591447 -0.28360389]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 95 ] state=[-0.28830833  0.03724459  0.05591447 -0.28360389], action=0, reward=1.0, next_state=[-0.28756344 -0.15862844  0.05024239  0.02617678]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 96 ] state=[-0.28756344 -0.15862844  0.05024239  0.02617678], action=0, reward=1.0, next_state=[-0.29073601 -0.35443356  0.05076593  0.33427889]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 97 ] state=[-0.29073601 -0.35443356  0.05076593  0.33427889], action=1, reward=1.0, next_state=[-0.29782468 -0.16006951  0.05745151  0.05802718]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 98 ] state=[-0.29782468 -0.16006951  0.05745151  0.05802718], action=1, reward=1.0, next_state=[-0.30102607  0.03418364  0.05861205 -0.21599035]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 99 ] state=[-0.30102607  0.03418364  0.05861205 -0.21599035], action=1, reward=1.0, next_state=[-0.3003424   0.22842083  0.05429224 -0.48962374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 100 ] state=[-0.3003424   0.22842083  0.05429224 -0.48962374], action=1, reward=1.0, next_state=[-0.29577398  0.4227365   0.04449977 -0.7647137 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 101 ] state=[-0.29577398  0.4227365   0.04449977 -0.7647137 ], action=0, reward=1.0, next_state=[-0.28731925  0.22703092  0.0292055  -0.45836732]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 102 ] state=[-0.28731925  0.22703092  0.0292055  -0.45836732], action=0, reward=1.0, next_state=[-0.28277863  0.03150854  0.02003815 -0.15662362]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 103 ] state=[-0.28277863  0.03150854  0.02003815 -0.15662362], action=1, reward=1.0, next_state=[-0.28214846  0.22633795  0.01690568 -0.44291824]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 104 ] state=[-0.28214846  0.22633795  0.01690568 -0.44291824], action=1, reward=1.0, next_state=[-0.2776217   0.42121666  0.00804731 -0.7302245 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 105 ] state=[-0.2776217   0.42121666  0.00804731 -0.7302245 ], action=0, reward=1.0, next_state=[-0.26919737  0.22598441 -0.00655718 -0.43501973]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 106 ] state=[-0.26919737  0.22598441 -0.00655718 -0.43501973], action=0, reward=1.0, next_state=[-0.26467768  0.0309559  -0.01525757 -0.14441105]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 107 ] state=[-0.26467768  0.0309559  -0.01525757 -0.14441105], action=1, reward=1.0, next_state=[-0.26405856  0.226293   -0.01814579 -0.44186814]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 108 ] state=[-0.26405856  0.226293   -0.01814579 -0.44186814], action=0, reward=1.0, next_state=[-0.2595327   0.03143246 -0.02698316 -0.15496003]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 109 ] state=[-0.2595327   0.03143246 -0.02698316 -0.15496003], action=1, reward=1.0, next_state=[-0.25890405  0.22693016 -0.03008236 -0.45603191]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 110 ] state=[-0.25890405  0.22693016 -0.03008236 -0.45603191], action=1, reward=1.0, next_state=[-0.25436545  0.42246423 -0.039203   -0.7580432 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 111 ] state=[-0.25436545  0.42246423 -0.039203   -0.7580432 ], action=0, reward=1.0, next_state=[-0.24591617  0.22790383 -0.05436386 -0.47794955]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 112 ] state=[-0.24591617  0.22790383 -0.05436386 -0.47794955], action=1, reward=1.0, next_state=[-0.24135809  0.42374948 -0.06392285 -0.78725914]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 109 ][ timestamp 113 ] state=[-0.24135809  0.42374948 -0.06392285 -0.78725914], action=0, reward=1.0, next_state=[-0.2328831   0.22956114 -0.07966803 -0.51535205]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 114 ] state=[-0.2328831   0.22956114 -0.07966803 -0.51535205], action=0, reward=1.0, next_state=[-0.22829188  0.03564616 -0.08997507 -0.24879981]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 115 ] state=[-0.22829188  0.03564616 -0.08997507 -0.24879981], action=0, reward=1.0, next_state=[-0.22757895 -0.15808335 -0.09495107  0.014202  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 116 ] state=[-0.22757895 -0.15808335 -0.09495107  0.014202  ], action=1, reward=1.0, next_state=[-0.23074062  0.03826301 -0.09466703 -0.30686458]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 117 ] state=[-0.23074062  0.03826301 -0.09466703 -0.30686458], action=0, reward=1.0, next_state=[-0.22997536 -0.15539145 -0.10080432 -0.0454741 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 118 ] state=[-0.22997536 -0.15539145 -0.10080432 -0.0454741 ], action=1, reward=1.0, next_state=[-0.23308319  0.04102066 -0.1017138  -0.36818296]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 119 ] state=[-0.23308319  0.04102066 -0.1017138  -0.36818296], action=0, reward=1.0, next_state=[-0.23226278 -0.15252002 -0.10907746 -0.10922469]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 120 ] state=[-0.23226278 -0.15252002 -0.10907746 -0.10922469], action=0, reward=1.0, next_state=[-0.23531318 -0.34592358 -0.11126196  0.14715132]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 121 ] state=[-0.23531318 -0.34592358 -0.11126196  0.14715132], action=1, reward=1.0, next_state=[-0.24223165 -0.14939873 -0.10831893 -0.17845678]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 122 ] state=[-0.24223165 -0.14939873 -0.10831893 -0.17845678], action=1, reward=1.0, next_state=[-0.24521962  0.04709316 -0.11188807 -0.50325076]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 123 ] state=[-0.24521962  0.04709316 -0.11188807 -0.50325076], action=0, reward=1.0, next_state=[-0.24427776 -0.1462887  -0.12195308 -0.24781828]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 124 ] state=[-0.24427776 -0.1462887  -0.12195308 -0.24781828], action=0, reward=1.0, next_state=[-0.24720353 -0.33947705 -0.12690945  0.00404661]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 125 ] state=[-0.24720353 -0.33947705 -0.12690945  0.00404661], action=0, reward=1.0, next_state=[-0.25399307 -0.53257207 -0.12682851  0.25414849]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 126 ] state=[-0.25399307 -0.53257207 -0.12682851  0.25414849], action=1, reward=1.0, next_state=[-0.26464452 -0.33588889 -0.12174554 -0.07569437]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 127 ] state=[-0.26464452 -0.33588889 -0.12174554 -0.07569437], action=0, reward=1.0, next_state=[-0.27136229 -0.52907422 -0.12325943  0.17623391]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 128 ] state=[-0.27136229 -0.52907422 -0.12325943  0.17623391], action=1, reward=1.0, next_state=[-0.28194378 -0.33242364 -0.11973475 -0.15265062]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 129 ] state=[-0.28194378 -0.33242364 -0.11973475 -0.15265062], action=0, reward=1.0, next_state=[-0.28859225 -0.52564577 -0.12278777  0.09998951]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 130 ] state=[-0.28859225 -0.52564577 -0.12278777  0.09998951], action=0, reward=1.0, next_state=[-0.29910517 -0.71881358 -0.12078798  0.35155074]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 131 ] state=[-0.29910517 -0.71881358 -0.12078798  0.35155074], action=1, reward=1.0, next_state=[-0.31348144 -0.52219955 -0.11375696  0.02335312]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 132 ] state=[-0.31348144 -0.52219955 -0.11375696  0.02335312], action=1, reward=1.0, next_state=[-0.32392543 -0.32564557 -0.1132899  -0.30294472]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 133 ] state=[-0.32392543 -0.32564557 -0.1132899  -0.30294472], action=0, reward=1.0, next_state=[-0.33043834 -0.51898597 -0.11934879 -0.04802924]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 134 ] state=[-0.33043834 -0.51898597 -0.11934879 -0.04802924], action=0, reward=1.0, next_state=[-0.34081806 -0.71221233 -0.12030938  0.20474318]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 135 ] state=[-0.34081806 -0.71221233 -0.12030938  0.20474318], action=0, reward=1.0, next_state=[-0.35506231 -0.90542673 -0.11621451  0.45718413]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 136 ] state=[-0.35506231 -0.90542673 -0.11621451  0.45718413], action=0, reward=1.0, next_state=[-0.37317084 -1.09873043 -0.10707083  0.71109364]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 137 ] state=[-0.37317084 -1.09873043 -0.10707083  0.71109364], action=0, reward=1.0, next_state=[-0.39514545 -1.29221957 -0.09284896  0.96824658]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 138 ] state=[-0.39514545 -1.29221957 -0.09284896  0.96824658], action=0, reward=1.0, next_state=[-0.42098984 -1.4859805  -0.07348403  1.23037769]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 139 ] state=[-0.42098984 -1.4859805  -0.07348403  1.23037769], action=1, reward=1.0, next_state=[-0.45070945 -1.2899942  -0.04887647  0.91560674]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 140 ] state=[-0.45070945 -1.2899942  -0.04887647  0.91560674], action=1, reward=1.0, next_state=[-0.47650934 -1.09424655 -0.03056434  0.60797196]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 141 ] state=[-0.47650934 -1.09424655 -0.03056434  0.60797196], action=1, reward=1.0, next_state=[-0.49839427 -0.89871092 -0.0184049   0.30582098]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 142 ] state=[-0.49839427 -0.89871092 -0.0184049   0.30582098], action=0, reward=1.0, next_state=[-0.51636848 -1.09356583 -0.01228848  0.5926431 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 143 ] state=[-0.51636848 -1.09356583 -0.01228848  0.5926431 ], action=0, reward=1.0, next_state=[-5.38239801e-01 -1.28851362e+00 -4.35618115e-04  8.81429987e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 144 ] state=[-5.38239801e-01 -1.28851362e+00 -4.35618115e-04  8.81429987e-01], action=0, reward=1.0, next_state=[-0.56401007 -1.48362965  0.01719298  1.17397593]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 145 ] state=[-0.56401007 -1.48362965  0.01719298  1.17397593], action=1, reward=1.0, next_state=[-0.59368267 -1.28873532  0.0406725   0.88673214]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 146 ] state=[-0.59368267 -1.28873532  0.0406725   0.88673214], action=0, reward=1.0, next_state=[-0.61945737 -1.48438507  0.05840714  1.19191847]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 147 ] state=[-0.61945737 -1.48438507  0.05840714  1.19191847], action=1, reward=1.0, next_state=[-0.64914507 -1.2900664   0.08224551  0.91809944]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 148 ] state=[-0.64914507 -1.2900664   0.08224551  0.91809944], action=1, reward=1.0, next_state=[-0.6749464  -1.09614687  0.1006075   0.65235631]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 149 ] state=[-0.6749464  -1.09614687  0.1006075   0.65235631], action=0, reward=1.0, next_state=[-0.69686934 -1.29251531  0.11365463  0.97494826]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 150 ] state=[-0.69686934 -1.29251531  0.11365463  0.97494826], action=0, reward=1.0, next_state=[-0.72271965 -1.48896319  0.13315359  1.3010615 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 151 ] state=[-0.72271965 -1.48896319  0.13315359  1.3010615 ], action=1, reward=1.0, next_state=[-0.75249891 -1.29575828  0.15917482  1.05285105]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 152 ] state=[-0.75249891 -1.29575828  0.15917482  1.05285105], action=0, reward=1.0, next_state=[-0.77841408 -1.49259142  0.18023184  1.39096836]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 153 ] state=[-0.77841408 -1.49259142  0.18023184  1.39096836], action=0, reward=1.0, next_state=[-0.8082659  -1.68944134  0.20805121  1.73416218]\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 154 ] state=[-0.8082659  -1.68944134  0.20805121  1.73416218], action=0, reward=-1.0, next_state=[-0.84205473 -1.8862413   0.24273445  2.08372296]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 109: Exploration_rate=0.01. Score=154.\n",
      "[ episode 110 ] state=[-0.04090479 -0.04054065 -0.02417352 -0.0214664 ]\n",
      "[ episode 110 ][ timestamp 1 ] state=[-0.04090479 -0.04054065 -0.02417352 -0.0214664 ], action=0, reward=1.0, next_state=[-0.0417156  -0.23530774 -0.02460285  0.26349255]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 2 ] state=[-0.0417156  -0.23530774 -0.02460285  0.26349255], action=0, reward=1.0, next_state=[-0.04642176 -0.43007003 -0.019333    0.54831507]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 3 ] state=[-0.04642176 -0.43007003 -0.019333    0.54831507], action=0, reward=1.0, next_state=[-0.05502316 -0.62491513 -0.00836669  0.83484456]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 4 ] state=[-0.05502316 -0.62491513 -0.00836669  0.83484456], action=0, reward=1.0, next_state=[-0.06752146 -0.81992179  0.0083302   1.12488452]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 110 ][ timestamp 5 ] state=[-0.06752146 -0.81992179  0.0083302   1.12488452], action=0, reward=1.0, next_state=[-0.0839199  -1.01515193  0.03082789  1.42016862]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 6 ] state=[-0.0839199  -1.01515193  0.03082789  1.42016862], action=1, reward=1.0, next_state=[-0.10422294 -0.8204247   0.05923126  1.13727852]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 7 ] state=[-0.10422294 -0.8204247   0.05923126  1.13727852], action=0, reward=1.0, next_state=[-0.12063143 -1.01626922  0.08197683  1.44793394]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 8 ] state=[-0.12063143 -1.01626922  0.08197683  1.44793394], action=1, reward=1.0, next_state=[-0.14095682 -0.82224538  0.11093551  1.18194975]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 9 ] state=[-0.14095682 -0.82224538  0.11093551  1.18194975], action=1, reward=1.0, next_state=[-0.15740172 -0.62872405  0.1345745   0.92600031]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 10 ] state=[-0.15740172 -0.62872405  0.1345745   0.92600031], action=1, reward=1.0, next_state=[-0.1699762  -0.43565094  0.15309451  0.67845474]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 11 ] state=[-0.1699762  -0.43565094  0.15309451  0.67845474], action=1, reward=1.0, next_state=[-0.17868922 -0.24294986  0.1666636   0.43761807]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 12 ] state=[-0.17868922 -0.24294986  0.1666636   0.43761807], action=1, reward=1.0, next_state=[-0.18354822 -0.05053057  0.17541597  0.20176103]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 13 ] state=[-0.18354822 -0.05053057  0.17541597  0.20176103], action=1, reward=1.0, next_state=[-0.18455883  0.14170547  0.17945119 -0.03085976]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 14 ] state=[-0.18455883  0.14170547  0.17945119 -0.03085976], action=1, reward=1.0, next_state=[-0.18172472  0.33386096  0.17883399 -0.26198853]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 15 ] state=[-0.18172472  0.33386096  0.17883399 -0.26198853], action=0, reward=1.0, next_state=[-0.1750475   0.136697    0.17359422  0.08133817]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 16 ] state=[-0.1750475   0.136697    0.17359422  0.08133817], action=1, reward=1.0, next_state=[-0.17231356  0.32896049  0.17522098 -0.15194182]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 17 ] state=[-0.17231356  0.32896049  0.17522098 -0.15194182], action=1, reward=1.0, next_state=[-0.16573435  0.5211971   0.17218215 -0.38462969]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 18 ] state=[-0.16573435  0.5211971   0.17218215 -0.38462969], action=0, reward=1.0, next_state=[-0.15531041  0.32410239  0.16448955 -0.04298742]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 19 ] state=[-0.15531041  0.32410239  0.16448955 -0.04298742], action=0, reward=1.0, next_state=[-0.14882836  0.12705065  0.16362981  0.29674265]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 20 ] state=[-0.14882836  0.12705065  0.16362981  0.29674265], action=1, reward=1.0, next_state=[-0.14628735  0.31950787  0.16956466  0.05980572]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 21 ] state=[-0.14628735  0.31950787  0.16956466  0.05980572], action=1, reward=1.0, next_state=[-0.13989719  0.51184418  0.17076077 -0.17494764]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 22 ] state=[-0.13989719  0.51184418  0.17076077 -0.17494764], action=1, reward=1.0, next_state=[-0.12966031  0.70416321  0.16726182 -0.40927046]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 23 ] state=[-0.12966031  0.70416321  0.16726182 -0.40927046], action=0, reward=1.0, next_state=[-0.11557704  0.5071138   0.15907641 -0.06887528]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 24 ] state=[-0.11557704  0.5071138   0.15907641 -0.06887528], action=1, reward=1.0, next_state=[-0.10543477  0.69963991  0.15769891 -0.30744671]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 25 ] state=[-0.10543477  0.69963991  0.15769891 -0.30744671], action=1, reward=1.0, next_state=[-0.09144197  0.89220451  0.15154997 -0.54653785]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 26 ] state=[-0.09144197  0.89220451  0.15154997 -0.54653785], action=1, reward=1.0, next_state=[-0.07359788  1.08490876  0.14061921 -0.7878958 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 27 ] state=[-0.07359788  1.08490876  0.14061921 -0.7878958 ], action=1, reward=1.0, next_state=[-0.05189971  1.27784807  0.1248613  -1.03324219]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 28 ] state=[-0.05189971  1.27784807  0.1248613  -1.03324219], action=0, reward=1.0, next_state=[-0.02634274  1.08130673  0.10419645 -0.7041114 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 29 ] state=[-0.02634274  1.08130673  0.10419645 -0.7041114 ], action=1, reward=1.0, next_state=[-0.00471661  1.27484226  0.09011423 -0.96226186]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 30 ] state=[-0.00471661  1.27484226  0.09011423 -0.96226186], action=0, reward=1.0, next_state=[ 0.02078024  1.07863249  0.07086899 -0.64268366]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 31 ] state=[ 0.02078024  1.07863249  0.07086899 -0.64268366], action=1, reward=1.0, next_state=[ 0.04235289  1.27269881  0.05801532 -0.91223439]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 32 ] state=[ 0.04235289  1.27269881  0.05801532 -0.91223439], action=1, reward=1.0, next_state=[ 0.06780686  1.46698988  0.03977063 -1.18613375]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 33 ] state=[ 0.06780686  1.46698988  0.03977063 -1.18613375], action=1, reward=1.0, next_state=[ 0.09714666  1.66157413  0.01604795 -1.46608984]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 34 ] state=[ 0.09714666  1.66157413  0.01604795 -1.46608984], action=0, reward=1.0, next_state=[ 0.13037814  1.4662594  -0.01327384 -1.16843757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 35 ] state=[ 0.13037814  1.4662594  -0.01327384 -1.16843757], action=0, reward=1.0, next_state=[ 0.15970333  1.27131263 -0.0366426  -0.87994557]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 36 ] state=[ 0.15970333  1.27131263 -0.0366426  -0.87994557], action=0, reward=1.0, next_state=[ 0.18512958  1.07670715 -0.05424151 -0.59900381]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 37 ] state=[ 0.18512958  1.07670715 -0.05424151 -0.59900381], action=1, reward=1.0, next_state=[ 0.20666373  1.27254438 -0.06622158 -0.90826681]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 38 ] state=[ 0.20666373  1.27254438 -0.06622158 -0.90826681], action=0, reward=1.0, next_state=[ 0.23211461  1.07837833 -0.08438692 -0.63711102]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 39 ] state=[ 0.23211461  1.07837833 -0.08438692 -0.63711102], action=1, reward=1.0, next_state=[ 0.25368218  1.27456941 -0.09712914 -0.95513076]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 40 ] state=[ 0.25368218  1.27456941 -0.09712914 -0.95513076], action=0, reward=1.0, next_state=[ 0.27917357  1.08087866 -0.11623175 -0.69447511]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 41 ] state=[ 0.27917357  1.08087866 -0.11623175 -0.69447511], action=1, reward=1.0, next_state=[ 0.30079114  1.27740461 -0.13012126 -1.02137025]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 42 ] state=[ 0.30079114  1.27740461 -0.13012126 -1.02137025], action=1, reward=1.0, next_state=[ 0.32633923  1.47399745 -0.15054866 -1.35191435]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 43 ] state=[ 0.32633923  1.47399745 -0.15054866 -1.35191435], action=1, reward=1.0, next_state=[ 0.35581918  1.6706549  -0.17758695 -1.68765822]\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 44 ] state=[ 0.35581918  1.6706549  -0.17758695 -1.68765822], action=1, reward=-1.0, next_state=[ 0.38923228  1.86733053 -0.21134011 -2.0299685 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 110: Exploration_rate=0.01. Score=44.\n",
      "[ episode 111 ] state=[-0.03722399 -0.00268046  0.04671278 -0.00991563]\n",
      "[ episode 111 ][ timestamp 1 ] state=[-0.03722399 -0.00268046  0.04671278 -0.00991563], action=0, reward=1.0, next_state=[-0.0372776  -0.19844013  0.04651446  0.29713212]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 2 ] state=[-0.0372776  -0.19844013  0.04651446  0.29713212], action=1, reward=1.0, next_state=[-0.0412464  -0.00401106  0.0524571   0.01947427]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 3 ] state=[-0.0412464  -0.00401106  0.0524571   0.01947427], action=1, reward=1.0, next_state=[-0.04132662  0.19032089  0.05284659 -0.25620736]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 111 ][ timestamp 4 ] state=[-0.04132662  0.19032089  0.05284659 -0.25620736], action=0, reward=1.0, next_state=[-0.0375202  -0.00551419  0.04772244  0.05266482]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 5 ] state=[-0.0375202  -0.00551419  0.04772244  0.05266482], action=0, reward=1.0, next_state=[-0.03763049 -0.20128678  0.04877574  0.36001446]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 6 ] state=[-0.03763049 -0.20128678  0.04877574  0.36001446], action=1, reward=1.0, next_state=[-0.04165622 -0.00689088  0.05597603  0.08310179]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 7 ] state=[-0.04165622 -0.00689088  0.05597603  0.08310179], action=1, reward=1.0, next_state=[-0.04179404  0.18738585  0.05763806 -0.19140852]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 8 ] state=[-0.04179404  0.18738585  0.05763806 -0.19140852], action=1, reward=1.0, next_state=[-0.03804632  0.3816379   0.05380989 -0.46536653]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 9 ] state=[-0.03804632  0.3816379   0.05380989 -0.46536653], action=1, reward=1.0, next_state=[-0.03041356  0.57595987  0.04450256 -0.7406151 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 10 ] state=[-0.03041356  0.57595987  0.04450256 -0.7406151 ], action=0, reward=1.0, next_state=[-0.01889437  0.38025267  0.02969026 -0.43426553]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 11 ] state=[-0.01889437  0.38025267  0.02969026 -0.43426553], action=1, reward=1.0, next_state=[-0.01128931  0.57494198  0.02100495 -0.71744313]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 12 ] state=[-0.01128931  0.57494198  0.02100495 -0.71744313], action=1, reward=1.0, next_state=[ 2.09525653e-04  7.69767038e-01  6.65608821e-03 -1.00344125e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 13 ] state=[ 2.09525653e-04  7.69767038e-01  6.65608821e-03 -1.00344125e+00], action=0, reward=1.0, next_state=[ 0.01560487  0.5745568  -0.01341274 -0.7086755 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 14 ] state=[ 0.01560487  0.5745568  -0.01341274 -0.7086755 ], action=0, reward=1.0, next_state=[ 0.027096    0.37962318 -0.02758625 -0.4202446 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 15 ] state=[ 0.027096    0.37962318 -0.02758625 -0.4202446 ], action=0, reward=1.0, next_state=[ 0.03468847  0.18490274 -0.03599114 -0.1363844 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 16 ] state=[ 0.03468847  0.18490274 -0.03599114 -0.1363844 ], action=0, reward=1.0, next_state=[ 0.03838652 -0.00968571 -0.03871883  0.14473014]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 17 ] state=[ 0.03838652 -0.00968571 -0.03871883  0.14473014], action=0, reward=1.0, next_state=[ 0.03819281 -0.20423239 -0.03582422  0.42495096]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 18 ] state=[ 0.03819281 -0.20423239 -0.03582422  0.42495096], action=1, reward=1.0, next_state=[ 0.03410816 -0.00862178 -0.0273252   0.12119323]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 19 ] state=[ 0.03410816 -0.00862178 -0.0273252   0.12119323], action=1, reward=1.0, next_state=[ 0.03393572  0.18688079 -0.02490134 -0.17998376]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 20 ] state=[ 0.03393572  0.18688079 -0.02490134 -0.17998376], action=0, reward=1.0, next_state=[ 0.03767334 -0.00787614 -0.02850102  0.10474083]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 21 ] state=[ 0.03767334 -0.00787614 -0.02850102  0.10474083], action=0, reward=1.0, next_state=[ 0.03751582 -0.2025783  -0.0264062   0.38829729]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 22 ] state=[ 0.03751582 -0.2025783  -0.0264062   0.38829729], action=0, reward=1.0, next_state=[ 0.03346425 -0.39731567 -0.01864025  0.672539  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 23 ] state=[ 0.03346425 -0.39731567 -0.01864025  0.672539  ], action=0, reward=1.0, next_state=[ 0.02551794 -0.59217363 -0.00518947  0.95929524]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 24 ] state=[ 0.02551794 -0.59217363 -0.00518947  0.95929524], action=1, reward=1.0, next_state=[ 0.01367446 -0.3969823   0.01399643  0.66498649]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 25 ] state=[ 0.01367446 -0.3969823   0.01399643  0.66498649], action=1, reward=1.0, next_state=[ 0.00573482 -0.2020578   0.02729616  0.37674321]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 26 ] state=[ 0.00573482 -0.2020578   0.02729616  0.37674321], action=1, reward=1.0, next_state=[ 0.00169366 -0.00733396  0.03483103  0.09279032]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 27 ] state=[ 0.00169366 -0.00733396  0.03483103  0.09279032], action=0, reward=1.0, next_state=[ 0.00154698 -0.20293739  0.03668683  0.39625575]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 28 ] state=[ 0.00154698 -0.20293739  0.03668683  0.39625575], action=1, reward=1.0, next_state=[-0.00251176 -0.00835464  0.04461195  0.11536154]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 29 ] state=[-0.00251176 -0.00835464  0.04461195  0.11536154], action=1, reward=1.0, next_state=[-0.00267886  0.18610064  0.04691918 -0.16291961]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 30 ] state=[-0.00267886  0.18610064  0.04691918 -0.16291961], action=0, reward=1.0, next_state=[ 0.00104316 -0.00966049  0.04366079  0.14418811]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 31 ] state=[ 0.00104316 -0.00966049  0.04366079  0.14418811], action=1, reward=1.0, next_state=[ 0.00084995  0.18480987  0.04654455 -0.13440725]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 32 ] state=[ 0.00084995  0.18480987  0.04654455 -0.13440725], action=1, reward=1.0, next_state=[ 0.00454614  0.37923529  0.0438564  -0.41205038]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 33 ] state=[ 0.00454614  0.37923529  0.0438564  -0.41205038], action=1, reward=1.0, next_state=[ 0.01213085  0.573709    0.03561539 -0.6905908 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 34 ] state=[ 0.01213085  0.573709    0.03561539 -0.6905908 ], action=1, reward=1.0, next_state=[ 0.02360503  0.76831912  0.02180358 -0.97185214]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 35 ] state=[ 0.02360503  0.76831912  0.02180358 -0.97185214], action=1, reward=1.0, next_state=[ 0.03897141  0.96314178  0.00236654 -1.25760694]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 36 ] state=[ 0.03897141  0.96314178  0.00236654 -1.25760694], action=1, reward=1.0, next_state=[ 0.05823425  1.15823337 -0.0227856  -1.54954774]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 37 ] state=[ 0.05823425  1.15823337 -0.0227856  -1.54954774], action=0, reward=1.0, next_state=[ 0.08139891  0.96339211 -0.05377656 -1.2640601 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 38 ] state=[ 0.08139891  0.96339211 -0.05377656 -1.2640601 ], action=0, reward=1.0, next_state=[ 0.10066676  0.76899721 -0.07905776 -0.98869197]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 39 ] state=[ 0.10066676  0.76899721 -0.07905776 -0.98869197], action=1, reward=1.0, next_state=[ 0.1160467   0.96508344 -0.0988316  -1.3051214 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 40 ] state=[ 0.1160467   0.96508344 -0.0988316  -1.3051214 ], action=1, reward=1.0, next_state=[ 0.13534837  1.16130981 -0.12493403 -1.62703383]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 41 ] state=[ 0.13534837  1.16130981 -0.12493403 -1.62703383], action=0, reward=1.0, next_state=[ 0.15857457  0.96785898 -0.1574747  -1.37575437]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 42 ] state=[ 0.15857457  0.96785898 -0.1574747  -1.37575437], action=0, reward=1.0, next_state=[ 0.17793175  0.77501574 -0.18498979 -1.1361752 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 43 ] state=[ 0.17793175  0.77501574 -0.18498979 -1.1361752 ], action=0, reward=1.0, next_state=[ 0.19343206  0.58273106 -0.2077133  -0.90674661]\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 44 ] state=[ 0.19343206  0.58273106 -0.2077133  -0.90674661], action=0, reward=-1.0, next_state=[ 0.20508668  0.39093449 -0.22584823 -0.68586527]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 111: Exploration_rate=0.01. Score=44.\n",
      "[ episode 112 ] state=[-0.03772223  0.03039005  0.00177477  0.02138015]\n",
      "[ episode 112 ][ timestamp 1 ] state=[-0.03772223  0.03039005  0.00177477  0.02138015], action=0, reward=1.0, next_state=[-0.03711443 -0.16475731  0.00220237  0.31462251]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 2 ] state=[-0.03711443 -0.16475731  0.00220237  0.31462251], action=0, reward=1.0, next_state=[-0.04040958 -0.35991056  0.00849482  0.60799918]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 3 ] state=[-0.04040958 -0.35991056  0.00849482  0.60799918], action=1, reward=1.0, next_state=[-0.04760779 -0.1649084   0.02065481  0.31800394]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 112 ][ timestamp 4 ] state=[-0.04760779 -0.1649084   0.02065481  0.31800394], action=1, reward=1.0, next_state=[-0.05090596  0.02991338  0.02701489  0.0319057 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 5 ] state=[-0.05090596  0.02991338  0.02701489  0.0319057 ], action=1, reward=1.0, next_state=[-0.05030769  0.22463771  0.027653   -0.25213282]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 6 ] state=[-0.05030769  0.22463771  0.027653   -0.25213282], action=1, reward=1.0, next_state=[-0.04581493  0.4193541   0.02261034 -0.5359668 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 7 ] state=[-0.04581493  0.4193541   0.02261034 -0.5359668 ], action=1, reward=1.0, next_state=[-0.03742785  0.61415095  0.01189101 -0.82144051]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 8 ] state=[-0.03742785  0.61415095  0.01189101 -0.82144051], action=0, reward=1.0, next_state=[-0.02514483  0.41886833 -0.0045378  -0.52504141]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 9 ] state=[-0.02514483  0.41886833 -0.0045378  -0.52504141], action=0, reward=1.0, next_state=[-0.01676747  0.22381053 -0.01503863 -0.23379184]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 10 ] state=[-0.01676747  0.22381053 -0.01503863 -0.23379184], action=1, reward=1.0, next_state=[-0.01229126  0.41914409 -0.01971447 -0.53118024]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 11 ] state=[-0.01229126  0.41914409 -0.01971447 -0.53118024], action=0, reward=1.0, next_state=[-0.00390837  0.22430491 -0.03033807 -0.24477394]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 12 ] state=[-0.00390837  0.22430491 -0.03033807 -0.24477394], action=0, reward=1.0, next_state=[ 0.00057772  0.02962912 -0.03523355  0.03818735]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 13 ] state=[ 0.00057772  0.02962912 -0.03523355  0.03818735], action=0, reward=1.0, next_state=[ 0.00117031 -0.16497033 -0.03446981  0.31954884]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 14 ] state=[ 0.00117031 -0.16497033 -0.03446981  0.31954884], action=0, reward=1.0, next_state=[-0.0021291  -0.35958484 -0.02807883  0.60116508]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 15 ] state=[-0.0021291  -0.35958484 -0.02807883  0.60116508], action=1, reward=1.0, next_state=[-0.0093208  -0.16408159 -0.01605553  0.2997717 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 16 ] state=[-0.0093208  -0.16408159 -0.01605553  0.2997717 ], action=1, reward=1.0, next_state=[-0.01260243  0.03126549 -0.01006009  0.00206873]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 17 ] state=[-0.01260243  0.03126549 -0.01006009  0.00206873], action=0, reward=1.0, next_state=[-0.01197712 -0.16371075 -0.01001872  0.29156067]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 18 ] state=[-0.01197712 -0.16371075 -0.01001872  0.29156067], action=0, reward=1.0, next_state=[-0.01525133 -0.35868842 -0.0041875   0.58106706]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 19 ] state=[-0.01525133 -0.35868842 -0.0041875   0.58106706], action=0, reward=1.0, next_state=[-0.0224251  -0.55375145  0.00743384  0.87242791]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 20 ] state=[-0.0224251  -0.55375145  0.00743384  0.87242791], action=0, reward=1.0, next_state=[-0.03350013 -0.7489737   0.02488239  1.16743872]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 21 ] state=[-0.03350013 -0.7489737   0.02488239  1.16743872], action=1, reward=1.0, next_state=[-0.0484796  -0.55418419  0.04823117  0.88265957]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 22 ] state=[-0.0484796  -0.55418419  0.04823117  0.88265957], action=0, reward=1.0, next_state=[-0.05956329 -0.74992684  0.06588436  1.19010657]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 23 ] state=[-0.05956329 -0.74992684  0.06588436  1.19010657], action=0, reward=1.0, next_state=[-0.07456183 -0.94583783  0.08968649  1.50269147]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 24 ] state=[-0.07456183 -0.94583783  0.08968649  1.50269147], action=1, reward=1.0, next_state=[-0.09347858 -0.75191174  0.11974032  1.23930396]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 25 ] state=[-0.09347858 -0.75191174  0.11974032  1.23930396], action=0, reward=1.0, next_state=[-0.10851682 -0.94835048  0.1445264   1.56697182]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 26 ] state=[-0.10851682 -0.94835048  0.1445264   1.56697182], action=1, reward=1.0, next_state=[-0.12748383 -0.75522063  0.17586584  1.32264032]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 27 ] state=[-0.12748383 -0.75522063  0.17586584  1.32264032], action=1, reward=1.0, next_state=[-0.14258824 -0.56270177  0.20231864  1.08975476]\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 28 ] state=[-0.14258824 -0.56270177  0.20231864  1.08975476], action=1, reward=-1.0, next_state=[-0.15384227 -0.37073717  0.22411374  0.86675774]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 112: Exploration_rate=0.01. Score=28.\n",
      "[ episode 113 ] state=[ 0.00143792 -0.01668451 -0.01368553 -0.01063847]\n",
      "[ episode 113 ][ timestamp 1 ] state=[ 0.00143792 -0.01668451 -0.01368553 -0.01063847], action=0, reward=1.0, next_state=[ 0.00110423 -0.21160754 -0.0138983   0.27769528]\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 2 ] state=[ 0.00110423 -0.21160754 -0.0138983   0.27769528], action=0, reward=1.0, next_state=[-0.00312792 -0.40652849 -0.0083444   0.56596249]\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 3 ] state=[-0.00312792 -0.40652849 -0.0083444   0.56596249], action=1, reward=1.0, next_state=[-0.01125849 -0.21129047  0.00297485  0.27066244]\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 4 ] state=[-0.01125849 -0.21129047  0.00297485  0.27066244], action=1, reward=1.0, next_state=[-0.0154843  -0.0162111   0.0083881  -0.02108072]\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 5 ] state=[-0.0154843  -0.0162111   0.0083881  -0.02108072], action=0, reward=1.0, next_state=[-0.01580852 -0.21145233  0.00796649  0.2742369 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 6 ] state=[-0.01580852 -0.21145233  0.00796649  0.2742369 ], action=0, reward=1.0, next_state=[-0.02003756 -0.40668704  0.01345123  0.56942179]\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 7 ] state=[-0.02003756 -0.40668704  0.01345123  0.56942179], action=0, reward=1.0, next_state=[-0.02817131 -0.60199504  0.02483966  0.86631183]\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 8 ] state=[-0.02817131 -0.60199504  0.02483966  0.86631183], action=0, reward=1.0, next_state=[-0.04021121 -0.79744607  0.0421659   1.16670005]\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 9 ] state=[-0.04021121 -0.79744607  0.0421659   1.16670005], action=1, reward=1.0, next_state=[-0.05616013 -0.60289744  0.0654999   0.88752959]\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 10 ] state=[-0.05616013 -0.60289744  0.0654999   0.88752959], action=0, reward=1.0, next_state=[-0.06821808 -0.79884435  0.08325049  1.20006288]\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 11 ] state=[-0.06821808 -0.79884435  0.08325049  1.20006288], action=1, reward=1.0, next_state=[-0.08419496 -0.60489212  0.10725175  0.9345895 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 12 ] state=[-0.08419496 -0.60489212  0.10725175  0.9345895 ], action=0, reward=1.0, next_state=[-0.09629281 -0.80128456  0.12594354  1.25895707]\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 13 ] state=[-0.09629281 -0.80128456  0.12594354  1.25895707], action=1, reward=1.0, next_state=[-0.1123185  -0.60797872  0.15112268  1.00822449]\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 14 ] state=[-0.1123185  -0.60797872  0.15112268  1.00822449], action=1, reward=1.0, next_state=[-0.12447807 -0.41516171  0.17128717  0.76655651]\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 15 ] state=[-0.12447807 -0.41516171  0.17128717  0.76655651], action=1, reward=1.0, next_state=[-0.13278131 -0.2227598   0.1866183   0.53228956]\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 16 ] state=[-0.13278131 -0.2227598   0.1866183   0.53228956], action=1, reward=1.0, next_state=[-0.1372365  -0.03068504  0.19726409  0.3037277 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 17 ] state=[-0.1372365  -0.03068504  0.19726409  0.3037277 ], action=1, reward=1.0, next_state=[-0.1378502   0.16115913  0.20333865  0.0791625 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 18 ] state=[-0.1378502   0.16115913  0.20333865  0.0791625 ], action=1, reward=1.0, next_state=[-0.13462702  0.3528738   0.2049219  -0.14311446]\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 19 ] state=[-0.13462702  0.3528738   0.2049219  -0.14311446], action=0, reward=1.0, next_state=[-0.12756954  0.15549665  0.20205961  0.20658291]\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 20 ] state=[-0.12756954  0.15549665  0.20205961  0.20658291], action=1, reward=1.0, next_state=[-0.12445961  0.34724229  0.20619126 -0.01618193]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 113 ][ timestamp 21 ] state=[-0.12445961  0.34724229  0.20619126 -0.01618193], action=0, reward=1.0, next_state=[-0.11751476  0.14985181  0.20586763  0.33382364]\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 22 ] state=[-0.11751476  0.14985181  0.20586763  0.33382364], action=0, reward=-1.0, next_state=[-0.11451773 -0.04751419  0.2125441   0.68371974]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 113: Exploration_rate=0.01. Score=22.\n",
      "[ episode 114 ] state=[-0.00087059 -0.01127649  0.01616434 -0.01784859]\n",
      "[ episode 114 ][ timestamp 1 ] state=[-0.00087059 -0.01127649  0.01616434 -0.01784859], action=1, reward=1.0, next_state=[-0.00109612  0.18360996  0.01580736 -0.30538797]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 2 ] state=[-0.00109612  0.18360996  0.01580736 -0.30538797], action=1, reward=1.0, next_state=[ 0.00257608  0.37850312  0.00969961 -0.59304402]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 3 ] state=[ 0.00257608  0.37850312  0.00969961 -0.59304402], action=0, reward=1.0, next_state=[ 0.01014615  0.18324674 -0.00216128 -0.2973216 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 4 ] state=[ 0.01014615  0.18324674 -0.00216128 -0.2973216 ], action=0, reward=1.0, next_state=[ 0.01381108 -0.01184433 -0.00810771 -0.00532108]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 5 ] state=[ 0.01381108 -0.01184433 -0.00810771 -0.00532108], action=0, reward=1.0, next_state=[ 0.01357419 -0.20684907 -0.00821413  0.28479278]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 6 ] state=[ 0.01357419 -0.20684907 -0.00821413  0.28479278], action=0, reward=1.0, next_state=[ 0.00943721 -0.40185292 -0.00251827  0.57487375]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 7 ] state=[ 0.00943721 -0.40185292 -0.00251827  0.57487375], action=0, reward=1.0, next_state=[ 0.00140015 -0.59693947  0.0089792   0.86676228]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 8 ] state=[ 0.00140015 -0.59693947  0.0089792   0.86676228], action=1, reward=1.0, next_state=[-0.01053863 -0.40194086  0.02631445  0.576916  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 9 ] state=[-0.01053863 -0.40194086  0.02631445  0.576916  ], action=1, reward=1.0, next_state=[-0.01857745 -0.20719744  0.03785277  0.29263756]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 10 ] state=[-0.01857745 -0.20719744  0.03785277  0.29263756], action=1, reward=1.0, next_state=[-0.0227214  -0.01263506  0.04370552  0.01212911]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 11 ] state=[-0.0227214  -0.01263506  0.04370552  0.01212911], action=1, reward=1.0, next_state=[-0.0229741   0.18183373  0.0439481  -0.26645019]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 12 ] state=[-0.0229741   0.18183373  0.0439481  -0.26645019], action=1, reward=1.0, next_state=[-0.01933743  0.37630179  0.0386191  -0.54495404]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 13 ] state=[-0.01933743  0.37630179  0.0386191  -0.54495404], action=1, reward=1.0, next_state=[-0.01181139  0.5708604   0.02772002 -0.82522316]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 14 ] state=[-0.01181139  0.5708604   0.02772002 -0.82522316], action=0, reward=1.0, next_state=[-3.94183916e-04  3.75370509e-01  1.12155531e-02 -5.23952341e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 15 ] state=[-3.94183916e-04  3.75370509e-01  1.12155531e-02 -5.23952341e-01], action=0, reward=1.0, next_state=[ 0.00711323  0.18009253  0.00073651 -0.22775648]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 16 ] state=[ 0.00711323  0.18009253  0.00073651 -0.22775648], action=1, reward=1.0, next_state=[ 0.01071508  0.37520394 -0.00381862 -0.520207  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 17 ] state=[ 0.01071508  0.37520394 -0.00381862 -0.520207  ], action=1, reward=1.0, next_state=[ 0.01821916  0.57037944 -0.01422276 -0.81409079]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 18 ] state=[ 0.01821916  0.57037944 -0.01422276 -0.81409079], action=1, reward=1.0, next_state=[ 0.02962674  0.76569325 -0.03050458 -1.11121322]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 19 ] state=[ 0.02962674  0.76569325 -0.03050458 -1.11121322], action=0, reward=1.0, next_state=[ 0.04494061  0.57098502 -0.05272884 -0.82825371]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 20 ] state=[ 0.04494061  0.57098502 -0.05272884 -0.82825371], action=1, reward=1.0, next_state=[ 0.05636031  0.76678671 -0.06929392 -1.13704314]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 21 ] state=[ 0.05636031  0.76678671 -0.06929392 -1.13704314], action=0, reward=1.0, next_state=[ 0.07169604  0.57263613 -0.09203478 -0.86687228]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 22 ] state=[ 0.07169604  0.57263613 -0.09203478 -0.86687228], action=0, reward=1.0, next_state=[ 0.08314877  0.37887902 -0.10937223 -0.60448669]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 23 ] state=[ 0.08314877  0.37887902 -0.10937223 -0.60448669], action=1, reward=1.0, next_state=[ 0.09072635  0.57534686 -0.12146196 -0.92951891]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 24 ] state=[ 0.09072635  0.57534686 -0.12146196 -0.92951891], action=0, reward=1.0, next_state=[ 0.10223328  0.38205527 -0.14005234 -0.6773397 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 25 ] state=[ 0.10223328  0.38205527 -0.14005234 -0.6773397 ], action=0, reward=1.0, next_state=[ 0.10987439  0.18912831 -0.15359913 -0.43182369]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 26 ] state=[ 0.10987439  0.18912831 -0.15359913 -0.43182369], action=0, reward=1.0, next_state=[ 0.11365696 -0.00352316 -0.16223561 -0.19122945]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 27 ] state=[ 0.11365696 -0.00352316 -0.16223561 -0.19122945], action=0, reward=1.0, next_state=[ 0.11358649 -0.19599734 -0.16606019  0.04620235]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 28 ] state=[ 0.11358649 -0.19599734 -0.16606019  0.04620235], action=1, reward=1.0, next_state=[ 0.10966655  0.00106828 -0.16513615 -0.29392736]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 29 ] state=[ 0.10966655  0.00106828 -0.16513615 -0.29392736], action=1, reward=1.0, next_state=[ 0.10968791  0.19811224 -0.17101469 -0.63380207]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 30 ] state=[ 0.10968791  0.19811224 -0.17101469 -0.63380207], action=0, reward=1.0, next_state=[ 0.11365016  0.00573633 -0.18369074 -0.39948122]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 31 ] state=[ 0.11365016  0.00573633 -0.18369074 -0.39948122], action=0, reward=1.0, next_state=[ 0.11376488 -0.18636954 -0.19168036 -0.1698722 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 32 ] state=[ 0.11376488 -0.18636954 -0.19168036 -0.1698722 ], action=0, reward=1.0, next_state=[ 0.11003749 -0.37830501 -0.1950778   0.05674862]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 33 ] state=[ 0.11003749 -0.37830501 -0.1950778   0.05674862], action=0, reward=1.0, next_state=[ 0.10247139 -0.57017279 -0.19394283  0.28210164]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 34 ] state=[ 0.10247139 -0.57017279 -0.19394283  0.28210164], action=1, reward=1.0, next_state=[ 0.09106794 -0.3728898  -0.1883008  -0.06493726]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 35 ] state=[ 0.09106794 -0.3728898  -0.1883008  -0.06493726], action=1, reward=1.0, next_state=[ 0.08361014 -0.17563723 -0.18959954 -0.41061994]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 36 ] state=[ 0.08361014 -0.17563723 -0.18959954 -0.41061994], action=0, reward=1.0, next_state=[ 0.0800974  -0.36763674 -0.19781194 -0.18319057]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 37 ] state=[ 0.0800974  -0.36763674 -0.19781194 -0.18319057], action=0, reward=1.0, next_state=[ 0.07274466 -0.55945955 -0.20147576  0.04115434]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 38 ] state=[ 0.07274466 -0.55945955 -0.20147576  0.04115434], action=0, reward=1.0, next_state=[ 0.06155547 -0.75120808 -0.20065267  0.26412524]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 39 ] state=[ 0.06155547 -0.75120808 -0.20065267  0.26412524], action=1, reward=1.0, next_state=[ 0.04653131 -0.55387224 -0.19537016 -0.08453653]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 40 ] state=[ 0.04653131 -0.55387224 -0.19537016 -0.08453653], action=0, reward=1.0, next_state=[ 0.03545386 -0.74573534 -0.19706089  0.14070895]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 41 ] state=[ 0.03545386 -0.74573534 -0.19706089  0.14070895], action=1, reward=1.0, next_state=[ 0.02053916 -0.54841707 -0.19424672 -0.20710183]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 42 ] state=[ 0.02053916 -0.54841707 -0.19424672 -0.20710183], action=0, reward=1.0, next_state=[ 0.00957081 -0.74030771 -0.19838875  0.01857082]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 114 ][ timestamp 43 ] state=[ 0.00957081 -0.74030771 -0.19838875  0.01857082], action=0, reward=1.0, next_state=[-0.00523534 -0.93211345 -0.19801734  0.2426917 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 44 ] state=[-0.00523534 -0.93211345 -0.19801734  0.2426917 ], action=0, reward=1.0, next_state=[-0.02387761 -1.1239373  -0.1931635   0.46696731]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 45 ] state=[-0.02387761 -1.1239373  -0.1931635   0.46696731], action=1, reward=1.0, next_state=[-0.04635635 -0.9266862  -0.18382416  0.12015584]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 46 ] state=[-0.04635635 -0.9266862  -0.18382416  0.12015584], action=0, reward=1.0, next_state=[-0.06489008 -1.11876381 -0.18142104  0.3496776 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 47 ] state=[-0.06489008 -1.11876381 -0.18142104  0.3496776 ], action=0, reward=1.0, next_state=[-0.08726535 -1.31090448 -0.17442749  0.58011289]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 48 ] state=[-0.08726535 -1.31090448 -0.17442749  0.58011289], action=1, reward=1.0, next_state=[-0.11348344 -1.11382271 -0.16282523  0.23795395]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 49 ] state=[-0.11348344 -1.11382271 -0.16282523  0.23795395], action=0, reward=1.0, next_state=[-0.1357599  -1.30628944 -0.15806615  0.47517611]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 50 ] state=[-0.1357599  -1.30628944 -0.15806615  0.47517611], action=1, reward=1.0, next_state=[-0.16188569 -1.10932992 -0.14856263  0.13714173]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 51 ] state=[-0.16188569 -1.10932992 -0.14856263  0.13714173], action=1, reward=1.0, next_state=[-0.18407229 -0.91242694 -0.14581979 -0.19847628]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 52 ] state=[-0.18407229 -0.91242694 -0.14581979 -0.19847628], action=1, reward=1.0, next_state=[-0.20232082 -0.71555297 -0.14978932 -0.53337239]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 53 ] state=[-0.20232082 -0.71555297 -0.14978932 -0.53337239], action=0, reward=1.0, next_state=[-0.21663188 -0.90828608 -0.16045677 -0.29138348]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 54 ] state=[-0.21663188 -0.90828608 -0.16045677 -0.29138348], action=0, reward=1.0, next_state=[-0.23479761 -1.1007998  -0.16628444 -0.05329444]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 55 ] state=[-0.23479761 -1.1007998  -0.16628444 -0.05329444], action=0, reward=1.0, next_state=[-0.2568136  -1.29319548 -0.16735032  0.18265575]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 56 ] state=[-0.2568136  -1.29319548 -0.16735032  0.18265575], action=0, reward=1.0, next_state=[-0.28267751 -1.4855768  -0.16369721  0.41822459]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 57 ] state=[-0.28267751 -1.4855768  -0.16369721  0.41822459], action=0, reward=1.0, next_state=[-0.31238905 -1.6780466  -0.15533272  0.65515741]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 58 ] state=[-0.31238905 -1.6780466  -0.15533272  0.65515741], action=0, reward=1.0, next_state=[-0.34594998 -1.87070389 -0.14222957  0.8951796 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 59 ] state=[-0.34594998 -1.87070389 -0.14222957  0.8951796 ], action=0, reward=1.0, next_state=[-0.38336406 -2.06364062 -0.12432598  1.13998775]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 60 ] state=[-0.38336406 -2.06364062 -0.12432598  1.13998775], action=1, reward=1.0, next_state=[-0.42463687 -1.8671323  -0.10152622  0.81104265]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 61 ] state=[-0.42463687 -1.8671323  -0.10152622  0.81104265], action=1, reward=1.0, next_state=[-0.46197952 -1.67077692 -0.08530537  0.48822879]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 62 ] state=[-0.46197952 -1.67077692 -0.08530537  0.48822879], action=1, reward=1.0, next_state=[-0.49539505 -1.47456153 -0.07554079  0.16992658]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 63 ] state=[-0.49539505 -1.47456153 -0.07554079  0.16992658], action=1, reward=1.0, next_state=[-0.52488628 -1.27844419 -0.07214226 -0.14559837]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 64 ] state=[-0.52488628 -1.27844419 -0.07214226 -0.14559837], action=1, reward=1.0, next_state=[-0.55045517 -1.08236723 -0.07505423 -0.46014021]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 65 ] state=[-0.55045517 -1.08236723 -0.07505423 -0.46014021], action=1, reward=1.0, next_state=[-0.57210251 -0.8862691  -0.08425703 -0.77550455]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 66 ] state=[-0.57210251 -0.8862691  -0.08425703 -0.77550455], action=0, reward=1.0, next_state=[-0.58982789 -1.0801373  -0.09976712 -0.51047614]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 67 ] state=[-0.58982789 -1.0801373  -0.09976712 -0.51047614], action=1, reward=1.0, next_state=[-0.61143064 -0.88376189 -0.10997665 -0.83285741]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 68 ] state=[-0.61143064 -0.88376189 -0.10997665 -0.83285741], action=0, reward=1.0, next_state=[-0.62910588 -1.07722305 -0.1266338  -0.57668681]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 69 ] state=[-0.62910588 -1.07722305 -0.1266338  -0.57668681], action=0, reward=1.0, next_state=[-0.65065034 -1.2703638  -0.13816753 -0.32642641]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 70 ] state=[-0.65065034 -1.2703638  -0.13816753 -0.32642641], action=0, reward=1.0, next_state=[-0.67605762 -1.46327591 -0.14469606 -0.08030805]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 71 ] state=[-0.67605762 -1.46327591 -0.14469606 -0.08030805], action=0, reward=1.0, next_state=[-0.70532313 -1.65605918 -0.14630222  0.16345256]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 72 ] state=[-0.70532313 -1.65605918 -0.14630222  0.16345256], action=0, reward=1.0, next_state=[-0.73844432 -1.84881681 -0.14303317  0.40664056]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 73 ] state=[-0.73844432 -1.84881681 -0.14303317  0.40664056], action=1, reward=1.0, next_state=[-0.77542065 -1.65198705 -0.13490036  0.0725024 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 74 ] state=[-0.77542065 -1.65198705 -0.13490036  0.0725024 ], action=1, reward=1.0, next_state=[-0.80846039 -1.45521494 -0.13345031 -0.2595147 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 75 ] state=[-0.80846039 -1.45521494 -0.13345031 -0.2595147 ], action=0, reward=1.0, next_state=[-0.83756469 -1.64820441 -0.1386406  -0.01172243]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 76 ] state=[-0.83756469 -1.64820441 -0.1386406  -0.01172243], action=0, reward=1.0, next_state=[-0.87052878 -1.8410939  -0.13887505  0.23420569]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 77 ] state=[-0.87052878 -1.8410939  -0.13887505  0.23420569], action=1, reward=1.0, next_state=[-0.90735066 -1.64428914 -0.13419094 -0.09885744]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 78 ] state=[-0.90735066 -1.64428914 -0.13419094 -0.09885744], action=0, reward=1.0, next_state=[-0.94023644 -1.83725816 -0.13616809  0.14866003]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 79 ] state=[-0.94023644 -1.83725816 -0.13616809  0.14866003], action=1, reward=1.0, next_state=[-0.97698161 -1.64047567 -0.13319489 -0.18369123]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 80 ] state=[-0.97698161 -1.64047567 -0.13319489 -0.18369123], action=0, reward=1.0, next_state=[-1.00979112 -1.83346525 -0.13686871  0.06418547]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 81 ] state=[-1.00979112 -1.83346525 -0.13686871  0.06418547], action=1, reward=1.0, next_state=[-1.04646042 -1.63667343 -0.135585   -0.26835557]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 82 ] state=[-1.04646042 -1.63667343 -0.135585   -0.26835557], action=1, reward=1.0, next_state=[-1.07919389 -1.4399033  -0.14095211 -0.60054194]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 83 ] state=[-1.07919389 -1.4399033  -0.14095211 -0.60054194], action=0, reward=1.0, next_state=[-1.10799196 -1.63280132 -0.15296295 -0.35536729]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 114 ][ timestamp 84 ] state=[-1.10799196 -1.63280132 -0.15296295 -0.35536729], action=1, reward=1.0, next_state=[-1.14064798 -1.43587301 -0.1600703  -0.69210668]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 85 ] state=[-1.14064798 -1.43587301 -0.1600703  -0.69210668], action=0, reward=1.0, next_state=[-1.16936544 -1.62845484 -0.17391243 -0.45378682]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 86 ] state=[-1.16936544 -1.62845484 -0.17391243 -0.45378682], action=1, reward=1.0, next_state=[-1.20193454 -1.4313553  -0.18298817 -0.79584925]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 87 ] state=[-1.20193454 -1.4313553  -0.18298817 -0.79584925], action=0, reward=1.0, next_state=[-1.23056165 -1.62355788 -0.19890515 -0.56585759]\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 88 ] state=[-1.23056165 -1.62355788 -0.19890515 -0.56585759], action=0, reward=-1.0, next_state=[-1.26303281 -1.8154155  -0.21022231 -0.34183859]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 114: Exploration_rate=0.01. Score=88.\n",
      "[ episode 115 ] state=[-0.0224107   0.02202674  0.02470541  0.02561601]\n",
      "[ episode 115 ][ timestamp 1 ] state=[-0.0224107   0.02202674  0.02470541  0.02561601], action=1, reward=1.0, next_state=[-0.02197017  0.21678585  0.02521773 -0.25917084]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 2 ] state=[-0.02197017  0.21678585  0.02521773 -0.25917084], action=1, reward=1.0, next_state=[-0.01763445  0.41153888  0.02003431 -0.54379429]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 3 ] state=[-0.01763445  0.41153888  0.02003431 -0.54379429], action=1, reward=1.0, next_state=[-0.00940367  0.60637365  0.00915842 -0.83009809]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 4 ] state=[-0.00940367  0.60637365  0.00915842 -0.83009809], action=1, reward=1.0, next_state=[ 0.0027238   0.80136922 -0.00744354 -1.11988665]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 5 ] state=[ 0.0027238   0.80136922 -0.00744354 -1.11988665], action=1, reward=1.0, next_state=[ 0.01875118  0.99658802 -0.02984127 -1.41489512]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 6 ] state=[ 0.01875118  0.99658802 -0.02984127 -1.41489512], action=0, reward=1.0, next_state=[ 0.03868294  0.80184819 -0.05813917 -1.13168746]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 7 ] state=[ 0.03868294  0.80184819 -0.05813917 -1.13168746], action=0, reward=1.0, next_state=[ 0.05471991  0.60753357 -0.08077292 -0.85779128]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 8 ] state=[ 0.05471991  0.60753357 -0.08077292 -0.85779128], action=0, reward=1.0, next_state=[ 0.06687058  0.41359945 -0.09792875 -0.59155997]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 9 ] state=[ 0.06687058  0.41359945 -0.09792875 -0.59155997], action=0, reward=1.0, next_state=[ 0.07514257  0.21997503 -0.10975995 -0.33125994]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 10 ] state=[ 0.07514257  0.21997503 -0.10975995 -0.33125994], action=0, reward=1.0, next_state=[ 0.07954207  0.02657266 -0.11638515 -0.07510677]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 11 ] state=[ 0.07954207  0.02657266 -0.11638515 -0.07510677], action=1, reward=1.0, next_state=[ 0.08007352  0.22315411 -0.11788728 -0.40212415]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 12 ] state=[ 0.08007352  0.22315411 -0.11788728 -0.40212415], action=0, reward=1.0, next_state=[ 0.08453661  0.02988429 -0.12592977 -0.14881018]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 13 ] state=[ 0.08453661  0.02988429 -0.12592977 -0.14881018], action=0, reward=1.0, next_state=[ 0.08513429 -0.16323049 -0.12890597  0.10164259]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 14 ] state=[ 0.08513429 -0.16323049 -0.12890597  0.10164259], action=0, reward=1.0, next_state=[ 0.08186968 -0.35629201 -0.12687312  0.35103868]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 15 ] state=[ 0.08186968 -0.35629201 -0.12687312  0.35103868], action=1, reward=1.0, next_state=[ 0.07474384 -0.1596156  -0.11985234  0.02119456]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 16 ] state=[ 0.07474384 -0.1596156  -0.11985234  0.02119456], action=0, reward=1.0, next_state=[ 0.07155153 -0.35283304 -0.11942845  0.27378931]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 17 ] state=[ 0.07155153 -0.35283304 -0.11942845  0.27378931], action=1, reward=1.0, next_state=[ 0.06449487 -0.15622748 -0.11395267 -0.05404693]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 18 ] state=[ 0.06449487 -0.15622748 -0.11395267 -0.05404693], action=1, reward=1.0, next_state=[ 0.06137032  0.04032834 -0.1150336  -0.38039811]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 19 ] state=[ 0.06137032  0.04032834 -0.1150336  -0.38039811], action=0, reward=1.0, next_state=[ 0.06217688 -0.15298823 -0.12264157 -0.12608506]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 20 ] state=[ 0.06217688 -0.15298823 -0.12264157 -0.12608506], action=0, reward=1.0, next_state=[ 0.05911712 -0.34615927 -0.12516327  0.12552882]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 21 ] state=[ 0.05911712 -0.34615927 -0.12516327  0.12552882], action=0, reward=1.0, next_state=[ 0.05219393 -0.53928658 -0.12265269  0.37625162]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 22 ] state=[ 0.05219393 -0.53928658 -0.12265269  0.37625162], action=0, reward=1.0, next_state=[ 0.0414082  -0.73247241 -0.11512766  0.62788388]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 23 ] state=[ 0.0414082  -0.73247241 -0.11512766  0.62788388], action=0, reward=1.0, next_state=[ 0.02675876 -0.92581531 -0.10256998  0.88220557]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 24 ] state=[ 0.02675876 -0.92581531 -0.10256998  0.88220557], action=0, reward=1.0, next_state=[ 0.00824245 -1.1194058  -0.08492587  1.14096239]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 25 ] state=[ 0.00824245 -1.1194058  -0.08492587  1.14096239], action=1, reward=1.0, next_state=[-0.01414567 -0.92328272 -0.06210662  0.82289981]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 26 ] state=[-0.01414567 -0.92328272 -0.06210662  0.82289981], action=1, reward=1.0, next_state=[-0.03261132 -0.72736858 -0.04564863  0.51134758]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 27 ] state=[-0.03261132 -0.72736858 -0.04564863  0.51134758], action=1, reward=1.0, next_state=[-0.04715869 -0.53163433 -0.03542167  0.20463603]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 28 ] state=[-0.04715869 -0.53163433 -0.03542167  0.20463603], action=0, reward=1.0, next_state=[-0.05779138 -0.7262323  -0.03132895  0.48593808]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 29 ] state=[-0.05779138 -0.7262323  -0.03132895  0.48593808], action=1, reward=1.0, next_state=[-0.07231603 -0.5306826  -0.02161019  0.18354826]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 30 ] state=[-0.07231603 -0.5306826  -0.02161019  0.18354826], action=1, reward=1.0, next_state=[-0.08292968 -0.33525821 -0.01793923 -0.11587279]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 31 ] state=[-0.08292968 -0.33525821 -0.01793923 -0.11587279], action=0, reward=1.0, next_state=[-0.08963484 -0.53011859 -0.02025668  0.1710969 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 32 ] state=[-0.08963484 -0.53011859 -0.02025668  0.1710969 ], action=0, reward=1.0, next_state=[-0.10023721 -0.72494484 -0.01683475  0.45732126]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 33 ] state=[-0.10023721 -0.72494484 -0.01683475  0.45732126], action=0, reward=1.0, next_state=[-0.11473611 -0.91982479 -0.00768832  0.74465059]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 34 ] state=[-0.11473611 -0.91982479 -0.00768832  0.74465059], action=0, reward=1.0, next_state=[-0.13313261 -1.1148398   0.00720469  1.03490412]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 35 ] state=[-0.13313261 -1.1148398   0.00720469  1.03490412], action=1, reward=1.0, next_state=[-0.1554294  -0.91981438  0.02790277  0.74449175]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 36 ] state=[-0.1554294  -0.91981438  0.02790277  0.74449175], action=0, reward=1.0, next_state=[-0.17382569 -1.11531007  0.04279261  1.04582349]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 37 ] state=[-0.17382569 -1.11531007  0.04279261  1.04582349], action=1, reward=1.0, next_state=[-0.19613189 -0.92078148  0.06370908  0.76687491]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 38 ] state=[-0.19613189 -0.92078148  0.06370908  0.76687491], action=1, reward=1.0, next_state=[-0.21454752 -0.72659181  0.07904658  0.49489914]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 39 ] state=[-0.21454752 -0.72659181  0.07904658  0.49489914], action=1, reward=1.0, next_state=[-0.22907936 -0.53266839  0.08894456  0.22813782]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 115 ][ timestamp 40 ] state=[-0.22907936 -0.53266839  0.08894456  0.22813782], action=1, reward=1.0, next_state=[-0.23973272 -0.33892267  0.09350732 -0.03521673]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 41 ] state=[-0.23973272 -0.33892267  0.09350732 -0.03521673], action=1, reward=1.0, next_state=[-0.24651118 -0.14525737  0.09280298 -0.29699449]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 42 ] state=[-0.24651118 -0.14525737  0.09280298 -0.29699449], action=0, reward=1.0, next_state=[-0.24941633 -0.34157129  0.08686309  0.02345418]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 43 ] state=[-0.24941633 -0.34157129  0.08686309  0.02345418], action=1, reward=1.0, next_state=[-0.25624775 -0.14779549  0.08733218 -0.240608  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 44 ] state=[-0.25624775 -0.14779549  0.08733218 -0.240608  ], action=1, reward=1.0, next_state=[-0.25920366  0.04597741  0.08252002 -0.5045166 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 45 ] state=[-0.25920366  0.04597741  0.08252002 -0.5045166 ], action=1, reward=1.0, next_state=[-0.25828411  0.23984536  0.07242968 -0.77009562]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 46 ] state=[-0.25828411  0.23984536  0.07242968 -0.77009562], action=0, reward=1.0, next_state=[-0.25348721  0.04380528  0.05702777 -0.45553077]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 47 ] state=[-0.25348721  0.04380528  0.05702777 -0.45553077], action=1, reward=1.0, next_state=[-0.2526111   0.2380765   0.04791716 -0.7297068 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 48 ] state=[-0.2526111   0.2380765   0.04791716 -0.7297068 ], action=0, reward=1.0, next_state=[-0.24784957  0.04232614  0.03332302 -0.42233604]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 49 ] state=[-0.24784957  0.04232614  0.03332302 -0.42233604], action=1, reward=1.0, next_state=[-0.24700305  0.23696053  0.0248763  -0.7043304 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 50 ] state=[-0.24700305  0.23696053  0.0248763  -0.7043304 ], action=1, reward=1.0, next_state=[-0.24226384  0.43172909  0.01078969 -0.98907996]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 51 ] state=[-0.24226384  0.43172909  0.01078969 -0.98907996], action=1, reward=1.0, next_state=[-0.23362926  0.62670495 -0.00899191 -1.27835462]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 52 ] state=[-0.23362926  0.62670495 -0.00899191 -1.27835462], action=1, reward=1.0, next_state=[-0.22109516  0.82194036 -0.034559   -1.57383948]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 53 ] state=[-0.22109516  0.82194036 -0.034559   -1.57383948], action=0, reward=1.0, next_state=[-0.20465635  0.62724715 -0.06603579 -1.29213237]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 54 ] state=[-0.20465635  0.62724715 -0.06603579 -1.29213237], action=0, reward=1.0, next_state=[-0.19211141  0.4330239  -0.09187844 -1.02083289]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 55 ] state=[-0.19211141  0.4330239  -0.09187844 -1.02083289], action=0, reward=1.0, next_state=[-0.18345093  0.23923829 -0.1122951  -0.7583548 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 56 ] state=[-0.18345093  0.23923829 -0.1122951  -0.7583548 ], action=0, reward=1.0, next_state=[-0.17866616  0.04582815 -0.12746219 -0.50301228]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 57 ] state=[-0.17866616  0.04582815 -0.12746219 -0.50301228], action=0, reward=1.0, next_state=[-0.1777496  -0.14728859 -0.13752244 -0.2530596 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 58 ] state=[-0.1777496  -0.14728859 -0.13752244 -0.2530596 ], action=1, reward=1.0, next_state=[-0.18069537  0.04950169 -0.14258363 -0.58576234]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 59 ] state=[-0.18069537  0.04950169 -0.14258363 -0.58576234], action=0, reward=1.0, next_state=[-0.17970534 -0.14336561 -0.15429888 -0.34117487]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 60 ] state=[-0.17970534 -0.14336561 -0.15429888 -0.34117487], action=0, reward=1.0, next_state=[-0.18257265 -0.33599419 -0.16112237 -0.10084886]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 61 ] state=[-0.18257265 -0.33599419 -0.16112237 -0.10084886], action=1, reward=1.0, next_state=[-0.18929253 -0.1389739  -0.16313935 -0.43971682]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 62 ] state=[-0.18929253 -0.1389739  -0.16313935 -0.43971682], action=0, reward=1.0, next_state=[-0.19207201 -0.3314566  -0.17193369 -0.20257688]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 63 ] state=[-0.19207201 -0.3314566  -0.17193369 -0.20257688], action=0, reward=1.0, next_state=[-0.19870114 -0.52375606 -0.17598522  0.03131953]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 64 ] state=[-0.19870114 -0.52375606 -0.17598522  0.03131953], action=0, reward=1.0, next_state=[-0.20917626 -0.71597497 -0.17535883  0.26372152]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 65 ] state=[-0.20917626 -0.71597497 -0.17535883  0.26372152], action=1, reward=1.0, next_state=[-0.22349576 -0.51884023 -0.1700844  -0.07873738]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 66 ] state=[-0.22349576 -0.51884023 -0.1700844  -0.07873738], action=0, reward=1.0, next_state=[-0.23387257 -0.71116752 -0.17165915  0.15582671]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 67 ] state=[-0.23387257 -0.71116752 -0.17165915  0.15582671], action=0, reward=1.0, next_state=[-0.24809592 -0.90346918 -0.16854262  0.38981944]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 68 ] state=[-0.24809592 -0.90346918 -0.16854262  0.38981944], action=0, reward=1.0, next_state=[-0.2661653  -1.09584828 -0.16074623  0.62498188]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 69 ] state=[-0.2661653  -1.09584828 -0.16074623  0.62498188], action=0, reward=1.0, next_state=[-0.28808227 -1.28840446 -0.14824659  0.8630364 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 70 ] state=[-0.28808227 -1.28840446 -0.14824659  0.8630364 ], action=1, reward=1.0, next_state=[-0.31385036 -1.09160893 -0.13098586  0.52765588]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 71 ] state=[-0.31385036 -1.09160893 -0.13098586  0.52765588], action=0, reward=1.0, next_state=[-0.33568254 -1.28466825 -0.12043274  0.77636432]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 72 ] state=[-0.33568254 -1.28466825 -0.12043274  0.77636432], action=1, reward=1.0, next_state=[-0.3613759  -1.08811401 -0.10490546  0.4483468 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 73 ] state=[-0.3613759  -1.08811401 -0.10490546  0.4483468 ], action=1, reward=1.0, next_state=[-0.38313818 -0.89167672 -0.09593852  0.12452507]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 74 ] state=[-0.38313818 -0.89167672 -0.09593852  0.12452507], action=0, reward=1.0, next_state=[-0.40097172 -1.08530264 -0.09344802  0.38546568]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 75 ] state=[-0.40097172 -1.08530264 -0.09344802  0.38546568], action=1, reward=1.0, next_state=[-0.42267777 -0.8889869  -0.08573871  0.06484313]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 76 ] state=[-0.42267777 -0.8889869  -0.08573871  0.06484313], action=1, reward=1.0, next_state=[-0.44045751 -0.69274699 -0.08444184 -0.25361176]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 77 ] state=[-0.44045751 -0.69274699 -0.08444184 -0.25361176], action=1, reward=1.0, next_state=[-0.45431245 -0.4965272  -0.08951408 -0.57168914]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 78 ] state=[-0.45431245 -0.4965272  -0.08951408 -0.57168914], action=0, reward=1.0, next_state=[-0.46424299 -0.69028744 -0.10094786 -0.30849443]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 79 ] state=[-0.46424299 -0.69028744 -0.10094786 -0.30849443], action=0, reward=1.0, next_state=[-0.47804874 -0.88383699 -0.10711775 -0.0492764 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 80 ] state=[-0.47804874 -0.88383699 -0.10711775 -0.0492764 ], action=0, reward=1.0, next_state=[-0.49572548 -1.07727291 -0.10810328  0.20778199]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 81 ] state=[-0.49572548 -1.07727291 -0.10810328  0.20778199], action=1, reward=1.0, next_state=[-0.51727094 -0.88078456 -0.10394764 -0.11695054]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 82 ] state=[-0.51727094 -0.88078456 -0.10394764 -0.11695054], action=0, reward=1.0, next_state=[-0.53488663 -1.07427542 -0.10628665  0.14121355]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 83 ] state=[-0.53488663 -1.07427542 -0.10628665  0.14121355], action=0, reward=1.0, next_state=[-0.55637214 -1.26772728 -0.10346238  0.39856435]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 115 ][ timestamp 84 ] state=[-0.55637214 -1.26772728 -0.10346238  0.39856435], action=1, reward=1.0, next_state=[-0.58172668 -1.0713014  -0.09549109  0.07513741]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 85 ] state=[-0.58172668 -1.0713014  -0.09549109  0.07513741], action=0, reward=1.0, next_state=[-0.60315271 -1.2649339  -0.09398834  0.3362312 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 86 ] state=[-0.60315271 -1.2649339  -0.09398834  0.3362312 ], action=0, reward=1.0, next_state=[-0.62845139 -1.45860138 -0.08726372  0.59785834]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 87 ] state=[-0.62845139 -1.45860138 -0.08726372  0.59785834], action=1, reward=1.0, next_state=[-0.65762342 -1.26237374 -0.07530655  0.27901387]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 88 ] state=[-0.65762342 -1.26237374 -0.07530655  0.27901387], action=0, reward=1.0, next_state=[-0.68287089 -1.45634514 -0.06972628  0.54702714]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 89 ] state=[-0.68287089 -1.45634514 -0.06972628  0.54702714], action=1, reward=1.0, next_state=[-0.71199779 -1.26031641 -0.05878573  0.23321562]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 90 ] state=[-0.71199779 -1.26031641 -0.05878573  0.23321562], action=1, reward=1.0, next_state=[-0.73720412 -1.06440592 -0.05412142 -0.07741554]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 91 ] state=[-0.73720412 -1.06440592 -0.05412142 -0.07741554], action=0, reward=1.0, next_state=[-0.75849224 -1.25871192 -0.05566973  0.19771277]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 92 ] state=[-0.75849224 -1.25871192 -0.05566973  0.19771277], action=0, reward=1.0, next_state=[-0.78366648 -1.45299525 -0.05171548  0.47232784]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 93 ] state=[-0.78366648 -1.45299525 -0.05171548  0.47232784], action=0, reward=1.0, next_state=[-0.81272638 -1.64735009 -0.04226892  0.74827278]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 94 ] state=[-0.81272638 -1.64735009 -0.04226892  0.74827278], action=0, reward=1.0, next_state=[-0.84567338 -1.84186424 -0.02730346  1.02736002]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 95 ] state=[-0.84567338 -1.84186424 -0.02730346  1.02736002], action=1, reward=1.0, next_state=[-0.88251067 -1.64638968 -0.00675626  0.72623125]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 96 ] state=[-0.88251067 -1.64638968 -0.00675626  0.72623125], action=0, reward=1.0, next_state=[-0.91543846 -1.84141757  0.00776836  1.01678007]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 97 ] state=[-0.91543846 -1.84141757  0.00776836  1.01678007], action=1, reward=1.0, next_state=[-0.95226681 -1.64640005  0.02810396  0.72654649]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 98 ] state=[-0.95226681 -1.64640005  0.02810396  0.72654649], action=1, reward=1.0, next_state=[-0.98519482 -1.45167771  0.04263489  0.44283981]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 99 ] state=[-0.98519482 -1.45167771  0.04263489  0.44283981], action=1, reward=1.0, next_state=[-1.01422837 -1.25718417  0.05149169  0.16389547]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 100 ] state=[-1.01422837 -1.25718417  0.05149169  0.16389547], action=1, reward=1.0, next_state=[-1.03937205 -1.0628357   0.0547696  -0.11210898]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 101 ] state=[-1.03937205 -1.0628357   0.0547696  -0.11210898], action=0, reward=1.0, next_state=[-1.06062877 -1.25869794  0.05252742  0.19733805]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 102 ] state=[-1.06062877 -1.25869794  0.05252742  0.19733805], action=1, reward=1.0, next_state=[-1.08580273 -1.06436513  0.05647418 -0.07832316]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 103 ] state=[-1.08580273 -1.06436513  0.05647418 -0.07832316], action=1, reward=1.0, next_state=[-1.10709003 -0.87009633  0.05490772 -0.35266721]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 104 ] state=[-1.10709003 -0.87009633  0.05490772 -0.35266721], action=0, reward=1.0, next_state=[-1.12449195 -1.06595433  0.04785437 -0.04318819]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 105 ] state=[-1.12449195 -1.06595433  0.04785437 -0.04318819], action=1, reward=1.0, next_state=[-1.14581104 -0.8715501   0.04699061 -0.32039689]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 106 ] state=[-1.14581104 -0.8715501   0.04699061 -0.32039689], action=0, reward=1.0, next_state=[-1.16324204 -1.06730866  0.04058267 -0.01327303]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 107 ] state=[-1.16324204 -1.06730866  0.04058267 -0.01327303], action=1, reward=1.0, next_state=[-1.18458822 -0.87279151  0.04031721 -0.29288049]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 108 ] state=[-1.18458822 -0.87279151  0.04031721 -0.29288049], action=0, reward=1.0, next_state=[-1.20204405 -1.06846441  0.0344596   0.01224039]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 109 ] state=[-1.20204405 -1.06846441  0.0344596   0.01224039], action=0, reward=1.0, next_state=[-1.22341333 -1.26406317  0.03470441  0.31559348]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 110 ] state=[-1.22341333 -1.26406317  0.03470441  0.31559348], action=1, reward=1.0, next_state=[-1.2486946  -1.06945231  0.04101628  0.034054  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 111 ] state=[-1.2486946  -1.06945231  0.04101628  0.034054  ], action=0, reward=1.0, next_state=[-1.27008364 -1.26513771  0.04169736  0.33939064]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 112 ] state=[-1.27008364 -1.26513771  0.04169736  0.33939064], action=1, reward=1.0, next_state=[-1.2953864  -1.07063311  0.04848517  0.06014281]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 113 ] state=[-1.2953864  -1.07063311  0.04848517  0.06014281], action=1, reward=1.0, next_state=[-1.31679906 -0.87623866  0.04968803 -0.21685714]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 114 ] state=[-1.31679906 -0.87623866  0.04968803 -0.21685714], action=0, reward=1.0, next_state=[-1.33432383 -1.07203442  0.04535088  0.09107629]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 115 ] state=[-1.33432383 -1.07203442  0.04535088  0.09107629], action=1, reward=1.0, next_state=[-1.35576452 -0.87759085  0.04717241 -0.18696059]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 116 ] state=[-1.35576452 -0.87759085  0.04717241 -0.18696059], action=0, reward=1.0, next_state=[-1.37331634 -1.07335486  0.0434332   0.12022232]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 117 ] state=[-1.37331634 -1.07335486  0.0434332   0.12022232], action=1, reward=1.0, next_state=[-1.39478344 -0.87888124  0.04583764 -0.15844767]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 118 ] state=[-1.39478344 -0.87888124  0.04583764 -0.15844767], action=0, reward=1.0, next_state=[-1.41236106 -1.07462845  0.04266869  0.14833629]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 119 ] state=[-1.41236106 -1.07462845  0.04266869  0.14833629], action=1, reward=1.0, next_state=[-1.43385363 -0.88014267  0.04563542 -0.13058607]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 120 ] state=[-1.43385363 -0.88014267  0.04563542 -0.13058607], action=0, reward=1.0, next_state=[-1.45145648 -1.07588762  0.04302369  0.17613783]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 121 ] state=[-1.45145648 -1.07588762  0.04302369  0.17613783], action=1, reward=1.0, next_state=[-1.47297424 -0.88140695  0.04654645 -0.10266816]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 122 ] state=[-1.47297424 -0.88140695  0.04654645 -0.10266816], action=1, reward=1.0, next_state=[-1.49060238 -0.6869819   0.04449309 -0.38031015]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 123 ] state=[-1.49060238 -0.6869819   0.04449309 -0.38031015], action=0, reward=1.0, next_state=[-1.50434201 -0.8827065   0.03688689 -0.07393715]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 124 ] state=[-1.50434201 -0.8827065   0.03688689 -0.07393715], action=1, reward=1.0, next_state=[-1.52199614 -0.68813225  0.03540814 -0.35475771]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 115 ][ timestamp 125 ] state=[-1.52199614 -0.68813225  0.03540814 -0.35475771], action=0, reward=1.0, next_state=[-1.53575879 -0.8837393   0.02831299 -0.05112323]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 126 ] state=[-1.53575879 -0.8837393   0.02831299 -0.05112323], action=1, reward=1.0, next_state=[-1.55343357 -0.68903452  0.02729052 -0.33474043]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 127 ] state=[-1.55343357 -0.68903452  0.02729052 -0.33474043], action=0, reward=1.0, next_state=[-1.56721426 -0.88453403  0.02059572 -0.03357794]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 128 ] state=[-1.56721426 -0.88453403  0.02059572 -0.03357794], action=1, reward=1.0, next_state=[-1.58490495 -0.68971339  0.01992416 -0.31969221]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 129 ] state=[-1.58490495 -0.68971339  0.01992416 -0.31969221], action=1, reward=1.0, next_state=[-1.59869921 -0.49488078  0.01353031 -0.60602581]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 130 ] state=[-1.59869921 -0.49488078  0.01353031 -0.60602581], action=0, reward=1.0, next_state=[-1.60859683e+00 -6.90189287e-01  1.40979614e-03 -3.09112072e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 131 ] state=[-1.60859683e+00 -6.90189287e-01  1.40979614e-03 -3.09112072e-01], action=1, reward=1.0, next_state=[-1.62240061 -0.49508745 -0.00477245 -0.60135005]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 132 ] state=[-1.62240061 -0.49508745 -0.00477245 -0.60135005], action=0, reward=1.0, next_state=[-1.63230236 -0.69014232 -0.01679945 -0.31017418]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 133 ] state=[-1.63230236 -0.69014232 -0.01679945 -0.31017418], action=1, reward=1.0, next_state=[-1.64610521 -0.49478509 -0.02300293 -0.60810747]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 134 ] state=[-1.64610521 -0.49478509 -0.02300293 -0.60810747], action=0, reward=1.0, next_state=[-1.65600091 -0.68957802 -0.03516508 -0.32275764]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 135 ] state=[-1.65600091 -0.68957802 -0.03516508 -0.32275764], action=0, reward=1.0, next_state=[-1.66979247 -0.88418204 -0.04162023 -0.04136848]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 136 ] state=[-1.66979247 -0.88418204 -0.04162023 -0.04136848], action=1, reward=1.0, next_state=[-1.68747611 -0.68848875 -0.0424476  -0.34688704]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 137 ] state=[-1.68747611 -0.68848875 -0.0424476  -0.34688704], action=0, reward=1.0, next_state=[-1.70124589 -0.88298202 -0.04938534 -0.06788576]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 138 ] state=[-1.70124589 -0.88298202 -0.04938534 -0.06788576], action=1, reward=1.0, next_state=[-1.71890553 -0.6871881  -0.05074306 -0.37573196]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 139 ] state=[-1.71890553 -0.6871881  -0.05074306 -0.37573196], action=1, reward=1.0, next_state=[-1.73264929 -0.49138351 -0.0582577  -0.68397286]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 140 ] state=[-1.73264929 -0.49138351 -0.0582577  -0.68397286], action=0, reward=1.0, next_state=[-1.74247696 -0.68565024 -0.07193715 -0.41018521]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 141 ] state=[-1.74247696 -0.68565024 -0.07193715 -0.41018521], action=0, reward=1.0, next_state=[-1.75618997 -0.87968251 -0.08014086 -0.14102083]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 142 ] state=[-1.75618997 -0.87968251 -0.08014086 -0.14102083], action=1, reward=1.0, next_state=[-1.77378362 -0.68350969 -0.08296127 -0.45787183]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 143 ] state=[-1.77378362 -0.68350969 -0.08296127 -0.45787183], action=0, reward=1.0, next_state=[-1.78745381 -0.87736685 -0.09211871 -0.19244884]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 144 ] state=[-1.78745381 -0.87736685 -0.09211871 -0.19244884], action=1, reward=1.0, next_state=[-1.80500115 -0.68105611 -0.09596769 -0.51271104]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 145 ] state=[-1.80500115 -0.68105611 -0.09596769 -0.51271104], action=0, reward=1.0, next_state=[-1.81862227 -0.87470466 -0.10622191 -0.251746  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 146 ] state=[-1.81862227 -0.87470466 -0.10622191 -0.251746  ], action=1, reward=1.0, next_state=[-1.83611636 -0.67823891 -0.11125683 -0.57595418]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 147 ] state=[-1.83611636 -0.67823891 -0.11125683 -0.57595418], action=0, reward=1.0, next_state=[-1.84968114 -0.8716399  -0.12277591 -0.32028837]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 148 ] state=[-1.84968114 -0.8716399  -0.12277591 -0.32028837], action=1, reward=1.0, next_state=[-1.86711394 -0.67500286 -0.12918168 -0.64902916]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 149 ] state=[-1.86711394 -0.67500286 -0.12918168 -0.64902916], action=0, reward=1.0, next_state=[-1.880614   -0.86811115 -0.14216226 -0.39965416]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 150 ] state=[-1.880614   -0.86811115 -0.14216226 -0.39965416], action=1, reward=1.0, next_state=[-1.89797622 -0.67128892 -0.15015535 -0.73356423]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 151 ] state=[-1.89797622 -0.67128892 -0.15015535 -0.73356423], action=0, reward=1.0, next_state=[-1.911402   -0.86405255 -0.16482663 -0.49165226]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 152 ] state=[-1.911402   -0.86405255 -0.16482663 -0.49165226], action=0, reward=1.0, next_state=[-1.92868305 -1.05651279 -0.17465968 -0.25511449]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 153 ] state=[-1.92868305 -1.05651279 -0.17465968 -0.25511449], action=1, reward=1.0, next_state=[-1.9498133  -0.85938319 -0.17976197 -0.59739937]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 154 ] state=[-1.9498133  -0.85938319 -0.17976197 -0.59739937], action=0, reward=1.0, next_state=[-1.96700097 -1.05159493 -0.19170995 -0.36629348]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 155 ] state=[-1.96700097 -1.05159493 -0.19170995 -0.36629348], action=1, reward=1.0, next_state=[-1.98803287 -0.85433938 -0.19903582 -0.71277429]\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 156 ] state=[-1.98803287 -0.85433938 -0.19903582 -0.71277429], action=0, reward=-1.0, next_state=[-2.00511965 -1.04623074 -0.21329131 -0.48875076]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 115: Exploration_rate=0.01. Score=156.\n",
      "[ episode 116 ] state=[ 0.02401809 -0.02863897  0.00505457 -0.04299058]\n",
      "[ episode 116 ][ timestamp 1 ] state=[ 0.02401809 -0.02863897  0.00505457 -0.04299058], action=0, reward=1.0, next_state=[ 0.02344531 -0.22383303  0.00419476  0.25128281]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 2 ] state=[ 0.02344531 -0.22383303  0.00419476  0.25128281], action=1, reward=1.0, next_state=[ 0.01896865 -0.02877123  0.00922042 -0.04007406]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 3 ] state=[ 0.01896865 -0.02877123  0.00922042 -0.04007406], action=1, reward=1.0, next_state=[ 0.01839322  0.16621729  0.00841893 -0.32983365]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 4 ] state=[ 0.01839322  0.16621729  0.00841893 -0.32983365], action=0, reward=1.0, next_state=[ 0.02171757 -0.02902349  0.00182226 -0.03450772]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 5 ] state=[ 0.02171757 -0.02902349  0.00182226 -0.03450772], action=0, reward=1.0, next_state=[ 0.0211371  -0.22417152  0.00113211  0.25874959]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 6 ] state=[ 0.0211371  -0.22417152  0.00113211  0.25874959], action=1, reward=1.0, next_state=[ 0.01665367 -0.02906575  0.0063071  -0.03357604]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 7 ] state=[ 0.01665367 -0.02906575  0.0063071  -0.03357604], action=0, reward=1.0, next_state=[ 0.01607236 -0.22427758  0.00563558  0.26109015]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 8 ] state=[ 0.01607236 -0.22427758  0.00563558  0.26109015], action=1, reward=1.0, next_state=[ 0.0115868  -0.02923653  0.01085738 -0.02980993]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 9 ] state=[ 0.0115868  -0.02923653  0.01085738 -0.02980993], action=0, reward=1.0, next_state=[ 0.01100207 -0.22451248  0.01026118  0.26627875]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 10 ] state=[ 0.01100207 -0.22451248  0.01026118  0.26627875], action=1, reward=1.0, next_state=[ 0.00651182 -0.02953847  0.01558676 -0.02315013]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 116 ][ timestamp 11 ] state=[ 0.00651182 -0.02953847  0.01558676 -0.02315013], action=0, reward=1.0, next_state=[ 0.00592105 -0.22488044  0.01512376  0.27440955]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 12 ] state=[ 0.00592105 -0.22488044  0.01512376  0.27440955], action=1, reward=1.0, next_state=[ 0.00142345 -0.02997751  0.02061195 -0.0134652 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 13 ] state=[ 0.00142345 -0.02997751  0.02061195 -0.0134652 ], action=1, reward=1.0, next_state=[ 0.0008239   0.16484287  0.02034264 -0.29957421]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 14 ] state=[ 0.0008239   0.16484287  0.02034264 -0.29957421], action=1, reward=1.0, next_state=[ 0.00412075  0.35966904  0.01435116 -0.58577268]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 15 ] state=[ 0.00412075  0.35966904  0.01435116 -0.58577268], action=0, reward=1.0, next_state=[ 0.01131413  0.16434905  0.0026357  -0.28860376]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 16 ] state=[ 0.01131413  0.16434905  0.0026357  -0.28860376], action=0, reward=1.0, next_state=[ 0.01460111 -0.03081039 -0.00313637  0.00490927]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 17 ] state=[ 0.01460111 -0.03081039 -0.00313637  0.00490927], action=1, reward=1.0, next_state=[ 0.01398491  0.1643564  -0.00303819 -0.28876157]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 18 ] state=[ 0.01398491  0.1643564  -0.00303819 -0.28876157], action=1, reward=1.0, next_state=[ 0.01727203  0.35952155 -0.00881342 -0.58240116]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 19 ] state=[ 0.01727203  0.35952155 -0.00881342 -0.58240116], action=1, reward=1.0, next_state=[ 0.02446247  0.55476586 -0.02046144 -0.87784737]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 20 ] state=[ 0.02446247  0.55476586 -0.02046144 -0.87784737], action=1, reward=1.0, next_state=[ 0.03555778  0.75015981 -0.03801839 -1.17689218]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 21 ] state=[ 0.03555778  0.75015981 -0.03801839 -1.17689218], action=0, reward=1.0, next_state=[ 0.05056098  0.55555178 -0.06155623 -0.89636579]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 22 ] state=[ 0.05056098  0.55555178 -0.06155623 -0.89636579], action=0, reward=1.0, next_state=[ 0.06167201  0.36131597 -0.07948355 -0.62365   ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 23 ] state=[ 0.06167201  0.36131597 -0.07948355 -0.62365   ], action=0, reward=1.0, next_state=[ 0.06889833  0.16738847 -0.09195655 -0.3570207 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 24 ] state=[ 0.06889833  0.16738847 -0.09195655 -0.3570207 ], action=0, reward=1.0, next_state=[ 0.0722461  -0.02631402 -0.09909696 -0.0946917 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 25 ] state=[ 0.0722461  -0.02631402 -0.09909696 -0.0946917 ], action=0, reward=1.0, next_state=[ 0.07171982 -0.21988629 -0.10099079  0.16515534]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 26 ] state=[ 0.07171982 -0.21988629 -0.10099079  0.16515534], action=0, reward=1.0, next_state=[ 0.0673221  -0.41342845 -0.09768769  0.42434851]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 27 ] state=[ 0.0673221  -0.41342845 -0.09768769  0.42434851], action=0, reward=1.0, next_state=[ 0.05905353 -0.60704069 -0.08920072  0.68470773]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 28 ] state=[ 0.05905353 -0.60704069 -0.08920072  0.68470773], action=0, reward=1.0, next_state=[ 0.04691271 -0.80081837 -0.07550656  0.94802839]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 29 ] state=[ 0.04691271 -0.80081837 -0.07550656  0.94802839], action=1, reward=1.0, next_state=[ 0.03089635 -0.60476543 -0.056546    0.63260905]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 30 ] state=[ 0.03089635 -0.60476543 -0.056546    0.63260905], action=0, reward=1.0, next_state=[ 0.01880104 -0.79905482 -0.04389381  0.90696166]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 31 ] state=[ 0.01880104 -0.79905482 -0.04389381  0.90696166], action=1, reward=1.0, next_state=[ 0.00281994 -0.60336695 -0.02575458  0.60081196]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 32 ] state=[ 0.00281994 -0.60336695 -0.02575458  0.60081196], action=0, reward=1.0, next_state=[-0.0092474  -0.79811933 -0.01373834  0.88527263]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 33 ] state=[-0.0092474  -0.79811933 -0.01373834  0.88527263], action=0, reward=1.0, next_state=[-0.02520978 -0.99305209  0.00396711  1.17360523]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 34 ] state=[-0.02520978 -0.99305209  0.00396711  1.17360523], action=1, reward=1.0, next_state=[-0.04507083 -0.79798193  0.02743922  0.88216862]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 35 ] state=[-0.04507083 -0.79798193  0.02743922  0.88216862], action=1, reward=1.0, next_state=[-0.06103046 -0.60324319  0.04508259  0.59823659]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 36 ] state=[-0.06103046 -0.60324319  0.04508259  0.59823659], action=0, reward=1.0, next_state=[-0.07309533 -0.79896599  0.05704732  0.90477229]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 37 ] state=[-0.07309533 -0.79896599  0.05704732  0.90477229], action=1, reward=1.0, next_state=[-0.08907465 -0.60466109  0.07514277  0.63055189]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 38 ] state=[-0.08907465 -0.60466109  0.07514277  0.63055189], action=1, reward=1.0, next_state=[-0.10116787 -0.41066363  0.0877538   0.36244804]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 39 ] state=[-0.10116787 -0.41066363  0.0877538   0.36244804], action=1, reward=1.0, next_state=[-0.10938114 -0.21689142  0.09500276  0.09867466]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 40 ] state=[-0.10938114 -0.21689142  0.09500276  0.09867466], action=1, reward=1.0, next_state=[-0.11371897 -0.02325036  0.09697626 -0.16258832]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 41 ] state=[-0.11371897 -0.02325036  0.09697626 -0.16258832], action=1, reward=1.0, next_state=[-0.11418398  0.1703592   0.09372449 -0.42317179]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 42 ] state=[-0.11418398  0.1703592   0.09372449 -0.42317179], action=1, reward=1.0, next_state=[-0.11077679  0.36403709  0.08526105 -0.68489889]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 43 ] state=[-0.11077679  0.36403709  0.08526105 -0.68489889], action=1, reward=1.0, next_state=[-0.10349605  0.55787831  0.07156308 -0.94956812]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 44 ] state=[-0.10349605  0.55787831  0.07156308 -0.94956812], action=0, reward=1.0, next_state=[-0.09233849  0.36186976  0.05257171 -0.63528625]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 45 ] state=[-0.09233849  0.36186976  0.05257171 -0.63528625], action=0, reward=1.0, next_state=[-0.08510109  0.1660555   0.03986599 -0.32652169]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 46 ] state=[-0.08510109  0.1660555   0.03986599 -0.32652169], action=0, reward=1.0, next_state=[-0.08177998 -0.0296107   0.03333556 -0.0215381 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 47 ] state=[-0.08177998 -0.0296107   0.03333556 -0.0215381 ], action=1, reward=1.0, next_state=[-0.08237219  0.16501772  0.03290479 -0.30351969]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 48 ] state=[-0.08237219  0.16501772  0.03290479 -0.30351969], action=1, reward=1.0, next_state=[-0.07907184  0.35965566  0.0268344  -0.58564628]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 49 ] state=[-0.07907184  0.35965566  0.0268344  -0.58564628], action=1, reward=1.0, next_state=[-0.07187873  0.55439167  0.01512147 -0.86975677]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 50 ] state=[-0.07187873  0.55439167  0.01512147 -0.86975677], action=0, reward=1.0, next_state=[-0.06079089  0.35906731 -0.00227366 -0.57235819]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 51 ] state=[-0.06079089  0.35906731 -0.00227366 -0.57235819], action=0, reward=1.0, next_state=[-0.05360955  0.16397732 -0.01372082 -0.28039241]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 52 ] state=[-0.05360955  0.16397732 -0.01372082 -0.28039241], action=0, reward=1.0, next_state=[-0.05033    -0.03094625 -0.01932867  0.00793162]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 53 ] state=[-0.05033    -0.03094625 -0.01932867  0.00793162], action=0, reward=1.0, next_state=[-0.05094893 -0.22578575 -0.01917004  0.294454  ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 116 ][ timestamp 54 ] state=[-0.05094893 -0.22578575 -0.01917004  0.294454  ], action=1, reward=1.0, next_state=[-0.05546464 -0.03039581 -0.01328096 -0.0042127 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 55 ] state=[-0.05546464 -0.03039581 -0.01328096 -0.0042127 ], action=0, reward=1.0, next_state=[-0.05607256 -0.2253248  -0.01336521  0.28425051]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 56 ] state=[-0.05607256 -0.2253248  -0.01336521  0.28425051], action=1, reward=1.0, next_state=[-0.06057905 -0.0300148  -0.0076802  -0.01261758]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 57 ] state=[-0.06057905 -0.0300148  -0.0076802  -0.01261758], action=0, reward=1.0, next_state=[-0.06117935 -0.22502577 -0.00793256  0.27763229]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 58 ] state=[-0.06117935 -0.22502577 -0.00793256  0.27763229], action=1, reward=1.0, next_state=[-0.06567986 -0.02979156 -0.00237991 -0.01754197]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 59 ] state=[-0.06567986 -0.02979156 -0.00237991 -0.01754197], action=0, reward=1.0, next_state=[-0.0662757  -0.2248793  -0.00273075  0.27438912]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 60 ] state=[-0.0662757  -0.2248793  -0.00273075  0.27438912], action=1, reward=1.0, next_state=[-0.07077328 -0.02971849  0.00275703 -0.01915384]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 61 ] state=[-0.07077328 -0.02971849  0.00275703 -0.01915384], action=0, reward=1.0, next_state=[-0.07136765 -0.22487987  0.00237396  0.27439769]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 62 ] state=[-0.07136765 -0.22487987  0.00237396  0.27439769], action=0, reward=1.0, next_state=[-0.07586525 -0.42003561  0.00786191  0.56782842]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 63 ] state=[-0.07586525 -0.42003561  0.00786191  0.56782842], action=1, reward=1.0, next_state=[-0.08426596 -0.22502482  0.01921848  0.27763265]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 64 ] state=[-0.08426596 -0.22502482  0.01921848  0.27763265], action=0, reward=1.0, next_state=[-0.08876646 -0.4204156   0.02477113  0.57631458]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 65 ] state=[-0.08876646 -0.4204156   0.02477113  0.57631458], action=0, reward=1.0, next_state=[-0.09717477 -0.61587586  0.03629742  0.87669699]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 66 ] state=[-0.09717477 -0.61587586  0.03629742  0.87669699], action=1, reward=1.0, next_state=[-0.10949229 -0.42126555  0.05383136  0.5956429 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 67 ] state=[-0.10949229 -0.42126555  0.05383136  0.5956429 ], action=0, reward=1.0, next_state=[-0.1179176  -0.61709792  0.06574422  0.90478473]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 68 ] state=[-0.1179176  -0.61709792  0.06574422  0.90478473], action=1, reward=1.0, next_state=[-0.13025956 -0.42292497  0.08383992  0.63346942]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 69 ] state=[-0.13025956 -0.42292497  0.08383992  0.63346942], action=1, reward=1.0, next_state=[-0.13871805 -0.22906647  0.0965093   0.36832313]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 70 ] state=[-0.13871805 -0.22906647  0.0965093   0.36832313], action=1, reward=1.0, next_state=[-0.14329938 -0.03543883  0.10387577  0.10756291]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 71 ] state=[-0.14329938 -0.03543883  0.10387577  0.10756291], action=1, reward=1.0, next_state=[-0.14400816  0.15805304  0.10602702 -0.15062585]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 72 ] state=[-0.14400816  0.15805304  0.10602702 -0.15062585], action=1, reward=1.0, next_state=[-0.1408471   0.35150959  0.10301451 -0.40806754]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 73 ] state=[-0.1408471   0.35150959  0.10301451 -0.40806754], action=1, reward=1.0, next_state=[-0.13381691  0.54503156  0.09485316 -0.66657889]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 74 ] state=[-0.13381691  0.54503156  0.09485316 -0.66657889], action=0, reward=1.0, next_state=[-0.12291628  0.34872732  0.08152158 -0.34560114]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 75 ] state=[-0.12291628  0.34872732  0.08152158 -0.34560114], action=1, reward=1.0, next_state=[-0.11594173  0.54260073  0.07460956 -0.61150465]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 76 ] state=[-0.11594173  0.54260073  0.07460956 -0.61150465], action=0, reward=1.0, next_state=[-0.10508972  0.34651964  0.06237946 -0.29628641]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 77 ] state=[-0.10508972  0.34651964  0.06237946 -0.29628641], action=0, reward=1.0, next_state=[-0.09815932  0.15056646  0.05645374  0.01539936]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 78 ] state=[-0.09815932  0.15056646  0.05645374  0.01539936], action=1, reward=1.0, next_state=[-0.09514799  0.34483526  0.05676172 -0.25895103]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 79 ] state=[-0.09514799  0.34483526  0.05676172 -0.25895103], action=0, reward=1.0, next_state=[-0.08825129  0.14895085  0.0515827   0.05108136]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 80 ] state=[-0.08825129  0.14895085  0.0515827   0.05108136], action=0, reward=1.0, next_state=[-0.08527227 -0.04687134  0.05260433  0.35958254]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 81 ] state=[-0.08527227 -0.04687134  0.05260433  0.35958254], action=1, reward=1.0, next_state=[-0.0862097   0.1474649   0.05979598  0.08393996]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 82 ] state=[-0.0862097   0.1474649   0.05979598  0.08393996], action=1, reward=1.0, next_state=[-0.0832604   0.34168099  0.06147478 -0.18929397]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 83 ] state=[-0.0832604   0.34168099  0.06147478 -0.18929397], action=1, reward=1.0, next_state=[-0.07642678  0.53587204  0.0576889  -0.4619681 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 84 ] state=[-0.07642678  0.53587204  0.0576889  -0.4619681 ], action=1, reward=1.0, next_state=[-0.06570934  0.73013323  0.04844954 -0.73592402]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 85 ] state=[-0.06570934  0.73013323  0.04844954 -0.73592402], action=1, reward=1.0, next_state=[-0.05110668  0.92455366  0.03373106 -1.01297385]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 86 ] state=[-0.05110668  0.92455366  0.03373106 -1.01297385], action=0, reward=1.0, next_state=[-0.0326156   0.72899836  0.01347158 -0.70989271]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 87 ] state=[-0.0326156   0.72899836  0.01347158 -0.70989271], action=0, reward=1.0, next_state=[-0.01803564  0.53369244 -0.00072627 -0.41299989]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 88 ] state=[-0.01803564  0.53369244 -0.00072627 -0.41299989], action=0, reward=1.0, next_state=[-0.00736179  0.33858079 -0.00898627 -0.12054602]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 89 ] state=[-0.00736179  0.33858079 -0.00898627 -0.12054602], action=0, reward=1.0, next_state=[-0.00059017  0.14358874 -0.01139719  0.16928833]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 90 ] state=[-0.00059017  0.14358874 -0.01139719  0.16928833], action=0, reward=1.0, next_state=[ 0.0022816  -0.05136824 -0.00801143  0.4583541 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 91 ] state=[ 0.0022816  -0.05136824 -0.00801143  0.4583541 ], action=1, reward=1.0, next_state=[0.00125424 0.14386604 0.00115566 0.16315674]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 92 ] state=[0.00125424 0.14386604 0.00115566 0.16315674], action=1, reward=1.0, next_state=[ 0.00413156  0.33897143  0.00441879 -0.12916139]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 93 ] state=[ 0.00413156  0.33897143  0.00441879 -0.12916139], action=1, reward=1.0, next_state=[ 0.01091099  0.5340298   0.00183556 -0.42044697]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 94 ] state=[ 0.01091099  0.5340298   0.00183556 -0.42044697], action=1, reward=1.0, next_state=[ 0.02159158  0.7291257  -0.00657338 -0.71255066]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 116 ][ timestamp 95 ] state=[ 0.02159158  0.7291257  -0.00657338 -0.71255066], action=0, reward=1.0, next_state=[ 0.0361741   0.53409538 -0.02082439 -0.42194406]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 96 ] state=[ 0.0361741   0.53409538 -0.02082439 -0.42194406], action=0, reward=1.0, next_state=[ 0.04685601  0.33927455 -0.02926327 -0.13589812]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 97 ] state=[ 0.04685601  0.33927455 -0.02926327 -0.13589812], action=0, reward=1.0, next_state=[ 0.0536415   0.14458371 -0.03198123  0.14741094]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 98 ] state=[ 0.0536415   0.14458371 -0.03198123  0.14741094], action=1, reward=1.0, next_state=[ 0.05653317  0.3401487  -0.02903301 -0.15518741]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 99 ] state=[ 0.05653317  0.3401487  -0.02903301 -0.15518741], action=1, reward=1.0, next_state=[ 0.06333615  0.53567405 -0.03213676 -0.45688635]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 100 ] state=[ 0.06333615  0.53567405 -0.03213676 -0.45688635], action=1, reward=1.0, next_state=[ 0.07404963  0.73123526 -0.04127449 -0.75952328]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 101 ] state=[ 0.07404963  0.73123526 -0.04127449 -0.75952328], action=1, reward=1.0, next_state=[ 0.08867433  0.92690087 -0.05646495 -1.06490299]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 102 ] state=[ 0.08867433  0.92690087 -0.05646495 -1.06490299], action=1, reward=1.0, next_state=[ 0.10721235  1.12272282 -0.07776301 -1.37475967]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 103 ] state=[ 0.10721235  1.12272282 -0.07776301 -1.37475967], action=0, reward=1.0, next_state=[ 0.12966681  0.92865407 -0.10525821 -1.10737555]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 104 ] state=[ 0.12966681  0.92865407 -0.10525821 -1.10737555], action=0, reward=1.0, next_state=[ 0.14823989  0.73506122 -0.12740572 -0.84948224]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 105 ] state=[ 0.14823989  0.73506122 -0.12740572 -0.84948224], action=1, reward=1.0, next_state=[ 0.16294111  0.93166894 -0.14439536 -1.17935955]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 106 ] state=[ 0.16294111  0.93166894 -0.14439536 -1.17935955], action=0, reward=1.0, next_state=[ 0.18157449  0.73868628 -0.16798255 -0.93520296]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 107 ] state=[ 0.18157449  0.73868628 -0.16798255 -0.93520296], action=0, reward=1.0, next_state=[ 0.19634822  0.54617947 -0.18668661 -0.69966224]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 108 ] state=[ 0.19634822  0.54617947 -0.18668661 -0.69966224], action=0, reward=1.0, next_state=[ 0.20727181  0.35406871 -0.20067986 -0.4710707 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 109 ] state=[ 0.20727181  0.35406871 -0.20067986 -0.4710707 ], action=0, reward=-1.0, next_state=[ 0.21435318  0.16226265 -0.21010127 -0.24774024]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 116: Exploration_rate=0.01. Score=109.\n",
      "[ episode 117 ] state=[-0.00371274  0.04786056 -0.02399354  0.00857516]\n",
      "[ episode 117 ][ timestamp 1 ] state=[-0.00371274  0.04786056 -0.02399354  0.00857516], action=1, reward=1.0, next_state=[-0.00275553  0.24331825 -0.02382204 -0.29158041]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 2 ] state=[-0.00275553  0.24331825 -0.02382204 -0.29158041], action=0, reward=1.0, next_state=[ 0.00211083  0.04854392 -0.02965365 -0.00650483]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 3 ] state=[ 0.00211083  0.04854392 -0.02965365 -0.00650483], action=0, reward=1.0, next_state=[ 0.00308171 -0.14614048 -0.02978375  0.27667649]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 4 ] state=[ 0.00308171 -0.14614048 -0.02978375  0.27667649], action=1, reward=1.0, next_state=[ 0.0001589   0.04939345 -0.02425022 -0.02524945]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 5 ] state=[ 0.0001589   0.04939345 -0.02425022 -0.02524945], action=1, reward=1.0, next_state=[ 0.00114677  0.24485462 -0.02475521 -0.32548387]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 6 ] state=[ 0.00114677  0.24485462 -0.02475521 -0.32548387], action=1, reward=1.0, next_state=[ 0.00604386  0.44032013 -0.03126488 -0.62586959]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 7 ] state=[ 0.00604386  0.44032013 -0.03126488 -0.62586959], action=0, reward=1.0, next_state=[ 0.01485027  0.24564825 -0.04378227 -0.34319485]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 8 ] state=[ 0.01485027  0.24564825 -0.04378227 -0.34319485], action=0, reward=1.0, next_state=[ 0.01976323  0.05117562 -0.05064617 -0.06463333]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 9 ] state=[ 0.01976323  0.05117562 -0.05064617 -0.06463333], action=0, reward=1.0, next_state=[ 0.02078674 -0.14318499 -0.05193884  0.21165015]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 10 ] state=[ 0.02078674 -0.14318499 -0.05193884  0.21165015], action=1, reward=1.0, next_state=[ 0.01792304  0.05263962 -0.04770584 -0.0969538 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 11 ] state=[ 0.01792304  0.05263962 -0.04770584 -0.0969538 ], action=1, reward=1.0, next_state=[ 0.01897584  0.24841169 -0.04964491 -0.40429801]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 12 ] state=[ 0.01897584  0.24841169 -0.04964491 -0.40429801], action=0, reward=1.0, next_state=[ 0.02394407  0.05402766 -0.05773087 -0.12767081]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 13 ] state=[ 0.02394407  0.05402766 -0.05773087 -0.12767081], action=0, reward=1.0, next_state=[ 0.02502462 -0.14022178 -0.06028429  0.14625448]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 14 ] state=[ 0.02502462 -0.14022178 -0.06028429  0.14625448], action=0, reward=1.0, next_state=[ 0.02222019 -0.33443094 -0.0573592   0.41932618]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 15 ] state=[ 0.02222019 -0.33443094 -0.0573592   0.41932618], action=1, reward=1.0, next_state=[ 0.01553157 -0.13854512 -0.04897267  0.10912632]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 16 ] state=[ 0.01553157 -0.13854512 -0.04897267  0.10912632], action=1, reward=1.0, next_state=[ 0.01276067  0.05724315 -0.04679015 -0.19859619]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 17 ] state=[ 0.01276067  0.05724315 -0.04679015 -0.19859619], action=0, reward=1.0, next_state=[ 0.01390553 -0.13717941 -0.05076207  0.07896719]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 18 ] state=[ 0.01390553 -0.13717941 -0.05076207  0.07896719], action=0, reward=1.0, next_state=[ 0.01116194 -0.33153831 -0.04918273  0.35521236]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 19 ] state=[ 0.01116194 -0.33153831 -0.04918273  0.35521236], action=0, reward=1.0, next_state=[ 0.00453118 -0.52592772 -0.04207848  0.63198999]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 20 ] state=[ 0.00453118 -0.52592772 -0.04207848  0.63198999], action=0, reward=1.0, next_state=[-0.00598738 -0.72043812 -0.02943868  0.91112991]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 21 ] state=[-0.00598738 -0.72043812 -0.02943868  0.91112991], action=0, reward=1.0, next_state=[-0.02039614 -0.91514962 -0.01121608  1.19441688]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 22 ] state=[-0.02039614 -0.91514962 -0.01121608  1.19441688], action=1, reward=1.0, next_state=[-0.03869913 -0.71988423  0.01267225  0.89823977]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 23 ] state=[-0.03869913 -0.71988423  0.01267225  0.89823977], action=0, reward=1.0, next_state=[-0.05309682 -0.91517563  0.03063705  1.19487889]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 24 ] state=[-0.05309682 -0.91517563  0.03063705  1.19487889], action=1, reward=1.0, next_state=[-0.07140033 -0.72046349  0.05453463  0.91195362]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 25 ] state=[-0.07140033 -0.72046349  0.05453463  0.91195362], action=1, reward=1.0, next_state=[-0.0858096  -0.52612012  0.0727737   0.63689717]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 26 ] state=[-0.0858096  -0.52612012  0.0727737   0.63689717], action=1, reward=1.0, next_state=[-0.096332   -0.33208445  0.08551164  0.36799063]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 27 ] state=[-0.096332   -0.33208445  0.08551164  0.36799063], action=1, reward=1.0, next_state=[-0.10297369 -0.13827505  0.09287146  0.10344857]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 28 ] state=[-0.10297369 -0.13827505  0.09287146  0.10344857], action=0, reward=1.0, next_state=[-0.10573919 -0.33459676  0.09494043  0.42392704]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 29 ] state=[-0.10573919 -0.33459676  0.09494043  0.42392704], action=1, reward=1.0, next_state=[-0.11243113 -0.14093895  0.10341897  0.16261909]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 30 ] state=[-0.11243113 -0.14093895  0.10341897  0.16261909], action=1, reward=1.0, next_state=[-0.11524991  0.05256215  0.10667135 -0.09573074]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 117 ][ timestamp 31 ] state=[-0.11524991  0.05256215  0.10667135 -0.09573074], action=1, reward=1.0, next_state=[-0.11419866  0.24600636  0.10475674 -0.35294581]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 32 ] state=[-0.11419866  0.24600636  0.10475674 -0.35294581], action=1, reward=1.0, next_state=[-0.10927854  0.43949482  0.09769782 -0.61084528]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 33 ] state=[-0.10927854  0.43949482  0.09769782 -0.61084528], action=0, reward=1.0, next_state=[-0.10048864  0.24315284  0.08548091 -0.28905924]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 34 ] state=[-0.10048864  0.24315284  0.08548091 -0.28905924], action=1, reward=1.0, next_state=[-0.09562558  0.43695842  0.07969973 -0.55360537]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 35 ] state=[-0.09562558  0.43695842  0.07969973 -0.55360537], action=0, reward=1.0, next_state=[-0.08688642  0.24081307  0.06862762 -0.23691437]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 36 ] state=[-0.08688642  0.24081307  0.06862762 -0.23691437], action=1, reward=1.0, next_state=[-0.08207015  0.43489087  0.06388933 -0.5071851 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 37 ] state=[-0.08207015  0.43489087  0.06388933 -0.5071851 ], action=0, reward=1.0, next_state=[-0.07337234  0.23892961  0.05374563 -0.19507224]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 38 ] state=[-0.07337234  0.23892961  0.05374563 -0.19507224], action=1, reward=1.0, next_state=[-0.06859374  0.43324322  0.04984419 -0.47032817]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 39 ] state=[-0.06859374  0.43324322  0.04984419 -0.47032817], action=0, reward=1.0, next_state=[-0.05992888  0.23745393  0.04043762 -0.16236086]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 40 ] state=[-0.05992888  0.23745393  0.04043762 -0.16236086], action=0, reward=1.0, next_state=[-0.0551798   0.04177711  0.03719041  0.14279984]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 41 ] state=[-0.0551798   0.04177711  0.03719041  0.14279984], action=1, reward=1.0, next_state=[-0.05434426  0.23634725  0.0400464  -0.13792209]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 42 ] state=[-0.05434426  0.23634725  0.0400464  -0.13792209], action=1, reward=1.0, next_state=[-0.04961731  0.43087342  0.03728796 -0.41770691]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 43 ] state=[-0.04961731  0.43087342  0.03728796 -0.41770691], action=1, reward=1.0, next_state=[-0.04099985  0.62544766  0.02893382 -0.69840527]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 44 ] state=[-0.04099985  0.62544766  0.02893382 -0.69840527], action=1, reward=1.0, next_state=[-0.02849089  0.82015673  0.01496572 -0.98184128]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 45 ] state=[-0.02849089  0.82015673  0.01496572 -0.98184128], action=0, reward=1.0, next_state=[-0.01208776  0.62483746 -0.00467111 -0.68449542]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 46 ] state=[-0.01208776  0.62483746 -0.00467111 -0.68449542], action=0, reward=1.0, next_state=[ 4.08991383e-04  4.29780675e-01 -1.83610157e-02 -3.93286735e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 47 ] state=[ 4.08991383e-04  4.29780675e-01 -1.83610157e-02 -3.93286735e-01], action=0, reward=1.0, next_state=[ 0.0090046   0.23492402 -0.02622675 -0.10644886]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 48 ] state=[ 0.0090046   0.23492402 -0.02622675 -0.10644886], action=1, reward=1.0, next_state=[ 0.01370309  0.43041181 -0.02835573 -0.40728947]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 49 ] state=[ 0.01370309  0.43041181 -0.02835573 -0.40728947], action=0, reward=1.0, next_state=[ 0.02231132  0.23570316 -0.03650152 -0.12367938]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 50 ] state=[ 0.02231132  0.23570316 -0.03650152 -0.12367938], action=0, reward=1.0, next_state=[ 0.02702538  0.04112265 -0.0389751   0.15726791]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 51 ] state=[ 0.02702538  0.04112265 -0.0389751   0.15726791], action=1, reward=1.0, next_state=[ 0.02784784  0.23678032 -0.03582975 -0.14745149]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 52 ] state=[ 0.02784784  0.23678032 -0.03582975 -0.14745149], action=0, reward=1.0, next_state=[ 0.03258344  0.04218929 -0.03877878  0.13371602]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 53 ] state=[ 0.03258344  0.04218929 -0.03877878  0.13371602], action=1, reward=1.0, next_state=[ 0.03342723  0.23784463 -0.03610446 -0.17094446]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 54 ] state=[ 0.03342723  0.23784463 -0.03610446 -0.17094446], action=0, reward=1.0, next_state=[ 0.03818412  0.04325755 -0.03952335  0.11013355]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 55 ] state=[ 0.03818412  0.04325755 -0.03952335  0.11013355], action=0, reward=1.0, next_state=[ 0.03904927 -0.15127642 -0.03732067  0.39008978]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 56 ] state=[ 0.03904927 -0.15127642 -0.03732067  0.39008978], action=0, reward=1.0, next_state=[ 0.03602375 -0.34584935 -0.02951888  0.67077622]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 57 ] state=[ 0.03602375 -0.34584935 -0.02951888  0.67077622], action=0, reward=1.0, next_state=[ 0.02910676 -0.54054875 -0.01610335  0.95402079]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 58 ] state=[ 0.02910676 -0.54054875 -0.01610335  0.95402079], action=0, reward=1.0, next_state=[ 0.01829578 -0.7354504   0.00297706  1.24160118]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 59 ] state=[ 0.01829578 -0.7354504   0.00297706  1.24160118], action=0, reward=1.0, next_state=[ 0.00358678 -0.93061044  0.02780909  1.5352152 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 60 ] state=[ 0.00358678 -0.93061044  0.02780909  1.5352152 ], action=1, reward=1.0, next_state=[-0.01502543 -0.73583419  0.05851339  1.2513386 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 61 ] state=[-0.01502543 -0.73583419  0.05851339  1.2513386 ], action=1, reward=1.0, next_state=[-0.02974212 -0.54150878  0.08354016  0.97754247]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 62 ] state=[-0.02974212 -0.54150878  0.08354016  0.97754247], action=0, reward=1.0, next_state=[-0.04057229 -0.73764553  0.10309101  1.29525383]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 63 ] state=[-0.04057229 -0.73764553  0.10309101  1.29525383], action=0, reward=1.0, next_state=[-0.0553252  -0.93391498  0.12899609  1.61835006]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 64 ] state=[-0.0553252  -0.93391498  0.12899609  1.61835006], action=0, reward=1.0, next_state=[-0.0740035  -1.13029999  0.16136309  1.94829984]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 65 ] state=[-0.0740035  -1.13029999  0.16136309  1.94829984], action=0, reward=1.0, next_state=[-0.0966095  -1.32673017  0.20032909  2.28635258]\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 66 ] state=[-0.0966095  -1.32673017  0.20032909  2.28635258], action=0, reward=-1.0, next_state=[-0.12314411 -1.52306518  0.24605614  2.63346898]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 117: Exploration_rate=0.01. Score=66.\n",
      "[ episode 118 ] state=[ 0.00116269 -0.02272633  0.04180216  0.03742394]\n",
      "[ episode 118 ][ timestamp 1 ] state=[ 0.00116269 -0.02272633  0.04180216  0.03742394], action=1, reward=1.0, next_state=[ 0.00070816  0.17177202  0.04255064 -0.24178246]\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 2 ] state=[ 0.00070816  0.17177202  0.04255064 -0.24178246], action=0, reward=1.0, next_state=[ 0.0041436  -0.0239311   0.03771499  0.06401262]\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 3 ] state=[ 0.0041436  -0.0239311   0.03771499  0.06401262], action=0, reward=1.0, next_state=[ 0.00366498 -0.21957292  0.03899524  0.36835224]\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 4 ] state=[ 0.00366498 -0.21957292  0.03899524  0.36835224], action=0, reward=1.0, next_state=[-0.00072648 -0.41522663  0.04636228  0.6730714 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 5 ] state=[-0.00072648 -0.41522663  0.04636228  0.6730714 ], action=0, reward=1.0, next_state=[-0.00903101 -0.61096129  0.05982371  0.97998353]\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 6 ] state=[-0.00903101 -0.61096129  0.05982371  0.97998353], action=0, reward=1.0, next_state=[-0.02125024 -0.80683191  0.07942338  1.29084154]\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 7 ] state=[-0.02125024 -0.80683191  0.07942338  1.29084154], action=0, reward=1.0, next_state=[-0.03738688 -1.00286885  0.10524021  1.60729591]\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 8 ] state=[-0.03738688 -1.00286885  0.10524021  1.60729591], action=0, reward=1.0, next_state=[-0.05744425 -1.19906581  0.13738613  1.93084667]\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 9 ] state=[-0.05744425 -1.19906581  0.13738613  1.93084667], action=1, reward=1.0, next_state=[-0.08142557 -1.00565678  0.17600307  1.68373134]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 118 ][ timestamp 10 ] state=[-0.08142557 -1.00565678  0.17600307  1.68373134], action=0, reward=-1.0, next_state=[-0.10153871 -1.2023256   0.20967769  2.02565534]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 118: Exploration_rate=0.01. Score=10.\n",
      "[ episode 119 ] state=[ 0.04896425 -0.01667025 -0.02636561 -0.02444538]\n",
      "[ episode 119 ][ timestamp 1 ] state=[ 0.04896425 -0.01667025 -0.02636561 -0.02444538], action=0, reward=1.0, next_state=[ 0.04863085 -0.21140437 -0.02685452  0.25980369]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 2 ] state=[ 0.04863085 -0.21140437 -0.02685452  0.25980369], action=0, reward=1.0, next_state=[ 0.04440276 -0.40613287 -0.02165844  0.54389684]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 3 ] state=[ 0.04440276 -0.40613287 -0.02165844  0.54389684], action=0, reward=1.0, next_state=[ 0.0362801  -0.60094387 -0.01078051  0.82967772]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 4 ] state=[ 0.0362801  -0.60094387 -0.01078051  0.82967772], action=1, reward=1.0, next_state=[ 0.02426122 -0.40567622  0.00581305  0.53362386]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 5 ] state=[ 0.02426122 -0.40567622  0.00581305  0.53362386], action=1, reward=1.0, next_state=[ 0.0161477  -0.2106365   0.01648552  0.24277825]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 6 ] state=[ 0.0161477  -0.2106365   0.01648552  0.24277825], action=1, reward=1.0, next_state=[ 0.01193497 -0.01575386  0.02134109 -0.04465946]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 7 ] state=[ 0.01193497 -0.01575386  0.02134109 -0.04465946], action=0, reward=1.0, next_state=[ 0.01161989 -0.21117523  0.0204479   0.25467964]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 8 ] state=[ 0.01161989 -0.21117523  0.0204479   0.25467964], action=0, reward=1.0, next_state=[ 0.00739639 -0.40658308  0.02554149  0.55374141]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 9 ] state=[ 0.00739639 -0.40658308  0.02554149  0.55374141], action=1, reward=1.0, next_state=[-0.00073527 -0.21182892  0.03661632  0.26921384]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 10 ] state=[-0.00073527 -0.21182892  0.03661632  0.26921384], action=0, reward=1.0, next_state=[-0.00497185 -0.40745377  0.0420006   0.57321721]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 11 ] state=[-0.00497185 -0.40745377  0.0420006   0.57321721], action=0, reward=1.0, next_state=[-0.01312093 -0.60313866  0.05346494  0.87883023]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 12 ] state=[-0.01312093 -0.60313866  0.05346494  0.87883023], action=1, reward=1.0, next_state=[-0.0251837  -0.40878236  0.07104155  0.60342356]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 13 ] state=[-0.0251837  -0.40878236  0.07104155  0.60342356], action=1, reward=1.0, next_state=[-0.03335935 -0.21472215  0.08311002  0.33393614]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 14 ] state=[-0.03335935 -0.21472215  0.08311002  0.33393614], action=1, reward=1.0, next_state=[-0.03765379 -0.02087535  0.08978874  0.06857581]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 15 ] state=[-0.03765379 -0.02087535  0.08978874  0.06857581], action=1, reward=1.0, next_state=[-0.0380713   0.17285225  0.09116026 -0.19448256]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 16 ] state=[-0.0380713   0.17285225  0.09116026 -0.19448256], action=1, reward=1.0, next_state=[-0.03461425  0.36655996  0.08727061 -0.45707364]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 17 ] state=[-0.03461425  0.36655996  0.08727061 -0.45707364], action=1, reward=1.0, next_state=[-0.02728305  0.56034671  0.07812913 -0.72102254]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 18 ] state=[-0.02728305  0.56034671  0.07812913 -0.72102254], action=0, reward=1.0, next_state=[-0.01607612  0.36423583  0.06370868 -0.40480698]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 19 ] state=[-0.01607612  0.36423583  0.06370868 -0.40480698], action=1, reward=1.0, next_state=[-0.0087914   0.55839917  0.05561254 -0.67674345]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 20 ] state=[-0.0087914   0.55839917  0.05561254 -0.67674345], action=1, reward=1.0, next_state=[ 0.00237658  0.7527061   0.04207767 -0.9514116 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 21 ] state=[ 0.00237658  0.7527061   0.04207767 -0.9514116 ], action=0, reward=1.0, next_state=[ 0.0174307   0.55704389  0.02304944 -0.64581088]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 22 ] state=[ 0.0174307   0.55704389  0.02304944 -0.64581088], action=0, reward=1.0, next_state=[ 0.02857158  0.36160847  0.01013322 -0.34595968]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 23 ] state=[ 0.02857158  0.36160847  0.01013322 -0.34595968], action=0, reward=1.0, next_state=[ 0.03580375  0.16634385  0.00321403 -0.05009867]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 24 ] state=[ 0.03580375  0.16634385  0.00321403 -0.05009867], action=1, reward=1.0, next_state=[ 0.03913063  0.36141957  0.00221206 -0.34176582]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 25 ] state=[ 0.03913063  0.36141957  0.00221206 -0.34176582], action=1, reward=1.0, next_state=[ 0.04635902  0.55650998 -0.00462326 -0.63375037]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 26 ] state=[ 0.04635902  0.55650998 -0.00462326 -0.63375037], action=0, reward=1.0, next_state=[ 0.05748922  0.36145283 -0.01729827 -0.342527  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 27 ] state=[ 0.05748922  0.36145283 -0.01729827 -0.342527  ], action=0, reward=1.0, next_state=[ 0.06471827  0.16658119 -0.02414881 -0.05534872]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 28 ] state=[ 0.06471827  0.16658119 -0.02414881 -0.05534872], action=0, reward=1.0, next_state=[ 0.0680499  -0.02818633 -0.02525578  0.22961832]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 29 ] state=[ 0.0680499  -0.02818633 -0.02525578  0.22961832], action=1, reward=1.0, next_state=[ 0.06748617  0.16728726 -0.02066341 -0.07092296]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 30 ] state=[ 0.06748617  0.16728726 -0.02066341 -0.07092296], action=0, reward=1.0, next_state=[ 0.07083192 -0.02753245 -0.02208187  0.21516961]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 31 ] state=[ 0.07083192 -0.02753245 -0.02208187  0.21516961], action=1, reward=1.0, next_state=[ 0.07028127  0.16789811 -0.01777848 -0.08439632]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 32 ] state=[ 0.07028127  0.16789811 -0.01777848 -0.08439632], action=0, reward=1.0, next_state=[ 0.07363923 -0.02696454 -0.01946641  0.20262487]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 33 ] state=[ 0.07363923 -0.02696454 -0.01946641  0.20262487], action=1, reward=1.0, next_state=[ 0.07309994  0.16843032 -0.01541391 -0.09613465]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 34 ] state=[ 0.07309994  0.16843032 -0.01541391 -0.09613465], action=1, reward=1.0, next_state=[ 0.07646854  0.36376976 -0.0173366  -0.39364051]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 35 ] state=[ 0.07646854  0.36376976 -0.0173366  -0.39364051], action=0, reward=1.0, next_state=[ 0.08374394  0.16889806 -0.02520941 -0.10647359]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 36 ] state=[ 0.08374394  0.16889806 -0.02520941 -0.10647359], action=0, reward=1.0, next_state=[ 0.0871219  -0.02585373 -0.02733889  0.17815049]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 37 ] state=[ 0.0871219  -0.02585373 -0.02733889  0.17815049], action=0, reward=1.0, next_state=[ 0.08660483 -0.220574   -0.02377588  0.46208511]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 38 ] state=[ 0.08660483 -0.220574   -0.02377588  0.46208511], action=1, reward=1.0, next_state=[ 0.08219335 -0.02512423 -0.01453417  0.16200387]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 39 ] state=[ 0.08219335 -0.02512423 -0.01453417  0.16200387], action=1, reward=1.0, next_state=[ 0.08169086  0.17020274 -0.0112941  -0.13522854]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 40 ] state=[ 0.08169086  0.17020274 -0.0112941  -0.13522854], action=0, reward=1.0, next_state=[ 0.08509492 -0.02475563 -0.01399867  0.15386998]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 41 ] state=[ 0.08509492 -0.02475563 -0.01399867  0.15386998], action=1, reward=1.0, next_state=[ 0.0845998   0.17056393 -0.01092127 -0.14319613]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 42 ] state=[ 0.0845998   0.17056393 -0.01092127 -0.14319613], action=1, reward=1.0, next_state=[ 0.08801108  0.36584057 -0.01378519 -0.43930442]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 43 ] state=[ 0.08801108  0.36584057 -0.01378519 -0.43930442], action=0, reward=1.0, next_state=[ 0.09532789  0.17091641 -0.02257128 -0.15099867]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 44 ] state=[ 0.09532789  0.17091641 -0.02257128 -0.15099867], action=0, reward=1.0, next_state=[ 0.09874622 -0.02387519 -0.02559125  0.1344789 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 45 ] state=[ 0.09874622 -0.02387519 -0.02559125  0.1344789 ], action=1, reward=1.0, next_state=[ 0.09826872  0.1716038  -0.02290167 -0.16616658]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 46 ] state=[ 0.09826872  0.1716038  -0.02290167 -0.16616658], action=0, reward=1.0, next_state=[ 0.10170079 -0.02318296 -0.02622501  0.11920445]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 119 ][ timestamp 47 ] state=[ 0.10170079 -0.02318296 -0.02622501  0.11920445], action=0, reward=1.0, next_state=[ 0.10123714 -0.21791955 -0.02384092  0.40349962]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 48 ] state=[ 0.10123714 -0.21791955 -0.02384092  0.40349962], action=1, reward=1.0, next_state=[ 0.09687874 -0.02246772 -0.01577092  0.10339663]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 49 ] state=[ 0.09687874 -0.02246772 -0.01577092  0.10339663], action=0, reward=1.0, next_state=[ 0.09642939 -0.21736015 -0.01370299  0.39106246]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 50 ] state=[ 0.09642939 -0.21736015 -0.01370299  0.39106246], action=1, reward=1.0, next_state=[ 0.09208219 -0.02204643 -0.00588174  0.09409084]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 51 ] state=[ 0.09208219 -0.02204643 -0.00588174  0.09409084], action=1, reward=1.0, next_state=[ 0.09164126  0.17315932 -0.00399993 -0.20044196]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 52 ] state=[ 0.09164126  0.17315932 -0.00399993 -0.20044196], action=1, reward=1.0, next_state=[ 0.09510445  0.36833825 -0.00800876 -0.49438399]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 53 ] state=[ 0.09510445  0.36833825 -0.00800876 -0.49438399], action=1, reward=1.0, next_state=[ 0.10247121  0.56357223 -0.01789644 -0.78958011]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 54 ] state=[ 0.10247121  0.56357223 -0.01789644 -0.78958011], action=0, reward=1.0, next_state=[ 0.11374265  0.36870057 -0.03368805 -0.5025807 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 55 ] state=[ 0.11374265  0.36870057 -0.03368805 -0.5025807 ], action=0, reward=1.0, next_state=[ 0.12111667  0.17406924 -0.04373966 -0.22070177]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 56 ] state=[ 0.12111667  0.17406924 -0.04373966 -0.22070177], action=0, reward=1.0, next_state=[ 0.12459805 -0.02040109 -0.0481537   0.05786937]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 57 ] state=[ 0.12459805 -0.02040109 -0.0481537   0.05786937], action=1, reward=1.0, next_state=[ 0.12419003  0.17537704 -0.04699631 -0.24960913]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 58 ] state=[ 0.12419003  0.17537704 -0.04699631 -0.24960913], action=0, reward=1.0, next_state=[ 0.12769757 -0.01904336 -0.05198849  0.02788765]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 59 ] state=[ 0.12769757 -0.01904336 -0.05198849  0.02788765], action=0, reward=1.0, next_state=[ 0.1273167  -0.21338271 -0.05143074  0.30372508]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 60 ] state=[ 0.1273167  -0.21338271 -0.05143074  0.30372508], action=1, reward=1.0, next_state=[ 0.12304905 -0.01756696 -0.04535624 -0.00472414]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 61 ] state=[ 0.12304905 -0.01756696 -0.04535624 -0.00472414], action=0, reward=1.0, next_state=[ 0.12269771 -0.21201008 -0.04545072  0.27331043]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 62 ] state=[ 0.12269771 -0.21201008 -0.04545072  0.27331043], action=1, reward=1.0, next_state=[ 0.11845751 -0.01627007 -0.03998451 -0.03335428]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 63 ] state=[ 0.11845751 -0.01627007 -0.03998451 -0.03335428], action=0, reward=1.0, next_state=[ 0.11813211 -0.2107965  -0.0406516   0.24644982]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 64 ] state=[ 0.11813211 -0.2107965  -0.0406516   0.24644982], action=1, reward=1.0, next_state=[ 0.11391618 -0.01511824 -0.0357226  -0.05877335]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 65 ] state=[ 0.11391618 -0.01511824 -0.0357226  -0.05877335], action=0, reward=1.0, next_state=[ 0.11361381 -0.20971027 -0.03689807  0.22242827]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 66 ] state=[ 0.11361381 -0.20971027 -0.03689807  0.22242827], action=0, reward=1.0, next_state=[ 0.10941961 -0.40428594 -0.0324495   0.50324754]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 67 ] state=[ 0.10941961 -0.40428594 -0.0324495   0.50324754], action=0, reward=1.0, next_state=[ 0.10133389 -0.59893586 -0.02238455  0.78553024]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 68 ] state=[ 0.10133389 -0.59893586 -0.02238455  0.78553024], action=1, reward=1.0, next_state=[ 0.08935517 -0.40351363 -0.00667395  0.48588982]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 69 ] state=[ 0.08935517 -0.40351363 -0.00667395  0.48588982], action=1, reward=1.0, next_state=[ 0.0812849  -0.20829814  0.00304385  0.19111099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 70 ] state=[ 0.0812849  -0.20829814  0.00304385  0.19111099], action=0, reward=1.0, next_state=[ 0.07711894 -0.40346351  0.00686607  0.48475257]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 71 ] state=[ 0.07711894 -0.40346351  0.00686607  0.48475257], action=1, reward=1.0, next_state=[ 0.06904966 -0.20843912  0.01656112  0.19424149]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 72 ] state=[ 0.06904966 -0.20843912  0.01656112  0.19424149], action=1, reward=1.0, next_state=[ 0.06488088 -0.01355794  0.02044595 -0.09317145]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 73 ] state=[ 0.06488088 -0.01355794  0.02044595 -0.09317145], action=0, reward=1.0, next_state=[ 0.06460972 -0.20896688  0.01858252  0.20589139]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 74 ] state=[ 0.06460972 -0.20896688  0.01858252  0.20589139], action=1, reward=1.0, next_state=[ 0.06043039 -0.01411552  0.02270035 -0.08087223]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 75 ] state=[ 0.06043039 -0.01411552  0.02270035 -0.08087223], action=1, reward=1.0, next_state=[ 0.06014808  0.18067378  0.02108291 -0.36630759]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 76 ] state=[ 0.06014808  0.18067378  0.02108291 -0.36630759], action=0, reward=1.0, next_state=[ 0.06376155 -0.01474132  0.01375675 -0.06705215]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 77 ] state=[ 0.06376155 -0.01474132  0.01375675 -0.06705215], action=0, reward=1.0, next_state=[ 0.06346672 -0.21005778  0.01241571  0.22993916]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 78 ] state=[ 0.06346672 -0.21005778  0.01241571  0.22993916], action=1, reward=1.0, next_state=[ 0.05926557 -0.01511542  0.01701449 -0.0588017 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 79 ] state=[ 0.05926557 -0.01511542  0.01701449 -0.0588017 ], action=0, reward=1.0, next_state=[ 0.05896326 -0.21047715  0.01583846  0.23920048]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 80 ] state=[ 0.05896326 -0.21047715  0.01583846  0.23920048], action=1, reward=1.0, next_state=[ 0.05475372 -0.015585    0.02062247 -0.04844476]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 81 ] state=[ 0.05475372 -0.015585    0.02062247 -0.04844476], action=0, reward=1.0, next_state=[ 0.05444202 -0.21099649  0.01965357  0.25067273]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 82 ] state=[ 0.05444202 -0.21099649  0.01965357  0.25067273], action=0, reward=1.0, next_state=[ 0.05022209 -0.40639351  0.02466703  0.54948943]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 83 ] state=[ 0.05022209 -0.40639351  0.02466703  0.54948943], action=1, reward=1.0, next_state=[ 0.04209422 -0.21162658  0.03565682  0.26467928]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 84 ] state=[ 0.04209422 -0.21162658  0.03565682  0.26467928], action=1, reward=1.0, next_state=[ 0.03786169 -0.01703122  0.0409504  -0.01654733]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 85 ] state=[ 0.03786169 -0.01703122  0.0409504  -0.01654733], action=0, reward=1.0, next_state=[ 0.03752106 -0.2127158   0.04061946  0.28876951]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 86 ] state=[ 0.03752106 -0.2127158   0.04061946  0.28876951], action=1, reward=1.0, next_state=[ 0.03326675 -0.01819591  0.04639485  0.0091692 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 87 ] state=[ 0.03326675 -0.01819591  0.04639485  0.0091692 ], action=0, reward=1.0, next_state=[ 0.03290283 -0.21395147  0.04657823  0.31612176]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 88 ] state=[ 0.03290283 -0.21395147  0.04657823  0.31612176], action=0, reward=1.0, next_state=[ 0.0286238  -0.40970486  0.05290067  0.62312243]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 89 ] state=[ 0.0286238  -0.40970486  0.05290067  0.62312243], action=1, reward=1.0, next_state=[ 0.0204297  -0.2153599   0.06536311  0.34755834]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 90 ] state=[ 0.0204297  -0.2153599   0.06536311  0.34755834], action=0, reward=1.0, next_state=[ 0.0161225  -0.41134771  0.07231428  0.66011535]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 91 ] state=[ 0.0161225  -0.41134771  0.07231428  0.66011535], action=1, reward=1.0, next_state=[ 0.00789555 -0.21730264  0.08551659  0.39105034]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 92 ] state=[ 0.00789555 -0.21730264  0.08551659  0.39105034], action=1, reward=1.0, next_state=[ 0.0035495  -0.02349186  0.09333759  0.12650779]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 93 ] state=[ 0.0035495  -0.02349186  0.09333759  0.12650779], action=1, reward=1.0, next_state=[ 0.00307966  0.17017761  0.09586775 -0.13533049]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 94 ] state=[ 0.00307966  0.17017761  0.09586775 -0.13533049], action=1, reward=1.0, next_state=[ 0.00648321  0.36380498  0.09316114 -0.39629593]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 95 ] state=[ 0.00648321  0.36380498  0.09316114 -0.39629593], action=0, reward=1.0, next_state=[ 0.01375931  0.16749324  0.08523522 -0.07575547]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 119 ][ timestamp 96 ] state=[ 0.01375931  0.16749324  0.08523522 -0.07575547], action=0, reward=1.0, next_state=[ 0.01710918 -0.02874066  0.08372011  0.24255561]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 97 ] state=[ 0.01710918 -0.02874066  0.08372011  0.24255561], action=1, reward=1.0, next_state=[ 0.01653436  0.16509185  0.08857123 -0.02258984]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 98 ] state=[ 0.01653436  0.16509185  0.08857123 -0.02258984], action=1, reward=1.0, next_state=[ 0.0198362   0.35883932  0.08811943 -0.28606593]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 99 ] state=[ 0.0198362   0.35883932  0.08811943 -0.28606593], action=0, reward=1.0, next_state=[0.02701299 0.16257838 0.08239811 0.03305683]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 100 ] state=[0.02701299 0.16257838 0.08239811 0.03305683], action=1, reward=1.0, next_state=[ 0.03026455  0.35642797  0.08305925 -0.23253337]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 101 ] state=[ 0.03026455  0.35642797  0.08305925 -0.23253337], action=1, reward=1.0, next_state=[ 0.03739311  0.55027095  0.07840858 -0.4979041 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 102 ] state=[ 0.03739311  0.55027095  0.07840858 -0.4979041 ], action=0, reward=1.0, next_state=[ 0.04839853  0.3541361   0.0684505  -0.18157722]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 103 ] state=[ 0.04839853  0.3541361   0.0684505  -0.18157722], action=0, reward=1.0, next_state=[0.05548125 0.15810484 0.06481895 0.13188981]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 104 ] state=[0.05548125 0.15810484 0.06481895 0.13188981], action=1, reward=1.0, next_state=[ 0.05864335  0.35224129  0.06745675 -0.13965991]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 105 ] state=[ 0.05864335  0.35224129  0.06745675 -0.13965991], action=0, reward=1.0, next_state=[0.06568818 0.1562213  0.06466355 0.17351861]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 106 ] state=[0.06568818 0.1562213  0.06466355 0.17351861], action=0, reward=1.0, next_state=[ 0.0688126  -0.03976367  0.06813392  0.48587949]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 107 ] state=[ 0.0688126  -0.03976367  0.06813392  0.48587949], action=1, reward=1.0, next_state=[0.06801733 0.15433402 0.07785151 0.21542436]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 108 ] state=[0.06801733 0.15433402 0.07785151 0.21542436], action=0, reward=1.0, next_state=[ 0.07110401 -0.0418096   0.08216     0.53161387]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 109 ] state=[ 0.07110401 -0.0418096   0.08216     0.53161387], action=1, reward=1.0, next_state=[0.07026782 0.15206644 0.09279228 0.26590868]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 110 ] state=[0.07026782 0.15206644 0.09279228 0.26590868], action=1, reward=1.0, next_state=[0.07330915 0.34574995 0.09811045 0.00387507]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 111 ] state=[0.07330915 0.34574995 0.09811045 0.00387507], action=0, reward=1.0, next_state=[0.08022415 0.14936784 0.09818795 0.32582987]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 112 ] state=[0.08022415 0.14936784 0.09818795 0.32582987], action=1, reward=1.0, next_state=[0.0832115  0.34296469 0.10470455 0.0656542 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 113 ] state=[0.0832115  0.34296469 0.10470455 0.0656542 ], action=1, reward=1.0, next_state=[ 0.0900708   0.53644177  0.10601763 -0.19224514]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 114 ] state=[ 0.0900708   0.53644177  0.10601763 -0.19224514], action=0, reward=1.0, next_state=[0.10079963 0.33997553 0.10217273 0.13191043]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 115 ] state=[0.10079963 0.33997553 0.10217273 0.13191043], action=0, reward=1.0, next_state=[0.10759914 0.1435497  0.10481094 0.45499916]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 116 ] state=[0.10759914 0.1435497  0.10481094 0.45499916], action=1, reward=1.0, next_state=[0.11047014 0.33704566 0.11391092 0.197106  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 117 ] state=[0.11047014 0.33704566 0.11391092 0.197106  ], action=1, reward=1.0, next_state=[ 0.11721105  0.53036961  0.11785304 -0.05758314]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 118 ] state=[ 0.11721105  0.53036961  0.11785304 -0.05758314], action=1, reward=1.0, next_state=[ 0.12781844  0.72362187  0.11670138 -0.31088211]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 119 ] state=[ 0.12781844  0.72362187  0.11670138 -0.31088211], action=1, reward=1.0, next_state=[ 0.14229088  0.91690455  0.11048374 -0.56460171]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 120 ] state=[ 0.14229088  0.91690455  0.11048374 -0.56460171], action=1, reward=1.0, next_state=[ 0.16062897  1.1103171   0.0991917  -0.82053548]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 121 ] state=[ 0.16062897  1.1103171   0.0991917  -0.82053548], action=0, reward=1.0, next_state=[ 0.18283531  0.91398784  0.08278099 -0.4983746 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 122 ] state=[ 0.18283531  0.91398784  0.08278099 -0.4983746 ], action=1, reward=1.0, next_state=[ 0.20111507  1.10785106  0.0728135  -0.76386381]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 123 ] state=[ 0.20111507  1.10785106  0.0728135  -0.76386381], action=1, reward=1.0, next_state=[ 0.22327209  1.30189871  0.05753623 -1.03277577]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 124 ] state=[ 0.22327209  1.30189871  0.05753623 -1.03277577], action=0, reward=1.0, next_state=[ 0.24931006  1.10606065  0.03688071 -0.72259845]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 125 ] state=[ 0.24931006  1.10606065  0.03688071 -0.72259845], action=1, reward=1.0, next_state=[ 0.27143128  1.30065358  0.02242874 -1.00344889]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 126 ] state=[ 0.27143128  1.30065358  0.02242874 -1.00344889], action=0, reward=1.0, next_state=[ 0.29744435  1.1052393   0.00235976 -0.7038077 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 127 ] state=[ 0.29744435  1.1052393   0.00235976 -0.7038077 ], action=1, reward=1.0, next_state=[ 0.31954913  1.30032847 -0.01171639 -0.99574687]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 128 ] state=[ 0.31954913  1.30032847 -0.01171639 -0.99574687], action=0, reward=1.0, next_state=[ 0.3455557   1.10536516 -0.03163133 -0.70676651]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 129 ] state=[ 0.3455557   1.10536516 -0.03163133 -0.70676651], action=0, reward=1.0, next_state=[ 0.36766301  0.91069538 -0.04576666 -0.42420597]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 130 ] state=[ 0.36766301  0.91069538 -0.04576666 -0.42420597], action=0, reward=1.0, next_state=[ 0.38587691  0.71625062 -0.05425078 -0.14629494]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 131 ] state=[ 0.38587691  0.71625062 -0.05425078 -0.14629494], action=0, reward=1.0, next_state=[ 0.40020193  0.52194585 -0.05717668  0.12879151]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 132 ] state=[ 0.40020193  0.52194585 -0.05717668  0.12879151], action=1, reward=1.0, next_state=[ 0.41064084  0.71783828 -0.05460085 -0.18136774]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 133 ] state=[ 0.41064084  0.71783828 -0.05460085 -0.18136774], action=1, reward=1.0, next_state=[ 0.42499761  0.9136973  -0.0582282  -0.49076312]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 134 ] state=[ 0.42499761  0.9136973  -0.0582282  -0.49076312], action=1, reward=1.0, next_state=[ 0.44327156  1.10959022 -0.06804346 -0.80121393]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 135 ] state=[ 0.44327156  1.10959022 -0.06804346 -0.80121393], action=0, reward=1.0, next_state=[ 0.46546336  0.91546419 -0.08406774 -0.53068806]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 136 ] state=[ 0.46546336  0.91546419 -0.08406774 -0.53068806], action=0, reward=1.0, next_state=[ 0.48377264  0.72161915 -0.0946815  -0.2656342 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 137 ] state=[ 0.48377264  0.72161915 -0.0946815  -0.2656342 ], action=0, reward=1.0, next_state=[ 0.49820503  0.52796711 -0.09999419 -0.00425196]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 138 ] state=[ 0.49820503  0.52796711 -0.09999419 -0.00425196], action=1, reward=1.0, next_state=[ 0.50876437  0.72437037 -0.10007923 -0.32673454]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 139 ] state=[ 0.50876437  0.72437037 -0.10007923 -0.32673454], action=0, reward=1.0, next_state=[ 0.52325178  0.53080511 -0.10661392 -0.06721369]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 140 ] state=[ 0.52325178  0.53080511 -0.10661392 -0.06721369], action=0, reward=1.0, next_state=[ 0.53386788  0.33736041 -0.10795819  0.19002068]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 119 ][ timestamp 141 ] state=[ 0.53386788  0.33736041 -0.10795819  0.19002068], action=0, reward=1.0, next_state=[ 0.54061509  0.14393525 -0.10415778  0.4467912 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 142 ] state=[ 0.54061509  0.14393525 -0.10415778  0.4467912 ], action=1, reward=1.0, next_state=[ 0.54349379  0.34036459 -0.09522195  0.12317698]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 143 ] state=[ 0.54349379  0.34036459 -0.09522195  0.12317698], action=1, reward=1.0, next_state=[ 0.55030108  0.53671264 -0.09275841 -0.19796382]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 144 ] state=[ 0.55030108  0.53671264 -0.09275841 -0.19796382], action=0, reward=1.0, next_state=[ 0.56103534  0.3430314  -0.09671769  0.0640772 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 145 ] state=[ 0.56103534  0.3430314  -0.09671769  0.0640772 ], action=0, reward=1.0, next_state=[ 0.56789596  0.14941961 -0.09543615  0.32474692]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 146 ] state=[ 0.56789596  0.14941961 -0.09543615  0.32474692], action=1, reward=1.0, next_state=[ 0.57088436  0.3457617  -0.08894121  0.00355834]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 147 ] state=[ 0.57088436  0.3457617  -0.08894121  0.00355834], action=1, reward=1.0, next_state=[ 0.57779959  0.54203917 -0.08887004 -0.31580839]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 148 ] state=[ 0.57779959  0.54203917 -0.08887004 -0.31580839], action=1, reward=1.0, next_state=[ 0.58864037  0.73830719 -0.09518621 -0.63514202]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 149 ] state=[ 0.58864037  0.73830719 -0.09518621 -0.63514202], action=1, reward=1.0, next_state=[ 0.60340652  0.93461886 -0.10788905 -0.95621904]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 150 ] state=[ 0.60340652  0.93461886 -0.10788905 -0.95621904], action=0, reward=1.0, next_state=[ 0.6220989   0.74110035 -0.12701343 -0.69928693]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 151 ] state=[ 0.6220989   0.74110035 -0.12701343 -0.69928693], action=0, reward=1.0, next_state=[ 0.6369209   0.54794677 -0.14099917 -0.44913208]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 152 ] state=[ 0.6369209   0.54794677 -0.14099917 -0.44913208], action=0, reward=1.0, next_state=[ 0.64787984  0.35507132 -0.14998181 -0.20400657]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 153 ] state=[ 0.64787984  0.35507132 -0.14998181 -0.20400657], action=1, reward=1.0, next_state=[ 0.65498126  0.55198437 -0.15406194 -0.53998978]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 154 ] state=[ 0.65498126  0.55198437 -0.15406194 -0.53998978], action=0, reward=1.0, next_state=[ 0.66602095  0.3593254  -0.16486174 -0.29953938]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 155 ] state=[ 0.66602095  0.3593254  -0.16486174 -0.29953938], action=0, reward=1.0, next_state=[ 0.67320746  0.16689008 -0.17085252 -0.06305032]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 156 ] state=[ 0.67320746  0.16689008 -0.17085252 -0.06305032], action=0, reward=1.0, next_state=[ 0.67654526 -0.02542288 -0.17211353  0.17123244]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 157 ] state=[ 0.67654526 -0.02542288 -0.17211353  0.17123244], action=1, reward=1.0, next_state=[ 0.6760368   0.17169102 -0.16868888 -0.17042179]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 158 ] state=[ 0.6760368   0.17169102 -0.16868888 -0.17042179], action=0, reward=1.0, next_state=[ 0.67947062 -0.02066535 -0.17209732  0.06465757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 159 ] state=[ 0.67947062 -0.02066535 -0.17209732  0.06465757], action=0, reward=1.0, next_state=[ 0.67905732 -0.21295569 -0.17080417  0.29848503]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 160 ] state=[ 0.67905732 -0.21295569 -0.17080417  0.29848503], action=1, reward=1.0, next_state=[ 0.6747982  -0.01586296 -0.16483447 -0.04282467]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 161 ] state=[ 0.6747982  -0.01586296 -0.16483447 -0.04282467], action=0, reward=1.0, next_state=[ 0.67448094 -0.20828473 -0.16569096  0.19365354]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 162 ] state=[ 0.67448094 -0.20828473 -0.16569096  0.19365354], action=0, reward=1.0, next_state=[ 0.67031525 -0.40069672 -0.16181789  0.42982825]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 163 ] state=[ 0.67031525 -0.40069672 -0.16181789  0.42982825], action=1, reward=1.0, next_state=[ 0.66230132 -0.20369757 -0.15322132  0.0908228 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 164 ] state=[ 0.66230132 -0.20369757 -0.15322132  0.0908228 ], action=0, reward=1.0, next_state=[ 0.65822736 -0.39632924 -0.15140487  0.33151414]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 165 ] state=[ 0.65822736 -0.39632924 -0.15140487  0.33151414], action=0, reward=1.0, next_state=[ 0.65030078 -0.5890083  -0.14477459  0.57288325]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 166 ] state=[ 0.65030078 -0.5890083  -0.14477459  0.57288325], action=0, reward=1.0, next_state=[ 0.63852061 -0.78183539 -0.13331692  0.81668278]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 167 ] state=[ 0.63852061 -0.78183539 -0.13331692  0.81668278], action=1, reward=1.0, next_state=[ 0.62288391 -0.5851648  -0.11698326  0.48521548]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 168 ] state=[ 0.62288391 -0.5851648  -0.11698326  0.48521548], action=1, reward=1.0, next_state=[ 0.61118061 -0.38860315 -0.10727895  0.15807347]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 169 ] state=[ 0.61118061 -0.38860315 -0.10727895  0.15807347], action=1, reward=1.0, next_state=[ 0.60340855 -0.19212187 -0.10411749 -0.16643367]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 170 ] state=[ 0.60340855 -0.19212187 -0.10411749 -0.16643367], action=0, reward=1.0, next_state=[ 0.59956611 -0.38561128 -0.10744616  0.09167346]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 171 ] state=[ 0.59956611 -0.38561128 -0.10744616  0.09167346], action=1, reward=1.0, next_state=[ 0.59185388 -0.18912644 -0.10561269 -0.23288259]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 172 ] state=[ 0.59185388 -0.18912644 -0.10561269 -0.23288259], action=0, reward=1.0, next_state=[ 0.58807135 -0.38259326 -0.11027034  0.02470824]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 173 ] state=[ 0.58807135 -0.38259326 -0.11027034  0.02470824], action=0, reward=1.0, next_state=[ 0.58041949 -0.57597526 -0.10977618  0.28066564]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 174 ] state=[ 0.58041949 -0.57597526 -0.10977618  0.28066564], action=1, reward=1.0, next_state=[ 0.56889998 -0.37947257 -0.10416286 -0.04452358]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 175 ] state=[ 0.56889998 -0.37947257 -0.10416286 -0.04452358], action=0, reward=1.0, next_state=[ 0.56131053 -0.57295861 -0.10505334  0.21356389]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 176 ] state=[ 0.56131053 -0.57295861 -0.10505334  0.21356389], action=1, reward=1.0, next_state=[ 0.54985136 -0.37650382 -0.10078206 -0.1103226 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 177 ] state=[ 0.54985136 -0.37650382 -0.10078206 -0.1103226 ], action=1, reward=1.0, next_state=[ 0.54232128 -0.18009295 -0.10298851 -0.43302376]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 178 ] state=[ 0.54232128 -0.18009295 -0.10298851 -0.43302376], action=0, reward=1.0, next_state=[ 0.53871943 -0.37361746 -0.11164898 -0.17450024]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 179 ] state=[ 0.53871943 -0.37361746 -0.11164898 -0.17450024], action=0, reward=1.0, next_state=[ 0.53124708 -0.56697915 -0.11513899  0.08097977]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 180 ] state=[ 0.53124708 -0.56697915 -0.11513899  0.08097977], action=0, reward=1.0, next_state=[ 0.51990749 -0.76027845 -0.11351939  0.3352328 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 181 ] state=[ 0.51990749 -0.76027845 -0.11351939  0.3352328 ], action=0, reward=1.0, next_state=[ 0.50470192 -0.95361722 -0.10681474  0.59007127]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 182 ] state=[ 0.50470192 -0.95361722 -0.10681474  0.59007127], action=0, reward=1.0, next_state=[ 0.48562958 -1.14709422 -0.09501331  0.8472889 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 183 ] state=[ 0.48562958 -1.14709422 -0.09501331  0.8472889 ], action=0, reward=1.0, next_state=[ 0.4626877  -1.34080061 -0.07806753  1.10864605]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 184 ] state=[ 0.4626877  -1.34080061 -0.07806753  1.10864605], action=1, reward=1.0, next_state=[ 0.43587168 -1.14474437 -0.05589461  0.79252884]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 185 ] state=[ 0.43587168 -1.14474437 -0.05589461  0.79252884], action=0, reward=1.0, next_state=[ 0.4129768  -1.33905623 -0.04004404  1.06711699]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 186 ] state=[ 0.4129768  -1.33905623 -0.04004404  1.06711699], action=0, reward=1.0, next_state=[ 0.38619567 -1.53362617 -0.0187017   1.34696813]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 119 ][ timestamp 187 ] state=[ 0.38619567 -1.53362617 -0.0187017   1.34696813], action=0, reward=1.0, next_state=[ 0.35552315 -1.72850809  0.00823767  1.63374191]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 188 ] state=[ 0.35552315 -1.72850809  0.00823767  1.63374191], action=1, reward=1.0, next_state=[ 0.32095299 -1.53348379  0.0409125   1.34363723]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 189 ] state=[ 0.32095299 -1.53348379  0.0409125   1.34363723], action=1, reward=1.0, next_state=[ 0.29028331 -1.33889971  0.06778525  1.06403027]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 190 ] state=[ 0.29028331 -1.33889971  0.06778525  1.06403027], action=1, reward=1.0, next_state=[ 0.26350532 -1.14473728  0.08906585  0.79336909]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 191 ] state=[ 0.26350532 -1.14473728  0.08906585  0.79336909], action=1, reward=1.0, next_state=[ 0.24061057 -0.95094349  0.10493324  0.52998138]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 192 ] state=[ 0.24061057 -0.95094349  0.10493324  0.52998138], action=0, reward=1.0, next_state=[ 0.2215917  -1.14737293  0.11553286  0.85379866]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 193 ] state=[ 0.2215917  -1.14737293  0.11553286  0.85379866], action=0, reward=1.0, next_state=[ 0.19864424 -1.34386405  0.13260884  1.18046162]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 194 ] state=[ 0.19864424 -1.34386405  0.13260884  1.18046162], action=0, reward=1.0, next_state=[ 0.17176696 -1.54043438  0.15621807  1.51160122]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 195 ] state=[ 0.17176696 -1.54043438  0.15621807  1.51160122], action=1, reward=1.0, next_state=[ 0.14095827 -1.34751143  0.18645009  1.27148223]\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 196 ] state=[ 0.14095827 -1.34751143  0.18645009  1.27148223], action=1, reward=-1.0, next_state=[ 0.11400804 -1.15519308  0.21187974  1.04250374]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 119: Exploration_rate=0.01. Score=196.\n",
      "[ episode 120 ] state=[-0.0305309   0.00329186  0.04484428 -0.04593805]\n",
      "[ episode 120 ][ timestamp 1 ] state=[-0.0305309   0.00329186  0.04484428 -0.04593805], action=0, reward=1.0, next_state=[-0.03046507 -0.19244348  0.04392552  0.2605496 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 2 ] state=[-0.03046507 -0.19244348  0.04392552  0.2605496 ], action=1, reward=1.0, next_state=[-0.03431394  0.00202479  0.04913651 -0.01796149]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 3 ] state=[-0.03431394  0.00202479  0.04913651 -0.01796149], action=0, reward=1.0, next_state=[-0.03427344 -0.19376614  0.04877728  0.28981076]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 4 ] state=[-0.03427344 -0.19376614  0.04877728  0.28981076], action=1, reward=1.0, next_state=[-0.03814876  0.00062757  0.0545735   0.01290185]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 5 ] state=[-0.03814876  0.00062757  0.0545735   0.01290185], action=0, reward=1.0, next_state=[-0.03813621 -0.19523285  0.05483153  0.32229173]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 6 ] state=[-0.03813621 -0.19523285  0.05483153  0.32229173], action=1, reward=1.0, next_state=[-0.04204087 -0.00093283  0.06127737  0.04739211]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 7 ] state=[-0.04204087 -0.00093283  0.06127737  0.04739211], action=0, reward=1.0, next_state=[-0.04205953 -0.19687749  0.06222521  0.35876173]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 8 ] state=[-0.04205953 -0.19687749  0.06222521  0.35876173], action=1, reward=1.0, next_state=[-0.04599708 -0.00269277  0.06940044  0.08633079]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 9 ] state=[-0.04599708 -0.00269277  0.06940044  0.08633079], action=0, reward=1.0, next_state=[-0.04605093 -0.19873735  0.07112706  0.40007712]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 10 ] state=[-0.04605093 -0.19873735  0.07112706  0.40007712], action=1, reward=1.0, next_state=[-0.05002568 -0.00469264  0.0791286   0.13063974]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 11 ] state=[-0.05002568 -0.00469264  0.0791286   0.13063974], action=0, reward=1.0, next_state=[-0.05011953 -0.20085368  0.0817414   0.44720014]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 12 ] state=[-0.05011953 -0.20085368  0.0817414   0.44720014], action=1, reward=1.0, next_state=[-0.0541366  -0.00697745  0.0906854   0.18136204]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 13 ] state=[-0.0541366  -0.00697745  0.0906854   0.18136204], action=1, reward=1.0, next_state=[-0.05427615  0.18673771  0.09431264 -0.08139174]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 14 ] state=[-0.05427615  0.18673771  0.09431264 -0.08139174], action=1, reward=1.0, next_state=[-0.0505414   0.38039004  0.09268481 -0.34289247]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 15 ] state=[-0.0505414   0.38039004  0.09268481 -0.34289247], action=1, reward=1.0, next_state=[-0.0429336   0.57407957  0.08582696 -0.60496941]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 16 ] state=[-0.0429336   0.57407957  0.08582696 -0.60496941], action=1, reward=1.0, next_state=[-0.03145201  0.76790306  0.07372757 -0.86943234]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 17 ] state=[-0.03145201  0.76790306  0.07372757 -0.86943234], action=1, reward=1.0, next_state=[-0.01609395  0.96194877  0.05633892 -1.13805389]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 18 ] state=[-0.01609395  0.96194877  0.05633892 -1.13805389], action=0, reward=1.0, next_state=[ 0.00314503  0.76613712  0.03357784 -0.82824755]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 19 ] state=[ 0.00314503  0.76613712  0.03357784 -0.82824755], action=1, reward=1.0, next_state=[ 0.01846777  0.96078429  0.01701289 -1.11018371]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 20 ] state=[ 0.01846777  0.96078429  0.01701289 -1.11018371], action=0, reward=1.0, next_state=[ 0.03768346  0.76544299 -0.00519078 -0.81221261]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 21 ] state=[ 0.03768346  0.76544299 -0.00519078 -0.81221261], action=0, reward=1.0, next_state=[ 0.05299232  0.57039253 -0.02143503 -0.52116694]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 22 ] state=[ 0.05299232  0.57039253 -0.02143503 -0.52116694], action=1, reward=1.0, next_state=[ 0.06440017  0.76580955 -0.03185837 -0.82052654]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 23 ] state=[ 0.06440017  0.76580955 -0.03185837 -0.82052654], action=0, reward=1.0, next_state=[ 0.07971636  0.57113771 -0.0482689  -0.53803174]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 24 ] state=[ 0.07971636  0.57113771 -0.0482689  -0.53803174], action=0, reward=1.0, next_state=[ 0.09113911  0.37672642 -0.05902954 -0.26094001]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 25 ] state=[ 0.09113911  0.37672642 -0.05902954 -0.26094001], action=0, reward=1.0, next_state=[ 0.09867364  0.18249462 -0.06424834  0.01255563]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 26 ] state=[ 0.09867364  0.18249462 -0.06424834  0.01255563], action=1, reward=1.0, next_state=[ 0.10232353  0.37847633 -0.06399723 -0.29968642]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 27 ] state=[ 0.10232353  0.37847633 -0.06399723 -0.29968642], action=0, reward=1.0, next_state=[ 0.10989306  0.18432219 -0.06999095 -0.02785374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 28 ] state=[ 0.10989306  0.18432219 -0.06999095 -0.02785374], action=0, reward=1.0, next_state=[ 0.1135795  -0.00972987 -0.07054803  0.24195114]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 29 ] state=[ 0.1135795  -0.00972987 -0.07054803  0.24195114], action=1, reward=1.0, next_state=[ 0.11338491  0.18632518 -0.06570901 -0.07212384]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 30 ] state=[ 0.11338491  0.18632518 -0.06570901 -0.07212384], action=0, reward=1.0, next_state=[ 0.11711141 -0.00779621 -0.06715148  0.19912531]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 31 ] state=[ 0.11711141 -0.00779621 -0.06715148  0.19912531], action=0, reward=1.0, next_state=[ 0.11695549 -0.20189665 -0.06316898  0.46989207]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 32 ] state=[ 0.11695549 -0.20189665 -0.06316898  0.46989207], action=1, reward=1.0, next_state=[ 0.11291755 -0.00594191 -0.05377114  0.15798688]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 120 ][ timestamp 33 ] state=[ 0.11291755 -0.00594191 -0.05377114  0.15798688], action=1, reward=1.0, next_state=[ 0.11279872  0.18990701 -0.0506114  -0.151163  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 34 ] state=[ 0.11279872  0.18990701 -0.0506114  -0.151163  ], action=1, reward=1.0, next_state=[ 0.11659686  0.38571577 -0.05363466 -0.45937344]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 35 ] state=[ 0.11659686  0.38571577 -0.05363466 -0.45937344], action=0, reward=1.0, next_state=[ 0.12431117  0.19139137 -0.06282213 -0.18406703]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 36 ] state=[ 0.12431117  0.19139137 -0.06282213 -0.18406703], action=0, reward=1.0, next_state=[ 0.128139   -0.00277806 -0.06650347  0.08815502]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 37 ] state=[ 0.128139   -0.00277806 -0.06650347  0.08815502], action=1, reward=1.0, next_state=[ 0.12808344  0.19323099 -0.06474037 -0.22474625]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 38 ] state=[ 0.12808344  0.19323099 -0.06474037 -0.22474625], action=0, reward=1.0, next_state=[ 0.13194806 -0.00090881 -0.06923529  0.04683302]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 39 ] state=[ 0.13194806 -0.00090881 -0.06923529  0.04683302], action=0, reward=1.0, next_state=[ 0.13192988 -0.19497319 -0.06829863  0.31689325]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 40 ] state=[ 0.13192988 -0.19497319 -0.06829863  0.31689325], action=1, reward=1.0, next_state=[ 0.12803042  0.00105173 -0.06196077  0.00347721]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 41 ] state=[ 0.12803042  0.00105173 -0.06196077  0.00347721], action=0, reward=1.0, next_state=[ 0.12805145 -0.1931294  -0.06189122  0.27598517]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 42 ] state=[ 0.12805145 -0.1931294  -0.06189122  0.27598517], action=1, reward=1.0, next_state=[ 0.12418886  0.00281845 -0.05637152 -0.03555825]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 43 ] state=[ 0.12418886  0.00281845 -0.05637152 -0.03555825], action=0, reward=1.0, next_state=[ 0.12424523 -0.19145171 -0.05708268  0.23881965]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 44 ] state=[ 0.12424523 -0.19145171 -0.05708268  0.23881965], action=1, reward=1.0, next_state=[ 0.1204162   0.00443728 -0.05230629 -0.07130845]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 45 ] state=[ 0.1204162   0.00443728 -0.05230629 -0.07130845], action=0, reward=1.0, next_state=[ 0.12050494 -0.18989728 -0.05373246  0.20442368]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 46 ] state=[ 0.12050494 -0.18989728 -0.05373246  0.20442368], action=1, reward=1.0, next_state=[ 0.116707    0.00595027 -0.04964399 -0.1047134 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 47 ] state=[ 0.116707    0.00595027 -0.04964399 -0.1047134 ], action=1, reward=1.0, next_state=[ 0.116826    0.20174721 -0.05173826 -0.41263631]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 48 ] state=[ 0.116826    0.20174721 -0.05173826 -0.41263631], action=0, reward=1.0, next_state=[ 0.12086095  0.00739538 -0.05999098 -0.13670293]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 49 ] state=[ 0.12086095  0.00739538 -0.05999098 -0.13670293], action=1, reward=1.0, next_state=[ 0.12100886  0.203323   -0.06272504 -0.44769245]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 50 ] state=[ 0.12100886  0.203323   -0.06272504 -0.44769245], action=0, reward=1.0, next_state=[ 0.12507532  0.00914183 -0.07167889 -0.17542257]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 51 ] state=[ 0.12507532  0.00914183 -0.07167889 -0.17542257], action=0, reward=1.0, next_state=[ 0.12525815 -0.18488499 -0.07518734  0.09381476]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 52 ] state=[ 0.12525815 -0.18488499 -0.07518734  0.09381476], action=0, reward=1.0, next_state=[ 0.12156045 -0.37885327 -0.07331105  0.36186091]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 53 ] state=[ 0.12156045 -0.37885327 -0.07331105  0.36186091], action=0, reward=1.0, next_state=[ 0.11398339 -0.57286079 -0.06607383  0.63055638]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 54 ] state=[ 0.11398339 -0.57286079 -0.06607383  0.63055638], action=1, reward=1.0, next_state=[ 0.10252617 -0.37688213 -0.0534627   0.31781827]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 55 ] state=[ 0.10252617 -0.37688213 -0.0534627   0.31781827], action=0, reward=1.0, next_state=[ 0.09498853 -0.57120347 -0.04710633  0.59317327]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 56 ] state=[ 0.09498853 -0.57120347 -0.04710633  0.59317327], action=0, reward=1.0, next_state=[ 0.08356446 -0.76563546 -0.03524287  0.8706536 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 57 ] state=[ 0.08356446 -0.76563546 -0.03524287  0.8706536 ], action=1, reward=1.0, next_state=[ 0.06825175 -0.57005232 -0.0178298   0.56710181]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 58 ] state=[ 0.06825175 -0.57005232 -0.0178298   0.56710181], action=1, reward=1.0, next_state=[ 0.0568507  -0.37468486 -0.00648776  0.26885551]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 59 ] state=[ 0.0568507  -0.37468486 -0.00648776  0.26885551], action=1, reward=1.0, next_state=[ 0.04935701 -0.17947092 -0.00111065 -0.02586662]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 60 ] state=[ 0.04935701 -0.17947092 -0.00111065 -0.02586662], action=1, reward=1.0, next_state=[ 0.04576759  0.01566694 -0.00162798 -0.31889976]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 61 ] state=[ 0.04576759  0.01566694 -0.00162798 -0.31889976], action=1, reward=1.0, next_state=[ 0.04608093  0.21081204 -0.00800598 -0.61209565]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 62 ] state=[ 0.04608093  0.21081204 -0.00800598 -0.61209565], action=1, reward=1.0, next_state=[ 0.05029717  0.40604496 -0.02024789 -0.90728938]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 63 ] state=[ 0.05029717  0.40604496 -0.02024789 -0.90728938], action=1, reward=1.0, next_state=[ 0.05841807  0.60143509 -0.03839368 -1.20626698]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 64 ] state=[ 0.05841807  0.60143509 -0.03839368 -1.20626698], action=1, reward=1.0, next_state=[ 0.07044677  0.79703155 -0.06251902 -1.51073042]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 65 ] state=[ 0.07044677  0.79703155 -0.06251902 -1.51073042], action=1, reward=1.0, next_state=[ 0.0863874   0.99285273 -0.09273363 -1.82225695]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 66 ] state=[ 0.0863874   0.99285273 -0.09273363 -1.82225695], action=1, reward=1.0, next_state=[ 0.10624445  1.18887403 -0.12917877 -2.14225016]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 67 ] state=[ 0.10624445  1.18887403 -0.12917877 -2.14225016], action=1, reward=1.0, next_state=[ 0.13002193  1.38501312 -0.17202377 -2.47188048]\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 68 ] state=[ 0.13002193  1.38501312 -0.17202377 -2.47188048], action=1, reward=-1.0, next_state=[ 0.1577222   1.58111291 -0.22146138 -2.81201455]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 120: Exploration_rate=0.01. Score=68.\n",
      "[ episode 121 ] state=[-0.04014437 -0.02598301 -0.02450625  0.04399546]\n",
      "[ episode 121 ][ timestamp 1 ] state=[-0.04014437 -0.02598301 -0.02450625  0.04399546], action=1, reward=1.0, next_state=[-0.04066403  0.16948162 -0.02362634 -0.25631757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 2 ] state=[-0.04066403  0.16948162 -0.02362634 -0.25631757], action=1, reward=1.0, next_state=[-0.0372744   0.36493279 -0.02875269 -0.556358  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 3 ] state=[-0.0372744   0.36493279 -0.02875269 -0.556358  ], action=1, reward=1.0, next_state=[-0.02997574  0.56044636 -0.03987985 -0.85795927]\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 4 ] state=[-0.02997574  0.56044636 -0.03987985 -0.85795927], action=1, reward=1.0, next_state=[-0.01876682  0.75608825 -0.05703904 -1.16291035]\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 5 ] state=[-0.01876682  0.75608825 -0.05703904 -1.16291035], action=1, reward=1.0, next_state=[-0.00364505  0.95190468 -0.08029725 -1.47291768]\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 6 ] state=[-0.00364505  0.95190468 -0.08029725 -1.47291768], action=1, reward=1.0, next_state=[ 0.01539304  1.14791122 -0.1097556  -1.7895622 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 7 ] state=[ 0.01539304  1.14791122 -0.1097556  -1.7895622 ], action=1, reward=1.0, next_state=[ 0.03835127  1.34408016 -0.14554684 -2.11424846]\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 8 ] state=[ 0.03835127  1.34408016 -0.14554684 -2.11424846], action=1, reward=1.0, next_state=[ 0.06523287  1.54032557 -0.18783181 -2.448144  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 9 ] state=[ 0.06523287  1.54032557 -0.18783181 -2.448144  ], action=1, reward=-1.0, next_state=[ 0.09603938  1.73648608 -0.23679469 -2.79210789]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Ended! ] Episode 121: Exploration_rate=0.01. Score=9.\n",
      "[ episode 122 ] state=[ 0.00185129 -0.04566346  0.04989536  0.00481088]\n",
      "[ episode 122 ][ timestamp 1 ] state=[ 0.00185129 -0.04566346  0.04989536  0.00481088], action=1, reward=1.0, next_state=[ 0.00093802  0.14870873  0.04999157 -0.27172141]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 2 ] state=[ 0.00093802  0.14870873  0.04999157 -0.27172141], action=0, reward=1.0, next_state=[ 0.0039122  -0.04708961  0.04455715  0.03630057]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 3 ] state=[ 0.0039122  -0.04708961  0.04455715  0.03630057], action=1, reward=1.0, next_state=[ 0.0029704   0.147366    0.04528316 -0.24199788]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 4 ] state=[ 0.0029704   0.147366    0.04528316 -0.24199788], action=1, reward=1.0, next_state=[ 0.00591772  0.34181284  0.0404432  -0.52006045]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 5 ] state=[ 0.00591772  0.34181284  0.0404432  -0.52006045], action=1, reward=1.0, next_state=[ 0.01275398  0.53634281  0.03004199 -0.79972974]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 6 ] state=[ 0.01275398  0.53634281  0.03004199 -0.79972974], action=1, reward=1.0, next_state=[ 0.02348084  0.73104007  0.0140474  -1.08281282]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 7 ] state=[ 0.02348084  0.73104007  0.0140474  -1.08281282], action=0, reward=1.0, next_state=[ 0.03810164  0.53573557 -0.00760886 -0.78575518]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 8 ] state=[ 0.03810164  0.53573557 -0.00760886 -0.78575518], action=0, reward=1.0, next_state=[ 0.04881635  0.34071898 -0.02332396 -0.49547575]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 9 ] state=[ 0.04881635  0.34071898 -0.02332396 -0.49547575], action=0, reward=1.0, next_state=[ 0.05563073  0.14593357 -0.03323348 -0.21023373]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 10 ] state=[ 0.05563073  0.14593357 -0.03323348 -0.21023373], action=0, reward=1.0, next_state=[ 0.0585494  -0.04869782 -0.03743815  0.07178331]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 11 ] state=[ 0.0585494  -0.04869782 -0.03743815  0.07178331], action=0, reward=1.0, next_state=[ 0.05757545 -0.2432636  -0.03600249  0.35242322]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 12 ] state=[ 0.05757545 -0.2432636  -0.03600249  0.35242322], action=0, reward=1.0, next_state=[ 0.05271017 -0.43785558 -0.02895402  0.6335396 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 13 ] state=[ 0.05271017 -0.43785558 -0.02895402  0.6335396 ], action=0, reward=1.0, next_state=[ 0.04395306 -0.63256192 -0.01628323  0.9169654 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 14 ] state=[ 0.04395306 -0.63256192 -0.01628323  0.9169654 ], action=0, reward=1.0, next_state=[ 0.03130182 -0.82745996  0.00205608  1.20448665]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 15 ] state=[ 0.03130182 -0.82745996  0.00205608  1.20448665], action=0, reward=1.0, next_state=[ 0.01475262 -1.02260843  0.02614581  1.49781322]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 16 ] state=[ 0.01475262 -1.02260843  0.02614581  1.49781322], action=1, reward=1.0, next_state=[-0.00569954 -0.8278138   0.05610207  1.21340713]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 17 ] state=[-0.00569954 -0.8278138   0.05610207  1.21340713], action=1, reward=1.0, next_state=[-0.02225582 -0.63345891  0.08037022  0.93881883]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 18 ] state=[-0.02225582 -0.63345891  0.08037022  0.93881883], action=1, reward=1.0, next_state=[-0.034925   -0.43950707  0.09914659  0.67243358]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 19 ] state=[-0.034925   -0.43950707  0.09914659  0.67243358], action=1, reward=1.0, next_state=[-0.04371514 -0.24589287  0.11259526  0.4125399 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 20 ] state=[-0.04371514 -0.24589287  0.11259526  0.4125399 ], action=1, reward=1.0, next_state=[-0.048633   -0.05253204  0.12084606  0.15736835]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 21 ] state=[-0.048633   -0.05253204  0.12084606  0.15736835], action=1, reward=1.0, next_state=[-0.04968364  0.14067109  0.12399343 -0.09488047]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 22 ] state=[-0.04968364  0.14067109  0.12399343 -0.09488047], action=1, reward=1.0, next_state=[-0.04687022  0.33381788  0.12209582 -0.34601565]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 23 ] state=[-0.04687022  0.33381788  0.12209582 -0.34601565], action=1, reward=1.0, next_state=[-0.04019386  0.52701069  0.11517551 -0.59784049]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 24 ] state=[-0.04019386  0.52701069  0.11517551 -0.59784049], action=1, reward=1.0, next_state=[-0.02965365  0.72034867  0.1032187  -0.85213926]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 25 ] state=[-0.02965365  0.72034867  0.1032187  -0.85213926], action=1, reward=1.0, next_state=[-0.01524667  0.91392343  0.08617591 -1.11066356]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 26 ] state=[-0.01524667  0.91392343  0.08617591 -1.11066356], action=1, reward=1.0, next_state=[ 0.0030318   1.10781408  0.06396264 -1.37511593]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 27 ] state=[ 0.0030318   1.10781408  0.06396264 -1.37511593], action=0, reward=1.0, next_state=[ 0.02518808  0.91195379  0.03646032 -1.06313406]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 28 ] state=[ 0.02518808  0.91195379  0.03646032 -1.06313406], action=1, reward=1.0, next_state=[ 0.04342715  1.10657458  0.01519764 -1.34415427]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 29 ] state=[ 0.04342715  1.10657458  0.01519764 -1.34415427], action=0, reward=1.0, next_state=[ 0.06555865  0.91126479 -0.01168544 -1.04675549]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 30 ] state=[ 0.06555865  0.91126479 -0.01168544 -1.04675549], action=0, reward=1.0, next_state=[ 0.08378394  0.71629987 -0.03262055 -0.75776351]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 31 ] state=[ 0.08378394  0.71629987 -0.03262055 -0.75776351], action=0, reward=1.0, next_state=[ 0.09810994  0.52164229 -0.04777582 -0.47552123]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 32 ] state=[ 0.09810994  0.52164229 -0.04777582 -0.47552123], action=0, reward=1.0, next_state=[ 0.10854278  0.32722639 -0.05728625 -0.19827087]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 33 ] state=[ 0.10854278  0.32722639 -0.05728625 -0.19827087], action=0, reward=1.0, next_state=[ 0.11508731  0.13296861 -0.06125167  0.07580486]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 34 ] state=[ 0.11508731  0.13296861 -0.06125167  0.07580486], action=0, reward=1.0, next_state=[ 0.11774668 -0.0612242  -0.05973557  0.34855108]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 35 ] state=[ 0.11774668 -0.0612242  -0.05973557  0.34855108], action=0, reward=1.0, next_state=[ 0.1165222  -0.25544792 -0.05276455  0.62181521]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 36 ] state=[ 0.1165222  -0.25544792 -0.05276455  0.62181521], action=0, reward=1.0, next_state=[ 0.11141324 -0.44979489 -0.04032824  0.89742437]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 37 ] state=[ 0.11141324 -0.44979489 -0.04032824  0.89742437], action=1, reward=1.0, next_state=[ 0.10241734 -0.25415015 -0.02237976  0.59234258]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 38 ] state=[ 0.10241734 -0.25415015 -0.02237976  0.59234258], action=1, reward=1.0, next_state=[ 0.09733434 -0.05872217 -0.0105329   0.29269491]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 39 ] state=[ 0.09733434 -0.05872217 -0.0105329   0.29269491], action=1, reward=1.0, next_state=[ 0.0961599   0.13654836 -0.00467901 -0.00329125]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 40 ] state=[ 0.0961599   0.13654836 -0.00467901 -0.00329125], action=0, reward=1.0, next_state=[ 0.09889087 -0.05850617 -0.00474483  0.28791173]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 41 ] state=[ 0.09889087 -0.05850617 -0.00474483  0.28791173], action=1, reward=1.0, next_state=[ 0.09772074  0.13668312  0.0010134  -0.00626389]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 122 ][ timestamp 42 ] state=[ 0.09772074  0.13668312  0.0010134  -0.00626389], action=0, reward=1.0, next_state=[ 0.1004544  -0.05845335  0.00088813  0.2867386 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 43 ] state=[ 0.1004544  -0.05845335  0.00088813  0.2867386 ], action=1, reward=1.0, next_state=[ 0.09928534  0.13665592  0.0066229  -0.00566408]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 44 ] state=[ 0.09928534  0.13665592  0.0066229  -0.00566408], action=1, reward=1.0, next_state=[ 0.10201846  0.33168227  0.00650962 -0.29625007]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 45 ] state=[ 0.10201846  0.33168227  0.00650962 -0.29625007], action=1, reward=1.0, next_state=[ 1.08652101e-01  5.26710818e-01  5.84614748e-04 -5.86872881e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 46 ] state=[ 1.08652101e-01  5.26710818e-01  5.84614748e-04 -5.86872881e-01], action=0, reward=1.0, next_state=[ 0.11918632  0.33158068 -0.01115284 -0.29400585]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 47 ] state=[ 0.11918632  0.33158068 -0.01115284 -0.29400585], action=1, reward=1.0, next_state=[ 0.12581793  0.52685985 -0.01703296 -0.59018526]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 48 ] state=[ 0.12581793  0.52685985 -0.01703296 -0.59018526], action=1, reward=1.0, next_state=[ 0.13635513  0.7222161  -0.02883667 -0.88818457]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 49 ] state=[ 0.13635513  0.7222161  -0.02883667 -0.88818457], action=1, reward=1.0, next_state=[ 0.15079945  0.9177173  -0.04660036 -1.18979125]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 50 ] state=[ 0.15079945  0.9177173  -0.04660036 -1.18979125], action=0, reward=1.0, next_state=[ 0.1691538   0.72322924 -0.07039618 -0.91207142]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 51 ] state=[ 0.1691538   0.72322924 -0.07039618 -0.91207142], action=0, reward=1.0, next_state=[ 0.18361838  0.52912673 -0.08863761 -0.64231816]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 52 ] state=[ 0.18361838  0.52912673 -0.08863761 -0.64231816], action=0, reward=1.0, next_state=[ 0.19420092  0.3353448  -0.10148397 -0.37881172]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 53 ] state=[ 0.19420092  0.3353448  -0.10148397 -0.37881172], action=0, reward=1.0, next_state=[ 0.20090781  0.14179947 -0.10906021 -0.11977254]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 54 ] state=[ 0.20090781  0.14179947 -0.10906021 -0.11977254], action=0, reward=1.0, next_state=[ 0.2037438  -0.05160464 -0.11145566  0.13660988]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 55 ] state=[ 0.2037438  -0.05160464 -0.11145566  0.13660988], action=0, reward=1.0, next_state=[ 0.20271171 -0.24496835 -0.10872346  0.39215562]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 56 ] state=[ 0.20271171 -0.24496835 -0.10872346  0.39215562], action=1, reward=1.0, next_state=[ 0.19781234 -0.04848496 -0.10088035  0.067269  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 57 ] state=[ 0.19781234 -0.04848496 -0.10088035  0.067269  ], action=1, reward=1.0, next_state=[ 0.19684264  0.14792776 -0.09953497 -0.25546075]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 58 ] state=[ 0.19684264  0.14792776 -0.09953497 -0.25546075], action=0, reward=1.0, next_state=[ 0.1998012  -0.04564259 -0.10464418  0.00424268]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 59 ] state=[ 0.1998012  -0.04564259 -0.10464418  0.00424268], action=0, reward=1.0, next_state=[ 0.19888835 -0.23912027 -0.10455933  0.26216237]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 60 ] state=[ 0.19888835 -0.23912027 -0.10455933  0.26216237], action=0, reward=1.0, next_state=[ 0.19410594 -0.43260637 -0.09931608  0.52012203]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 61 ] state=[ 0.19410594 -0.43260637 -0.09931608  0.52012203], action=0, reward=1.0, next_state=[ 0.18545381 -0.62620024 -0.08891364  0.77993089]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 62 ] state=[ 0.18545381 -0.62620024 -0.08891364  0.77993089], action=0, reward=1.0, next_state=[ 0.17292981 -0.81999464 -0.07331502  1.04336801]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 63 ] state=[ 0.17292981 -0.81999464 -0.07331502  1.04336801], action=0, reward=1.0, next_state=[ 0.15652992 -1.01407051 -0.05244766  1.31216448]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 64 ] state=[ 0.15652992 -1.01407051 -0.05244766  1.31216448], action=1, reward=1.0, next_state=[ 0.1362485  -0.81832518 -0.02620437  1.00353769]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 65 ] state=[ 0.1362485  -0.81832518 -0.02620437  1.00353769], action=1, reward=1.0, next_state=[ 0.119882   -0.62286316 -0.00613362  0.7027421 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 66 ] state=[ 0.119882   -0.62286316 -0.00613362  0.7027421 ], action=1, reward=1.0, next_state=[ 0.10742474 -0.42765674  0.00792122  0.4081347 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 67 ] state=[ 0.10742474 -0.42765674  0.00792122  0.4081347 ], action=1, reward=1.0, next_state=[ 0.0988716  -0.23264799  0.01608392  0.11795958]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 68 ] state=[ 0.0988716  -0.23264799  0.01608392  0.11795958], action=1, reward=1.0, next_state=[ 0.09421864 -0.03776014  0.01844311 -0.16960593]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 69 ] state=[ 0.09421864 -0.03776014  0.01844311 -0.16960593], action=0, reward=1.0, next_state=[ 0.09346344 -0.23314115  0.01505099  0.12883772]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 70 ] state=[ 0.09346344 -0.23314115  0.01505099  0.12883772], action=1, reward=1.0, next_state=[ 0.08880062 -0.038238    0.01762774 -0.15905906]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 71 ] state=[ 0.08880062 -0.038238    0.01762774 -0.15905906], action=1, reward=1.0, next_state=[ 0.08803586  0.1566272   0.01444656 -0.44612916]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 72 ] state=[ 0.08803586  0.1566272   0.01444656 -0.44612916], action=0, reward=1.0, next_state=[ 0.0911684  -0.03869612  0.00552398 -0.14892761]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 73 ] state=[ 0.0911684  -0.03869612  0.00552398 -0.14892761], action=0, reward=1.0, next_state=[ 0.09039448 -0.23389674  0.00254543  0.14549288]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 74 ] state=[ 0.09039448 -0.23389674  0.00254543  0.14549288], action=0, reward=1.0, next_state=[ 0.08571654 -0.42905505  0.00545528  0.43897776]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 75 ] state=[ 0.08571654 -0.42905505  0.00545528  0.43897776], action=1, reward=1.0, next_state=[ 0.07713544 -0.23401073  0.01423484  0.14801948]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 76 ] state=[ 0.07713544 -0.23401073  0.01423484  0.14801948], action=1, reward=1.0, next_state=[ 0.07245523 -0.03909549  0.01719523 -0.14013886]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 77 ] state=[ 0.07245523 -0.03909549  0.01719523 -0.14013886], action=1, reward=1.0, next_state=[ 0.07167332  0.15577602  0.01439245 -0.42734777]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 78 ] state=[ 0.07167332  0.15577602  0.01439245 -0.42734777], action=0, reward=1.0, next_state=[ 0.07478884 -0.03954679  0.0058455  -0.13016267]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 79 ] state=[ 0.07478884 -0.03954679  0.0058455  -0.13016267], action=0, reward=1.0, next_state=[ 0.0739979  -0.23475198  0.00324224  0.16435869]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 80 ] state=[ 0.0739979  -0.23475198  0.00324224  0.16435869], action=1, reward=1.0, next_state=[ 0.06930286 -0.0396766   0.00652942 -0.12729963]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 81 ] state=[ 0.06930286 -0.0396766   0.00652942 -0.12729963], action=0, reward=1.0, next_state=[ 0.06850933 -0.23489147  0.00398342  0.16743608]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 82 ] state=[ 0.06850933 -0.23489147  0.00398342  0.16743608], action=0, reward=1.0, next_state=[ 0.0638115  -0.43007022  0.00733215  0.46137299]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 83 ] state=[ 0.0638115  -0.43007022  0.00733215  0.46137299], action=0, reward=1.0, next_state=[ 0.0552101  -0.62529503  0.01655961  0.75635797]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 84 ] state=[ 0.0552101  -0.62529503  0.01655961  0.75635797], action=1, reward=1.0, next_state=[ 0.0427042  -0.43040519  0.03168676  0.46893159]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 85 ] state=[ 0.0427042  -0.43040519  0.03168676  0.46893159], action=1, reward=1.0, next_state=[ 0.03409609 -0.23574487  0.0410654   0.18640204]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 122 ][ timestamp 86 ] state=[ 0.03409609 -0.23574487  0.0410654   0.18640204], action=1, reward=1.0, next_state=[ 0.0293812  -0.04123379  0.04479344 -0.09304877]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 87 ] state=[ 0.0293812  -0.04123379  0.04479344 -0.09304877], action=1, reward=1.0, next_state=[ 0.02855652  0.15321847  0.04293246 -0.37126972]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 88 ] state=[ 0.02855652  0.15321847  0.04293246 -0.37126972], action=1, reward=1.0, next_state=[ 0.03162089  0.34770502  0.03550707 -0.65011246]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 89 ] state=[ 0.03162089  0.34770502  0.03550707 -0.65011246], action=0, reward=1.0, next_state=[ 0.03857499  0.15210695  0.02250482 -0.3464634 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 90 ] state=[ 0.03857499  0.15210695  0.02250482 -0.3464634 ], action=0, reward=1.0, next_state=[ 0.04161713 -0.04332777  0.01557555 -0.0467697 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 91 ] state=[ 0.04161713 -0.04332777  0.01557555 -0.0467697 ], action=1, reward=1.0, next_state=[ 0.04075057  0.15156741  0.01464016 -0.33449799]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 92 ] state=[ 0.04075057  0.15156741  0.01464016 -0.33449799], action=0, reward=1.0, next_state=[ 0.04378192 -0.04375981  0.0079502  -0.0372345 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 93 ] state=[ 0.04378192 -0.04375981  0.0079502  -0.0372345 ], action=0, reward=1.0, next_state=[ 0.04290673 -0.23899486  0.00720551  0.25794615]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 94 ] state=[ 0.04290673 -0.23899486  0.00720551  0.25794615], action=1, reward=1.0, next_state=[ 0.03812683 -0.04397651  0.01236443 -0.03245537]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 95 ] state=[ 0.03812683 -0.04397651  0.01236443 -0.03245537], action=1, reward=1.0, next_state=[ 0.0372473   0.15096596  0.01171532 -0.32121168]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 96 ] state=[ 0.0372473   0.15096596  0.01171532 -0.32121168], action=0, reward=1.0, next_state=[ 0.04026662 -0.04432085  0.00529109 -0.02485734]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 97 ] state=[ 0.04026662 -0.04432085  0.00529109 -0.02485734], action=1, reward=1.0, next_state=[ 0.0393802   0.15072483  0.00479394 -0.31586619]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 98 ] state=[ 0.0393802   0.15072483  0.00479394 -0.31586619], action=1, reward=1.0, next_state=[ 0.0423947   0.34577817 -0.00152338 -0.60703342]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 99 ] state=[ 0.0423947   0.34577817 -0.00152338 -0.60703342], action=0, reward=1.0, next_state=[ 0.04931026  0.15067755 -0.01366405 -0.31483071]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 100 ] state=[ 0.04931026  0.15067755 -0.01366405 -0.31483071], action=0, reward=1.0, next_state=[ 0.05232381 -0.04424712 -0.01996066 -0.0264881 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 101 ] state=[ 0.05232381 -0.04424712 -0.01996066 -0.0264881 ], action=0, reward=1.0, next_state=[ 0.05143887 -0.23907722 -0.02049043  0.25983078]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 102 ] state=[ 0.05143887 -0.23907722 -0.02049043  0.25983078], action=1, reward=1.0, next_state=[ 0.04665732 -0.04366884 -0.01529381 -0.03924402]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 103 ] state=[ 0.04665732 -0.04366884 -0.01529381 -0.03924402], action=1, reward=1.0, next_state=[ 0.04578395  0.15166905 -0.01607869 -0.33671279]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 104 ] state=[ 0.04578395  0.15166905 -0.01607869 -0.33671279], action=0, reward=1.0, next_state=[ 0.04881733 -0.04322045 -0.02281295 -0.04914327]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 105 ] state=[ 0.04881733 -0.04322045 -0.02281295 -0.04914327], action=0, reward=1.0, next_state=[ 0.04795292 -0.23800798 -0.02379581  0.23625557]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 106 ] state=[ 0.04795292 -0.23800798 -0.02379581  0.23625557], action=1, reward=1.0, next_state=[ 0.04319276 -0.04255428 -0.0190707  -0.06383728]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 107 ] state=[ 0.04319276 -0.04255428 -0.0190707  -0.06383728], action=1, reward=1.0, next_state=[ 0.04234168  0.15283583 -0.02034745 -0.3624756 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 108 ] state=[ 0.04234168  0.15283583 -0.02034745 -0.3624756 ], action=0, reward=1.0, next_state=[ 0.04539839 -0.04199109 -0.02759696 -0.07627744]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 109 ] state=[ 0.04539839 -0.04199109 -0.02759696 -0.07627744], action=1, reward=1.0, next_state=[ 0.04455857  0.15351539 -0.02912251 -0.37753797]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 110 ] state=[ 0.04455857  0.15351539 -0.02912251 -0.37753797], action=0, reward=1.0, next_state=[ 0.04762888 -0.04118111 -0.03667327 -0.09417787]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 111 ] state=[ 0.04762888 -0.04118111 -0.03667327 -0.09417787], action=1, reward=1.0, next_state=[ 0.04680526  0.15444676 -0.03855682 -0.39820189]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 112 ] state=[ 0.04680526  0.15444676 -0.03855682 -0.39820189], action=0, reward=1.0, next_state=[ 0.04989419 -0.04010759 -0.04652086 -0.11792016]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 113 ] state=[ 0.04989419 -0.04010759 -0.04652086 -0.11792016], action=0, reward=1.0, next_state=[ 0.04909204 -0.23453319 -0.04887926  0.15973052]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 114 ] state=[ 0.04909204 -0.23453319 -0.04887926  0.15973052], action=1, reward=1.0, next_state=[ 0.04440137 -0.03874677 -0.04568465 -0.14796314]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 115 ] state=[ 0.04440137 -0.03874677 -0.04568465 -0.14796314], action=1, reward=1.0, next_state=[ 0.04362644  0.15699861 -0.04864392 -0.45470147]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 116 ] state=[ 0.04362644  0.15699861 -0.04864392 -0.45470147], action=0, reward=1.0, next_state=[ 0.04676641 -0.037403   -0.05773795 -0.17773967]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 117 ] state=[ 0.04676641 -0.037403   -0.05773795 -0.17773967], action=0, reward=1.0, next_state=[ 0.04601835 -0.23165319 -0.06129274  0.09618455]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 118 ] state=[ 0.04601835 -0.23165319 -0.06129274  0.09618455], action=0, reward=1.0, next_state=[ 0.04138529 -0.42584555 -0.05936905  0.36891733]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 119 ] state=[ 0.04138529 -0.42584555 -0.05936905  0.36891733], action=0, reward=1.0, next_state=[ 0.03286838 -0.62007591 -0.0519907   0.64230531]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 120 ] state=[ 0.03286838 -0.62007591 -0.0519907   0.64230531], action=1, reward=1.0, next_state=[ 0.02046686 -0.42426929 -0.0391446   0.33371386]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 121 ] state=[ 0.02046686 -0.42426929 -0.0391446   0.33371386], action=1, reward=1.0, next_state=[ 0.01198147 -0.22861269 -0.03247032  0.02894823]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 122 ] state=[ 0.01198147 -0.22861269 -0.03247032  0.02894823], action=1, reward=1.0, next_state=[ 0.00740922 -0.03304051 -0.03189135 -0.27380002]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 123 ] state=[ 0.00740922 -0.03304051 -0.03189135 -0.27380002], action=0, reward=1.0, next_state=[ 0.00674841 -0.22769325 -0.03736735  0.00865616]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 124 ] state=[ 0.00674841 -0.22769325 -0.03736735  0.00865616], action=0, reward=1.0, next_state=[ 0.00219454 -0.42225993 -0.03719423  0.28931901]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 125 ] state=[ 0.00219454 -0.42225993 -0.03719423  0.28931901], action=0, reward=1.0, next_state=[-0.00625065 -0.6168323  -0.03140785  0.57004313]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 126 ] state=[-0.00625065 -0.6168323  -0.03140785  0.57004313], action=1, reward=1.0, next_state=[-0.0185873  -0.42128428 -0.02000699  0.26763336]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 127 ] state=[-0.0185873  -0.42128428 -0.02000699  0.26763336], action=1, reward=1.0, next_state=[-0.02701299 -0.2258826  -0.01465432 -0.03129217]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 122 ][ timestamp 128 ] state=[-0.02701299 -0.2258826  -0.01465432 -0.03129217], action=1, reward=1.0, next_state=[-0.03153064 -0.03055359 -0.01528016 -0.32856243]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 129 ] state=[-0.03153064 -0.03055359 -0.01528016 -0.32856243], action=1, reward=1.0, next_state=[-0.03214171  0.16478252 -0.02185141 -0.62602458]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 130 ] state=[-0.03214171  0.16478252 -0.02185141 -0.62602458], action=0, reward=1.0, next_state=[-0.02884606 -0.0300277  -0.0343719  -0.34030282]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 131 ] state=[-0.02884606 -0.0300277  -0.0343719  -0.34030282], action=1, reward=1.0, next_state=[-0.02944661  0.16556602 -0.04117796 -0.64362346]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 132 ] state=[-0.02944661  0.16556602 -0.04117796 -0.64362346], action=0, reward=1.0, next_state=[-0.02613529 -0.02895856 -0.05405043 -0.36418683]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 133 ] state=[-0.02613529 -0.02895856 -0.05405043 -0.36418683], action=1, reward=1.0, next_state=[-0.02671446  0.16688823 -0.06133417 -0.67341109]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 134 ] state=[-0.02671446  0.16688823 -0.06133417 -0.67341109], action=0, reward=1.0, next_state=[-0.0233767  -0.02733003 -0.07480239 -0.40065244]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 135 ] state=[-0.0233767  -0.02733003 -0.07480239 -0.40065244], action=0, reward=1.0, next_state=[-0.0239233  -0.22131563 -0.08281544 -0.13245913]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 136 ] state=[-0.0239233  -0.22131563 -0.08281544 -0.13245913], action=0, reward=1.0, next_state=[-0.02834961 -0.41515967 -0.08546462  0.13299049]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 137 ] state=[-0.02834961 -0.41515967 -0.08546462  0.13299049], action=1, reward=1.0, next_state=[-0.03665281 -0.21892409 -0.08280481 -0.18538455]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 138 ] state=[-0.03665281 -0.21892409 -0.08280481 -0.18538455], action=0, reward=1.0, next_state=[-0.04103129 -0.41276966 -0.0865125   0.08007072]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 139 ] state=[-0.04103129 -0.41276966 -0.0865125   0.08007072], action=0, reward=1.0, next_state=[-0.04928668 -0.60655176 -0.08491109  0.34425383]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 140 ] state=[-0.04928668 -0.60655176 -0.08491109  0.34425383], action=1, reward=1.0, next_state=[-0.06141772 -0.41033098 -0.07802601  0.0260492 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 141 ] state=[-0.06141772 -0.41033098 -0.07802601  0.0260492 ], action=0, reward=1.0, next_state=[-0.06962434 -0.6042523  -0.07750503  0.2931298 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 142 ] state=[-0.06962434 -0.6042523  -0.07750503  0.2931298 ], action=1, reward=1.0, next_state=[-0.08170938 -0.40811579 -0.07164243 -0.02295544]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 143 ] state=[-0.08170938 -0.40811579 -0.07164243 -0.02295544], action=0, reward=1.0, next_state=[-0.0898717  -0.60214109 -0.07210154  0.24629107]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 144 ] state=[-0.0898717  -0.60214109 -0.07210154  0.24629107], action=1, reward=1.0, next_state=[-0.10191452 -0.40606739 -0.06717572 -0.0682348 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 145 ] state=[-0.10191452 -0.40606739 -0.06717572 -0.0682348 ], action=0, reward=1.0, next_state=[-0.11003587 -0.60016516 -0.06854041  0.20252036]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 146 ] state=[-0.11003587 -0.60016516 -0.06854041  0.20252036], action=1, reward=1.0, next_state=[-0.12203917 -0.40413333 -0.06449001 -0.11097207]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 147 ] state=[-0.12203917 -0.40413333 -0.06449001 -0.11097207], action=0, reward=1.0, next_state=[-0.13012184 -0.59827474 -0.06670945  0.16068776]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 148 ] state=[-0.13012184 -0.59827474 -0.06670945  0.16068776], action=1, reward=1.0, next_state=[-0.14208733 -0.40226432 -0.06349569 -0.15227193]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 149 ] state=[-0.14208733 -0.40226432 -0.06349569 -0.15227193], action=1, reward=1.0, next_state=[-0.15013262 -0.20629336 -0.06654113 -0.46429119]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 150 ] state=[-0.15013262 -0.20629336 -0.06654113 -0.46429119], action=0, reward=1.0, next_state=[-0.15425849 -0.40041502 -0.07582696 -0.19330177]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 151 ] state=[-0.15425849 -0.40041502 -0.07582696 -0.19330177], action=0, reward=1.0, next_state=[-0.16226679 -0.59437498 -0.07969299  0.0745304 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 152 ] state=[-0.16226679 -0.59437498 -0.07969299  0.0745304 ], action=0, reward=1.0, next_state=[-0.17415429 -0.78826937 -0.07820238  0.34104396]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 153 ] state=[-0.17415429 -0.78826937 -0.07820238  0.34104396], action=0, reward=1.0, next_state=[-0.18991967 -0.98219661 -0.0713815   0.60807772]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 154 ] state=[-0.18991967 -0.98219661 -0.0713815   0.60807772], action=1, reward=1.0, next_state=[-0.20956361 -0.78615311 -0.05921995  0.29379298]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 155 ] state=[-0.20956361 -0.78615311 -0.05921995  0.29379298], action=0, reward=1.0, next_state=[-0.22528667 -0.98038295 -0.05334409  0.56722652]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 156 ] state=[-0.22528667 -0.98038295 -0.05334409  0.56722652], action=0, reward=1.0, next_state=[-0.24489433 -1.17471763 -0.04199956  0.84263817]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 157 ] state=[-0.24489433 -1.17471763 -0.04199956  0.84263817], action=1, reward=1.0, next_state=[-0.26838868 -0.97904838 -0.0251468   0.53704888]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 158 ] state=[-0.26838868 -0.97904838 -0.0251468   0.53704888], action=0, reward=1.0, next_state=[-0.28796965 -1.17380791 -0.01440582  0.82170343]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 159 ] state=[-0.28796965 -1.17380791 -0.01440582  0.82170343], action=1, reward=1.0, next_state=[-0.31144581 -0.97849184  0.00202825  0.52452456]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 160 ] state=[-0.31144581 -0.97849184  0.00202825  0.52452456], action=1, reward=1.0, next_state=[-0.33101564 -0.78339849  0.01251874  0.23248144]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 161 ] state=[-0.33101564 -0.78339849  0.01251874  0.23248144], action=1, reward=1.0, next_state=[-0.34668361 -0.58845763  0.01716837 -0.05622652]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 162 ] state=[-0.34668361 -0.58845763  0.01716837 -0.05622652], action=1, reward=1.0, next_state=[-0.35845276 -0.393586    0.01604384 -0.34344363]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 163 ] state=[-0.35845276 -0.393586    0.01604384 -0.34344363], action=0, reward=1.0, next_state=[-0.36632448 -0.58893248  0.00917497 -0.04574494]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 164 ] state=[-0.36632448 -0.58893248  0.00917497 -0.04574494], action=0, reward=1.0, next_state=[-0.37810313 -0.78418478  0.00826007  0.24981859]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 165 ] state=[-0.37810313 -0.78418478  0.00826007  0.24981859], action=0, reward=1.0, next_state=[-0.39378683 -0.97942371  0.01325644  0.54509543]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 166 ] state=[-0.39378683 -0.97942371  0.01325644  0.54509543], action=0, reward=1.0, next_state=[-0.4133753  -1.1747294   0.02415835  0.84192551]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 167 ] state=[-0.4133753  -1.1747294   0.02415835  0.84192551], action=1, reward=1.0, next_state=[-0.43686989 -0.9799454   0.04099686  0.55693661]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 168 ] state=[-0.43686989 -0.9799454   0.04099686  0.55693661], action=1, reward=1.0, next_state=[-0.4564688  -0.78542225  0.05213559  0.27744676]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 169 ] state=[-0.4564688  -0.78542225  0.05213559  0.27744676], action=1, reward=1.0, next_state=[-0.47217725 -0.59108135  0.05768453  0.00165243]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 170 ] state=[-0.47217725 -0.59108135  0.05768453  0.00165243], action=1, reward=1.0, next_state=[-0.48399887 -0.39683208  0.05771758 -0.27228698]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 171 ] state=[-0.48399887 -0.39683208  0.05771758 -0.27228698], action=0, reward=1.0, next_state=[-0.49193551 -0.59272808  0.05227184  0.03802726]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 172 ] state=[-0.49193551 -0.59272808  0.05227184  0.03802726], action=1, reward=1.0, next_state=[-0.50379008 -0.39839317  0.05303238 -0.23771605]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 173 ] state=[-0.50379008 -0.39839317  0.05303238 -0.23771605], action=0, reward=1.0, next_state=[-0.51175794 -0.59423107  0.04827806  0.07121203]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 122 ][ timestamp 174 ] state=[-0.51175794 -0.59423107  0.04827806  0.07121203], action=0, reward=1.0, next_state=[-0.52364256 -0.79001072  0.0497023   0.37872757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 175 ] state=[-0.52364256 -0.79001072  0.0497023   0.37872757], action=1, reward=1.0, next_state=[-0.53944277 -0.59562855  0.05727685  0.10212085]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 176 ] state=[-0.53944277 -0.59562855  0.05727685  0.10212085], action=0, reward=1.0, next_state=[-0.55135535 -0.79152259  0.05931927  0.41231023]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 177 ] state=[-0.55135535 -0.79152259  0.05931927  0.41231023], action=0, reward=1.0, next_state=[-0.5671858  -0.98743306  0.06756547  0.7230887 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 178 ] state=[-0.5671858  -0.98743306  0.06756547  0.7230887 ], action=1, reward=1.0, next_state=[-0.58693446 -0.79330745  0.08202725  0.45241383]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 179 ] state=[-0.58693446 -0.79330745  0.08202725  0.45241383], action=0, reward=1.0, next_state=[-0.60280061 -0.98948779  0.09107552  0.76978388]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 180 ] state=[-0.60280061 -0.98948779  0.09107552  0.76978388], action=1, reward=1.0, next_state=[-0.62259036 -0.79572948  0.1064712   0.50709016]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 181 ] state=[-0.62259036 -0.79572948  0.1064712   0.50709016], action=1, reward=1.0, next_state=[-0.63850495 -0.60225619  0.116613    0.24976702]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 182 ] state=[-0.63850495 -0.60225619  0.116613    0.24976702], action=1, reward=1.0, next_state=[-0.65055008 -0.40897588  0.12160834 -0.00397784]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 183 ] state=[-0.65055008 -0.40897588  0.12160834 -0.00397784], action=1, reward=1.0, next_state=[-0.65872959 -0.21578884  0.12152879 -0.25595352]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 184 ] state=[-0.65872959 -0.21578884  0.12152879 -0.25595352], action=1, reward=1.0, next_state=[-0.66304537 -0.02259269  0.11640972 -0.50796878]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 185 ] state=[-0.66304537 -0.02259269  0.11640972 -0.50796878], action=0, reward=1.0, next_state=[-0.66349723 -0.21914583  0.10625034 -0.18098725]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 186 ] state=[-0.66349723 -0.21914583  0.10625034 -0.18098725], action=1, reward=1.0, next_state=[-0.66788014 -0.02569204  0.1026306  -0.43835269]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 187 ] state=[-0.66788014 -0.02569204  0.1026306  -0.43835269], action=1, reward=1.0, next_state=[-0.66839398  0.16783893  0.09386354 -0.69700117]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 188 ] state=[-0.66839398  0.16783893  0.09386354 -0.69700117], action=1, reward=1.0, next_state=[-0.6650372   0.36154255  0.07992352 -0.95872221]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 189 ] state=[-0.6650372   0.36154255  0.07992352 -0.95872221], action=1, reward=1.0, next_state=[-0.65780635  0.55550428  0.06074908 -1.22526356]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 190 ] state=[-0.65780635  0.55550428  0.06074908 -1.22526356], action=1, reward=1.0, next_state=[-0.64669627  0.74979367  0.0362438  -1.4983108 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 191 ] state=[-0.64669627  0.74979367  0.0362438  -1.4983108 ], action=0, reward=1.0, next_state=[-0.63170039  0.55425053  0.00627759 -1.19453538]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 192 ] state=[-0.63170039  0.55425053  0.00627759 -1.19453538], action=0, reward=1.0, next_state=[-0.62061538  0.35904785 -0.01761312 -0.89989153]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 193 ] state=[-0.62061538  0.35904785 -0.01761312 -0.89989153], action=0, reward=1.0, next_state=[-0.61343443  0.16416896 -0.03561095 -0.61279652]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 194 ] state=[-0.61343443  0.16416896 -0.03561095 -0.61279652], action=0, reward=1.0, next_state=[-0.61015105 -0.03043771 -0.04786688 -0.331539  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 195 ] state=[-0.61015105 -0.03043771 -0.04786688 -0.331539  ], action=1, reward=1.0, next_state=[-0.6107598   0.16533175 -0.05449766 -0.63892433]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 196 ] state=[-0.6107598   0.16533175 -0.05449766 -0.63892433], action=0, reward=1.0, next_state=[-0.60745317 -0.02898968 -0.06727615 -0.36388931]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 197 ] state=[-0.60745317 -0.02898968 -0.06727615 -0.36388931], action=0, reward=1.0, next_state=[-0.60803296 -0.2230942  -0.07455393 -0.09315545]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 198 ] state=[-0.60803296 -0.2230942  -0.07455393 -0.09315545], action=0, reward=1.0, next_state=[-0.61249484 -0.41707278 -0.07641704  0.1751056 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 199 ] state=[-0.61249484 -0.41707278 -0.07641704  0.1751056 ], action=0, reward=1.0, next_state=[-0.6208363  -0.61102263 -0.07291493  0.4427366 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 200 ] state=[-0.6208363  -0.61102263 -0.07291493  0.4427366 ], action=1, reward=1.0, next_state=[-0.63305675 -0.41494875 -0.0640602   0.12798927]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 201 ] state=[-0.63305675 -0.41494875 -0.0640602   0.12798927], action=0, reward=1.0, next_state=[-0.64135573 -0.6090973  -0.06150041  0.39979393]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 202 ] state=[-0.64135573 -0.6090973  -0.06150041  0.39979393], action=0, reward=1.0, next_state=[-0.65353767 -0.80329537 -0.05350453  0.6724706 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 203 ] state=[-0.65353767 -0.80329537 -0.05350453  0.6724706 ], action=1, reward=1.0, next_state=[-0.66960358 -0.60747213 -0.04005512  0.36343325]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 204 ] state=[-0.66960358 -0.60747213 -0.04005512  0.36343325], action=1, reward=1.0, next_state=[-0.68175302 -0.41180446 -0.03278646  0.05839411]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 205 ] state=[-0.68175302 -0.41180446 -0.03278646  0.05839411], action=0, reward=1.0, next_state=[-0.68998911 -0.60644135 -0.03161857  0.34055504]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 206 ] state=[-0.68998911 -0.60644135 -0.03161857  0.34055504], action=1, reward=1.0, next_state=[-0.70211794 -0.41088412 -0.02480747  0.0380715 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 207 ] state=[-0.70211794 -0.41088412 -0.02480747  0.0380715 ], action=0, reward=1.0, next_state=[-0.71033562 -0.60564171 -0.02404604  0.32282535]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 208 ] state=[-0.71033562 -0.60564171 -0.02404604  0.32282535], action=0, reward=1.0, next_state=[-0.72244846 -0.80041314 -0.01758954  0.60782919]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 209 ] state=[-0.72244846 -0.80041314 -0.01758954  0.60782919], action=0, reward=1.0, next_state=[-0.73845672 -0.99528481 -0.00543295  0.89492042]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 210 ] state=[-0.73845672 -0.99528481 -0.00543295  0.89492042], action=0, reward=1.0, next_state=[-0.75836241 -1.19033267  0.01246546  1.18589061]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 211 ] state=[-0.75836241 -1.19033267  0.01246546  1.18589061], action=1, reward=1.0, next_state=[-0.78216907 -0.99537459  0.03618327  0.89714096]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 212 ] state=[-0.78216907 -0.99537459  0.03618327  0.89714096], action=1, reward=1.0, next_state=[-0.80207656 -0.80076134  0.05412609  0.61604771]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 213 ] state=[-0.80207656 -0.80076134  0.05412609  0.61604771], action=0, reward=1.0, next_state=[-0.81809179 -0.99659605  0.06644704  0.92527489]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 214 ] state=[-0.81809179 -0.99659605  0.06644704  0.92527489], action=1, reward=1.0, next_state=[-0.83802371 -0.80243142  0.08495254  0.65419173]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 122 ][ timestamp 215 ] state=[-0.83802371 -0.80243142  0.08495254  0.65419173], action=0, reward=1.0, next_state=[-0.85407234 -0.99862708  0.09803637  0.97236992]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 216 ] state=[-0.85407234 -0.99862708  0.09803637  0.97236992], action=1, reward=1.0, next_state=[-0.87404488 -0.8049476   0.11748377  0.71202224]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 217 ] state=[-0.87404488 -0.8049476   0.11748377  0.71202224], action=1, reward=1.0, next_state=[-0.89014383 -0.61163145  0.13172422  0.45850771]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 218 ] state=[-0.89014383 -0.61163145  0.13172422  0.45850771], action=1, reward=1.0, next_state=[-0.90237646 -0.41859357  0.14089437  0.21007437]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 219 ] state=[-0.90237646 -0.41859357  0.14089437  0.21007437], action=0, reward=1.0, next_state=[-0.91074833 -0.61541946  0.14509586  0.54367366]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 220 ] state=[-0.91074833 -0.61541946  0.14509586  0.54367366], action=0, reward=1.0, next_state=[-0.92305672 -0.81225043  0.15596933  0.87832635]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 221 ] state=[-0.92305672 -0.81225043  0.15596933  0.87832635], action=1, reward=1.0, next_state=[-0.93930173 -0.61955252  0.17353586  0.63845739]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 222 ] state=[-0.93930173 -0.61955252  0.17353586  0.63845739], action=0, reward=1.0, next_state=[-0.95169278 -0.81661501  0.18630501  0.98037528]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 223 ] state=[-0.95169278 -0.81661501  0.18630501  0.98037528], action=1, reward=1.0, next_state=[-0.96802508 -0.62441238  0.20591251  0.75151769]\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 224 ] state=[-0.96802508 -0.62441238  0.20591251  0.75151769], action=1, reward=-1.0, next_state=[-0.98051333 -0.4326347   0.22094287  0.53003956]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 122: Exploration_rate=0.01. Score=224.\n",
      "[ episode 123 ] state=[ 0.03382162  0.0281966   0.02138898 -0.04810449]\n",
      "[ episode 123 ][ timestamp 1 ] state=[ 0.03382162  0.0281966   0.02138898 -0.04810449], action=1, reward=1.0, next_state=[ 0.03438555  0.22300543  0.02042689 -0.33396301]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 2 ] state=[ 0.03438555  0.22300543  0.02042689 -0.33396301], action=0, reward=1.0, next_state=[ 0.03884566  0.02759879  0.01374763 -0.03490912]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 3 ] state=[ 0.03884566  0.02759879  0.01374763 -0.03490912], action=0, reward=1.0, next_state=[ 0.03939763 -0.16771758  0.01304945  0.26207943]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 4 ] state=[ 0.03939763 -0.16771758  0.01304945  0.26207943], action=1, reward=1.0, next_state=[ 0.03604328  0.02721569  0.01829104 -0.02645915]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 5 ] state=[ 0.03604328  0.02721569  0.01829104 -0.02645915], action=0, reward=1.0, next_state=[ 0.0365876  -0.16816373  0.01776186  0.27193822]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 6 ] state=[ 0.0365876  -0.16816373  0.01776186  0.27193822], action=1, reward=1.0, next_state=[ 0.03322432  0.02670033  0.02320062 -0.01509005]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 7 ] state=[ 0.03322432  0.02670033  0.02320062 -0.01509005], action=0, reward=1.0, next_state=[ 0.03375833 -0.16874653  0.02289882  0.28482171]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 8 ] state=[ 0.03375833 -0.16874653  0.02289882  0.28482171], action=1, reward=1.0, next_state=[ 0.0303834   0.02604147  0.02859525 -0.00055203]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 9 ] state=[ 0.0303834   0.02604147  0.02859525 -0.00055203], action=1, reward=1.0, next_state=[ 0.03090423  0.2207419   0.02858421 -0.28407742]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 10 ] state=[ 0.03090423  0.2207419   0.02858421 -0.28407742], action=1, reward=1.0, next_state=[ 0.03531907  0.41544475  0.02290267 -0.56760977]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 11 ] state=[ 0.03531907  0.41544475  0.02290267 -0.56760977], action=1, reward=1.0, next_state=[ 0.04362796  0.61023808  0.01155047 -0.85299034]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 12 ] state=[ 0.04362796  0.61023808  0.01155047 -0.85299034], action=0, reward=1.0, next_state=[ 0.05583272  0.4149606  -0.00550934 -0.5566979 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 13 ] state=[ 0.05583272  0.4149606  -0.00550934 -0.5566979 ], action=0, reward=1.0, next_state=[ 0.06413193  0.21991643 -0.01664329 -0.26575581]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 14 ] state=[ 0.06413193  0.21991643 -0.01664329 -0.26575581], action=1, reward=1.0, next_state=[ 0.06853026  0.41527192 -0.02195841 -0.56364137]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 15 ] state=[ 0.06853026  0.41527192 -0.02195841 -0.56364137], action=1, reward=1.0, next_state=[ 0.0768357   0.61069499 -0.03323124 -0.86316056]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 16 ] state=[ 0.0768357   0.61069499 -0.03323124 -0.86316056], action=1, reward=1.0, next_state=[ 0.0890496   0.80625322 -0.05049445 -1.16610415]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 17 ] state=[ 0.0890496   0.80625322 -0.05049445 -1.16610415], action=1, reward=1.0, next_state=[ 0.10517467  1.00199466 -0.07381653 -1.47418113]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 18 ] state=[ 0.10517467  1.00199466 -0.07381653 -1.47418113], action=0, reward=1.0, next_state=[ 0.12521456  0.80784841 -0.10330016 -1.20543716]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 19 ] state=[ 0.12521456  0.80784841 -0.10330016 -1.20543716], action=0, reward=1.0, next_state=[ 0.14137153  0.6142019  -0.1274089  -0.94683207]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 20 ] state=[ 0.14137153  0.6142019  -0.1274089  -0.94683207], action=0, reward=1.0, next_state=[ 0.15365556  0.42100457 -0.14634554 -0.69674198]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 21 ] state=[ 0.15365556  0.42100457 -0.14634554 -0.69674198], action=0, reward=1.0, next_state=[ 0.16207566  0.22818255 -0.16028038 -0.45347285]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 22 ] state=[ 0.16207566  0.22818255 -0.16028038 -0.45347285], action=0, reward=1.0, next_state=[ 0.16663931  0.03564689 -0.16934984 -0.21529202]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 23 ] state=[ 0.16663931  0.03564689 -0.16934984 -0.21529202], action=0, reward=1.0, next_state=[ 0.16735224 -0.15670033 -0.17365568  0.01955019]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 24 ] state=[ 0.16735224 -0.15670033 -0.17365568  0.01955019], action=0, reward=1.0, next_state=[ 0.16421824 -0.34896165 -0.17326467  0.25280612]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 25 ] state=[ 0.16421824 -0.34896165 -0.17326467  0.25280612], action=0, reward=1.0, next_state=[ 0.15723901 -0.54124076 -0.16820855  0.48622102]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 26 ] state=[ 0.15723901 -0.54124076 -0.16820855  0.48622102], action=1, reward=1.0, next_state=[ 0.14641419 -0.34419442 -0.15848413  0.14560267]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 27 ] state=[ 0.14641419 -0.34419442 -0.15848413  0.14560267], action=1, reward=1.0, next_state=[ 0.1395303  -0.14719973 -0.15557208 -0.19258568]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 28 ] state=[ 0.1395303  -0.14719973 -0.15557208 -0.19258568], action=0, reward=1.0, next_state=[ 0.13658631 -0.33979366 -0.15942379  0.04726237]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 29 ] state=[ 0.13658631 -0.33979366 -0.15942379  0.04726237], action=0, reward=1.0, next_state=[ 0.12979043 -0.53231312 -0.15847854  0.28570723]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 30 ] state=[ 0.12979043 -0.53231312 -0.15847854  0.28570723], action=0, reward=1.0, next_state=[ 0.11914417 -0.7248618  -0.1527644   0.52451296]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 31 ] state=[ 0.11914417 -0.7248618  -0.1527644   0.52451296], action=0, reward=1.0, next_state=[ 0.10464694 -0.91754119 -0.14227414  0.76542794]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 32 ] state=[ 0.10464694 -0.91754119 -0.14227414  0.76542794], action=0, reward=1.0, next_state=[ 0.08629611 -1.1104474  -0.12696558  1.01017597]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 33 ] state=[ 0.08629611 -1.1104474  -0.12696558  1.01017597], action=1, reward=1.0, next_state=[ 0.06408716 -0.91388076 -0.10676206  0.68047168]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 34 ] state=[ 0.06408716 -0.91388076 -0.10676206  0.68047168], action=1, reward=1.0, next_state=[ 0.04580955 -0.71745063 -0.09315263  0.35617565]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 123 ][ timestamp 35 ] state=[ 0.04580955 -0.71745063 -0.09315263  0.35617565], action=1, reward=1.0, next_state=[ 0.03146054 -0.52113625 -0.08602911  0.03563349]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 36 ] state=[ 0.03146054 -0.52113625 -0.08602911  0.03563349], action=0, reward=1.0, next_state=[ 0.02103781 -0.71492591 -0.08531644  0.29998161]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 37 ] state=[ 0.02103781 -0.71492591 -0.08531644  0.29998161], action=0, reward=1.0, next_state=[ 0.00673929 -0.90873474 -0.07931681  0.56458483]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 38 ] state=[ 0.00673929 -0.90873474 -0.07931681  0.56458483], action=0, reward=1.0, next_state=[-0.0114354  -1.1026595  -0.06802511  0.83126274]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 39 ] state=[-0.0114354  -1.1026595  -0.06802511  0.83126274], action=0, reward=1.0, next_state=[-0.03348859 -1.29678907 -0.05139986  1.10179965]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 40 ] state=[-0.03348859 -1.29678907 -0.05139986  1.10179965], action=0, reward=1.0, next_state=[-0.05942437 -1.49119847 -0.02936387  1.37792372]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 41 ] state=[-0.05942437 -1.49119847 -0.02936387  1.37792372], action=0, reward=1.0, next_state=[-0.08924834 -1.68594164 -0.00180539  1.66128082]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 42 ] state=[-0.08924834 -1.68594164 -0.00180539  1.66128082], action=0, reward=1.0, next_state=[-0.12296718 -1.88104252  0.03142022  1.95340087]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 43 ] state=[-0.12296718 -1.88104252  0.03142022  1.95340087], action=1, reward=1.0, next_state=[-0.16058803 -1.686268    0.07048824  1.67061932]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 44 ] state=[-0.16058803 -1.686268    0.07048824  1.67061932], action=1, reward=1.0, next_state=[-0.19431339 -1.49203233  0.10390063  1.40069571]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 45 ] state=[-0.19431339 -1.49203233  0.10390063  1.40069571], action=1, reward=1.0, next_state=[-0.22415403 -1.29834369  0.13191454  1.1422214 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 46 ] state=[-0.22415403 -1.29834369  0.13191454  1.1422214 ], action=1, reward=1.0, next_state=[-0.25012091 -1.10516884  0.15475897  0.8936471 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 47 ] state=[-0.25012091 -1.10516884  0.15475897  0.8936471 ], action=0, reward=1.0, next_state=[-0.27222428 -1.30201268  0.17263191  1.23070179]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 48 ] state=[-0.27222428 -1.30201268  0.17263191  1.23070179], action=1, reward=1.0, next_state=[-0.29826454 -1.10947948  0.19724595  0.99669675]\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 49 ] state=[-0.29826454 -1.10947948  0.19724595  0.99669675], action=0, reward=-1.0, next_state=[-0.32045413 -1.30661354  0.21717988  1.34427921]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 123: Exploration_rate=0.01. Score=49.\n",
      "[ episode 124 ] state=[-0.04948881 -0.01997585  0.00866993  0.02198243]\n",
      "[ episode 124 ][ timestamp 1 ] state=[-0.04948881 -0.01997585  0.00866993  0.02198243], action=1, reward=1.0, next_state=[-0.04988832  0.1750207   0.00910958 -0.26795248]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 2 ] state=[-0.04988832  0.1750207   0.00910958 -0.26795248], action=1, reward=1.0, next_state=[-0.04638791  0.37001147  0.00375053 -0.55774831]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 3 ] state=[-0.04638791  0.37001147  0.00375053 -0.55774831], action=0, reward=1.0, next_state=[-0.03898768  0.17483707 -0.00740443 -0.26388612]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 4 ] state=[-0.03898768  0.17483707 -0.00740443 -0.26388612], action=1, reward=1.0, next_state=[-0.03549094  0.37006392 -0.01268216 -0.55889525]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 5 ] state=[-0.03549094  0.37006392 -0.01268216 -0.55889525], action=0, reward=1.0, next_state=[-0.02808966  0.17512226 -0.02386006 -0.27023473]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 6 ] state=[-0.02808966  0.17512226 -0.02386006 -0.27023473], action=0, reward=1.0, next_state=[-0.02458722 -0.01965122 -0.02926476  0.01482814]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 7 ] state=[-0.02458722 -0.01965122 -0.02926476  0.01482814], action=0, reward=1.0, next_state=[-0.02498024 -0.21434153 -0.02896819  0.29813594]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 8 ] state=[-0.02498024 -0.21434153 -0.02896819  0.29813594], action=0, reward=1.0, next_state=[-0.02926707 -0.40903882 -0.02300547  0.5815439 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 9 ] state=[-0.02926707 -0.40903882 -0.02300547  0.5815439 ], action=1, reward=1.0, next_state=[-0.03744785 -0.21360221 -0.0113746   0.28170354]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 10 ] state=[-0.03744785 -0.21360221 -0.0113746   0.28170354], action=1, reward=1.0, next_state=[-0.04171989 -0.01831988 -0.00574053 -0.01454507]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 11 ] state=[-0.04171989 -0.01831988 -0.00574053 -0.01454507], action=0, reward=1.0, next_state=[-0.04208629 -0.21335903 -0.00603143  0.27632114]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 12 ] state=[-0.04208629 -0.21335903 -0.00603143  0.27632114], action=0, reward=1.0, next_state=[-4.63534708e-02 -4.08394417e-01 -5.05003900e-04  5.67095664e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 13 ] state=[-4.63534708e-02 -4.08394417e-01 -5.05003900e-04  5.67095664e-01], action=1, reward=1.0, next_state=[-0.05452136 -0.21326539  0.01083691  0.27425368]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 14 ] state=[-0.05452136 -0.21326539  0.01083691  0.27425368], action=1, reward=1.0, next_state=[-0.05878667 -0.01829972  0.01632198 -0.01499165]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 15 ] state=[-0.05878667 -0.01829972  0.01632198 -0.01499165], action=1, reward=1.0, next_state=[-0.05915266  0.1765844   0.01602215 -0.30248044]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 16 ] state=[-0.05915266  0.1765844   0.01602215 -0.30248044], action=1, reward=1.0, next_state=[-0.05562097  0.37147438  0.00997254 -0.59006758]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 17 ] state=[-0.05562097  0.37147438  0.00997254 -0.59006758], action=0, reward=1.0, next_state=[-0.04819149  0.17621422 -0.00182881 -0.29426003]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 18 ] state=[-0.04819149  0.17621422 -0.00182881 -0.29426003], action=1, reward=1.0, next_state=[-0.0446672   0.3713622  -0.00771401 -0.58751918]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 19 ] state=[-0.0446672   0.3713622  -0.00771401 -0.58751918], action=1, reward=1.0, next_state=[-0.03723996  0.56659133 -0.01946439 -0.88262206]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 20 ] state=[-0.03723996  0.56659133 -0.01946439 -0.88262206], action=0, reward=1.0, next_state=[-0.02590813  0.37173906 -0.03711684 -0.59612119]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 21 ] state=[-0.02590813  0.37173906 -0.03711684 -0.59612119], action=0, reward=1.0, next_state=[-0.01847335  0.17715567 -0.04903926 -0.31535698]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 22 ] state=[-0.01847335  0.17715567 -0.04903926 -0.31535698], action=0, reward=1.0, next_state=[-0.01493024 -0.01723469 -0.0553464  -0.03853374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 23 ] state=[-0.01493024 -0.01723469 -0.0553464  -0.03853374], action=1, reward=1.0, next_state=[-0.01527493  0.17863545 -0.05611707 -0.34815261]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 24 ] state=[-0.01527493  0.17863545 -0.05611707 -0.34815261], action=0, reward=1.0, next_state=[-0.01170222 -0.0156453  -0.06308013 -0.07367999]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 25 ] state=[-0.01170222 -0.0156453  -0.06308013 -0.07367999], action=0, reward=1.0, next_state=[-0.01201513 -0.20980886 -0.06455373  0.19845284]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 26 ] state=[-0.01201513 -0.20980886 -0.06455373  0.19845284], action=0, reward=1.0, next_state=[-0.0162113  -0.40395095 -0.06058467  0.4700938 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 27 ] state=[-0.0162113  -0.40395095 -0.06058467  0.4700938 ], action=0, reward=1.0, next_state=[-0.02429032 -0.59816711 -0.05118279  0.74308255]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 28 ] state=[-0.02429032 -0.59816711 -0.05118279  0.74308255], action=1, reward=1.0, next_state=[-0.03625367 -0.40237746 -0.03632114  0.4347415 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 124 ][ timestamp 29 ] state=[-0.03625367 -0.40237746 -0.03632114  0.4347415 ], action=0, reward=1.0, next_state=[-0.04430121 -0.59696689 -0.02762631  0.71575707]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 30 ] state=[-0.04430121 -0.59696689 -0.02762631  0.71575707], action=0, reward=1.0, next_state=[-0.05624055 -0.79169578 -0.01331117  0.99961784]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 31 ] state=[-0.05624055 -0.79169578 -0.01331117  0.99961784], action=1, reward=1.0, next_state=[-0.07207447 -0.59639846  0.00668119  0.70278444]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 32 ] state=[-0.07207447 -0.59639846  0.00668119  0.70278444], action=0, reward=1.0, next_state=[-0.08400244 -0.79161237  0.02073688  0.99756303]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 33 ] state=[-0.08400244 -0.79161237  0.02073688  0.99756303], action=1, reward=1.0, next_state=[-0.09983468 -0.59677373  0.04068814  0.7114641 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 34 ] state=[-0.09983468 -0.59677373  0.04068814  0.7114641 ], action=0, reward=1.0, next_state=[-0.11177016 -0.79243478  0.05491742  1.0166718 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 35 ] state=[-0.11177016 -0.79243478  0.05491742  1.0166718 ], action=0, reward=1.0, next_state=[-0.12761885 -0.98824423  0.07525085  1.32608077]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 36 ] state=[-0.12761885 -0.98824423  0.07525085  1.32608077], action=0, reward=1.0, next_state=[-0.14738374 -1.18423129  0.10177247  1.64133227]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 37 ] state=[-0.14738374 -1.18423129  0.10177247  1.64133227], action=1, reward=1.0, next_state=[-0.17106836 -0.99043819  0.13459911  1.38201623]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 38 ] state=[-0.17106836 -0.99043819  0.13459911  1.38201623], action=0, reward=1.0, next_state=[-0.19087713 -1.18695826  0.16223944  1.71358287]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 39 ] state=[-0.19087713 -1.18695826  0.16223944  1.71358287], action=0, reward=1.0, next_state=[-0.21461629 -1.38352833  0.1965111   2.05205537]\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 40 ] state=[-0.21461629 -1.38352833  0.1965111   2.05205537], action=1, reward=-1.0, next_state=[-0.24228686 -1.19088787  0.2375522   1.82605922]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 124: Exploration_rate=0.01. Score=40.\n",
      "[ episode 125 ] state=[-0.01410906 -0.04826137  0.03261837  0.02333651]\n",
      "[ episode 125 ][ timestamp 1 ] state=[-0.01410906 -0.04826137  0.03261837  0.02333651], action=0, reward=1.0, next_state=[-0.01507429 -0.24383555  0.0330851   0.32612983]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 2 ] state=[-0.01507429 -0.24383555  0.0330851   0.32612983], action=1, reward=1.0, next_state=[-0.019951   -0.04919989  0.03960769  0.04406136]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 3 ] state=[-0.019951   -0.04919989  0.03960769  0.04406136], action=0, reward=1.0, next_state=[-0.020935   -0.24486676  0.04048892  0.34897309]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 4 ] state=[-0.020935   -0.24486676  0.04048892  0.34897309], action=0, reward=1.0, next_state=[-0.02583234 -0.44054048  0.04746838  0.65414361]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 5 ] state=[-0.02583234 -0.44054048  0.04746838  0.65414361], action=1, reward=1.0, next_state=[-0.03464314 -0.24611049  0.06055126  0.37677761]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 6 ] state=[-0.03464314 -0.24611049  0.06055126  0.37677761], action=1, reward=1.0, next_state=[-0.03956535 -0.05189847  0.06808681  0.10378466]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 7 ] state=[-0.03956535 -0.05189847  0.06808681  0.10378466], action=1, reward=1.0, next_state=[-0.04060332  0.14218501  0.0701625  -0.16666396]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 8 ] state=[-0.04060332  0.14218501  0.0701625  -0.16666396], action=1, reward=1.0, next_state=[-0.03775962  0.33623615  0.06682922 -0.43641365]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 9 ] state=[-0.03775962  0.33623615  0.06682922 -0.43641365], action=0, reward=1.0, next_state=[-0.0310349   0.14023498  0.05810095 -0.12343502]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 10 ] state=[-0.0310349   0.14023498  0.05810095 -0.12343502], action=1, reward=1.0, next_state=[-0.0282302   0.33447849  0.05563225 -0.39723657]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 11 ] state=[-0.0282302   0.33447849  0.05563225 -0.39723657], action=0, reward=1.0, next_state=[-0.02154063  0.13861321  0.04768752 -0.08754573]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 12 ] state=[-0.02154063  0.13861321  0.04768752 -0.08754573], action=0, reward=1.0, next_state=[-0.01876837 -0.05715871  0.0459366   0.21979312]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 13 ] state=[-0.01876837 -0.05715871  0.0459366   0.21979312], action=1, reward=1.0, next_state=[-0.01991154  0.13727754  0.05033246 -0.05805296]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 14 ] state=[-0.01991154  0.13727754  0.05033246 -0.05805296], action=1, reward=1.0, next_state=[-0.01716599  0.33164303  0.04917141 -0.33444049]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 15 ] state=[-0.01716599  0.33164303  0.04917141 -0.33444049], action=1, reward=1.0, next_state=[-0.01053313  0.52603193  0.0424826  -0.61122084]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 16 ] state=[-0.01053313  0.52603193  0.0424826  -0.61122084], action=0, reward=1.0, next_state=[-1.24914765e-05  3.30342754e-01  3.02581786e-02 -3.05465791e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 17 ] state=[-1.24914765e-05  3.30342754e-01  3.02581786e-02 -3.05465791e-01], action=0, reward=1.0, next_state=[ 0.00659436  0.13480297  0.02414886 -0.00339583]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 18 ] state=[ 0.00659436  0.13480297  0.02414886 -0.00339583], action=0, reward=1.0, next_state=[ 0.00929042 -0.06065684  0.02408095  0.29680747]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 19 ] state=[ 0.00929042 -0.06065684  0.02408095  0.29680747], action=0, reward=1.0, next_state=[ 0.00807729 -0.25611365  0.0300171   0.5969868 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 20 ] state=[ 0.00807729 -0.25611365  0.0300171   0.5969868 ], action=1, reward=1.0, next_state=[ 0.00295501 -0.06142433  0.04195683  0.31390807]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 21 ] state=[ 0.00295501 -0.06142433  0.04195683  0.31390807], action=1, reward=1.0, next_state=[0.00172653 0.1330756  0.04823499 0.03474662]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 22 ] state=[0.00172653 0.1330756  0.04823499 0.03474662], action=0, reward=1.0, next_state=[ 0.00438804 -0.06270368  0.04892993  0.34224957]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 23 ] state=[ 0.00438804 -0.06270368  0.04892993  0.34224957], action=0, reward=1.0, next_state=[ 0.00313397 -0.25848637  0.05577492  0.6499518 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 24 ] state=[ 0.00313397 -0.25848637  0.05577492  0.6499518 ], action=1, reward=1.0, next_state=[-0.00203576 -0.06418387  0.06877395  0.37534058]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 25 ] state=[-0.00203576 -0.06418387  0.06877395  0.37534058], action=1, reward=1.0, next_state=[-0.00331944  0.12989726  0.07628076  0.10511071]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 26 ] state=[-0.00331944  0.12989726  0.07628076  0.10511071], action=1, reward=1.0, next_state=[-0.00072149  0.32384787  0.07838298 -0.16256441]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 27 ] state=[-0.00072149  0.32384787  0.07838298 -0.16256441], action=1, reward=1.0, next_state=[ 0.00575546  0.51776532  0.07513169 -0.42952647]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 28 ] state=[ 0.00575546  0.51776532  0.07513169 -0.42952647], action=1, reward=1.0, next_state=[ 0.01611077  0.71174736  0.06654116 -0.69761074]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 29 ] state=[ 0.01611077  0.71174736  0.06654116 -0.69761074], action=1, reward=1.0, next_state=[ 0.03034572  0.90588659  0.05258895 -0.96862646]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 30 ] state=[ 0.03034572  0.90588659  0.05258895 -0.96862646], action=1, reward=1.0, next_state=[ 0.04846345  1.10026455  0.03321642 -1.24433629]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 31 ] state=[ 0.04846345  1.10026455  0.03321642 -1.24433629], action=1, reward=1.0, next_state=[ 0.07046874  1.29494493  0.00832969 -1.52643195]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 32 ] state=[ 0.07046874  1.29494493  0.00832969 -1.52643195], action=1, reward=1.0, next_state=[ 0.09636764  1.48996538 -0.02219895 -1.81650356]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 125 ][ timestamp 33 ] state=[ 0.09636764  1.48996538 -0.02219895 -1.81650356], action=0, reward=1.0, next_state=[ 0.12616695  1.29509725 -0.05852902 -1.53079936]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 34 ] state=[ 0.12616695  1.29509725 -0.05852902 -1.53079936], action=0, reward=1.0, next_state=[ 0.15206889  1.10072771 -0.08914501 -1.25694199]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 35 ] state=[ 0.15206889  1.10072771 -0.08914501 -1.25694199], action=0, reward=1.0, next_state=[ 0.17408344  0.90685268 -0.11428385 -0.99345813]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 36 ] state=[ 0.17408344  0.90685268 -0.11428385 -0.99345813], action=1, reward=1.0, next_state=[ 0.1922205   1.10330267 -0.13415301 -1.31973722]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 37 ] state=[ 0.1922205   1.10330267 -0.13415301 -1.31973722], action=1, reward=1.0, next_state=[ 0.21428655  1.29984103 -0.16054775 -1.6512187 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 38 ] state=[ 0.21428655  1.29984103 -0.16054775 -1.6512187 ], action=0, reward=1.0, next_state=[ 0.24028337  1.10691772 -0.19357213 -1.41255379]\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 39 ] state=[ 0.24028337  1.10691772 -0.19357213 -1.41255379], action=0, reward=-1.0, next_state=[ 0.26242173  0.91464917 -0.2218232  -1.18609283]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 125: Exploration_rate=0.01. Score=39.\n",
      "[ episode 126 ] state=[ 0.04590618 -0.01381282 -0.03683213  0.01557535]\n",
      "[ episode 126 ][ timestamp 1 ] state=[ 0.04590618 -0.01381282 -0.03683213  0.01557535], action=1, reward=1.0, next_state=[ 0.04562992  0.18181746 -0.03652063 -0.28849725]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 2 ] state=[ 0.04562992  0.18181746 -0.03652063 -0.28849725], action=1, reward=1.0, next_state=[ 0.04926627  0.37744066 -0.04229057 -0.59247106]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 3 ] state=[ 0.04926627  0.37744066 -0.04229057 -0.59247106], action=0, reward=1.0, next_state=[ 0.05681509  0.18293545 -0.05413999 -0.31340384]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 4 ] state=[ 0.05681509  0.18293545 -0.05413999 -0.31340384], action=0, reward=1.0, next_state=[ 0.06047379 -0.01137512 -0.06040807 -0.03827442]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 5 ] state=[ 0.06047379 -0.01137512 -0.06040807 -0.03827442], action=1, reward=1.0, next_state=[ 0.06024629  0.18455872 -0.06117356 -0.34938829]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 6 ] state=[ 0.06024629  0.18455872 -0.06117356 -0.34938829], action=0, reward=1.0, next_state=[ 0.06393747 -0.00964227 -0.06816132 -0.0766055 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 7 ] state=[ 0.06393747 -0.00964227 -0.06816132 -0.0766055 ], action=0, reward=1.0, next_state=[ 0.06374462 -0.20372422 -0.06969343  0.19381751]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 8 ] state=[ 0.06374462 -0.20372422 -0.06969343  0.19381751], action=0, reward=1.0, next_state=[ 0.05967014 -0.3977836  -0.06581708  0.46372664]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 9 ] state=[ 0.05967014 -0.3977836  -0.06581708  0.46372664], action=0, reward=1.0, next_state=[ 0.05171446 -0.59191671 -0.05654255  0.73495956]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 10 ] state=[ 0.05171446 -0.59191671 -0.05654255  0.73495956], action=1, reward=1.0, next_state=[ 0.03987613 -0.39606113 -0.04184336  0.42503103]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 11 ] state=[ 0.03987613 -0.39606113 -0.04184336  0.42503103], action=1, reward=1.0, next_state=[ 0.03195491 -0.20037222 -0.03334274  0.11945623]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 12 ] state=[ 0.03195491 -0.20037222 -0.03334274  0.11945623], action=1, reward=1.0, next_state=[ 0.02794746 -0.0047888  -0.03095361 -0.18355677]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 13 ] state=[ 0.02794746 -0.0047888  -0.03095361 -0.18355677], action=0, reward=1.0, next_state=[ 0.02785169 -0.19945449 -0.03462475  0.09920297]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 14 ] state=[ 0.02785169 -0.19945449 -0.03462475  0.09920297], action=1, reward=1.0, next_state=[ 0.0238626  -0.00385385 -0.03264069 -0.20419977]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 15 ] state=[ 0.0238626  -0.00385385 -0.03264069 -0.20419977], action=0, reward=1.0, next_state=[ 0.02378552 -0.19849418 -0.03672469  0.07801055]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 16 ] state=[ 0.02378552 -0.19849418 -0.03672469  0.07801055], action=0, reward=1.0, next_state=[ 0.01981564 -0.39307094 -0.03516448  0.35888426]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 17 ] state=[ 0.01981564 -0.39307094 -0.03516448  0.35888426], action=1, reward=1.0, next_state=[ 0.01195422 -0.19746721 -0.02798679  0.05532382]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 18 ] state=[ 0.01195422 -0.19746721 -0.02798679  0.05532382], action=1, reward=1.0, next_state=[ 0.00800487 -0.00195538 -0.02688031 -0.24605612]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 19 ] state=[ 0.00800487 -0.00195538 -0.02688031 -0.24605612], action=1, reward=1.0, next_state=[ 0.00796577  0.19353997 -0.03180144 -0.54709507]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 20 ] state=[ 0.00796577  0.19353997 -0.03180144 -0.54709507], action=0, reward=1.0, next_state=[ 0.01183657 -0.0011211  -0.04274334 -0.26459916]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 21 ] state=[ 0.01183657 -0.0011211  -0.04274334 -0.26459916], action=0, reward=1.0, next_state=[ 0.01181414 -0.19560773 -0.04803532  0.01430162]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 22 ] state=[ 0.01181414 -0.19560773 -0.04803532  0.01430162], action=0, reward=1.0, next_state=[ 0.00790199 -0.39000905 -0.04774929  0.2914503 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 23 ] state=[ 0.00790199 -0.39000905 -0.04774929  0.2914503 ], action=0, reward=1.0, next_state=[ 1.01807865e-04 -5.84418798e-01 -4.19202820e-02  5.68699586e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 24 ] state=[ 1.01807865e-04 -5.84418798e-01 -4.19202820e-02  5.68699586e-01], action=1, reward=1.0, next_state=[-0.01158657 -0.38873472 -0.03054629  0.26311039]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 25 ] state=[-0.01158657 -0.38873472 -0.03054629  0.26311039], action=1, reward=1.0, next_state=[-0.01936126 -0.19319037 -0.02528408 -0.03904851]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 26 ] state=[-0.01936126 -0.19319037 -0.02528408 -0.03904851], action=1, reward=1.0, next_state=[-0.02322507  0.00228485 -0.02606505 -0.33960036]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 27 ] state=[-0.02322507  0.00228485 -0.02606505 -0.33960036], action=1, reward=1.0, next_state=[-0.02317937  0.1977678  -0.03285706 -0.64038744]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 28 ] state=[-0.02317937  0.1977678  -0.03285706 -0.64038744], action=0, reward=1.0, next_state=[-0.01922402  0.00311895 -0.04566481 -0.35823   ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 29 ] state=[-0.01922402  0.00311895 -0.04566481 -0.35823   ], action=0, reward=1.0, next_state=[-0.01916164 -0.19132507 -0.05282941 -0.08028879]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 30 ] state=[-0.01916164 -0.19132507 -0.05282941 -0.08028879], action=1, reward=1.0, next_state=[-0.02298814  0.00451283 -0.05443518 -0.38916044]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 31 ] state=[-0.02298814  0.00451283 -0.05443518 -0.38916044], action=0, reward=1.0, next_state=[-0.02289788 -0.18979593 -0.06221839 -0.11412506]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 32 ] state=[-0.02289788 -0.18979593 -0.06221839 -0.11412506], action=0, reward=1.0, next_state=[-0.0266938  -0.38397374 -0.06450089  0.15829766]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 33 ] state=[-0.0266938  -0.38397374 -0.06450089  0.15829766], action=1, reward=1.0, next_state=[-0.03437328 -0.18799047 -0.06133494 -0.15401605]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 34 ] state=[-0.03437328 -0.18799047 -0.06133494 -0.15401605], action=0, reward=1.0, next_state=[-0.03813309 -0.38218302 -0.06441526  0.11870387]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 35 ] state=[-0.03813309 -0.38218302 -0.06441526  0.11870387], action=0, reward=1.0, next_state=[-0.04577675 -0.57632574 -0.06204119  0.39038899]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 36 ] state=[-0.04577675 -0.57632574 -0.06204119  0.39038899], action=0, reward=1.0, next_state=[-0.05730326 -0.77051481 -0.05423341  0.66288377]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 37 ] state=[-0.05730326 -0.77051481 -0.05423341  0.66288377], action=1, reward=1.0, next_state=[-0.07271356 -0.57468194 -0.04097573  0.35362955]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 126 ][ timestamp 38 ] state=[-0.07271356 -0.57468194 -0.04097573  0.35362955], action=0, reward=1.0, next_state=[-0.0842072  -0.76919801 -0.03390314  0.63311525]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 39 ] state=[-0.0842072  -0.76919801 -0.03390314  0.63311525], action=0, reward=1.0, next_state=[-0.09959116 -0.96383101 -0.02124083  0.91493136]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 40 ] state=[-0.09959116 -0.96383101 -0.02124083  0.91493136], action=0, reward=1.0, next_state=[-0.11886778 -1.15865934 -0.00294221  1.2008636 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 41 ] state=[-0.11886778 -1.15865934 -0.00294221  1.2008636 ], action=1, reward=1.0, next_state=[-0.14204096 -0.96349946  0.02107507  0.90726004]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 42 ] state=[-0.14204096 -0.96349946  0.02107507  0.90726004], action=0, reward=1.0, next_state=[-0.16131095 -1.15890029  0.03922027  1.20649181]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 43 ] state=[-0.16131095 -1.15890029  0.03922027  1.20649181], action=1, reward=1.0, next_state=[-0.18448896 -0.96430645  0.0633501   0.92635332]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 44 ] state=[-0.18448896 -0.96430645  0.0633501   0.92635332], action=1, reward=1.0, next_state=[-0.20377509 -0.77009456  0.08187717  0.65423232]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 45 ] state=[-0.20377509 -0.77009456  0.08187717  0.65423232], action=1, reward=1.0, next_state=[-0.21917698 -0.57620235  0.09496182  0.38841334]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 46 ] state=[-0.21917698 -0.57620235  0.09496182  0.38841334], action=1, reward=1.0, next_state=[-0.23070103 -0.38254755  0.10273008  0.12711674]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 47 ] state=[-0.23070103 -0.38254755  0.10273008  0.12711674], action=0, reward=1.0, next_state=[-0.23835198 -0.57897969  0.10527242  0.45036108]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 48 ] state=[-0.23835198 -0.57897969  0.10527242  0.45036108], action=1, reward=1.0, next_state=[-0.24993157 -0.3854919   0.11427964  0.19262908]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 49 ] state=[-0.24993157 -0.3854919   0.11427964  0.19262908], action=1, reward=1.0, next_state=[-0.25764141 -0.19217446  0.11813222 -0.06193049]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 50 ] state=[-0.25764141 -0.19217446  0.11813222 -0.06193049], action=1, reward=1.0, next_state=[-0.2614849   0.00107305  0.11689361 -0.31513135]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 51 ] state=[-0.2614849   0.00107305  0.11689361 -0.31513135], action=1, reward=1.0, next_state=[-0.26146344  0.19435274  0.11059098 -0.56878388]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 52 ] state=[-0.26146344  0.19435274  0.11059098 -0.56878388], action=1, reward=1.0, next_state=[-0.25757638  0.38776401  0.09921531 -0.82468096]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 53 ] state=[-0.25757638  0.38776401  0.09921531 -0.82468096], action=1, reward=1.0, next_state=[-0.2498211   0.5813991   0.08272169 -1.08458374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 54 ] state=[-0.2498211   0.5813991   0.08272169 -1.08458374], action=1, reward=1.0, next_state=[-0.23819312  0.77533805  0.06103001 -1.35020495]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 55 ] state=[-0.23819312  0.77533805  0.06103001 -1.35020495], action=1, reward=1.0, next_state=[-0.22268636  0.9696425   0.03402591 -1.62318733]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 56 ] state=[-0.22268636  0.9696425   0.03402591 -1.62318733], action=1, reward=1.0, next_state=[-2.03293508e-01  1.16434781e+00  1.56216597e-03 -1.90507454e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 57 ] state=[-2.03293508e-01  1.16434781e+00  1.56216597e-03 -1.90507454e+00], action=1, reward=1.0, next_state=[-0.18000655  1.35945285 -0.03653932 -2.19727248]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 58 ] state=[-0.18000655  1.35945285 -0.03653932 -2.19727248], action=0, reward=1.0, next_state=[-0.15281749  1.16470141 -0.08048477 -1.91608047]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 59 ] state=[-0.15281749  1.16470141 -0.08048477 -1.91608047], action=1, reward=1.0, next_state=[-0.12952347  1.36059203 -0.11880638 -2.2326022 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 60 ] state=[-0.12952347  1.36059203 -0.11880638 -2.2326022 ], action=1, reward=1.0, next_state=[-0.10231163  1.55662395 -0.16345843 -2.55942425]\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 61 ] state=[-0.10231163  1.55662395 -0.16345843 -2.55942425], action=0, reward=-1.0, next_state=[-0.07117915  1.36313961 -0.21464691 -2.3209094 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 126: Exploration_rate=0.01. Score=61.\n",
      "[ episode 127 ] state=[ 0.01269337  0.04416733  0.02495607 -0.04305077]\n",
      "[ episode 127 ][ timestamp 1 ] state=[ 0.01269337  0.04416733  0.02495607 -0.04305077], action=0, reward=1.0, next_state=[ 0.01357672 -0.15130343  0.02409505  0.25740039]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 2 ] state=[ 0.01357672 -0.15130343  0.02409505  0.25740039], action=1, reward=1.0, next_state=[ 0.01055065  0.04346638  0.02924306 -0.02758627]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 3 ] state=[ 0.01055065  0.04346638  0.02924306 -0.02758627], action=1, reward=1.0, next_state=[ 0.01141997  0.23815702  0.02869133 -0.31090114]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 4 ] state=[ 0.01141997  0.23815702  0.02869133 -0.31090114], action=1, reward=1.0, next_state=[ 0.01618312  0.43285871  0.02247331 -0.59439937]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 5 ] state=[ 0.01618312  0.43285871  0.02247331 -0.59439937], action=1, reward=1.0, next_state=[ 0.02484029  0.62765901  0.01058532 -0.87991945]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 6 ] state=[ 0.02484029  0.62765901  0.01058532 -0.87991945], action=1, reward=1.0, next_state=[ 0.03739347  0.82263556 -0.00701306 -1.16925586]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 7 ] state=[ 0.03739347  0.82263556 -0.00701306 -1.16925586], action=0, reward=1.0, next_state=[ 0.05384618  0.62760554 -0.03039818 -0.87877984]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 8 ] state=[ 0.05384618  0.62760554 -0.03039818 -0.87877984], action=0, reward=1.0, next_state=[ 0.06639829  0.43290953 -0.04797378 -0.59580644]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 9 ] state=[ 0.06639829  0.43290953 -0.04797378 -0.59580644], action=0, reward=1.0, next_state=[ 0.07505648  0.23849065 -0.05988991 -0.31861252]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 10 ] state=[ 0.07505648  0.23849065 -0.05988991 -0.31861252], action=0, reward=1.0, next_state=[ 0.0798263   0.04427054 -0.06626216 -0.04540178]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 11 ] state=[ 0.0798263   0.04427054 -0.06626216 -0.04540178], action=0, reward=1.0, next_state=[ 0.08071171 -0.14984176 -0.06717019  0.22566087]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 12 ] state=[ 0.08071171 -0.14984176 -0.06717019  0.22566087], action=1, reward=1.0, next_state=[ 0.07771487  0.04617267 -0.06265698 -0.08743093]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 13 ] state=[ 0.07771487  0.04617267 -0.06265698 -0.08743093], action=0, reward=1.0, next_state=[ 0.07863832 -0.14799781 -0.06440559  0.18484415]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 14 ] state=[ 0.07863832 -0.14799781 -0.06440559  0.18484415], action=0, reward=1.0, next_state=[ 0.07567837 -0.34214194 -0.06070871  0.45653441]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 15 ] state=[ 0.07567837 -0.34214194 -0.06070871  0.45653441], action=0, reward=1.0, next_state=[ 0.06883553 -0.5363554  -0.05157802  0.72948053]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 16 ] state=[ 0.06883553 -0.5363554  -0.05157802  0.72948053], action=1, reward=1.0, next_state=[ 0.05810842 -0.3405599  -0.03698841  0.42102063]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 127 ][ timestamp 17 ] state=[ 0.05810842 -0.3405599  -0.03698841  0.42102063], action=1, reward=1.0, next_state=[ 0.05129722 -0.14493394 -0.028568    0.11691028]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 18 ] state=[ 0.05129722 -0.14493394 -0.028568    0.11691028], action=1, reward=1.0, next_state=[ 0.04839854  0.05058544 -0.02622979 -0.18464697]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 19 ] state=[ 0.04839854  0.05058544 -0.02622979 -0.18464697], action=0, reward=1.0, next_state=[ 0.04941025 -0.14415158 -0.02992273  0.0996474 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 20 ] state=[ 0.04941025 -0.14415158 -0.02992273  0.0996474 ], action=0, reward=1.0, next_state=[ 0.04652722 -0.33883219 -0.02792979  0.38274163]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 21 ] state=[ 0.04652722 -0.33883219 -0.02792979  0.38274163], action=1, reward=1.0, next_state=[ 0.03975058 -0.14332504 -0.02027495  0.08138499]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 22 ] state=[ 0.03975058 -0.14332504 -0.02027495  0.08138499], action=0, reward=1.0, next_state=[ 0.03688408 -0.33815057 -0.01864725  0.36760279]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 23 ] state=[ 0.03688408 -0.33815057 -0.01864725  0.36760279], action=0, reward=1.0, next_state=[ 0.03012107 -0.53300265 -0.0112952   0.65434813]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 24 ] state=[ 0.03012107 -0.53300265 -0.0112952   0.65434813], action=1, reward=1.0, next_state=[ 0.01946101 -0.33772527  0.00179176  0.35813002]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 25 ] state=[ 0.01946101 -0.33772527  0.00179176  0.35813002], action=0, reward=1.0, next_state=[ 0.01270651 -0.53287265  0.00895436  0.65137739]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 26 ] state=[ 0.01270651 -0.53287265  0.00895436  0.65137739], action=0, reward=1.0, next_state=[ 0.00204905 -0.72811816  0.02198191  0.94686647]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 27 ] state=[ 0.00204905 -0.72811816  0.02198191  0.94686647], action=0, reward=1.0, next_state=[-0.01251331 -0.92352913  0.04091924  1.24637427]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 28 ] state=[-0.01251331 -0.92352913  0.04091924  1.24637427], action=1, reward=1.0, next_state=[-0.03098389 -0.72895519  0.06584673  0.96678457]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 29 ] state=[-0.03098389 -0.72895519  0.06584673  0.96678457], action=1, reward=1.0, next_state=[-0.045563   -0.53477636  0.08518242  0.69549249]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 30 ] state=[-0.045563   -0.53477636  0.08518242  0.69549249], action=0, reward=1.0, next_state=[-0.05625852 -0.73096999  0.09909227  1.01372924]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 31 ] state=[-0.05625852 -0.73096999  0.09909227  1.01372924], action=0, reward=1.0, next_state=[-0.07087792 -0.92726403  0.11936685  1.33581135]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 32 ] state=[-0.07087792 -0.92726403  0.11936685  1.33581135], action=0, reward=1.0, next_state=[-0.0894232  -1.12367055  0.14608308  1.66333534]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 33 ] state=[-0.0894232  -1.12367055  0.14608308  1.66333534], action=1, reward=1.0, next_state=[-0.11189661 -0.93052052  0.17934979  1.41949204]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 34 ] state=[-0.11189661 -0.93052052  0.17934979  1.41949204], action=1, reward=1.0, next_state=[-0.13050702 -0.73801348  0.20773963  1.18780984]\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 35 ] state=[-0.13050702 -0.73801348  0.20773963  1.18780984], action=1, reward=-1.0, next_state=[-0.14526729 -0.54609929  0.23149582  0.966765  ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 127: Exploration_rate=0.01. Score=35.\n",
      "[ episode 128 ] state=[ 0.03417165  0.02890025 -0.01499403  0.00572418]\n",
      "[ episode 128 ][ timestamp 1 ] state=[ 0.03417165  0.02890025 -0.01499403  0.00572418], action=1, reward=1.0, next_state=[ 0.03474966  0.22423399 -0.01487955 -0.29165158]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 2 ] state=[ 0.03474966  0.22423399 -0.01487955 -0.29165158], action=1, reward=1.0, next_state=[ 0.03923434  0.4195649  -0.02071258 -0.58898994]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 3 ] state=[ 0.03923434  0.4195649  -0.02071258 -0.58898994], action=1, reward=1.0, next_state=[ 0.04762563  0.61497068 -0.03249238 -0.88812479]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 4 ] state=[ 0.04762563  0.61497068 -0.03249238 -0.88812479], action=0, reward=1.0, next_state=[ 0.05992505  0.42030442 -0.05025487 -0.60583061]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 5 ] state=[ 0.05992505  0.42030442 -0.05025487 -0.60583061], action=0, reward=1.0, next_state=[ 0.06833114  0.22591989 -0.06237148 -0.32939065]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 6 ] state=[ 0.06833114  0.22591989 -0.06237148 -0.32939065], action=0, reward=1.0, next_state=[ 0.07284953  0.03173872 -0.0689593  -0.05701059]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 7 ] state=[ 0.07284953  0.03173872 -0.0689593  -0.05701059], action=0, reward=1.0, next_state=[ 0.07348431 -0.16233018 -0.07009951  0.21314291]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 8 ] state=[ 0.07348431 -0.16233018 -0.07009951  0.21314291], action=0, reward=1.0, next_state=[ 0.0702377  -0.35638354 -0.06583665  0.48291569]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 9 ] state=[ 0.0702377  -0.35638354 -0.06583665  0.48291569], action=0, reward=1.0, next_state=[ 0.06311003 -0.55051751 -0.05617834  0.75414377]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 10 ] state=[ 0.06311003 -0.55051751 -0.05617834  0.75414377], action=0, reward=1.0, next_state=[ 0.05209968 -0.74482181 -0.04109546  1.02863268]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 11 ] state=[ 0.05209968 -0.74482181 -0.04109546  1.02863268], action=1, reward=1.0, next_state=[ 0.03720325 -0.54917773 -0.02052281  0.72333566]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 12 ] state=[ 0.03720325 -0.54917773 -0.02052281  0.72333566], action=1, reward=1.0, next_state=[ 0.02621969 -0.35377803 -0.00605609  0.42426455]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 13 ] state=[ 0.02621969 -0.35377803 -0.00605609  0.42426455], action=1, reward=1.0, next_state=[ 0.01914413 -0.15857081  0.0024292   0.12967862]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 14 ] state=[ 0.01914413 -0.15857081  0.0024292   0.12967862], action=1, reward=1.0, next_state=[ 0.01597272  0.03651626  0.00502277 -0.16223694]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 15 ] state=[ 0.01597272  0.03651626  0.00502277 -0.16223694], action=0, reward=1.0, next_state=[ 0.01670304 -0.15867724  0.00177803  0.1320263 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 16 ] state=[ 0.01670304 -0.15867724  0.00177803  0.1320263 ], action=1, reward=1.0, next_state=[ 0.0135295   0.0364192   0.00441856 -0.16009516]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 17 ] state=[ 0.0135295   0.0364192   0.00441856 -0.16009516], action=0, reward=1.0, next_state=[ 0.01425788 -0.15876573  0.00121665  0.13397843]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 18 ] state=[ 0.01425788 -0.15876573  0.00121665  0.13397843], action=0, reward=1.0, next_state=[ 0.01108256 -0.35390509  0.00389622  0.42704495]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 19 ] state=[ 0.01108256 -0.35390509  0.00389622  0.42704495], action=1, reward=1.0, next_state=[ 0.00400446 -0.15883854  0.01243712  0.13559283]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 20 ] state=[ 0.00400446 -0.15883854  0.01243712  0.13559283], action=0, reward=1.0, next_state=[ 0.00082769 -0.3541364   0.01514898  0.43217339]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 21 ] state=[ 0.00082769 -0.3541364   0.01514898  0.43217339], action=1, reward=1.0, next_state=[-0.00625504 -0.15923219  0.02379244  0.14430424]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 22 ] state=[-0.00625504 -0.15923219  0.02379244  0.14430424], action=1, reward=1.0, next_state=[-0.00943968  0.03554109  0.02667853 -0.14077867]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 23 ] state=[-0.00943968  0.03554109  0.02667853 -0.14077867], action=0, reward=1.0, next_state=[-0.00872886 -0.15995259  0.02386296  0.16020007]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 24 ] state=[-0.00872886 -0.15995259  0.02386296  0.16020007], action=0, reward=1.0, next_state=[-0.01192791 -0.35540791  0.02706696  0.46031461]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 25 ] state=[-0.01192791 -0.35540791  0.02706696  0.46031461], action=0, reward=1.0, next_state=[-0.01903607 -0.55090177  0.03627325  0.76140471]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 26 ] state=[-0.01903607 -0.55090177  0.03627325  0.76140471], action=1, reward=1.0, next_state=[-0.0300541  -0.35629779  0.05150134  0.48035276]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 128 ][ timestamp 27 ] state=[-0.0300541  -0.35629779  0.05150134  0.48035276], action=0, reward=1.0, next_state=[-0.03718006 -0.55210749  0.0611084   0.78881256]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 28 ] state=[-0.03718006 -0.55210749  0.0611084   0.78881256], action=1, reward=1.0, next_state=[-0.04822221 -0.35787567  0.07688465  0.51596334]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 29 ] state=[-0.04822221 -0.35787567  0.07688465  0.51596334], action=1, reward=1.0, next_state=[-0.05537972 -0.1639158   0.08720392  0.24846484]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 30 ] state=[-0.05537972 -0.1639158   0.08720392  0.24846484], action=1, reward=1.0, next_state=[-0.05865804  0.02985955  0.09217321 -0.01548823]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 31 ] state=[-0.05865804  0.02985955  0.09217321 -0.01548823], action=1, reward=1.0, next_state=[-0.05806085  0.22354703  0.09186345 -0.2777256 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 32 ] state=[-0.05806085  0.22354703  0.09186345 -0.2777256 ], action=0, reward=1.0, next_state=[-0.05358991  0.0272428   0.08630894  0.04245906]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 33 ] state=[-0.05358991  0.0272428   0.08630894  0.04245906], action=1, reward=1.0, next_state=[-0.05304505  0.22102788  0.08715812 -0.22179323]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 34 ] state=[-0.05304505  0.22102788  0.08715812 -0.22179323], action=0, reward=1.0, next_state=[-0.04862449  0.02477526  0.08272225  0.09706033]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 35 ] state=[-0.04862449  0.02477526  0.08272225  0.09706033], action=1, reward=1.0, next_state=[-0.04812899  0.21862018  0.08466346 -0.16842015]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 36 ] state=[-0.04812899  0.21862018  0.08466346 -0.16842015], action=0, reward=1.0, next_state=[-0.04375658  0.02239484  0.08129506  0.14972494]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 37 ] state=[-0.04375658  0.02239484  0.08129506  0.14972494], action=0, reward=1.0, next_state=[-0.04330869 -0.17379144  0.08428956  0.46690688]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 38 ] state=[-0.04330869 -0.17379144  0.08428956  0.46690688], action=1, reward=1.0, next_state=[-0.04678452  0.02004477  0.09362769  0.20193662]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 39 ] state=[-0.04678452  0.02004477  0.09362769  0.20193662], action=0, reward=1.0, next_state=[-0.04638362 -0.17628282  0.09766643  0.52262452]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 40 ] state=[-0.04638362 -0.17628282  0.09766643  0.52262452], action=1, reward=1.0, next_state=[-0.04990928  0.01733861  0.10811892  0.26224474]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 41 ] state=[-0.04990928  0.01733861  0.10811892  0.26224474], action=1, reward=1.0, next_state=[-0.0495625   0.21076448  0.11336381  0.00552518]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 42 ] state=[-0.0495625   0.21076448  0.11336381  0.00552518], action=1, reward=1.0, next_state=[-0.04534722  0.4040935   0.11347431 -0.24934933]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 43 ] state=[-0.04534722  0.4040935   0.11347431 -0.24934933], action=1, reward=1.0, next_state=[-0.03726535  0.59742749  0.10848733 -0.50419533]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 44 ] state=[-0.03726535  0.59742749  0.10848733 -0.50419533], action=1, reward=1.0, next_state=[-0.0253168   0.79086668  0.09840342 -0.76081552]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 45 ] state=[-0.0253168   0.79086668  0.09840342 -0.76081552], action=0, reward=1.0, next_state=[-0.00949946  0.59453671  0.08318711 -0.43886131]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 46 ] state=[-0.00949946  0.59453671  0.08318711 -0.43886131], action=0, reward=1.0, next_state=[ 0.00239127  0.39834196  0.07440988 -0.12115805]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 47 ] state=[ 0.00239127  0.39834196  0.07440988 -0.12115805], action=0, reward=1.0, next_state=[0.01035811 0.20223716 0.07198672 0.1940415 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 48 ] state=[0.01035811 0.20223716 0.07198672 0.1940415 ], action=0, reward=1.0, next_state=[0.01440285 0.00616324 0.07586755 0.50853648]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 49 ] state=[0.01440285 0.00616324 0.07586755 0.50853648], action=1, reward=1.0, next_state=[0.01452612 0.20013889 0.08603828 0.24069364]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 50 ] state=[0.01452612 0.20013889 0.08603828 0.24069364], action=0, reward=1.0, next_state=[0.0185289  0.00389999 0.09085216 0.55922721]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 51 ] state=[0.0185289  0.00389999 0.09085216 0.55922721], action=1, reward=1.0, next_state=[0.0186069  0.19763714 0.1020367  0.29649381]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 52 ] state=[0.0186069  0.19763714 0.1020367  0.29649381], action=0, reward=1.0, next_state=[0.02255964 0.00121982 0.10796658 0.61953414]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 53 ] state=[0.02255964 0.00121982 0.10796658 0.61953414], action=0, reward=1.0, next_state=[ 0.02258404 -0.19523126  0.12035726  0.94417548]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 54 ] state=[ 0.02258404 -0.19523126  0.12035726  0.94417548], action=0, reward=1.0, next_state=[ 0.01867941 -0.39175097  0.13924077  1.27212222]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 55 ] state=[ 0.01867941 -0.39175097  0.13924077  1.27212222], action=0, reward=1.0, next_state=[ 0.01084439 -0.5883479   0.16468321  1.60496814]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 56 ] state=[ 0.01084439 -0.5883479   0.16468321  1.60496814], action=1, reward=1.0, next_state=[-9.22566338e-04 -3.95512499e-01  1.96782576e-01  1.36782685e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 57 ] state=[-9.22566338e-04 -3.95512499e-01  1.96782576e-01  1.36782685e+00], action=1, reward=-1.0, next_state=[-0.00883282 -0.20332109  0.22413911  1.1425849 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 128: Exploration_rate=0.01. Score=57.\n",
      "[ episode 129 ] state=[-0.02702943  0.01478727 -0.03010024 -0.00744901]\n",
      "[ episode 129 ][ timestamp 1 ] state=[-0.02702943  0.01478727 -0.03010024 -0.00744901], action=0, reward=1.0, next_state=[-0.02673368 -0.17989036 -0.03024922  0.27558702]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 2 ] state=[-0.02673368 -0.17989036 -0.03024922  0.27558702], action=0, reward=1.0, next_state=[-0.03033149 -0.37456797 -0.02473748  0.55857793]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 3 ] state=[-0.03033149 -0.37456797 -0.02473748  0.55857793], action=1, reward=1.0, next_state=[-0.03782285 -0.17910767 -0.01356592  0.25820511]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 4 ] state=[-0.03782285 -0.17910767 -0.01356592  0.25820511], action=1, reward=1.0, next_state=[-0.041405    0.0162053  -0.00840181 -0.03872564]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 5 ] state=[-0.041405    0.0162053  -0.00840181 -0.03872564], action=0, reward=1.0, next_state=[-0.0410809  -0.17879517 -0.00917633  0.25129463]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 6 ] state=[-0.0410809  -0.17879517 -0.00917633  0.25129463], action=0, reward=1.0, next_state=[-0.0446568  -0.37378489 -0.00415043  0.54106909]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 7 ] state=[-0.0446568  -0.37378489 -0.00415043  0.54106909], action=0, reward=1.0, next_state=[-0.0521325  -0.56884826  0.00667095  0.8324414 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 8 ] state=[-0.0521325  -0.56884826  0.00667095  0.8324414 ], action=1, reward=1.0, next_state=[-0.06350946 -0.3738181   0.02331978  0.54186392]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 9 ] state=[-0.06350946 -0.3738181   0.02331978  0.54186392], action=1, reward=1.0, next_state=[-0.07098582 -0.17903154  0.03415705  0.25661891]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 10 ] state=[-0.07098582 -0.17903154  0.03415705  0.25661891], action=1, reward=1.0, next_state=[-0.07456645  0.01558652  0.03928943 -0.02509767]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 11 ] state=[-0.07456645  0.01558652  0.03928943 -0.02509767], action=1, reward=1.0, next_state=[-0.07425472  0.21012364  0.03878748 -0.30513005]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 12 ] state=[-0.07425472  0.21012364  0.03878748 -0.30513005], action=0, reward=1.0, next_state=[-0.07005225  0.01447103  0.03268488 -0.0004712 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 13 ] state=[-0.07005225  0.01447103  0.03268488 -0.0004712 ], action=0, reward=1.0, next_state=[-0.06976283 -0.18110405  0.03267545  0.30234238]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 14 ] state=[-0.06976283 -0.18110405  0.03267545  0.30234238], action=1, reward=1.0, next_state=[-0.07338491  0.01353733  0.0387223   0.02014103]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 129 ][ timestamp 15 ] state=[-0.07338491  0.01353733  0.0387223   0.02014103], action=0, reward=1.0, next_state=[-0.07311417 -0.18211793  0.03912512  0.32478543]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 16 ] state=[-0.07311417 -0.18211793  0.03912512  0.32478543], action=1, reward=1.0, next_state=[-0.07675652  0.01242573  0.04562083  0.04469312]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 17 ] state=[-0.07675652  0.01242573  0.04562083  0.04469312], action=0, reward=1.0, next_state=[-0.07650801 -0.18331971  0.04651469  0.35141365]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 18 ] state=[-0.07650801 -0.18331971  0.04651469  0.35141365], action=0, reward=1.0, next_state=[-0.0801744  -0.37907122  0.05354297  0.65839371]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 19 ] state=[-0.0801744  -0.37907122  0.05354297  0.65839371], action=1, reward=1.0, next_state=[-0.08775583 -0.18473378  0.06671084  0.38303941]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 20 ] state=[-0.08775583 -0.18473378  0.06671084  0.38303941], action=0, reward=1.0, next_state=[-0.0914505  -0.38073635  0.07437163  0.69598775]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 21 ] state=[-0.0914505  -0.38073635  0.07437163  0.69598775], action=1, reward=1.0, next_state=[-0.09906523 -0.18672033  0.08829138  0.4276133 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 22 ] state=[-0.09906523 -0.18672033  0.08829138  0.4276133 ], action=1, reward=1.0, next_state=[-0.10279964  0.00704746  0.09684365  0.16401771]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 23 ] state=[-0.10279964  0.00704746  0.09684365  0.16401771], action=1, reward=1.0, next_state=[-0.10265869  0.20065928  0.100124   -0.09661168]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 24 ] state=[-0.10265869  0.20065928  0.100124   -0.09661168], action=1, reward=1.0, next_state=[-0.0986455   0.39421429  0.09819177 -0.35610284]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 25 ] state=[-0.0986455   0.39421429  0.09819177 -0.35610284], action=0, reward=1.0, next_state=[-0.09076122  0.19784338  0.09106971 -0.03414333]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 26 ] state=[-0.09076122  0.19784338  0.09106971 -0.03414333], action=1, reward=1.0, next_state=[-0.08680435  0.39154935  0.09038685 -0.2967607 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 27 ] state=[-0.08680435  0.39154935  0.09038685 -0.2967607 ], action=0, reward=1.0, next_state=[-0.07897336  0.19526294  0.08445163  0.02300458]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 28 ] state=[-0.07897336  0.19526294  0.08445163  0.02300458], action=1, reward=1.0, next_state=[-0.0750681   0.38907862  0.08491172 -0.24188355]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 29 ] state=[-0.0750681   0.38907862  0.08491172 -0.24188355], action=1, reward=1.0, next_state=[-0.06728653  0.5828915   0.08007405 -0.50662139]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 30 ] state=[-0.06728653  0.5828915   0.08007405 -0.50662139], action=1, reward=1.0, next_state=[-0.0556287   0.77679922  0.06994162 -0.77303437]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 31 ] state=[-0.0556287   0.77679922  0.06994162 -0.77303437], action=1, reward=1.0, next_state=[-0.04009272  0.97089276  0.05448094 -1.04291679]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 32 ] state=[-0.04009272  0.97089276  0.05448094 -1.04291679], action=1, reward=1.0, next_state=[-0.02067486  1.16525053  0.0336226  -1.31801142]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 33 ] state=[-0.02067486  1.16525053  0.0336226  -1.31801142], action=0, reward=1.0, next_state=[ 0.00263015  0.96971989  0.00726237 -1.01499803]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 34 ] state=[ 0.00263015  0.96971989  0.00726237 -1.01499803], action=0, reward=1.0, next_state=[ 0.02202455  0.77450184 -0.01303759 -0.72004357]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 35 ] state=[ 0.02202455  0.77450184 -0.01303759 -0.72004357], action=0, reward=1.0, next_state=[ 0.03751458  0.57956267 -0.02743846 -0.43149261]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 36 ] state=[ 0.03751458  0.57956267 -0.02743846 -0.43149261], action=0, reward=1.0, next_state=[ 0.04910584  0.38483977 -0.03606831 -0.1475841 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 37 ] state=[ 0.04910584  0.38483977 -0.03606831 -0.1475841 ], action=0, reward=1.0, next_state=[ 0.05680263  0.19025239 -0.03901999  0.13350535]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 38 ] state=[ 0.05680263  0.19025239 -0.03901999  0.13350535], action=0, reward=1.0, next_state=[ 0.06060768 -0.00428954 -0.03634989  0.41362715]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 39 ] state=[ 0.06060768 -0.00428954 -0.03634989  0.41362715], action=0, reward=1.0, next_state=[ 0.06052189 -0.1988779  -0.02807734  0.69463237]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 40 ] state=[ 0.06052189 -0.1988779  -0.02807734  0.69463237], action=1, reward=1.0, next_state=[ 0.05654433 -0.00337798 -0.0141847   0.39324442]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 41 ] state=[ 0.05654433 -0.00337798 -0.0141847   0.39324442], action=1, reward=1.0, next_state=[ 0.05647677  0.19194236 -0.00631981  0.09612322]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 42 ] state=[ 0.05647677  0.19194236 -0.00631981  0.09612322], action=0, reward=1.0, next_state=[ 0.06031562 -0.00308844 -0.00439734  0.38680558]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 43 ] state=[ 0.06031562 -0.00308844 -0.00439734  0.38680558], action=1, reward=1.0, next_state=[0.06025385 0.19209565 0.00333877 0.09273945]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 44 ] state=[0.06025385 0.19209565 0.00333877 0.09273945], action=0, reward=1.0, next_state=[ 0.06409576 -0.00307399  0.00519356  0.38647388]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 45 ] state=[ 0.06409576 -0.00307399  0.00519356  0.38647388], action=1, reward=1.0, next_state=[0.06403428 0.19197385 0.01292303 0.09543296]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 46 ] state=[0.06403428 0.19197385 0.01292303 0.09543296], action=1, reward=1.0, next_state=[ 0.06787376  0.38690822  0.01483169 -0.19314491]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 47 ] state=[ 0.06787376  0.38690822  0.01483169 -0.19314491], action=1, reward=1.0, next_state=[ 0.07561192  0.58181489  0.0109688  -0.48111241]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 48 ] state=[ 0.07561192  0.58181489  0.0109688  -0.48111241], action=1, reward=1.0, next_state=[ 0.08724822  0.77678031  0.00134655 -0.77031818]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 49 ] state=[ 0.08724822  0.77678031  0.00134655 -0.77031818], action=0, reward=1.0, next_state=[ 0.10278383  0.58163985 -0.01405982 -0.47721187]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 50 ] state=[ 0.10278383  0.58163985 -0.01405982 -0.47721187], action=1, reward=1.0, next_state=[ 0.11441663  0.77695747 -0.02360405 -0.77429279]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 51 ] state=[ 0.11441663  0.77695747 -0.02360405 -0.77429279], action=0, reward=1.0, next_state=[ 0.12995578  0.58216804 -0.03908991 -0.48912899]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 52 ] state=[ 0.12995578  0.58216804 -0.03908991 -0.48912899], action=0, reward=1.0, next_state=[ 0.14159914  0.38761874 -0.04887249 -0.20901748]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 53 ] state=[ 0.14159914  0.38761874 -0.04887249 -0.20901748], action=0, reward=1.0, next_state=[ 0.14935151  0.19322844 -0.05305284  0.06785702]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 54 ] state=[ 0.14935151  0.19322844 -0.05305284  0.06785702], action=1, reward=1.0, next_state=[ 0.15321608  0.38906929 -0.0516957  -0.24108116]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 55 ] state=[ 0.15321608  0.38906929 -0.0516957  -0.24108116], action=0, reward=1.0, next_state=[ 0.16099747  0.19472245 -0.05651732  0.03485789]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 56 ] state=[ 0.16099747  0.19472245 -0.05651732  0.03485789], action=0, reward=1.0, next_state=[ 0.16489191  0.00045461 -0.05582016  0.30918713]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 57 ] state=[ 0.16489191  0.00045461 -0.05582016  0.30918713], action=1, reward=1.0, next_state=[ 0.16490101  0.19632562 -0.04963642 -0.00056439]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 58 ] state=[ 0.16490101  0.19632562 -0.04963642 -0.00056439], action=0, reward=1.0, next_state=[ 0.16882752  0.00194938 -0.04964771  0.27605377]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 59 ] state=[ 0.16882752  0.00194938 -0.04964771  0.27605377], action=0, reward=1.0, next_state=[ 0.16886651 -0.19243038 -0.04412663  0.55267371]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 129 ][ timestamp 60 ] state=[ 0.16886651 -0.19243038 -0.04412663  0.55267371], action=1, reward=1.0, next_state=[ 0.1650179   0.00328257 -0.03307316  0.24642102]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 61 ] state=[ 0.1650179   0.00328257 -0.03307316  0.24642102], action=0, reward=1.0, next_state=[ 0.16508355 -0.19135179 -0.02814474  0.52849117]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 62 ] state=[ 0.16508355 -0.19135179 -0.02814474  0.52849117], action=1, reward=1.0, next_state=[ 0.16125651  0.00415459 -0.01757492  0.22707429]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 63 ] state=[ 0.16125651  0.00415459 -0.01757492  0.22707429], action=0, reward=1.0, next_state=[ 0.16133961 -0.19071185 -0.01303343  0.51416204]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 64 ] state=[ 0.16133961 -0.19071185 -0.01303343  0.51416204], action=1, reward=1.0, next_state=[ 0.15752537  0.00459121 -0.00275019  0.21740061]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 65 ] state=[ 0.15752537  0.00459121 -0.00275019  0.21740061], action=0, reward=1.0, next_state=[ 0.15761719 -0.19049132  0.00159782  0.50921474]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 66 ] state=[ 0.15761719 -0.19049132  0.00159782  0.50921474], action=1, reward=1.0, next_state=[0.15380737 0.00460809 0.01178212 0.21703577]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 67 ] state=[0.15380737 0.00460809 0.01178212 0.21703577], action=0, reward=1.0, next_state=[ 0.15389953 -0.1906803   0.01612283  0.51341187]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 68 ] state=[ 0.15389953 -0.1906803   0.01612283  0.51341187], action=0, reward=1.0, next_state=[ 0.15008592 -0.38602557  0.02639107  0.81113161]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 69 ] state=[ 0.15008592 -0.38602557  0.02639107  0.81113161], action=1, reward=1.0, next_state=[ 0.14236541 -0.19127491  0.0426137   0.52686543]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 70 ] state=[ 0.14236541 -0.19127491  0.0426137   0.52686543], action=0, reward=1.0, next_state=[ 0.13853991 -0.38696975  0.05315101  0.83266582]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 71 ] state=[ 0.13853991 -0.38696975  0.05315101  0.83266582], action=0, reward=1.0, next_state=[ 0.13080052 -0.58277615  0.06980433  1.1415797 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 72 ] state=[ 0.13080052 -0.58277615  0.06980433  1.1415797 ], action=0, reward=1.0, next_state=[ 0.119145   -0.77873752  0.09263592  1.45531172]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 73 ] state=[ 0.119145   -0.77873752  0.09263592  1.45531172], action=0, reward=1.0, next_state=[ 0.10357025 -0.97486648  0.12174216  1.77543979]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 74 ] state=[ 0.10357025 -0.97486648  0.12174216  1.77543979], action=1, reward=1.0, next_state=[ 0.08407292 -0.78130869  0.15725095  1.52295586]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 75 ] state=[ 0.08407292 -0.78130869  0.15725095  1.52295586], action=1, reward=1.0, next_state=[ 0.06844674 -0.58839677  0.18771007  1.28319981]\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 76 ] state=[ 0.06844674 -0.58839677  0.18771007  1.28319981], action=0, reward=-1.0, next_state=[ 0.05667881 -0.78534635  0.21337407  1.62829805]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 129: Exploration_rate=0.01. Score=76.\n",
      "[ episode 130 ] state=[ 0.03811173 -0.04206317 -0.00359147 -0.01091699]\n",
      "[ episode 130 ][ timestamp 1 ] state=[ 0.03811173 -0.04206317 -0.00359147 -0.01091699], action=1, reward=1.0, next_state=[ 0.03727046  0.1531101  -0.00380981 -0.3047309 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 2 ] state=[ 0.03727046  0.1531101  -0.00380981 -0.3047309 ], action=1, reward=1.0, next_state=[ 0.04033266  0.34828614 -0.00990442 -0.59861291]\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 3 ] state=[ 0.04033266  0.34828614 -0.00990442 -0.59861291], action=0, reward=1.0, next_state=[ 0.04729839  0.15330416 -0.02187668 -0.30906614]\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 4 ] state=[ 0.04729839  0.15330416 -0.02187668 -0.30906614], action=1, reward=1.0, next_state=[ 0.05036447  0.34873087 -0.02805801 -0.6085673 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 5 ] state=[ 0.05036447  0.34873087 -0.02805801 -0.6085673 ], action=0, reward=1.0, next_state=[ 0.05733909  0.15401218 -0.04022935 -0.32485219]\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 6 ] state=[ 0.05733909  0.15401218 -0.04022935 -0.32485219], action=0, reward=1.0, next_state=[ 0.06041933 -0.04051456 -0.0467264  -0.0451224 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 7 ] state=[ 0.06041933 -0.04051456 -0.0467264  -0.0451224 ], action=1, reward=1.0, next_state=[ 0.05960904  0.15524519 -0.04762884 -0.3521741 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 8 ] state=[ 0.05960904  0.15524519 -0.04762884 -0.3521741 ], action=0, reward=1.0, next_state=[ 0.06271394 -0.03916824 -0.05467233 -0.07488224]\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 9 ] state=[ 0.06271394 -0.03916824 -0.05467233 -0.07488224], action=1, reward=1.0, next_state=[ 0.06193058  0.15669314 -0.05616997 -0.38430099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 10 ] state=[ 0.06193058  0.15669314 -0.05616997 -0.38430099], action=1, reward=1.0, next_state=[ 0.06506444  0.3525657  -0.06385599 -0.69415175]\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 11 ] state=[ 0.06506444  0.3525657  -0.06385599 -0.69415175], action=1, reward=1.0, next_state=[ 0.07211576  0.54851257 -0.07773902 -1.00623392]\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 12 ] state=[ 0.07211576  0.54851257 -0.07773902 -1.00623392], action=0, reward=1.0, next_state=[ 0.08308601  0.35450992 -0.0978637  -0.73894109]\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 13 ] state=[ 0.08308601  0.35450992 -0.0978637  -0.73894109], action=0, reward=1.0, next_state=[ 0.09017621  0.16086575 -0.11264252 -0.47859069]\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 14 ] state=[ 0.09017621  0.16086575 -0.11264252 -0.47859069], action=1, reward=1.0, next_state=[ 0.09339352  0.35738274 -0.12221434 -0.80454495]\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 15 ] state=[ 0.09339352  0.35738274 -0.12221434 -0.80454495], action=0, reward=1.0, next_state=[ 0.10054118  0.16412929 -0.13830524 -0.5526686 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 16 ] state=[ 0.10054118  0.16412929 -0.13830524 -0.5526686 ], action=1, reward=1.0, next_state=[ 0.10382376  0.36089481 -0.14935861 -0.88553076]\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 17 ] state=[ 0.10382376  0.36089481 -0.14935861 -0.88553076], action=1, reward=1.0, next_state=[ 0.11104166  0.55769437 -0.16706922 -1.22119192]\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 18 ] state=[ 0.11104166  0.55769437 -0.16706922 -1.22119192], action=1, reward=1.0, next_state=[ 0.12219554  0.75452778 -0.19149306 -1.56122125]\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 19 ] state=[ 0.12219554  0.75452778 -0.19149306 -1.56122125], action=1, reward=-1.0, next_state=[ 0.1372861   0.9513551  -0.22271749 -1.90702108]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 130: Exploration_rate=0.01. Score=19.\n",
      "[ episode 131 ] state=[ 0.01095847 -0.01497615 -0.03708367 -0.04765981]\n",
      "[ episode 131 ][ timestamp 1 ] state=[ 0.01095847 -0.01497615 -0.03708367 -0.04765981], action=1, reward=1.0, next_state=[ 0.01065895  0.18065739 -0.03803686 -0.35180846]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 2 ] state=[ 0.01065895  0.18065739 -0.03803686 -0.35180846], action=1, reward=1.0, next_state=[ 0.0142721   0.37629903 -0.04507303 -0.65623879]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 3 ] state=[ 0.0142721   0.37629903 -0.04507303 -0.65623879], action=0, reward=1.0, next_state=[ 0.02179808  0.18183257 -0.05819781 -0.37808235]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 4 ] state=[ 0.02179808  0.18183257 -0.05819781 -0.37808235], action=0, reward=1.0, next_state=[ 0.02543473 -0.01241665 -0.06575946 -0.10430232]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 5 ] state=[ 0.02543473 -0.01241665 -0.06575946 -0.10430232], action=0, reward=1.0, next_state=[ 0.02518639 -0.20653759 -0.0678455   0.16693039]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 6 ] state=[ 0.02518639 -0.20653759 -0.0678455   0.16693039], action=0, reward=1.0, next_state=[ 0.02105564 -0.40062609 -0.06450689  0.43746208]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 7 ] state=[ 0.02105564 -0.40062609 -0.06450689  0.43746208], action=0, reward=1.0, next_state=[ 0.01304312 -0.59477849 -0.05575765  0.70913309]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 8 ] state=[ 0.01304312 -0.59477849 -0.05575765  0.70913309], action=1, reward=1.0, next_state=[ 0.00114755 -0.39893039 -0.04157499  0.39943322]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 9 ] state=[ 0.00114755 -0.39893039 -0.04157499  0.39943322], action=0, reward=1.0, next_state=[-0.00683106 -0.59343867 -0.03358633  0.67872399]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 131 ][ timestamp 10 ] state=[-0.00683106 -0.59343867 -0.03358633  0.67872399], action=1, reward=1.0, next_state=[-0.01869983 -0.39786663 -0.02001185  0.37565886]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 11 ] state=[-0.01869983 -0.39786663 -0.02001185  0.37565886], action=1, reward=1.0, next_state=[-0.02665716 -0.20246624 -0.01249867  0.07673387]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 12 ] state=[-0.02665716 -0.20246624 -0.01249867  0.07673387], action=1, reward=1.0, next_state=[-0.03070649 -0.00716736 -0.01096399 -0.21986608]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 13 ] state=[-0.03070649 -0.00716736 -0.01096399 -0.21986608], action=0, reward=1.0, next_state=[-0.03084983 -0.20213089 -0.01536131  0.06933828]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 14 ] state=[-0.03084983 -0.20213089 -0.01536131  0.06933828], action=0, reward=1.0, next_state=[-0.03489245 -0.39702928 -0.01397455  0.35713532]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 15 ] state=[-0.03489245 -0.39702928 -0.01397455  0.35713532], action=1, reward=1.0, next_state=[-0.04283304 -0.20171146 -0.00683184  0.06007883]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 16 ] state=[-0.04283304 -0.20171146 -0.00683184  0.06007883], action=1, reward=1.0, next_state=[-0.04686727 -0.00649223 -0.00563027 -0.23475174]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 17 ] state=[-0.04686727 -0.00649223 -0.00563027 -0.23475174], action=0, reward=1.0, next_state=[-0.04699711 -0.20153329 -0.0103253   0.05614992]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 18 ] state=[-0.04699711 -0.20153329 -0.0103253   0.05614992], action=1, reward=1.0, next_state=[-0.05102778 -0.00626482 -0.0092023  -0.23977275]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 19 ] state=[-0.05102778 -0.00626482 -0.0092023  -0.23977275], action=0, reward=1.0, next_state=[-0.05115307 -0.20125411 -0.01399776  0.04999337]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 20 ] state=[-0.05115307 -0.20125411 -0.01399776  0.04999337], action=1, reward=1.0, next_state=[-0.05517816 -0.00593427 -0.01299789 -0.24707289]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 21 ] state=[-0.05517816 -0.00593427 -0.01299789 -0.24707289], action=0, reward=1.0, next_state=[-0.05529684 -0.2008682  -0.01793935  0.04148203]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 22 ] state=[-0.05529684 -0.2008682  -0.01793935  0.04148203], action=0, reward=1.0, next_state=[-0.05931421 -0.39572837 -0.01710971  0.32845137]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 23 ] state=[-0.05931421 -0.39572837 -0.01710971  0.32845137], action=1, reward=1.0, next_state=[-0.06722877 -0.20036708 -0.01054068  0.03042231]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 24 ] state=[-0.06722877 -0.20036708 -0.01054068  0.03042231], action=0, reward=1.0, next_state=[-0.07123611 -0.39533629 -0.00993223  0.31976099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 25 ] state=[-0.07123611 -0.39533629 -0.00993223  0.31976099], action=1, reward=1.0, next_state=[-0.07914284 -0.20007431 -0.00353701  0.02396243]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 26 ] state=[-0.07914284 -0.20007431 -0.00353701  0.02396243], action=1, reward=1.0, next_state=[-0.08314433 -0.00490181 -0.00305777 -0.26983437]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 27 ] state=[-0.08314433 -0.00490181 -0.00305777 -0.26983437], action=1, reward=1.0, next_state=[-0.08324236  0.19026364 -0.00845445 -0.56348016]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 28 ] state=[-0.08324236  0.19026364 -0.00845445 -0.56348016], action=0, reward=1.0, next_state=[-0.07943709 -0.00473867 -0.01972406 -0.27347273]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 29 ] state=[-0.07943709 -0.00473867 -0.01972406 -0.27347273], action=0, reward=1.0, next_state=[-0.07953186 -0.19957371 -0.02519351  0.01292449]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 30 ] state=[-0.07953186 -0.19957371 -0.02519351  0.01292449], action=0, reward=1.0, next_state=[-0.08352334 -0.39432547 -0.02493502  0.29755331]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 31 ] state=[-0.08352334 -0.39432547 -0.02493502  0.29755331], action=1, reward=1.0, next_state=[-0.09140985 -0.1988571  -0.01898395 -0.00288822]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 32 ] state=[-0.09140985 -0.1988571  -0.01898395 -0.00288822], action=0, reward=1.0, next_state=[-0.09538699 -0.39370173 -0.01904172  0.2837451 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 33 ] state=[-0.09538699 -0.39370173 -0.01904172  0.2837451 ], action=1, reward=1.0, next_state=[-0.10326102 -0.19831343 -0.01336682 -0.01488213]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 34 ] state=[-0.10326102 -0.19831343 -0.01336682 -0.01488213], action=0, reward=1.0, next_state=[-0.10722729 -0.39324116 -0.01366446  0.27355361]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 35 ] state=[-0.10722729 -0.39324116 -0.01366446  0.27355361], action=1, reward=1.0, next_state=[-0.11509212 -0.19792693 -0.00819339 -0.02340761]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 36 ] state=[-0.11509212 -0.19792693 -0.00819339 -0.02340761], action=1, reward=1.0, next_state=[-0.11905065 -0.00268845 -0.00866154 -0.31866434]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 37 ] state=[-0.11905065 -0.00268845 -0.00866154 -0.31866434], action=0, reward=1.0, next_state=[-0.11910442 -0.19768597 -0.01503483 -0.02872548]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 38 ] state=[-0.11910442 -0.19768597 -0.01503483 -0.02872548], action=0, reward=1.0, next_state=[-0.12305814 -0.39258912 -0.01560934  0.25917613]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 39 ] state=[-0.12305814 -0.39258912 -0.01560934  0.25917613], action=0, reward=1.0, next_state=[-0.13090993 -0.5874848  -0.01042581  0.54689507]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 40 ] state=[-0.13090993 -0.5874848  -0.01042581  0.54689507], action=0, reward=1.0, next_state=[-1.42659621e-01 -7.82458730e-01  5.12088318e-04  8.36274940e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 41 ] state=[-1.42659621e-01 -7.82458730e-01  5.12088318e-04  8.36274940e-01], action=1, reward=1.0, next_state=[-0.1583088  -0.58734378  0.01723759  0.5437531 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 42 ] state=[-0.1583088  -0.58734378  0.01723759  0.5437531 ], action=0, reward=1.0, next_state=[-0.17005567 -0.78270367  0.02811265  0.84181701]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 43 ] state=[-0.17005567 -0.78270367  0.02811265  0.84181701], action=1, reward=1.0, next_state=[-0.18570974 -0.58797651  0.04494899  0.55810571]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 44 ] state=[-0.18570974 -0.58797651  0.04494899  0.55810571], action=1, reward=1.0, next_state=[-0.19746927 -0.3935134   0.0561111   0.27991622]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 45 ] state=[-0.19746927 -0.3935134   0.0561111   0.27991622], action=1, reward=1.0, next_state=[-0.20533954 -0.1992349   0.06170943  0.00544512]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 46 ] state=[-0.20533954 -0.1992349   0.06170943  0.00544512], action=1, reward=1.0, next_state=[-0.20932424 -0.00504974  0.06181833 -0.26714713]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 47 ] state=[-0.20932424 -0.00504974  0.06181833 -0.26714713], action=1, reward=1.0, next_state=[-0.20942524  0.18913797  0.05647539 -0.5397093 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 48 ] state=[-0.20942524  0.18913797  0.05647539 -0.5397093 ], action=1, reward=1.0, next_state=[-0.20564248  0.38342245  0.0456812  -0.81407645]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 49 ] state=[-0.20564248  0.38342245  0.0456812  -0.81407645], action=0, reward=1.0, next_state=[-0.19797403  0.18770567  0.02939967 -0.50738193]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 50 ] state=[-0.19797403  0.18770567  0.02939967 -0.50738193], action=0, reward=1.0, next_state=[-0.19421991 -0.00781793  0.01925203 -0.20558102]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 51 ] state=[-0.19421991 -0.00781793  0.01925203 -0.20558102], action=0, reward=1.0, next_state=[-0.19437627 -0.20320983  0.01514041  0.09311226]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 52 ] state=[-0.19437627 -0.20320983  0.01514041  0.09311226], action=1, reward=1.0, next_state=[-0.19844047 -0.00830812  0.01700266 -0.19475568]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 53 ] state=[-0.19844047 -0.00830812  0.01700266 -0.19475568], action=1, reward=1.0, next_state=[-0.19860663  0.18656654  0.01310755 -0.48202689]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 131 ][ timestamp 54 ] state=[-0.19860663  0.18656654  0.01310755 -0.48202689], action=0, reward=1.0, next_state=[-0.1948753  -0.00873794  0.00346701 -0.18524182]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 55 ] state=[-0.1948753  -0.00873794  0.00346701 -0.18524182], action=1, reward=1.0, next_state=[-1.95050059e-01  1.86334234e-01 -2.37828689e-04 -4.76829020e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 56 ] state=[-1.95050059e-01  1.86334234e-01 -2.37828689e-04 -4.76829020e-01], action=0, reward=1.0, next_state=[-0.19132337 -0.00878436 -0.00977441 -0.18422106]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 57 ] state=[-0.19132337 -0.00878436 -0.00977441 -0.18422106], action=0, reward=1.0, next_state=[-0.19149906 -0.2037651  -0.01345883  0.10536245]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 58 ] state=[-0.19149906 -0.2037651  -0.01345883  0.10536245], action=1, reward=1.0, next_state=[-0.19557436 -0.00845289 -0.01135158 -0.19153611]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 59 ] state=[-0.19557436 -0.00845289 -0.01135158 -0.19153611], action=0, reward=1.0, next_state=[-0.19574342 -0.20341062 -0.0151823   0.09754435]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 60 ] state=[-0.19574342 -0.20341062 -0.0151823   0.09754435], action=1, reward=1.0, next_state=[-0.19981163 -0.0080744  -0.01323142 -0.19988963]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 61 ] state=[-0.19981163 -0.0080744  -0.01323142 -0.19988963], action=1, reward=1.0, next_state=[-0.19997312  0.18723427 -0.01722921 -0.49671691]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 62 ] state=[-0.19997312  0.18723427 -0.01722921 -0.49671691], action=0, reward=1.0, next_state=[-0.19622844 -0.00764056 -0.02716355 -0.2095132 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 63 ] state=[-0.19622844 -0.00764056 -0.02716355 -0.2095132 ], action=1, reward=1.0, next_state=[-0.19638125  0.18785905 -0.03135381 -0.51063953]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 64 ] state=[-0.19638125  0.18785905 -0.03135381 -0.51063953], action=0, reward=1.0, next_state=[-0.19262407 -0.00680751 -0.0415666  -0.22799972]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 65 ] state=[-0.19262407 -0.00680751 -0.0415666  -0.22799972], action=0, reward=1.0, next_state=[-0.19276022 -0.20131155 -0.0461266   0.05128728]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 66 ] state=[-0.19276022 -0.20131155 -0.0461266   0.05128728], action=0, reward=1.0, next_state=[-0.19678645 -0.39574279 -0.04510085  0.32906751]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 67 ] state=[-0.19678645 -0.39574279 -0.04510085  0.32906751], action=1, reward=1.0, next_state=[-0.2047013  -0.20000878 -0.0385195   0.0225099 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 68 ] state=[-0.2047013  -0.20000878 -0.0385195   0.0225099 ], action=0, reward=1.0, next_state=[-0.20870148 -0.39455776 -0.0380693   0.30279496]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 69 ] state=[-0.20870148 -0.39455776 -0.0380693   0.30279496], action=1, reward=1.0, next_state=[-0.21659263 -0.19891451 -0.0320134  -0.00164696]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 70 ] state=[-0.21659263 -0.19891451 -0.0320134  -0.00164696], action=0, reward=1.0, next_state=[-0.22057092 -0.39356306 -0.03204634  0.28076594]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 71 ] state=[-0.22057092 -0.39356306 -0.03204634  0.28076594], action=1, reward=1.0, next_state=[-0.22844219 -0.19799899 -0.02643102 -0.02184957]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 72 ] state=[-0.22844219 -0.19799899 -0.02643102 -0.02184957], action=1, reward=1.0, next_state=[-0.23240217 -0.00250816 -0.02686802 -0.32275321]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 73 ] state=[-0.23240217 -0.00250816 -0.02686802 -0.32275321], action=0, reward=1.0, next_state=[-0.23245233 -0.19723741 -0.03332308 -0.03866299]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 74 ] state=[-0.23245233 -0.19723741 -0.03332308 -0.03866299], action=1, reward=1.0, next_state=[-0.23639708 -0.00165385 -0.03409634 -0.34167064]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 75 ] state=[-0.23639708 -0.00165385 -0.03409634 -0.34167064], action=0, reward=1.0, next_state=[-0.23643015 -0.19627452 -0.04092975 -0.05993169]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 76 ] state=[-0.23643015 -0.19627452 -0.04092975 -0.05993169], action=0, reward=1.0, next_state=[-0.24035564 -0.39078645 -0.04212839  0.21956185]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 77 ] state=[-0.24035564 -0.39078645 -0.04212839  0.21956185], action=0, reward=1.0, next_state=[-0.24817137 -0.58528167 -0.03773715  0.49866375]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 78 ] state=[-0.24817137 -0.58528167 -0.03773715  0.49866375], action=1, reward=1.0, next_state=[-0.25987701 -0.38964855 -0.02776387  0.19433091]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 79 ] state=[-0.25987701 -0.38964855 -0.02776387  0.19433091], action=1, reward=1.0, next_state=[-0.26766998 -0.19414068 -0.02387726 -0.10697941]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 80 ] state=[-0.26766998 -0.19414068 -0.02387726 -0.10697941], action=0, reward=1.0, next_state=[-0.27155279 -0.38891247 -0.02601684  0.17807575]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 81 ] state=[-0.27155279 -0.38891247 -0.02601684  0.17807575], action=0, reward=1.0, next_state=[-0.27933104 -0.58365263 -0.02245533  0.46243905]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 82 ] state=[-0.27933104 -0.58365263 -0.02245533  0.46243905], action=1, reward=1.0, next_state=[-0.29100409 -0.38822064 -0.01320655  0.16276365]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 83 ] state=[-0.29100409 -0.38822064 -0.01320655  0.16276365], action=0, reward=1.0, next_state=[-0.29876851 -0.58315106 -0.00995127  0.45125118]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 84 ] state=[-0.29876851 -0.58315106 -0.00995127  0.45125118], action=1, reward=1.0, next_state=[-0.31043153 -0.3878898  -0.00092625  0.15544816]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 85 ] state=[-0.31043153 -0.3878898  -0.00092625  0.15544816], action=1, reward=1.0, next_state=[-0.31818932 -0.1927546   0.00218271 -0.13752683]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 86 ] state=[-0.31818932 -0.1927546   0.00218271 -0.13752683], action=1, reward=1.0, next_state=[-0.32204442  0.00233602 -0.00056782 -0.42952035]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 87 ] state=[-0.32204442  0.00233602 -0.00056782 -0.42952035], action=1, reward=1.0, next_state=[-0.32199769  0.19746601 -0.00915823 -0.72238223]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 88 ] state=[-0.32199769  0.19746601 -0.00915823 -0.72238223], action=1, reward=1.0, next_state=[-0.31804837  0.39271344 -0.02360588 -1.01793357]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 89 ] state=[-0.31804837  0.39271344 -0.02360588 -1.01793357], action=1, reward=1.0, next_state=[-0.31019411  0.58814198 -0.04396455 -1.31793419]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 90 ] state=[-0.31019411  0.58814198 -0.04396455 -1.31793419], action=1, reward=1.0, next_state=[-0.29843127  0.7837915  -0.07032323 -1.62404631]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 91 ] state=[-0.29843127  0.7837915  -0.07032323 -1.62404631], action=0, reward=1.0, next_state=[-0.28275544  0.58956413 -0.10280416 -1.35408333]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 92 ] state=[-0.28275544  0.58956413 -0.10280416 -1.35408333], action=1, reward=1.0, next_state=[-0.27096415  0.78581527 -0.12988582 -1.67707704]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 93 ] state=[-0.27096415  0.78581527 -0.12988582 -1.67707704], action=1, reward=1.0, next_state=[-0.25524785  0.98218265 -0.16342737 -2.00722617]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 94 ] state=[-0.25524785  0.98218265 -0.16342737 -2.00722617], action=1, reward=1.0, next_state=[-0.2356042   1.17858693 -0.20357189 -2.34574114]\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 95 ] state=[-0.2356042   1.17858693 -0.20357189 -2.34574114], action=1, reward=-1.0, next_state=[-0.21203246  1.37487607 -0.25048671 -2.69353261]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Ended! ] Episode 131: Exploration_rate=0.01. Score=95.\n",
      "[ episode 132 ] state=[ 0.00565933 -0.03695251 -0.03243902 -0.02869907]\n",
      "[ episode 132 ][ timestamp 1 ] state=[ 0.00565933 -0.03695251 -0.03243902 -0.02869907], action=1, reward=1.0, next_state=[ 0.00492028  0.15861926 -0.03301301 -0.33143779]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 2 ] state=[ 0.00492028  0.15861926 -0.03301301 -0.33143779], action=0, reward=1.0, next_state=[ 0.00809267 -0.03601761 -0.03964176 -0.04934564]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 3 ] state=[ 0.00809267 -0.03601761 -0.03964176 -0.04934564], action=1, reward=1.0, next_state=[ 0.00737232  0.15964969 -0.04062867 -0.35426762]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 4 ] state=[ 0.00737232  0.15964969 -0.04062867 -0.35426762], action=1, reward=1.0, next_state=[ 0.01056531  0.35532508 -0.04771403 -0.65948003]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 5 ] state=[ 0.01056531  0.35532508 -0.04771403 -0.65948003], action=0, reward=1.0, next_state=[ 0.01767181  0.16089851 -0.06090363 -0.38219469]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 6 ] state=[ 0.01767181  0.16089851 -0.06090363 -0.38219469], action=0, reward=1.0, next_state=[ 0.02088978 -0.03330819 -0.06854752 -0.10931935]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 7 ] state=[ 0.02088978 -0.03330819 -0.06854752 -0.10931935], action=0, reward=1.0, next_state=[ 0.02022362 -0.22738432 -0.07073391  0.16097398]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 8 ] state=[ 0.02022362 -0.22738432 -0.07073391  0.16097398], action=0, reward=1.0, next_state=[ 0.01567593 -0.42142609 -0.06751443  0.43053038]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 9 ] state=[ 0.01567593 -0.42142609 -0.06751443  0.43053038], action=1, reward=1.0, next_state=[ 0.00724741 -0.2254163  -0.05890382  0.11735136]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 10 ] state=[ 0.00724741 -0.2254163  -0.05890382  0.11735136], action=1, reward=1.0, next_state=[ 0.00273908 -0.029502   -0.05655679 -0.19331812]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 11 ] state=[ 0.00273908 -0.029502   -0.05655679 -0.19331812], action=0, reward=1.0, next_state=[ 0.00214904 -0.22377122 -0.06042316  0.08100093]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 12 ] state=[ 0.00214904 -0.22377122 -0.06042316  0.08100093], action=0, reward=1.0, next_state=[-0.00232638 -0.41797729 -0.05880314  0.35402482]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 13 ] state=[-0.00232638 -0.41797729 -0.05880314  0.35402482], action=1, reward=1.0, next_state=[-0.01068593 -0.22207065 -0.05172264  0.04339462]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 14 ] state=[-0.01068593 -0.22207065 -0.05172264  0.04339462], action=0, reward=1.0, next_state=[-0.01512734 -0.41641423 -0.05085475  0.31932047]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 15 ] state=[-0.01512734 -0.41641423 -0.05085475  0.31932047], action=1, reward=1.0, next_state=[-0.02345563 -0.22060629 -0.04446834  0.01104341]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 16 ] state=[-0.02345563 -0.22060629 -0.04446834  0.01104341], action=0, reward=1.0, next_state=[-0.02786775 -0.41506323 -0.04424747  0.28937109]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 17 ] state=[-0.02786775 -0.41506323 -0.04424747  0.28937109], action=1, reward=1.0, next_state=[-0.03616902 -0.21933916 -0.03846005 -0.01693217]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 18 ] state=[-0.03616902 -0.21933916 -0.03846005 -0.01693217], action=0, reward=1.0, next_state=[-0.0405558  -0.41388905 -0.03879869  0.26337239]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 19 ] state=[-0.0405558  -0.41388905 -0.03879869  0.26337239], action=1, reward=1.0, next_state=[-0.04883358 -0.21823539 -0.03353124 -0.04129119]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 20 ] state=[-0.04883358 -0.21823539 -0.03353124 -0.04129119], action=1, reward=1.0, next_state=[-0.05319829 -0.02264905 -0.03435707 -0.34436212]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 21 ] state=[-0.05319829 -0.02264905 -0.03435707 -0.34436212], action=0, reward=1.0, next_state=[-0.05365127 -0.21726583 -0.04124431 -0.06270822]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 22 ] state=[-0.05365127 -0.21726583 -0.04124431 -0.06270822], action=0, reward=1.0, next_state=[-0.05799659 -0.41177291 -0.04249847  0.21668189]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 23 ] state=[-0.05799659 -0.41177291 -0.04249847  0.21668189], action=1, reward=1.0, next_state=[-0.06623204 -0.21607    -0.03816484 -0.08909821]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 24 ] state=[-0.06623204 -0.21607    -0.03816484 -0.08909821], action=1, reward=1.0, next_state=[-0.07055344 -0.02042238 -0.0399468  -0.39357368]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 25 ] state=[-0.07055344 -0.02042238 -0.0399468  -0.39357368], action=0, reward=1.0, next_state=[-0.07096189 -0.21495537 -0.04781827 -0.11374821]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 26 ] state=[-0.07096189 -0.21495537 -0.04781827 -0.11374821], action=0, reward=1.0, next_state=[-0.075261   -0.40936069 -0.05009324  0.16347322]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 27 ] state=[-0.075261   -0.40936069 -0.05009324  0.16347322], action=0, reward=1.0, next_state=[-0.08344821 -0.60373108 -0.04682377  0.43994182]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 28 ] state=[-0.08344821 -0.60373108 -0.04682377  0.43994182], action=1, reward=1.0, next_state=[-0.09552283 -0.4079788  -0.03802494  0.13287408]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 29 ] state=[-0.09552283 -0.4079788  -0.03802494  0.13287408], action=0, reward=1.0, next_state=[-0.10368241 -0.60253603 -0.03536746  0.41332232]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 30 ] state=[-0.10368241 -0.60253603 -0.03536746  0.41332232], action=0, reward=1.0, next_state=[-0.11573313 -0.79713927 -0.02710101  0.69464877]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 31 ] state=[-0.11573313 -0.79713927 -0.02710101  0.69464877], action=0, reward=1.0, next_state=[-0.13167592 -0.99187504 -0.01320803  0.97867844]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 32 ] state=[-0.13167592 -0.99187504 -0.01320803  0.97867844], action=1, reward=1.0, next_state=[-0.15151342 -0.79657852  0.00636553  0.68187616]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 33 ] state=[-0.15151342 -0.79657852  0.00636553  0.68187616], action=1, reward=1.0, next_state=[-0.16744499 -0.60154555  0.02000306  0.39120409]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 34 ] state=[-0.16744499 -0.60154555  0.02000306  0.39120409], action=0, reward=1.0, next_state=[-0.1794759  -0.79694559  0.02782714  0.69012602]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 35 ] state=[-0.1794759  -0.79694559  0.02782714  0.69012602], action=1, reward=1.0, next_state=[-0.19541481 -0.60222062  0.04162966  0.40633177]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 36 ] state=[-0.19541481 -0.60222062  0.04162966  0.40633177], action=1, reward=1.0, next_state=[-0.20745922 -0.40771295  0.04975629  0.12705863]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 37 ] state=[-0.20745922 -0.40771295  0.04975629  0.12705863], action=1, reward=1.0, next_state=[-0.21561348 -0.2133378   0.05229747 -0.14952094]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 38 ] state=[-0.21561348 -0.2133378   0.05229747 -0.14952094], action=1, reward=1.0, next_state=[-0.21988024 -0.01900222  0.04930705 -0.42525732]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 39 ] state=[-0.21988024 -0.01900222  0.04930705 -0.42525732], action=0, reward=1.0, next_state=[-0.22026028 -0.21478667  0.0408019  -0.11744718]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 40 ] state=[-0.22026028 -0.21478667  0.0408019  -0.11744718], action=1, reward=1.0, next_state=[-0.22455602 -0.02027236  0.03845296 -0.39698337]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 41 ] state=[-0.22455602 -0.02027236  0.03845296 -0.39698337], action=0, reward=1.0, next_state=[-0.22496146 -0.21591817  0.03051329 -0.09242921]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 42 ] state=[-0.22496146 -0.21591817  0.03051329 -0.09242921], action=0, reward=1.0, next_state=[-0.22927983 -0.41146388  0.02866471  0.20972234]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 43 ] state=[-0.22927983 -0.41146388  0.02866471  0.20972234], action=1, reward=1.0, next_state=[-0.2375091  -0.21676327  0.03285915 -0.07378233]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 44 ] state=[-0.2375091  -0.21676327  0.03285915 -0.07378233], action=0, reward=1.0, next_state=[-0.24184437 -0.41234051  0.03138351  0.22908402]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 45 ] state=[-0.24184437 -0.41234051  0.03138351  0.22908402], action=0, reward=1.0, next_state=[-0.25009118 -0.60789656  0.03596519  0.53149888]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 46 ] state=[-0.25009118 -0.60789656  0.03596519  0.53149888], action=0, reward=1.0, next_state=[-0.26224911 -0.80350544  0.04659517  0.83529395]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 132 ][ timestamp 47 ] state=[-0.26224911 -0.80350544  0.04659517  0.83529395], action=0, reward=1.0, next_state=[-0.27831922 -0.99923189  0.06330104  1.142259  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 48 ] state=[-0.27831922 -0.99923189  0.06330104  1.142259  ], action=1, reward=1.0, next_state=[-0.29830386 -0.8049917   0.08614622  0.87008033]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 49 ] state=[-0.29830386 -0.8049917   0.08614622  0.87008033], action=1, reward=1.0, next_state=[-0.31440369 -0.61114056  0.10354783  0.60567759]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 50 ] state=[-0.31440369 -0.61114056  0.10354783  0.60567759], action=1, reward=1.0, next_state=[-0.3266265  -0.41760735  0.11566138  0.34732139]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 51 ] state=[-0.3266265  -0.41760735  0.11566138  0.34732139], action=1, reward=1.0, next_state=[-0.33497865 -0.22430419  0.12260781  0.09323261]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 52 ] state=[-0.33497865 -0.22430419  0.12260781  0.09323261], action=0, reward=1.0, next_state=[-0.33946473 -0.42095069  0.12447246  0.42194449]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 53 ] state=[-0.33946473 -0.42095069  0.12447246  0.42194449], action=1, reward=1.0, next_state=[-0.34788375 -0.22779176  0.13291135  0.1709482 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 54 ] state=[-0.34788375 -0.22779176  0.13291135  0.1709482 ], action=1, reward=1.0, next_state=[-0.35243958 -0.0347978   0.13633032 -0.07702852]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 55 ] state=[-0.35243958 -0.0347978   0.13633032 -0.07702852], action=1, reward=1.0, next_state=[-0.35313554  0.15813315  0.13478975 -0.32378267]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 56 ] state=[-0.35313554  0.15813315  0.13478975 -0.32378267], action=1, reward=1.0, next_state=[-0.34997287  0.3511041   0.12831409 -0.57110533]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 57 ] state=[-0.34997287  0.3511041   0.12831409 -0.57110533], action=1, reward=1.0, next_state=[-0.34295079  0.54421532  0.11689199 -0.82076991]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 58 ] state=[-0.34295079  0.54421532  0.11689199 -0.82076991], action=1, reward=1.0, next_state=[-0.33206649  0.73756033  0.10047659 -1.07452027]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 59 ] state=[-0.33206649  0.73756033  0.10047659 -1.07452027], action=1, reward=1.0, next_state=[-0.31731528  0.93122139  0.07898618 -1.33405633]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 60 ] state=[-0.31731528  0.93122139  0.07898618 -1.33405633], action=0, reward=1.0, next_state=[-0.29869085  0.73519767  0.05230506 -1.01773968]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 61 ] state=[-0.29869085  0.73519767  0.05230506 -1.01773968], action=0, reward=1.0, next_state=[-0.2839869   0.53941894  0.03195026 -0.70910254]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 62 ] state=[-0.2839869   0.53941894  0.03195026 -0.70910254], action=1, reward=1.0, next_state=[-0.27319852  0.73408412  0.01776821 -0.99155951]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 63 ] state=[-0.27319852  0.73408412  0.01776821 -0.99155951], action=0, reward=1.0, next_state=[-0.25851684  0.53872896 -0.00206298 -0.69334944]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 64 ] state=[-0.25851684  0.53872896 -0.00206298 -0.69334944], action=0, reward=1.0, next_state=[-0.24774226  0.34363569 -0.01592997 -0.40131667]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 65 ] state=[-0.24774226  0.34363569 -0.01592997 -0.40131667], action=0, reward=1.0, next_state=[-0.24086954  0.14874327 -0.0239563  -0.11369835]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 66 ] state=[-0.24086954  0.14874327 -0.0239563  -0.11369835], action=0, reward=1.0, next_state=[-0.23789468 -0.04602737 -0.02623027  0.1713313 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 67 ] state=[-0.23789468 -0.04602737 -0.02623027  0.1713313 ], action=0, reward=1.0, next_state=[-0.23881523 -0.24076426 -0.02280364  0.45562534]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 68 ] state=[-0.23881523 -0.24076426 -0.02280364  0.45562534], action=0, reward=1.0, next_state=[-0.24363051 -0.4355565  -0.01369114  0.74103404]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 69 ] state=[-0.24363051 -0.4355565  -0.01369114  0.74103404], action=1, reward=1.0, next_state=[-0.25234164 -0.24024823  0.00112955  0.44407403]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 70 ] state=[-0.25234164 -0.24024823  0.00112955  0.44407403], action=1, reward=1.0, next_state=[-0.25714661 -0.04514228  0.01001103  0.15174738]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 71 ] state=[-0.25714661 -0.04514228  0.01001103  0.15174738], action=1, reward=1.0, next_state=[-0.25804945  0.1498349   0.01304597 -0.13776055]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 72 ] state=[-0.25804945  0.1498349   0.01304597 -0.13776055], action=1, reward=1.0, next_state=[-0.25505275  0.34476759  0.01029076 -0.42629929]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 73 ] state=[-0.25505275  0.34476759  0.01029076 -0.42629929], action=0, reward=1.0, next_state=[-0.2481574   0.1495014   0.00176478 -0.13039009]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 74 ] state=[-0.2481574   0.1495014   0.00176478 -0.13039009], action=0, reward=1.0, next_state=[-0.24516737 -0.04564578 -0.00084303  0.16284908]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 75 ] state=[-0.24516737 -0.04564578 -0.00084303  0.16284908], action=1, reward=1.0, next_state=[-0.24608029  0.14948823  0.00241396 -0.13009968]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 76 ] state=[-0.24608029  0.14948823  0.00241396 -0.13009968], action=1, reward=1.0, next_state=[-2.43090525e-01  3.44575515e-01 -1.88037475e-04 -4.22020059e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 77 ] state=[-2.43090525e-01  3.44575515e-01 -1.88037475e-04 -4.22020059e-01], action=0, reward=1.0, next_state=[-0.23619901  0.14945623 -0.00862844 -0.12939642]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 78 ] state=[-0.23619901  0.14945623 -0.00862844 -0.12939642], action=0, reward=1.0, next_state=[-0.23320989 -0.04554106 -0.01121637  0.1605519 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 79 ] state=[-0.23320989 -0.04554106 -0.01121637  0.1605519 ], action=0, reward=1.0, next_state=[-0.23412071 -0.24050066 -0.00800533  0.44967535]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 80 ] state=[-0.23412071 -0.24050066 -0.00800533  0.44967535], action=1, reward=1.0, next_state=[-0.23893072 -0.0452664   0.00098818  0.15447981]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 81 ] state=[-0.23893072 -0.0452664   0.00098818  0.15447981], action=1, reward=1.0, next_state=[-0.23983605  0.14984139  0.00407777 -0.13789121]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 82 ] state=[-0.23983605  0.14984139  0.00407777 -0.13789121], action=1, reward=1.0, next_state=[-0.23683922  0.3449047   0.00131995 -0.42928488]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 83 ] state=[-0.23683922  0.3449047   0.00131995 -0.42928488], action=0, reward=1.0, next_state=[-0.22994113  0.14976408 -0.00726575 -0.13618614]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 84 ] state=[-0.22994113  0.14976408 -0.00726575 -0.13618614], action=0, reward=1.0, next_state=[-0.22694585 -0.04525305 -0.00998947  0.15419573]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 85 ] state=[-0.22694585 -0.04525305 -0.00998947  0.15419573], action=1, reward=1.0, next_state=[-0.22785091  0.1500105  -0.00690556 -0.14162184]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 86 ] state=[-0.22785091  0.1500105  -0.00690556 -0.14162184], action=1, reward=1.0, next_state=[-0.2248507   0.34523067 -0.00973799 -0.43647532]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 87 ] state=[-0.2248507   0.34523067 -0.00973799 -0.43647532], action=0, reward=1.0, next_state=[-0.21794609  0.15024791 -0.0184675  -0.14687797]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 88 ] state=[-0.21794609  0.15024791 -0.0184675  -0.14687797], action=1, reward=1.0, next_state=[-0.21494113  0.34562938 -0.02140506 -0.44532935]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 89 ] state=[-0.21494113  0.34562938 -0.02140506 -0.44532935], action=0, reward=1.0, next_state=[-0.20802854  0.15081671 -0.03031165 -0.15946989]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 132 ][ timestamp 90 ] state=[-0.20802854  0.15081671 -0.03031165 -0.15946989], action=0, reward=1.0, next_state=[-0.20501221 -0.04385847 -0.03350104  0.12349847]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 91 ] state=[-0.20501221 -0.04385847 -0.03350104  0.12349847], action=1, reward=1.0, next_state=[-0.20588938  0.15172703 -0.03103107 -0.17956261]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 92 ] state=[-0.20588938  0.15172703 -0.03103107 -0.17956261], action=0, reward=1.0, next_state=[-0.20285484 -0.04293744 -0.03462233  0.10317185]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 93 ] state=[-0.20285484 -0.04293744 -0.03462233  0.10317185], action=0, reward=1.0, next_state=[-0.20371358 -0.23754655 -0.03255889  0.38473364]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 94 ] state=[-0.20371358 -0.23754655 -0.03255889  0.38473364], action=1, reward=1.0, next_state=[-0.20846452 -0.04197786 -0.02486422  0.08196545]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 95 ] state=[-0.20846452 -0.04197786 -0.02486422  0.08196545], action=1, reward=1.0, next_state=[-0.20930407  0.15349153 -0.02322491 -0.21845734]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 96 ] state=[-0.20930407  0.15349153 -0.02322491 -0.21845734], action=0, reward=1.0, next_state=[-0.20623424 -0.04129085 -0.02759405  0.06680994]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 97 ] state=[-0.20623424 -0.04129085 -0.02759405  0.06680994], action=1, reward=1.0, next_state=[-0.20706006  0.15421563 -0.02625786 -0.23444976]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 98 ] state=[-0.20706006  0.15421563 -0.02625786 -0.23444976], action=1, reward=1.0, next_state=[-0.20397575  0.34970271 -0.03094685 -0.53529823]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 99 ] state=[-0.20397575  0.34970271 -0.03094685 -0.53529823], action=1, reward=1.0, next_state=[-0.19698169  0.54524586 -0.04165282 -0.83756942]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 100 ] state=[-0.19698169  0.54524586 -0.04165282 -0.83756942], action=0, reward=1.0, next_state=[-0.18607678  0.35071676 -0.0584042  -0.55827125]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 101 ] state=[-0.18607678  0.35071676 -0.0584042  -0.55827125], action=0, reward=1.0, next_state=[-0.17906244  0.15646119 -0.06956963 -0.28454579]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 102 ] state=[-0.17906244  0.15646119 -0.06956963 -0.28454579], action=0, reward=1.0, next_state=[-0.17593322 -0.03760313 -0.07526054 -0.01459046]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 103 ] state=[-0.17593322 -0.03760313 -0.07526054 -0.01459046], action=0, reward=1.0, next_state=[-0.17668528 -0.23156958 -0.07555235  0.2534299 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 104 ] state=[-0.17668528 -0.23156958 -0.07555235  0.2534299 ], action=1, reward=1.0, next_state=[-0.18131667 -0.0354547  -0.07048376 -0.06209449]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 105 ] state=[-0.18131667 -0.0354547  -0.07048376 -0.06209449], action=1, reward=1.0, next_state=[-0.18202576  0.16060333 -0.07172565 -0.3761564 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 106 ] state=[-0.18202576  0.16060333 -0.07172565 -0.3761564 ], action=0, reward=1.0, next_state=[-0.1788137  -0.03343047 -0.07924877 -0.1069233 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 107 ] state=[-0.1788137  -0.03343047 -0.07924877 -0.1069233 ], action=0, reward=1.0, next_state=[-0.17948231 -0.2273326  -0.08138724  0.15974227]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 108 ] state=[-0.17948231 -0.2273326  -0.08138724  0.15974227], action=1, reward=1.0, next_state=[-0.18402896 -0.03114548 -0.07819239 -0.15746574]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 109 ] state=[-0.18402896 -0.03114548 -0.07819239 -0.15746574], action=0, reward=1.0, next_state=[-0.18465187 -0.22506592 -0.08134171  0.10956099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 110 ] state=[-0.18465187 -0.22506592 -0.08134171  0.10956099], action=1, reward=1.0, next_state=[-0.18915319 -0.02887827 -0.07915049 -0.20763557]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 111 ] state=[-0.18915319 -0.02887827 -0.07915049 -0.20763557], action=0, reward=1.0, next_state=[-0.18973075 -0.22278445 -0.0833032   0.05906713]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 112 ] state=[-0.18973075 -0.22278445 -0.0833032   0.05906713], action=0, reward=1.0, next_state=[-0.19418644 -0.41661932 -0.08212186  0.32434837]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 113 ] state=[-0.19418644 -0.41661932 -0.08212186  0.32434837], action=0, reward=1.0, next_state=[-0.20251883 -0.61048178 -0.07563489  0.59004536]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 114 ] state=[-0.20251883 -0.61048178 -0.07563489  0.59004536], action=1, reward=1.0, next_state=[-0.21472846 -0.41438684 -0.06383398  0.27452842]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 115 ] state=[-0.21472846 -0.41438684 -0.06383398  0.27452842], action=0, reward=1.0, next_state=[-0.2230162  -0.60854268 -0.05834342  0.54641458]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 116 ] state=[-0.2230162  -0.60854268 -0.05834342  0.54641458], action=1, reward=1.0, next_state=[-0.23518705 -0.41265162 -0.04741512  0.23593471]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 117 ] state=[-0.23518705 -0.41265162 -0.04741512  0.23593471], action=1, reward=1.0, next_state=[-0.24344009 -0.21688543 -0.04269643 -0.07131935]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 118 ] state=[-0.24344009 -0.21688543 -0.04269643 -0.07131935], action=0, reward=1.0, next_state=[-0.2477778  -0.41137009 -0.04412282  0.20759282]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 119 ] state=[-0.2477778  -0.41137009 -0.04412282  0.20759282], action=0, reward=1.0, next_state=[-0.2560052  -0.60583424 -0.03997096  0.48603726]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 120 ] state=[-0.2560052  -0.60583424 -0.03997096  0.48603726], action=1, reward=1.0, next_state=[-0.26812188 -0.41017173 -0.03025021  0.18102958]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 121 ] state=[-0.26812188 -0.41017173 -0.03025021  0.18102958], action=0, reward=1.0, next_state=[-0.27632532 -0.60484805 -0.02662962  0.46401826]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 122 ] state=[-0.27632532 -0.60484805 -0.02662962  0.46401826], action=1, reward=1.0, next_state=[-0.28842228 -0.40936011 -0.01734926  0.16306212]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 123 ] state=[-0.28842228 -0.40936011 -0.01734926  0.16306212], action=1, reward=1.0, next_state=[-0.29660948 -0.21399414 -0.01408802 -0.13504314]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 124 ] state=[-0.29660948 -0.21399414 -0.01408802 -0.13504314], action=1, reward=1.0, next_state=[-0.30088936 -0.01867326 -0.01678888 -0.43213713]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 125 ] state=[-0.30088936 -0.01867326 -0.01678888 -0.43213713], action=0, reward=1.0, next_state=[-0.30126283 -0.21355352 -0.02543162 -0.14479364]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 126 ] state=[-0.30126283 -0.21355352 -0.02543162 -0.14479364], action=0, reward=1.0, next_state=[-0.3055339  -0.40830221 -0.02832749  0.13975884]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 127 ] state=[-0.3055339  -0.40830221 -0.02832749  0.13975884], action=0, reward=1.0, next_state=[-0.31369994 -0.60300723 -0.02553232  0.42337203]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 128 ] state=[-0.31369994 -0.60300723 -0.02553232  0.42337203], action=1, reward=1.0, next_state=[-0.32576009 -0.40753306 -0.01706488  0.12275065]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 129 ] state=[-0.32576009 -0.40753306 -0.01706488  0.12275065], action=1, reward=1.0, next_state=[-0.33391075 -0.21217083 -0.01460986 -0.17526685]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 130 ] state=[-0.33391075 -0.21217083 -0.01460986 -0.17526685], action=1, reward=1.0, next_state=[-0.33815416 -0.01684287 -0.0181152  -0.47252267]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 131 ] state=[-0.33815416 -0.01684287 -0.0181152  -0.47252267], action=0, reward=1.0, next_state=[-0.33849102 -0.21170435 -0.02756565 -0.18560399]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 132 ] state=[-0.33849102 -0.21170435 -0.02756565 -0.18560399], action=0, reward=1.0, next_state=[-0.34272511 -0.40642127 -0.03127773  0.09825716]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 133 ] state=[-0.34272511 -0.40642127 -0.03127773  0.09825716], action=0, reward=1.0, next_state=[-0.35085353 -0.60108131 -0.02931259  0.38091025]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 134 ] state=[-0.35085353 -0.60108131 -0.02931259  0.38091025], action=0, reward=1.0, next_state=[-0.36287516 -0.79577503 -0.02169439  0.6642087 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 135 ] state=[-0.36287516 -0.79577503 -0.02169439  0.6642087 ], action=1, reward=1.0, next_state=[-0.37879066 -0.60035811 -0.00841021  0.36477465]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 136 ] state=[-0.37879066 -0.60035811 -0.00841021  0.36477465], action=1, reward=1.0, next_state=[-0.39079782 -0.40511765 -0.00111472  0.06945175]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 132 ][ timestamp 137 ] state=[-0.39079782 -0.40511765 -0.00111472  0.06945175], action=1, reward=1.0, next_state=[-3.98900176e-01 -2.09979737e-01  2.74316956e-04 -2.23582668e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 138 ] state=[-3.98900176e-01 -2.09979737e-01  2.74316956e-04 -2.23582668e-01], action=0, reward=1.0, next_state=[-0.40309977 -0.40510561 -0.00419734  0.06918678]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 139 ] state=[-0.40309977 -0.40510561 -0.00419734  0.06918678], action=0, reward=1.0, next_state=[-0.41120188 -0.60016713 -0.0028136   0.36054247]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 140 ] state=[-0.41120188 -0.60016713 -0.0028136   0.36054247], action=0, reward=1.0, next_state=[-0.42320523 -0.79524898  0.00439725  0.65233688]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 141 ] state=[-0.42320523 -0.79524898  0.00439725  0.65233688], action=1, reward=1.0, next_state=[-0.43911021 -0.60018854  0.01744399  0.36104184]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 142 ] state=[-0.43911021 -0.60018854  0.01744399  0.36104184], action=0, reward=1.0, next_state=[-0.45111398 -0.79555404  0.02466482  0.65917378]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 143 ] state=[-0.45111398 -0.79555404  0.02466482  0.65917378], action=0, reward=1.0, next_state=[-0.46702506 -0.99101043  0.0378483   0.95951991]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 144 ] state=[-0.46702506 -0.99101043  0.0378483   0.95951991], action=0, reward=1.0, next_state=[-0.48684527 -1.18662018  0.0570387   1.26384915]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 145 ] state=[-0.48684527 -1.18662018  0.0570387   1.26384915], action=0, reward=1.0, next_state=[-0.51057767 -1.38242298  0.08231568  1.57383599]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 146 ] state=[-0.51057767 -1.38242298  0.08231568  1.57383599], action=1, reward=1.0, next_state=[-0.53822613 -1.18837352  0.1137924   1.30792088]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 147 ] state=[-0.53822613 -1.18837352  0.1137924   1.30792088], action=1, reward=1.0, next_state=[-0.5619936  -0.99486252  0.13995082  1.05291445]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 148 ] state=[-0.5619936  -0.99486252  0.13995082  1.05291445], action=0, reward=1.0, next_state=[-0.58189085 -1.19153457  0.16100911  1.38604955]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 149 ] state=[-0.58189085 -1.19153457  0.16100911  1.38604955], action=1, reward=1.0, next_state=[-0.60572154 -0.9987441   0.1887301   1.14773658]\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 150 ] state=[-0.60572154 -0.9987441   0.1887301   1.14773658], action=1, reward=-1.0, next_state=[-0.62569642 -0.80651896  0.21168483  0.91967664]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 132: Exploration_rate=0.01. Score=150.\n",
      "[ episode 133 ] state=[-0.03961441  0.04094627  0.00965788  0.01862216]\n",
      "[ episode 133 ][ timestamp 1 ] state=[-0.03961441  0.04094627  0.00965788  0.01862216], action=1, reward=1.0, next_state=[-0.03879549  0.2359284   0.01003033 -0.27099801]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 2 ] state=[-0.03879549  0.2359284   0.01003033 -0.27099801], action=0, reward=1.0, next_state=[-0.03407692  0.04066476  0.00461037  0.02483157]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 3 ] state=[-0.03407692  0.04066476  0.00461037  0.02483157], action=0, reward=1.0, next_state=[-0.03326362 -0.154523    0.005107    0.31896555]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 4 ] state=[-0.03326362 -0.154523    0.005107    0.31896555], action=1, reward=1.0, next_state=[-0.03635408  0.04052584  0.01148631  0.02789755]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 5 ] state=[-0.03635408  0.04052584  0.01148631  0.02789755], action=0, reward=1.0, next_state=[-0.03554357 -0.15475893  0.01204426  0.32418229]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 6 ] state=[-0.03554357 -0.15475893  0.01204426  0.32418229], action=1, reward=1.0, next_state=[-0.03863875  0.04018947  0.01852791  0.03532182]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 7 ] state=[-0.03863875  0.04018947  0.01852791  0.03532182], action=1, reward=1.0, next_state=[-0.03783496  0.2350409   0.01923434 -0.25145826]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 8 ] state=[-0.03783496  0.2350409   0.01923434 -0.25145826], action=1, reward=1.0, next_state=[-0.03313414  0.42988298  0.01420518 -0.53801277]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 9 ] state=[-0.03313414  0.42988298  0.01420518 -0.53801277], action=0, reward=1.0, next_state=[-0.02453648  0.23456423  0.00344492 -0.24088802]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 10 ] state=[-0.02453648  0.23456423  0.00344492 -0.24088802], action=1, reward=1.0, next_state=[-0.01984519  0.4296368  -0.00137284 -0.53248234]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 11 ] state=[-0.01984519  0.4296368  -0.00137284 -0.53248234], action=0, reward=1.0, next_state=[-0.01125246  0.23453418 -0.01202249 -0.24023231]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 12 ] state=[-0.01125246  0.23453418 -0.01202249 -0.24023231], action=1, reward=1.0, next_state=[-0.00656178  0.4298258  -0.01682713 -0.53668308]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 13 ] state=[-0.00656178  0.4298258  -0.01682713 -0.53668308], action=0, reward=1.0, next_state=[ 0.00203474  0.23494443 -0.02756079 -0.24934936]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 14 ] state=[ 0.00203474  0.23494443 -0.02756079 -0.24934936], action=0, reward=1.0, next_state=[ 0.00673363  0.04022669 -0.03254778  0.03451448]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 15 ] state=[ 0.00673363  0.04022669 -0.03254778  0.03451448], action=1, reward=1.0, next_state=[ 0.00753816  0.23579991 -0.03185749 -0.26825733]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 16 ] state=[ 0.00753816  0.23579991 -0.03185749 -0.26825733], action=1, reward=1.0, next_state=[ 0.01225416  0.43136168 -0.03722264 -0.57081566]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 17 ] state=[ 0.01225416  0.43136168 -0.03722264 -0.57081566], action=1, reward=1.0, next_state=[ 0.02088139  0.62698531 -0.04863895 -0.87498878]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 18 ] state=[ 0.02088139  0.62698531 -0.04863895 -0.87498878], action=0, reward=1.0, next_state=[ 0.0334211   0.43255713 -0.06613873 -0.59798563]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 19 ] state=[ 0.0334211   0.43255713 -0.06613873 -0.59798563], action=0, reward=1.0, next_state=[ 0.04207224  0.23841993 -0.07809844 -0.32684712]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 20 ] state=[ 0.04207224  0.23841993 -0.07809844 -0.32684712], action=0, reward=1.0, next_state=[ 0.04684064  0.04449172 -0.08463538 -0.05977909]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 21 ] state=[ 0.04684064  0.04449172 -0.08463538 -0.05977909], action=1, reward=1.0, next_state=[ 0.04773048  0.24071878 -0.08583096 -0.37791921]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 22 ] state=[ 0.04773048  0.24071878 -0.08583096 -0.37791921], action=0, reward=1.0, next_state=[ 0.05254485  0.046914   -0.09338935 -0.11348553]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 23 ] state=[ 0.05254485  0.046914   -0.09338935 -0.11348553], action=1, reward=1.0, next_state=[ 0.05348313  0.24324142 -0.09565906 -0.43410996]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 24 ] state=[ 0.05348313  0.24324142 -0.09565906 -0.43410996], action=1, reward=1.0, next_state=[ 0.05834796  0.43957829 -0.10434126 -0.75534972]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 25 ] state=[ 0.05834796  0.43957829 -0.10434126 -0.75534972], action=1, reward=1.0, next_state=[ 0.06713953  0.63597195 -0.11944825 -1.07895875]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 26 ] state=[ 0.06713953  0.63597195 -0.11944825 -1.07895875], action=0, reward=1.0, next_state=[ 0.07985897  0.44261238 -0.14102743 -0.82602039]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 133 ][ timestamp 27 ] state=[ 0.07985897  0.44261238 -0.14102743 -0.82602039], action=0, reward=1.0, next_state=[ 0.08871121  0.24967161 -0.15754783 -0.58080726]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 28 ] state=[ 0.08871121  0.24967161 -0.15754783 -0.58080726], action=0, reward=1.0, next_state=[ 0.09370464  0.05706721 -0.16916398 -0.34160645]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 29 ] state=[ 0.09370464  0.05706721 -0.16916398 -0.34160645], action=0, reward=1.0, next_state=[ 0.09484599 -0.13529491 -0.17599611 -0.10667932]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 30 ] state=[ 0.09484599 -0.13529491 -0.17599611 -0.10667932], action=0, reward=1.0, next_state=[ 0.09214009 -0.32751539 -0.1781297   0.12572129]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 31 ] state=[ 0.09214009 -0.32751539 -0.1781297   0.12572129], action=0, reward=1.0, next_state=[ 0.08558978 -0.51969763 -0.17561527  0.35733964]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 32 ] state=[ 0.08558978 -0.51969763 -0.17561527  0.35733964], action=1, reward=1.0, next_state=[ 0.07519583 -0.32257062 -0.16846848  0.01483116]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 33 ] state=[ 0.07519583 -0.32257062 -0.16846848  0.01483116], action=0, reward=1.0, next_state=[ 0.06874442 -0.51492628 -0.16817185  0.24998403]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 34 ] state=[ 0.06874442 -0.51492628 -0.16817185  0.24998403], action=0, reward=1.0, next_state=[ 0.05844589 -0.70729747 -0.16317217  0.48526017]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 35 ] state=[ 0.05844589 -0.70729747 -0.16317217  0.48526017], action=0, reward=1.0, next_state=[ 0.04429994 -0.89978624 -0.15346697  0.72239804]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 36 ] state=[ 0.04429994 -0.89978624 -0.15346697  0.72239804], action=0, reward=1.0, next_state=[ 0.02630422 -1.09248998 -0.13901901  0.96311401]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 37 ] state=[ 0.02630422 -1.09248998 -0.13901901  0.96311401], action=1, reward=1.0, next_state=[ 0.00445442 -0.89580164 -0.11975673  0.63018778]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 38 ] state=[ 0.00445442 -0.89580164 -0.11975673  0.63018778], action=0, reward=1.0, next_state=[-0.01346161 -1.08906693 -0.10715297  0.882885  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 39 ] state=[-0.01346161 -1.08906693 -0.10715297  0.882885  ], action=0, reward=1.0, next_state=[-0.03524295 -1.28258326 -0.08949527  1.14005194]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 40 ] state=[-0.03524295 -1.28258326 -0.08949527  1.14005194], action=1, reward=1.0, next_state=[-0.06089462 -1.08641264 -0.06669423  0.82069714]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 41 ] state=[-0.06089462 -1.08641264 -0.06669423  0.82069714], action=1, reward=1.0, next_state=[-0.08262287 -0.89044452 -0.05028029  0.5078049 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 42 ] state=[-0.08262287 -0.89044452 -0.05028029  0.5078049 ], action=0, reward=1.0, next_state=[-0.10043176 -1.08482331 -0.04012419  0.78422843]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 43 ] state=[-0.10043176 -1.08482331 -0.04012419  0.78422843], action=1, reward=1.0, next_state=[-0.12212823 -0.88917364 -0.02443962  0.47919678]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 44 ] state=[-0.12212823 -0.88917364 -0.02443962  0.47919678], action=0, reward=1.0, next_state=[-0.1399117  -1.08394219 -0.01485569  0.76407783]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 45 ] state=[-0.1399117  -1.08394219 -0.01485569  0.76407783], action=1, reward=1.0, next_state=[-1.61590544e-01 -8.88618834e-01  4.25867809e-04  4.66757715e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 46 ] state=[-1.61590544e-01 -8.88618834e-01  4.25867809e-04  4.66757715e-01], action=0, reward=1.0, next_state=[-0.17936292 -1.0837468   0.00976102  0.75957484]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 47 ] state=[-0.17936292 -1.0837468   0.00976102  0.75957484], action=1, reward=1.0, next_state=[-0.20103786 -0.88876069  0.02495252  0.46997931]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 48 ] state=[-0.20103786 -0.88876069  0.02495252  0.46997931], action=1, reward=1.0, next_state=[-0.21881307 -0.69399994  0.03435211  0.18526441]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 49 ] state=[-0.21881307 -0.69399994  0.03435211  0.18526441], action=1, reward=1.0, next_state=[-0.23269307 -0.49938591  0.03805739 -0.09638688]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 50 ] state=[-0.23269307 -0.49938591  0.03805739 -0.09638688], action=0, reward=1.0, next_state=[-0.24268079 -0.69503207  0.03612966  0.20805602]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 51 ] state=[-0.24268079 -0.69503207  0.03612966  0.20805602], action=1, reward=1.0, next_state=[-0.25658143 -0.50044487  0.04029078 -0.07301448]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 52 ] state=[-0.25658143 -0.50044487  0.04029078 -0.07301448], action=1, reward=1.0, next_state=[-0.26659033 -0.305923    0.03883049 -0.3527182 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 53 ] state=[-0.26659033 -0.305923    0.03883049 -0.3527182 ], action=1, reward=1.0, next_state=[-0.27270879 -0.11137412  0.03177612 -0.63290825]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 54 ] state=[-0.27270879 -0.11137412  0.03177612 -0.63290825], action=0, reward=1.0, next_state=[-0.27493627 -0.30692462  0.01911796 -0.33038996]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 55 ] state=[-0.27493627 -0.30692462  0.01911796 -0.33038996], action=0, reward=1.0, next_state=[-0.28107476 -0.50231342  0.01251016 -0.03173998]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 56 ] state=[-0.28107476 -0.50231342  0.01251016 -0.03173998], action=0, reward=1.0, next_state=[-0.29112103 -0.69761252  0.01187536  0.26486363]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 57 ] state=[-0.29112103 -0.69761252  0.01187536  0.26486363], action=0, reward=1.0, next_state=[-0.30507328 -0.89290194  0.01717263  0.56126838]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 58 ] state=[-0.30507328 -0.89290194  0.01717263  0.56126838], action=1, reward=1.0, next_state=[-0.32293132 -0.69802515  0.028398    0.2740448 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 59 ] state=[-0.32293132 -0.69802515  0.028398    0.2740448 ], action=1, reward=1.0, next_state=[-0.33689182 -0.50331965  0.03387889 -0.0095478 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 60 ] state=[-0.33689182 -0.50331965  0.03387889 -0.0095478 ], action=1, reward=1.0, next_state=[-0.34695821 -0.30869954  0.03368794 -0.29135196]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 61 ] state=[-0.34695821 -0.30869954  0.03368794 -0.29135196], action=0, reward=1.0, next_state=[-0.35313221 -0.50428523  0.0278609   0.0117625 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 62 ] state=[-0.35313221 -0.50428523  0.0278609   0.0117625 ], action=0, reward=1.0, next_state=[-0.36321791 -0.69979544  0.02809615  0.31310404]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 63 ] state=[-0.36321791 -0.69979544  0.02809615  0.31310404], action=1, reward=1.0, next_state=[-0.37721382 -0.50508477  0.03435823  0.02941249]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 64 ] state=[-0.37721382 -0.50508477  0.03435823  0.02941249], action=0, reward=1.0, next_state=[-0.38731551 -0.70068217  0.03494648  0.33273476]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 65 ] state=[-0.38731551 -0.70068217  0.03494648  0.33273476], action=1, reward=1.0, next_state=[-0.40132916 -0.5060746   0.04160118  0.05127381]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 66 ] state=[-0.40132916 -0.5060746   0.04160118  0.05127381], action=1, reward=1.0, next_state=[-0.41145065 -0.31157309  0.04262665 -0.2279988 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 67 ] state=[-0.41145065 -0.31157309  0.04262665 -0.2279988 ], action=1, reward=1.0, next_state=[-0.41768211 -0.11708541  0.03806668 -0.50693688]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 68 ] state=[-0.41768211 -0.11708541  0.03806668 -0.50693688], action=0, reward=1.0, next_state=[-0.42002382 -0.31272249  0.02792794 -0.20250496]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 69 ] state=[-0.42002382 -0.31272249  0.02792794 -0.20250496], action=0, reward=1.0, next_state=[-0.42627827 -0.50823249  0.02387784  0.09885543]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 133 ][ timestamp 70 ] state=[-0.42627827 -0.50823249  0.02387784  0.09885543], action=0, reward=1.0, next_state=[-0.43644292 -0.70368837  0.02585495  0.39897509]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 71 ] state=[-0.43644292 -0.70368837  0.02585495  0.39897509], action=0, reward=1.0, next_state=[-0.45051669 -0.89916738  0.03383445  0.69969612]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 72 ] state=[-0.45051669 -0.89916738  0.03383445  0.69969612], action=0, reward=1.0, next_state=[-0.46850003 -1.09474167  0.04782837  1.00283507]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 73 ] state=[-0.46850003 -1.09474167  0.04782837  1.00283507], action=0, reward=1.0, next_state=[-0.49039487 -1.29046885  0.06788507  1.3101463 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 74 ] state=[-0.49039487 -1.29046885  0.06788507  1.3101463 ], action=0, reward=1.0, next_state=[-0.51620424 -1.48638183  0.094088    1.62328178]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 75 ] state=[-0.51620424 -1.48638183  0.094088    1.62328178], action=1, reward=1.0, next_state=[-0.54593188 -1.292485    0.12655363  1.36134401]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 76 ] state=[-0.54593188 -1.292485    0.12655363  1.36134401], action=0, reward=1.0, next_state=[-0.57178158 -1.48894539  0.15378051  1.69078542]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 77 ] state=[-0.57178158 -1.48894539  0.15378051  1.69078542], action=1, reward=1.0, next_state=[-0.60156049 -1.29589848  0.18759622  1.44966574]\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 78 ] state=[-0.60156049 -1.29589848  0.18759622  1.44966574], action=0, reward=-1.0, next_state=[-0.62747846 -1.49276476  0.21658954  1.79461461]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 133: Exploration_rate=0.01. Score=78.\n",
      "[ episode 134 ] state=[-0.02015259  0.04698064 -0.0248349   0.03637913]\n",
      "[ episode 134 ][ timestamp 1 ] state=[-0.02015259  0.04698064 -0.0248349   0.03637913], action=1, reward=1.0, next_state=[-0.01921298  0.24244976 -0.02410731 -0.26403485]\n",
      "[ Experience replay ] starts\n",
      "[ episode 134 ][ timestamp 2 ] state=[-0.01921298  0.24244976 -0.02410731 -0.26403485], action=0, reward=1.0, next_state=[-0.01436399  0.04768005 -0.02938801  0.02094796]\n",
      "[ Experience replay ] starts\n",
      "[ episode 134 ][ timestamp 3 ] state=[-0.01436399  0.04768005 -0.02938801  0.02094796], action=1, reward=1.0, next_state=[-0.01341039  0.24321086 -0.02896905 -0.28086044]\n",
      "[ Experience replay ] starts\n",
      "[ episode 134 ][ timestamp 4 ] state=[-0.01341039  0.24321086 -0.02896905 -0.28086044], action=0, reward=1.0, next_state=[-0.00854617  0.04851386 -0.03458626  0.00254682]\n",
      "[ Experience replay ] starts\n",
      "[ episode 134 ][ timestamp 5 ] state=[-0.00854617  0.04851386 -0.03458626  0.00254682], action=1, reward=1.0, next_state=[-0.00757589  0.24411432 -0.03453532 -0.30084474]\n",
      "[ Experience replay ] starts\n",
      "[ episode 134 ][ timestamp 6 ] state=[-0.00757589  0.24411432 -0.03453532 -0.30084474], action=0, reward=1.0, next_state=[-0.0026936   0.04950119 -0.04055222 -0.01925048]\n",
      "[ Experience replay ] starts\n",
      "[ episode 134 ][ timestamp 7 ] state=[-0.0026936   0.04950119 -0.04055222 -0.01925048], action=1, reward=1.0, next_state=[-0.00170358  0.24518054 -0.04093723 -0.32444727]\n",
      "[ Experience replay ] starts\n",
      "[ episode 134 ][ timestamp 8 ] state=[-0.00170358  0.24518054 -0.04093723 -0.32444727], action=1, reward=1.0, next_state=[ 0.00320003  0.44086075 -0.04742617 -0.62975385]\n",
      "[ Experience replay ] starts\n",
      "[ episode 134 ][ timestamp 9 ] state=[ 0.00320003  0.44086075 -0.04742617 -0.62975385], action=1, reward=1.0, next_state=[ 0.01201724  0.6366113  -0.06002125 -0.9369876 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 134 ][ timestamp 10 ] state=[ 0.01201724  0.6366113  -0.06002125 -0.9369876 ], action=1, reward=1.0, next_state=[ 0.02474947  0.83248903 -0.078761   -1.24791076]\n",
      "[ Experience replay ] starts\n",
      "[ episode 134 ][ timestamp 11 ] state=[ 0.02474947  0.83248903 -0.078761   -1.24791076], action=1, reward=1.0, next_state=[ 0.04139925  1.02852753 -0.10371922 -1.56418872]\n",
      "[ Experience replay ] starts\n",
      "[ episode 134 ][ timestamp 12 ] state=[ 0.04139925  1.02852753 -0.10371922 -1.56418872], action=0, reward=1.0, next_state=[ 0.0619698   0.83478724 -0.13500299 -1.30557884]\n",
      "[ Experience replay ] starts\n",
      "[ episode 134 ][ timestamp 13 ] state=[ 0.0619698   0.83478724 -0.13500299 -1.30557884], action=0, reward=1.0, next_state=[ 0.07866555  0.64161022 -0.16111457 -1.05802033]\n",
      "[ Experience replay ] starts\n",
      "[ episode 134 ][ timestamp 14 ] state=[ 0.07866555  0.64161022 -0.16111457 -1.05802033], action=0, reward=1.0, next_state=[ 0.09149775  0.4489467  -0.18227498 -0.81993082]\n",
      "[ Experience replay ] starts\n",
      "[ episode 134 ][ timestamp 15 ] state=[ 0.09149775  0.4489467  -0.18227498 -0.81993082], action=0, reward=1.0, next_state=[ 0.10047668  0.25672458 -0.19867359 -0.5896668 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 134 ][ timestamp 16 ] state=[ 0.10047668  0.25672458 -0.19867359 -0.5896668 ], action=0, reward=-1.0, next_state=[ 0.10561118  0.06485744 -0.21046693 -0.3655539 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 134: Exploration_rate=0.01. Score=16.\n",
      "[ episode 135 ] state=[ 0.04586912 -0.02857769 -0.03946805 -0.03769281]\n",
      "[ episode 135 ][ timestamp 1 ] state=[ 0.04586912 -0.02857769 -0.03946805 -0.03769281], action=1, reward=1.0, next_state=[ 0.04529757  0.16708736 -0.0402219  -0.34256241]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 2 ] state=[ 0.04529757  0.16708736 -0.0402219  -0.34256241], action=1, reward=1.0, next_state=[ 0.04863932  0.36275778 -0.04707315 -0.6476527 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 3 ] state=[ 0.04863932  0.36275778 -0.04707315 -0.6476527 ], action=0, reward=1.0, next_state=[ 0.05589447  0.16832217 -0.06002621 -0.37015677]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 4 ] state=[ 0.05589447  0.16832217 -0.06002621 -0.37015677], action=0, reward=1.0, next_state=[ 0.05926091 -0.02589786 -0.06742934 -0.09698853]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 5 ] state=[ 0.05926091 -0.02589786 -0.06742934 -0.09698853], action=1, reward=1.0, next_state=[ 0.05874296  0.17012247 -0.06936911 -0.41016004]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 6 ] state=[ 0.05874296  0.17012247 -0.06936911 -0.41016004], action=0, reward=1.0, next_state=[ 0.06214541 -0.02395097 -0.07757231 -0.14012819]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 7 ] state=[ 0.06214541 -0.02395097 -0.07757231 -0.14012819], action=0, reward=1.0, next_state=[ 0.06166639 -0.21788115 -0.08037488  0.12710889]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 8 ] state=[ 0.06166639 -0.21788115 -0.08037488  0.12710889], action=1, reward=1.0, next_state=[ 0.05730876 -0.02170526 -0.0778327  -0.18980975]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 9 ] state=[ 0.05730876 -0.02170526 -0.0778327  -0.18980975], action=1, reward=1.0, next_state=[ 0.05687466  0.17443893 -0.08162889 -0.50599501]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 10 ] state=[ 0.05687466  0.17443893 -0.08162889 -0.50599501], action=1, reward=1.0, next_state=[ 0.06036344  0.37061056 -0.09174879 -0.82324491]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 11 ] state=[ 0.06036344  0.37061056 -0.09174879 -0.82324491], action=0, reward=1.0, next_state=[ 0.06777565  0.17685548 -0.10821369 -0.56077099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 12 ] state=[ 0.06777565  0.17685548 -0.10821369 -0.56077099], action=0, reward=1.0, next_state=[ 0.07131276 -0.01659464 -0.11942911 -0.30404593]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 13 ] state=[ 0.07131276 -0.01659464 -0.11942911 -0.30404593], action=0, reward=1.0, next_state=[ 0.07098087 -0.20983008 -0.12551003 -0.0512862 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 14 ] state=[ 0.07098087 -0.20983008 -0.12551003 -0.0512862 ], action=0, reward=1.0, next_state=[ 0.06678426 -0.40294976 -0.12653575  0.19931155]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 15 ] state=[ 0.06678426 -0.40294976 -0.12653575  0.19931155], action=0, reward=1.0, next_state=[ 0.05872527 -0.59605618 -0.12254952  0.44955303]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 135 ][ timestamp 16 ] state=[ 0.05872527 -0.59605618 -0.12254952  0.44955303], action=0, reward=1.0, next_state=[ 0.04680415 -0.78925099 -0.11355846  0.70123243]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 17 ] state=[ 0.04680415 -0.78925099 -0.11355846  0.70123243], action=0, reward=1.0, next_state=[ 0.03101913 -0.98263099 -0.09953381  0.95611965]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 18 ] state=[ 0.03101913 -0.98263099 -0.09953381  0.95611965], action=0, reward=1.0, next_state=[ 0.01136651 -1.1762836  -0.08041142  1.21594622]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 19 ] state=[ 0.01136651 -1.1762836  -0.08041142  1.21594622], action=1, reward=1.0, next_state=[-0.01215917 -0.98022182 -0.0560925   0.89918835]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 20 ] state=[-0.01215917 -0.98022182 -0.0560925   0.89918835], action=0, reward=1.0, next_state=[-0.0317636  -1.17454054 -0.03810873  1.17372546]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 21 ] state=[-0.0317636  -1.17454054 -0.03810873  1.17372546], action=0, reward=1.0, next_state=[-0.05525441 -1.36914703 -0.01463422  1.45422199]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 22 ] state=[-0.05525441 -1.36914703 -0.01463422  1.45422199], action=0, reward=1.0, next_state=[-0.08263735 -1.56408627  0.01445022  1.74229724]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 23 ] state=[-0.08263735 -1.56408627  0.01445022  1.74229724], action=0, reward=1.0, next_state=[-0.11391908 -1.75936965  0.04929616  2.03943994]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 24 ] state=[-0.11391908 -1.75936965  0.04929616  2.03943994], action=0, reward=1.0, next_state=[-0.14910647 -1.95496273  0.09008496  2.34696035]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 25 ] state=[-0.14910647 -1.95496273  0.09008496  2.34696035], action=1, reward=1.0, next_state=[-0.18820573 -1.76075731  0.13702417  2.08328262]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 26 ] state=[-0.18820573 -1.76075731  0.13702417  2.08328262], action=1, reward=1.0, next_state=[-0.22342087 -1.56726177  0.17868982  1.83591896]\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ][ timestamp 27 ] state=[-0.22342087 -1.56726177  0.17868982  1.83591896], action=0, reward=-1.0, next_state=[-0.25476611 -1.76385328  0.2154082   2.17836653]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 135: Exploration_rate=0.01. Score=27.\n",
      "[ episode 136 ] state=[-0.00462421  0.03435132 -0.03151275  0.00561057]\n",
      "[ episode 136 ][ timestamp 1 ] state=[-0.00462421  0.03435132 -0.03151275  0.00561057], action=1, reward=1.0, next_state=[-0.00393719  0.22991071 -0.03140054 -0.29684609]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 2 ] state=[-0.00393719  0.22991071 -0.03140054 -0.29684609], action=0, reward=1.0, next_state=[ 0.00066103  0.03525013 -0.03733746 -0.0142294 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 3 ] state=[ 0.00066103  0.03525013 -0.03733746 -0.0142294 ], action=0, reward=1.0, next_state=[ 0.00136603 -0.15931701 -0.03762205  0.26644324]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 4 ] state=[ 0.00136603 -0.15931701 -0.03762205  0.26644324], action=1, reward=1.0, next_state=[-0.00182031  0.03632113 -0.03229318 -0.03786458]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 5 ] state=[-0.00182031  0.03632113 -0.03229318 -0.03786458], action=1, reward=1.0, next_state=[-0.00109389  0.23189092 -0.03305048 -0.34055888]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 6 ] state=[-0.00109389  0.23189092 -0.03305048 -0.34055888], action=0, reward=1.0, next_state=[ 0.00354393  0.03725443 -0.03986165 -0.05847865]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 7 ] state=[ 0.00354393  0.03725443 -0.03986165 -0.05847865], action=0, reward=1.0, next_state=[ 0.00428902 -0.15727398 -0.04103123  0.22136595]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 8 ] state=[ 0.00428902 -0.15727398 -0.04103123  0.22136595], action=1, reward=1.0, next_state=[ 0.00114354  0.03840971 -0.03660391 -0.08397232]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 9 ] state=[ 0.00114354  0.03840971 -0.03660391 -0.08397232], action=0, reward=1.0, next_state=[ 0.00191174 -0.15616894 -0.03828335  0.196941  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 10 ] state=[ 0.00191174 -0.15616894 -0.03828335  0.196941  ], action=0, reward=1.0, next_state=[-0.00121164 -0.35072299 -0.03434453  0.47730568]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 11 ] state=[-0.00121164 -0.35072299 -0.03434453  0.47730568], action=0, reward=1.0, next_state=[-0.0082261  -0.54534362 -0.02479842  0.75896916]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 12 ] state=[-0.0082261  -0.54534362 -0.02479842  0.75896916], action=1, reward=1.0, next_state=[-0.01913298 -0.34988889 -0.00961904  0.45858723]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 13 ] state=[-0.01913298 -0.34988889 -0.00961904  0.45858723], action=1, reward=1.0, next_state=[-0.02613075 -0.15463229 -0.00044729  0.16288793]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 14 ] state=[-0.02613075 -0.15463229 -0.00044729  0.16288793], action=1, reward=1.0, next_state=[-0.0292234   0.04049606  0.00281047 -0.12993608]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 15 ] state=[-0.0292234   0.04049606  0.00281047 -0.12993608], action=1, reward=1.0, next_state=[-2.84134782e-02  2.35577636e-01  2.11744365e-04 -4.21731014e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 16 ] state=[-2.84134782e-02  2.35577636e-01  2.11744365e-04 -4.21731014e-01], action=0, reward=1.0, next_state=[-0.02370193  0.04045269 -0.00822288 -0.12898134]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 17 ] state=[-0.02370193  0.04045269 -0.00822288 -0.12898134], action=0, reward=1.0, next_state=[-0.02289287 -0.15455051 -0.0108025   0.16109607]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 18 ] state=[-0.02289287 -0.15455051 -0.0108025   0.16109607], action=0, reward=1.0, next_state=[-0.02598388 -0.34951616 -0.00758058  0.4503516 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 19 ] state=[-0.02598388 -0.34951616 -0.00758058  0.4503516 ], action=1, reward=1.0, next_state=[-0.03297421 -0.15428782  0.00142645  0.15528883]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 20 ] state=[-0.03297421 -0.15428782  0.00142645  0.15528883], action=1, reward=1.0, next_state=[-0.03605996  0.04081368  0.00453223 -0.13694374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 21 ] state=[-0.03605996  0.04081368  0.00453223 -0.13694374], action=1, reward=1.0, next_state=[-0.03524369  0.23587042  0.00179335 -0.42819338]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 22 ] state=[-0.03524369  0.23587042  0.00179335 -0.42819338], action=1, reward=1.0, next_state=[-0.03052628  0.43096693 -0.00677052 -0.72031042]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 23 ] state=[-0.03052628  0.43096693 -0.00677052 -0.72031042], action=1, reward=1.0, next_state=[-0.02190694  0.6261819  -0.02117672 -1.01511668]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 24 ] state=[-0.02190694  0.6261819  -0.02117672 -1.01511668], action=0, reward=1.0, next_state=[-0.0093833   0.43134866 -0.04147906 -0.72915785]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 25 ] state=[-0.0093833   0.43134866 -0.04147906 -0.72915785], action=0, reward=1.0, next_state=[-0.00075633  0.23682387 -0.05606221 -0.44981297]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 26 ] state=[-0.00075633  0.23682387 -0.05606221 -0.44981297], action=0, reward=1.0, next_state=[ 0.00398015  0.04253782 -0.06505847 -0.17531542]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 27 ] state=[ 0.00398015  0.04253782 -0.06505847 -0.17531542], action=0, reward=1.0, next_state=[ 0.0048309  -0.15159564 -0.06856478  0.09615502]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 28 ] state=[ 0.0048309  -0.15159564 -0.06856478  0.09615502], action=0, reward=1.0, next_state=[ 0.00179899 -0.34567131 -0.06664168  0.36644226]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 29 ] state=[ 0.00179899 -0.34567131 -0.06664168  0.36644226], action=0, reward=1.0, next_state=[-0.00511443 -0.53978609 -0.05931284  0.63738995]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 30 ] state=[-0.00511443 -0.53978609 -0.05931284  0.63738995], action=1, reward=1.0, next_state=[-0.01591016 -0.34388936 -0.04656504  0.32663383]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 31 ] state=[-0.01591016 -0.34388936 -0.04656504  0.32663383], action=0, reward=1.0, next_state=[-0.02278794 -0.53831849 -0.04003236  0.60427621]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 32 ] state=[-0.02278794 -0.53831849 -0.04003236  0.60427621], action=1, reward=1.0, next_state=[-0.03355431 -0.34266022 -0.02794684  0.29925757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 33 ] state=[-0.03355431 -0.34266022 -0.02794684  0.29925757], action=1, reward=1.0, next_state=[-0.04040752 -0.14715129 -0.02196169 -0.00210661]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 34 ] state=[-0.04040752 -0.14715129 -0.02196169 -0.00210661], action=1, reward=1.0, next_state=[-0.04335054  0.04827863 -0.02200382 -0.30163701]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 136 ][ timestamp 35 ] state=[-0.04335054  0.04827863 -0.02200382 -0.30163701], action=0, reward=1.0, next_state=[-0.04238497 -0.14652291 -0.02803656 -0.01597403]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 36 ] state=[-0.04238497 -0.14652291 -0.02803656 -0.01597403], action=1, reward=1.0, next_state=[-0.04531543  0.04898967 -0.02835604 -0.31736931]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 37 ] state=[-0.04531543  0.04898967 -0.02835604 -0.31736931], action=1, reward=1.0, next_state=[-0.04433564  0.24450378 -0.03470342 -0.61885814]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 38 ] state=[-0.04433564  0.24450378 -0.03470342 -0.61885814], action=0, reward=1.0, next_state=[-0.03944556  0.04988331 -0.04708059 -0.33730397]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 39 ] state=[-0.03944556  0.04988331 -0.04708059 -0.33730397], action=0, reward=1.0, next_state=[-0.03844789 -0.14453815 -0.05382667 -0.05983151]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 40 ] state=[-0.03844789 -0.14453815 -0.05382667 -0.05983151], action=0, reward=1.0, next_state=[-0.04133866 -0.33884867 -0.0550233   0.21539474]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 41 ] state=[-0.04133866 -0.33884867 -0.0550233   0.21539474], action=1, reward=1.0, next_state=[-0.04811563 -0.14298503 -0.0507154  -0.09412478]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 42 ] state=[-0.04811563 -0.14298503 -0.0507154  -0.09412478], action=1, reward=1.0, next_state=[-0.05097533  0.05282575 -0.0525979  -0.40236724]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 43 ] state=[-0.05097533  0.05282575 -0.0525979  -0.40236724], action=0, reward=1.0, next_state=[-0.04991882 -0.14151226 -0.06064524 -0.12672001]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 44 ] state=[-0.04991882 -0.14151226 -0.06064524 -0.12672001], action=1, reward=1.0, next_state=[-0.05274906  0.05442368 -0.06317964 -0.43790241]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 45 ] state=[-0.05274906  0.05442368 -0.06317964 -0.43790241], action=0, reward=1.0, next_state=[-0.05166059 -0.13974976 -0.07193769 -0.16578581]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 46 ] state=[-0.05166059 -0.13974976 -0.07193769 -0.16578581], action=0, reward=1.0, next_state=[-0.05445558 -0.33377216 -0.07525341  0.10336362]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 47 ] state=[-0.05445558 -0.33377216 -0.07525341  0.10336362], action=0, reward=1.0, next_state=[-0.06113103 -0.5277395  -0.07318613  0.37138755]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 48 ] state=[-0.06113103 -0.5277395  -0.07318613  0.37138755], action=1, reward=1.0, next_state=[-0.07168582 -0.33165825 -0.06575838  0.05655548]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 49 ] state=[-0.07168582 -0.33165825 -0.06575838  0.05655548], action=1, reward=1.0, next_state=[-0.07831898 -0.13565807 -0.06462727 -0.25612839]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 50 ] state=[-0.07831898 -0.13565807 -0.06462727 -0.25612839], action=0, reward=1.0, next_state=[-0.08103214 -0.32980064 -0.06974984  0.01549032]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 51 ] state=[-0.08103214 -0.32980064 -0.06974984  0.01549032], action=0, reward=1.0, next_state=[-0.08762816 -0.52385656 -0.06944004  0.2853766 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 52 ] state=[-0.08762816 -0.52385656 -0.06944004  0.2853766 ], action=0, reward=1.0, next_state=[-0.09810529 -0.71792299 -0.0637325   0.55537573]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 53 ] state=[-0.09810529 -0.71792299 -0.0637325   0.55537573], action=1, reward=1.0, next_state=[-0.11246375 -0.52196682 -0.05262499  0.24331355]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 54 ] state=[-0.11246375 -0.52196682 -0.05262499  0.24331355], action=1, reward=1.0, next_state=[-0.12290308 -0.32613423 -0.04775872 -0.06549329]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 55 ] state=[-0.12290308 -0.32613423 -0.04775872 -0.06549329], action=0, reward=1.0, next_state=[-0.12942577 -0.52054007 -0.04906858  0.21174725]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 56 ] state=[-0.12942577 -0.52054007 -0.04906858  0.21174725], action=0, reward=1.0, next_state=[-0.13983657 -0.71492736 -0.04483364  0.48855686]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 57 ] state=[-0.13983657 -0.71492736 -0.04483364  0.48855686], action=1, reward=1.0, next_state=[-0.15413512 -0.5192025  -0.0350625   0.1820879 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 58 ] state=[-0.15413512 -0.5192025  -0.0350625   0.1820879 ], action=0, reward=1.0, next_state=[-0.16451917 -0.71380565 -0.03142074  0.46350695]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 59 ] state=[-0.16451917 -0.71380565 -0.03142074  0.46350695], action=1, reward=1.0, next_state=[-0.17879528 -0.51825408 -0.0221506   0.1610882 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 60 ] state=[-0.17879528 -0.51825408 -0.0221506   0.1610882 ], action=0, reward=1.0, next_state=[-0.18916036 -0.71305203 -0.01892884  0.44670171]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 61 ] state=[-0.18916036 -0.71305203 -0.01892884  0.44670171], action=1, reward=1.0, next_state=[-0.2034214  -0.51766748 -0.00999481  0.14811264]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 62 ] state=[-0.2034214  -0.51766748 -0.00999481  0.14811264], action=1, reward=1.0, next_state=[-0.21377475 -0.32240384 -0.00703255 -0.14770661]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 63 ] state=[-0.21377475 -0.32240384 -0.00703255 -0.14770661], action=1, reward=1.0, next_state=[-0.22022283 -0.12718189 -0.00998669 -0.44259985]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 64 ] state=[-0.22022283 -0.12718189 -0.00998669 -0.44259985], action=0, reward=1.0, next_state=[-0.22276647 -0.32216112 -0.01883868 -0.15308164]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 65 ] state=[-0.22276647 -0.32216112 -0.01883868 -0.15308164], action=0, reward=1.0, next_state=[-0.22920969 -0.51700833 -0.02190032  0.13359908]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 66 ] state=[-0.22920969 -0.51700833 -0.02190032  0.13359908], action=1, reward=1.0, next_state=[-0.23954985 -0.32157963 -0.01922833 -0.16591185]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 67 ] state=[-0.23954985 -0.32157963 -0.01922833 -0.16591185], action=0, reward=1.0, next_state=[-0.24598145 -0.51642114 -0.02254657  0.1206436 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 68 ] state=[-0.24598145 -0.51642114 -0.02254657  0.1206436 ], action=1, reward=1.0, next_state=[-0.25630987 -0.32098353 -0.0201337  -0.17906643]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 69 ] state=[-0.25630987 -0.32098353 -0.0201337  -0.17906643], action=0, reward=1.0, next_state=[-0.26272954 -0.51581167 -0.02371503  0.10719763]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 70 ] state=[-0.26272954 -0.51581167 -0.02371503  0.10719763], action=0, reward=1.0, next_state=[-0.27304577 -0.71058589 -0.02157107  0.39230525]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 71 ] state=[-0.27304577 -0.71058589 -0.02157107  0.39230525], action=1, reward=1.0, next_state=[-0.28725749 -0.51516457 -0.01372497  0.09290005]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 72 ] state=[-0.28725749 -0.51516457 -0.01372497  0.09290005], action=0, reward=1.0, next_state=[-0.29756078 -0.71008713 -0.01186697  0.38122135]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 73 ] state=[-0.29756078 -0.71008713 -0.01186697  0.38122135], action=1, reward=1.0, next_state=[-0.31176253 -0.5147987  -0.00424254  0.08482053]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 74 ] state=[-0.31176253 -0.5147987  -0.00424254  0.08482053], action=0, reward=1.0, next_state=[-0.3220585  -0.70985958 -0.00254613  0.37616191]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 75 ] state=[-0.3220585  -0.70985958 -0.00254613  0.37616191], action=0, reward=1.0, next_state=[-0.33625569 -0.90494528  0.00497711  0.66804095]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 76 ] state=[-0.33625569 -0.90494528  0.00497711  0.66804095], action=0, reward=1.0, next_state=[-0.3543546  -1.10013609  0.01833793  0.96228679]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 77 ] state=[-0.3543546  -1.10013609  0.01833793  0.96228679], action=0, reward=1.0, next_state=[-0.37635732 -1.2954996   0.03758366  1.26067384]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 78 ] state=[-0.37635732 -1.2954996   0.03758366  1.26067384], action=0, reward=1.0, next_state=[-0.40226731 -1.49108158  0.06279714  1.56488663]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 79 ] state=[-0.40226731 -1.49108158  0.06279714  1.56488663], action=0, reward=1.0, next_state=[-0.43208894 -1.68689541  0.09409487  1.87647865]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 80 ] state=[-0.43208894 -1.68689541  0.09409487  1.87647865], action=0, reward=1.0, next_state=[-0.46582685 -1.88290949  0.13162444  2.19682221]\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ][ timestamp 81 ] state=[-0.46582685 -1.88290949  0.13162444  2.19682221], action=1, reward=1.0, next_state=[-0.50348504 -1.6892796   0.17556089  1.94747567]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 136 ][ timestamp 82 ] state=[-0.50348504 -1.6892796   0.17556089  1.94747567], action=1, reward=-1.0, next_state=[-0.53727063 -1.49640829  0.2145104   1.71396588]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 136: Exploration_rate=0.01. Score=82.\n",
      "[ episode 137 ] state=[-0.04011504  0.00272797  0.02955492 -0.00266256]\n",
      "[ episode 137 ][ timestamp 1 ] state=[-0.04011504  0.00272797  0.02955492 -0.00266256], action=1, reward=1.0, next_state=[-0.04006048  0.19741386  0.02950167 -0.285876  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 2 ] state=[-0.04006048  0.19741386  0.02950167 -0.285876  ], action=0, reward=1.0, next_state=[-0.0361122   0.00188386  0.02378415  0.01596362]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 3 ] state=[-0.0361122   0.00188386  0.02378415  0.01596362], action=1, reward=1.0, next_state=[-0.03607452  0.19665679  0.02410343 -0.26912126]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 4 ] state=[-0.03607452  0.19665679  0.02410343 -0.26912126], action=0, reward=1.0, next_state=[-0.03214139  0.0011993   0.018721    0.03106553]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 5 ] state=[-0.03214139  0.0011993   0.018721    0.03106553], action=0, reward=1.0, next_state=[-0.0321174  -0.19418605  0.01934231  0.32959585]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 6 ] state=[-0.0321174  -0.19418605  0.01934231  0.32959585], action=1, reward=1.0, next_state=[-0.03600112  0.00065529  0.02593423  0.04307479]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 7 ] state=[-0.03600112  0.00065529  0.02593423  0.04307479], action=1, reward=1.0, next_state=[-0.03598802  0.19539594  0.02679572 -0.24131415]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 8 ] state=[-0.03598802  0.19539594  0.02679572 -0.24131415], action=0, reward=1.0, next_state=[-3.20800978e-02 -9.83233514e-05  2.19694410e-02  5.96989858e-02]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 9 ] state=[-3.20800978e-02 -9.83233514e-05  2.19694410e-02  5.96989858e-02], action=1, reward=1.0, next_state=[-0.03208206  0.19470185  0.02316342 -0.22597227]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 10 ] state=[-0.03208206  0.19470185  0.02316342 -0.22597227], action=1, reward=1.0, next_state=[-0.02818803  0.38948523  0.01864398 -0.51125953]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 11 ] state=[-0.02818803  0.38948523  0.01864398 -0.51125953], action=0, reward=1.0, next_state=[-0.02039832  0.19410569  0.00841878 -0.21276013]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 12 ] state=[-0.02039832  0.19410569  0.00841878 -0.21276013], action=1, reward=1.0, next_state=[-0.01651621  0.38910627  0.00416358 -0.50277554]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 13 ] state=[-0.01651621  0.38910627  0.00416358 -0.50277554], action=0, reward=1.0, next_state=[-0.00873408  0.19392588 -0.00589193 -0.2087834 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 14 ] state=[-0.00873408  0.19392588 -0.00589193 -0.2087834 ], action=0, reward=1.0, next_state=[-0.00485557 -0.00111133 -0.0100676   0.08203512]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 15 ] state=[-0.00485557 -0.00111133 -0.0100676   0.08203512], action=1, reward=1.0, next_state=[-0.00487779  0.19415348 -0.00842689 -0.21380708]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 16 ] state=[-0.00487779  0.19415348 -0.00842689 -0.21380708], action=0, reward=1.0, next_state=[-0.00099472 -0.00084699 -0.01270304  0.07620576]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 17 ] state=[-0.00099472 -0.00084699 -0.01270304  0.07620576], action=1, reward=1.0, next_state=[-0.00101166  0.19445475 -0.01117892 -0.2204578 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 18 ] state=[-0.00101166  0.19445475 -0.01117892 -0.2204578 ], action=0, reward=1.0, next_state=[ 0.00287743 -0.00050564 -0.01558808  0.06867798]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 19 ] state=[ 0.00287743 -0.00050564 -0.01558808  0.06867798], action=1, reward=1.0, next_state=[ 0.00286732  0.19483628 -0.01421452 -0.22888202]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 20 ] state=[ 0.00286732  0.19483628 -0.01421452 -0.22888202], action=1, reward=1.0, next_state=[ 0.00676405  0.39015845 -0.01879216 -0.5260146 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 21 ] state=[ 0.00676405  0.39015845 -0.01879216 -0.5260146 ], action=0, reward=1.0, next_state=[ 0.01456721  0.19530591 -0.02931245 -0.23931196]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 22 ] state=[ 0.01456721  0.19530591 -0.02931245 -0.23931196], action=0, reward=1.0, next_state=[ 0.01847333  0.0006147  -0.03409869  0.04398278]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 23 ] state=[ 0.01847333  0.0006147  -0.03409869  0.04398278], action=1, reward=1.0, next_state=[ 0.01848563  0.19620859 -0.03321903 -0.25926059]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 24 ] state=[ 0.01848563  0.19620859 -0.03321903 -0.25926059], action=0, reward=1.0, next_state=[ 0.0224098   0.00157623 -0.03840424  0.02276228]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 25 ] state=[ 0.0224098   0.00157623 -0.03840424  0.02276228], action=1, reward=1.0, next_state=[ 0.02244132  0.19722729 -0.037949   -0.28178599]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 26 ] state=[ 0.02244132  0.19722729 -0.037949   -0.28178599], action=0, reward=1.0, next_state=[ 0.02638587  0.00266662 -0.04358472 -0.00130942]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 27 ] state=[ 0.02638587  0.00266662 -0.04358472 -0.00130942], action=0, reward=1.0, next_state=[ 0.0264392  -0.19180404 -0.04361091  0.2773097 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 28 ] state=[ 0.0264392  -0.19180404 -0.04361091  0.2773097 ], action=1, reward=1.0, next_state=[ 0.02260312  0.00391207 -0.03806471 -0.02880288]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 29 ] state=[ 0.02260312  0.00391207 -0.03806471 -0.02880288], action=0, reward=1.0, next_state=[ 0.02268136 -0.19064392 -0.03864077  0.25163138]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 30 ] state=[ 0.02268136 -0.19064392 -0.03864077  0.25163138], action=0, reward=1.0, next_state=[ 0.01886848 -0.3851934  -0.03360814  0.5318802 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 31 ] state=[ 0.01886848 -0.3851934  -0.03360814  0.5318802 ], action=1, reward=1.0, next_state=[ 0.01116462 -0.18961525 -0.02297054  0.22879971]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 32 ] state=[ 0.01116462 -0.18961525 -0.02297054  0.22879971], action=1, reward=1.0, next_state=[ 0.00737231  0.0058273  -0.01839455 -0.07103951]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 33 ] state=[ 0.00737231  0.0058273  -0.01839455 -0.07103951], action=0, reward=1.0, next_state=[ 0.00748886 -0.18902618 -0.01981534  0.21578356]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 34 ] state=[ 0.00748886 -0.18902618 -0.01981534  0.21578356], action=1, reward=1.0, next_state=[ 0.00370833  0.00637337 -0.01549966 -0.08308355]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 35 ] state=[ 0.00370833  0.00637337 -0.01549966 -0.08308355], action=1, reward=1.0, next_state=[ 0.0038358   0.20171403 -0.01716134 -0.38061607]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 36 ] state=[ 0.0038358   0.20171403 -0.01716134 -0.38061607], action=0, reward=1.0, next_state=[ 0.00787008  0.00683993 -0.02477366 -0.09339314]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 37 ] state=[ 0.00787008  0.00683993 -0.02477366 -0.09339314], action=1, reward=1.0, next_state=[ 0.00800688  0.20230804 -0.02664152 -0.39378804]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 38 ] state=[ 0.00800688  0.20230804 -0.02664152 -0.39378804], action=0, reward=1.0, next_state=[ 0.01205304  0.00757406 -0.03451728 -0.10962242]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 39 ] state=[ 0.01205304  0.00757406 -0.03451728 -0.10962242], action=0, reward=1.0, next_state=[ 0.01220452 -0.1870367  -0.03670973  0.17197377]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 40 ] state=[ 0.01220452 -0.1870367  -0.03670973  0.17197377], action=1, reward=1.0, next_state=[ 0.00846379  0.00859091 -0.03327025 -0.13206018]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 41 ] state=[ 0.00846379  0.00859091 -0.03327025 -0.13206018], action=1, reward=1.0, next_state=[ 0.00863561  0.20417325 -0.03591146 -0.43505099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 42 ] state=[ 0.00863561  0.20417325 -0.03591146 -0.43505099], action=0, reward=1.0, next_state=[ 0.01271907  0.00957761 -0.04461248 -0.15390141]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 43 ] state=[ 0.01271907  0.00957761 -0.04461248 -0.15390141], action=1, reward=1.0, next_state=[ 0.01291062  0.205309   -0.0476905  -0.4603181 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 44 ] state=[ 0.01291062  0.205309   -0.0476905  -0.4603181 ], action=0, reward=1.0, next_state=[ 0.0170168   0.01089244 -0.05689687 -0.18304053]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 45 ] state=[ 0.0170168   0.01089244 -0.05689687 -0.18304053], action=0, reward=1.0, next_state=[ 0.01723465 -0.18337118 -0.06055768  0.09116472]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 137 ][ timestamp 46 ] state=[ 0.01723465 -0.18337118 -0.06055768  0.09116472], action=1, reward=1.0, next_state=[ 0.01356723  0.01256413 -0.05873438 -0.21999259]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 47 ] state=[ 0.01356723  0.01256413 -0.05873438 -0.21999259], action=1, reward=1.0, next_state=[ 0.01381851  0.20847432 -0.06313423 -0.53060912]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 48 ] state=[ 0.01381851  0.20847432 -0.06313423 -0.53060912], action=0, reward=1.0, next_state=[ 0.017988    0.01429463 -0.07374642 -0.25846902]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 49 ] state=[ 0.017988    0.01429463 -0.07374642 -0.25846902], action=0, reward=1.0, next_state=[ 0.01827389 -0.17970126 -0.0789158   0.01007209]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 50 ] state=[ 0.01827389 -0.17970126 -0.0789158   0.01007209], action=1, reward=1.0, next_state=[ 0.01467986  0.01645855 -0.07871436 -0.30642906]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 51 ] state=[ 0.01467986  0.01645855 -0.07871436 -0.30642906], action=0, reward=1.0, next_state=[ 0.01500904 -0.17745865 -0.08484294 -0.03957205]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 52 ] state=[ 0.01500904 -0.17745865 -0.08484294 -0.03957205], action=1, reward=1.0, next_state=[ 0.01145986  0.01877101 -0.08563438 -0.35777169]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 53 ] state=[ 0.01145986  0.01877101 -0.08563438 -0.35777169], action=0, reward=1.0, next_state=[ 0.01183528 -0.17503576 -0.09278981 -0.09327256]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 54 ] state=[ 0.01183528 -0.17503576 -0.09278981 -0.09327256], action=0, reward=1.0, next_state=[ 0.00833457 -0.36871371 -0.09465526  0.16875353]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 55 ] state=[ 0.00833457 -0.36871371 -0.09465526  0.16875353], action=0, reward=1.0, next_state=[ 0.00096029 -0.56236231 -0.09128019  0.43013903]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 56 ] state=[ 0.00096029 -0.56236231 -0.09128019  0.43013903], action=0, reward=1.0, next_state=[-0.01028695 -0.75608111 -0.08267741  0.69270838]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 57 ] state=[-0.01028695 -0.75608111 -0.08267741  0.69270838], action=1, reward=1.0, next_state=[-0.02540858 -0.55991539 -0.06882324  0.37518543]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 58 ] state=[-0.02540858 -0.55991539 -0.06882324  0.37518543], action=0, reward=1.0, next_state=[-0.03660688 -0.75399572 -0.06131954  0.64539866]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 59 ] state=[-0.03660688 -0.75399572 -0.06131954  0.64539866], action=1, reward=1.0, next_state=[-0.0516868  -0.5580753  -0.04841156  0.33405372]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 60 ] state=[-0.0516868  -0.5580753  -0.04841156  0.33405372], action=0, reward=1.0, next_state=[-0.0628483  -0.752476   -0.04173049  0.61108568]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 61 ] state=[-0.0628483  -0.752476   -0.04173049  0.61108568], action=1, reward=1.0, next_state=[-0.07789782 -0.55679638 -0.02950877  0.30555659]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 62 ] state=[-0.07789782 -0.55679638 -0.02950877  0.30555659], action=1, reward=1.0, next_state=[-0.08903375 -0.36126662 -0.02339764  0.00371531]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 63 ] state=[-0.08903375 -0.36126662 -0.02339764  0.00371531], action=0, reward=1.0, next_state=[-0.09625908 -0.55604534 -0.02332334  0.28892514]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 64 ] state=[-0.09625908 -0.55604534 -0.02332334  0.28892514], action=0, reward=1.0, next_state=[-0.10737999 -0.75082706 -0.01754483  0.57416183]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 65 ] state=[-0.10737999 -0.75082706 -0.01754483  0.57416183], action=0, reward=1.0, next_state=[-0.12239653 -0.9456987  -0.0060616   0.86126638]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 66 ] state=[-0.12239653 -0.9456987  -0.0060616   0.86126638], action=0, reward=1.0, next_state=[-0.14131051 -1.14073759  0.01116373  1.15203723]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 67 ] state=[-0.14131051 -1.14073759  0.01116373  1.15203723], action=0, reward=1.0, next_state=[-0.16412526 -1.33600339  0.03420448  1.44819976]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 68 ] state=[-0.16412526 -1.33600339  0.03420448  1.44819976], action=0, reward=1.0, next_state=[-0.19084533 -1.5315288   0.06316847  1.75137048]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 69 ] state=[-0.19084533 -1.5315288   0.06316847  1.75137048], action=0, reward=1.0, next_state=[-0.2214759  -1.72730828  0.09819588  2.06301317]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 70 ] state=[-0.2214759  -1.72730828  0.09819588  2.06301317], action=0, reward=1.0, next_state=[-0.25602207 -1.92328457  0.13945614  2.38438469]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 71 ] state=[-0.25602207 -1.92328457  0.13945614  2.38438469], action=0, reward=1.0, next_state=[-0.29448776 -2.11933251  0.18714384  2.71646901]\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ][ timestamp 72 ] state=[-0.29448776 -2.11933251  0.18714384  2.71646901], action=0, reward=-1.0, next_state=[-0.33687441 -2.3152405   0.24147322  3.05989975]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 137: Exploration_rate=0.01. Score=72.\n",
      "[ episode 138 ] state=[ 0.00704646 -0.02661459 -0.03318057 -0.01436639]\n",
      "[ episode 138 ][ timestamp 1 ] state=[ 0.00704646 -0.02661459 -0.03318057 -0.01436639], action=1, reward=1.0, next_state=[ 0.00651417  0.16896711 -0.0334679  -0.31733076]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 2 ] state=[ 0.00651417  0.16896711 -0.0334679  -0.31733076], action=0, reward=1.0, next_state=[ 0.00989351 -0.02566256 -0.03981452 -0.03538747]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 3 ] state=[ 0.00989351 -0.02566256 -0.03981452 -0.03538747], action=0, reward=1.0, next_state=[ 0.00938026 -0.22019161 -0.04052227  0.24447249]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 4 ] state=[ 0.00938026 -0.22019161 -0.04052227  0.24447249], action=1, reward=1.0, next_state=[ 0.00497643 -0.02451501 -0.03563282 -0.06071175]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 5 ] state=[ 0.00497643 -0.02451501 -0.03563282 -0.06071175], action=1, reward=1.0, next_state=[ 0.00448613  0.17109924 -0.03684705 -0.3644207 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 6 ] state=[ 0.00448613  0.17109924 -0.03684705 -0.3644207 ], action=1, reward=1.0, next_state=[ 0.00790811  0.36672496 -0.04413546 -0.66849068]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 7 ] state=[ 0.00790811  0.36672496 -0.04413546 -0.66849068], action=0, reward=1.0, next_state=[ 0.01524261  0.17224362 -0.05750528 -0.39002436]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 8 ] state=[ 0.01524261  0.17224362 -0.05750528 -0.39002436], action=0, reward=1.0, next_state=[ 0.01868748 -0.02201702 -0.06530577 -0.11601231]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 9 ] state=[ 0.01868748 -0.02201702 -0.06530577 -0.11601231], action=0, reward=1.0, next_state=[ 0.01824714 -0.21614541 -0.06762601  0.15537331]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 10 ] state=[ 0.01824714 -0.21614541 -0.06762601  0.15537331], action=1, reward=1.0, next_state=[ 0.01392423 -0.02012364 -0.06451855 -0.15785415]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 11 ] state=[ 0.01392423 -0.02012364 -0.06451855 -0.15785415], action=0, reward=1.0, next_state=[ 0.01352176 -0.21426539 -0.06767563  0.11379728]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 12 ] state=[ 0.01352176 -0.21426539 -0.06767563  0.11379728], action=0, reward=1.0, next_state=[ 0.00923645 -0.40835564 -0.06539968  0.38438476]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 13 ] state=[ 0.00923645 -0.40835564 -0.06539968  0.38438476], action=0, reward=1.0, next_state=[ 0.00106934 -0.60249109 -0.05771199  0.6557516 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 14 ] state=[ 0.00106934 -0.60249109 -0.05771199  0.6557516 ], action=1, reward=1.0, next_state=[-0.01098048 -0.40661518 -0.04459696  0.34546899]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 15 ] state=[-0.01098048 -0.40661518 -0.04459696  0.34546899], action=1, reward=1.0, next_state=[-0.01911279 -0.21088815 -0.03768758  0.0390632 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 16 ] state=[-0.01911279 -0.21088815 -0.03768758  0.0390632 ], action=1, reward=1.0, next_state=[-0.02333055 -0.01524659 -0.03690631 -0.26526828]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 17 ] state=[-0.02333055 -0.01524659 -0.03690631 -0.26526828], action=0, reward=1.0, next_state=[-0.02363548 -0.20982288 -0.04221168  0.01554942]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 18 ] state=[-0.02363548 -0.20982288 -0.04221168  0.01554942], action=0, reward=1.0, next_state=[-0.02783194 -0.40431484 -0.04190069  0.29462094]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 19 ] state=[-0.02783194 -0.40431484 -0.04190069  0.29462094], action=1, reward=1.0, next_state=[-0.03591823 -0.20862135 -0.03600827 -0.01097686]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 20 ] state=[-0.03591823 -0.20862135 -0.03600827 -0.01097686], action=0, reward=1.0, next_state=[-0.04009066 -0.40320889 -0.03622781  0.2701311 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 21 ] state=[-0.04009066 -0.40320889 -0.03622781  0.2701311 ], action=0, reward=1.0, next_state=[-0.04815484 -0.59779563 -0.03082519  0.55117105]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 22 ] state=[-0.04815484 -0.59779563 -0.03082519  0.55117105], action=1, reward=1.0, next_state=[-0.06011075 -0.40225461 -0.01980176  0.24893769]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 138 ][ timestamp 23 ] state=[-0.06011075 -0.40225461 -0.01980176  0.24893769], action=1, reward=1.0, next_state=[-0.06815584 -0.20685555 -0.01482301 -0.04992478]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 24 ] state=[-0.06815584 -0.20685555 -0.01482301 -0.04992478], action=0, reward=1.0, next_state=[-0.07229296 -0.40176185 -0.01582151  0.23804475]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 25 ] state=[-0.07229296 -0.40176185 -0.01582151  0.23804475], action=1, reward=1.0, next_state=[-0.08032819 -0.20641749 -0.01106061 -0.05958646]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 26 ] state=[-0.08032819 -0.20641749 -0.01106061 -0.05958646], action=0, reward=1.0, next_state=[-0.08445654 -0.40137912 -0.01225234  0.22958635]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 27 ] state=[-0.08445654 -0.40137912 -0.01225234  0.22958635], action=1, reward=1.0, next_state=[-0.09248412 -0.20608424 -0.00766061 -0.06693607]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 28 ] state=[-0.09248412 -0.20608424 -0.00766061 -0.06693607], action=1, reward=1.0, next_state=[-0.09660581 -0.0108533  -0.00899933 -0.36202609]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 29 ] state=[-0.09660581 -0.0108533  -0.00899933 -0.36202609], action=1, reward=1.0, next_state=[-0.09682288  0.1843954  -0.01623986 -0.65753305]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 30 ] state=[-0.09682288  0.1843954  -0.01623986 -0.65753305], action=0, reward=1.0, next_state=[-0.09313497 -0.01049678 -0.02939052 -0.37000764]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 31 ] state=[-0.09313497 -0.01049678 -0.02939052 -0.37000764], action=0, reward=1.0, next_state=[-0.0933449  -0.20518909 -0.03679067 -0.08673486]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 32 ] state=[-0.0933449  -0.20518909 -0.03679067 -0.08673486], action=0, reward=1.0, next_state=[-0.09744868 -0.3997649  -0.03852537  0.19411733]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 33 ] state=[-0.09744868 -0.3997649  -0.03852537  0.19411733], action=0, reward=1.0, next_state=[-0.10544398 -0.59431518 -0.03464302  0.47440256]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 34 ] state=[-0.10544398 -0.59431518 -0.03464302  0.47440256], action=1, reward=1.0, next_state=[-0.11733029 -0.39872157 -0.02515497  0.17100517]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 35 ] state=[-0.11733029 -0.39872157 -0.02515497  0.17100517], action=1, reward=1.0, next_state=[-0.12530472 -0.20324878 -0.02173487 -0.12950604]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 36 ] state=[-0.12530472 -0.20324878 -0.02173487 -0.12950604], action=0, reward=1.0, next_state=[-0.12936969 -0.39805274 -0.02432499  0.15624134]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 37 ] state=[-0.12936969 -0.39805274 -0.02432499  0.15624134], action=0, reward=1.0, next_state=[-0.13733075 -0.59281812 -0.02120016  0.44115215]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 38 ] state=[-0.13733075 -0.59281812 -0.02120016  0.44115215], action=1, reward=1.0, next_state=[-0.14918711 -0.39740267 -0.01237712  0.14186246]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 39 ] state=[-0.14918711 -0.39740267 -0.01237712  0.14186246], action=0, reward=1.0, next_state=[-0.15713516 -0.59234519 -0.00953987  0.43061506]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 40 ] state=[-0.15713516 -0.59234519 -0.00953987  0.43061506], action=1, reward=1.0, next_state=[-0.16898207 -0.39708946 -0.00092757  0.13494011]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 41 ] state=[-0.16898207 -0.39708946 -0.00092757  0.13494011], action=0, reward=1.0, next_state=[-0.17692386 -0.59219811  0.00177124  0.42733026]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 42 ] state=[-0.17692386 -0.59219811  0.00177124  0.42733026], action=1, reward=1.0, next_state=[-0.18876782 -0.39710129  0.01031784  0.13520623]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 43 ] state=[-0.18876782 -0.39710129  0.01031784  0.13520623], action=1, reward=1.0, next_state=[-0.19670985 -0.20212864  0.01302197 -0.15420379]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 44 ] state=[-0.19670985 -0.20212864  0.01302197 -0.15420379], action=0, reward=1.0, next_state=[-0.20075242 -0.3974346   0.00993789  0.14255866]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 45 ] state=[-0.20075242 -0.3974346   0.00993789  0.14255866], action=1, reward=1.0, next_state=[-0.20870111 -0.20245637  0.01278906 -0.14697254]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 46 ] state=[-0.20870111 -0.20245637  0.01278906 -0.14697254], action=0, reward=1.0, next_state=[-0.21275024 -0.39775911  0.00984961  0.14971749]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 47 ] state=[-0.21275024 -0.39775911  0.00984961  0.14971749], action=1, reward=1.0, next_state=[-0.22070542 -0.20277958  0.01284396 -0.13984189]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 48 ] state=[-0.22070542 -0.20277958  0.01284396 -0.13984189], action=1, reward=1.0, next_state=[-0.22476101 -0.00784392  0.01004712 -0.42844524]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 49 ] state=[-0.22476101 -0.00784392  0.01004712 -0.42844524], action=0, reward=1.0, next_state=[-0.22491789 -0.20310671  0.00147822 -0.13261203]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 50 ] state=[-0.22491789 -0.20310671  0.00147822 -0.13261203], action=0, reward=1.0, next_state=[-0.22898002 -0.3982498  -0.00117402  0.16053689]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 51 ] state=[-0.22898002 -0.3982498  -0.00117402  0.16053689], action=0, reward=1.0, next_state=[-0.23694502 -0.59335493  0.00203672  0.45284921]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 52 ] state=[-0.23694502 -0.59335493  0.00203672  0.45284921], action=0, reward=1.0, next_state=[-0.24881212 -0.78850562  0.0110937   0.74617343]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 53 ] state=[-0.24881212 -0.78850562  0.0110937   0.74617343], action=1, reward=1.0, next_state=[-0.26458223 -0.59353848  0.02601717  0.45700221]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 54 ] state=[-0.26458223 -0.59353848  0.02601717  0.45700221], action=0, reward=1.0, next_state=[-0.276453   -0.78901841  0.03515721  0.75777105]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 55 ] state=[-0.276453   -0.78901841  0.03515721  0.75777105], action=1, reward=1.0, next_state=[-0.29223337 -0.59439815  0.05031263  0.47635515]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 56 ] state=[-0.29223337 -0.59439815  0.05031263  0.47635515], action=1, reward=1.0, next_state=[-0.30412133 -0.40002138  0.05983974  0.19994462]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 57 ] state=[-0.30412133 -0.40002138  0.05983974  0.19994462], action=0, reward=1.0, next_state=[-0.31212176 -0.59594587  0.06383863  0.51088772]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 58 ] state=[-0.31212176 -0.59594587  0.06383863  0.51088772], action=1, reward=1.0, next_state=[-0.32404068 -0.40177855  0.07405638  0.23898583]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 59 ] state=[-0.32404068 -0.40177855  0.07405638  0.23898583], action=0, reward=1.0, next_state=[-0.33207625 -0.59787603  0.0788361   0.55407849]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 60 ] state=[-0.33207625 -0.59787603  0.0788361   0.55407849], action=1, reward=1.0, next_state=[-0.34403377 -0.40394444  0.08991767  0.28723845]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 61 ] state=[-0.34403377 -0.40394444  0.08991767  0.28723845], action=1, reward=1.0, next_state=[-0.35211266 -0.21021218  0.09566244  0.02421422]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 62 ] state=[-0.35211266 -0.21021218  0.09566244  0.02421422], action=1, reward=1.0, next_state=[-0.3563169  -0.01658302  0.09614672 -0.23681969]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 63 ] state=[-0.3563169  -0.01658302  0.09614672 -0.23681969], action=0, reward=1.0, next_state=[-0.35664856 -0.21293771  0.09141033  0.08457565]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 64 ] state=[-0.35664856 -0.21293771  0.09141033  0.08457565], action=0, reward=1.0, next_state=[-0.36090732 -0.40924301  0.09310184  0.40464145]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 138 ][ timestamp 65 ] state=[-0.36090732 -0.40924301  0.09310184  0.40464145], action=1, reward=1.0, next_state=[-0.36909218 -0.21555621  0.10119467  0.14270191]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 66 ] state=[-0.36909218 -0.21555621  0.10119467  0.14270191], action=0, reward=1.0, next_state=[-0.3734033  -0.41197087  0.10404871  0.46551716]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 67 ] state=[-0.3734033  -0.41197087  0.10404871  0.46551716], action=1, reward=1.0, next_state=[-0.38164272 -0.21846117  0.11335905  0.20735756]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 68 ] state=[-0.38164272 -0.21846117  0.11335905  0.20735756], action=1, reward=1.0, next_state=[-0.38601194 -0.02512732  0.11750621 -0.04752568]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 69 ] state=[-0.38601194 -0.02512732  0.11750621 -0.04752568], action=1, reward=1.0, next_state=[-0.38651449  0.16813079  0.11655569 -0.30094643]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 70 ] state=[-0.38651449  0.16813079  0.11655569 -0.30094643], action=0, reward=1.0, next_state=[-0.38315187 -0.02844285  0.11053676  0.02610326]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 71 ] state=[-0.38315187 -0.02844285  0.11053676  0.02610326], action=1, reward=1.0, next_state=[-0.38372073  0.16493462  0.11105883 -0.22976101]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 72 ] state=[-0.38372073  0.16493462  0.11105883 -0.22976101], action=1, reward=1.0, next_state=[-0.38042204  0.35830879  0.10646361 -0.48545107]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 73 ] state=[-0.38042204  0.35830879  0.10646361 -0.48545107], action=0, reward=1.0, next_state=[-0.37325586  0.16185825  0.09675459 -0.16120248]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 74 ] state=[-0.37325586  0.16185825  0.09675459 -0.16120248], action=1, reward=1.0, next_state=[-0.37001869  0.35547149  0.09353054 -0.42186253]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 75 ] state=[-0.37001869  0.35547149  0.09353054 -0.42186253], action=1, reward=1.0, next_state=[-0.36290927  0.54915249  0.08509329 -0.68365632]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 76 ] state=[-0.36290927  0.54915249  0.08509329 -0.68365632], action=1, reward=1.0, next_state=[-0.35192622  0.74299626  0.07142016 -0.94838266]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 77 ] state=[-0.35192622  0.74299626  0.07142016 -0.94838266], action=1, reward=1.0, next_state=[-0.33706629  0.93708772  0.05245251 -1.21779797]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 78 ] state=[-0.33706629  0.93708772  0.05245251 -1.21779797], action=1, reward=1.0, next_state=[-0.31832454  1.13149556  0.02809655 -1.4935947 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 79 ] state=[-0.31832454  1.13149556  0.02809655 -1.4935947 ], action=0, reward=1.0, next_state=[-0.29569462  0.9360433  -0.00177535 -1.19227273]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 80 ] state=[-0.29569462  0.9360433  -0.00177535 -1.19227273], action=1, reward=1.0, next_state=[-0.27697376  1.13118821 -0.0256208  -1.48551158]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 81 ] state=[-0.27697376  1.13118821 -0.0256208  -1.48551158], action=0, reward=1.0, next_state=[-0.25434999  0.93638774 -0.05533103 -1.20093846]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 82 ] state=[-0.25434999  0.93638774 -0.05533103 -1.20093846], action=1, reward=1.0, next_state=[-0.23562224  1.13217997 -0.0793498  -1.51043638]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 83 ] state=[-0.23562224  1.13217997 -0.0793498  -1.51043638], action=0, reward=1.0, next_state=[-0.21297864  0.93810403 -0.10955853 -1.24354285]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 84 ] state=[-0.21297864  0.93810403 -0.10955853 -1.24354285], action=1, reward=1.0, next_state=[-0.19421656  1.13444796 -0.13442939 -1.56843877]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 85 ] state=[-0.19421656  1.13444796 -0.13442939 -1.56843877], action=0, reward=1.0, next_state=[-0.1715276   0.94116316 -0.16579816 -1.32053061]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 86 ] state=[-0.1715276   0.94116316 -0.16579816 -1.32053061], action=0, reward=1.0, next_state=[-0.15270434  0.74847891 -0.19220877 -1.08398932]\n",
      "[ Experience replay ] starts\n",
      "[ episode 138 ][ timestamp 87 ] state=[-0.15270434  0.74847891 -0.19220877 -1.08398932], action=0, reward=-1.0, next_state=[-0.13773476  0.55634016 -0.21388856 -0.85725071]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 138: Exploration_rate=0.01. Score=87.\n",
      "[ episode 139 ] state=[ 0.00947007 -0.04333272  0.04734578 -0.03950568]\n",
      "[ episode 139 ][ timestamp 1 ] state=[ 0.00947007 -0.04333272  0.04734578 -0.03950568], action=1, reward=1.0, next_state=[ 0.00860341  0.15107945  0.04655567 -0.31688268]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 2 ] state=[ 0.00860341  0.15107945  0.04655567 -0.31688268], action=0, reward=1.0, next_state=[ 0.011625   -0.04467364  0.04021801 -0.00988879]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 3 ] state=[ 0.011625   -0.04467364  0.04021801 -0.00988879], action=1, reward=1.0, next_state=[ 0.01073153  0.14984915  0.04002024 -0.28961611]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 4 ] state=[ 0.01073153  0.14984915  0.04002024 -0.28961611], action=0, reward=1.0, next_state=[ 0.01372851 -0.04581995  0.03422792  0.01541533]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 5 ] state=[ 0.01372851 -0.04581995  0.03422792  0.01541533], action=1, reward=1.0, next_state=[ 0.01281211  0.14879484  0.03453622 -0.26627482]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 6 ] state=[ 0.01281211  0.14879484  0.03453622 -0.26627482], action=0, reward=1.0, next_state=[ 0.01578801 -0.04680257  0.02921073  0.03709796]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 7 ] state=[ 0.01578801 -0.04680257  0.02921073  0.03709796], action=0, reward=1.0, next_state=[ 0.01485196 -0.24233096  0.02995269  0.33885216]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 8 ] state=[ 0.01485196 -0.24233096  0.02995269  0.33885216], action=1, reward=1.0, next_state=[ 0.01000534 -0.04764774  0.03672973  0.05576309]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 9 ] state=[ 0.01000534 -0.04764774  0.03672973  0.05576309], action=0, reward=1.0, next_state=[ 0.00905239 -0.24327656  0.03784499  0.35980452]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 10 ] state=[ 0.00905239 -0.24327656  0.03784499  0.35980452], action=0, reward=1.0, next_state=[ 0.00418685 -0.43891547  0.04504108  0.66417654]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 11 ] state=[ 0.00418685 -0.43891547  0.04504108  0.66417654], action=1, reward=1.0, next_state=[-0.00459146 -0.24444807  0.05832461  0.38600888]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 12 ] state=[-0.00459146 -0.24444807  0.05832461  0.38600888], action=1, reward=1.0, next_state=[-0.00948042 -0.05020051  0.06604479  0.11227069]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 13 ] state=[-0.00948042 -0.05020051  0.06604479  0.11227069], action=1, reward=1.0, next_state=[-0.01048443  0.14391596  0.0682902  -0.15886615]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 14 ] state=[-0.01048443  0.14391596  0.0682902  -0.15886615], action=1, reward=1.0, next_state=[-0.00760611  0.33799713  0.06511288 -0.42924762]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 15 ] state=[-0.00760611  0.33799713  0.06511288 -0.42924762], action=1, reward=1.0, next_state=[-0.00084617  0.53213946  0.05652793 -0.70071434]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 16 ] state=[-0.00084617  0.53213946  0.05652793 -0.70071434], action=0, reward=1.0, next_state=[ 0.00979662  0.33628135  0.04251364 -0.39078607]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 17 ] state=[ 0.00979662  0.33628135  0.04251364 -0.39078607], action=0, reward=1.0, next_state=[ 0.01652225  0.14058262  0.03469792 -0.08500798]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 18 ] state=[ 0.01652225  0.14058262  0.03469792 -0.08500798], action=0, reward=1.0, next_state=[ 0.0193339  -0.05501908  0.03299776  0.21841711]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 19 ] state=[ 0.0193339  -0.05501908  0.03299776  0.21841711], action=0, reward=1.0, next_state=[ 0.01823352 -0.25059681  0.0373661   0.52132359]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 20 ] state=[ 0.01823352 -0.25059681  0.0373661   0.52132359], action=1, reward=1.0, next_state=[ 0.01322159 -0.05602021  0.04779257  0.2406455 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 139 ][ timestamp 21 ] state=[ 0.01322159 -0.05602021  0.04779257  0.2406455 ], action=1, reward=1.0, next_state=[ 0.01210118  0.1383876   0.05260548 -0.03658758]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 22 ] state=[ 0.01210118  0.1383876   0.05260548 -0.03658758], action=1, reward=1.0, next_state=[ 0.01486893  0.33271726  0.05187373 -0.31221994]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 23 ] state=[ 0.01486893  0.33271726  0.05187373 -0.31221994], action=0, reward=1.0, next_state=[ 0.02152328  0.13689615  0.04562933 -0.00363935]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 24 ] state=[ 0.02152328  0.13689615  0.04562933 -0.00363935], action=1, reward=1.0, next_state=[ 0.0242612   0.33133501  0.04555655 -0.28158371]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 25 ] state=[ 0.0242612   0.33133501  0.04555655 -0.28158371], action=1, reward=1.0, next_state=[ 0.0308879   0.52577854  0.03992487 -0.5595574 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 26 ] state=[ 0.0308879   0.52577854  0.03992487 -0.5595574 ], action=0, reward=1.0, next_state=[ 0.04140347  0.33011961  0.02873372 -0.2545681 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 27 ] state=[ 0.04140347  0.33011961  0.02873372 -0.2545681 ], action=0, reward=1.0, next_state=[0.04800586 0.13459943 0.02364236 0.04703767]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 28 ] state=[0.04800586 0.13459943 0.02364236 0.04703767], action=0, reward=1.0, next_state=[ 0.05069785 -0.06085342  0.02458312  0.34708521]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 29 ] state=[ 0.05069785 -0.06085342  0.02458312  0.34708521], action=1, reward=1.0, next_state=[0.04948078 0.1339104  0.03152482 0.06225447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 30 ] state=[0.04948078 0.1339104  0.03152482 0.06225447], action=1, reward=1.0, next_state=[ 0.05215899  0.3285665   0.03276991 -0.22031785]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 31 ] state=[ 0.05215899  0.3285665   0.03276991 -0.22031785], action=1, reward=1.0, next_state=[ 0.05873032  0.52320509  0.02836355 -0.50248635]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 32 ] state=[ 0.05873032  0.52320509  0.02836355 -0.50248635], action=1, reward=1.0, next_state=[ 0.06919442  0.71791601  0.01831382 -0.78609749]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 33 ] state=[ 0.06919442  0.71791601  0.01831382 -0.78609749], action=0, reward=1.0, next_state=[ 0.08355274  0.5225473   0.00259188 -0.48770961]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 34 ] state=[ 0.08355274  0.5225473   0.00259188 -0.48770961], action=0, reward=1.0, next_state=[ 0.09400369  0.32738888 -0.00716232 -0.19421095]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 35 ] state=[ 0.09400369  0.32738888 -0.00716232 -0.19421095], action=0, reward=1.0, next_state=[ 0.10055147  0.13237011 -0.01104654  0.096204  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 36 ] state=[ 0.10055147  0.13237011 -0.01104654  0.096204  ], action=1, reward=1.0, next_state=[ 0.10319887  0.32764863 -0.00912246 -0.19994352]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 37 ] state=[ 0.10319887  0.32764863 -0.00912246 -0.19994352], action=0, reward=1.0, next_state=[ 0.10975184  0.13265833 -0.01312133  0.08984779]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 38 ] state=[ 0.10975184  0.13265833 -0.01312133  0.08984779], action=0, reward=1.0, next_state=[ 0.11240501 -0.06227311 -0.01132437  0.37836222]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 39 ] state=[ 0.11240501 -0.06227311 -0.01132437  0.37836222], action=1, reward=1.0, next_state=[ 0.11115955  0.13300782 -0.00375713  0.08213031]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 40 ] state=[ 0.11115955  0.13300782 -0.00375713  0.08213031], action=0, reward=1.0, next_state=[ 0.1138197  -0.06206007 -0.00211452  0.37362549]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 41 ] state=[ 0.1138197  -0.06206007 -0.00211452  0.37362549], action=1, reward=1.0, next_state=[0.1125785  0.13309185 0.00535799 0.08027659]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 42 ] state=[0.1125785  0.13309185 0.00535799 0.08027659], action=1, reward=1.0, next_state=[ 0.11524034  0.32813659  0.00696352 -0.21071107]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 43 ] state=[ 0.11524034  0.32813659  0.00696352 -0.21071107], action=1, reward=1.0, next_state=[ 0.12180307  0.52315829  0.0027493  -0.50118927]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 44 ] state=[ 0.12180307  0.52315829  0.0027493  -0.50118927], action=0, reward=1.0, next_state=[ 0.13226624  0.32799769 -0.00727449 -0.20764118]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 45 ] state=[ 0.13226624  0.32799769 -0.00727449 -0.20764118], action=1, reward=1.0, next_state=[ 0.13882619  0.5232229  -0.01142731 -0.50260993]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 46 ] state=[ 0.13882619  0.5232229  -0.01142731 -0.50260993], action=0, reward=1.0, next_state=[ 0.14929065  0.32826387 -0.02147951 -0.21355003]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 47 ] state=[ 0.14929065  0.32826387 -0.02147951 -0.21355003], action=0, reward=1.0, next_state=[ 0.15585593  0.13345549 -0.02575051  0.07228064]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 48 ] state=[ 0.15585593  0.13345549 -0.02575051  0.07228064], action=0, reward=1.0, next_state=[ 0.15852504 -0.06128801 -0.0243049   0.35672924]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 49 ] state=[ 0.15852504 -0.06128801 -0.0243049   0.35672924], action=1, reward=1.0, next_state=[ 0.15729928  0.13417091 -0.01717031  0.05648252]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 50 ] state=[ 0.15729928  0.13417091 -0.01717031  0.05648252], action=1, reward=1.0, next_state=[ 0.15998269  0.32953479 -0.01604066 -0.24156793]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 51 ] state=[ 0.15998269  0.32953479 -0.01604066 -0.24156793], action=1, reward=1.0, next_state=[ 0.16657339  0.52488216 -0.02087202 -0.53926704]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 52 ] state=[ 0.16657339  0.52488216 -0.02087202 -0.53926704], action=0, reward=1.0, next_state=[ 0.17707103  0.33005974 -0.03165736 -0.253233  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 53 ] state=[ 0.17707103  0.33005974 -0.03165736 -0.253233  ], action=0, reward=1.0, next_state=[ 0.18367223  0.13540379 -0.03672202  0.02929892]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 54 ] state=[ 0.18367223  0.13540379 -0.03672202  0.02929892], action=0, reward=1.0, next_state=[ 0.1863803  -0.05917283 -0.03613604  0.31017323]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 55 ] state=[ 0.1863803  -0.05917283 -0.03613604  0.31017323], action=0, reward=1.0, next_state=[ 0.18519685 -0.25376179 -0.02993258  0.59124444]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 56 ] state=[ 0.18519685 -0.25376179 -0.02993258  0.59124444], action=1, reward=1.0, next_state=[ 0.18012161 -0.05823384 -0.01810769  0.28928503]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 57 ] state=[ 0.18012161 -0.05823384 -0.01810769  0.28928503], action=1, reward=1.0, next_state=[ 0.17895693  0.13714158 -0.01232199 -0.00905342]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 58 ] state=[ 0.17895693  0.13714158 -0.01232199 -0.00905342], action=1, reward=1.0, next_state=[ 0.18169977  0.33243805 -0.01250306 -0.30559847]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 59 ] state=[ 0.18169977  0.33243805 -0.01250306 -0.30559847], action=1, reward=1.0, next_state=[ 0.18834853  0.52773593 -0.01861503 -0.60219819]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 60 ] state=[ 0.18834853  0.52773593 -0.01861503 -0.60219819], action=0, reward=1.0, next_state=[ 0.19890325  0.33287923 -0.03065899 -0.31543629]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 61 ] state=[ 0.19890325  0.33287923 -0.03065899 -0.31543629], action=0, reward=1.0, next_state=[ 0.20556083  0.13820711 -0.03696771 -0.03257767]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 62 ] state=[ 0.20556083  0.13820711 -0.03696771 -0.03257767], action=0, reward=1.0, next_state=[ 0.20832497 -0.05636574 -0.03761927  0.24821617]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 63 ] state=[ 0.20832497 -0.05636574 -0.03761927  0.24821617], action=0, reward=1.0, next_state=[ 0.20719766 -0.25093081 -0.03265494  0.52879983]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 64 ] state=[ 0.20719766 -0.25093081 -0.03265494  0.52879983], action=0, reward=1.0, next_state=[ 0.20217904 -0.4455785  -0.02207895  0.81101686]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 65 ] state=[ 0.20217904 -0.4455785  -0.02207895  0.81101686], action=1, reward=1.0, next_state=[ 0.19326747 -0.25016114 -0.00585861  0.51147159]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 66 ] state=[ 0.19326747 -0.25016114 -0.00585861  0.51147159], action=1, reward=1.0, next_state=[ 0.18826425 -0.05495716  0.00437082  0.21694821]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 67 ] state=[ 0.18826425 -0.05495716  0.00437082  0.21694821], action=1, reward=1.0, next_state=[ 0.18716511  0.14010204  0.00870979 -0.07435277]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 139 ][ timestamp 68 ] state=[ 0.18716511  0.14010204  0.00870979 -0.07435277], action=0, reward=1.0, next_state=[ 0.18996715 -0.05514369  0.00722273  0.22106535]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 69 ] state=[ 0.18996715 -0.05514369  0.00722273  0.22106535], action=1, reward=1.0, next_state=[ 0.18886427  0.13987428  0.01164404 -0.06933051]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 70 ] state=[ 0.18886427  0.13987428  0.01164404 -0.06933051], action=0, reward=1.0, next_state=[ 0.19166176 -0.05541265  0.01025743  0.22700331]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 71 ] state=[ 0.19166176 -0.05541265  0.01025743  0.22700331], action=0, reward=1.0, next_state=[ 0.1905535  -0.25067968  0.01479749  0.52290407]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 72 ] state=[ 0.1905535  -0.25067968  0.01479749  0.52290407], action=1, reward=1.0, next_state=[ 0.18553991 -0.05576909  0.02525557  0.2349205 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 73 ] state=[ 0.18553991 -0.05576909  0.02525557  0.2349205 ], action=0, reward=1.0, next_state=[ 0.18442453 -0.25124261  0.02995398  0.53546162]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 74 ] state=[ 0.18442453 -0.25124261  0.02995398  0.53546162], action=1, reward=1.0, next_state=[ 0.17939968 -0.05655439  0.04066322  0.25236544]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 75 ] state=[ 0.17939968 -0.05655439  0.04066322  0.25236544], action=0, reward=1.0, next_state=[ 0.17826859 -0.25223268  0.04571053  0.55759194]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 76 ] state=[ 0.17826859 -0.25223268  0.04571053  0.55759194], action=1, reward=1.0, next_state=[ 0.17322394 -0.05778122  0.05686236  0.27965363]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 77 ] state=[ 0.17322394 -0.05778122  0.05686236  0.27965363], action=1, reward=1.0, next_state=[0.17206831 0.13648543 0.06245544 0.00543317]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 78 ] state=[0.17206831 0.13648543 0.06245544 0.00543317], action=1, reward=1.0, next_state=[ 0.17479802  0.33065866  0.0625641  -0.26690884]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 79 ] state=[ 0.17479802  0.33065866  0.0625641  -0.26690884], action=1, reward=1.0, next_state=[ 0.18141119  0.52483449  0.05722592 -0.53922089]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 80 ] state=[ 0.18141119  0.52483449  0.05722592 -0.53922089], action=1, reward=1.0, next_state=[ 0.19190788  0.71910725  0.04644151 -0.81333777]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 81 ] state=[ 0.19190788  0.71910725  0.04644151 -0.81333777], action=1, reward=1.0, next_state=[ 0.20629003  0.91356342  0.03017475 -1.09105862]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 82 ] state=[ 0.20629003  0.91356342  0.03017475 -1.09105862], action=1, reward=1.0, next_state=[ 0.2245613   1.10827495  0.00835358 -1.37412294]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 83 ] state=[ 0.2245613   1.10827495  0.00835358 -1.37412294], action=1, reward=1.0, next_state=[ 0.2467268   1.3032915  -0.01912888 -1.66418163]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 84 ] state=[ 0.2467268   1.3032915  -0.01912888 -1.66418163], action=1, reward=1.0, next_state=[ 0.27279263  1.49863081 -0.05241251 -1.96276054]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 85 ] state=[ 0.27279263  1.49863081 -0.05241251 -1.96276054], action=1, reward=1.0, next_state=[ 0.30276524  1.69426687 -0.09166772 -2.27121387]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 86 ] state=[ 0.30276524  1.69426687 -0.09166772 -2.27121387], action=1, reward=1.0, next_state=[ 0.33665058  1.89011535 -0.137092   -2.59066577]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 87 ] state=[ 0.33665058  1.89011535 -0.137092   -2.59066577], action=1, reward=1.0, next_state=[ 0.37445289  2.0860163  -0.18890532 -2.92193907]\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ][ timestamp 88 ] state=[ 0.37445289  2.0860163  -0.18890532 -2.92193907], action=1, reward=-1.0, next_state=[ 0.41617321  2.28171435 -0.2473441  -3.26547249]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 139: Exploration_rate=0.01. Score=88.\n",
      "[ episode 140 ] state=[-0.04265227 -0.04900602 -0.02977544 -0.02468383]\n",
      "[ episode 140 ][ timestamp 1 ] state=[-0.04265227 -0.04900602 -0.02977544 -0.02468383], action=1, reward=1.0, next_state=[-0.04363239  0.14653001 -0.03026911 -0.32661054]\n",
      "[ Experience replay ] starts\n",
      "[ episode 140 ][ timestamp 2 ] state=[-0.04363239  0.14653001 -0.03026911 -0.32661054], action=1, reward=1.0, next_state=[-0.04070179  0.34206954 -0.03680132 -0.62868325]\n",
      "[ Experience replay ] starts\n",
      "[ episode 140 ][ timestamp 3 ] state=[-0.04070179  0.34206954 -0.03680132 -0.62868325], action=1, reward=1.0, next_state=[-0.0338604   0.53768524 -0.04937499 -0.93272527]\n",
      "[ Experience replay ] starts\n",
      "[ episode 140 ][ timestamp 4 ] state=[-0.0338604   0.53768524 -0.04937499 -0.93272527], action=1, reward=1.0, next_state=[-0.02310669  0.73343737 -0.0680295  -1.24050597]\n",
      "[ Experience replay ] starts\n",
      "[ episode 140 ][ timestamp 5 ] state=[-0.02310669  0.73343737 -0.0680295  -1.24050597], action=1, reward=1.0, next_state=[-0.00843795  0.92936365 -0.09283961 -1.55370084]\n",
      "[ Experience replay ] starts\n",
      "[ episode 140 ][ timestamp 6 ] state=[-0.00843795  0.92936365 -0.09283961 -1.55370084], action=1, reward=1.0, next_state=[ 0.01014933  1.12546777 -0.12391363 -1.87384588]\n",
      "[ Experience replay ] starts\n",
      "[ episode 140 ][ timestamp 7 ] state=[ 0.01014933  1.12546777 -0.12391363 -1.87384588], action=1, reward=1.0, next_state=[ 0.03265868  1.32170594 -0.16139055 -2.20228361]\n",
      "[ Experience replay ] starts\n",
      "[ episode 140 ][ timestamp 8 ] state=[ 0.03265868  1.32170594 -0.16139055 -2.20228361], action=1, reward=1.0, next_state=[ 0.0590928   1.51797131 -0.20543622 -2.54009901]\n",
      "[ Experience replay ] starts\n",
      "[ episode 140 ][ timestamp 9 ] state=[ 0.0590928   1.51797131 -0.20543622 -2.54009901], action=1, reward=-1.0, next_state=[ 0.08945223  1.71407611 -0.2562382  -2.88804501]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 140: Exploration_rate=0.01. Score=9.\n",
      "[ episode 141 ] state=[-0.04081025  0.04661038  0.00893141  0.03985192]\n",
      "[ episode 141 ][ timestamp 1 ] state=[-0.04081025  0.04661038  0.00893141  0.03985192], action=1, reward=1.0, next_state=[-0.03987804  0.24160312  0.00972845 -0.24999973]\n",
      "[ Experience replay ] starts\n",
      "[ episode 141 ][ timestamp 2 ] state=[-0.03987804  0.24160312  0.00972845 -0.24999973], action=1, reward=1.0, next_state=[-0.03504598  0.43658481  0.00472845 -0.5395983 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 141 ][ timestamp 3 ] state=[-0.03504598  0.43658481  0.00472845 -0.5395983 ], action=1, reward=1.0, next_state=[-0.02631428  0.63163997 -0.00606351 -0.83078761]\n",
      "[ Experience replay ] starts\n",
      "[ episode 141 ][ timestamp 4 ] state=[-0.02631428  0.63163997 -0.00606351 -0.83078761], action=1, reward=1.0, next_state=[-0.01368148  0.82684427 -0.02267926 -1.12537134]\n",
      "[ Experience replay ] starts\n",
      "[ episode 141 ][ timestamp 5 ] state=[-0.01368148  0.82684427 -0.02267926 -1.12537134], action=1, reward=1.0, next_state=[ 0.0028554   1.02225599 -0.04518669 -1.42508068]\n",
      "[ Experience replay ] starts\n",
      "[ episode 141 ][ timestamp 6 ] state=[ 0.0028554   1.02225599 -0.04518669 -1.42508068], action=1, reward=1.0, next_state=[ 0.02330052  1.21790639 -0.0736883  -1.73153708]\n",
      "[ Experience replay ] starts\n",
      "[ episode 141 ][ timestamp 7 ] state=[ 0.02330052  1.21790639 -0.0736883  -1.73153708], action=1, reward=1.0, next_state=[ 0.04765865  1.41378829 -0.10831905 -2.04620732]\n",
      "[ Experience replay ] starts\n",
      "[ episode 141 ][ timestamp 8 ] state=[ 0.04765865  1.41378829 -0.10831905 -2.04620732], action=1, reward=1.0, next_state=[ 0.07593442  1.60984231 -0.14924319 -2.37034837]\n",
      "[ Experience replay ] starts\n",
      "[ episode 141 ][ timestamp 9 ] state=[ 0.07593442  1.60984231 -0.14924319 -2.37034837], action=1, reward=1.0, next_state=[ 0.10813126  1.80594071 -0.19665016 -2.70494099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 141 ][ timestamp 10 ] state=[ 0.10813126  1.80594071 -0.19665016 -2.70494099], action=1, reward=-1.0, next_state=[ 0.14425008  2.00186877 -0.25074898 -3.05061201]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 141: Exploration_rate=0.01. Score=10.\n",
      "[ episode 142 ] state=[0.03021777 0.03114787 0.02552391 0.00792217]\n",
      "[ episode 142 ][ timestamp 1 ] state=[0.03021777 0.03114787 0.02552391 0.00792217], action=1, reward=1.0, next_state=[ 0.03084072  0.22589465  0.02568236 -0.27659964]\n",
      "[ Experience replay ] starts\n",
      "[ episode 142 ][ timestamp 2 ] state=[ 0.03084072  0.22589465  0.02568236 -0.27659964], action=1, reward=1.0, next_state=[ 0.03535862  0.42064097  0.02015036 -0.56107299]\n",
      "[ Experience replay ] starts\n",
      "[ episode 142 ][ timestamp 3 ] state=[ 0.03535862  0.42064097  0.02015036 -0.56107299], action=1, reward=1.0, next_state=[ 0.04377144  0.61547441  0.0089289  -0.84734002]\n",
      "[ Experience replay ] starts\n",
      "[ episode 142 ][ timestamp 4 ] state=[ 0.04377144  0.61547441  0.0089289  -0.84734002], action=1, reward=1.0, next_state=[ 0.05608092  0.81047343 -0.0080179  -1.13720183]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 142 ][ timestamp 5 ] state=[ 0.05608092  0.81047343 -0.0080179  -1.13720183], action=1, reward=1.0, next_state=[ 0.07229039  1.00569933 -0.03076193 -1.4323885 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 142 ][ timestamp 6 ] state=[ 0.07229039  1.00569933 -0.03076193 -1.4323885 ], action=1, reward=1.0, next_state=[ 0.09240438  1.20118708 -0.0594097  -1.73452397]\n",
      "[ Experience replay ] starts\n",
      "[ episode 142 ][ timestamp 7 ] state=[ 0.09240438  1.20118708 -0.0594097  -1.73452397], action=1, reward=1.0, next_state=[ 0.11642812  1.39693428 -0.09410018 -2.04508294]\n",
      "[ Experience replay ] starts\n",
      "[ episode 142 ][ timestamp 8 ] state=[ 0.11642812  1.39693428 -0.09410018 -2.04508294], action=1, reward=1.0, next_state=[ 0.14436681  1.59288783 -0.13500184 -2.36533751]\n",
      "[ Experience replay ] starts\n",
      "[ episode 142 ][ timestamp 9 ] state=[ 0.14436681  1.59288783 -0.13500184 -2.36533751], action=1, reward=1.0, next_state=[ 0.17622456  1.78892796 -0.18230859 -2.69629217]\n",
      "[ Experience replay ] starts\n",
      "[ episode 142 ][ timestamp 10 ] state=[ 0.17622456  1.78892796 -0.18230859 -2.69629217], action=1, reward=-1.0, next_state=[ 0.21200312  1.98484982 -0.23623443 -3.03860697]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 142: Exploration_rate=0.01. Score=10.\n",
      "[ episode 143 ] state=[-0.02479487  0.00178177  0.04925159 -0.02043435]\n",
      "[ episode 143 ][ timestamp 1 ] state=[-0.02479487  0.00178177  0.04925159 -0.02043435], action=1, reward=1.0, next_state=[-0.02475923  0.19616407  0.04884291 -0.29718011]\n",
      "[ Experience replay ] starts\n",
      "[ episode 143 ][ timestamp 2 ] state=[-0.02475923  0.19616407  0.04884291 -0.29718011], action=1, reward=1.0, next_state=[-0.02083595  0.39055696  0.04289931 -0.57406759]\n",
      "[ Experience replay ] starts\n",
      "[ episode 143 ][ timestamp 3 ] state=[-0.02083595  0.39055696  0.04289931 -0.57406759], action=1, reward=1.0, next_state=[-0.01302481  0.58505204  0.03141795 -0.85293327]\n",
      "[ Experience replay ] starts\n",
      "[ episode 143 ][ timestamp 4 ] state=[-0.01302481  0.58505204  0.03141795 -0.85293327], action=1, reward=1.0, next_state=[-0.00132377  0.77973194  0.01435929 -1.13557366]\n",
      "[ Experience replay ] starts\n",
      "[ episode 143 ][ timestamp 5 ] state=[-0.00132377  0.77973194  0.01435929 -1.13557366], action=1, reward=1.0, next_state=[ 0.01427087  0.97466311 -0.00835219 -1.42371879]\n",
      "[ Experience replay ] starts\n",
      "[ episode 143 ][ timestamp 6 ] state=[ 0.01427087  0.97466311 -0.00835219 -1.42371879], action=1, reward=1.0, next_state=[ 0.03376413  1.16988733 -0.03682656 -1.71900041]\n",
      "[ Experience replay ] starts\n",
      "[ episode 143 ][ timestamp 7 ] state=[ 0.03376413  1.16988733 -0.03682656 -1.71900041], action=1, reward=1.0, next_state=[ 0.05716188  1.36541141 -0.07120657 -2.02291225]\n",
      "[ Experience replay ] starts\n",
      "[ episode 143 ][ timestamp 8 ] state=[ 0.05716188  1.36541141 -0.07120657 -2.02291225], action=1, reward=1.0, next_state=[ 0.08447011  1.56119457 -0.11166481 -2.33675981]\n",
      "[ Experience replay ] starts\n",
      "[ episode 143 ][ timestamp 9 ] state=[ 0.08447011  1.56119457 -0.11166481 -2.33675981], action=1, reward=1.0, next_state=[ 0.115694    1.75713302 -0.15840001 -2.66159829]\n",
      "[ Experience replay ] starts\n",
      "[ episode 143 ][ timestamp 10 ] state=[ 0.115694    1.75713302 -0.15840001 -2.66159829], action=1, reward=-1.0, next_state=[ 0.15083666  1.953042   -0.21163198 -2.99815797]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 143: Exploration_rate=0.01. Score=10.\n",
      "[ episode 144 ] state=[-0.00317782  0.00122219 -0.00782214  0.02878663]\n",
      "[ episode 144 ][ timestamp 1 ] state=[-0.00317782  0.00122219 -0.00782214  0.02878663], action=1, reward=1.0, next_state=[-0.00315337  0.19645543 -0.00724641 -0.26635397]\n",
      "[ Experience replay ] starts\n",
      "[ episode 144 ][ timestamp 2 ] state=[-0.00315337  0.19645543 -0.00724641 -0.26635397], action=1, reward=1.0, next_state=[ 0.00077574  0.39168005 -0.01257349 -0.56131364]\n",
      "[ Experience replay ] starts\n",
      "[ episode 144 ][ timestamp 3 ] state=[ 0.00077574  0.39168005 -0.01257349 -0.56131364], action=1, reward=1.0, next_state=[ 0.00860934  0.58697618 -0.02379976 -0.85793119]\n",
      "[ Experience replay ] starts\n",
      "[ episode 144 ][ timestamp 4 ] state=[ 0.00860934  0.58697618 -0.02379976 -0.85793119], action=1, reward=1.0, next_state=[ 0.02034886  0.78241414 -0.04095838 -1.15800157]\n",
      "[ Experience replay ] starts\n",
      "[ episode 144 ][ timestamp 5 ] state=[ 0.02034886  0.78241414 -0.04095838 -1.15800157], action=1, reward=1.0, next_state=[ 0.03599714  0.97804527 -0.06411841 -1.46324055]\n",
      "[ Experience replay ] starts\n",
      "[ episode 144 ][ timestamp 6 ] state=[ 0.03599714  0.97804527 -0.06411841 -1.46324055], action=1, reward=1.0, next_state=[ 0.05555805  1.17389158 -0.09338323 -1.77524426]\n",
      "[ Experience replay ] starts\n",
      "[ episode 144 ][ timestamp 7 ] state=[ 0.05555805  1.17389158 -0.09338323 -1.77524426], action=1, reward=1.0, next_state=[ 0.07903588  1.36993358 -0.12888811 -2.0954408 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 144 ][ timestamp 8 ] state=[ 0.07903588  1.36993358 -0.12888811 -2.0954408 ], action=1, reward=1.0, next_state=[ 0.10643455  1.56609585 -0.17079693 -2.42503185]\n",
      "[ Experience replay ] starts\n",
      "[ episode 144 ][ timestamp 9 ] state=[ 0.10643455  1.56609585 -0.17079693 -2.42503185], action=1, reward=-1.0, next_state=[ 0.13775647  1.76223016 -0.21929756 -2.76492309]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 144: Exploration_rate=0.01. Score=9.\n",
      "[ episode 145 ] state=[-0.02055042 -0.03767578 -0.02357646  0.03811718]\n",
      "[ episode 145 ][ timestamp 1 ] state=[-0.02055042 -0.03767578 -0.02357646  0.03811718], action=1, reward=1.0, next_state=[-0.02130393  0.15777618 -0.02281412 -0.26191013]\n",
      "[ Experience replay ] starts\n",
      "[ episode 145 ][ timestamp 2 ] state=[-0.02130393  0.15777618 -0.02281412 -0.26191013], action=1, reward=1.0, next_state=[-0.01814841  0.35321624 -0.02805232 -0.56170069]\n",
      "[ Experience replay ] starts\n",
      "[ episode 145 ][ timestamp 3 ] state=[-0.01814841  0.35321624 -0.02805232 -0.56170069], action=1, reward=1.0, next_state=[-0.01108408  0.5487204  -0.03928634 -0.86308786]\n",
      "[ Experience replay ] starts\n",
      "[ episode 145 ][ timestamp 4 ] state=[-0.01108408  0.5487204  -0.03928634 -0.86308786], action=1, reward=1.0, next_state=[-1.09674535e-04  7.44354573e-01 -5.65480944e-02 -1.16785990e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 145 ][ timestamp 5 ] state=[-1.09674535e-04  7.44354573e-01 -5.65480944e-02 -1.16785990e+00], action=1, reward=1.0, next_state=[ 0.01477742  0.94016481 -0.07990529 -1.47772205]\n",
      "[ Experience replay ] starts\n",
      "[ episode 145 ][ timestamp 6 ] state=[ 0.01477742  0.94016481 -0.07990529 -1.47772205], action=1, reward=1.0, next_state=[ 0.03358071  1.13616642 -0.10945973 -1.79425356]\n",
      "[ Experience replay ] starts\n",
      "[ episode 145 ][ timestamp 7 ] state=[ 0.03358071  1.13616642 -0.10945973 -1.79425356], action=1, reward=1.0, next_state=[ 0.05630404  1.33233128 -0.1453448  -2.11885679]\n",
      "[ Experience replay ] starts\n",
      "[ episode 145 ][ timestamp 8 ] state=[ 0.05630404  1.33233128 -0.1453448  -2.11885679], action=1, reward=1.0, next_state=[ 0.08295067  1.52857287 -0.18772194 -2.4526965 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 145 ][ timestamp 9 ] state=[ 0.08295067  1.52857287 -0.18772194 -2.4526965 ], action=1, reward=-1.0, next_state=[ 0.11352212  1.72472907 -0.23677587 -2.79662834]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 145: Exploration_rate=0.01. Score=9.\n",
      "[ episode 146 ] state=[-0.03656316  0.02316451  0.01615401 -0.0059303 ]\n",
      "[ episode 146 ][ timestamp 1 ] state=[-0.03656316  0.02316451  0.01615401 -0.0059303 ], action=1, reward=1.0, next_state=[-0.03609987  0.21805111  0.0160354  -0.29347299]\n",
      "[ Experience replay ] starts\n",
      "[ episode 146 ][ timestamp 2 ] state=[-0.03609987  0.21805111  0.0160354  -0.29347299], action=1, reward=1.0, next_state=[-0.03173884  0.41294081  0.01016594 -0.58105575]\n",
      "[ Experience replay ] starts\n",
      "[ episode 146 ][ timestamp 3 ] state=[-0.03173884  0.41294081  0.01016594 -0.58105575], action=1, reward=1.0, next_state=[-0.02348003  0.60791885 -0.00145517 -0.87051896]\n",
      "[ Experience replay ] starts\n",
      "[ episode 146 ][ timestamp 4 ] state=[-0.02348003  0.60791885 -0.00145517 -0.87051896], action=1, reward=1.0, next_state=[-0.01132165  0.80306057 -0.01886555 -1.16365904]\n",
      "[ Experience replay ] starts\n",
      "[ episode 146 ][ timestamp 5 ] state=[-0.01132165  0.80306057 -0.01886555 -1.16365904], action=1, reward=1.0, next_state=[ 0.00473956  0.99842301 -0.04213873 -1.4621967 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 146 ][ timestamp 6 ] state=[ 0.00473956  0.99842301 -0.04213873 -1.4621967 ], action=1, reward=1.0, next_state=[ 0.02470802  1.1940353  -0.07138267 -1.7677398 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 146 ][ timestamp 7 ] state=[ 0.02470802  1.1940353  -0.07138267 -1.7677398 ], action=1, reward=1.0, next_state=[ 0.04858873  1.38988718 -0.10673746 -2.08173814]\n",
      "[ Experience replay ] starts\n",
      "[ episode 146 ][ timestamp 8 ] state=[ 0.04858873  1.38988718 -0.10673746 -2.08173814], action=1, reward=1.0, next_state=[ 0.07638647  1.58591508 -0.14837223 -2.40542786]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 146 ][ timestamp 9 ] state=[ 0.07638647  1.58591508 -0.14837223 -2.40542786], action=1, reward=1.0, next_state=[ 0.10810477  1.78198581 -0.19648078 -2.73976417]\n",
      "[ Experience replay ] starts\n",
      "[ episode 146 ][ timestamp 10 ] state=[ 0.10810477  1.78198581 -0.19648078 -2.73976417], action=1, reward=-1.0, next_state=[ 0.14374449  1.97787773 -0.25127607 -3.08534291]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 146: Exploration_rate=0.01. Score=10.\n",
      "[ episode 147 ] state=[ 0.00292609 -0.02939866  0.02254782  0.0265651 ]\n",
      "[ episode 147 ][ timestamp 1 ] state=[ 0.00292609 -0.02939866  0.02254782  0.0265651 ], action=1, reward=1.0, next_state=[ 0.00233811  0.1653928   0.02307912 -0.25891933]\n",
      "[ Experience replay ] starts\n",
      "[ episode 147 ][ timestamp 2 ] state=[ 0.00233811  0.1653928   0.02307912 -0.25891933], action=1, reward=1.0, next_state=[ 0.00564597  0.3601778   0.01790074 -0.54423436]\n",
      "[ Experience replay ] starts\n",
      "[ episode 147 ][ timestamp 3 ] state=[ 0.00564597  0.3601778   0.01790074 -0.54423436], action=1, reward=1.0, next_state=[ 0.01284952  0.55504369  0.00701605 -0.83122382]\n",
      "[ Experience replay ] starts\n",
      "[ episode 147 ][ timestamp 4 ] state=[ 0.01284952  0.55504369  0.00701605 -0.83122382], action=1, reward=1.0, next_state=[ 0.0239504   0.75006905 -0.00960843 -1.12169196]\n",
      "[ Experience replay ] starts\n",
      "[ episode 147 ][ timestamp 5 ] state=[ 0.0239504   0.75006905 -0.00960843 -1.12169196], action=1, reward=1.0, next_state=[ 0.03895178  0.94531568 -0.03204227 -1.41737322]\n",
      "[ Experience replay ] starts\n",
      "[ episode 147 ][ timestamp 6 ] state=[ 0.03895178  0.94531568 -0.03204227 -1.41737322], action=1, reward=1.0, next_state=[ 0.05785809  1.14081937 -0.06038973 -1.71989705]\n",
      "[ Experience replay ] starts\n",
      "[ episode 147 ][ timestamp 7 ] state=[ 0.05785809  1.14081937 -0.06038973 -1.71989705], action=1, reward=1.0, next_state=[ 0.08067448  1.33657895 -0.09478767 -2.03074493]\n",
      "[ Experience replay ] starts\n",
      "[ episode 147 ][ timestamp 8 ] state=[ 0.08067448  1.33657895 -0.09478767 -2.03074493], action=1, reward=1.0, next_state=[ 0.10740606  1.53254293 -0.13540257 -2.35119725]\n",
      "[ Experience replay ] starts\n",
      "[ episode 147 ][ timestamp 9 ] state=[ 0.10740606  1.53254293 -0.13540257 -2.35119725], action=1, reward=1.0, next_state=[ 0.13805692  1.72859367 -0.18242652 -2.68226852]\n",
      "[ Experience replay ] starts\n",
      "[ episode 147 ][ timestamp 10 ] state=[ 0.13805692  1.72859367 -0.18242652 -2.68226852], action=1, reward=-1.0, next_state=[ 0.17262879  1.924529   -0.23607189 -3.02463101]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 147: Exploration_rate=0.01. Score=10.\n",
      "[ episode 148 ] state=[ 0.02102205 -0.02751876 -0.03897069 -0.03222045]\n",
      "[ episode 148 ][ timestamp 1 ] state=[ 0.02102205 -0.02751876 -0.03897069 -0.03222045], action=1, reward=1.0, next_state=[ 0.02047168  0.16813975 -0.0396151  -0.33693987]\n",
      "[ Experience replay ] starts\n",
      "[ episode 148 ][ timestamp 2 ] state=[ 0.02047168  0.16813975 -0.0396151  -0.33693987], action=1, reward=1.0, next_state=[ 0.02383447  0.3638024  -0.0463539  -0.64184737]\n",
      "[ Experience replay ] starts\n",
      "[ episode 148 ][ timestamp 3 ] state=[ 0.02383447  0.3638024  -0.0463539  -0.64184737], action=1, reward=1.0, next_state=[ 0.03111052  0.55953881 -0.05919084 -0.94875977]\n",
      "[ Experience replay ] starts\n",
      "[ episode 148 ][ timestamp 4 ] state=[ 0.03111052  0.55953881 -0.05919084 -0.94875977], action=1, reward=1.0, next_state=[ 0.0423013   0.75540556 -0.07816604 -1.25943732]\n",
      "[ Experience replay ] starts\n",
      "[ episode 148 ][ timestamp 5 ] state=[ 0.0423013   0.75540556 -0.07816604 -1.25943732], action=1, reward=1.0, next_state=[ 0.05740941  0.95143567 -0.10335478 -1.57554206]\n",
      "[ Experience replay ] starts\n",
      "[ episode 148 ][ timestamp 6 ] state=[ 0.05740941  0.95143567 -0.10335478 -1.57554206], action=1, reward=1.0, next_state=[ 0.07643812  1.14762668 -0.13486563 -1.89859041]\n",
      "[ Experience replay ] starts\n",
      "[ episode 148 ][ timestamp 7 ] state=[ 0.07643812  1.14762668 -0.13486563 -1.89859041], action=1, reward=1.0, next_state=[ 0.09939065  1.34392691 -0.17283743 -2.22989738]\n",
      "[ Experience replay ] starts\n",
      "[ episode 148 ][ timestamp 8 ] state=[ 0.09939065  1.34392691 -0.17283743 -2.22989738], action=1, reward=-1.0, next_state=[ 0.12626919  1.54021944 -0.21743538 -2.57051084]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 148: Exploration_rate=0.01. Score=8.\n",
      "[ episode 149 ] state=[ 0.00411082 -0.00211621  0.01991598  0.00631159]\n",
      "[ episode 149 ][ timestamp 1 ] state=[ 0.00411082 -0.00211621  0.01991598  0.00631159], action=1, reward=1.0, next_state=[ 0.0040685   0.19271454  0.02004221 -0.28002167]\n",
      "[ Experience replay ] starts\n",
      "[ episode 149 ][ timestamp 2 ] state=[ 0.0040685   0.19271454  0.02004221 -0.28002167], action=1, reward=1.0, next_state=[ 0.00792279  0.38754494  0.01444177 -0.56631656]\n",
      "[ Experience replay ] starts\n",
      "[ episode 149 ][ timestamp 3 ] state=[ 0.00792279  0.38754494  0.01444177 -0.56631656], action=1, reward=1.0, next_state=[ 0.01567369  0.58246135  0.00311544 -0.85441495]\n",
      "[ Experience replay ] starts\n",
      "[ episode 149 ][ timestamp 4 ] state=[ 0.01567369  0.58246135  0.00311544 -0.85441495], action=1, reward=1.0, next_state=[ 0.02732291  0.7775407  -0.01397286 -1.14611662]\n",
      "[ Experience replay ] starts\n",
      "[ episode 149 ][ timestamp 5 ] state=[ 0.02732291  0.7775407  -0.01397286 -1.14611662], action=1, reward=1.0, next_state=[ 0.04287373  0.97284233 -0.03689519 -1.44314834]\n",
      "[ Experience replay ] starts\n",
      "[ episode 149 ][ timestamp 6 ] state=[ 0.04287373  0.97284233 -0.03689519 -1.44314834], action=1, reward=1.0, next_state=[ 0.06233057  1.16839851 -0.06575816 -1.74712771]\n",
      "[ Experience replay ] starts\n",
      "[ episode 149 ][ timestamp 7 ] state=[ 0.06233057  1.16839851 -0.06575816 -1.74712771], action=1, reward=1.0, next_state=[ 0.08569854  1.36420326 -0.10070071 -2.05951902]\n",
      "[ Experience replay ] starts\n",
      "[ episode 149 ][ timestamp 8 ] state=[ 0.08569854  1.36420326 -0.10070071 -2.05951902], action=1, reward=1.0, next_state=[ 0.11298261  1.56019871 -0.14189109 -2.38157882]\n",
      "[ Experience replay ] starts\n",
      "[ episode 149 ][ timestamp 9 ] state=[ 0.11298261  1.56019871 -0.14189109 -2.38157882], action=1, reward=1.0, next_state=[ 0.14418658  1.75625896 -0.18952267 -2.71428982]\n",
      "[ Experience replay ] starts\n",
      "[ episode 149 ][ timestamp 10 ] state=[ 0.14418658  1.75625896 -0.18952267 -2.71428982], action=1, reward=-1.0, next_state=[ 0.17931176  1.95217147 -0.24380846 -3.05828336]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 149: Exploration_rate=0.01. Score=10.\n",
      "[ episode 150 ] state=[-0.00022005  0.0277343   0.00207141  0.00244835]\n",
      "[ episode 150 ][ timestamp 1 ] state=[-0.00022005  0.0277343   0.00207141  0.00244835], action=1, reward=1.0, next_state=[ 0.00033464  0.22282648  0.00212038 -0.28958031]\n",
      "[ Experience replay ] starts\n",
      "[ episode 150 ][ timestamp 2 ] state=[ 0.00033464  0.22282648  0.00212038 -0.28958031], action=1, reward=1.0, next_state=[ 0.00479117  0.41791814 -0.00367123 -0.58159374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 150 ][ timestamp 3 ] state=[ 0.00479117  0.41791814 -0.00367123 -0.58159374], action=1, reward=1.0, next_state=[ 0.01314953  0.61309133 -0.0153031  -0.8754309 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 150 ][ timestamp 4 ] state=[ 0.01314953  0.61309133 -0.0153031  -0.8754309 ], action=1, reward=1.0, next_state=[ 0.02541135  0.80841793 -0.03281172 -1.17288542]\n",
      "[ Experience replay ] starts\n",
      "[ episode 150 ][ timestamp 5 ] state=[ 0.02541135  0.80841793 -0.03281172 -1.17288542], action=1, reward=1.0, next_state=[ 0.04157971  1.00395069 -0.05626943 -1.4756716 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 150 ][ timestamp 6 ] state=[ 0.04157971  1.00395069 -0.05626943 -1.4756716 ], action=1, reward=1.0, next_state=[ 0.06165873  1.19971313 -0.08578286 -1.78538501]\n",
      "[ Experience replay ] starts\n",
      "[ episode 150 ][ timestamp 7 ] state=[ 0.06165873  1.19971313 -0.08578286 -1.78538501], action=1, reward=1.0, next_state=[ 0.08565299  1.3956876  -0.12149056 -2.10345503]\n",
      "[ Experience replay ] starts\n",
      "[ episode 150 ][ timestamp 8 ] state=[ 0.08565299  1.3956876  -0.12149056 -2.10345503], action=1, reward=1.0, next_state=[ 0.11356674  1.59180091 -0.16355966 -2.43108712]\n",
      "[ Experience replay ] starts\n",
      "[ episode 150 ][ timestamp 9 ] state=[ 0.11356674  1.59180091 -0.16355966 -2.43108712], action=1, reward=-1.0, next_state=[ 0.14540276  1.78790759 -0.2121814  -2.76919367]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 150: Exploration_rate=0.01. Score=9.\n",
      "[ episode 151 ] state=[-0.00339615  0.03737656  0.04057464  0.04518783]\n",
      "[ episode 151 ][ timestamp 1 ] state=[-0.00339615  0.03737656  0.04057464  0.04518783], action=1, reward=1.0, next_state=[-0.00264862  0.23189391  0.0414784  -0.23442238]\n",
      "[ Experience replay ] starts\n",
      "[ episode 151 ][ timestamp 2 ] state=[-0.00264862  0.23189391  0.0414784  -0.23442238], action=1, reward=1.0, next_state=[ 0.00198926  0.42639944  0.03678995 -0.51373858]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 151 ][ timestamp 3 ] state=[ 0.00198926  0.42639944  0.03678995 -0.51373858], action=1, reward=1.0, next_state=[ 0.01051725  0.62098445  0.02651518 -0.79460479]\n",
      "[ Experience replay ] starts\n",
      "[ episode 151 ][ timestamp 4 ] state=[ 0.01051725  0.62098445  0.02651518 -0.79460479], action=1, reward=1.0, next_state=[ 0.02293694  0.81573263  0.01062308 -1.07882982]\n",
      "[ Experience replay ] starts\n",
      "[ episode 151 ][ timestamp 5 ] state=[ 0.02293694  0.81573263  0.01062308 -1.07882982], action=1, reward=1.0, next_state=[ 0.03925159  1.01071269 -0.01095352 -1.36816029]\n",
      "[ Experience replay ] starts\n",
      "[ episode 151 ][ timestamp 6 ] state=[ 0.03925159  1.01071269 -0.01095352 -1.36816029], action=1, reward=1.0, next_state=[ 0.05946585  1.20597001 -0.03831672 -1.66424895]\n",
      "[ Experience replay ] starts\n",
      "[ episode 151 ][ timestamp 7 ] state=[ 0.05946585  1.20597001 -0.03831672 -1.66424895], action=1, reward=1.0, next_state=[ 0.08358525  1.40151643 -0.0716017  -1.96861565]\n",
      "[ Experience replay ] starts\n",
      "[ episode 151 ][ timestamp 8 ] state=[ 0.08358525  1.40151643 -0.0716017  -1.96861565], action=1, reward=1.0, next_state=[ 0.11161558  1.59731793 -0.11097401 -2.28259827]\n",
      "[ Experience replay ] starts\n",
      "[ episode 151 ][ timestamp 9 ] state=[ 0.11161558  1.59731793 -0.11097401 -2.28259827], action=1, reward=1.0, next_state=[ 0.14356193  1.79327957 -0.15662598 -2.60729204]\n",
      "[ Experience replay ] starts\n",
      "[ episode 151 ][ timestamp 10 ] state=[ 0.14356193  1.79327957 -0.15662598 -2.60729204], action=1, reward=1.0, next_state=[ 0.17942753  1.98922782 -0.20877182 -2.94347657]\n",
      "[ Experience replay ] starts\n",
      "[ episode 151 ][ timestamp 11 ] state=[ 0.17942753  1.98922782 -0.20877182 -2.94347657], action=1, reward=-1.0, next_state=[ 0.21921208  2.18489073 -0.26764135 -3.29153206]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 151: Exploration_rate=0.01. Score=11.\n",
      "[ episode 152 ] state=[-0.00419706 -0.0307659   0.03599952  0.03711562]\n",
      "[ episode 152 ][ timestamp 1 ] state=[-0.00419706 -0.0307659   0.03599952  0.03711562], action=1, reward=1.0, next_state=[-0.00481238  0.16382182  0.03674184 -0.24399526]\n",
      "[ Experience replay ] starts\n",
      "[ episode 152 ][ timestamp 2 ] state=[-0.00481238  0.16382182  0.03674184 -0.24399526], action=1, reward=1.0, next_state=[-0.00153594  0.35840023  0.03186193 -0.52486623]\n",
      "[ Experience replay ] starts\n",
      "[ episode 152 ][ timestamp 3 ] state=[-0.00153594  0.35840023  0.03186193 -0.52486623], action=1, reward=1.0, next_state=[ 0.00563206  0.55305965  0.02136461 -0.80734134]\n",
      "[ Experience replay ] starts\n",
      "[ episode 152 ][ timestamp 4 ] state=[ 0.00563206  0.55305965  0.02136461 -0.80734134], action=1, reward=1.0, next_state=[ 0.01669326  0.74788238  0.00521778 -1.09322802]\n",
      "[ Experience replay ] starts\n",
      "[ episode 152 ][ timestamp 5 ] state=[ 0.01669326  0.74788238  0.00521778 -1.09322802], action=1, reward=1.0, next_state=[ 0.0316509   0.9429352  -0.01664678 -1.38426924]\n",
      "[ Experience replay ] starts\n",
      "[ episode 152 ][ timestamp 6 ] state=[ 0.0316509   0.9429352  -0.01664678 -1.38426924], action=1, reward=1.0, next_state=[ 0.05050961  1.13826076 -0.04433217 -1.68211093]\n",
      "[ Experience replay ] starts\n",
      "[ episode 152 ][ timestamp 7 ] state=[ 0.05050961  1.13826076 -0.04433217 -1.68211093], action=1, reward=1.0, next_state=[ 0.07327482  1.33386722 -0.07797439 -1.98826172]\n",
      "[ Experience replay ] starts\n",
      "[ episode 152 ][ timestamp 8 ] state=[ 0.07327482  1.33386722 -0.07797439 -1.98826172], action=1, reward=1.0, next_state=[ 0.09995217  1.52971553 -0.11773962 -2.30404283]\n",
      "[ Experience replay ] starts\n",
      "[ episode 152 ][ timestamp 9 ] state=[ 0.09995217  1.52971553 -0.11773962 -2.30404283], action=1, reward=1.0, next_state=[ 0.13054648  1.72570422 -0.16382048 -2.63052604]\n",
      "[ Experience replay ] starts\n",
      "[ episode 152 ][ timestamp 10 ] state=[ 0.13054648  1.72570422 -0.16382048 -2.63052604], action=1, reward=-1.0, next_state=[ 0.16506056  1.92165144 -0.216431   -2.96845977]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 152: Exploration_rate=0.01. Score=10.\n",
      "[ episode 153 ] state=[-0.03895744 -0.02057825 -0.00440669  0.02779055]\n",
      "[ episode 153 ][ timestamp 1 ] state=[-0.03895744 -0.02057825 -0.00440669  0.02779055], action=1, reward=1.0, next_state=[-0.039369    0.17460662 -0.00385088 -0.26627948]\n",
      "[ Experience replay ] starts\n",
      "[ episode 153 ][ timestamp 2 ] state=[-0.039369    0.17460662 -0.00385088 -0.26627948], action=1, reward=1.0, next_state=[-0.03587687  0.36978332 -0.00917647 -0.56017451]\n",
      "[ Experience replay ] starts\n",
      "[ episode 153 ][ timestamp 3 ] state=[-0.03587687  0.36978332 -0.00917647 -0.56017451], action=1, reward=1.0, next_state=[-0.02848121  0.56503286 -0.02037996 -0.85573433]\n",
      "[ Experience replay ] starts\n",
      "[ episode 153 ][ timestamp 4 ] state=[-0.02848121  0.56503286 -0.02037996 -0.85573433], action=1, reward=1.0, next_state=[-0.01718055  0.76042651 -0.03749465 -1.15475524]\n",
      "[ Experience replay ] starts\n",
      "[ episode 153 ][ timestamp 5 ] state=[-0.01718055  0.76042651 -0.03749465 -1.15475524], action=1, reward=1.0, next_state=[-0.00197202  0.95601681 -0.06058975 -1.45895532]\n",
      "[ Experience replay ] starts\n",
      "[ episode 153 ][ timestamp 6 ] state=[-0.00197202  0.95601681 -0.06058975 -1.45895532], action=1, reward=1.0, next_state=[ 0.01714832  1.1518273  -0.08976886 -1.76993458]\n",
      "[ Experience replay ] starts\n",
      "[ episode 153 ][ timestamp 7 ] state=[ 0.01714832  1.1518273  -0.08976886 -1.76993458], action=1, reward=1.0, next_state=[ 0.04018486  1.34784051 -0.12516755 -2.08912713]\n",
      "[ Experience replay ] starts\n",
      "[ episode 153 ][ timestamp 8 ] state=[ 0.04018486  1.34784051 -0.12516755 -2.08912713], action=1, reward=1.0, next_state=[ 0.06714167  1.5439836  -0.16695009 -2.41774331]\n",
      "[ Experience replay ] starts\n",
      "[ episode 153 ][ timestamp 9 ] state=[ 0.06714167  1.5439836  -0.16695009 -2.41774331], action=1, reward=-1.0, next_state=[ 0.09802135  1.74011165 -0.21530496 -2.75670061]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 153: Exploration_rate=0.01. Score=9.\n",
      "[ episode 154 ] state=[ 0.02924398 -0.02575911  0.03816327 -0.04973862]\n",
      "[ episode 154 ][ timestamp 1 ] state=[ 0.02924398 -0.02575911  0.03816327 -0.04973862], action=1, reward=1.0, next_state=[ 0.0287288   0.16879542  0.0371685  -0.33014065]\n",
      "[ Experience replay ] starts\n",
      "[ episode 154 ][ timestamp 2 ] state=[ 0.0287288   0.16879542  0.0371685  -0.33014065], action=1, reward=1.0, next_state=[ 0.0321047   0.3633691   0.03056568 -0.61087457]\n",
      "[ Experience replay ] starts\n",
      "[ episode 154 ][ timestamp 3 ] state=[ 0.0321047   0.3633691   0.03056568 -0.61087457], action=1, reward=1.0, next_state=[ 0.03937209  0.55805079  0.01834819 -0.89377579]\n",
      "[ Experience replay ] starts\n",
      "[ episode 154 ][ timestamp 4 ] state=[ 0.03937209  0.55805079  0.01834819 -0.89377579], action=1, reward=1.0, next_state=[ 5.05331015e-02  7.52919164e-01  4.72676574e-04 -1.18063508e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 154 ][ timestamp 5 ] state=[ 5.05331015e-02  7.52919164e-01  4.72676574e-04 -1.18063508e+00], action=1, reward=1.0, next_state=[ 0.06559148  0.94803498 -0.02314003 -1.4731698 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 154 ][ timestamp 6 ] state=[ 0.06559148  0.94803498 -0.02314003 -1.4731698 ], action=1, reward=1.0, next_state=[ 0.08455218  1.14343203 -0.05260342 -1.77298947]\n",
      "[ Experience replay ] starts\n",
      "[ episode 154 ][ timestamp 7 ] state=[ 0.08455218  1.14343203 -0.05260342 -1.77298947], action=1, reward=1.0, next_state=[ 0.10742082  1.33910616 -0.08806321 -2.08155295]\n",
      "[ Experience replay ] starts\n",
      "[ episode 154 ][ timestamp 8 ] state=[ 0.10742082  1.33910616 -0.08806321 -2.08155295], action=1, reward=1.0, next_state=[ 0.13420295  1.5350019  -0.12969427 -2.40011502]\n",
      "[ Experience replay ] starts\n",
      "[ episode 154 ][ timestamp 9 ] state=[ 0.13420295  1.5350019  -0.12969427 -2.40011502], action=1, reward=1.0, next_state=[ 0.16490299  1.73099635 -0.17769657 -2.72966091]\n",
      "[ Experience replay ] starts\n",
      "[ episode 154 ][ timestamp 10 ] state=[ 0.16490299  1.73099635 -0.17769657 -2.72966091], action=1, reward=-1.0, next_state=[ 0.19952291  1.92688064 -0.23228979 -3.07082888]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 154: Exploration_rate=0.01. Score=10.\n",
      "[ episode 155 ] state=[ 0.03437358 -0.03533685  0.00352382  0.02884037]\n",
      "[ episode 155 ][ timestamp 1 ] state=[ 0.03437358 -0.03533685  0.00352382  0.02884037], action=1, reward=1.0, next_state=[ 0.03366684  0.1597344   0.00410063 -0.26272867]\n",
      "[ Experience replay ] starts\n",
      "[ episode 155 ][ timestamp 2 ] state=[ 0.03366684  0.1597344   0.00410063 -0.26272867], action=1, reward=1.0, next_state=[ 0.03686153  0.35479757 -0.00115394 -0.55411539]\n",
      "[ Experience replay ] starts\n",
      "[ episode 155 ][ timestamp 3 ] state=[ 0.03686153  0.35479757 -0.00115394 -0.55411539], action=1, reward=1.0, next_state=[ 0.04395748  0.54993571 -0.01223625 -0.84716166]\n",
      "[ Experience replay ] starts\n",
      "[ episode 155 ][ timestamp 4 ] state=[ 0.04395748  0.54993571 -0.01223625 -0.84716166], action=1, reward=1.0, next_state=[ 0.0549562   0.74522242 -0.02917948 -1.14366717]\n",
      "[ Experience replay ] starts\n",
      "[ episode 155 ][ timestamp 5 ] state=[ 0.0549562   0.74522242 -0.02917948 -1.14366717], action=1, reward=1.0, next_state=[ 0.06986064  0.94071321 -0.05205283 -1.44535607]\n",
      "[ Experience replay ] starts\n",
      "[ episode 155 ][ timestamp 6 ] state=[ 0.06986064  0.94071321 -0.05205283 -1.44535607], action=1, reward=1.0, next_state=[ 0.08867491  1.13643552 -0.08095995 -1.75383851]\n",
      "[ Experience replay ] starts\n",
      "[ episode 155 ][ timestamp 7 ] state=[ 0.08867491  1.13643552 -0.08095995 -1.75383851], action=1, reward=1.0, next_state=[ 0.11140362  1.33237703 -0.11603672 -2.07056431]\n",
      "[ Experience replay ] starts\n",
      "[ episode 155 ][ timestamp 8 ] state=[ 0.11140362  1.33237703 -0.11603672 -2.07056431], action=1, reward=1.0, next_state=[ 0.13805116  1.52847163 -0.157448   -2.39676649]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 155 ][ timestamp 9 ] state=[ 0.13805116  1.52847163 -0.157448   -2.39676649], action=1, reward=1.0, next_state=[ 0.16862059  1.72458293 -0.20538333 -2.73339348]\n",
      "[ Experience replay ] starts\n",
      "[ episode 155 ][ timestamp 10 ] state=[ 0.16862059  1.72458293 -0.20538333 -2.73339348], action=1, reward=-1.0, next_state=[ 0.20311225  1.92048547 -0.2600512  -3.0810304 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 155: Exploration_rate=0.01. Score=10.\n",
      "[ episode 156 ] state=[-0.01576573  0.0240623  -0.01023444  0.04085849]\n",
      "[ episode 156 ][ timestamp 1 ] state=[-0.01576573  0.0240623  -0.01023444  0.04085849], action=1, reward=1.0, next_state=[-0.01528448  0.21932951 -0.00941727 -0.25503585]\n",
      "[ Experience replay ] starts\n",
      "[ episode 156 ][ timestamp 2 ] state=[-0.01528448  0.21932951 -0.00941727 -0.25503585], action=1, reward=1.0, next_state=[-0.01089789  0.41458464 -0.01451798 -0.5506742 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 156 ][ timestamp 3 ] state=[-0.01089789  0.41458464 -0.01451798 -0.5506742 ], action=1, reward=1.0, next_state=[-0.0026062   0.60990747 -0.02553147 -0.8478957 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 156 ][ timestamp 4 ] state=[-0.0026062   0.60990747 -0.02553147 -0.8478957 ], action=1, reward=1.0, next_state=[ 0.00959195  0.80536819 -0.04248938 -1.14849667]\n",
      "[ Experience replay ] starts\n",
      "[ episode 156 ][ timestamp 5 ] state=[ 0.00959195  0.80536819 -0.04248938 -1.14849667], action=1, reward=1.0, next_state=[ 0.02569932  1.00101828 -0.06545932 -1.45419505]\n",
      "[ Experience replay ] starts\n",
      "[ episode 156 ][ timestamp 6 ] state=[ 0.02569932  1.00101828 -0.06545932 -1.45419505], action=1, reward=1.0, next_state=[ 0.04571968  1.19688006 -0.09454322 -1.7665898 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 156 ][ timestamp 7 ] state=[ 0.04571968  1.19688006 -0.09454322 -1.7665898 ], action=1, reward=1.0, next_state=[ 0.06965728  1.39293456 -0.12987501 -2.08711253]\n",
      "[ Experience replay ] starts\n",
      "[ episode 156 ][ timestamp 8 ] state=[ 0.06965728  1.39293456 -0.12987501 -2.08711253], action=1, reward=1.0, next_state=[ 0.09751597  1.58910708 -0.17161726 -2.4169691 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 156 ][ timestamp 9 ] state=[ 0.09751597  1.58910708 -0.17161726 -2.4169691 ], action=1, reward=-1.0, next_state=[ 0.12929812  1.78525037 -0.21995665 -2.75707014]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 156: Exploration_rate=0.01. Score=9.\n",
      "[ episode 157 ] state=[ 0.00981381  0.02992475 -0.00955208  0.01551959]\n",
      "[ episode 157 ][ timestamp 1 ] state=[ 0.00981381  0.02992475 -0.00955208  0.01551959], action=1, reward=1.0, next_state=[ 0.01041231  0.22518238 -0.00924169 -0.28016176]\n",
      "[ Experience replay ] starts\n",
      "[ episode 157 ][ timestamp 2 ] state=[ 0.01041231  0.22518238 -0.00924169 -0.28016176], action=1, reward=1.0, next_state=[ 0.01491596  0.42043493 -0.01484492 -0.5757451 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 157 ][ timestamp 3 ] state=[ 0.01491596  0.42043493 -0.01484492 -0.5757451 ], action=1, reward=1.0, next_state=[ 0.02332466  0.6157618  -0.02635983 -0.87306737]\n",
      "[ Experience replay ] starts\n",
      "[ episode 157 ][ timestamp 4 ] state=[ 0.02332466  0.6157618  -0.02635983 -0.87306737], action=1, reward=1.0, next_state=[ 0.03563989  0.81123208 -0.04382117 -1.17391982]\n",
      "[ Experience replay ] starts\n",
      "[ episode 157 ][ timestamp 5 ] state=[ 0.03563989  0.81123208 -0.04382117 -1.17391982], action=1, reward=1.0, next_state=[ 0.05186453  1.00689531 -0.06729957 -1.48001222]\n",
      "[ Experience replay ] starts\n",
      "[ episode 157 ][ timestamp 6 ] state=[ 0.05186453  1.00689531 -0.06729957 -1.48001222], action=1, reward=1.0, next_state=[ 0.07200244  1.202771   -0.09689981 -1.79293177]\n",
      "[ Experience replay ] starts\n",
      "[ episode 157 ][ timestamp 7 ] state=[ 0.07200244  1.202771   -0.09689981 -1.79293177], action=1, reward=1.0, next_state=[ 0.09605786  1.39883624 -0.13275845 -2.11409397]\n",
      "[ Experience replay ] starts\n",
      "[ episode 157 ][ timestamp 8 ] state=[ 0.09605786  1.39883624 -0.13275845 -2.11409397], action=1, reward=1.0, next_state=[ 0.12403458  1.59501117 -0.17504033 -2.44468344]\n",
      "[ Experience replay ] starts\n",
      "[ episode 157 ][ timestamp 9 ] state=[ 0.12403458  1.59501117 -0.17504033 -2.44468344], action=1, reward=-1.0, next_state=[ 0.15593481  1.79114185 -0.223934   -2.78558348]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 157: Exploration_rate=0.01. Score=9.\n",
      "[ episode 158 ] state=[ 0.02206545 -0.01928646 -0.00057364  0.04152522]\n",
      "[ episode 158 ][ timestamp 1 ] state=[ 0.02206545 -0.01928646 -0.00057364  0.04152522], action=1, reward=1.0, next_state=[ 0.02167972  0.17584371  0.00025687 -0.25133864]\n",
      "[ Experience replay ] starts\n",
      "[ episode 158 ][ timestamp 2 ] state=[ 0.02167972  0.17584371  0.00025687 -0.25133864], action=1, reward=1.0, next_state=[ 0.0251966   0.37096199 -0.00476991 -0.54394054]\n",
      "[ Experience replay ] starts\n",
      "[ episode 158 ][ timestamp 3 ] state=[ 0.0251966   0.37096199 -0.00476991 -0.54394054], action=1, reward=1.0, next_state=[ 0.03261584  0.56615065 -0.01564872 -0.83812254]\n",
      "[ Experience replay ] starts\n",
      "[ episode 158 ][ timestamp 4 ] state=[ 0.03261584  0.56615065 -0.01564872 -0.83812254], action=1, reward=1.0, next_state=[ 0.04393885  0.76148276 -0.03241117 -1.13568537]\n",
      "[ Experience replay ] starts\n",
      "[ episode 158 ][ timestamp 5 ] state=[ 0.04393885  0.76148276 -0.03241117 -1.13568537], action=1, reward=1.0, next_state=[ 0.05916851  0.95701341 -0.05512488 -1.43835452]\n",
      "[ Experience replay ] starts\n",
      "[ episode 158 ][ timestamp 6 ] state=[ 0.05916851  0.95701341 -0.05512488 -1.43835452], action=1, reward=1.0, next_state=[ 0.07830878  1.15276964 -0.08389197 -1.74774135]\n",
      "[ Experience replay ] starts\n",
      "[ episode 158 ][ timestamp 7 ] state=[ 0.07830878  1.15276964 -0.08389197 -1.74774135], action=1, reward=1.0, next_state=[ 0.10136417  1.3487387  -0.11884679 -2.06529645]\n",
      "[ Experience replay ] starts\n",
      "[ episode 158 ][ timestamp 8 ] state=[ 0.10136417  1.3487387  -0.11884679 -2.06529645], action=1, reward=1.0, next_state=[ 0.12833894  1.54485394 -0.16015272 -2.39225299]\n",
      "[ Experience replay ] starts\n",
      "[ episode 158 ][ timestamp 9 ] state=[ 0.12833894  1.54485394 -0.16015272 -2.39225299], action=1, reward=1.0, next_state=[ 0.15923602  1.74097834 -0.20799778 -2.72955875]\n",
      "[ Experience replay ] starts\n",
      "[ episode 158 ][ timestamp 10 ] state=[ 0.15923602  1.74097834 -0.20799778 -2.72955875], action=1, reward=-1.0, next_state=[ 0.19405559  1.93688571 -0.26258896 -3.07779739]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 158: Exploration_rate=0.01. Score=10.\n",
      "[ episode 159 ] state=[-0.03400541 -0.00463489  0.03554818  0.01472772]\n",
      "[ episode 159 ][ timestamp 1 ] state=[-0.03400541 -0.00463489  0.03554818  0.01472772], action=1, reward=1.0, next_state=[-0.03409811  0.1899597   0.03584273 -0.26653079]\n",
      "[ Experience replay ] starts\n",
      "[ episode 159 ][ timestamp 2 ] state=[-0.03409811  0.1899597   0.03584273 -0.26653079], action=1, reward=1.0, next_state=[-0.03029891  0.38455226  0.03051212 -0.54769664]\n",
      "[ Experience replay ] starts\n",
      "[ episode 159 ][ timestamp 3 ] state=[-0.03029891  0.38455226  0.03051212 -0.54769664], action=1, reward=1.0, next_state=[-0.02260787  0.57923256  0.01955819 -0.830612  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 159 ][ timestamp 4 ] state=[-0.02260787  0.57923256  0.01955819 -0.830612  ], action=1, reward=1.0, next_state=[-0.01102322  0.7740818   0.00294595 -1.11708022]\n",
      "[ Experience replay ] starts\n",
      "[ episode 159 ][ timestamp 5 ] state=[-0.01102322  0.7740818   0.00294595 -1.11708022], action=1, reward=1.0, next_state=[ 0.00445842  0.96916497 -0.01939566 -1.4088376 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 159 ][ timestamp 6 ] state=[ 0.00445842  0.96916497 -0.01939566 -1.4088376 ], action=1, reward=1.0, next_state=[ 0.02384172  1.16452208 -0.04757241 -1.70752011]\n",
      "[ Experience replay ] starts\n",
      "[ episode 159 ][ timestamp 7 ] state=[ 0.02384172  1.16452208 -0.04757241 -1.70752011], action=1, reward=1.0, next_state=[ 0.04713216  1.36015762 -0.08172281 -2.01462244]\n",
      "[ Experience replay ] starts\n",
      "[ episode 159 ][ timestamp 8 ] state=[ 0.04713216  1.36015762 -0.08172281 -2.01462244], action=1, reward=1.0, next_state=[ 0.07433531  1.55602764 -0.12201526 -2.33144668]\n",
      "[ Experience replay ] starts\n",
      "[ episode 159 ][ timestamp 9 ] state=[ 0.07433531  1.55602764 -0.12201526 -2.33144668], action=1, reward=1.0, next_state=[ 0.10545587  1.75202425 -0.1686442  -2.6590394 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 159 ][ timestamp 10 ] state=[ 0.10545587  1.75202425 -0.1686442  -2.6590394 ], action=1, reward=-1.0, next_state=[ 0.14049635  1.9479575  -0.22182498 -2.9981165 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 159: Exploration_rate=0.01. Score=10.\n",
      "[ episode 160 ] state=[ 0.02788666 -0.01375458  0.0263237  -0.02025658]\n",
      "[ episode 160 ][ timestamp 1 ] state=[ 0.02788666 -0.01375458  0.0263237  -0.02025658], action=1, reward=1.0, next_state=[ 0.02761157  0.18098016  0.02591857 -0.30451922]\n",
      "[ Experience replay ] starts\n",
      "[ episode 160 ][ timestamp 2 ] state=[ 0.02761157  0.18098016  0.02591857 -0.30451922], action=1, reward=1.0, next_state=[ 0.03123117  0.37572334  0.01982818 -0.58891668]\n",
      "[ Experience replay ] starts\n",
      "[ episode 160 ][ timestamp 3 ] state=[ 0.03123117  0.37572334  0.01982818 -0.58891668], action=1, reward=1.0, next_state=[ 0.03874564  0.57056211  0.00804985 -0.87528827]\n",
      "[ Experience replay ] starts\n",
      "[ episode 160 ][ timestamp 4 ] state=[ 0.03874564  0.57056211  0.00804985 -0.87528827], action=1, reward=1.0, next_state=[ 0.05015688  0.76557371 -0.00945592 -1.16542957]\n",
      "[ Experience replay ] starts\n",
      "[ episode 160 ][ timestamp 5 ] state=[ 0.05015688  0.76557371 -0.00945592 -1.16542957], action=1, reward=1.0, next_state=[ 0.06546835  0.96081746 -0.03276451 -1.46106209]\n",
      "[ Experience replay ] starts\n",
      "[ episode 160 ][ timestamp 6 ] state=[ 0.06546835  0.96081746 -0.03276451 -1.46106209], action=1, reward=1.0, next_state=[ 0.0846847   1.15632539 -0.06198575 -1.76379763]\n",
      "[ Experience replay ] starts\n",
      "[ episode 160 ][ timestamp 7 ] state=[ 0.0846847   1.15632539 -0.06198575 -1.76379763], action=1, reward=1.0, next_state=[ 0.10781121  1.35209105 -0.0972617  -2.07509432]\n",
      "[ Experience replay ] starts\n",
      "[ episode 160 ][ timestamp 8 ] state=[ 0.10781121  1.35209105 -0.0972617  -2.07509432], action=1, reward=1.0, next_state=[ 0.13485303  1.54805593 -0.13876359 -2.39620226]\n",
      "[ Experience replay ] starts\n",
      "[ episode 160 ][ timestamp 9 ] state=[ 0.13485303  1.54805593 -0.13876359 -2.39620226], action=1, reward=1.0, next_state=[ 0.16581415  1.74409325 -0.18668763 -2.7280974 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 160 ][ timestamp 10 ] state=[ 0.16581415  1.74409325 -0.18668763 -2.7280974 ], action=1, reward=-1.0, next_state=[ 0.20069601  1.93998933 -0.24124958 -3.07140371]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 160: Exploration_rate=0.01. Score=10.\n",
      "[ episode 161 ] state=[ 0.01339791 -0.01356778  0.04982747  0.02208875]\n",
      "[ episode 161 ][ timestamp 1 ] state=[ 0.01339791 -0.01356778  0.04982747  0.02208875], action=1, reward=1.0, next_state=[ 0.01312656  0.18080549  0.05026924 -0.25446609]\n",
      "[ Experience replay ] starts\n",
      "[ episode 161 ][ timestamp 2 ] state=[ 0.01312656  0.18080549  0.05026924 -0.25446609], action=1, reward=1.0, next_state=[ 0.01674267  0.37517499  0.04517992 -0.5308791 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 161 ][ timestamp 3 ] state=[ 0.01674267  0.37517499  0.04517992 -0.5308791 ], action=1, reward=1.0, next_state=[ 0.02424617  0.56963327  0.03456234 -0.80899049]\n",
      "[ Experience replay ] starts\n",
      "[ episode 161 ][ timestamp 4 ] state=[ 0.02424617  0.56963327  0.03456234 -0.80899049], action=1, reward=1.0, next_state=[ 0.03563883  0.764265    0.01838253 -1.09060442]\n",
      "[ Experience replay ] starts\n",
      "[ episode 161 ][ timestamp 5 ] state=[ 0.03563883  0.764265    0.01838253 -1.09060442], action=1, reward=1.0, next_state=[ 0.05092413  0.95913989 -0.00342956 -1.37746321]\n",
      "[ Experience replay ] starts\n",
      "[ episode 161 ][ timestamp 6 ] state=[ 0.05092413  0.95913989 -0.00342956 -1.37746321], action=1, reward=1.0, next_state=[ 0.07010693  1.15430451 -0.03097882 -1.67121671]\n",
      "[ Experience replay ] starts\n",
      "[ episode 161 ][ timestamp 7 ] state=[ 0.07010693  1.15430451 -0.03097882 -1.67121671], action=1, reward=1.0, next_state=[ 0.09319302  1.34977234 -0.06440316 -1.97338408]\n",
      "[ Experience replay ] starts\n",
      "[ episode 161 ][ timestamp 8 ] state=[ 0.09319302  1.34977234 -0.06440316 -1.97338408], action=1, reward=1.0, next_state=[ 0.12018847  1.54551153 -0.10387084 -2.28530562]\n",
      "[ Experience replay ] starts\n",
      "[ episode 161 ][ timestamp 9 ] state=[ 0.12018847  1.54551153 -0.10387084 -2.28530562], action=1, reward=1.0, next_state=[ 0.1510987   1.74143004 -0.14957695 -2.60808261]\n",
      "[ Experience replay ] starts\n",
      "[ episode 161 ][ timestamp 10 ] state=[ 0.1510987   1.74143004 -0.14957695 -2.60808261], action=1, reward=1.0, next_state=[ 0.1859273   1.93735801 -0.2017386  -2.94250484]\n",
      "[ Experience replay ] starts\n",
      "[ episode 161 ][ timestamp 11 ] state=[ 0.1859273   1.93735801 -0.2017386  -2.94250484], action=1, reward=-1.0, next_state=[ 0.22467446  2.13302791 -0.2605887  -3.28896698]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 161: Exploration_rate=0.01. Score=11.\n",
      "[ episode 162 ] state=[ 0.04564916  0.00350867 -0.03269501  0.00262439]\n",
      "[ episode 162 ][ timestamp 1 ] state=[ 0.04564916  0.00350867 -0.03269501  0.00262439], action=1, reward=1.0, next_state=[ 0.04571933  0.19908389 -0.03264252 -0.30019228]\n",
      "[ Experience replay ] starts\n",
      "[ episode 162 ][ timestamp 2 ] state=[ 0.04571933  0.19908389 -0.03264252 -0.30019228], action=1, reward=1.0, next_state=[ 0.04970101  0.39465554 -0.03864636 -0.60298867]\n",
      "[ Experience replay ] starts\n",
      "[ episode 162 ][ timestamp 3 ] state=[ 0.04970101  0.39465554 -0.03864636 -0.60298867], action=1, reward=1.0, next_state=[ 0.05759412  0.59029611 -0.05070614 -0.90758961]\n",
      "[ Experience replay ] starts\n",
      "[ episode 162 ][ timestamp 4 ] state=[ 0.05759412  0.59029611 -0.05070614 -0.90758961], action=1, reward=1.0, next_state=[ 0.06940004  0.78606649 -0.06885793 -1.21576896]\n",
      "[ Experience replay ] starts\n",
      "[ episode 162 ][ timestamp 5 ] state=[ 0.06940004  0.78606649 -0.06885793 -1.21576896], action=1, reward=1.0, next_state=[ 0.08512137  0.98200576 -0.09317331 -1.5292096 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 162 ][ timestamp 6 ] state=[ 0.08512137  0.98200576 -0.09317331 -1.5292096 ], action=1, reward=1.0, next_state=[ 0.10476149  1.17811975 -0.1237575  -1.84945796]\n",
      "[ Experience replay ] starts\n",
      "[ episode 162 ][ timestamp 7 ] state=[ 0.10476149  1.17811975 -0.1237575  -1.84945796], action=1, reward=1.0, next_state=[ 0.12832388  1.37436775 -0.16074666 -2.17787045]\n",
      "[ Experience replay ] starts\n",
      "[ episode 162 ][ timestamp 8 ] state=[ 0.12832388  1.37436775 -0.16074666 -2.17787045], action=0, reward=1.0, next_state=[ 0.15581124  1.1811332  -0.20430407 -1.93881164]\n",
      "[ Experience replay ] starts\n",
      "[ episode 162 ][ timestamp 9 ] state=[ 0.15581124  1.1811332  -0.20430407 -1.93881164], action=0, reward=-1.0, next_state=[ 0.1794339   0.98869597 -0.2430803  -1.71580755]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 162: Exploration_rate=0.01. Score=9.\n",
      "[ episode 163 ] state=[-0.01775278  0.03640775 -0.02781933  0.00787557]\n",
      "[ episode 163 ][ timestamp 1 ] state=[-0.01775278  0.03640775 -0.02781933  0.00787557], action=0, reward=1.0, next_state=[-0.01702463 -0.15830442 -0.02766182  0.29165298]\n",
      "[ Experience replay ] starts\n",
      "[ episode 163 ][ timestamp 2 ] state=[-0.01702463 -0.15830442 -0.02766182  0.29165298], action=0, reward=1.0, next_state=[-0.02019072 -0.35302125 -0.02182876  0.57548496]\n",
      "[ Experience replay ] starts\n",
      "[ episode 163 ][ timestamp 3 ] state=[-0.02019072 -0.35302125 -0.02182876  0.57548496], action=0, reward=1.0, next_state=[-0.02725114 -0.54783051 -0.01031906  0.86121208]\n",
      "[ Experience replay ] starts\n",
      "[ episode 163 ][ timestamp 4 ] state=[-0.02725114 -0.54783051 -0.01031906  0.86121208], action=1, reward=1.0, next_state=[-0.03820775 -0.35256957  0.00690518  0.56530252]\n",
      "[ Experience replay ] starts\n",
      "[ episode 163 ][ timestamp 5 ] state=[-0.03820775 -0.35256957  0.00690518  0.56530252], action=1, reward=1.0, next_state=[-0.04525914 -0.15754517  0.01821123  0.274803  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 163 ][ timestamp 6 ] state=[-0.04525914 -0.15754517  0.01821123  0.274803  ], action=1, reward=1.0, next_state=[-0.04841005  0.03731228  0.02370729 -0.01208089]\n",
      "[ Experience replay ] starts\n",
      "[ episode 163 ][ timestamp 7 ] state=[-0.04841005  0.03731228  0.02370729 -0.01208089], action=1, reward=1.0, next_state=[-0.0476638   0.23208635  0.02346568 -0.29719062]\n",
      "[ Experience replay ] starts\n",
      "[ episode 163 ][ timestamp 8 ] state=[-0.0476638   0.23208635  0.02346568 -0.29719062], action=1, reward=1.0, next_state=[-0.04302207  0.42686607  0.01752186 -0.58238148]\n",
      "[ Experience replay ] starts\n",
      "[ episode 163 ][ timestamp 9 ] state=[-0.04302207  0.42686607  0.01752186 -0.58238148], action=1, reward=1.0, next_state=[-0.03448475  0.6217382   0.00587423 -0.86949365]\n",
      "[ Experience replay ] starts\n",
      "[ episode 163 ][ timestamp 10 ] state=[-0.03448475  0.6217382   0.00587423 -0.86949365], action=1, reward=1.0, next_state=[-0.02204999  0.81677975 -0.01151564 -1.16032391]\n",
      "[ Experience replay ] starts\n",
      "[ episode 163 ][ timestamp 11 ] state=[-0.02204999  0.81677975 -0.01151564 -1.16032391], action=0, reward=1.0, next_state=[-0.00571439  0.6218097  -0.03472212 -0.87127375]\n",
      "[ Experience replay ] starts\n",
      "[ episode 163 ][ timestamp 12 ] state=[-0.00571439  0.6218097  -0.03472212 -0.87127375], action=1, reward=1.0, next_state=[ 0.0067218   0.81738626 -0.05214759 -1.17466802]\n",
      "[ Experience replay ] starts\n",
      "[ episode 163 ][ timestamp 13 ] state=[ 0.0067218   0.81738626 -0.05214759 -1.17466802], action=1, reward=1.0, next_state=[ 0.02306953  1.01314565 -0.07564095 -1.48323237]\n",
      "[ Experience replay ] starts\n",
      "[ episode 163 ][ timestamp 14 ] state=[ 0.02306953  1.01314565 -0.07564095 -1.48323237], action=1, reward=1.0, next_state=[ 0.04333244  1.20910419 -0.1053056  -1.79854693]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 163 ][ timestamp 15 ] state=[ 0.04333244  1.20910419 -0.1053056  -1.79854693], action=0, reward=1.0, next_state=[ 0.06751452  1.01530629 -0.14127654 -1.54036305]\n",
      "[ Experience replay ] starts\n",
      "[ episode 163 ][ timestamp 16 ] state=[ 0.06751452  1.01530629 -0.14127654 -1.54036305], action=0, reward=1.0, next_state=[ 0.08782065  0.82213789 -0.1720838  -1.29489451]\n",
      "[ Experience replay ] starts\n",
      "[ episode 163 ][ timestamp 17 ] state=[ 0.08782065  0.82213789 -0.1720838  -1.29489451], action=0, reward=1.0, next_state=[ 0.10426341  0.62956847 -0.19798169 -1.06065005]\n",
      "[ Experience replay ] starts\n",
      "[ episode 163 ][ timestamp 18 ] state=[ 0.10426341  0.62956847 -0.19798169 -1.06065005], action=0, reward=-1.0, next_state=[ 0.11685478  0.43753997 -0.21919469 -0.83606116]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 163: Exploration_rate=0.01. Score=18.\n",
      "[ episode 164 ] state=[ 0.03794026  0.00385104  0.00030561 -0.00850178]\n",
      "[ episode 164 ][ timestamp 1 ] state=[ 0.03794026  0.00385104  0.00030561 -0.00850178], action=0, reward=1.0, next_state=[ 3.80172822e-02 -1.91275295e-01  1.35570377e-04  2.84277556e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 164 ][ timestamp 2 ] state=[ 3.80172822e-02 -1.91275295e-01  1.35570377e-04  2.84277556e-01], action=0, reward=1.0, next_state=[ 0.03419178 -0.38639918  0.00582112  0.57700324]\n",
      "[ Experience replay ] starts\n",
      "[ episode 164 ][ timestamp 3 ] state=[ 0.03419178 -0.38639918  0.00582112  0.57700324], action=0, reward=1.0, next_state=[ 0.02646379 -0.58160224  0.01736119  0.87151426]\n",
      "[ Experience replay ] starts\n",
      "[ episode 164 ][ timestamp 4 ] state=[ 0.02646379 -0.58160224  0.01736119  0.87151426], action=0, reward=1.0, next_state=[ 0.01483175 -0.77695595  0.03479147  1.16960461]\n",
      "[ Experience replay ] starts\n",
      "[ episode 164 ][ timestamp 5 ] state=[ 0.01483175 -0.77695595  0.03479147  1.16960461], action=0, reward=1.0, next_state=[-7.07371063e-04 -9.72512724e-01  5.81835637e-02  1.47298888e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 164 ][ timestamp 6 ] state=[-7.07371063e-04 -9.72512724e-01  5.81835637e-02  1.47298888e+00], action=0, reward=1.0, next_state=[-0.02015763 -1.1682957   0.08764334  1.7832627 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 164 ][ timestamp 7 ] state=[-0.02015763 -1.1682957   0.08764334  1.7832627 ], action=0, reward=1.0, next_state=[-0.04352354 -1.36428672  0.1233086   2.10185502]\n",
      "[ Experience replay ] starts\n",
      "[ episode 164 ][ timestamp 8 ] state=[-0.04352354 -1.36428672  0.1233086   2.10185502], action=0, reward=1.0, next_state=[-0.07080927 -1.56041201  0.1653457   2.42997014]\n",
      "[ Experience replay ] starts\n",
      "[ episode 164 ][ timestamp 9 ] state=[-0.07080927 -1.56041201  0.1653457   2.42997014], action=0, reward=-1.0, next_state=[-0.10201751 -1.75652534  0.2139451   2.76851855]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 164: Exploration_rate=0.01. Score=9.\n",
      "[ episode 165 ] state=[ 0.04364593 -0.0403509   0.04354138  0.00671852]\n",
      "[ episode 165 ][ timestamp 1 ] state=[ 0.04364593 -0.0403509   0.04354138  0.00671852], action=0, reward=1.0, next_state=[ 0.04283891 -0.23606937  0.04367575  0.3128151 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 165 ][ timestamp 2 ] state=[ 0.04283891 -0.23606937  0.04367575  0.3128151 ], action=0, reward=1.0, next_state=[ 0.03811752 -0.43178543  0.04993205  0.61894582]\n",
      "[ Experience replay ] starts\n",
      "[ episode 165 ][ timestamp 3 ] state=[ 0.03811752 -0.43178543  0.04993205  0.61894582], action=0, reward=1.0, next_state=[ 0.02948181 -0.62756795  0.06231097  0.9269275 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 165 ][ timestamp 4 ] state=[ 0.02948181 -0.62756795  0.06231097  0.9269275 ], action=0, reward=1.0, next_state=[ 0.01693045 -0.82347344  0.08084952  1.23852302]\n",
      "[ Experience replay ] starts\n",
      "[ episode 165 ][ timestamp 5 ] state=[ 0.01693045 -0.82347344  0.08084952  1.23852302], action=0, reward=1.0, next_state=[ 4.60985507e-04 -1.01953541e+00  1.05619980e-01  1.55539918e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 165 ][ timestamp 6 ] state=[ 4.60985507e-04 -1.01953541e+00  1.05619980e-01  1.55539918e+00], action=0, reward=1.0, next_state=[-0.01992972 -1.21575247  0.13672796  1.87907918]\n",
      "[ Experience replay ] starts\n",
      "[ episode 165 ][ timestamp 7 ] state=[-0.01992972 -1.21575247  0.13672796  1.87907918], action=0, reward=1.0, next_state=[-0.04424477 -1.41207454  0.17430955  2.21088685]\n",
      "[ Experience replay ] starts\n",
      "[ episode 165 ][ timestamp 8 ] state=[-0.04424477 -1.41207454  0.17430955  2.21088685], action=0, reward=-1.0, next_state=[-0.07248626 -1.60838693  0.21852728  2.5518811 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 165: Exploration_rate=0.01. Score=8.\n",
      "[ episode 166 ] state=[ 0.03171047  0.03692297  0.00213741 -0.02982244]\n",
      "[ episode 166 ][ timestamp 1 ] state=[ 0.03171047  0.03692297  0.00213741 -0.02982244], action=0, reward=1.0, next_state=[ 0.03244893 -0.15822956  0.00154096  0.2635341 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 166 ][ timestamp 2 ] state=[ 0.03244893 -0.15822956  0.00154096  0.2635341 ], action=0, reward=1.0, next_state=[ 0.02928433 -0.35337348  0.00681164  0.55670266]\n",
      "[ Experience replay ] starts\n",
      "[ episode 166 ][ timestamp 3 ] state=[ 0.02928433 -0.35337348  0.00681164  0.55670266], action=0, reward=1.0, next_state=[ 0.02221686 -0.54859039  0.0179457   0.85152385]\n",
      "[ Experience replay ] starts\n",
      "[ episode 166 ][ timestamp 4 ] state=[ 0.02221686 -0.54859039  0.0179457   0.85152385], action=0, reward=1.0, next_state=[ 0.01124506 -0.74395236  0.03497618  1.14979537]\n",
      "[ Experience replay ] starts\n",
      "[ episode 166 ][ timestamp 5 ] state=[ 0.01124506 -0.74395236  0.03497618  1.14979537], action=0, reward=1.0, next_state=[-0.00363399 -0.93951291  0.05797208  1.45323768]\n",
      "[ Experience replay ] starts\n",
      "[ episode 166 ][ timestamp 6 ] state=[-0.00363399 -0.93951291  0.05797208  1.45323768], action=0, reward=1.0, next_state=[-0.02242425 -1.13529693  0.08703684  1.76345462]\n",
      "[ Experience replay ] starts\n",
      "[ episode 166 ][ timestamp 7 ] state=[-0.02242425 -1.13529693  0.08703684  1.76345462], action=0, reward=1.0, next_state=[-0.04513019 -1.33128873  0.12230593  2.08188602]\n",
      "[ Experience replay ] starts\n",
      "[ episode 166 ][ timestamp 8 ] state=[-0.04513019 -1.33128873  0.12230593  2.08188602], action=0, reward=1.0, next_state=[-0.07175596 -1.5274178   0.16394365  2.40975034]\n",
      "[ Experience replay ] starts\n",
      "[ episode 166 ][ timestamp 9 ] state=[-0.07175596 -1.5274178   0.16394365  2.40975034], action=0, reward=-1.0, next_state=[-0.10230432 -1.72354215  0.21213866  2.74797601]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 166: Exploration_rate=0.01. Score=9.\n",
      "[ episode 167 ] state=[-0.04823959 -0.04532804 -0.01390444 -0.00476109]\n",
      "[ episode 167 ][ timestamp 1 ] state=[-0.04823959 -0.04532804 -0.01390444 -0.00476109], action=0, reward=1.0, next_state=[-0.04914615 -0.24024785 -0.01399966  0.28350259]\n",
      "[ Experience replay ] starts\n",
      "[ episode 167 ][ timestamp 2 ] state=[-0.04914615 -0.24024785 -0.01399966  0.28350259], action=0, reward=1.0, next_state=[-0.0539511  -0.43516736 -0.00832961  0.57173743]\n",
      "[ Experience replay ] starts\n",
      "[ episode 167 ][ timestamp 3 ] state=[-0.0539511  -0.43516736 -0.00832961  0.57173743], action=0, reward=1.0, next_state=[-0.06265445 -0.63017152  0.00310514  0.86178465]\n",
      "[ Experience replay ] starts\n",
      "[ episode 167 ][ timestamp 4 ] state=[-0.06265445 -0.63017152  0.00310514  0.86178465], action=0, reward=1.0, next_state=[-0.07525788 -0.82533562  0.02034083  1.15544229]\n",
      "[ Experience replay ] starts\n",
      "[ episode 167 ][ timestamp 5 ] state=[-0.07525788 -0.82533562  0.02034083  1.15544229], action=0, reward=1.0, next_state=[-0.09176459 -1.0207168   0.04344967  1.45443323]\n",
      "[ Experience replay ] starts\n",
      "[ episode 167 ][ timestamp 6 ] state=[-0.09176459 -1.0207168   0.04344967  1.45443323], action=0, reward=1.0, next_state=[-0.11217893 -1.21634445  0.07253834  1.76036794]\n",
      "[ Experience replay ] starts\n",
      "[ episode 167 ][ timestamp 7 ] state=[-0.11217893 -1.21634445  0.07253834  1.76036794], action=0, reward=1.0, next_state=[-0.13650582 -1.41220867  0.1077457   2.07469923]\n",
      "[ Experience replay ] starts\n",
      "[ episode 167 ][ timestamp 8 ] state=[-0.13650582 -1.41220867  0.1077457   2.07469923], action=0, reward=1.0, next_state=[-0.16474999 -1.60824645  0.14923968  2.39866666]\n",
      "[ Experience replay ] starts\n",
      "[ episode 167 ][ timestamp 9 ] state=[-0.16474999 -1.60824645  0.14923968  2.39866666], action=0, reward=1.0, next_state=[-0.19691492 -1.80432528  0.19721302  2.73322938]\n",
      "[ Experience replay ] starts\n",
      "[ episode 167 ][ timestamp 10 ] state=[-0.19691492 -1.80432528  0.19721302  2.73322938], action=0, reward=-1.0, next_state=[-0.23300143 -2.00022443  0.2518776   3.07898779]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Ended! ] Episode 167: Exploration_rate=0.01. Score=10.\n",
      "[ episode 168 ] state=[ 0.00313351 -0.00710415  0.03041037  0.03271747]\n",
      "[ episode 168 ][ timestamp 1 ] state=[ 0.00313351 -0.00710415  0.03041037  0.03271747], action=0, reward=1.0, next_state=[ 0.00299143 -0.2026487   0.03106472  0.33483795]\n",
      "[ Experience replay ] starts\n",
      "[ episode 168 ][ timestamp 2 ] state=[ 0.00299143 -0.2026487   0.03106472  0.33483795], action=0, reward=1.0, next_state=[-0.00106154 -0.39819868  0.03776148  0.63715296]\n",
      "[ Experience replay ] starts\n",
      "[ episode 168 ][ timestamp 3 ] state=[-0.00106154 -0.39819868  0.03776148  0.63715296], action=0, reward=1.0, next_state=[-0.00902552 -0.59382632  0.05050453  0.94148446]\n",
      "[ Experience replay ] starts\n",
      "[ episode 168 ][ timestamp 4 ] state=[-0.00902552 -0.59382632  0.05050453  0.94148446], action=0, reward=1.0, next_state=[-0.02090204 -0.78959118  0.06933422  1.24959936]\n",
      "[ Experience replay ] starts\n",
      "[ episode 168 ][ timestamp 5 ] state=[-0.02090204 -0.78959118  0.06933422  1.24959936], action=0, reward=1.0, next_state=[-0.03669387 -0.98552995  0.09432621  1.56316929]\n",
      "[ Experience replay ] starts\n",
      "[ episode 168 ][ timestamp 6 ] state=[-0.03669387 -0.98552995  0.09432621  1.56316929], action=0, reward=1.0, next_state=[-0.05640447 -1.18164484  0.1255896   1.8837247 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 168 ][ timestamp 7 ] state=[-0.05640447 -1.18164484  0.1255896   1.8837247 ], action=0, reward=1.0, next_state=[-0.08003736 -1.37789012  0.16326409  2.21260053]\n",
      "[ Experience replay ] starts\n",
      "[ episode 168 ][ timestamp 8 ] state=[-0.08003736 -1.37789012  0.16326409  2.21260053], action=0, reward=1.0, next_state=[-0.10759517 -1.5741564   0.2075161   2.55087172]\n",
      "[ Experience replay ] starts\n",
      "[ episode 168 ][ timestamp 9 ] state=[-0.10759517 -1.5741564   0.2075161   2.55087172], action=0, reward=-1.0, next_state=[-0.13907829 -1.77025274  0.25853354  2.89927835]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 168: Exploration_rate=0.01. Score=9.\n",
      "[ episode 169 ] state=[ 0.01762523 -0.02873718  0.00261088  0.02861057]\n",
      "[ episode 169 ][ timestamp 1 ] state=[ 0.01762523 -0.02873718  0.00261088  0.02861057], action=0, reward=1.0, next_state=[ 0.01705048 -0.22389648  0.00318309  0.32211611]\n",
      "[ Experience replay ] starts\n",
      "[ episode 169 ][ timestamp 2 ] state=[ 0.01705048 -0.22389648  0.00318309  0.32211611], action=0, reward=1.0, next_state=[ 0.01257255 -0.41906361  0.00962541  0.61580116]\n",
      "[ Experience replay ] starts\n",
      "[ episode 169 ][ timestamp 3 ] state=[ 0.01257255 -0.41906361  0.00962541  0.61580116], action=0, reward=1.0, next_state=[ 0.00419128 -0.61431871  0.02194144  0.91150007]\n",
      "[ Experience replay ] starts\n",
      "[ episode 169 ][ timestamp 4 ] state=[ 0.00419128 -0.61431871  0.02194144  0.91150007], action=0, reward=1.0, next_state=[-0.00809509 -0.80973057  0.04017144  1.21099756]\n",
      "[ Experience replay ] starts\n",
      "[ episode 169 ][ timestamp 5 ] state=[-0.00809509 -0.80973057  0.04017144  1.21099756], action=0, reward=1.0, next_state=[-0.02428971 -1.00534747  0.06439139  1.51599342]\n",
      "[ Experience replay ] starts\n",
      "[ episode 169 ][ timestamp 6 ] state=[-0.02428971 -1.00534747  0.06439139  1.51599342], action=0, reward=1.0, next_state=[-0.04439665 -1.20118673  0.09471126  1.82806151]\n",
      "[ Experience replay ] starts\n",
      "[ episode 169 ][ timestamp 7 ] state=[-0.04439665 -1.20118673  0.09471126  1.82806151], action=0, reward=1.0, next_state=[-0.06842039 -1.39722226  0.13127249  2.14860042]\n",
      "[ Experience replay ] starts\n",
      "[ episode 169 ][ timestamp 8 ] state=[-0.06842039 -1.39722226  0.13127249  2.14860042], action=0, reward=1.0, next_state=[-0.09636483 -1.59336985  0.1742445   2.47877373]\n",
      "[ Experience replay ] starts\n",
      "[ episode 169 ][ timestamp 9 ] state=[-0.09636483 -1.59336985  0.1742445   2.47877373], action=0, reward=-1.0, next_state=[-0.12823223 -1.78946997  0.22381997  2.81943888]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 169: Exploration_rate=0.01. Score=9.\n",
      "[ episode 170 ] state=[-0.03487704 -0.03487494  0.01501005  0.00505931]\n",
      "[ episode 170 ][ timestamp 1 ] state=[-0.03487704 -0.03487494  0.01501005  0.00505931], action=0, reward=1.0, next_state=[-0.03557454 -0.2302089   0.01511123  0.30244004]\n",
      "[ Experience replay ] starts\n",
      "[ episode 170 ][ timestamp 2 ] state=[-0.03557454 -0.2302089   0.01511123  0.30244004], action=0, reward=1.0, next_state=[-0.04017872 -0.42554292  0.02116003  0.59985015]\n",
      "[ Experience replay ] starts\n",
      "[ episode 170 ][ timestamp 3 ] state=[-0.04017872 -0.42554292  0.02116003  0.59985015], action=0, reward=1.0, next_state=[-0.04868958 -0.62095442  0.03315703  0.89912236]\n",
      "[ Experience replay ] starts\n",
      "[ episode 170 ][ timestamp 4 ] state=[-0.04868958 -0.62095442  0.03315703  0.89912236], action=0, reward=1.0, next_state=[-0.06110866 -0.81650967  0.05113948  1.2020404 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 170 ][ timestamp 5 ] state=[-0.06110866 -0.81650967  0.05113948  1.2020404 ], action=0, reward=1.0, next_state=[-0.07743886 -1.01225428  0.07518029  1.5103019 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 170 ][ timestamp 6 ] state=[-0.07743886 -1.01225428  0.07518029  1.5103019 ], action=0, reward=1.0, next_state=[-0.09768394 -1.20820234  0.10538633  1.82547593]\n",
      "[ Experience replay ] starts\n",
      "[ episode 170 ][ timestamp 7 ] state=[-0.09768394 -1.20820234  0.10538633  1.82547593], action=0, reward=1.0, next_state=[-0.12184799 -1.40432374  0.14189585  2.14895218]\n",
      "[ Experience replay ] starts\n",
      "[ episode 170 ][ timestamp 8 ] state=[-0.12184799 -1.40432374  0.14189585  2.14895218], action=0, reward=1.0, next_state=[-0.14993446 -1.6005292   0.18487489  2.48188   ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 170 ][ timestamp 9 ] state=[-0.14993446 -1.6005292   0.18487489  2.48188   ], action=0, reward=-1.0, next_state=[-0.18194505 -1.79665292  0.23451249  2.82509657]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 170: Exploration_rate=0.01. Score=9.\n",
      "[ episode 171 ] state=[0.00828687 0.0323487  0.00601726 0.03939064]\n",
      "[ episode 171 ][ timestamp 1 ] state=[0.00828687 0.0323487  0.00601726 0.03939064], action=0, reward=1.0, next_state=[ 0.00893385 -0.16285902  0.00680507  0.33396598]\n",
      "[ Experience replay ] starts\n",
      "[ episode 171 ][ timestamp 2 ] state=[ 0.00893385 -0.16285902  0.00680507  0.33396598], action=0, reward=1.0, next_state=[ 0.00567667 -0.35807716  0.01348439  0.62878709]\n",
      "[ Experience replay ] starts\n",
      "[ episode 171 ][ timestamp 3 ] state=[ 0.00567667 -0.35807716  0.01348439  0.62878709], action=0, reward=1.0, next_state=[-0.00148488 -0.55338467  0.02606013  0.92568602]\n",
      "[ Experience replay ] starts\n",
      "[ episode 171 ][ timestamp 4 ] state=[-0.00148488 -0.55338467  0.02606013  0.92568602], action=0, reward=1.0, next_state=[-0.01255257 -0.7488487   0.04457385  1.22644332]\n",
      "[ Experience replay ] starts\n",
      "[ episode 171 ][ timestamp 5 ] state=[-0.01255257 -0.7488487   0.04457385  1.22644332], action=0, reward=1.0, next_state=[-0.02752954 -0.94451524  0.06910272  1.53275199]\n",
      "[ Experience replay ] starts\n",
      "[ episode 171 ][ timestamp 6 ] state=[-0.02752954 -0.94451524  0.06910272  1.53275199], action=0, reward=1.0, next_state=[-0.04641985 -1.14039848  0.09975776  1.84617562]\n",
      "[ Experience replay ] starts\n",
      "[ episode 171 ][ timestamp 7 ] state=[-0.04641985 -1.14039848  0.09975776  1.84617562], action=0, reward=1.0, next_state=[-0.06922782 -1.33646817  0.13668127  2.16809812]\n",
      "[ Experience replay ] starts\n",
      "[ episode 171 ][ timestamp 8 ] state=[-0.06922782 -1.33646817  0.13668127  2.16809812], action=0, reward=1.0, next_state=[-0.09595718 -1.53263465  0.18004323  2.49966286]\n",
      "[ Experience replay ] starts\n",
      "[ episode 171 ][ timestamp 9 ] state=[-0.09595718 -1.53263465  0.18004323  2.49966286], action=0, reward=-1.0, next_state=[-0.12660987 -1.72873152  0.23003649  2.84170079]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 171: Exploration_rate=0.01. Score=9.\n",
      "[ episode 172 ] state=[-0.02081527  0.03717552 -0.01362286  0.02733094]\n",
      "[ episode 172 ][ timestamp 1 ] state=[-0.02081527  0.03717552 -0.01362286  0.02733094], action=0, reward=1.0, next_state=[-0.02007176 -0.15774845 -0.01307624  0.31568477]\n",
      "[ Experience replay ] starts\n",
      "[ episode 172 ][ timestamp 2 ] state=[-0.02007176 -0.15774845 -0.01307624  0.31568477], action=0, reward=1.0, next_state=[-0.02322673 -0.35268172 -0.00676255  0.60421537]\n",
      "[ Experience replay ] starts\n",
      "[ episode 172 ][ timestamp 3 ] state=[-0.02322673 -0.35268172 -0.00676255  0.60421537], action=0, reward=1.0, next_state=[-0.03028036 -0.54770845  0.00532176  0.8947606 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 172 ][ timestamp 4 ] state=[-0.03028036 -0.54770845  0.00532176  0.8947606 ], action=0, reward=1.0, next_state=[-0.04123453 -0.74290216  0.02321697  1.18911161]\n",
      "[ Experience replay ] starts\n",
      "[ episode 172 ][ timestamp 5 ] state=[-0.04123453 -0.74290216  0.02321697  1.18911161], action=0, reward=1.0, next_state=[-0.05609257 -0.93831722  0.0469992   1.48898038]\n",
      "[ Experience replay ] starts\n",
      "[ episode 172 ][ timestamp 6 ] state=[-0.05609257 -0.93831722  0.0469992   1.48898038], action=0, reward=1.0, next_state=[-0.07485892 -1.133979    0.07677881  1.79596163]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 172 ][ timestamp 7 ] state=[-0.07485892 -1.133979    0.07677881  1.79596163], action=0, reward=1.0, next_state=[-0.0975385  -1.32987205  0.11269804  2.11148634]\n",
      "[ Experience replay ] starts\n",
      "[ episode 172 ][ timestamp 8 ] state=[-0.0975385  -1.32987205  0.11269804  2.11148634], action=0, reward=1.0, next_state=[-0.12413594 -1.52592601  0.15492777  2.43676486]\n",
      "[ Experience replay ] starts\n",
      "[ episode 172 ][ timestamp 9 ] state=[-0.12413594 -1.52592601  0.15492777  2.43676486], action=0, reward=1.0, next_state=[-0.15465446 -1.72199895  0.20366307  2.77271839]\n",
      "[ Experience replay ] starts\n",
      "[ episode 172 ][ timestamp 10 ] state=[-0.15465446 -1.72199895  0.20366307  2.77271839], action=0, reward=-1.0, next_state=[-0.18909444 -1.91785833  0.25911744  3.11989938]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 172: Exploration_rate=0.01. Score=10.\n",
      "[ episode 173 ] state=[-0.04622466 -0.01658417  0.04646375  0.03353385]\n",
      "[ episode 173 ][ timestamp 1 ] state=[-0.04622466 -0.01658417  0.04646375  0.03353385], action=0, reward=1.0, next_state=[-0.04655634 -0.21234057  0.04713443  0.34050697]\n",
      "[ Experience replay ] starts\n",
      "[ episode 173 ][ timestamp 2 ] state=[-0.04655634 -0.21234057  0.04713443  0.34050697], action=0, reward=1.0, next_state=[-0.05080316 -0.40810037  0.05394457  0.64767294]\n",
      "[ Experience replay ] starts\n",
      "[ episode 173 ][ timestamp 3 ] state=[-0.05080316 -0.40810037  0.05394457  0.64767294], action=0, reward=1.0, next_state=[-0.05896516 -0.60393074  0.06689803  0.95684321]\n",
      "[ Experience replay ] starts\n",
      "[ episode 173 ][ timestamp 4 ] state=[-0.05896516 -0.60393074  0.06689803  0.95684321], action=0, reward=1.0, next_state=[-0.07104378 -0.79988546  0.08603489  1.26977117]\n",
      "[ Experience replay ] starts\n",
      "[ episode 173 ][ timestamp 5 ] state=[-0.07104378 -0.79988546  0.08603489  1.26977117], action=0, reward=1.0, next_state=[-0.08704149 -0.99599408  0.11143031  1.58810914]\n",
      "[ Experience replay ] starts\n",
      "[ episode 173 ][ timestamp 6 ] state=[-0.08704149 -0.99599408  0.11143031  1.58810914], action=0, reward=1.0, next_state=[-0.10696137 -1.19224973  0.1431925   1.91335961]\n",
      "[ Experience replay ] starts\n",
      "[ episode 173 ][ timestamp 7 ] state=[-0.10696137 -1.19224973  0.1431925   1.91335961], action=0, reward=1.0, next_state=[-0.13080636 -1.38859505  0.18145969  2.24681822]\n",
      "[ Experience replay ] starts\n",
      "[ episode 173 ][ timestamp 8 ] state=[-0.13080636 -1.38859505  0.18145969  2.24681822], action=0, reward=-1.0, next_state=[-0.15857826 -1.58490603  0.22639605  2.5895068 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 173: Exploration_rate=0.01. Score=8.\n",
      "[ episode 174 ] state=[-0.01478623 -0.00603343  0.00132159  0.04332149]\n",
      "[ episode 174 ][ timestamp 1 ] state=[-0.01478623 -0.00603343  0.00132159  0.04332149], action=0, reward=1.0, next_state=[-0.0149069  -0.20117431  0.00218802  0.3364211 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 174 ][ timestamp 2 ] state=[-0.0149069  -0.20117431  0.00218802  0.3364211 ], action=0, reward=1.0, next_state=[-0.01893039 -0.39632733  0.00891644  0.6297932 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 174 ][ timestamp 3 ] state=[-0.01893039 -0.39632733  0.00891644  0.6297932 ], action=0, reward=1.0, next_state=[-0.02685693 -0.59157256  0.0215123   0.92527081]\n",
      "[ Experience replay ] starts\n",
      "[ episode 174 ][ timestamp 4 ] state=[-0.02685693 -0.59157256  0.0215123   0.92527081], action=0, reward=1.0, next_state=[-0.03868839 -0.78697835  0.04001772  1.2246358 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 174 ][ timestamp 5 ] state=[-0.03868839 -0.78697835  0.04001772  1.2246358 ], action=0, reward=1.0, next_state=[-0.05442795 -0.98259216  0.06451043  1.52958367]\n",
      "[ Experience replay ] starts\n",
      "[ episode 174 ][ timestamp 6 ] state=[-0.05442795 -0.98259216  0.06451043  1.52958367], action=0, reward=1.0, next_state=[-0.0740798  -1.17843002  0.09510211  1.84168234]\n",
      "[ Experience replay ] starts\n",
      "[ episode 174 ][ timestamp 7 ] state=[-0.0740798  -1.17843002  0.09510211  1.84168234], action=0, reward=1.0, next_state=[-0.0976484  -1.3744641   0.13193575  2.16232259]\n",
      "[ Experience replay ] starts\n",
      "[ episode 174 ][ timestamp 8 ] state=[-0.0976484  -1.3744641   0.13193575  2.16232259], action=0, reward=1.0, next_state=[-0.12513768 -1.57060785  0.17518221  2.49265789]\n",
      "[ Experience replay ] starts\n",
      "[ episode 174 ][ timestamp 9 ] state=[-0.12513768 -1.57060785  0.17518221  2.49265789], action=0, reward=-1.0, next_state=[-0.15654984 -1.76669874  0.22503536  2.83353296]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 174: Exploration_rate=0.01. Score=9.\n",
      "[ episode 175 ] state=[-0.01061661  0.00147769 -0.0350189   0.01378915]\n",
      "[ episode 175 ][ timestamp 1 ] state=[-0.01061661  0.00147769 -0.0350189   0.01378915], action=0, reward=1.0, next_state=[-0.01058705 -0.193125   -0.03474312  0.29522077]\n",
      "[ Experience replay ] starts\n",
      "[ episode 175 ][ timestamp 2 ] state=[-0.01058705 -0.193125   -0.03474312  0.29522077], action=0, reward=1.0, next_state=[-0.01444955 -0.38773486 -0.0288387   0.57674697]\n",
      "[ Experience replay ] starts\n",
      "[ episode 175 ][ timestamp 3 ] state=[-0.01444955 -0.38773486 -0.0288387   0.57674697], action=0, reward=1.0, next_state=[-0.02220425 -0.58244096 -0.01730376  0.86020728]\n",
      "[ Experience replay ] starts\n",
      "[ episode 175 ][ timestamp 4 ] state=[-0.02220425 -0.58244096 -0.01730376  0.86020728], action=0, reward=1.0, next_state=[-3.38530682e-02 -7.77323024e-01 -9.96157525e-05  1.14739956e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 175 ][ timestamp 5 ] state=[-3.38530682e-02 -7.77323024e-01 -9.96157525e-05  1.14739956e+00], action=0, reward=1.0, next_state=[-0.04939953 -0.97244367  0.02284838  1.44005125]\n",
      "[ Experience replay ] starts\n",
      "[ episode 175 ][ timestamp 6 ] state=[-0.04939953 -0.97244367  0.02284838  1.44005125], action=0, reward=1.0, next_state=[-0.0688484  -1.16783951  0.0516494   1.73978533]\n",
      "[ Experience replay ] starts\n",
      "[ episode 175 ][ timestamp 7 ] state=[-0.0688484  -1.16783951  0.0516494   1.73978533], action=0, reward=1.0, next_state=[-0.09220519 -1.36351025  0.08644511  2.04807823]\n",
      "[ Experience replay ] starts\n",
      "[ episode 175 ][ timestamp 8 ] state=[-0.09220519 -1.36351025  0.08644511  2.04807823], action=0, reward=1.0, next_state=[-0.1194754  -1.55940562  0.12740667  2.36620727]\n",
      "[ Experience replay ] starts\n",
      "[ episode 175 ][ timestamp 9 ] state=[-0.1194754  -1.55940562  0.12740667  2.36620727], action=0, reward=1.0, next_state=[-0.15066351 -1.75540945  0.17473082  2.69518634]\n",
      "[ Experience replay ] starts\n",
      "[ episode 175 ][ timestamp 10 ] state=[-0.15066351 -1.75540945  0.17473082  2.69518634], action=0, reward=-1.0, next_state=[-0.1857717  -1.95132138  0.22863454  3.03568947]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 175: Exploration_rate=0.01. Score=10.\n",
      "[ episode 176 ] state=[-0.02436186 -0.02133845 -0.02481327 -0.04995214]\n",
      "[ episode 176 ][ timestamp 1 ] state=[-0.02436186 -0.02133845 -0.02481327 -0.04995214], action=0, reward=1.0, next_state=[-0.02478863 -0.21609598 -0.02581231  0.23479987]\n",
      "[ Experience replay ] starts\n",
      "[ episode 176 ][ timestamp 2 ] state=[-0.02478863 -0.21609598 -0.02581231  0.23479987], action=0, reward=1.0, next_state=[-0.02911055 -0.4108398  -0.02111631  0.51923032]\n",
      "[ Experience replay ] starts\n",
      "[ episode 176 ][ timestamp 3 ] state=[-0.02911055 -0.4108398  -0.02111631  0.51923032], action=0, reward=1.0, next_state=[-0.03732735 -0.6056582  -0.01073171  0.80518504]\n",
      "[ Experience replay ] starts\n",
      "[ episode 176 ][ timestamp 4 ] state=[-0.03732735 -0.6056582  -0.01073171  0.80518504], action=0, reward=1.0, next_state=[-0.04944051 -0.8006314   0.005372    1.09447293]\n",
      "[ Experience replay ] starts\n",
      "[ episode 176 ][ timestamp 5 ] state=[-0.04944051 -0.8006314   0.005372    1.09447293], action=0, reward=1.0, next_state=[-0.06545314 -0.9958237   0.02726145  1.38883652]\n",
      "[ Experience replay ] starts\n",
      "[ episode 176 ][ timestamp 6 ] state=[-0.06545314 -0.9958237   0.02726145  1.38883652], action=0, reward=1.0, next_state=[-0.08536961 -1.19127451  0.05503818  1.68991767]\n",
      "[ Experience replay ] starts\n",
      "[ episode 176 ][ timestamp 7 ] state=[-0.08536961 -1.19127451  0.05503818  1.68991767], action=0, reward=1.0, next_state=[-0.1091951  -1.38698759  0.08883654  1.99921582]\n",
      "[ Experience replay ] starts\n",
      "[ episode 176 ][ timestamp 8 ] state=[-0.1091951  -1.38698759  0.08883654  1.99921582], action=0, reward=1.0, next_state=[-0.13693486 -1.5829181   0.12882085  2.31803625]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 176 ][ timestamp 9 ] state=[-0.13693486 -1.5829181   0.12882085  2.31803625], action=0, reward=1.0, next_state=[-0.16859322 -1.77895699  0.17518158  2.6474267 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 176 ][ timestamp 10 ] state=[-0.16859322 -1.77895699  0.17518158  2.6474267 ], action=0, reward=-1.0, next_state=[-0.20417236 -1.97491292  0.22813011  2.98810227]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 176: Exploration_rate=0.01. Score=10.\n",
      "[ episode 177 ] state=[-0.00364855  0.04819682  0.02226791 -0.03699246]\n",
      "[ episode 177 ][ timestamp 1 ] state=[-0.00364855  0.04819682  0.02226791 -0.03699246], action=0, reward=1.0, next_state=[-0.00268462 -0.14723726  0.02152806  0.26263221]\n",
      "[ Experience replay ] starts\n",
      "[ episode 177 ][ timestamp 2 ] state=[-0.00268462 -0.14723726  0.02152806  0.26263221], action=0, reward=1.0, next_state=[-0.00562936 -0.34265978  0.02678071  0.56202684]\n",
      "[ Experience replay ] starts\n",
      "[ episode 177 ][ timestamp 3 ] state=[-0.00562936 -0.34265978  0.02678071  0.56202684], action=0, reward=1.0, next_state=[-0.01248256 -0.53814712  0.03802124  0.86302528]\n",
      "[ Experience replay ] starts\n",
      "[ episode 177 ][ timestamp 4 ] state=[-0.01248256 -0.53814712  0.03802124  0.86302528], action=0, reward=1.0, next_state=[-0.0232455  -0.73376552  0.05528175  1.16741637]\n",
      "[ Experience replay ] starts\n",
      "[ episode 177 ][ timestamp 5 ] state=[-0.0232455  -0.73376552  0.05528175  1.16741637], action=0, reward=1.0, next_state=[-0.03792081 -0.92956147  0.07863008  1.47690619]\n",
      "[ Experience replay ] starts\n",
      "[ episode 177 ][ timestamp 6 ] state=[-0.03792081 -0.92956147  0.07863008  1.47690619], action=0, reward=1.0, next_state=[-0.05651204 -1.12555081  0.1081682   1.7930753 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 177 ][ timestamp 7 ] state=[-0.05651204 -1.12555081  0.1081682   1.7930753 ], action=0, reward=1.0, next_state=[-0.07902306 -1.32170607  0.14402971  2.11732802]\n",
      "[ Experience replay ] starts\n",
      "[ episode 177 ][ timestamp 8 ] state=[-0.07902306 -1.32170607  0.14402971  2.11732802], action=0, reward=1.0, next_state=[-0.10545718 -1.51794159  0.18637627  2.45083194]\n",
      "[ Experience replay ] starts\n",
      "[ episode 177 ][ timestamp 9 ] state=[-0.10545718 -1.51794159  0.18637627  2.45083194], action=0, reward=-1.0, next_state=[-0.13581601 -1.7140963   0.2353929   2.79444649]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 177: Exploration_rate=0.01. Score=9.\n",
      "[ episode 178 ] state=[-0.03405801 -0.0408816   0.03339777 -0.01129282]\n",
      "[ episode 178 ][ timestamp 1 ] state=[-0.03405801 -0.0408816   0.03339777 -0.01129282], action=0, reward=1.0, next_state=[-0.03487564 -0.23646621  0.03317191  0.2917376 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 178 ][ timestamp 2 ] state=[-0.03487564 -0.23646621  0.03317191  0.2917376 ], action=0, reward=1.0, next_state=[-0.03960496 -0.43204505  0.03900666  0.59469522]\n",
      "[ Experience replay ] starts\n",
      "[ episode 178 ][ timestamp 3 ] state=[-0.03960496 -0.43204505  0.03900666  0.59469522], action=0, reward=1.0, next_state=[-0.04824586 -0.62769062  0.05090057  0.8994054 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 178 ][ timestamp 4 ] state=[-0.04824586 -0.62769062  0.05090057  0.8994054 ], action=0, reward=1.0, next_state=[-0.06079968 -0.82346407  0.06888868  1.20764354]\n",
      "[ Experience replay ] starts\n",
      "[ episode 178 ][ timestamp 5 ] state=[-0.06079968 -0.82346407  0.06888868  1.20764354], action=0, reward=1.0, next_state=[-0.07726896 -1.01940499  0.09304155  1.52109505]\n",
      "[ Experience replay ] starts\n",
      "[ episode 178 ][ timestamp 6 ] state=[-0.07726896 -1.01940499  0.09304155  1.52109505], action=0, reward=1.0, next_state=[-0.09765706 -1.21552002  0.12346345  1.84131   ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 178 ][ timestamp 7 ] state=[-0.09765706 -1.21552002  0.12346345  1.84131   ], action=0, reward=1.0, next_state=[-0.12196746 -1.41176955  0.16028965  2.16964964]\n",
      "[ Experience replay ] starts\n",
      "[ episode 178 ][ timestamp 8 ] state=[-0.12196746 -1.41176955  0.16028965  2.16964964], action=0, reward=1.0, next_state=[-0.15020285 -1.60805227  0.20368264  2.50722315]\n",
      "[ Experience replay ] starts\n",
      "[ episode 178 ][ timestamp 9 ] state=[-0.15020285 -1.60805227  0.20368264  2.50722315], action=0, reward=-1.0, next_state=[-0.18236389 -1.80418747  0.2538271   2.85481378]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 178: Exploration_rate=0.01. Score=9.\n",
      "[ episode 179 ] state=[ 0.01770708 -0.02460655  0.0040765  -0.02219265]\n",
      "[ episode 179 ][ timestamp 1 ] state=[ 0.01770708 -0.02460655  0.0040765  -0.02219265], action=0, reward=1.0, next_state=[ 0.01721495 -0.21978672  0.00363265  0.27177367]\n",
      "[ Experience replay ] starts\n",
      "[ episode 179 ][ timestamp 2 ] state=[ 0.01721495 -0.21978672  0.00363265  0.27177367], action=0, reward=1.0, next_state=[ 0.01281921 -0.41496032  0.00906813  0.56560013]\n",
      "[ Experience replay ] starts\n",
      "[ episode 179 ][ timestamp 3 ] state=[ 0.01281921 -0.41496032  0.00906813  0.56560013], action=0, reward=1.0, next_state=[ 0.00452001 -0.61020831  0.02038013  0.86112607]\n",
      "[ Experience replay ] starts\n",
      "[ episode 179 ][ timestamp 4 ] state=[ 0.00452001 -0.61020831  0.02038013  0.86112607], action=0, reward=1.0, next_state=[-0.00768416 -0.80560178  0.03760265  1.16014675]\n",
      "[ Experience replay ] starts\n",
      "[ episode 179 ][ timestamp 5 ] state=[-0.00768416 -0.80560178  0.03760265  1.16014675], action=0, reward=1.0, next_state=[-0.02379619 -1.00119291  0.06080558  1.46437862]\n",
      "[ Experience replay ] starts\n",
      "[ episode 179 ][ timestamp 6 ] state=[-0.02379619 -1.00119291  0.06080558  1.46437862], action=0, reward=1.0, next_state=[-0.04382005 -1.19700471  0.09009316  1.77541934]\n",
      "[ Experience replay ] starts\n",
      "[ episode 179 ][ timestamp 7 ] state=[-0.04382005 -1.19700471  0.09009316  1.77541934], action=0, reward=1.0, next_state=[-0.06776015 -1.39301897  0.12560154  2.09469985]\n",
      "[ Experience replay ] starts\n",
      "[ episode 179 ][ timestamp 8 ] state=[-0.06776015 -1.39301897  0.12560154  2.09469985], action=0, reward=1.0, next_state=[-0.09562053 -1.58916186  0.16749554  2.42342635]\n",
      "[ Experience replay ] starts\n",
      "[ episode 179 ][ timestamp 9 ] state=[-0.09562053 -1.58916186  0.16749554  2.42342635], action=0, reward=-1.0, next_state=[-0.12740376 -1.78528719  0.21596407  2.76251105]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 179: Exploration_rate=0.01. Score=9.\n",
      "[ episode 180 ] state=[ 0.0445649   0.02762255  0.01477563 -0.04718067]\n",
      "[ episode 180 ][ timestamp 1 ] state=[ 0.0445649   0.02762255  0.01477563 -0.04718067], action=0, reward=1.0, next_state=[ 0.04511735 -0.16770812  0.01383201  0.25012723]\n",
      "[ Experience replay ] starts\n",
      "[ episode 180 ][ timestamp 2 ] state=[ 0.04511735 -0.16770812  0.01383201  0.25012723], action=0, reward=1.0, next_state=[ 0.04176319 -0.36302484  0.01883456  0.54714076]\n",
      "[ Experience replay ] starts\n",
      "[ episode 180 ][ timestamp 3 ] state=[ 0.04176319 -0.36302484  0.01883456  0.54714076], action=0, reward=1.0, next_state=[ 0.03450269 -0.55840627  0.02977737  0.84569796]\n",
      "[ Experience replay ] starts\n",
      "[ episode 180 ][ timestamp 4 ] state=[ 0.03450269 -0.55840627  0.02977737  0.84569796], action=0, reward=1.0, next_state=[ 0.02333457 -0.75392157  0.04669133  1.14759414]\n",
      "[ Experience replay ] starts\n",
      "[ episode 180 ][ timestamp 5 ] state=[ 0.02333457 -0.75392157  0.04669133  1.14759414], action=0, reward=1.0, next_state=[ 0.00825614 -0.949621    0.06964321  1.45454564]\n",
      "[ Experience replay ] starts\n",
      "[ episode 180 ][ timestamp 6 ] state=[ 0.00825614 -0.949621    0.06964321  1.45454564], action=0, reward=1.0, next_state=[-0.01073628 -1.14552546  0.09873413  1.76814855]\n",
      "[ Experience replay ] starts\n",
      "[ episode 180 ][ timestamp 7 ] state=[-0.01073628 -1.14552546  0.09873413  1.76814855], action=0, reward=1.0, next_state=[-0.03364679 -1.34161415  0.1340971   2.08982977]\n",
      "[ Experience replay ] starts\n",
      "[ episode 180 ][ timestamp 8 ] state=[-0.03364679 -1.34161415  0.1340971   2.08982977], action=0, reward=1.0, next_state=[-0.06047908 -1.53781001  0.17589369  2.42078802]\n",
      "[ Experience replay ] starts\n",
      "[ episode 180 ][ timestamp 9 ] state=[-0.06047908 -1.53781001  0.17589369  2.42078802], action=0, reward=-1.0, next_state=[-0.09123528 -1.73396288  0.22430945  2.76192403]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 180: Exploration_rate=0.01. Score=9.\n",
      "[ episode 181 ] state=[ 0.01111134 -0.02687508  0.02909443 -0.03170128]\n",
      "[ episode 181 ][ timestamp 1 ] state=[ 0.01111134 -0.02687508  0.02909443 -0.03170128], action=0, reward=1.0, next_state=[ 0.01057383 -0.22240191  0.02846041  0.27001741]\n",
      "[ Experience replay ] starts\n",
      "[ episode 181 ][ timestamp 2 ] state=[ 0.01057383 -0.22240191  0.02846041  0.27001741], action=0, reward=1.0, next_state=[ 0.0061258  -0.4179182   0.03386076  0.5715393 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 181 ][ timestamp 3 ] state=[ 0.0061258  -0.4179182   0.03386076  0.5715393 ], action=0, reward=1.0, next_state=[-0.00223257 -0.6134982   0.04529154  0.87469429]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 181 ][ timestamp 4 ] state=[-0.00223257 -0.6134982   0.04529154  0.87469429], action=0, reward=1.0, next_state=[-0.01450253 -0.80920566  0.06278543  1.1812656 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 181 ][ timestamp 5 ] state=[-0.01450253 -0.80920566  0.06278543  1.1812656 ], action=0, reward=1.0, next_state=[-0.03068664 -1.00508384  0.08641074  1.49295074]\n",
      "[ Experience replay ] starts\n",
      "[ episode 181 ][ timestamp 6 ] state=[-0.03068664 -1.00508384  0.08641074  1.49295074], action=0, reward=1.0, next_state=[-0.05078832 -1.20114437  0.11626975  1.8113174 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 181 ][ timestamp 7 ] state=[-0.05078832 -1.20114437  0.11626975  1.8113174 ], action=0, reward=1.0, next_state=[-0.07481121 -1.39735427  0.1524961   2.13775147]\n",
      "[ Experience replay ] starts\n",
      "[ episode 181 ][ timestamp 8 ] state=[-0.07481121 -1.39735427  0.1524961   2.13775147], action=0, reward=1.0, next_state=[-0.10275829 -1.59362083  0.19525113  2.47339509]\n",
      "[ Experience replay ] starts\n",
      "[ episode 181 ][ timestamp 9 ] state=[-0.10275829 -1.59362083  0.19525113  2.47339509], action=0, reward=-1.0, next_state=[-0.13463071 -1.78977414  0.24471903  2.81907416]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 181: Exploration_rate=0.01. Score=9.\n",
      "[ episode 182 ] state=[ 0.03812662 -0.02905343  0.01954024 -0.04512028]\n",
      "[ episode 182 ][ timestamp 1 ] state=[ 0.03812662 -0.02905343  0.01954024 -0.04512028], action=0, reward=1.0, next_state=[ 0.03754555 -0.22445004  0.01863784  0.25366316]\n",
      "[ Experience replay ] starts\n",
      "[ episode 182 ][ timestamp 2 ] state=[ 0.03754555 -0.22445004  0.01863784  0.25366316], action=0, reward=1.0, next_state=[ 0.03305655 -0.41983309  0.0237111   0.55216604]\n",
      "[ Experience replay ] starts\n",
      "[ episode 182 ][ timestamp 3 ] state=[ 0.03305655 -0.41983309  0.0237111   0.55216604], action=0, reward=1.0, next_state=[ 0.02465989 -0.61527988  0.03475442  0.85222421]\n",
      "[ Experience replay ] starts\n",
      "[ episode 182 ][ timestamp 4 ] state=[ 0.02465989 -0.61527988  0.03475442  0.85222421], action=0, reward=1.0, next_state=[ 0.01235429 -0.81085796  0.0517989   1.15562992]\n",
      "[ Experience replay ] starts\n",
      "[ episode 182 ][ timestamp 5 ] state=[ 0.01235429 -0.81085796  0.0517989   1.15562992], action=0, reward=1.0, next_state=[-0.00386287 -1.0066156   0.0749115   1.4640946 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 182 ][ timestamp 6 ] state=[-0.00386287 -1.0066156   0.0749115   1.4640946 ], action=0, reward=1.0, next_state=[-0.02399518 -1.20257105  0.1041934   1.77920681]\n",
      "[ Experience replay ] starts\n",
      "[ episode 182 ][ timestamp 7 ] state=[-0.02399518 -1.20257105  0.1041934   1.77920681], action=0, reward=1.0, next_state=[-0.0480466  -1.39870005  0.13977753  2.10238231]\n",
      "[ Experience replay ] starts\n",
      "[ episode 182 ][ timestamp 8 ] state=[-0.0480466  -1.39870005  0.13977753  2.10238231], action=0, reward=1.0, next_state=[-0.0760206  -1.59492113  0.18182518  2.43480422]\n",
      "[ Experience replay ] starts\n",
      "[ episode 182 ][ timestamp 9 ] state=[-0.0760206  -1.59492113  0.18182518  2.43480422], action=0, reward=-1.0, next_state=[-0.10791902 -1.79107848  0.23052126  2.77735239]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 182: Exploration_rate=0.01. Score=9.\n",
      "[ episode 183 ] state=[-0.0360021   0.00160784 -0.00931792 -0.01975688]\n",
      "[ episode 183 ][ timestamp 1 ] state=[-0.0360021   0.00160784 -0.00931792 -0.01975688], action=0, reward=1.0, next_state=[-0.03596995 -0.19337925 -0.00971305  0.26997163]\n",
      "[ Experience replay ] starts\n",
      "[ episode 183 ][ timestamp 2 ] state=[-0.03596995 -0.19337925 -0.00971305  0.26997163], action=0, reward=1.0, next_state=[-0.03983753 -0.38836126 -0.00431362  0.55957525]\n",
      "[ Experience replay ] starts\n",
      "[ episode 183 ][ timestamp 3 ] state=[-0.03983753 -0.38836126 -0.00431362  0.55957525], action=0, reward=1.0, next_state=[-0.04760476 -0.5834224   0.00687788  0.85089604]\n",
      "[ Experience replay ] starts\n",
      "[ episode 183 ][ timestamp 4 ] state=[-0.04760476 -0.5834224   0.00687788  0.85089604], action=0, reward=1.0, next_state=[-0.05927321 -0.77863745  0.0238958   1.14573378]\n",
      "[ Experience replay ] starts\n",
      "[ episode 183 ][ timestamp 5 ] state=[-0.05927321 -0.77863745  0.0238958   1.14573378], action=0, reward=1.0, next_state=[-0.07484595 -0.97406321  0.04681048  1.44581342]\n",
      "[ Experience replay ] starts\n",
      "[ episode 183 ][ timestamp 6 ] state=[-0.07484595 -0.97406321  0.04681048  1.44581342], action=0, reward=1.0, next_state=[-0.09432722 -1.16972873  0.07572675  1.75274746]\n",
      "[ Experience replay ] starts\n",
      "[ episode 183 ][ timestamp 7 ] state=[-0.09432722 -1.16972873  0.07572675  1.75274746], action=0, reward=1.0, next_state=[-0.11772179 -1.36562377  0.1107817   2.0679903 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 183 ][ timestamp 8 ] state=[-0.11772179 -1.36562377  0.1107817   2.0679903 ], action=0, reward=1.0, next_state=[-0.14503427 -1.56168492  0.1521415   2.39278247]\n",
      "[ Experience replay ] starts\n",
      "[ episode 183 ][ timestamp 9 ] state=[-0.14503427 -1.56168492  0.1521415   2.39278247], action=0, reward=1.0, next_state=[-0.17626797 -1.75777917  0.19999715  2.7280834 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 183 ][ timestamp 10 ] state=[-0.17626797 -1.75777917  0.19999715  2.7280834 ], action=0, reward=-1.0, next_state=[-0.21142355 -1.95368523  0.25455882  3.074493  ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 183: Exploration_rate=0.01. Score=10.\n",
      "[ episode 184 ] state=[-0.02334434 -0.02089062 -0.01269578 -0.04377504]\n",
      "[ episode 184 ][ timestamp 1 ] state=[-0.02334434 -0.02089062 -0.01269578 -0.04377504], action=0, reward=1.0, next_state=[-0.02376215 -0.21582824 -0.01357128  0.24487536]\n",
      "[ Experience replay ] starts\n",
      "[ episode 184 ][ timestamp 2 ] state=[-0.02376215 -0.21582824 -0.01357128  0.24487536], action=0, reward=1.0, next_state=[-0.02807872 -0.41075375 -0.00867377  0.53324687]\n",
      "[ Experience replay ] starts\n",
      "[ episode 184 ][ timestamp 3 ] state=[-0.02807872 -0.41075375 -0.00867377  0.53324687], action=0, reward=1.0, next_state=[-0.03629379 -0.60575264  0.00199116  0.82318415]\n",
      "[ Experience replay ] starts\n",
      "[ episode 184 ][ timestamp 4 ] state=[-0.03629379 -0.60575264  0.00199116  0.82318415], action=1, reward=1.0, next_state=[-0.04840884 -0.41065799  0.01845485  0.53112815]\n",
      "[ Experience replay ] starts\n",
      "[ episode 184 ][ timestamp 5 ] state=[-0.04840884 -0.41065799  0.01845485  0.53112815], action=0, reward=1.0, next_state=[-0.056622   -0.6060346   0.02907741  0.82956858]\n",
      "[ Experience replay ] starts\n",
      "[ episode 184 ][ timestamp 6 ] state=[-0.056622   -0.6060346   0.02907741  0.82956858], action=0, reward=1.0, next_state=[-0.06874269 -0.80154172  0.04566878  1.13125285]\n",
      "[ Experience replay ] starts\n",
      "[ episode 184 ][ timestamp 7 ] state=[-0.06874269 -0.80154172  0.04566878  1.13125285], action=0, reward=1.0, next_state=[-0.08477353 -0.99723087  0.06829384  1.43790249]\n",
      "[ Experience replay ] starts\n",
      "[ episode 184 ][ timestamp 8 ] state=[-0.08477353 -0.99723087  0.06829384  1.43790249], action=0, reward=1.0, next_state=[-0.10471815 -1.19312482  0.09705189  1.75112121]\n",
      "[ Experience replay ] starts\n",
      "[ episode 184 ][ timestamp 9 ] state=[-0.10471815 -1.19312482  0.09705189  1.75112121], action=0, reward=1.0, next_state=[-0.12858064 -1.38920531  0.13207431  2.07234634]\n",
      "[ Experience replay ] starts\n",
      "[ episode 184 ][ timestamp 10 ] state=[-0.12858064 -1.38920531  0.13207431  2.07234634], action=0, reward=1.0, next_state=[-0.15636475 -1.58539872  0.17352124  2.4027905 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 184 ][ timestamp 11 ] state=[-0.15636475 -1.58539872  0.17352124  2.4027905 ], action=0, reward=-1.0, next_state=[-0.18807272 -1.78155924  0.22157705  2.74337227]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 184: Exploration_rate=0.01. Score=11.\n",
      "[ episode 185 ] state=[ 0.04632882  0.04150984 -0.04083728  0.02254559]\n",
      "[ episode 185 ][ timestamp 1 ] state=[ 0.04632882  0.04150984 -0.04083728  0.02254559], action=0, reward=1.0, next_state=[ 0.04715902 -0.1530034  -0.04038636  0.30206936]\n",
      "[ Experience replay ] starts\n",
      "[ episode 185 ][ timestamp 2 ] state=[ 0.04715902 -0.1530034  -0.04038636  0.30206936], action=0, reward=1.0, next_state=[ 0.04409895 -0.34752717 -0.03434498  0.58174673]\n",
      "[ Experience replay ] starts\n",
      "[ episode 185 ][ timestamp 3 ] state=[ 0.04409895 -0.34752717 -0.03434498  0.58174673], action=0, reward=1.0, next_state=[ 0.03714841 -0.5421515  -0.02271004  0.86341562]\n",
      "[ Experience replay ] starts\n",
      "[ episode 185 ][ timestamp 4 ] state=[ 0.03714841 -0.5421515  -0.02271004  0.86341562], action=0, reward=1.0, next_state=[ 0.02630538 -0.73695703 -0.00544173  1.14887239]\n",
      "[ Experience replay ] starts\n",
      "[ episode 185 ][ timestamp 5 ] state=[ 0.02630538 -0.73695703 -0.00544173  1.14887239], action=0, reward=1.0, next_state=[ 0.01156623 -0.93200752  0.01753572  1.43984394]\n",
      "[ Experience replay ] starts\n",
      "[ episode 185 ][ timestamp 6 ] state=[ 0.01156623 -0.93200752  0.01753572  1.43984394], action=0, reward=1.0, next_state=[-0.00707392 -1.12734105  0.0463326   1.73795442]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 185 ][ timestamp 7 ] state=[-0.00707392 -1.12734105  0.0463326   1.73795442], action=0, reward=1.0, next_state=[-0.02962074 -1.32295934  0.08109169  2.04468387]\n",
      "[ Experience replay ] starts\n",
      "[ episode 185 ][ timestamp 8 ] state=[-0.02962074 -1.32295934  0.08109169  2.04468387], action=0, reward=1.0, next_state=[-0.05607992 -1.51881475  0.12198536  2.36131641]\n",
      "[ Experience replay ] starts\n",
      "[ episode 185 ][ timestamp 9 ] state=[-0.05607992 -1.51881475  0.12198536  2.36131641], action=0, reward=1.0, next_state=[-0.08645622 -1.71479458  0.16921169  2.68887649]\n",
      "[ Experience replay ] starts\n",
      "[ episode 185 ][ timestamp 10 ] state=[-0.08645622 -1.71479458  0.16921169  2.68887649], action=0, reward=-1.0, next_state=[-0.12075211 -1.9107028   0.22298922  3.028053  ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 185: Exploration_rate=0.01. Score=10.\n",
      "[ episode 186 ] state=[-0.04080568  0.00208479 -0.02259522 -0.00050439]\n",
      "[ episode 186 ][ timestamp 1 ] state=[-0.04080568  0.00208479 -0.02259522 -0.00050439], action=0, reward=1.0, next_state=[-0.04076399 -0.19270595 -0.0226053   0.28496471]\n",
      "[ Experience replay ] starts\n",
      "[ episode 186 ][ timestamp 2 ] state=[-0.04076399 -0.19270595 -0.0226053   0.28496471], action=0, reward=1.0, next_state=[-0.04461811 -0.38749833 -0.01690601  0.57043323]\n",
      "[ Experience replay ] starts\n",
      "[ episode 186 ][ timestamp 3 ] state=[-0.04461811 -0.38749833 -0.01690601  0.57043323], action=1, reward=1.0, next_state=[-0.05236807 -0.19214342 -0.00549734  0.27247261]\n",
      "[ Experience replay ] starts\n",
      "[ episode 186 ][ timestamp 4 ] state=[-0.05236807 -0.19214342 -0.00549734  0.27247261], action=1, reward=1.0, next_state=[-5.62109404e-02  3.05654419e-03 -4.78921438e-05 -2.19391167e-02]\n",
      "[ Experience replay ] starts\n",
      "[ episode 186 ][ timestamp 5 ] state=[-5.62109404e-02  3.05654419e-03 -4.78921438e-05 -2.19391167e-02], action=0, reward=1.0, next_state=[-0.05614981 -0.19206472 -0.00048667  0.2707287 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 186 ][ timestamp 6 ] state=[-0.05614981 -0.19206472 -0.00048667  0.2707287 ], action=0, reward=1.0, next_state=[-0.0599911  -0.38717972  0.0049279   0.56325809]\n",
      "[ Experience replay ] starts\n",
      "[ episode 186 ][ timestamp 7 ] state=[-0.0599911  -0.38717972  0.0049279   0.56325809], action=0, reward=1.0, next_state=[-0.0677347  -0.58237047  0.01619306  0.85748946]\n",
      "[ Experience replay ] starts\n",
      "[ episode 186 ][ timestamp 8 ] state=[-0.0677347  -0.58237047  0.01619306  0.85748946], action=0, reward=1.0, next_state=[-0.07938211 -0.77770925  0.03334285  1.15521976]\n",
      "[ Experience replay ] starts\n",
      "[ episode 186 ][ timestamp 9 ] state=[-0.07938211 -0.77770925  0.03334285  1.15521976], action=0, reward=1.0, next_state=[-0.09493629 -0.97324973  0.05644725  1.45816843]\n",
      "[ Experience replay ] starts\n",
      "[ episode 186 ][ timestamp 10 ] state=[-0.09493629 -0.97324973  0.05644725  1.45816843], action=0, reward=1.0, next_state=[-0.11440129 -1.16901688  0.08561061  1.76793812]\n",
      "[ Experience replay ] starts\n",
      "[ episode 186 ][ timestamp 11 ] state=[-0.11440129 -1.16901688  0.08561061  1.76793812], action=0, reward=1.0, next_state=[-0.13778163 -1.36499504  0.12096938  2.08596754]\n",
      "[ Experience replay ] starts\n",
      "[ episode 186 ][ timestamp 12 ] state=[-0.13778163 -1.36499504  0.12096938  2.08596754], action=0, reward=1.0, next_state=[-0.16508153 -1.56111374  0.16268873  2.41347409]\n",
      "[ Experience replay ] starts\n",
      "[ episode 186 ][ timestamp 13 ] state=[-0.16508153 -1.56111374  0.16268873  2.41347409], action=0, reward=-1.0, next_state=[-0.1963038  -1.75723101  0.21095821  2.75138528]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 186: Exploration_rate=0.01. Score=13.\n",
      "[ episode 187 ] state=[-0.02295138  0.02522588  0.01809803 -0.04789758]\n",
      "[ episode 187 ][ timestamp 1 ] state=[-0.02295138  0.02522588  0.01809803 -0.04789758], action=0, reward=1.0, next_state=[-0.02244686 -0.17015085  0.01714008  0.25044004]\n",
      "[ Experience replay ] starts\n",
      "[ episode 187 ][ timestamp 2 ] state=[-0.02244686 -0.17015085  0.01714008  0.25044004], action=0, reward=1.0, next_state=[-0.02584988 -0.36551332  0.02214888  0.54847964]\n",
      "[ Experience replay ] starts\n",
      "[ episode 187 ][ timestamp 3 ] state=[-0.02584988 -0.36551332  0.02214888  0.54847964], action=0, reward=1.0, next_state=[-0.03316014 -0.5609393   0.03311847  0.84805795]\n",
      "[ Experience replay ] starts\n",
      "[ episode 187 ][ timestamp 4 ] state=[-0.03316014 -0.5609393   0.03311847  0.84805795], action=0, reward=1.0, next_state=[-0.04437893 -0.75649695  0.05007963  1.15096862]\n",
      "[ Experience replay ] starts\n",
      "[ episode 187 ][ timestamp 5 ] state=[-0.04437893 -0.75649695  0.05007963  1.15096862], action=0, reward=1.0, next_state=[-0.05950887 -0.95223532  0.07309901  1.45892534]\n",
      "[ Experience replay ] starts\n",
      "[ episode 187 ][ timestamp 6 ] state=[-0.05950887 -0.95223532  0.07309901  1.45892534], action=0, reward=1.0, next_state=[-0.07855357 -1.14817376  0.10227751  1.77352008]\n",
      "[ Experience replay ] starts\n",
      "[ episode 187 ][ timestamp 7 ] state=[-0.07855357 -1.14817376  0.10227751  1.77352008], action=0, reward=1.0, next_state=[-0.10151705 -1.34428942  0.13774791  2.09617347]\n",
      "[ Experience replay ] starts\n",
      "[ episode 187 ][ timestamp 8 ] state=[-0.10151705 -1.34428942  0.13774791  2.09617347], action=0, reward=1.0, next_state=[-0.12840284 -1.54050265  0.17967138  2.42807539]\n",
      "[ Experience replay ] starts\n",
      "[ episode 187 ][ timestamp 9 ] state=[-0.12840284 -1.54050265  0.17967138  2.42807539], action=0, reward=-1.0, next_state=[-0.15921289 -1.73666001  0.22823289  2.77011458]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 187: Exploration_rate=0.01. Score=9.\n",
      "[ episode 188 ] state=[ 0.04094592  0.01369824 -0.02072388  0.00309601]\n",
      "[ episode 188 ][ timestamp 1 ] state=[ 0.04094592  0.01369824 -0.02072388  0.00309601], action=0, reward=1.0, next_state=[ 0.04121988 -0.18112047 -0.02066196  0.28916893]\n",
      "[ Experience replay ] starts\n",
      "[ episode 188 ][ timestamp 2 ] state=[ 0.04121988 -0.18112047 -0.02066196  0.28916893], action=0, reward=1.0, next_state=[ 0.03759747 -0.37594178 -0.01487858  0.57526434]\n",
      "[ Experience replay ] starts\n",
      "[ episode 188 ][ timestamp 3 ] state=[ 0.03759747 -0.37594178 -0.01487858  0.57526434], action=0, reward=1.0, next_state=[ 0.03007864 -0.57085203 -0.00337329  0.86322321]\n",
      "[ Experience replay ] starts\n",
      "[ episode 188 ][ timestamp 4 ] state=[ 0.03007864 -0.57085203 -0.00337329  0.86322321], action=1, reward=1.0, next_state=[ 0.0186616  -0.37568431  0.01389117  0.56948156]\n",
      "[ Experience replay ] starts\n",
      "[ episode 188 ][ timestamp 5 ] state=[ 0.0186616  -0.37568431  0.01389117  0.56948156], action=1, reward=1.0, next_state=[ 0.01114791 -0.18075991  0.0252808   0.28120704]\n",
      "[ Experience replay ] starts\n",
      "[ episode 188 ][ timestamp 6 ] state=[ 0.01114791 -0.18075991  0.0252808   0.28120704], action=0, reward=1.0, next_state=[ 0.00753271 -0.37623319  0.03090494  0.58175502]\n",
      "[ Experience replay ] starts\n",
      "[ episode 188 ][ timestamp 7 ] state=[ 0.00753271 -0.37623319  0.03090494  0.58175502], action=0, reward=1.0, next_state=[ 8.04788497e-06 -5.71774212e-01  4.25400431e-02  8.84011104e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 188 ][ timestamp 8 ] state=[ 8.04788497e-06 -5.71774212e-01  4.25400431e-02  8.84011104e-01], action=0, reward=1.0, next_state=[-0.01142744 -0.7674472   0.06022027  1.18975805]\n",
      "[ Experience replay ] starts\n",
      "[ episode 188 ][ timestamp 9 ] state=[-0.01142744 -0.7674472   0.06022027  1.18975805], action=0, reward=1.0, next_state=[-0.02677638 -0.96329569  0.08401543  1.50069232]\n",
      "[ Experience replay ] starts\n",
      "[ episode 188 ][ timestamp 10 ] state=[-0.02677638 -0.96329569  0.08401543  1.50069232], action=0, reward=1.0, next_state=[-0.04604229 -1.15933149  0.11402927  1.81838032]\n",
      "[ Experience replay ] starts\n",
      "[ episode 188 ][ timestamp 11 ] state=[-0.04604229 -1.15933149  0.11402927  1.81838032], action=0, reward=1.0, next_state=[-0.06922892 -1.35552176  0.15039688  2.14420654]\n",
      "[ Experience replay ] starts\n",
      "[ episode 188 ][ timestamp 12 ] state=[-0.06922892 -1.35552176  0.15039688  2.14420654], action=0, reward=1.0, next_state=[-0.09633936 -1.55177381  0.19328101  2.47931177]\n",
      "[ Experience replay ] starts\n",
      "[ episode 188 ][ timestamp 13 ] state=[-0.09633936 -1.55177381  0.19328101  2.47931177], action=0, reward=-1.0, next_state=[-0.12737484 -1.74791775  0.24286725  2.82452064]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Ended! ] Episode 188: Exploration_rate=0.01. Score=13.\n",
      "[ episode 189 ] state=[-0.01301399 -0.03763022  0.04787367 -0.00685203]\n",
      "[ episode 189 ][ timestamp 1 ] state=[-0.01301399 -0.03763022  0.04787367 -0.00685203], action=0, reward=1.0, next_state=[-0.01376659 -0.2334049   0.04773663  0.30054301]\n",
      "[ Experience replay ] starts\n",
      "[ episode 189 ][ timestamp 2 ] state=[-0.01376659 -0.2334049   0.04773663  0.30054301], action=0, reward=1.0, next_state=[-0.01843469 -0.4291736   0.05374749  0.60789078]\n",
      "[ Experience replay ] starts\n",
      "[ episode 189 ][ timestamp 3 ] state=[-0.01843469 -0.4291736   0.05374749  0.60789078], action=0, reward=1.0, next_state=[-0.02701816 -0.62500416  0.06590531  0.9170066 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 189 ][ timestamp 4 ] state=[-0.02701816 -0.62500416  0.06590531  0.9170066 ], action=0, reward=1.0, next_state=[-0.03951824 -0.82095234  0.08424544  1.22965291]\n",
      "[ Experience replay ] starts\n",
      "[ episode 189 ][ timestamp 5 ] state=[-0.03951824 -0.82095234  0.08424544  1.22965291], action=0, reward=1.0, next_state=[-0.05593729 -1.01705106  0.1088385   1.54749665]\n",
      "[ Experience replay ] starts\n",
      "[ episode 189 ][ timestamp 6 ] state=[-0.05593729 -1.01705106  0.1088385   1.54749665], action=1, reward=1.0, next_state=[-0.07627831 -0.82339113  0.13978843  1.29066098]\n",
      "[ Experience replay ] starts\n",
      "[ episode 189 ][ timestamp 7 ] state=[-0.07627831 -0.82339113  0.13978843  1.29066098], action=0, reward=1.0, next_state=[-0.09274613 -1.01998613  0.16560165  1.62364103]\n",
      "[ Experience replay ] starts\n",
      "[ episode 189 ][ timestamp 8 ] state=[-0.09274613 -1.01998613  0.16560165  1.62364103], action=1, reward=1.0, next_state=[-0.11314586 -0.82715541  0.19807447  1.38681669]\n",
      "[ Experience replay ] starts\n",
      "[ episode 189 ][ timestamp 9 ] state=[-0.11314586 -0.82715541  0.19807447  1.38681669], action=1, reward=-1.0, next_state=[-0.12968897 -0.63497571  0.22581081  1.16203743]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 189: Exploration_rate=0.01. Score=9.\n",
      "[ episode 190 ] state=[-0.03029137 -0.01660116 -0.02513016  0.04076709]\n",
      "[ episode 190 ][ timestamp 1 ] state=[-0.03029137 -0.01660116 -0.02513016  0.04076709], action=1, reward=1.0, next_state=[-0.03062339  0.17887197 -0.02431481 -0.25973751]\n",
      "[ Experience replay ] starts\n",
      "[ episode 190 ][ timestamp 2 ] state=[-0.03062339  0.17887197 -0.02431481 -0.25973751], action=0, reward=1.0, next_state=[-0.02704595 -0.01589458 -0.02950956  0.02517811]\n",
      "[ Experience replay ] starts\n",
      "[ episode 190 ][ timestamp 3 ] state=[-0.02704595 -0.01589458 -0.02950956  0.02517811], action=0, reward=1.0, next_state=[-0.02736385 -0.21058118 -0.029006    0.30840632]\n",
      "[ Experience replay ] starts\n",
      "[ episode 190 ][ timestamp 4 ] state=[-0.02736385 -0.21058118 -0.029006    0.30840632], action=1, reward=1.0, next_state=[-0.03157547 -0.0150582  -0.02283787  0.00671865]\n",
      "[ Experience replay ] starts\n",
      "[ episode 190 ][ timestamp 5 ] state=[-0.03157547 -0.0150582  -0.02283787  0.00671865], action=0, reward=1.0, next_state=[-0.03187663 -0.20984531 -0.0227035   0.29210936]\n",
      "[ Experience replay ] starts\n",
      "[ episode 190 ][ timestamp 6 ] state=[-0.03187663 -0.20984531 -0.0227035   0.29210936], action=0, reward=1.0, next_state=[-0.03607354 -0.40463631 -0.01686131  0.57754632]\n",
      "[ Experience replay ] starts\n",
      "[ episode 190 ][ timestamp 7 ] state=[-0.03607354 -0.40463631 -0.01686131  0.57754632], action=0, reward=1.0, next_state=[-0.04416627 -0.59951793 -0.00531039  0.86487019]\n",
      "[ Experience replay ] starts\n",
      "[ episode 190 ][ timestamp 8 ] state=[-0.04416627 -0.59951793 -0.00531039  0.86487019], action=0, reward=1.0, next_state=[-0.05615662 -0.79456719  0.01198702  1.15587872]\n",
      "[ Experience replay ] starts\n",
      "[ episode 190 ][ timestamp 9 ] state=[-0.05615662 -0.79456719  0.01198702  1.15587872], action=0, reward=1.0, next_state=[-0.07204797 -0.98984336  0.03510459  1.45229603]\n",
      "[ Experience replay ] starts\n",
      "[ episode 190 ][ timestamp 10 ] state=[-0.07204797 -0.98984336  0.03510459  1.45229603], action=1, reward=1.0, next_state=[-0.09184484 -0.79516977  0.06415051  1.17078419]\n",
      "[ Experience replay ] starts\n",
      "[ episode 190 ][ timestamp 11 ] state=[-0.09184484 -0.79516977  0.06415051  1.17078419], action=0, reward=1.0, next_state=[-0.10774823 -0.99106458  0.08756619  1.48286929]\n",
      "[ Experience replay ] starts\n",
      "[ episode 190 ][ timestamp 12 ] state=[-0.10774823 -0.99106458  0.08756619  1.48286929], action=0, reward=1.0, next_state=[-0.12756952 -1.18713861  0.11722358  1.80156503]\n",
      "[ Experience replay ] starts\n",
      "[ episode 190 ][ timestamp 13 ] state=[-0.12756952 -1.18713861  0.11722358  1.80156503], action=1, reward=1.0, next_state=[-0.1513123  -0.99350592  0.15325488  1.54749416]\n",
      "[ Experience replay ] starts\n",
      "[ episode 190 ][ timestamp 14 ] state=[-0.1513123  -0.99350592  0.15325488  1.54749416], action=0, reward=1.0, next_state=[-0.17118241 -1.19009964  0.18420476  1.88380922]\n",
      "[ Experience replay ] starts\n",
      "[ episode 190 ][ timestamp 15 ] state=[-0.17118241 -1.19009964  0.18420476  1.88380922], action=1, reward=-1.0, next_state=[-0.19498441 -0.997399    0.22188095  1.65349881]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 190: Exploration_rate=0.01. Score=15.\n",
      "[ episode 191 ] state=[ 0.02869297  0.00877773 -0.03693703  0.0121021 ]\n",
      "[ episode 191 ][ timestamp 1 ] state=[ 0.02869297  0.00877773 -0.03693703  0.0121021 ], action=0, reward=1.0, next_state=[ 0.02886852 -0.18579556 -0.03669498  0.29290595]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 2 ] state=[ 0.02886852 -0.18579556 -0.03669498  0.29290595], action=0, reward=1.0, next_state=[ 0.02515261 -0.38037563 -0.03083687  0.57379367]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 3 ] state=[ 0.02515261 -0.38037563 -0.03083687  0.57379367], action=0, reward=1.0, next_state=[ 0.0175451  -0.57505198 -0.01936099  0.85660476]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 4 ] state=[ 0.0175451  -0.57505198 -0.01936099  0.85660476], action=1, reward=1.0, next_state=[ 0.00604406 -0.37967165 -0.0022289   0.55789742]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 5 ] state=[ 0.00604406 -0.37967165 -0.0022289   0.55789742], action=1, reward=1.0, next_state=[-0.00154937 -0.18451848  0.00892905  0.2645131 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 6 ] state=[-0.00154937 -0.18451848  0.00892905  0.2645131 ], action=1, reward=1.0, next_state=[-0.00523974  0.01047489  0.01421931 -0.0253402 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 7 ] state=[-0.00523974  0.01047489  0.01421931 -0.0253402 ], action=1, reward=1.0, next_state=[-0.00503025  0.20539007  0.01371251 -0.31350307]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 8 ] state=[-0.00503025  0.20539007  0.01371251 -0.31350307], action=1, reward=1.0, next_state=[-0.00092244  0.40031402  0.00744245 -0.60183016]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 9 ] state=[-0.00092244  0.40031402  0.00744245 -0.60183016], action=0, reward=1.0, next_state=[ 0.00708384  0.20508876 -0.00459416 -0.30681232]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 10 ] state=[ 0.00708384  0.20508876 -0.00459416 -0.30681232], action=0, reward=1.0, next_state=[ 0.01118561  0.01003257 -0.0107304  -0.0155818 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 11 ] state=[ 0.01118561  0.01003257 -0.0107304  -0.0155818 ], action=0, reward=1.0, next_state=[ 0.01138626 -0.18493386 -0.01104204  0.27369633]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 12 ] state=[ 0.01138626 -0.18493386 -0.01104204  0.27369633], action=0, reward=1.0, next_state=[ 0.00768759 -0.37989653 -0.00556811  0.56287622]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 13 ] state=[ 0.00768759 -0.37989653 -0.00556811  0.56287622], action=1, reward=1.0, next_state=[ 8.96553347e-05 -1.84696893e-01  5.68941322e-03  2.68444281e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 14 ] state=[ 8.96553347e-05 -1.84696893e-01  5.68941322e-03  2.68444281e-01], action=1, reward=1.0, next_state=[-0.00360428  0.0103434   0.0110583  -0.02243875]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 15 ] state=[-0.00360428  0.0103434   0.0110583  -0.02243875], action=1, reward=1.0, next_state=[-0.00339741  0.20530504  0.01060952 -0.31161225]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 16 ] state=[-0.00339741  0.20530504  0.01060952 -0.31161225], action=0, reward=1.0, next_state=[ 0.00070869  0.01003355  0.00437728 -0.01560237]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 17 ] state=[ 0.00070869  0.01003355  0.00437728 -0.01560237], action=0, reward=1.0, next_state=[ 0.00090936 -0.1851509   0.00406523  0.27845842]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 191 ][ timestamp 18 ] state=[ 0.00090936 -0.1851509   0.00406523  0.27845842], action=1, reward=1.0, next_state=[-0.00279366  0.00991282  0.0096344  -0.01293957]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 19 ] state=[-0.00279366  0.00991282  0.0096344  -0.01293957], action=1, reward=1.0, next_state=[-0.0025954   0.20489529  0.00937561 -0.30256722]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 20 ] state=[-0.0025954   0.20489529  0.00937561 -0.30256722], action=0, reward=1.0, next_state=[ 0.0015025   0.00964098  0.00332426 -0.00694224]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 21 ] state=[ 0.0015025   0.00964098  0.00332426 -0.00694224], action=1, reward=1.0, next_state=[ 0.00169532  0.2047151   0.00318542 -0.29857447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 22 ] state=[ 0.00169532  0.2047151   0.00318542 -0.29857447], action=1, reward=1.0, next_state=[ 0.00578962  0.3997915  -0.00278607 -0.59025107]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 23 ] state=[ 0.00578962  0.3997915  -0.00278607 -0.59025107], action=0, reward=1.0, next_state=[ 0.01378545  0.20470867 -0.01459109 -0.29844707]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 24 ] state=[ 0.01378545  0.20470867 -0.01459109 -0.29844707], action=0, reward=1.0, next_state=[ 0.01787963  0.00979771 -0.02056003 -0.01040138]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 25 ] state=[ 0.01787963  0.00979771 -0.02056003 -0.01040138], action=1, reward=1.0, next_state=[ 0.01807558  0.2052084  -0.02076806 -0.30949968]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 26 ] state=[ 0.01807558  0.2052084  -0.02076806 -0.30949968], action=1, reward=1.0, next_state=[ 0.02217975  0.40062    -0.02695805 -0.60865925]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 27 ] state=[ 0.02217975  0.40062    -0.02695805 -0.60865925], action=1, reward=1.0, next_state=[ 0.03019215  0.59610825 -0.03913124 -0.90970978]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 28 ] state=[ 0.03019215  0.59610825 -0.03913124 -0.90970978], action=1, reward=1.0, next_state=[ 0.04211431  0.79173733 -0.05732543 -1.21443042]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 29 ] state=[ 0.04211431  0.79173733 -0.05732543 -1.21443042], action=0, reward=1.0, next_state=[ 0.05794906  0.59739995 -0.08161404 -0.94024764]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 30 ] state=[ 0.05794906  0.59739995 -0.08161404 -0.94024764], action=0, reward=1.0, next_state=[ 0.06989706  0.40346727 -0.100419   -0.67428479]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 31 ] state=[ 0.06989706  0.40346727 -0.100419   -0.67428479], action=0, reward=1.0, next_state=[ 0.0779664   0.2098737  -0.11390469 -0.41483096]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 32 ] state=[ 0.0779664   0.2098737  -0.11390469 -0.41483096], action=0, reward=1.0, next_state=[ 0.08216388  0.01653489 -0.12220131 -0.16011764]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 33 ] state=[ 0.08216388  0.01653489 -0.12220131 -0.16011764], action=0, reward=1.0, next_state=[ 0.08249458 -0.17664493 -0.12540366  0.09165336]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 34 ] state=[ 0.08249458 -0.17664493 -0.12540366  0.09165336], action=0, reward=1.0, next_state=[ 0.07896168 -0.36976717 -0.1235706   0.34228979]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 35 ] state=[ 0.07896168 -0.36976717 -0.1235706   0.34228979], action=0, reward=1.0, next_state=[ 0.07156633 -0.56293428 -0.1167248   0.5935937 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 36 ] state=[ 0.07156633 -0.56293428 -0.1167248   0.5935937 ], action=0, reward=1.0, next_state=[ 0.06030765 -0.75624558 -0.10485293  0.84734832]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 37 ] state=[ 0.06030765 -0.75624558 -0.10485293  0.84734832], action=1, reward=1.0, next_state=[ 0.04518274 -0.5598616  -0.08790596  0.52361987]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 38 ] state=[ 0.04518274 -0.5598616  -0.08790596  0.52361987], action=1, reward=1.0, next_state=[ 0.03398551 -0.36361959 -0.07743356  0.20458238]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 39 ] state=[ 0.03398551 -0.36361959 -0.07743356  0.20458238], action=1, reward=1.0, next_state=[ 0.02671311 -0.1674806  -0.07334192 -0.11148724]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 40 ] state=[ 0.02671311 -0.1674806  -0.07334192 -0.11148724], action=1, reward=1.0, next_state=[ 0.0233635   0.02861148 -0.07557166 -0.42637783]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 41 ] state=[ 0.0233635   0.02861148 -0.07557166 -0.42637783], action=1, reward=1.0, next_state=[ 0.02393573  0.22471794 -0.08409922 -0.74189485]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 42 ] state=[ 0.02393573  0.22471794 -0.08409922 -0.74189485], action=1, reward=1.0, next_state=[ 0.02843009  0.42089392 -0.09893711 -1.05981486]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 43 ] state=[ 0.02843009  0.42089392 -0.09893711 -1.05981486], action=0, reward=1.0, next_state=[ 0.03684797  0.22721166 -0.12013341 -0.79975229]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 44 ] state=[ 0.03684797  0.22721166 -0.12013341 -0.79975229], action=0, reward=1.0, next_state=[ 0.0413922   0.03392442 -0.13612846 -0.54714539]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 45 ] state=[ 0.0413922   0.03392442 -0.13612846 -0.54714539], action=1, reward=1.0, next_state=[ 0.04207069  0.23066986 -0.14707136 -0.87943163]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 46 ] state=[ 0.04207069  0.23066986 -0.14707136 -0.87943163], action=0, reward=1.0, next_state=[ 0.04668409  0.0378193  -0.16466    -0.63636194]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 47 ] state=[ 0.04668409  0.0378193  -0.16466    -0.63636194], action=0, reward=1.0, next_state=[ 0.04744047 -0.15466996 -0.17738724 -0.399725  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 48 ] state=[ 0.04744047 -0.15466996 -0.17738724 -0.399725  ], action=0, reward=1.0, next_state=[ 0.04434707 -0.34689051 -0.18538174 -0.16779741]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 49 ] state=[ 0.04434707 -0.34689051 -0.18538174 -0.16779741], action=1, reward=1.0, next_state=[ 0.03740926 -0.14966576 -0.18873768 -0.51275624]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 50 ] state=[ 0.03740926 -0.14966576 -0.18873768 -0.51275624], action=1, reward=1.0, next_state=[ 0.03441595  0.04754307 -0.19899281 -0.85847642]\n",
      "[ Experience replay ] starts\n",
      "[ episode 191 ][ timestamp 51 ] state=[ 0.03441595  0.04754307 -0.19899281 -0.85847642], action=0, reward=-1.0, next_state=[ 0.03536681 -0.1443931  -0.21616234 -0.63437217]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 191: Exploration_rate=0.01. Score=51.\n",
      "[ episode 192 ] state=[ 0.04522908  0.01981946  0.02629863 -0.03419746]\n",
      "[ episode 192 ][ timestamp 1 ] state=[ 0.04522908  0.01981946  0.02629863 -0.03419746], action=1, reward=1.0, next_state=[ 0.04562546  0.2145546   0.02561468 -0.31846826]\n",
      "[ Experience replay ] starts\n",
      "[ episode 192 ][ timestamp 2 ] state=[ 0.04562546  0.2145546   0.02561468 -0.31846826], action=0, reward=1.0, next_state=[ 0.04991656  0.01907738  0.01924531 -0.01781872]\n",
      "[ Experience replay ] starts\n",
      "[ episode 192 ][ timestamp 3 ] state=[ 0.04991656  0.01907738  0.01924531 -0.01781872], action=0, reward=1.0, next_state=[ 0.0502981  -0.17631521  0.01888894  0.28087366]\n",
      "[ Experience replay ] starts\n",
      "[ episode 192 ][ timestamp 4 ] state=[ 0.0502981  -0.17631521  0.01888894  0.28087366], action=0, reward=1.0, next_state=[ 0.0467718  -0.37170144  0.02450641  0.57945374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 192 ][ timestamp 5 ] state=[ 0.0467718  -0.37170144  0.02450641  0.57945374], action=1, reward=1.0, next_state=[ 0.03933777 -0.17693134  0.03609549  0.29459047]\n",
      "[ Experience replay ] starts\n",
      "[ episode 192 ][ timestamp 6 ] state=[ 0.03933777 -0.17693134  0.03609549  0.29459047], action=1, reward=1.0, next_state=[0.03579914 0.01765792 0.0419873  0.01350649]\n",
      "[ Experience replay ] starts\n",
      "[ episode 192 ][ timestamp 7 ] state=[0.03579914 0.01765792 0.0419873  0.01350649], action=0, reward=1.0, next_state=[ 0.0361523  -0.17804025  0.04225743  0.31913566]\n",
      "[ Experience replay ] starts\n",
      "[ episode 192 ][ timestamp 8 ] state=[ 0.0361523  -0.17804025  0.04225743  0.31913566], action=1, reward=1.0, next_state=[0.0325915  0.01645519 0.04864014 0.04007293]\n",
      "[ Experience replay ] starts\n",
      "[ episode 192 ][ timestamp 9 ] state=[0.0325915  0.01645519 0.04864014 0.04007293], action=0, reward=1.0, next_state=[ 0.0329206  -0.17932929  0.0494416   0.34769688]\n",
      "[ Experience replay ] starts\n",
      "[ episode 192 ][ timestamp 10 ] state=[ 0.0329206  -0.17932929  0.0494416   0.34769688], action=0, reward=1.0, next_state=[ 0.02933402 -0.37511833  0.05639554  0.65555148]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 192 ][ timestamp 11 ] state=[ 0.02933402 -0.37511833  0.05639554  0.65555148], action=0, reward=1.0, next_state=[ 0.02183165 -0.5709782   0.06950657  0.96544571]\n",
      "[ Experience replay ] starts\n",
      "[ episode 192 ][ timestamp 12 ] state=[ 0.02183165 -0.5709782   0.06950657  0.96544571], action=1, reward=1.0, next_state=[ 0.01041208 -0.37685523  0.08881548  0.69538282]\n",
      "[ Experience replay ] starts\n",
      "[ episode 192 ][ timestamp 13 ] state=[ 0.01041208 -0.37685523  0.08881548  0.69538282], action=0, reward=1.0, next_state=[ 0.00287498 -0.57308942  0.10272314  1.01465137]\n",
      "[ Experience replay ] starts\n",
      "[ episode 192 ][ timestamp 14 ] state=[ 0.00287498 -0.57308942  0.10272314  1.01465137], action=0, reward=1.0, next_state=[-0.00858681 -0.76942018  0.12301616  1.33774262]\n",
      "[ Experience replay ] starts\n",
      "[ episode 192 ][ timestamp 15 ] state=[-0.00858681 -0.76942018  0.12301616  1.33774262], action=1, reward=1.0, next_state=[-0.02397521 -0.57604353  0.14977102  1.08594525]\n",
      "[ Experience replay ] starts\n",
      "[ episode 192 ][ timestamp 16 ] state=[-0.02397521 -0.57604353  0.14977102  1.08594525], action=1, reward=1.0, next_state=[-0.03549608 -0.38318006  0.17148992  0.84375687]\n",
      "[ Experience replay ] starts\n",
      "[ episode 192 ][ timestamp 17 ] state=[-0.03549608 -0.38318006  0.17148992  0.84375687], action=1, reward=1.0, next_state=[-0.04315968 -0.19076108  0.18836506  0.60953339]\n",
      "[ Experience replay ] starts\n",
      "[ episode 192 ][ timestamp 18 ] state=[-0.04315968 -0.19076108  0.18836506  0.60953339], action=1, reward=1.0, next_state=[-0.04697491  0.00129801  0.20055573  0.38159298]\n",
      "[ Experience replay ] starts\n",
      "[ episode 192 ][ timestamp 19 ] state=[-0.04697491  0.00129801  0.20055573  0.38159298], action=1, reward=1.0, next_state=[-0.04694895  0.19309157  0.20818759  0.15823801]\n",
      "[ Experience replay ] starts\n",
      "[ episode 192 ][ timestamp 20 ] state=[-0.04694895  0.19309157  0.20818759  0.15823801], action=0, reward=-1.0, next_state=[-0.04308711 -0.00430849  0.21135235  0.5087104 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 192: Exploration_rate=0.01. Score=20.\n",
      "[ episode 193 ] state=[ 0.02103456 -0.02086628  0.04777548 -0.01513857]\n",
      "[ episode 193 ][ timestamp 1 ] state=[ 0.02103456 -0.02086628  0.04777548 -0.01513857], action=1, reward=1.0, next_state=[ 0.02061724  0.17353911  0.04747271 -0.29237327]\n",
      "[ Experience replay ] starts\n",
      "[ episode 193 ][ timestamp 2 ] state=[ 0.02061724  0.17353911  0.04747271 -0.29237327], action=1, reward=1.0, next_state=[ 0.02408802  0.36795318  0.04162525 -0.56971409]\n",
      "[ Experience replay ] starts\n",
      "[ episode 193 ][ timestamp 3 ] state=[ 0.02408802  0.36795318  0.04162525 -0.56971409], action=0, reward=1.0, next_state=[ 0.03144708  0.17227292  0.03023096 -0.26421366]\n",
      "[ Experience replay ] starts\n",
      "[ episode 193 ][ timestamp 4 ] state=[ 0.03144708  0.17227292  0.03023096 -0.26421366], action=0, reward=1.0, next_state=[ 0.03489254 -0.02326719  0.02494669  0.03784903]\n",
      "[ Experience replay ] starts\n",
      "[ episode 193 ][ timestamp 5 ] state=[ 0.03489254 -0.02326719  0.02494669  0.03784903], action=0, reward=1.0, next_state=[ 0.0344272  -0.21873783  0.02570367  0.33829733]\n",
      "[ Experience replay ] starts\n",
      "[ episode 193 ][ timestamp 6 ] state=[ 0.0344272  -0.21873783  0.02570367  0.33829733], action=1, reward=1.0, next_state=[ 0.03005244 -0.02399089  0.03246962  0.05382946]\n",
      "[ Experience replay ] starts\n",
      "[ episode 193 ][ timestamp 7 ] state=[ 0.03005244 -0.02399089  0.03246962  0.05382946], action=0, reward=1.0, next_state=[ 0.02957262 -0.219563    0.03354621  0.35657739]\n",
      "[ Experience replay ] starts\n",
      "[ episode 193 ][ timestamp 8 ] state=[ 0.02957262 -0.219563    0.03354621  0.35657739], action=0, reward=1.0, next_state=[ 0.02518136 -0.41514543  0.04067775  0.65964672]\n",
      "[ Experience replay ] starts\n",
      "[ episode 193 ][ timestamp 9 ] state=[ 0.02518136 -0.41514543  0.04067775  0.65964672], action=0, reward=1.0, next_state=[ 0.01687845 -0.61080918  0.05387069  0.96485551]\n",
      "[ Experience replay ] starts\n",
      "[ episode 193 ][ timestamp 10 ] state=[ 0.01687845 -0.61080918  0.05387069  0.96485551], action=1, reward=1.0, next_state=[ 0.00466227 -0.41645064  0.0731678   0.68957096]\n",
      "[ Experience replay ] starts\n",
      "[ episode 193 ][ timestamp 11 ] state=[ 0.00466227 -0.41645064  0.0731678   0.68957096], action=1, reward=1.0, next_state=[-0.00366674 -0.22241624  0.08695922  0.42079023]\n",
      "[ Experience replay ] starts\n",
      "[ episode 193 ][ timestamp 12 ] state=[-0.00366674 -0.22241624  0.08695922  0.42079023], action=0, reward=1.0, next_state=[-0.00811507 -0.41865573  0.09537502  0.73957101]\n",
      "[ Experience replay ] starts\n",
      "[ episode 193 ][ timestamp 13 ] state=[-0.00811507 -0.41865573  0.09537502  0.73957101], action=0, reward=1.0, next_state=[-0.01648818 -0.61495614  0.11016644  1.06068118]\n",
      "[ Experience replay ] starts\n",
      "[ episode 193 ][ timestamp 14 ] state=[-0.01648818 -0.61495614  0.11016644  1.06068118], action=1, reward=1.0, next_state=[-0.02878731 -0.42145186  0.13138007  0.80450781]\n",
      "[ Experience replay ] starts\n",
      "[ episode 193 ][ timestamp 15 ] state=[-0.02878731 -0.42145186  0.13138007  0.80450781], action=1, reward=1.0, next_state=[-0.03721634 -0.22835224  0.14747022  0.55586928]\n",
      "[ Experience replay ] starts\n",
      "[ episode 193 ][ timestamp 16 ] state=[-0.03721634 -0.22835224  0.14747022  0.55586928], action=1, reward=1.0, next_state=[-0.04178339 -0.03557495  0.15858761  0.31304123]\n",
      "[ Experience replay ] starts\n",
      "[ episode 193 ][ timestamp 17 ] state=[-0.04178339 -0.03557495  0.15858761  0.31304123], action=0, reward=1.0, next_state=[-0.04249489 -0.23255877  0.16484843  0.6512387 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 193 ][ timestamp 18 ] state=[-0.04249489 -0.23255877  0.16484843  0.6512387 ], action=1, reward=1.0, next_state=[-0.04714606 -0.04006978  0.17787321  0.41466574]\n",
      "[ Experience replay ] starts\n",
      "[ episode 193 ][ timestamp 19 ] state=[-0.04714606 -0.04006978  0.17787321  0.41466574], action=1, reward=1.0, next_state=[-0.04794746  0.15214405  0.18616652  0.18291345]\n",
      "[ Experience replay ] starts\n",
      "[ episode 193 ][ timestamp 20 ] state=[-0.04794746  0.15214405  0.18616652  0.18291345], action=0, reward=1.0, next_state=[-0.04490458 -0.04508608  0.18982479  0.52806409]\n",
      "[ Experience replay ] starts\n",
      "[ episode 193 ][ timestamp 21 ] state=[-0.04490458 -0.04508608  0.18982479  0.52806409], action=1, reward=1.0, next_state=[-0.0458063   0.14692955  0.20038607  0.30068825]\n",
      "[ Experience replay ] starts\n",
      "[ episode 193 ][ timestamp 22 ] state=[-0.0458063   0.14692955  0.20038607  0.30068825], action=1, reward=1.0, next_state=[-0.04286771  0.33871555  0.20639984  0.07728578]\n",
      "[ Experience replay ] starts\n",
      "[ episode 193 ][ timestamp 23 ] state=[-0.04286771  0.33871555  0.20639984  0.07728578], action=0, reward=1.0, next_state=[-0.0360934   0.14132468  0.20794555  0.4273393 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 193 ][ timestamp 24 ] state=[-0.0360934   0.14132468  0.20794555  0.4273393 ], action=1, reward=-1.0, next_state=[-0.0332669   0.33298866  0.21649234  0.20673316]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 193: Exploration_rate=0.01. Score=24.\n",
      "[ episode 194 ] state=[-0.03341301  0.00494057  0.00551717  0.0094093 ]\n",
      "[ episode 194 ][ timestamp 1 ] state=[-0.03341301  0.00494057  0.00551717  0.0094093 ], action=1, reward=1.0, next_state=[-0.0333142   0.19998296  0.00570535 -0.2815278 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 2 ] state=[-0.0333142   0.19998296  0.00570535 -0.2815278 ], action=1, reward=1.0, next_state=[-2.93145373e-02  3.95023067e-01  7.47954364e-05 -5.72405835e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 3 ] state=[-2.93145373e-02  3.95023067e-01  7.47954364e-05 -5.72405835e-01], action=0, reward=1.0, next_state=[-0.02141408  0.19990007 -0.01137332 -0.27969935]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 4 ] state=[-0.02141408  0.19990007 -0.01137332 -0.27969935], action=0, reward=1.0, next_state=[-0.01741607  0.00494219 -0.01696731  0.00937487]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 5 ] state=[-0.01741607  0.00494219 -0.01696731  0.00937487], action=1, reward=1.0, next_state=[-0.01731723  0.20030331 -0.01677981 -0.28861278]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 6 ] state=[-0.01731723  0.20030331 -0.01677981 -0.28861278], action=0, reward=1.0, next_state=[-0.01331116  0.00542461 -0.02255207 -0.00126892]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 7 ] state=[-0.01331116  0.00542461 -0.02255207 -0.00126892], action=0, reward=1.0, next_state=[-0.01320267 -0.18936677 -0.02257744  0.28421411]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 8 ] state=[-0.01320267 -0.18936677 -0.02257744  0.28421411], action=1, reward=1.0, next_state=[-0.01699001  0.0060698  -0.01689316 -0.01550323]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 194 ][ timestamp 9 ] state=[-0.01699001  0.0060698  -0.01689316 -0.01550323], action=1, reward=1.0, next_state=[-0.01686861  0.20142989 -0.01720323 -0.31346791]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 10 ] state=[-0.01686861  0.20142989 -0.01720323 -0.31346791], action=0, reward=1.0, next_state=[-0.01284001  0.00655718 -0.02347259 -0.0262596 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 11 ] state=[-0.01284001  0.00655718 -0.02347259 -0.0262596 ], action=1, reward=1.0, next_state=[-0.01270887  0.20200775 -0.02399778 -0.326255  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 12 ] state=[-0.01270887  0.20200775 -0.02399778 -0.326255  ], action=0, reward=1.0, next_state=[-0.00866872  0.00723554 -0.03052288 -0.04123548]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 13 ] state=[-0.00866872  0.00723554 -0.03052288 -0.04123548], action=0, reward=1.0, next_state=[-0.008524   -0.18743572 -0.03134759  0.24166307]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 14 ] state=[-0.008524   -0.18743572 -0.03134759  0.24166307], action=1, reward=1.0, next_state=[-0.01227272  0.00811966 -0.02651433 -0.06074057]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 15 ] state=[-0.01227272  0.00811966 -0.02651433 -0.06074057], action=0, reward=1.0, next_state=[-0.01211033 -0.18661229 -0.02772914  0.2234604 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 16 ] state=[-0.01211033 -0.18661229 -0.02772914  0.2234604 ], action=0, reward=1.0, next_state=[-0.01584257 -0.38132717 -0.02325993  0.50726911]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 17 ] state=[-0.01584257 -0.38132717 -0.02325993  0.50726911], action=1, reward=1.0, next_state=[-0.02346911 -0.18588533 -0.01311455  0.20734785]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 18 ] state=[-0.02346911 -0.18588533 -0.01311455  0.20734785], action=0, reward=1.0, next_state=[-0.02718682 -0.38081732 -0.00896759  0.49586512]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 19 ] state=[-0.02718682 -0.38081732 -0.00896759  0.49586512], action=1, reward=1.0, next_state=[-0.03480317 -0.18557007  0.00094971  0.20036958]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 20 ] state=[-0.03480317 -0.18557007  0.00094971  0.20036958], action=0, reward=1.0, next_state=[-0.03851457 -0.38070559  0.0049571   0.49335195]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 21 ] state=[-0.03851457 -0.38070559  0.0049571   0.49335195], action=1, reward=1.0, next_state=[-0.04612868 -0.1856539   0.01482414  0.2022354 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 22 ] state=[-0.04612868 -0.1856539   0.01482414  0.2022354 ], action=1, reward=1.0, next_state=[-0.04984176  0.00925294  0.01886885 -0.0857346 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 23 ] state=[-0.04984176  0.00925294  0.01886885 -0.0857346 ], action=0, reward=1.0, next_state=[-0.0496567  -0.18613433  0.01715416  0.21284124]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 24 ] state=[-0.0496567  -0.18613433  0.01715416  0.21284124], action=1, reward=1.0, next_state=[-0.05337939  0.00873821  0.02141098 -0.07438149]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 25 ] state=[-0.05337939  0.00873821  0.02141098 -0.07438149], action=1, reward=1.0, next_state=[-0.05320462  0.20354678  0.01992335 -0.36023301]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 26 ] state=[-0.05320462  0.20354678  0.01992335 -0.36023301], action=1, reward=1.0, next_state=[-0.04913369  0.39837994  0.01271869 -0.64656767]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 27 ] state=[-0.04913369  0.39837994  0.01271869 -0.64656767], action=0, reward=1.0, next_state=[-4.11660884e-02  2.03083100e-01 -2.12659708e-04 -3.49906917e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 28 ] state=[-4.11660884e-02  2.03083100e-01 -2.12659708e-04 -3.49906917e-01], action=0, reward=1.0, next_state=[-0.03710443  0.00796417 -0.0072108  -0.05729106]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 29 ] state=[-0.03710443  0.00796417 -0.0072108  -0.05729106], action=1, reward=1.0, next_state=[-0.03694514  0.20318877 -0.00835662 -0.35224029]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 30 ] state=[-0.03694514  0.20318877 -0.00835662 -0.35224029], action=0, reward=1.0, next_state=[-0.03288137  0.00818664 -0.01540143 -0.06220413]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 31 ] state=[-0.03288137  0.00818664 -0.01540143 -0.06220413], action=1, reward=1.0, next_state=[-0.03271763  0.20352599 -0.01664551 -0.35970624]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 32 ] state=[-0.03271763  0.20352599 -0.01664551 -0.35970624], action=1, reward=1.0, next_state=[-0.02864711  0.39888056 -0.02383963 -0.65759105]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 33 ] state=[-0.02864711  0.39888056 -0.02383963 -0.65759105], action=1, reward=1.0, next_state=[-0.0206695   0.59432609 -0.03699145 -0.95768424]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 34 ] state=[-0.0206695   0.59432609 -0.03699145 -0.95768424], action=0, reward=1.0, next_state=[-0.00878298  0.39972055 -0.05614514 -0.67684863]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 35 ] state=[-0.00878298  0.39972055 -0.05614514 -0.67684863], action=0, reward=1.0, next_state=[-0.00078857  0.2054218  -0.06968211 -0.40235775]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 36 ] state=[-0.00078857  0.2054218  -0.06968211 -0.40235775], action=1, reward=1.0, next_state=[ 0.00331987  0.40145933 -0.07772927 -0.71617039]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 37 ] state=[ 0.00331987  0.40145933 -0.07772927 -0.71617039], action=0, reward=1.0, next_state=[ 0.01134905  0.20749437 -0.09205267 -0.44893083]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 38 ] state=[ 0.01134905  0.20749437 -0.09205267 -0.44893083], action=1, reward=1.0, next_state=[ 0.01549894  0.4037896  -0.10103129 -0.76915233]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 39 ] state=[ 0.01549894  0.4037896  -0.10103129 -0.76915233], action=0, reward=1.0, next_state=[ 0.02357473  0.21019264 -0.11641434 -0.5098904 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 40 ] state=[ 0.02357473  0.21019264 -0.11641434 -0.5098904 ], action=0, reward=1.0, next_state=[ 0.02777858  0.01688656 -0.12661214 -0.25604243]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 41 ] state=[ 0.02777858  0.01688656 -0.12661214 -0.25604243], action=1, reward=1.0, next_state=[ 0.02811632  0.21356742 -0.13173299 -0.58582679]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 42 ] state=[ 0.02811632  0.21356742 -0.13173299 -0.58582679], action=0, reward=1.0, next_state=[ 0.03238766  0.02051267 -0.14344953 -0.33737126]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 43 ] state=[ 0.03238766  0.02051267 -0.14344953 -0.33737126], action=0, reward=1.0, next_state=[ 0.03279792 -0.17230764 -0.15019695 -0.09314122]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 44 ] state=[ 0.03279792 -0.17230764 -0.15019695 -0.09314122], action=0, reward=1.0, next_state=[ 0.02935176 -0.36499339 -0.15205978  0.14864135]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 45 ] state=[ 0.02935176 -0.36499339 -0.15205978  0.14864135], action=0, reward=1.0, next_state=[ 0.0220519  -0.55764786 -0.14908695  0.38975506]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 46 ] state=[ 0.0220519  -0.55764786 -0.14908695  0.38975506], action=1, reward=1.0, next_state=[ 0.01089894 -0.36075919 -0.14129185  0.05402877]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 47 ] state=[ 0.01089894 -0.36075919 -0.14129185  0.05402877], action=1, reward=1.0, next_state=[ 0.00368376 -0.16392378 -0.14021128 -0.27968384]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 48 ] state=[ 0.00368376 -0.16392378 -0.14021128 -0.27968384], action=0, reward=1.0, next_state=[ 0.00040528 -0.35679619 -0.14580495 -0.03430155]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 49 ] state=[ 0.00040528 -0.35679619 -0.14580495 -0.03430155], action=1, reward=1.0, next_state=[-0.00673064 -0.15991695 -0.14649098 -0.36920179]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 50 ] state=[-0.00673064 -0.15991695 -0.14649098 -0.36920179], action=1, reward=1.0, next_state=[-0.00992898  0.03694964 -0.15387502 -0.70425331]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 51 ] state=[-0.00992898  0.03694964 -0.15387502 -0.70425331], action=0, reward=1.0, next_state=[-0.00918999 -0.15574313 -0.16796008 -0.4636902 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 194 ][ timestamp 52 ] state=[-0.00918999 -0.15574313 -0.16796008 -0.4636902 ], action=0, reward=1.0, next_state=[-0.01230485 -0.34814298 -0.17723389 -0.22830008]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 53 ] state=[-0.01230485 -0.34814298 -0.17723389 -0.22830008], action=1, reward=1.0, next_state=[-0.01926771 -0.15098951 -0.18179989 -0.5712321 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 54 ] state=[-0.01926771 -0.15098951 -0.18179989 -0.5712321 ], action=1, reward=1.0, next_state=[-0.0222875   0.04615355 -0.19322453 -0.91522851]\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ][ timestamp 55 ] state=[-0.0222875   0.04615355 -0.19322453 -0.91522851], action=0, reward=-1.0, next_state=[-0.02136443 -0.14590448 -0.2115291  -0.68895789]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 194: Exploration_rate=0.01. Score=55.\n",
      "[ episode 195 ] state=[-0.02503787  0.00643576  0.03536584  0.00891365]\n",
      "[ episode 195 ][ timestamp 1 ] state=[-0.02503787  0.00643576  0.03536584  0.00891365], action=0, reward=1.0, next_state=[-0.02490915 -0.18917507  0.03554411  0.31254181]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 2 ] state=[-0.02490915 -0.18917507  0.03554411  0.31254181], action=1, reward=1.0, next_state=[-0.02869265  0.00542296  0.04179495  0.0312769 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 3 ] state=[-0.02869265  0.00542296  0.04179495  0.0312769 ], action=1, reward=1.0, next_state=[-0.02858419  0.19992141  0.04242049 -0.24793185]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 4 ] state=[-0.02858419  0.19992141  0.04242049 -0.24793185], action=0, reward=1.0, next_state=[-0.02458577  0.00422011  0.03746185  0.0578239 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 5 ] state=[-0.02458577  0.00422011  0.03746185  0.0578239 ], action=1, reward=1.0, next_state=[-0.02450136  0.19878546  0.03861833 -0.22280815]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 6 ] state=[-0.02450136  0.19878546  0.03861833 -0.22280815], action=0, reward=1.0, next_state=[-0.02052565  0.00313342  0.03416217  0.08180205]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 7 ] state=[-0.02052565  0.00313342  0.03416217  0.08180205], action=0, reward=1.0, next_state=[-0.02046299 -0.19246116  0.03579821  0.38506447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 8 ] state=[-0.02046299 -0.19246116  0.03579821  0.38506447], action=1, reward=1.0, next_state=[-0.02431221  0.00213476  0.0434995   0.10388001]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 9 ] state=[-0.02431221  0.00213476  0.0434995   0.10388001], action=0, reward=1.0, next_state=[-0.02426951 -0.19358271  0.0455771   0.40996333]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 10 ] state=[-0.02426951 -0.19358271  0.0455771   0.40996333], action=0, reward=1.0, next_state=[-0.02814117 -0.38932019  0.05377636  0.71665969]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 11 ] state=[-0.02814117 -0.38932019  0.05377636  0.71665969], action=1, reward=1.0, next_state=[-0.03592757 -0.19498214  0.06810956  0.44137664]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 12 ] state=[-0.03592757 -0.19498214  0.06810956  0.44137664], action=1, reward=1.0, next_state=[-0.03982721 -0.00088681  0.07693709  0.17091741]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 13 ] state=[-0.03982721 -0.00088681  0.07693709  0.17091741], action=0, reward=1.0, next_state=[-0.03984495 -0.19702083  0.08035544  0.48684533]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 14 ] state=[-0.03984495 -0.19702083  0.08035544  0.48684533], action=1, reward=1.0, next_state=[-0.04378537 -0.00311919  0.09009234  0.22053047]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 15 ] state=[-0.04378537 -0.00311919  0.09009234  0.22053047], action=1, reward=1.0, next_state=[-0.04384775  0.19060721  0.09450295 -0.0424293 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 16 ] state=[-0.04384775  0.19060721  0.09450295 -0.0424293 ], action=1, reward=1.0, next_state=[-0.04003561  0.38425591  0.09365437 -0.30386369]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 17 ] state=[-0.04003561  0.38425591  0.09365437 -0.30386369], action=0, reward=1.0, next_state=[-0.03235049  0.18793271  0.08757709  0.01682472]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 18 ] state=[-0.03235049  0.18793271  0.08757709  0.01682472], action=0, reward=1.0, next_state=[-0.02859183 -0.00832893  0.08791359  0.3358037 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 19 ] state=[-0.02859183 -0.00832893  0.08791359  0.3358037 ], action=1, reward=1.0, next_state=[-0.02875841  0.18543906  0.09462966  0.07208751]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 20 ] state=[-0.02875841  0.18543906  0.09462966  0.07208751], action=1, reward=1.0, next_state=[-0.02504963  0.37908595  0.09607141 -0.18930363]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 21 ] state=[-0.02504963  0.37908595  0.09607141 -0.18930363], action=0, reward=1.0, next_state=[-0.01746791  0.18273021  0.09228534  0.13207336]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 22 ] state=[-0.01746791  0.18273021  0.09228534  0.13207336], action=1, reward=1.0, next_state=[-0.01381331  0.37641736  0.09492681 -0.13012769]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 23 ] state=[-0.01381331  0.37641736  0.09492681 -0.13012769], action=0, reward=1.0, next_state=[-0.00628496  0.18007282  0.09232425  0.19092974]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 24 ] state=[-0.00628496  0.18007282  0.09232425  0.19092974], action=0, reward=1.0, next_state=[-0.0026835  -0.01624031  0.09614285  0.51125012]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 25 ] state=[-0.0026835  -0.01624031  0.09614285  0.51125012], action=0, reward=1.0, next_state=[-0.00300831 -0.21257575  0.10636785  0.83261569]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 26 ] state=[-0.00300831 -0.21257575  0.10636785  0.83261569], action=0, reward=1.0, next_state=[-0.00725983 -0.4089779   0.12302016  1.15676712]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 27 ] state=[-0.00725983 -0.4089779   0.12302016  1.15676712], action=1, reward=1.0, next_state=[-0.01543938 -0.2156553   0.14615551  0.90505152]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 28 ] state=[-0.01543938 -0.2156553   0.14615551  0.90505152], action=1, reward=1.0, next_state=[-0.01975249 -0.02278265  0.16425654  0.66164398]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 29 ] state=[-0.01975249 -0.02278265  0.16425654  0.66164398], action=0, reward=1.0, next_state=[-0.02020814 -0.21976292  0.17748942  1.00121196]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 30 ] state=[-0.02020814 -0.21976292  0.17748942  1.00121196], action=1, reward=1.0, next_state=[-0.0246034  -0.0273994   0.19751366  0.76910806]\n",
      "[ Experience replay ] starts\n",
      "[ episode 195 ][ timestamp 31 ] state=[-0.0246034  -0.0273994   0.19751366  0.76910806], action=1, reward=-1.0, next_state=[-0.02515139  0.16453547  0.21289582  0.54449549]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 195: Exploration_rate=0.01. Score=31.\n",
      "[ episode 196 ] state=[-0.04779114  0.01644928  0.04742369 -0.04810195]\n",
      "[ episode 196 ][ timestamp 1 ] state=[-0.04779114  0.01644928  0.04742369 -0.04810195], action=0, reward=1.0, next_state=[-0.04746216 -0.17931947  0.04646165  0.25915838]\n",
      "[ Experience replay ] starts\n",
      "[ episode 196 ][ timestamp 2 ] state=[-0.04746216 -0.17931947  0.04646165  0.25915838], action=1, reward=1.0, next_state=[-0.05104854  0.01510946  0.05164482 -0.01851549]\n",
      "[ Experience replay ] starts\n",
      "[ episode 196 ][ timestamp 3 ] state=[-0.05104854  0.01510946  0.05164482 -0.01851549], action=1, reward=1.0, next_state=[-0.05074636  0.20945419  0.05127451 -0.29446707]\n",
      "[ Experience replay ] starts\n",
      "[ episode 196 ][ timestamp 4 ] state=[-0.05074636  0.20945419  0.05127451 -0.29446707], action=0, reward=1.0, next_state=[-0.04655727  0.01364015  0.04538517  0.01393607]\n",
      "[ Experience replay ] starts\n",
      "[ episode 196 ][ timestamp 5 ] state=[-0.04655727  0.01364015  0.04538517  0.01393607], action=0, reward=1.0, next_state=[-0.04628447 -0.18210231  0.04566389  0.32058607]\n",
      "[ Experience replay ] starts\n",
      "[ episode 196 ][ timestamp 6 ] state=[-0.04628447 -0.18210231  0.04566389  0.32058607], action=1, reward=1.0, next_state=[-0.04992651  0.01234059  0.05207561  0.04264627]\n",
      "[ Experience replay ] starts\n",
      "[ episode 196 ][ timestamp 7 ] state=[-0.04992651  0.01234059  0.05207561  0.04264627], action=0, reward=1.0, next_state=[-0.0496797  -0.18348793  0.05292854  0.35129417]\n",
      "[ Experience replay ] starts\n",
      "[ episode 196 ][ timestamp 8 ] state=[-0.0496797  -0.18348793  0.05292854  0.35129417], action=1, reward=1.0, next_state=[-0.05334946  0.01084294  0.05995442  0.0757598 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 196 ][ timestamp 9 ] state=[-0.05334946  0.01084294  0.05995442  0.0757598 ], action=0, reward=1.0, next_state=[-0.0531326  -0.18508499  0.06146962  0.38673968]\n",
      "[ Experience replay ] starts\n",
      "[ episode 196 ][ timestamp 10 ] state=[-0.0531326  -0.18508499  0.06146962  0.38673968], action=0, reward=1.0, next_state=[-0.0568343  -0.38102321  0.06920441  0.69815262]\n",
      "[ Experience replay ] starts\n",
      "[ episode 196 ][ timestamp 11 ] state=[-0.0568343  -0.38102321  0.06920441  0.69815262], action=1, reward=1.0, next_state=[-0.06445477 -0.18692562  0.08316746  0.42803299]\n",
      "[ Experience replay ] starts\n",
      "[ episode 196 ][ timestamp 12 ] state=[-0.06445477 -0.18692562  0.08316746  0.42803299], action=1, reward=1.0, next_state=[-0.06819328  0.00692605  0.09172812  0.1626836 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 196 ][ timestamp 13 ] state=[-0.06819328  0.00692605  0.09172812  0.1626836 ], action=1, reward=1.0, next_state=[-0.06805476  0.2006233   0.09498179 -0.09971054]\n",
      "[ Experience replay ] starts\n",
      "[ episode 196 ][ timestamp 14 ] state=[-0.06805476  0.2006233   0.09498179 -0.09971054], action=0, reward=1.0, next_state=[-0.06404229  0.00427749  0.09298758  0.22136335]\n",
      "[ Experience replay ] starts\n",
      "[ episode 196 ][ timestamp 15 ] state=[-0.06404229  0.00427749  0.09298758  0.22136335], action=0, reward=1.0, next_state=[-0.06395674 -0.19204208  0.09741485  0.54186945]\n",
      "[ Experience replay ] starts\n",
      "[ episode 196 ][ timestamp 16 ] state=[-0.06395674 -0.19204208  0.09741485  0.54186945], action=1, reward=1.0, next_state=[-0.06779758  0.00158546  0.10825224  0.28139983]\n",
      "[ Experience replay ] starts\n",
      "[ episode 196 ][ timestamp 17 ] state=[-0.06779758  0.00158546  0.10825224  0.28139983], action=0, reward=1.0, next_state=[-0.06776587 -0.19490072  0.11388024  0.60616792]\n",
      "[ Experience replay ] starts\n",
      "[ episode 196 ][ timestamp 18 ] state=[-0.06776587 -0.19490072  0.11388024  0.60616792], action=1, reward=1.0, next_state=[-0.07166389 -0.00153985  0.12600359  0.35141379]\n",
      "[ Experience replay ] starts\n",
      "[ episode 196 ][ timestamp 19 ] state=[-0.07166389 -0.00153985  0.12600359  0.35141379], action=0, reward=1.0, next_state=[-0.07169469 -0.1982074   0.13303187  0.68102347]\n",
      "[ Experience replay ] starts\n",
      "[ episode 196 ][ timestamp 20 ] state=[-0.07169469 -0.1982074   0.13303187  0.68102347], action=0, reward=1.0, next_state=[-0.07565883 -0.39490157  0.14665234  1.01245394]\n",
      "[ Experience replay ] starts\n",
      "[ episode 196 ][ timestamp 21 ] state=[-0.07565883 -0.39490157  0.14665234  1.01245394], action=1, reward=1.0, next_state=[-0.08355687 -0.20200805  0.16690142  0.7691809 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 196 ][ timestamp 22 ] state=[-0.08355687 -0.20200805  0.16690142  0.7691809 ], action=0, reward=1.0, next_state=[-0.08759703 -0.39898574  0.18228504  1.10938323]\n",
      "[ Experience replay ] starts\n",
      "[ episode 196 ][ timestamp 23 ] state=[-0.08759703 -0.39898574  0.18228504  1.10938323], action=1, reward=1.0, next_state=[-0.09557674 -0.20666527  0.2044727   0.87897756]\n",
      "[ Experience replay ] starts\n",
      "[ episode 196 ][ timestamp 24 ] state=[-0.09557674 -0.20666527  0.2044727   0.87897756], action=0, reward=-1.0, next_state=[-0.09971005 -0.40389056  0.22205225  1.22834962]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 196: Exploration_rate=0.01. Score=24.\n",
      "[ episode 197 ] state=[ 0.03145619  0.02278509 -0.01538882  0.00777884]\n",
      "[ episode 197 ][ timestamp 1 ] state=[ 0.03145619  0.02278509 -0.01538882  0.00777884], action=1, reward=1.0, next_state=[ 0.03191189  0.21812432 -0.01523324 -0.28971944]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 2 ] state=[ 0.03191189  0.21812432 -0.01523324 -0.28971944], action=1, reward=1.0, next_state=[ 0.03627438  0.41346014 -0.02102763 -0.58716758]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 3 ] state=[ 0.03627438  0.41346014 -0.02102763 -0.58716758], action=0, reward=1.0, next_state=[ 0.04454358  0.21863889 -0.03277098 -0.30118199]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 4 ] state=[ 0.04454358  0.21863889 -0.03277098 -0.30118199], action=0, reward=1.0, next_state=[ 0.04891636  0.02399898 -0.03879462 -0.01901182]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 5 ] state=[ 0.04891636  0.02399898 -0.03879462 -0.01901182], action=1, reward=1.0, next_state=[ 0.04939634  0.2196552  -0.03917485 -0.32367807]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 6 ] state=[ 0.04939634  0.2196552  -0.03917485 -0.32367807], action=0, reward=1.0, next_state=[ 0.05378945  0.02511233 -0.04564842 -0.04360212]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 7 ] state=[ 0.05378945  0.02511233 -0.04564842 -0.04360212], action=0, reward=1.0, next_state=[ 0.05429169 -0.16932633 -0.04652046  0.23433606]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 8 ] state=[ 0.05429169 -0.16932633 -0.04652046  0.23433606], action=0, reward=1.0, next_state=[ 0.05090517 -0.3637538  -0.04183374  0.51198966]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 9 ] state=[ 0.05090517 -0.3637538  -0.04183374  0.51198966], action=1, reward=1.0, next_state=[ 0.04363009 -0.16806833 -0.03159394  0.20642273]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 10 ] state=[ 0.04363009 -0.16806833 -0.03159394  0.20642273], action=0, reward=1.0, next_state=[ 0.04026872 -0.36272458 -0.02746549  0.48897432]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 11 ] state=[ 0.04026872 -0.36272458 -0.02746549  0.48897432], action=0, reward=1.0, next_state=[ 0.03301423 -0.55744849 -0.017686    0.77287619]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 12 ] state=[ 0.03301423 -0.55744849 -0.017686    0.77287619], action=0, reward=1.0, next_state=[ 0.02186526 -0.7523227  -0.00222848  1.05994237]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 13 ] state=[ 0.02186526 -0.7523227  -0.00222848  1.05994237], action=1, reward=1.0, next_state=[ 0.00681881 -0.5571713   0.01897037  0.76656083]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 14 ] state=[ 0.00681881 -0.5571713   0.01897037  0.76656083], action=1, reward=1.0, next_state=[-0.00432462 -0.3623156   0.03430158  0.47990683]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 15 ] state=[-0.00432462 -0.3623156   0.03430158  0.47990683], action=1, reward=1.0, next_state=[-0.01157093 -0.16769424  0.04389972  0.19822921]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 16 ] state=[-0.01157093 -0.16769424  0.04389972  0.19822921], action=0, reward=1.0, next_state=[-0.01492482 -0.36341571  0.04786431  0.50443093]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 17 ] state=[-0.01492482 -0.36341571  0.04786431  0.50443093], action=1, reward=1.0, next_state=[-0.02219313 -0.16899984  0.05795292  0.22720785]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 18 ] state=[-0.02219313 -0.16899984  0.05795292  0.22720785], action=1, reward=1.0, next_state=[-0.02557313  0.02524807  0.06249708 -0.04664624]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 19 ] state=[-0.02557313  0.02524807  0.06249708 -0.04664624], action=1, reward=1.0, next_state=[-0.02506816  0.21942077  0.06156416 -0.31897448]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 20 ] state=[-0.02506816  0.21942077  0.06156416 -0.31897448], action=0, reward=1.0, next_state=[-0.02067975  0.02347851  0.05518467 -0.00752946]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 21 ] state=[-0.02067975  0.02347851  0.05518467 -0.00752946], action=1, reward=1.0, next_state=[-0.02021018  0.21776739  0.05503408 -0.28230308]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 22 ] state=[-0.02021018  0.21776739  0.05503408 -0.28230308], action=0, reward=1.0, next_state=[-0.01585483  0.0219054   0.04938802  0.02721695]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 23 ] state=[-0.01585483  0.0219054   0.04938802  0.02721695], action=1, reward=1.0, next_state=[-0.01541672  0.21628558  0.04993235 -0.24948362]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 24 ] state=[-0.01541672  0.21628558  0.04993235 -0.24948362], action=0, reward=1.0, next_state=[-0.01109101  0.02048743  0.04494268  0.05852156]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 25 ] state=[-0.01109101  0.02048743  0.04494268  0.05852156], action=0, reward=1.0, next_state=[-0.01068126 -0.17524914  0.04611311  0.36503865]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 197 ][ timestamp 26 ] state=[-0.01068126 -0.17524914  0.04611311  0.36503865], action=0, reward=1.0, next_state=[-0.01418625 -0.37099505  0.05341389  0.67189785]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 27 ] state=[-0.01418625 -0.37099505  0.05341389  0.67189785], action=0, reward=1.0, next_state=[-0.02160615 -0.56681722  0.06685184  0.98090841]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 28 ] state=[-0.02160615 -0.56681722  0.06685184  0.98090841], action=0, reward=1.0, next_state=[-0.03294249 -0.76276838  0.08647001  1.29381839]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 29 ] state=[-0.03294249 -0.76276838  0.08647001  1.29381839], action=1, reward=1.0, next_state=[-0.04819786 -0.56884516  0.11234638  1.02941087]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 30 ] state=[-0.04819786 -0.56884516  0.11234638  1.02941087], action=0, reward=1.0, next_state=[-0.05957476 -0.76526825  0.1329346   1.35514847]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 31 ] state=[-0.05957476 -0.76526825  0.1329346   1.35514847], action=1, reward=1.0, next_state=[-0.07488013 -0.57204131  0.16003757  1.10683302]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 32 ] state=[-0.07488013 -0.57204131  0.16003757  1.10683302], action=0, reward=1.0, next_state=[-0.08632095 -0.76886327  0.18217423  1.44514372]\n",
      "[ Experience replay ] starts\n",
      "[ episode 197 ][ timestamp 33 ] state=[-0.08632095 -0.76886327  0.18217423  1.44514372], action=1, reward=-1.0, next_state=[-0.10169822 -0.57638965  0.2110771   1.21447431]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 197: Exploration_rate=0.01. Score=33.\n",
      "[ episode 198 ] state=[-0.02861371  0.01038549  0.01862435  0.00107502]\n",
      "[ episode 198 ][ timestamp 1 ] state=[-0.02861371  0.01038549  0.01862435  0.00107502], action=0, reward=1.0, next_state=[-0.028406   -0.18499854  0.01864585  0.29957548]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 2 ] state=[-0.028406   -0.18499854  0.01864585  0.29957548], action=1, reward=1.0, next_state=[-0.03210597  0.00985274  0.02463736  0.01283093]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 3 ] state=[-0.03210597  0.00985274  0.02463736  0.01283093], action=0, reward=1.0, next_state=[-0.03190892 -0.18561372  0.02489398  0.31318429]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 4 ] state=[-0.03190892 -0.18561372  0.02489398  0.31318429], action=1, reward=1.0, next_state=[-0.03562119  0.00914491  0.03115766  0.02845493]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 5 ] state=[-0.03562119  0.00914491  0.03115766  0.02845493], action=1, reward=1.0, next_state=[-0.03543829  0.20380651  0.03172676 -0.25423687]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 6 ] state=[-0.03543829  0.20380651  0.03172676 -0.25423687], action=0, reward=1.0, next_state=[-0.03136216  0.00824626  0.02664202  0.04828199]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 7 ] state=[-0.03136216  0.00824626  0.02664202  0.04828199], action=1, reward=1.0, next_state=[-0.03119724  0.20297625  0.02760766 -0.23587752]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 8 ] state=[-0.03119724  0.20297625  0.02760766 -0.23587752], action=1, reward=1.0, next_state=[-0.02713771  0.39769311  0.02289011 -0.51972589]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 9 ] state=[-0.02713771  0.39769311  0.02289011 -0.51972589], action=0, reward=1.0, next_state=[-0.01918385  0.20225652  0.0124956  -0.21991869]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 10 ] state=[-0.01918385  0.20225652  0.0124956  -0.21991869], action=1, reward=1.0, next_state=[-0.01513872  0.39719765  0.00809722 -0.50863394]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 11 ] state=[-0.01513872  0.39719765  0.00809722 -0.50863394], action=0, reward=1.0, next_state=[-0.00719477  0.20196255 -0.00207546 -0.21341035]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 12 ] state=[-0.00719477  0.20196255 -0.00207546 -0.21341035], action=1, reward=1.0, next_state=[-0.00315552  0.39711412 -0.00634366 -0.50674725]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 13 ] state=[-0.00315552  0.39711412 -0.00634366 -0.50674725], action=1, reward=1.0, next_state=[ 0.00478677  0.59232488 -0.01647861 -0.80142252]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 14 ] state=[ 0.00478677  0.59232488 -0.01647861 -0.80142252], action=0, reward=1.0, next_state=[ 0.01663326  0.39743276 -0.03250706 -0.51396852]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 15 ] state=[ 0.01663326  0.39743276 -0.03250706 -0.51396852], action=0, reward=1.0, next_state=[ 0.02458192  0.20278335 -0.04278643 -0.23170405]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 16 ] state=[ 0.02458192  0.20278335 -0.04278643 -0.23170405], action=0, reward=1.0, next_state=[ 0.02863759  0.00829806 -0.04742051  0.04718151]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 17 ] state=[ 0.02863759  0.00829806 -0.04742051  0.04718151], action=0, reward=1.0, next_state=[ 0.02880355 -0.18611297 -0.04647688  0.32453383]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 18 ] state=[ 0.02880355 -0.18611297 -0.04647688  0.32453383], action=1, reward=1.0, next_state=[ 0.02508129  0.00963888 -0.0399862   0.01756385]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 19 ] state=[ 0.02508129  0.00963888 -0.0399862   0.01756385], action=0, reward=1.0, next_state=[ 0.02527407 -0.18488749 -0.03963493  0.29736736]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 20 ] state=[ 0.02527407 -0.18488749 -0.03963493  0.29736736], action=0, reward=1.0, next_state=[ 0.02157632 -0.37942268 -0.03368758  0.57729136]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 21 ] state=[ 0.02157632 -0.37942268 -0.03368758  0.57729136], action=1, reward=1.0, next_state=[ 0.01398786 -0.18384515 -0.02214175  0.27418923]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 22 ] state=[ 0.01398786 -0.18384515 -0.02214175  0.27418923], action=1, reward=1.0, next_state=[ 0.01031096  0.01158561 -0.01665797 -0.02539419]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 23 ] state=[ 0.01031096  0.01158561 -0.01665797 -0.02539419], action=0, reward=1.0, next_state=[ 0.01054267 -0.18329354 -0.01716585  0.26198676]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 24 ] state=[ 0.01054267 -0.18329354 -0.01716585  0.26198676], action=1, reward=1.0, next_state=[ 0.0068768   0.01206918 -0.01192612 -0.03606067]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 25 ] state=[ 0.0068768   0.01206918 -0.01192612 -0.03606067], action=1, reward=1.0, next_state=[ 0.00711818  0.20736011 -0.01264733 -0.33248242]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 26 ] state=[ 0.00711818  0.20736011 -0.01264733 -0.33248242], action=0, reward=1.0, next_state=[ 0.01126539  0.01242044 -0.01929698 -0.04381451]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 27 ] state=[ 0.01126539  0.01242044 -0.01929698 -0.04381451], action=0, reward=1.0, next_state=[ 0.0115138  -0.18241956 -0.02017327  0.24271812]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 28 ] state=[ 0.0115138  -0.18241956 -0.02017327  0.24271812], action=0, reward=1.0, next_state=[ 0.0078654  -0.37724764 -0.01531891  0.52897023]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 29 ] state=[ 0.0078654  -0.37724764 -0.01531891  0.52897023], action=1, reward=1.0, next_state=[ 0.00032045 -0.18191356 -0.0047395   0.23149991]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 30 ] state=[ 0.00032045 -0.18191356 -0.0047395   0.23149991], action=0, reward=1.0, next_state=[-3.31782003e-03 -3.76967468e-01 -1.09503454e-04  5.22684075e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 31 ] state=[-3.31782003e-03 -3.76967468e-01 -1.09503454e-04  5.22684075e-01], action=1, reward=1.0, next_state=[-0.01085717 -0.18184398  0.01034418  0.22996664]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 32 ] state=[-0.01085717 -0.18184398  0.01034418  0.22996664], action=0, reward=1.0, next_state=[-0.01449405 -0.3771122   0.01494351  0.52589445]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 33 ] state=[-0.01449405 -0.3771122   0.01494351  0.52589445], action=1, reward=1.0, next_state=[-0.02203629 -0.18220369  0.0254614   0.23795754]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 198 ][ timestamp 34 ] state=[-0.02203629 -0.18220369  0.0254614   0.23795754], action=0, reward=1.0, next_state=[-0.02568037 -0.37767996  0.03022055  0.53856175]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 35 ] state=[-0.02568037 -0.37767996  0.03022055  0.53856175], action=1, reward=1.0, next_state=[-0.03323397 -0.1829956   0.04099179  0.25555206]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 36 ] state=[-0.03323397 -0.1829956   0.04099179  0.25555206], action=0, reward=1.0, next_state=[-0.03689388 -0.37867812  0.04610283  0.56087747]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 37 ] state=[-0.03689388 -0.37867812  0.04610283  0.56087747], action=0, reward=1.0, next_state=[-0.04446744 -0.57441575  0.05732038  0.86772138]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 38 ] state=[-0.04446744 -0.57441575  0.05732038  0.86772138], action=0, reward=1.0, next_state=[-0.05595576 -0.77026885  0.0746748   1.17786149]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 39 ] state=[-0.05595576 -0.77026885  0.0746748   1.17786149], action=1, reward=1.0, next_state=[-0.07136113 -0.57619193  0.09823203  0.9094914 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 40 ] state=[-0.07136113 -0.57619193  0.09823203  0.9094914 ], action=0, reward=1.0, next_state=[-0.08288497 -0.77249632  0.11642186  1.23136224]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 41 ] state=[-0.08288497 -0.77249632  0.11642186  1.23136224], action=1, reward=1.0, next_state=[-0.0983349  -0.57904814  0.14104911  0.97730502]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 42 ] state=[-0.0983349  -0.57904814  0.14104911  0.97730502], action=0, reward=1.0, next_state=[-0.10991586 -0.77575077  0.16059521  1.31075987]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 43 ] state=[-0.10991586 -0.77575077  0.16059521  1.31075987], action=1, reward=1.0, next_state=[-0.12543088 -0.58298522  0.1868104   1.07234452]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 44 ] state=[-0.12543088 -0.58298522  0.1868104   1.07234452], action=1, reward=1.0, next_state=[-0.13709058 -0.39075732  0.20825729  0.84362272]\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ][ timestamp 45 ] state=[-0.13709058 -0.39075732  0.20825729  0.84362272], action=1, reward=-1.0, next_state=[-0.14490573 -0.19899273  0.22512975  0.62297712]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 198: Exploration_rate=0.01. Score=45.\n",
      "[ episode 199 ] state=[ 3.89629549e-02  1.68746965e-02 -7.08327466e-05 -4.33471963e-02]\n",
      "[ episode 199 ][ timestamp 1 ] state=[ 3.89629549e-02  1.68746965e-02 -7.08327466e-05 -4.33471963e-02], action=0, reward=1.0, next_state=[ 0.03930045 -0.17824624 -0.00093778  0.24931338]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 2 ] state=[ 0.03930045 -0.17824624 -0.00093778  0.24931338], action=1, reward=1.0, next_state=[ 0.03573552  0.01688909  0.00404849 -0.04366519]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 3 ] state=[ 0.03573552  0.01688909  0.00404849 -0.04366519], action=1, reward=1.0, next_state=[ 0.03607331  0.21195276  0.00317519 -0.33506804]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 4 ] state=[ 0.03607331  0.21195276  0.00317519 -0.33506804], action=0, reward=1.0, next_state=[ 0.04031236  0.01678576 -0.00352617 -0.04138552]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 5 ] state=[ 0.04031236  0.01678576 -0.00352617 -0.04138552], action=1, reward=1.0, next_state=[ 0.04064808  0.2119581  -0.00435388 -0.3351789 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 6 ] state=[ 0.04064808  0.2119581  -0.00435388 -0.3351789 ], action=0, reward=1.0, next_state=[ 0.04488724  0.01689838 -0.01105746 -0.04387213]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 7 ] state=[ 0.04488724  0.01689838 -0.01105746 -0.04387213], action=1, reward=1.0, next_state=[ 0.04522521  0.21217713 -0.0119349  -0.34002318]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 8 ] state=[ 0.04522521  0.21217713 -0.0119349  -0.34002318], action=0, reward=1.0, next_state=[ 0.04946875  0.01722701 -0.01873537 -0.05112761]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 9 ] state=[ 0.04946875  0.01722701 -0.01873537 -0.05112761], action=0, reward=1.0, next_state=[ 0.04981329 -0.17762135 -0.01975792  0.23558577]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 10 ] state=[ 0.04981329 -0.17762135 -0.01975792  0.23558577], action=0, reward=1.0, next_state=[ 0.04626086 -0.37245552 -0.0150462   0.52197154]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 11 ] state=[ 0.04626086 -0.37245552 -0.0150462   0.52197154], action=1, reward=1.0, next_state=[ 0.03881175 -0.17712506 -0.00460677  0.22458558]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 12 ] state=[ 0.03881175 -0.17712506 -0.00460677  0.22458558], action=1, reward=1.0, next_state=[ 0.03526925  0.01806243 -0.00011506 -0.06954693]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 13 ] state=[ 0.03526925  0.01806243 -0.00011506 -0.06954693], action=0, reward=1.0, next_state=[ 0.0356305  -0.17705787 -0.001506    0.22309969]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 14 ] state=[ 0.0356305  -0.17705787 -0.001506    0.22309969], action=0, reward=1.0, next_state=[ 0.03208934 -0.37215826  0.00295599  0.51530719]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 15 ] state=[ 0.03208934 -0.37215826  0.00295599  0.51530719], action=0, reward=1.0, next_state=[ 0.02464618 -0.56732172  0.01326214  0.80892015]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 16 ] state=[ 0.02464618 -0.56732172  0.01326214  0.80892015], action=1, reward=1.0, next_state=[ 0.01329974 -0.37238398  0.02944054  0.52043822]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 17 ] state=[ 0.01329974 -0.37238398  0.02944054  0.52043822], action=0, reward=1.0, next_state=[ 0.00585206 -0.56790774  0.0398493   0.82225102]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 18 ] state=[ 0.00585206 -0.56790774  0.0398493   0.82225102], action=1, reward=1.0, next_state=[-0.00550609 -0.37335299  0.05629432  0.54236318]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 19 ] state=[-0.00550609 -0.37335299  0.05629432  0.54236318], action=0, reward=1.0, next_state=[-0.01297315 -0.56921907  0.06714159  0.85223867]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 20 ] state=[-0.01297315 -0.56921907  0.06714159  0.85223867], action=1, reward=1.0, next_state=[-0.02435753 -0.37507354  0.08418636  0.58140134]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 21 ] state=[-0.02435753 -0.37507354  0.08418636  0.58140134], action=1, reward=1.0, next_state=[-0.031859   -0.1812258   0.09581439  0.31638109]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 22 ] state=[-0.031859   -0.1812258   0.09581439  0.31638109], action=0, reward=1.0, next_state=[-0.03548352 -0.37757265  0.10214201  0.63767683]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 23 ] state=[-0.03548352 -0.37757265  0.10214201  0.63767683], action=1, reward=1.0, next_state=[-0.04303497 -0.18401219  0.11489555  0.37882696]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 24 ] state=[-0.04303497 -0.18401219  0.11489555  0.37882696], action=1, reward=1.0, next_state=[-0.04671522  0.00930659  0.12247209  0.12446569]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 25 ] state=[-0.04671522  0.00930659  0.12247209  0.12446569], action=0, reward=1.0, next_state=[-0.04652909 -0.18733769  0.1249614   0.45313956]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 26 ] state=[-0.04652909 -0.18733769  0.1249614   0.45313956], action=0, reward=1.0, next_state=[-0.05027584 -0.38398468  0.13402419  0.78245313]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 27 ] state=[-0.05027584 -0.38398468  0.13402419  0.78245313], action=1, reward=1.0, next_state=[-0.05795553 -0.1909343   0.14967325  0.53475967]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 28 ] state=[-0.05795553 -0.1909343   0.14967325  0.53475967], action=0, reward=1.0, next_state=[-0.06177422 -0.38780896  0.16036845  0.87060985]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 29 ] state=[-0.06177422 -0.38780896  0.16036845  0.87060985], action=1, reward=1.0, next_state=[-0.0695304  -0.19518899  0.17778064  0.63233379]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 30 ] state=[-0.0695304  -0.19518899  0.17778064  0.63233379], action=0, reward=1.0, next_state=[-0.07343418 -0.39228717  0.19042732  0.97531386]\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ][ timestamp 31 ] state=[-0.07343418 -0.39228717  0.19042732  0.97531386], action=1, reward=-1.0, next_state=[-0.08127992 -0.20015868  0.2099336   0.74797854]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 199: Exploration_rate=0.01. Score=31.\n",
      "[ episode 200 ] state=[ 0.0109699   0.03241533 -0.02406924  0.00413821]\n",
      "[ episode 200 ][ timestamp 1 ] state=[ 0.0109699   0.03241533 -0.02406924  0.00413821], action=0, reward=1.0, next_state=[ 0.0116182  -0.16235331 -0.02398647  0.28913088]\n",
      "[ Experience replay ] starts\n",
      "[ episode 200 ][ timestamp 2 ] state=[ 0.0116182  -0.16235331 -0.02398647  0.28913088], action=0, reward=1.0, next_state=[ 0.00837114 -0.35712515 -0.01820386  0.57415325]\n",
      "[ Experience replay ] starts\n",
      "[ episode 200 ][ timestamp 3 ] state=[ 0.00837114 -0.35712515 -0.01820386  0.57415325], action=1, reward=1.0, next_state=[ 0.00122863 -0.16175278 -0.00672079  0.2757916 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 200 ][ timestamp 4 ] state=[ 0.00122863 -0.16175278 -0.00672079  0.2757916 ], action=0, reward=1.0, next_state=[-0.00200642 -0.3567782  -0.00120496  0.56634723]\n",
      "[ Experience replay ] starts\n",
      "[ episode 200 ][ timestamp 5 ] state=[-0.00200642 -0.3567782  -0.00120496  0.56634723], action=1, reward=1.0, next_state=[-0.00914199 -0.16163937  0.01012198  0.27328494]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 200 ][ timestamp 6 ] state=[-0.00914199 -0.16163937  0.01012198  0.27328494], action=1, reward=1.0, next_state=[-0.01237477  0.0333367   0.01558768 -0.01618838]\n",
      "[ Experience replay ] starts\n",
      "[ episode 200 ][ timestamp 7 ] state=[-0.01237477  0.0333367   0.01558768 -0.01618838], action=0, reward=1.0, next_state=[-0.01170804 -0.16200528  0.01526392  0.2813716 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 200 ][ timestamp 8 ] state=[-0.01170804 -0.16200528  0.01526392  0.2813716 ], action=1, reward=1.0, next_state=[-0.01494815  0.03289565  0.02089135 -0.00645833]\n",
      "[ Experience replay ] starts\n",
      "[ episode 200 ][ timestamp 9 ] state=[-0.01494815  0.03289565  0.02089135 -0.00645833], action=1, reward=1.0, next_state=[-0.01429023  0.22771186  0.02076218 -0.29247726]\n",
      "[ Experience replay ] starts\n",
      "[ episode 200 ][ timestamp 10 ] state=[-0.01429023  0.22771186  0.02076218 -0.29247726], action=0, reward=1.0, next_state=[-0.009736    0.03230013  0.01491264  0.0066808 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 200 ][ timestamp 11 ] state=[-0.009736    0.03230013  0.01491264  0.0066808 ], action=0, reward=1.0, next_state=[-0.00908999 -0.16303248  0.01504625  0.30403129]\n",
      "[ Experience replay ] starts\n",
      "[ episode 200 ][ timestamp 12 ] state=[-0.00908999 -0.16303248  0.01504625  0.30403129], action=0, reward=1.0, next_state=[-0.01235064 -0.35836559  0.02112688  0.60142122]\n",
      "[ Experience replay ] starts\n",
      "[ episode 200 ][ timestamp 13 ] state=[-0.01235064 -0.35836559  0.02112688  0.60142122], action=0, reward=1.0, next_state=[-0.01951795 -0.55377661  0.0331553   0.90068318]\n",
      "[ Experience replay ] starts\n",
      "[ episode 200 ][ timestamp 14 ] state=[-0.01951795 -0.55377661  0.0331553   0.90068318], action=0, reward=1.0, next_state=[-0.03059349 -0.74933175  0.05116897  1.20360054]\n",
      "[ Experience replay ] starts\n",
      "[ episode 200 ][ timestamp 15 ] state=[-0.03059349 -0.74933175  0.05116897  1.20360054], action=1, reward=1.0, next_state=[-0.04558012 -0.55490728  0.07524098  0.92738267]\n",
      "[ Experience replay ] starts\n",
      "[ episode 200 ][ timestamp 16 ] state=[-0.04558012 -0.55490728  0.07524098  0.92738267], action=0, reward=1.0, next_state=[-0.05667827 -0.7509601   0.09378863  1.24272985]\n",
      "[ Experience replay ] starts\n",
      "[ episode 200 ][ timestamp 17 ] state=[-0.05667827 -0.7509601   0.09378863  1.24272985], action=1, reward=1.0, next_state=[-0.07169747 -0.55715861  0.11864323  0.98083867]\n",
      "[ Experience replay ] starts\n",
      "[ episode 200 ][ timestamp 18 ] state=[-0.07169747 -0.55715861  0.11864323  0.98083867], action=1, reward=1.0, next_state=[-0.08284064 -0.36380937  0.13826     0.72765297]\n",
      "[ Experience replay ] starts\n",
      "[ episode 200 ][ timestamp 19 ] state=[-0.08284064 -0.36380937  0.13826     0.72765297], action=1, reward=1.0, next_state=[-0.09011683 -0.17084209  0.15281306  0.48148326]\n",
      "[ Experience replay ] starts\n",
      "[ episode 200 ][ timestamp 20 ] state=[-0.09011683 -0.17084209  0.15281306  0.48148326], action=0, reward=1.0, next_state=[-0.09353367 -0.36775338  0.16244273  0.8181606 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 200 ][ timestamp 21 ] state=[-0.09353367 -0.36775338  0.16244273  0.8181606 ], action=1, reward=1.0, next_state=[-0.10088874 -0.17518326  0.17880594  0.58065656]\n",
      "[ Experience replay ] starts\n",
      "[ episode 200 ][ timestamp 22 ] state=[-0.10088874 -0.17518326  0.17880594  0.58065656], action=0, reward=1.0, next_state=[-0.1043924  -0.3723005   0.19041907  0.92390766]\n",
      "[ Experience replay ] starts\n",
      "[ episode 200 ][ timestamp 23 ] state=[-0.1043924  -0.3723005   0.19041907  0.92390766], action=1, reward=1.0, next_state=[-0.11183841 -0.18018984  0.20889722  0.69659577]\n",
      "[ Experience replay ] starts\n",
      "[ episode 200 ][ timestamp 24 ] state=[-0.11183841 -0.18018984  0.20889722  0.69659577], action=1, reward=-1.0, next_state=[-0.11544221  0.01151752  0.22282914  0.47625633]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 200: Exploration_rate=0.01. Score=24.\n",
      "[ episode 201 ] state=[-0.0340596  -0.02738938 -0.03179443  0.04312717]\n",
      "[ episode 201 ][ timestamp 1 ] state=[-0.0340596  -0.02738938 -0.03179443  0.04312717], action=0, reward=1.0, next_state=[-0.03460739 -0.22204132 -0.03093189  0.32561153]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 2 ] state=[-0.03460739 -0.22204132 -0.03093189  0.32561153], action=1, reward=1.0, next_state=[-0.03904822 -0.02649293 -0.02441965  0.02333673]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 3 ] state=[-0.03904822 -0.02649293 -0.02441965  0.02333673], action=0, reward=1.0, next_state=[-0.03957808 -0.22125632 -0.02395292  0.30821605]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 4 ] state=[-0.03957808 -0.22125632 -0.02395292  0.30821605], action=1, reward=1.0, next_state=[-0.0440032  -0.02580141 -0.0177886   0.0080763 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 5 ] state=[-0.0440032  -0.02580141 -0.0177886   0.0080763 ], action=0, reward=1.0, next_state=[-0.04451923 -0.22066379 -0.01762707  0.29509405]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 6 ] state=[-0.04451923 -0.22066379 -0.01762707  0.29509405], action=1, reward=1.0, next_state=[-0.04893251 -0.02529503 -0.01172519 -0.00309565]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 7 ] state=[-0.04893251 -0.02529503 -0.01172519 -0.00309565], action=1, reward=1.0, next_state=[-0.04943841  0.1699931  -0.01178711 -0.29945484]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 8 ] state=[-0.04943841  0.1699931  -0.01178711 -0.29945484], action=0, reward=1.0, next_state=[-0.04603854 -0.02495887 -0.0177762  -0.01051252]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 9 ] state=[-0.04603854 -0.02495887 -0.0177762  -0.01051252], action=1, reward=1.0, next_state=[-0.04653772  0.17041344 -0.01798645 -0.30875062]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 10 ] state=[-0.04653772  0.17041344 -0.01798645 -0.30875062], action=0, reward=1.0, next_state=[-0.04312945 -0.02444767 -0.02416146 -0.02179396]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 11 ] state=[-0.04312945 -0.02444767 -0.02416146 -0.02179396], action=1, reward=1.0, next_state=[-0.04361841  0.1710123  -0.02459734 -0.32200112]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 12 ] state=[-0.04361841  0.1710123  -0.02459734 -0.32200112], action=0, reward=1.0, next_state=[-0.04019816 -0.0237509  -0.03103737 -0.03717558]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 13 ] state=[-0.04019816 -0.0237509  -0.03103737 -0.03717558], action=1, reward=1.0, next_state=[-0.04067318  0.17180206 -0.03178088 -0.33948728]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 14 ] state=[-0.04067318  0.17180206 -0.03178088 -0.33948728], action=0, reward=1.0, next_state=[-0.03723714 -0.0228536  -0.03857062 -0.05699323]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 15 ] state=[-0.03723714 -0.0228536  -0.03857062 -0.05699323], action=0, reward=1.0, next_state=[-0.03769421 -0.2174019  -0.03971049  0.22327522]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 16 ] state=[-0.03769421 -0.2174019  -0.03971049  0.22327522], action=1, reward=1.0, next_state=[-0.04204225 -0.02173553 -0.03524498 -0.08166476]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 17 ] state=[-0.04204225 -0.02173553 -0.03524498 -0.08166476], action=0, reward=1.0, next_state=[-0.04247696 -0.21633499 -0.03687828  0.19969326]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 18 ] state=[-0.04247696 -0.21633499 -0.03687828  0.19969326], action=1, reward=1.0, next_state=[-0.04680366 -0.02070552 -0.03288441 -0.10439117]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 19 ] state=[-0.04680366 -0.02070552 -0.03288441 -0.10439117], action=0, reward=1.0, next_state=[-0.04721777 -0.21534116 -0.03497224  0.17773816]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 20 ] state=[-0.04721777 -0.21534116 -0.03497224  0.17773816], action=1, reward=1.0, next_state=[-0.05152459 -0.01973664 -0.03141747 -0.12576895]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 21 ] state=[-0.05152459 -0.01973664 -0.03141747 -0.12576895], action=0, reward=1.0, next_state=[-0.05191932 -0.21439474 -0.03393285  0.1568389 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 22 ] state=[-0.05191932 -0.21439474 -0.03393285  0.1568389 ], action=0, reward=1.0, next_state=[-0.05620722 -0.40901484 -0.03079607  0.43862665]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 23 ] state=[-0.05620722 -0.40901484 -0.03079607  0.43862665], action=0, reward=1.0, next_state=[-0.06438752 -0.60368769 -0.02202354  0.72144484]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 24 ] state=[-0.06438752 -0.60368769 -0.02202354  0.72144484], action=0, reward=1.0, next_state=[-0.07646127 -0.79849816 -0.00759464  1.00711529]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 201 ][ timestamp 25 ] state=[-0.07646127 -0.79849816 -0.00759464  1.00711529], action=1, reward=1.0, next_state=[-0.09243123 -0.60327563  0.01254766  0.71205714]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 26 ] state=[-0.09243123 -0.60327563  0.01254766  0.71205714], action=0, reward=1.0, next_state=[-0.10449675 -0.79856906  0.0267888   1.00866314]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 27 ] state=[-0.10449675 -0.79856906  0.0267888   1.00866314], action=1, reward=1.0, next_state=[-0.12046813 -0.60381475  0.04696207  0.72451146]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 28 ] state=[-0.12046813 -0.60381475  0.04696207  0.72451146], action=0, reward=1.0, next_state=[-0.13254442 -0.79955362  0.0614523   1.03159782]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 29 ] state=[-0.13254442 -0.79955362  0.0614523   1.03159782], action=0, reward=1.0, next_state=[-0.14853549 -0.99543685  0.08208425  1.34292364]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 30 ] state=[-0.14853549 -0.99543685  0.08208425  1.34292364], action=1, reward=1.0, next_state=[-0.16844423 -0.80143799  0.10894272  1.07701083]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 31 ] state=[-0.16844423 -0.80143799  0.10894272  1.07701083], action=1, reward=1.0, next_state=[-0.18447299 -0.60791046  0.13048294  0.82040635]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 32 ] state=[-0.18447299 -0.60791046  0.13048294  0.82040635], action=1, reward=1.0, next_state=[-0.1966312  -0.41479244  0.14689107  0.57144503]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 33 ] state=[-0.1966312  -0.41479244  0.14689107  0.57144503], action=1, reward=1.0, next_state=[-0.20492705 -0.22200252  0.15831997  0.32840524]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 34 ] state=[-0.20492705 -0.22200252  0.15831997  0.32840524], action=0, reward=1.0, next_state=[-0.2093671  -0.41898241  0.16488807  0.66653167]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 35 ] state=[-0.2093671  -0.41898241  0.16488807  0.66653167], action=1, reward=1.0, next_state=[-0.21774675 -0.2264909   0.17821871  0.42996835]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 36 ] state=[-0.21774675 -0.2264909   0.17821871  0.42996835], action=0, reward=1.0, next_state=[-0.22227657 -0.42362969  0.18681807  0.77311219]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 37 ] state=[-0.22227657 -0.42362969  0.18681807  0.77311219], action=0, reward=1.0, next_state=[-0.23074916 -0.62076307  0.20228032  1.11827274]\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ][ timestamp 38 ] state=[-0.23074916 -0.62076307  0.20228032  1.11827274], action=0, reward=-1.0, next_state=[-0.24316442 -0.81788027  0.22464577  1.46698568]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 201: Exploration_rate=0.01. Score=38.\n",
      "[ episode 202 ] state=[-0.01763633 -0.04762195 -0.04983239 -0.00271119]\n",
      "[ episode 202 ][ timestamp 1 ] state=[-0.01763633 -0.04762195 -0.04983239 -0.00271119], action=1, reward=1.0, next_state=[-0.01858876  0.14817794 -0.04988661 -0.31069109]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 2 ] state=[-0.01858876  0.14817794 -0.04988661 -0.31069109], action=0, reward=1.0, next_state=[-0.01562521 -0.04619908 -0.05610043 -0.03414887]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 3 ] state=[-0.01562521 -0.04619908 -0.05610043 -0.03414887], action=1, reward=1.0, next_state=[-0.01654919  0.14968062 -0.05678341 -0.34399106]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 4 ] state=[-0.01654919  0.14968062 -0.05678341 -0.34399106], action=1, reward=1.0, next_state=[-0.01355557  0.34556247 -0.06366323 -0.65402561]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 5 ] state=[-0.01355557  0.34556247 -0.06366323 -0.65402561], action=1, reward=1.0, next_state=[-0.00664432  0.54151039 -0.07674374 -0.96605642]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 6 ] state=[-0.00664432  0.54151039 -0.07674374 -0.96605642], action=0, reward=1.0, next_state=[ 0.00418588  0.34749838 -0.09606487 -0.69843549]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 7 ] state=[ 0.00418588  0.34749838 -0.09606487 -0.69843549], action=0, reward=1.0, next_state=[ 0.01113585  0.15383041 -0.11003358 -0.43747259]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 8 ] state=[ 0.01113585  0.15383041 -0.11003358 -0.43747259], action=1, reward=1.0, next_state=[ 0.01421246  0.35032382 -0.11878303 -0.76271487]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 9 ] state=[ 0.01421246  0.35032382 -0.11878303 -0.76271487], action=0, reward=1.0, next_state=[ 0.02121893  0.15702078 -0.13403733 -0.50964361]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 10 ] state=[ 0.02121893  0.15702078 -0.13403733 -0.50964361], action=1, reward=1.0, next_state=[ 0.02435935  0.35375114 -0.1442302  -0.84138136]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 11 ] state=[ 0.02435935  0.35375114 -0.1442302  -0.84138136], action=0, reward=1.0, next_state=[ 0.03143437  0.1608614  -0.16105783 -0.59730777]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 12 ] state=[ 0.03143437  0.1608614  -0.16105783 -0.59730777], action=0, reward=1.0, next_state=[ 0.0346516  -0.03168399 -0.17300398 -0.35937406]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 13 ] state=[ 0.0346516  -0.03168399 -0.17300398 -0.35937406], action=0, reward=1.0, next_state=[ 0.03401792 -0.22397877 -0.18019147 -0.12584755]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 14 ] state=[ 0.03401792 -0.22397877 -0.18019147 -0.12584755], action=0, reward=1.0, next_state=[ 0.02953835 -0.41612328 -0.18270842  0.10501274]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 15 ] state=[ 0.02953835 -0.41612328 -0.18270842  0.10501274], action=0, reward=1.0, next_state=[ 0.02121588 -0.60822079 -0.18060816  0.33494495]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 16 ] state=[ 0.02121588 -0.60822079 -0.18060816  0.33494495], action=1, reward=1.0, next_state=[ 0.00905146 -0.41104978 -0.17390926 -0.00881157]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 17 ] state=[ 0.00905146 -0.41104978 -0.17390926 -0.00881157], action=0, reward=1.0, next_state=[ 0.00083047 -0.60330642 -0.17408549  0.22435137]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 18 ] state=[ 0.00083047 -0.60330642 -0.17408549  0.22435137], action=0, reward=1.0, next_state=[-0.01123566 -0.79556833 -0.16959847  0.45746226]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 19 ] state=[-0.01123566 -0.79556833 -0.16959847  0.45746226], action=0, reward=1.0, next_state=[-0.02714703 -0.98793782 -0.16044922  0.69225324]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 20 ] state=[-0.02714703 -0.98793782 -0.16044922  0.69225324], action=1, reward=1.0, next_state=[-0.04690578 -0.79099658 -0.14660416  0.35366583]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 21 ] state=[-0.04690578 -0.79099658 -0.14660416  0.35366583], action=1, reward=1.0, next_state=[-0.06272571 -0.59412732 -0.13953084  0.01858233]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 22 ] state=[-0.06272571 -0.59412732 -0.13953084  0.01858233], action=1, reward=1.0, next_state=[-0.07460826 -0.39730878 -0.13915919 -0.31466537]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 23 ] state=[-0.07460826 -0.39730878 -0.13915919 -0.31466537], action=0, reward=1.0, next_state=[-0.08255444 -0.59020248 -0.1454525  -0.06890275]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 24 ] state=[-0.08255444 -0.59020248 -0.1454525  -0.06890275], action=1, reward=1.0, next_state=[-0.09435849 -0.39332712 -0.14683056 -0.40370982]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 25 ] state=[-0.09435849 -0.39332712 -0.14683056 -0.40370982], action=0, reward=1.0, next_state=[-0.10222503 -0.5860948  -0.15490475 -0.16068289]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 26 ] state=[-0.10222503 -0.5860948  -0.15490475 -0.16068289], action=1, reward=1.0, next_state=[-0.11394692 -0.3891336  -0.15811841 -0.49794722]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 27 ] state=[-0.11394692 -0.3891336  -0.15811841 -0.49794722], action=0, reward=1.0, next_state=[-0.1217296  -0.58171422 -0.16807736 -0.25897321]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 28 ] state=[-0.1217296  -0.58171422 -0.16807736 -0.25897321], action=1, reward=1.0, next_state=[-0.13336388 -0.38464143 -0.17325682 -0.59959915]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 29 ] state=[-0.13336388 -0.38464143 -0.17325682 -0.59959915], action=0, reward=1.0, next_state=[-0.14105671 -0.57697029 -0.1852488  -0.36610808]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 202 ][ timestamp 30 ] state=[-0.14105671 -0.57697029 -0.1852488  -0.36610808], action=1, reward=1.0, next_state=[-0.15259611 -0.37976559 -0.19257096 -0.71100617]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 31 ] state=[-0.15259611 -0.37976559 -0.19257096 -0.71100617], action=0, reward=1.0, next_state=[-0.16019143 -0.57177337 -0.20679109 -0.48458487]\n",
      "[ Experience replay ] starts\n",
      "[ episode 202 ][ timestamp 32 ] state=[-0.16019143 -0.57177337 -0.20679109 -0.48458487], action=0, reward=-1.0, next_state=[-0.17162689 -0.76346919 -0.21648279 -0.26353152]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 202: Exploration_rate=0.01. Score=32.\n",
      "[ episode 203 ] state=[-0.00711852 -0.04237722 -0.02606771  0.04374384]\n",
      "[ episode 203 ][ timestamp 1 ] state=[-0.00711852 -0.04237722 -0.02606771  0.04374384], action=0, reward=1.0, next_state=[-0.00796607 -0.23711586 -0.02519283  0.32808952]\n",
      "[ Experience replay ] starts\n",
      "[ episode 203 ][ timestamp 2 ] state=[-0.00796607 -0.23711586 -0.02519283  0.32808952], action=0, reward=1.0, next_state=[-0.01270839 -0.43187026 -0.01863104  0.61272252]\n",
      "[ Experience replay ] starts\n",
      "[ episode 203 ][ timestamp 3 ] state=[-0.01270839 -0.43187026 -0.01863104  0.61272252], action=0, reward=1.0, next_state=[-0.02134579 -0.62672695 -0.00637659  0.89947962]\n",
      "[ Experience replay ] starts\n",
      "[ episode 203 ][ timestamp 4 ] state=[-0.02134579 -0.62672695 -0.00637659  0.89947962], action=1, reward=1.0, next_state=[-0.03388033 -0.43151917  0.011613    0.60479919]\n",
      "[ Experience replay ] starts\n",
      "[ episode 203 ][ timestamp 5 ] state=[-0.03388033 -0.43151917  0.011613    0.60479919], action=0, reward=1.0, next_state=[-0.04251071 -0.62680158  0.02370898  0.9011172 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 203 ][ timestamp 6 ] state=[-0.04251071 -0.62680158  0.02370898  0.9011172 ], action=1, reward=1.0, next_state=[-0.05504675 -0.43200876  0.04173133  0.61597987]\n",
      "[ Experience replay ] starts\n",
      "[ episode 203 ][ timestamp 7 ] state=[-0.05504675 -0.43200876  0.04173133  0.61597987], action=1, reward=1.0, next_state=[-0.06368692 -0.23749393  0.05405092  0.3367271 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 203 ][ timestamp 8 ] state=[-0.06368692 -0.23749393  0.05405092  0.3367271 ], action=0, reward=1.0, next_state=[-0.0684368  -0.43334174  0.06078547  0.64595302]\n",
      "[ Experience replay ] starts\n",
      "[ episode 203 ][ timestamp 9 ] state=[-0.0684368  -0.43334174  0.06078547  0.64595302], action=0, reward=1.0, next_state=[-0.07710363 -0.62925566  0.07370453  0.9571411 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 203 ][ timestamp 10 ] state=[-0.07710363 -0.62925566  0.07370453  0.9571411 ], action=1, reward=1.0, next_state=[-0.08968875 -0.4351981   0.09284735  0.68849455]\n",
      "[ Experience replay ] starts\n",
      "[ episode 203 ][ timestamp 11 ] state=[-0.08968875 -0.4351981   0.09284735  0.68849455], action=0, reward=1.0, next_state=[-0.09839271 -0.63147765  0.10661724  1.00890367]\n",
      "[ Experience replay ] starts\n",
      "[ episode 203 ][ timestamp 12 ] state=[-0.09839271 -0.63147765  0.10661724  1.00890367], action=0, reward=1.0, next_state=[-0.11102226 -0.8278487   0.12679531  1.33307379]\n",
      "[ Experience replay ] starts\n",
      "[ episode 203 ][ timestamp 13 ] state=[-0.11102226 -0.8278487   0.12679531  1.33307379], action=1, reward=1.0, next_state=[-0.12757924 -0.6345327   0.15345679  1.08260566]\n",
      "[ Experience replay ] starts\n",
      "[ episode 203 ][ timestamp 14 ] state=[-0.12757924 -0.6345327   0.15345679  1.08260566], action=1, reward=1.0, next_state=[-0.14026989 -0.44173199  0.1751089   0.84174254]\n",
      "[ Experience replay ] starts\n",
      "[ episode 203 ][ timestamp 15 ] state=[-0.14026989 -0.44173199  0.1751089   0.84174254], action=1, reward=1.0, next_state=[-0.14910453 -0.24937709  0.19194375  0.60884187]\n",
      "[ Experience replay ] starts\n",
      "[ episode 203 ][ timestamp 16 ] state=[-0.14910453 -0.24937709  0.19194375  0.60884187], action=1, reward=1.0, next_state=[-0.15409207 -0.05738304  0.20412059  0.38222525]\n",
      "[ Experience replay ] starts\n",
      "[ episode 203 ][ timestamp 17 ] state=[-0.15409207 -0.05738304  0.20412059  0.38222525], action=1, reward=-1.0, next_state=[-0.15523973  0.13434462  0.2117651   0.16019987]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 203: Exploration_rate=0.01. Score=17.\n",
      "[ episode 204 ] state=[-0.01320333  0.01765117 -0.03462434 -0.03615609]\n",
      "[ episode 204 ][ timestamp 1 ] state=[-0.01320333  0.01765117 -0.03462434 -0.03615609], action=1, reward=1.0, next_state=[-0.01285031  0.21325209 -0.03534746 -0.33955914]\n",
      "[ Experience replay ] starts\n",
      "[ episode 204 ][ timestamp 2 ] state=[-0.01285031  0.21325209 -0.03534746 -0.33955914], action=1, reward=1.0, next_state=[-0.00858526  0.40885871 -0.04213864 -0.64317577]\n",
      "[ Experience replay ] starts\n",
      "[ episode 204 ][ timestamp 3 ] state=[-0.00858526  0.40885871 -0.04213864 -0.64317577], action=1, reward=1.0, next_state=[-4.08090090e-04  6.04541862e-01 -5.50021603e-02 -9.48825038e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 204 ][ timestamp 4 ] state=[-4.08090090e-04  6.04541862e-01 -5.50021603e-02 -9.48825038e-01], action=1, reward=1.0, next_state=[ 0.01168275  0.80035945 -0.07397866 -1.25826972]\n",
      "[ Experience replay ] starts\n",
      "[ episode 204 ][ timestamp 5 ] state=[ 0.01168275  0.80035945 -0.07397866 -1.25826972], action=0, reward=1.0, next_state=[ 0.02768994  0.60625801 -0.09914406 -0.98964381]\n",
      "[ Experience replay ] starts\n",
      "[ episode 204 ][ timestamp 6 ] state=[ 0.02768994  0.60625801 -0.09914406 -0.98964381], action=0, reward=1.0, next_state=[ 0.0398151   0.41259289 -0.11893693 -0.72967331]\n",
      "[ Experience replay ] starts\n",
      "[ episode 204 ][ timestamp 7 ] state=[ 0.0398151   0.41259289 -0.11893693 -0.72967331], action=0, reward=1.0, next_state=[ 0.04806695  0.21929813 -0.1335304  -0.47666458]\n",
      "[ Experience replay ] starts\n",
      "[ episode 204 ][ timestamp 8 ] state=[ 0.04806695  0.21929813 -0.1335304  -0.47666458], action=1, reward=1.0, next_state=[ 0.05245292  0.41602776 -0.14306369 -0.8082735 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 204 ][ timestamp 9 ] state=[ 0.05245292  0.41602776 -0.14306369 -0.8082735 ], action=0, reward=1.0, next_state=[ 0.06077347  0.22312572 -0.15922916 -0.56379391]\n",
      "[ Experience replay ] starts\n",
      "[ episode 204 ][ timestamp 10 ] state=[ 0.06077347  0.22312572 -0.15922916 -0.56379391], action=0, reward=1.0, next_state=[ 0.06523599  0.03055402 -0.17050504 -0.32520627]\n",
      "[ Experience replay ] starts\n",
      "[ episode 204 ][ timestamp 11 ] state=[ 0.06523599  0.03055402 -0.17050504 -0.32520627], action=0, reward=1.0, next_state=[ 0.06584707 -0.16178207 -0.17700916 -0.09077161]\n",
      "[ Experience replay ] starts\n",
      "[ episode 204 ][ timestamp 12 ] state=[ 0.06584707 -0.16178207 -0.17700916 -0.09077161], action=0, reward=1.0, next_state=[ 0.06261143 -0.35398351 -0.1788246   0.14125638]\n",
      "[ Experience replay ] starts\n",
      "[ episode 204 ][ timestamp 13 ] state=[ 0.06261143 -0.35398351 -0.1788246   0.14125638], action=1, reward=1.0, next_state=[ 0.05553176 -0.1568112  -0.17599947 -0.20208043]\n",
      "[ Experience replay ] starts\n",
      "[ episode 204 ][ timestamp 14 ] state=[ 0.05553176 -0.1568112  -0.17599947 -0.20208043], action=0, reward=1.0, next_state=[ 0.05239553 -0.34903663 -0.18004108  0.03032636]\n",
      "[ Experience replay ] starts\n",
      "[ episode 204 ][ timestamp 15 ] state=[ 0.05239553 -0.34903663 -0.18004108  0.03032636], action=1, reward=1.0, next_state=[ 0.0454148  -0.15185088 -0.17943455 -0.31331797]\n",
      "[ Experience replay ] starts\n",
      "[ episode 204 ][ timestamp 16 ] state=[ 0.0454148  -0.15185088 -0.17943455 -0.31331797], action=0, reward=1.0, next_state=[ 0.04237778 -0.34402356 -0.18570091 -0.08215815]\n",
      "[ Experience replay ] starts\n",
      "[ episode 204 ][ timestamp 17 ] state=[ 0.04237778 -0.34402356 -0.18570091 -0.08215815], action=1, reward=1.0, next_state=[ 0.03549731 -0.14679238 -0.18734407 -0.42720126]\n",
      "[ Experience replay ] starts\n",
      "[ episode 204 ][ timestamp 18 ] state=[ 0.03549731 -0.14679238 -0.18734407 -0.42720126], action=1, reward=1.0, next_state=[ 0.03256146  0.0504202  -0.1958881  -0.77260153]\n",
      "[ Experience replay ] starts\n",
      "[ episode 204 ][ timestamp 19 ] state=[ 0.03256146  0.0504202  -0.1958881  -0.77260153], action=0, reward=-1.0, next_state=[ 0.03356987 -0.14154497 -0.21134013 -0.54738421]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 204: Exploration_rate=0.01. Score=19.\n",
      "[ episode 205 ] state=[-0.03036048 -0.04278719  0.00271697  0.04763562]\n",
      "[ episode 205 ][ timestamp 1 ] state=[-0.03036048 -0.04278719  0.00271697  0.04763562], action=0, reward=1.0, next_state=[-0.03121622 -0.23794799  0.00366968  0.34117454]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 205 ][ timestamp 2 ] state=[-0.03121622 -0.23794799  0.00366968  0.34117454], action=1, reward=1.0, next_state=[-0.03597518 -0.04287844  0.01049318  0.04965107]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 3 ] state=[-0.03597518 -0.04287844  0.01049318  0.04965107], action=1, reward=1.0, next_state=[-0.03683275  0.15209149  0.0114862  -0.23970278]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 4 ] state=[-0.03683275  0.15209149  0.0114862  -0.23970278], action=1, reward=1.0, next_state=[-0.03379092  0.34704748  0.00669214 -0.52874062]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 5 ] state=[-0.03379092  0.34704748  0.00669214 -0.52874062], action=1, reward=1.0, next_state=[-0.02684997  0.54207465 -0.00388267 -0.81930734]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 6 ] state=[-0.02684997  0.54207465 -0.00388267 -0.81930734], action=0, reward=1.0, next_state=[-0.01600848  0.34700605 -0.02026882 -0.52784816]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 7 ] state=[-0.01600848  0.34700605 -0.02026882 -0.52784816], action=1, reward=1.0, next_state=[-0.00906836  0.54240723 -0.03082578 -0.82684834]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 8 ] state=[-0.00906836  0.54240723 -0.03082578 -0.82684834], action=0, reward=1.0, next_state=[ 0.00177979  0.34772006 -0.04736275 -0.54401766]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 9 ] state=[ 0.00177979  0.34772006 -0.04736275 -0.54401766], action=0, reward=1.0, next_state=[ 0.00873419  0.15329456 -0.0582431  -0.26662591]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 10 ] state=[ 0.00873419  0.15329456 -0.0582431  -0.26662591], action=0, reward=1.0, next_state=[ 0.01180008 -0.04094986 -0.06357562  0.00713288]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 11 ] state=[ 0.01180008 -0.04094986 -0.06357562  0.00713288], action=0, reward=1.0, next_state=[ 0.01098108 -0.23510516 -0.06343296  0.27909883]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 12 ] state=[ 0.01098108 -0.23510516 -0.06343296  0.27909883], action=1, reward=1.0, next_state=[ 0.00627898 -0.03913836 -0.05785098 -0.03289697]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 13 ] state=[ 0.00627898 -0.03913836 -0.05785098 -0.03289697], action=1, reward=1.0, next_state=[ 0.00549621  0.15676342 -0.05850892 -0.34325677]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 14 ] state=[ 0.00549621  0.15676342 -0.05850892 -0.34325677], action=0, reward=1.0, next_state=[ 0.00863148 -0.03747946 -0.06537406 -0.06958282]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 15 ] state=[ 0.00863148 -0.03747946 -0.06537406 -0.06958282], action=1, reward=1.0, next_state=[ 0.00788189  0.15851587 -0.06676572 -0.3821541 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 16 ] state=[ 0.00788189  0.15851587 -0.06676572 -0.3821541 ], action=0, reward=1.0, next_state=[ 0.01105221 -0.03559769 -0.0744088  -0.11124702]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 17 ] state=[ 0.01105221 -0.03559769 -0.0744088  -0.11124702], action=1, reward=1.0, next_state=[ 0.01034025  0.16050726 -0.07663374 -0.4264465 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 18 ] state=[ 0.01034025  0.16050726 -0.07663374 -0.4264465 ], action=0, reward=1.0, next_state=[ 0.0135504  -0.03345035 -0.08516267 -0.15887224]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 19 ] state=[ 0.0135504  -0.03345035 -0.08516267 -0.15887224], action=0, reward=1.0, next_state=[ 0.01288139 -0.22725634 -0.08834011  0.10577559]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 20 ] state=[ 0.01288139 -0.22725634 -0.08834011  0.10577559], action=1, reward=1.0, next_state=[ 0.00833626 -0.03098678 -0.0862246  -0.21341894]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 21 ] state=[ 0.00833626 -0.03098678 -0.0862246  -0.21341894], action=1, reward=1.0, next_state=[ 0.00771653  0.16525532 -0.09049298 -0.53200716]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 22 ] state=[ 0.00771653  0.16525532 -0.09049298 -0.53200716], action=1, reward=1.0, next_state=[ 0.01102163  0.36152576 -0.10113312 -0.85177684]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 23 ] state=[ 0.01102163  0.36152576 -0.10113312 -0.85177684], action=1, reward=1.0, next_state=[ 0.01825215  0.55787033 -0.11816866 -1.17447132]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 24 ] state=[ 0.01825215  0.55787033 -0.11816866 -1.17447132], action=1, reward=1.0, next_state=[ 0.02940956  0.75431283 -0.14165809 -1.50174093]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 25 ] state=[ 0.02940956  0.75431283 -0.14165809 -1.50174093], action=1, reward=1.0, next_state=[ 0.04449581  0.95084201 -0.17169291 -1.83509016]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 26 ] state=[ 0.04449581  0.95084201 -0.17169291 -1.83509016], action=1, reward=1.0, next_state=[ 0.06351265  1.14739706 -0.20839471 -2.17581787]\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ][ timestamp 27 ] state=[ 0.06351265  1.14739706 -0.20839471 -2.17581787], action=1, reward=-1.0, next_state=[ 0.08646059  1.34385107 -0.25191107 -2.52494879]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 205: Exploration_rate=0.01. Score=27.\n",
      "[ episode 206 ] state=[-0.03141849 -0.0398899   0.02263565 -0.00177845]\n",
      "[ episode 206 ][ timestamp 1 ] state=[-0.03141849 -0.0398899   0.02263565 -0.00177845], action=1, reward=1.0, next_state=[-0.03221629  0.15490023  0.02260008 -0.28723449]\n",
      "[ Experience replay ] starts\n",
      "[ episode 206 ][ timestamp 2 ] state=[-0.03221629  0.15490023  0.02260008 -0.28723449], action=1, reward=1.0, next_state=[-0.02911828  0.34969271  0.01685539 -0.57270474]\n",
      "[ Experience replay ] starts\n",
      "[ episode 206 ][ timestamp 3 ] state=[-0.02911828  0.34969271  0.01685539 -0.57270474], action=0, reward=1.0, next_state=[-0.02212443  0.15433853  0.00540129 -0.27475984]\n",
      "[ Experience replay ] starts\n",
      "[ episode 206 ][ timestamp 4 ] state=[-0.02212443  0.15433853  0.00540129 -0.27475984], action=0, reward=1.0, next_state=[-1.90376585e-02 -4.08600654e-02 -9.39023167e-05  1.96217597e-02]\n",
      "[ Experience replay ] starts\n",
      "[ episode 206 ][ timestamp 5 ] state=[-1.90376585e-02 -4.08600654e-02 -9.39023167e-05  1.96217597e-02], action=0, reward=1.0, next_state=[-1.98548598e-02 -2.35980670e-01  2.98532878e-04  3.12275058e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 206 ][ timestamp 6 ] state=[-1.98548598e-02 -2.35980670e-01  2.98532878e-04  3.12275058e-01], action=0, reward=1.0, next_state=[-0.02457447 -0.43110687  0.00654403  0.60505212]\n",
      "[ Experience replay ] starts\n",
      "[ episode 206 ][ timestamp 7 ] state=[-0.02457447 -0.43110687  0.00654403  0.60505212], action=0, reward=1.0, next_state=[-0.03319661 -0.62631972  0.01864508  0.89978906]\n",
      "[ Experience replay ] starts\n",
      "[ episode 206 ][ timestamp 8 ] state=[-0.03319661 -0.62631972  0.01864508  0.89978906], action=0, reward=1.0, next_state=[-0.04572301 -0.82168931  0.03664086  1.19827384]\n",
      "[ Experience replay ] starts\n",
      "[ episode 206 ][ timestamp 9 ] state=[-0.04572301 -0.82168931  0.03664086  1.19827384], action=1, reward=1.0, next_state=[-0.06215679 -0.62706017  0.06060633  0.91729608]\n",
      "[ Experience replay ] starts\n",
      "[ episode 206 ][ timestamp 10 ] state=[-0.06215679 -0.62706017  0.06060633  0.91729608], action=0, reward=1.0, next_state=[-0.07469799 -0.82294687  0.07895226  1.22839402]\n",
      "[ Experience replay ] starts\n",
      "[ episode 206 ][ timestamp 11 ] state=[-0.07469799 -0.82294687  0.07895226  1.22839402], action=1, reward=1.0, next_state=[-0.09115693 -0.62892472  0.10352014  0.96145524]\n",
      "[ Experience replay ] starts\n",
      "[ episode 206 ][ timestamp 12 ] state=[-0.09115693 -0.62892472  0.10352014  0.96145524], action=0, reward=1.0, next_state=[-0.10373543 -0.82527411  0.12274924  1.2847832 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 206 ][ timestamp 13 ] state=[-0.10373543 -0.82527411  0.12274924  1.2847832 ], action=1, reward=1.0, next_state=[-0.12024091 -0.63190986  0.14844491  1.03291693]\n",
      "[ Experience replay ] starts\n",
      "[ episode 206 ][ timestamp 14 ] state=[-0.12024091 -0.63190986  0.14844491  1.03291693], action=1, reward=1.0, next_state=[-0.13287911 -0.43904045  0.16910324  0.79027719]\n",
      "[ Experience replay ] starts\n",
      "[ episode 206 ][ timestamp 15 ] state=[-0.13287911 -0.43904045  0.16910324  0.79027719], action=0, reward=1.0, next_state=[-0.14165991 -0.6360308   0.18490879  1.13102769]\n",
      "[ Experience replay ] starts\n",
      "[ episode 206 ][ timestamp 16 ] state=[-0.14165991 -0.6360308   0.18490879  1.13102769], action=1, reward=1.0, next_state=[-0.15438053 -0.44374682  0.20752934  0.90157243]\n",
      "[ Experience replay ] starts\n",
      "[ episode 206 ][ timestamp 17 ] state=[-0.15438053 -0.44374682  0.20752934  0.90157243], action=0, reward=-1.0, next_state=[-0.16325547 -0.64098424  0.22556079  1.25165697]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 206: Exploration_rate=0.01. Score=17.\n",
      "[ episode 207 ] state=[-0.02078018  0.04952517 -0.03391884  0.04052092]\n",
      "[ episode 207 ][ timestamp 1 ] state=[-0.02078018  0.04952517 -0.03391884  0.04052092], action=0, reward=1.0, next_state=[-0.01978968 -0.14509438 -0.03310842  0.32231211]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 2 ] state=[-0.01978968 -0.14509438 -0.03310842  0.32231211], action=1, reward=1.0, next_state=[-0.02269157  0.05048301 -0.02666218  0.0193747 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 3 ] state=[-0.02269157  0.05048301 -0.02666218  0.0193747 ], action=0, reward=1.0, next_state=[-0.02168191 -0.14424663 -0.02627469  0.30352759]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 4 ] state=[-0.02168191 -0.14424663 -0.02627469  0.30352759], action=1, reward=1.0, next_state=[-0.02456684  0.05123973 -0.02020414  0.00267539]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 207 ][ timestamp 5 ] state=[-0.02456684  0.05123973 -0.02020414  0.00267539], action=0, reward=1.0, next_state=[-0.02354204 -0.14358673 -0.02015063  0.28891581]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 6 ] state=[-0.02354204 -0.14358673 -0.02015063  0.28891581], action=1, reward=1.0, next_state=[-0.02641378  0.05181669 -0.01437231 -0.01005369]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 7 ] state=[-0.02641378  0.05181669 -0.01437231 -0.01005369], action=0, reward=1.0, next_state=[-0.02537744 -0.14309623 -0.01457339  0.27806017]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 8 ] state=[-0.02537744 -0.14309623 -0.01457339  0.27806017], action=0, reward=1.0, next_state=[-0.02823937 -0.33800727 -0.00901218  0.56611127]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 9 ] state=[-0.02823937 -0.33800727 -0.00901218  0.56611127], action=1, reward=1.0, next_state=[-0.03499951 -0.14276006  0.00231004  0.2706028 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 10 ] state=[-0.03499951 -0.14276006  0.00231004  0.2706028 ], action=0, reward=1.0, next_state=[-0.03785472 -0.3379149   0.0077221   0.56401343]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 11 ] state=[-0.03785472 -0.3379149   0.0077221   0.56401343], action=0, reward=1.0, next_state=[-0.04461301 -0.53314434  0.01900237  0.85911914]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 12 ] state=[-0.04461301 -0.53314434  0.01900237  0.85911914], action=1, reward=1.0, next_state=[-0.0552759  -0.33828631  0.03618475  0.57247123]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 13 ] state=[-0.0552759  -0.33828631  0.03618475  0.57247123], action=0, reward=1.0, next_state=[-0.06204163 -0.53389646  0.04763418  0.87633036]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 14 ] state=[-0.06204163 -0.53389646  0.04763418  0.87633036], action=1, reward=1.0, next_state=[-0.07271956 -0.3394532   0.06516078  0.59899546]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 15 ] state=[-0.07271956 -0.3394532   0.06516078  0.59899546], action=0, reward=1.0, next_state=[-0.07950862 -0.53542342  0.07714069  0.91147067]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 16 ] state=[-0.07950862 -0.53542342  0.07714069  0.91147067], action=0, reward=1.0, next_state=[-0.09021709 -0.73149963  0.09537011  1.22736721]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 17 ] state=[-0.09021709 -0.73149963  0.09537011  1.22736721], action=1, reward=1.0, next_state=[-0.10484708 -0.53772576  0.11991745  0.96602358]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 18 ] state=[-0.10484708 -0.53772576  0.11991745  0.96602358], action=1, reward=1.0, next_state=[-0.1156016  -0.34440069  0.13923792  0.71328981]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 19 ] state=[-0.1156016  -0.34440069  0.13923792  0.71328981], action=1, reward=1.0, next_state=[-0.12248961 -0.15145296  0.15350372  0.46747303]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 20 ] state=[-0.12248961 -0.15145296  0.15350372  0.46747303], action=1, reward=1.0, next_state=[-0.12551867  0.04120497  0.16285318  0.22683727]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 21 ] state=[-0.12551867  0.04120497  0.16285318  0.22683727], action=1, reward=1.0, next_state=[-0.12469457  0.23367038  0.16738992 -0.01037351]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 22 ] state=[-0.12469457  0.23367038  0.16738992 -0.01037351], action=0, reward=1.0, next_state=[-0.12002116  0.03659271  0.16718245  0.3300943 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 23 ] state=[-0.12002116  0.03659271  0.16718245  0.3300943 ], action=1, reward=1.0, next_state=[-0.11928931  0.22898931  0.17378434  0.09444611]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 24 ] state=[-0.11928931  0.22898931  0.17378434  0.09444611], action=0, reward=1.0, next_state=[-0.11470952  0.03185768  0.17567326  0.43652542]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 25 ] state=[-0.11470952  0.03185768  0.17567326  0.43652542], action=1, reward=1.0, next_state=[-0.11407237  0.22411454  0.18440377  0.20396134]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 26 ] state=[-0.11407237  0.22411454  0.18440377  0.20396134], action=0, reward=1.0, next_state=[-0.10959008  0.02690012  0.188483    0.54867551]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 27 ] state=[-0.10959008  0.02690012  0.188483    0.54867551], action=0, reward=1.0, next_state=[-0.10905208 -0.17029979  0.19945651  0.89432311]\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ][ timestamp 28 ] state=[-0.10905208 -0.17029979  0.19945651  0.89432311], action=0, reward=-1.0, next_state=[-0.11245807 -0.36748594  0.21734297  1.24249052]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 207: Exploration_rate=0.01. Score=28.\n",
      "[ episode 208 ] state=[ 0.00134464 -0.01834486  0.0285467   0.0112791 ]\n",
      "[ episode 208 ][ timestamp 1 ] state=[ 0.00134464 -0.01834486  0.0285467   0.0112791 ], action=0, reward=1.0, next_state=[ 0.00097774 -0.21386434  0.02877228  0.3128304 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 208 ][ timestamp 2 ] state=[ 0.00097774 -0.21386434  0.02877228  0.3128304 ], action=0, reward=1.0, next_state=[-0.00329955 -0.40938411  0.03502889  0.61444656]\n",
      "[ Experience replay ] starts\n",
      "[ episode 208 ][ timestamp 3 ] state=[-0.00329955 -0.40938411  0.03502889  0.61444656], action=1, reward=1.0, next_state=[-0.01148723 -0.21476868  0.04731782  0.33299888]\n",
      "[ Experience replay ] starts\n",
      "[ episode 208 ][ timestamp 4 ] state=[-0.01148723 -0.21476868  0.04731782  0.33299888], action=1, reward=1.0, next_state=[-0.0157826  -0.02035103  0.0539778   0.05560507]\n",
      "[ Experience replay ] starts\n",
      "[ episode 208 ][ timestamp 5 ] state=[-0.0157826  -0.02035103  0.0539778   0.05560507], action=0, reward=1.0, next_state=[-0.01618962 -0.21620372  0.0550899   0.36481801]\n",
      "[ Experience replay ] starts\n",
      "[ episode 208 ][ timestamp 6 ] state=[-0.01618962 -0.21620372  0.0550899   0.36481801], action=0, reward=1.0, next_state=[-0.0205137  -0.41206355  0.06238626  0.67435029]\n",
      "[ Experience replay ] starts\n",
      "[ episode 208 ][ timestamp 7 ] state=[-0.0205137  -0.41206355  0.06238626  0.67435029], action=0, reward=1.0, next_state=[-0.02875497 -0.6079945   0.07587327  0.98600464]\n",
      "[ Experience replay ] starts\n",
      "[ episode 208 ][ timestamp 8 ] state=[-0.02875497 -0.6079945   0.07587327  0.98600464], action=1, reward=1.0, next_state=[-0.04091486 -0.41396619  0.09559336  0.71808485]\n",
      "[ Experience replay ] starts\n",
      "[ episode 208 ][ timestamp 9 ] state=[-0.04091486 -0.41396619  0.09559336  0.71808485], action=1, reward=1.0, next_state=[-0.04919418 -0.22028794  0.10995506  0.45695551]\n",
      "[ Experience replay ] starts\n",
      "[ episode 208 ][ timestamp 10 ] state=[-0.04919418 -0.22028794  0.10995506  0.45695551], action=1, reward=1.0, next_state=[-0.05359994 -0.02687829  0.11909417  0.2008547 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 208 ][ timestamp 11 ] state=[-0.05359994 -0.02687829  0.11909417  0.2008547 ], action=0, reward=1.0, next_state=[-0.05413751 -0.22348436  0.12311126  0.52860586]\n",
      "[ Experience replay ] starts\n",
      "[ episode 208 ][ timestamp 12 ] state=[-0.05413751 -0.22348436  0.12311126  0.52860586], action=0, reward=1.0, next_state=[-0.0586072  -0.4201037   0.13368338  0.857406  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 208 ][ timestamp 13 ] state=[-0.0586072  -0.4201037   0.13368338  0.857406  ], action=0, reward=1.0, next_state=[-0.06700927 -0.61676887  0.1508315   1.18895764]\n",
      "[ Experience replay ] starts\n",
      "[ episode 208 ][ timestamp 14 ] state=[-0.06700927 -0.61676887  0.1508315   1.18895764], action=0, reward=1.0, next_state=[-0.07934465 -0.81348908  0.17461065  1.52486427]\n",
      "[ Experience replay ] starts\n",
      "[ episode 208 ][ timestamp 15 ] state=[-0.07934465 -0.81348908  0.17461065  1.52486427], action=0, reward=1.0, next_state=[-0.09561443 -1.01023604  0.20510794  1.86657225]\n",
      "[ Experience replay ] starts\n",
      "[ episode 208 ][ timestamp 16 ] state=[-0.09561443 -1.01023604  0.20510794  1.86657225], action=0, reward=-1.0, next_state=[-0.11581915 -1.2069284   0.24243938  2.21530631]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 208: Exploration_rate=0.01. Score=16.\n",
      "[ episode 209 ] state=[-0.00254428  0.0314629  -0.03206711 -0.04317586]\n",
      "[ episode 209 ][ timestamp 1 ] state=[-0.00254428  0.0314629  -0.03206711 -0.04317586], action=1, reward=1.0, next_state=[-0.00191502  0.22702966 -0.03293063 -0.3458013 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 209 ][ timestamp 2 ] state=[-0.00191502  0.22702966 -0.03293063 -0.3458013 ], action=1, reward=1.0, next_state=[ 0.00262557  0.42260419 -0.03984665 -0.64868389]\n",
      "[ Experience replay ] starts\n",
      "[ episode 209 ][ timestamp 3 ] state=[ 0.00262557  0.42260419 -0.03984665 -0.64868389], action=0, reward=1.0, next_state=[ 0.01107766  0.22805932 -0.05282033 -0.36881005]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 209 ][ timestamp 4 ] state=[ 0.01107766  0.22805932 -0.05282033 -0.36881005], action=1, reward=1.0, next_state=[ 0.01563884  0.42389044 -0.06019653 -0.67766901]\n",
      "[ Experience replay ] starts\n",
      "[ episode 209 ][ timestamp 5 ] state=[ 0.01563884  0.42389044 -0.06019653 -0.67766901], action=0, reward=1.0, next_state=[ 0.02411665  0.2296542  -0.07374991 -0.40452945]\n",
      "[ Experience replay ] starts\n",
      "[ episode 209 ][ timestamp 6 ] state=[ 0.02411665  0.2296542  -0.07374991 -0.40452945], action=0, reward=1.0, next_state=[ 0.02870974  0.0356514  -0.0818405  -0.13597912]\n",
      "[ Experience replay ] starts\n",
      "[ episode 209 ][ timestamp 7 ] state=[ 0.02870974  0.0356514  -0.0818405  -0.13597912], action=1, reward=1.0, next_state=[ 0.02942276  0.23184442 -0.08456008 -0.4533179 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 209 ][ timestamp 8 ] state=[ 0.02942276  0.23184442 -0.08456008 -0.4533179 ], action=0, reward=1.0, next_state=[ 0.03405965  0.03801365 -0.09362644 -0.18844164]\n",
      "[ Experience replay ] starts\n",
      "[ episode 209 ][ timestamp 9 ] state=[ 0.03405965  0.03801365 -0.09362644 -0.18844164], action=1, reward=1.0, next_state=[ 0.03481992  0.23434171 -0.09739527 -0.50912991]\n",
      "[ Experience replay ] starts\n",
      "[ episode 209 ][ timestamp 10 ] state=[ 0.03481992  0.23434171 -0.09739527 -0.50912991], action=1, reward=1.0, next_state=[ 0.03950676  0.43069119 -0.10757787 -0.83084729]\n",
      "[ Experience replay ] starts\n",
      "[ episode 209 ][ timestamp 11 ] state=[ 0.03950676  0.43069119 -0.10757787 -0.83084729], action=0, reward=1.0, next_state=[ 0.04812058  0.2371911  -0.12419482 -0.573842  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 209 ][ timestamp 12 ] state=[ 0.04812058  0.2371911  -0.12419482 -0.573842  ], action=0, reward=1.0, next_state=[ 0.0528644   0.04400914 -0.13567166 -0.32272044]\n",
      "[ Experience replay ] starts\n",
      "[ episode 209 ][ timestamp 13 ] state=[ 0.0528644   0.04400914 -0.13567166 -0.32272044], action=1, reward=1.0, next_state=[ 0.05374459  0.2407759  -0.14212607 -0.65492357]\n",
      "[ Experience replay ] starts\n",
      "[ episode 209 ][ timestamp 14 ] state=[ 0.05374459  0.2407759  -0.14212607 -0.65492357], action=1, reward=1.0, next_state=[ 0.05856011  0.43756063 -0.15522454 -0.98876895]\n",
      "[ Experience replay ] starts\n",
      "[ episode 209 ][ timestamp 15 ] state=[ 0.05856011  0.43756063 -0.15522454 -0.98876895], action=1, reward=1.0, next_state=[ 0.06731132  0.63438146 -0.17499992 -1.32590355]\n",
      "[ Experience replay ] starts\n",
      "[ episode 209 ][ timestamp 16 ] state=[ 0.06731132  0.63438146 -0.17499992 -1.32590355], action=0, reward=1.0, next_state=[ 0.07999895  0.44184677 -0.20151799 -1.09270029]\n",
      "[ Experience replay ] starts\n",
      "[ episode 209 ][ timestamp 17 ] state=[ 0.07999895  0.44184677 -0.20151799 -1.09270029], action=0, reward=-1.0, next_state=[ 0.08883588  0.24986689 -0.22337199 -0.86940397]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 209: Exploration_rate=0.01. Score=17.\n",
      "[ episode 210 ] state=[-0.01491978  0.0400659  -0.04173926  0.00602452]\n",
      "[ episode 210 ][ timestamp 1 ] state=[-0.01491978  0.0400659  -0.04173926  0.00602452], action=1, reward=1.0, next_state=[-0.01411846  0.23576082 -0.04161877 -0.29952999]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 2 ] state=[-0.01411846  0.23576082 -0.04161877 -0.29952999], action=0, reward=1.0, next_state=[-0.00940325  0.04125605 -0.04760937 -0.02025786]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 3 ] state=[-0.00940325  0.04125605 -0.04760937 -0.02025786], action=0, reward=1.0, next_state=[-0.00857813 -0.15315195 -0.04801453  0.25703183]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 4 ] state=[-0.00857813 -0.15315195 -0.04801453  0.25703183], action=0, reward=1.0, next_state=[-0.01164117 -0.34755668 -0.04287389  0.53419202]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 5 ] state=[-0.01164117 -0.34755668 -0.04287389  0.53419202], action=1, reward=1.0, next_state=[-0.0185923  -0.15185885 -0.03219005  0.22831396]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 6 ] state=[-0.0185923  -0.15185885 -0.03219005  0.22831396], action=1, reward=1.0, next_state=[-0.02162948  0.04370798 -0.02762377 -0.07434655]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 7 ] state=[-0.02162948  0.04370798 -0.02762377 -0.07434655], action=0, reward=1.0, next_state=[-0.02075532 -0.15100729 -0.0291107   0.20949457]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 8 ] state=[-0.02075532 -0.15100729 -0.0291107   0.20949457], action=0, reward=1.0, next_state=[-0.02377546 -0.34570116 -0.02492081  0.49285431]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 9 ] state=[-0.02377546 -0.34570116 -0.02492081  0.49285431], action=1, reward=1.0, next_state=[-0.03068949 -0.15023674 -0.01506372  0.19242276]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 10 ] state=[-0.03068949 -0.15023674 -0.01506372  0.19242276], action=1, reward=1.0, next_state=[-0.03369422  0.04509742 -0.01121527 -0.10497381]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 11 ] state=[-0.03369422  0.04509742 -0.01121527 -0.10497381], action=1, reward=1.0, next_state=[-0.03279227  0.24037829 -0.01331474 -0.40117391]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 12 ] state=[-0.03279227  0.24037829 -0.01331474 -0.40117391], action=1, reward=1.0, next_state=[-0.02798471  0.43568654 -0.02133822 -0.69802474]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 13 ] state=[-0.02798471  0.43568654 -0.02133822 -0.69802474], action=1, reward=1.0, next_state=[-0.01927098  0.63109777 -0.03529872 -0.99734782]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 14 ] state=[-0.01927098  0.63109777 -0.03529872 -0.99734782], action=0, reward=1.0, next_state=[-0.00664902  0.43646512 -0.05524567 -0.71595638]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 15 ] state=[-0.00664902  0.43646512 -0.05524567 -0.71595638], action=0, reward=1.0, next_state=[ 0.00208028  0.2421496  -0.0695648  -0.44116175]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 16 ] state=[ 0.00208028  0.2421496  -0.0695648  -0.44116175], action=1, reward=1.0, next_state=[ 0.00692327  0.4381835  -0.07838804 -0.75493695]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 17 ] state=[ 0.00692327  0.4381835  -0.07838804 -0.75493695], action=0, reward=1.0, next_state=[ 0.01568694  0.24422463 -0.09348678 -0.48791454]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 18 ] state=[ 0.01568694  0.24422463 -0.09348678 -0.48791454], action=0, reward=1.0, next_state=[ 0.02057144  0.05053744 -0.10324507 -0.22609751]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 19 ] state=[ 0.02057144  0.05053744 -0.10324507 -0.22609751], action=0, reward=1.0, next_state=[ 0.02158219 -0.14296907 -0.10776702  0.03231646]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 20 ] state=[ 0.02158219 -0.14296907 -0.10776702  0.03231646], action=0, reward=1.0, next_state=[ 0.0187228  -0.3363938  -0.10712069  0.28914818]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 21 ] state=[ 0.0187228  -0.3363938  -0.10712069  0.28914818], action=0, reward=1.0, next_state=[ 0.01199493 -0.52983813 -0.10133772  0.54621818]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 22 ] state=[ 0.01199493 -0.52983813 -0.10133772  0.54621818], action=1, reward=1.0, next_state=[ 0.00139817 -0.33344929 -0.09041336  0.22340388]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 23 ] state=[ 0.00139817 -0.33344929 -0.09041336  0.22340388], action=0, reward=1.0, next_state=[-0.00527082 -0.52717047 -0.08594528  0.48625345]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 24 ] state=[-0.00527082 -0.52717047 -0.08594528  0.48625345], action=1, reward=1.0, next_state=[-0.01581423 -0.3309476  -0.07622021  0.16776871]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 25 ] state=[-0.01581423 -0.3309476  -0.07622021  0.16776871], action=0, reward=1.0, next_state=[-0.02243318 -0.52490047 -0.07286484  0.4354663 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 26 ] state=[-0.02243318 -0.52490047 -0.07286484  0.4354663 ], action=1, reward=1.0, next_state=[-0.03293119 -0.32882673 -0.06415551  0.1207328 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 27 ] state=[-0.03293119 -0.32882673 -0.06415551  0.1207328 ], action=1, reward=1.0, next_state=[-0.03950772 -0.13284707 -0.06174086 -0.19148071]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 28 ] state=[-0.03950772 -0.13284707 -0.06174086 -0.19148071], action=0, reward=1.0, next_state=[-0.04216467 -0.32703393 -0.06557047  0.0811043 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 29 ] state=[-0.04216467 -0.32703393 -0.06557047  0.0811043 ], action=1, reward=1.0, next_state=[-0.04870534 -0.13103629 -0.06394839 -0.23152428]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 30 ] state=[-0.04870534 -0.13103629 -0.06394839 -0.23152428], action=0, reward=1.0, next_state=[-0.05132607 -0.32518895 -0.06857887  0.04032143]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 210 ][ timestamp 31 ] state=[-0.05132607 -0.32518895 -0.06857887  0.04032143], action=1, reward=1.0, next_state=[-0.05782985 -0.12915402 -0.06777244 -0.27318615]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 32 ] state=[-0.05782985 -0.12915402 -0.06777244 -0.27318615], action=0, reward=1.0, next_state=[-0.06041293 -0.32324679 -0.07323616 -0.0026252 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 33 ] state=[-0.06041293 -0.32324679 -0.07323616 -0.0026252 ], action=1, reward=1.0, next_state=[-0.06687787 -0.1271551  -0.07328867 -0.31748648]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 34 ] state=[-0.06687787 -0.1271551  -0.07328867 -0.31748648], action=1, reward=1.0, next_state=[-0.06942097  0.06893003 -0.0796384  -0.6323522 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 35 ] state=[-0.06942097  0.06893003 -0.0796384  -0.6323522 ], action=1, reward=1.0, next_state=[-0.06804237  0.26506743 -0.09228544 -0.94901476]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 36 ] state=[-0.06804237  0.26506743 -0.09228544 -0.94901476], action=0, reward=1.0, next_state=[-0.06274102  0.07130092 -0.11126574 -0.68669522]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 37 ] state=[-0.06274102  0.07130092 -0.11126574 -0.68669522], action=0, reward=1.0, next_state=[-0.061315   -0.12211505 -0.12499964 -0.43100996]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 38 ] state=[-0.061315   -0.12211505 -0.12499964 -0.43100996], action=1, reward=1.0, next_state=[-0.0637573   0.07453471 -0.13361984 -0.76033739]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 39 ] state=[-0.0637573   0.07453471 -0.13361984 -0.76033739], action=0, reward=1.0, next_state=[-0.06226661 -0.1185181  -0.14882659 -0.51250688]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 40 ] state=[-0.06226661 -0.1185181  -0.14882659 -0.51250688], action=1, reward=1.0, next_state=[-0.06463697  0.07835215 -0.15907673 -0.84814155]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 41 ] state=[-0.06463697  0.07835215 -0.15907673 -0.84814155], action=0, reward=1.0, next_state=[-0.06306993 -0.11428418 -0.17603956 -0.60940697]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 42 ] state=[-0.06306993 -0.11428418 -0.17603956 -0.60940697], action=1, reward=1.0, next_state=[-0.06535561  0.08280483 -0.1882277  -0.9519602 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 43 ] state=[-0.06535561  0.08280483 -0.1882277  -0.9519602 ], action=0, reward=1.0, next_state=[-0.06369951 -0.1093541  -0.2072669  -0.72382558]\n",
      "[ Experience replay ] starts\n",
      "[ episode 210 ][ timestamp 44 ] state=[-0.06369951 -0.1093541  -0.2072669  -0.72382558], action=0, reward=-1.0, next_state=[-0.0658866  -0.30109899 -0.22174341 -0.50286523]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 210: Exploration_rate=0.01. Score=44.\n",
      "[ episode 211 ] state=[ 0.02199773 -0.03788185  0.01362846  0.03747212]\n",
      "[ episode 211 ][ timestamp 1 ] state=[ 0.02199773 -0.03788185  0.01362846  0.03747212], action=0, reward=1.0, next_state=[ 0.02124009 -0.23319655  0.0143779   0.33442362]\n",
      "[ Experience replay ] starts\n",
      "[ episode 211 ][ timestamp 2 ] state=[ 0.02124009 -0.23319655  0.0143779   0.33442362], action=1, reward=1.0, next_state=[ 0.01657616 -0.03828215  0.02106637  0.04630919]\n",
      "[ Experience replay ] starts\n",
      "[ episode 211 ][ timestamp 3 ] state=[ 0.01657616 -0.03828215  0.02106637  0.04630919], action=1, reward=1.0, next_state=[ 0.01581051  0.15653148  0.02199256 -0.23965336]\n",
      "[ Experience replay ] starts\n",
      "[ episode 211 ][ timestamp 4 ] state=[ 0.01581051  0.15653148  0.02199256 -0.23965336], action=0, reward=1.0, next_state=[ 0.01894114 -0.03889762  0.01719949  0.0598847 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 211 ][ timestamp 5 ] state=[ 0.01894114 -0.03889762  0.01719949  0.0598847 ], action=0, reward=1.0, next_state=[ 0.01816319 -0.2342619   0.01839719  0.35794418]\n",
      "[ Experience replay ] starts\n",
      "[ episode 211 ][ timestamp 6 ] state=[ 0.01816319 -0.2342619   0.01839719  0.35794418], action=0, reward=1.0, next_state=[ 0.01347795 -0.4296405   0.02555607  0.65637094]\n",
      "[ Experience replay ] starts\n",
      "[ episode 211 ][ timestamp 7 ] state=[ 0.01347795 -0.4296405   0.02555607  0.65637094], action=0, reward=1.0, next_state=[ 0.00488514 -0.62510873  0.03868349  0.9569902 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 211 ][ timestamp 8 ] state=[ 0.00488514 -0.62510873  0.03868349  0.9569902 ], action=0, reward=1.0, next_state=[-0.00761703 -0.82072894  0.05782329  1.26157111]\n",
      "[ Experience replay ] starts\n",
      "[ episode 211 ][ timestamp 9 ] state=[-0.00761703 -0.82072894  0.05782329  1.26157111], action=1, reward=1.0, next_state=[-0.02403161 -0.62639217  0.08305471  0.98754372]\n",
      "[ Experience replay ] starts\n",
      "[ episode 211 ][ timestamp 10 ] state=[-0.02403161 -0.62639217  0.08305471  0.98754372], action=1, reward=1.0, next_state=[-0.03655945 -0.43247459  0.10280559  0.72206005]\n",
      "[ Experience replay ] starts\n",
      "[ episode 211 ][ timestamp 11 ] state=[-0.03655945 -0.43247459  0.10280559  0.72206005], action=1, reward=1.0, next_state=[-0.04520894 -0.23891362  0.11724679  0.46342318]\n",
      "[ Experience replay ] starts\n",
      "[ episode 211 ][ timestamp 12 ] state=[-0.04520894 -0.23891362  0.11724679  0.46342318], action=1, reward=1.0, next_state=[-0.04998722 -0.04562681  0.12651525  0.20987513]\n",
      "[ Experience replay ] starts\n",
      "[ episode 211 ][ timestamp 13 ] state=[-0.04998722 -0.04562681  0.12651525  0.20987513], action=1, reward=1.0, next_state=[-0.05089975  0.14748049  0.13071276 -0.0403744 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 211 ][ timestamp 14 ] state=[-0.05089975  0.14748049  0.13071276 -0.0403744 ], action=0, reward=1.0, next_state=[-0.04795014 -0.04925004  0.12990527  0.29052421]\n",
      "[ Experience replay ] starts\n",
      "[ episode 211 ][ timestamp 15 ] state=[-0.04795014 -0.04925004  0.12990527  0.29052421], action=0, reward=1.0, next_state=[-0.04893514 -0.24596193  0.13571575  0.62119068]\n",
      "[ Experience replay ] starts\n",
      "[ episode 211 ][ timestamp 16 ] state=[-0.04893514 -0.24596193  0.13571575  0.62119068], action=0, reward=1.0, next_state=[-0.05385438 -0.44269198  0.14813957  0.95335034]\n",
      "[ Experience replay ] starts\n",
      "[ episode 211 ][ timestamp 17 ] state=[-0.05385438 -0.44269198  0.14813957  0.95335034], action=1, reward=1.0, next_state=[-0.06270822 -0.24984016  0.16720657  0.71063487]\n",
      "[ Experience replay ] starts\n",
      "[ episode 211 ][ timestamp 18 ] state=[-0.06270822 -0.24984016  0.16720657  0.71063487], action=1, reward=1.0, next_state=[-0.06770503 -0.05737982  0.18141927  0.47490055]\n",
      "[ Experience replay ] starts\n",
      "[ episode 211 ][ timestamp 19 ] state=[-0.06770503 -0.05737982  0.18141927  0.47490055], action=1, reward=1.0, next_state=[-0.06885262  0.13477902  0.19091728  0.24443786]\n",
      "[ Experience replay ] starts\n",
      "[ episode 211 ][ timestamp 20 ] state=[-0.06885262  0.13477902  0.19091728  0.24443786], action=0, reward=1.0, next_state=[-0.06615704 -0.06248411  0.19580604  0.59074564]\n",
      "[ Experience replay ] starts\n",
      "[ episode 211 ][ timestamp 21 ] state=[-0.06615704 -0.06248411  0.19580604  0.59074564], action=0, reward=1.0, next_state=[-0.06740672 -0.25973043  0.20762095  0.93816122]\n",
      "[ Experience replay ] starts\n",
      "[ episode 211 ][ timestamp 22 ] state=[-0.06740672 -0.25973043  0.20762095  0.93816122], action=0, reward=-1.0, next_state=[-0.07260133 -0.45695495  0.22638418  1.28824759]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 211: Exploration_rate=0.01. Score=22.\n",
      "[ episode 212 ] state=[-0.01494122  0.00728337 -0.03881968  0.04721626]\n",
      "[ episode 212 ][ timestamp 1 ] state=[-0.01494122  0.00728337 -0.03881968  0.04721626], action=0, reward=1.0, next_state=[-0.01479555 -0.18726105 -0.03787535  0.32740292]\n",
      "[ Experience replay ] starts\n",
      "[ episode 212 ][ timestamp 2 ] state=[-0.01479555 -0.18726105 -0.03787535  0.32740292], action=0, reward=1.0, next_state=[-0.01854077 -0.38182388 -0.03132729  0.60790517]\n",
      "[ Experience replay ] starts\n",
      "[ episode 212 ][ timestamp 3 ] state=[-0.01854077 -0.38182388 -0.03132729  0.60790517], action=1, reward=1.0, next_state=[-0.02617725 -0.18627828 -0.01916919  0.30552196]\n",
      "[ Experience replay ] starts\n",
      "[ episode 212 ][ timestamp 4 ] state=[-0.02617725 -0.18627828 -0.01916919  0.30552196], action=0, reward=1.0, next_state=[-0.02990281 -0.38112189 -0.01305875  0.59209829]\n",
      "[ Experience replay ] starts\n",
      "[ episode 212 ][ timestamp 5 ] state=[-0.02990281 -0.38112189 -0.01305875  0.59209829], action=0, reward=1.0, next_state=[-0.03752525 -0.57605861 -0.00121679  0.88063928]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 212 ][ timestamp 6 ] state=[-0.03752525 -0.57605861 -0.00121679  0.88063928], action=0, reward=1.0, next_state=[-0.04904642 -0.77116401  0.016396    1.17293943]\n",
      "[ Experience replay ] starts\n",
      "[ episode 212 ][ timestamp 7 ] state=[-0.04904642 -0.77116401  0.016396    1.17293943], action=0, reward=1.0, next_state=[-0.0644697  -0.96649522  0.03985479  1.47071706]\n",
      "[ Experience replay ] starts\n",
      "[ episode 212 ][ timestamp 8 ] state=[-0.0644697  -0.96649522  0.03985479  1.47071706], action=0, reward=1.0, next_state=[-0.08379961 -1.16208134  0.06926913  1.77557747]\n",
      "[ Experience replay ] starts\n",
      "[ episode 212 ][ timestamp 9 ] state=[-0.08379961 -1.16208134  0.06926913  1.77557747], action=1, reward=1.0, next_state=[-0.10704123 -0.96780484  0.10478068  1.50521043]\n",
      "[ Experience replay ] starts\n",
      "[ episode 212 ][ timestamp 10 ] state=[-0.10704123 -0.96780484  0.10478068  1.50521043], action=1, reward=1.0, next_state=[-0.12639733 -0.77409849  0.13488489  1.24699365]\n",
      "[ Experience replay ] starts\n",
      "[ episode 212 ][ timestamp 11 ] state=[-0.12639733 -0.77409849  0.13488489  1.24699365], action=0, reward=1.0, next_state=[-0.1418793  -0.9706674   0.15982476  1.57870482]\n",
      "[ Experience replay ] starts\n",
      "[ episode 212 ][ timestamp 12 ] state=[-0.1418793  -0.9706674   0.15982476  1.57870482], action=1, reward=1.0, next_state=[-0.16129265 -0.77776925  0.19139886  1.33983396]\n",
      "[ Experience replay ] starts\n",
      "[ episode 212 ][ timestamp 13 ] state=[-0.16129265 -0.77776925  0.19139886  1.33983396], action=0, reward=-1.0, next_state=[-0.17684803 -0.97471492  0.21819554  1.68578619]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 212: Exploration_rate=0.01. Score=13.\n",
      "[ episode 213 ] state=[-0.0270265   0.02428721  0.00760582  0.03840293]\n",
      "[ episode 213 ][ timestamp 1 ] state=[-0.0270265   0.02428721  0.00760582  0.03840293], action=1, reward=1.0, next_state=[-0.02654076  0.21929928  0.00837388 -0.25187062]\n",
      "[ Experience replay ] starts\n",
      "[ episode 213 ][ timestamp 2 ] state=[-0.02654076  0.21929928  0.00837388 -0.25187062], action=0, reward=1.0, next_state=[-0.02215477  0.02405876  0.00333647  0.04344179]\n",
      "[ Experience replay ] starts\n",
      "[ episode 213 ][ timestamp 3 ] state=[-0.02215477  0.02405876  0.00333647  0.04344179], action=0, reward=1.0, next_state=[-0.0216736  -0.17111088  0.00420531  0.33717553]\n",
      "[ Experience replay ] starts\n",
      "[ episode 213 ][ timestamp 4 ] state=[-0.0216736  -0.17111088  0.00420531  0.33717553], action=0, reward=1.0, next_state=[-0.02509582 -0.36629242  0.01094882  0.63118161]\n",
      "[ Experience replay ] starts\n",
      "[ episode 213 ][ timestamp 5 ] state=[-0.02509582 -0.36629242  0.01094882  0.63118161], action=1, reward=1.0, next_state=[-0.03242166 -0.17132493  0.02357245  0.3419668 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 213 ][ timestamp 6 ] state=[-0.03242166 -0.17132493  0.02357245  0.3419668 ], action=0, reward=1.0, next_state=[-0.03584816 -0.36677419  0.03041178  0.64198889]\n",
      "[ Experience replay ] starts\n",
      "[ episode 213 ][ timestamp 7 ] state=[-0.03584816 -0.36677419  0.03041178  0.64198889], action=0, reward=1.0, next_state=[-0.04318365 -0.56230657  0.04325156  0.94409152]\n",
      "[ Experience replay ] starts\n",
      "[ episode 213 ][ timestamp 8 ] state=[-0.04318365 -0.56230657  0.04325156  0.94409152], action=0, reward=1.0, next_state=[-0.05442978 -0.75798367  0.06213339  1.25004467]\n",
      "[ Experience replay ] starts\n",
      "[ episode 213 ][ timestamp 9 ] state=[-0.05442978 -0.75798367  0.06213339  1.25004467], action=1, reward=1.0, next_state=[-0.06958945 -0.56371066  0.08713429  0.97745294]\n",
      "[ Experience replay ] starts\n",
      "[ episode 213 ][ timestamp 10 ] state=[-0.06958945 -0.56371066  0.08713429  0.97745294], action=1, reward=1.0, next_state=[-0.08086367 -0.36985833  0.10668334  0.71336267]\n",
      "[ Experience replay ] starts\n",
      "[ episode 213 ][ timestamp 11 ] state=[-0.08086367 -0.36985833  0.10668334  0.71336267], action=1, reward=1.0, next_state=[-0.08826083 -0.17636245  0.1209506   0.45607442]\n",
      "[ Experience replay ] starts\n",
      "[ episode 213 ][ timestamp 12 ] state=[-0.08826083 -0.17636245  0.1209506   0.45607442], action=1, reward=1.0, next_state=[-0.09178808  0.01686042  0.13007209  0.20383037]\n",
      "[ Experience replay ] starts\n",
      "[ episode 213 ][ timestamp 13 ] state=[-0.09178808  0.01686042  0.13007209  0.20383037], action=1, reward=1.0, next_state=[-0.09145087  0.20990554  0.13414869 -0.04515775]\n",
      "[ Experience replay ] starts\n",
      "[ episode 213 ][ timestamp 14 ] state=[-0.09145087  0.20990554  0.13414869 -0.04515775], action=0, reward=1.0, next_state=[-0.08725276  0.01314051  0.13324554  0.28665959]\n",
      "[ Experience replay ] starts\n",
      "[ episode 213 ][ timestamp 15 ] state=[-0.08725276  0.01314051  0.13324554  0.28665959], action=1, reward=1.0, next_state=[-0.08698995  0.20613547  0.13897873  0.0387916 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 213 ][ timestamp 16 ] state=[-0.08698995  0.20613547  0.13897873  0.0387916 ], action=0, reward=1.0, next_state=[-0.08286724  0.0093225   0.13975456  0.37189288]\n",
      "[ Experience replay ] starts\n",
      "[ episode 213 ][ timestamp 17 ] state=[-0.08286724  0.0093225   0.13975456  0.37189288], action=0, reward=1.0, next_state=[-0.08268079 -0.18747952  0.14719242  0.70517196]\n",
      "[ Experience replay ] starts\n",
      "[ episode 213 ][ timestamp 18 ] state=[-0.08268079 -0.18747952  0.14719242  0.70517196], action=1, reward=1.0, next_state=[-0.08643038  0.00532953  0.16129586  0.46220421]\n",
      "[ Experience replay ] starts\n",
      "[ episode 213 ][ timestamp 19 ] state=[-0.08643038  0.00532953  0.16129586  0.46220421], action=0, reward=1.0, next_state=[-0.08632379 -0.1916605   0.17053994  0.80106949]\n",
      "[ Experience replay ] starts\n",
      "[ episode 213 ][ timestamp 20 ] state=[-0.08632379 -0.1916605   0.17053994  0.80106949], action=0, reward=1.0, next_state=[-0.090157   -0.38865966  0.18656133  1.14217757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 213 ][ timestamp 21 ] state=[-0.090157   -0.38865966  0.18656133  1.14217757], action=1, reward=1.0, next_state=[-0.09793019 -0.19639952  0.20940488  0.91332297]\n",
      "[ Experience replay ] starts\n",
      "[ episode 213 ][ timestamp 22 ] state=[-0.09793019 -0.19639952  0.20940488  0.91332297], action=1, reward=-1.0, next_state=[-0.10185819 -0.00463091  0.22767134  0.69306996]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 213: Exploration_rate=0.01. Score=22.\n",
      "[ episode 214 ] state=[ 0.04715388 -0.00318172  0.04697186 -0.04852134]\n",
      "[ episode 214 ][ timestamp 1 ] state=[ 0.04715388 -0.00318172  0.04697186 -0.04852134], action=1, reward=1.0, next_state=[ 0.04709024  0.19123632  0.04600144 -0.3260221 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 2 ] state=[ 0.04709024  0.19123632  0.04600144 -0.3260221 ], action=1, reward=1.0, next_state=[ 0.05091497  0.38567416  0.039481   -0.60385066]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 3 ] state=[ 0.05091497  0.38567416  0.039481   -0.60385066], action=0, reward=1.0, next_state=[ 0.05862845  0.19002293  0.02740398 -0.29899811]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 4 ] state=[ 0.05862845  0.19002293  0.02740398 -0.29899811], action=1, reward=1.0, next_state=[ 0.06242891  0.38474375  0.02142402 -0.58291392]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 5 ] state=[ 0.06242891  0.38474375  0.02142402 -0.58291392], action=0, reward=1.0, next_state=[ 0.07012379  0.18932831  0.00976574 -0.28355984]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 6 ] state=[ 0.07012379  0.18932831  0.00976574 -0.28355984], action=1, reward=1.0, next_state=[ 0.07391035  0.38430962  0.00409454 -0.57314678]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 7 ] state=[ 0.07391035  0.38430962  0.00409454 -0.57314678], action=0, reward=1.0, next_state=[ 0.08159655  0.1891305  -0.00736839 -0.27917676]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 8 ] state=[ 0.08159655  0.1891305  -0.00736839 -0.27917676], action=1, reward=1.0, next_state=[ 0.08537916  0.38435678 -0.01295193 -0.57417453]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 9 ] state=[ 0.08537916  0.38435678 -0.01295193 -0.57417453], action=0, reward=1.0, next_state=[ 0.09306629  0.18941879 -0.02443542 -0.28559982]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 10 ] state=[ 0.09306629  0.18941879 -0.02443542 -0.28559982], action=1, reward=1.0, next_state=[ 0.09685467  0.38488056 -0.03014741 -0.58588824]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 11 ] state=[ 0.09685467  0.38488056 -0.03014741 -0.58588824], action=0, reward=1.0, next_state=[ 0.10455228  0.19019355 -0.04186518 -0.30285243]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 12 ] state=[ 0.10455228  0.19019355 -0.04186518 -0.30285243], action=0, reward=1.0, next_state=[ 0.10835615 -0.00430751 -0.04792223 -0.02366124]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 13 ] state=[ 0.10835615 -0.00430751 -0.04792223 -0.02366124], action=0, reward=1.0, next_state=[ 0.10827    -0.19871062 -0.04839545  0.25352491]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 14 ] state=[ 0.10827    -0.19871062 -0.04839545  0.25352491], action=0, reward=1.0, next_state=[ 0.10429579 -0.39310934 -0.04332495  0.53055886]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 214 ][ timestamp 15 ] state=[ 0.10429579 -0.39310934 -0.04332495  0.53055886], action=1, reward=1.0, next_state=[ 0.0964336  -0.19740558 -0.03271378  0.22454513]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 16 ] state=[ 0.0964336  -0.19740558 -0.03271378  0.22454513], action=1, reward=1.0, next_state=[ 0.09248549 -0.00183172 -0.02822287 -0.07827483]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 17 ] state=[ 0.09248549 -0.00183172 -0.02822287 -0.07827483], action=1, reward=1.0, next_state=[ 0.09244885  0.19368322 -0.02978837 -0.37972686]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 18 ] state=[ 0.09244885  0.19368322 -0.02978837 -0.37972686], action=1, reward=1.0, next_state=[ 0.09632252  0.38921524 -0.03738291 -0.68165127]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 19 ] state=[ 0.09632252  0.38921524 -0.03738291 -0.68165127], action=1, reward=1.0, next_state=[ 0.10410682  0.58483588 -0.05101593 -0.98586523]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 20 ] state=[ 0.10410682  0.58483588 -0.05101593 -0.98586523], action=0, reward=1.0, next_state=[ 0.11580354  0.39043294 -0.07073324 -0.70963239]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 21 ] state=[ 0.11580354  0.39043294 -0.07073324 -0.70963239], action=0, reward=1.0, next_state=[ 0.1236122   0.19635824 -0.08492588 -0.44002651]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 22 ] state=[ 0.1236122   0.19635824 -0.08492588 -0.44002651], action=0, reward=1.0, next_state=[ 0.12753936  0.00253442 -0.09372641 -0.17527681]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 23 ] state=[ 0.12753936  0.00253442 -0.09372641 -0.17527681], action=1, reward=1.0, next_state=[ 0.12759005  0.19886405 -0.09723195 -0.49599393]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 24 ] state=[ 0.12759005  0.19886405 -0.09723195 -0.49599393], action=0, reward=1.0, next_state=[ 0.13156733  0.005238   -0.10715183 -0.23546785]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 25 ] state=[ 0.13156733  0.005238   -0.10715183 -0.23546785], action=0, reward=1.0, next_state=[ 0.13167209 -0.18820287 -0.11186119  0.0215869 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 26 ] state=[ 0.13167209 -0.18820287 -0.11186119  0.0215869 ], action=0, reward=1.0, next_state=[ 0.12790804 -0.38155767 -0.11142945  0.27698778]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 27 ] state=[ 0.12790804 -0.38155767 -0.11142945  0.27698778], action=1, reward=1.0, next_state=[ 0.12027688 -0.18503698 -0.10588969 -0.04865757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 28 ] state=[ 0.12027688 -0.18503698 -0.10588969 -0.04865757], action=0, reward=1.0, next_state=[ 0.11657614 -0.37849377 -0.10686284  0.20882883]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 29 ] state=[ 0.11657614 -0.37849377 -0.10686284  0.20882883], action=1, reward=1.0, next_state=[ 0.10900627 -0.18201897 -0.10268627 -0.11556012]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 30 ] state=[ 0.10900627 -0.18201897 -0.10268627 -0.11556012], action=0, reward=1.0, next_state=[ 0.10536589 -0.37553114 -0.10499747  0.14304237]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 31 ] state=[ 0.10536589 -0.37553114 -0.10499747  0.14304237], action=0, reward=1.0, next_state=[ 0.09785527 -0.56900491 -0.10213662  0.40084222]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 32 ] state=[ 0.09785527 -0.56900491 -0.10213662  0.40084222], action=1, reward=1.0, next_state=[ 0.08647517 -0.37259372 -0.09411978  0.07778482]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 33 ] state=[ 0.08647517 -0.37259372 -0.09411978  0.07778482], action=0, reward=1.0, next_state=[ 0.07902329 -0.56624921 -0.09256408  0.33935201]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 34 ] state=[ 0.07902329 -0.56624921 -0.09256408  0.33935201], action=1, reward=1.0, next_state=[ 0.06769831 -0.36994044 -0.08577704  0.01897444]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 35 ] state=[ 0.06769831 -0.36994044 -0.08577704  0.01897444], action=0, reward=1.0, next_state=[ 0.0602995  -0.5637342  -0.08539755  0.28340879]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 36 ] state=[ 0.0602995  -0.5637342  -0.08539755  0.28340879], action=1, reward=1.0, next_state=[ 0.04902482 -0.36750461 -0.07972938 -0.03493933]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 37 ] state=[ 0.04902482 -0.36750461 -0.07972938 -0.03493933], action=0, reward=1.0, next_state=[ 0.04167472 -0.56139806 -0.08042816  0.23156133]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 38 ] state=[ 0.04167472 -0.56139806 -0.08042816  0.23156133], action=1, reward=1.0, next_state=[ 0.03044676 -0.36522448 -0.07579694 -0.08536822]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 39 ] state=[ 0.03044676 -0.36522448 -0.07579694 -0.08536822], action=0, reward=1.0, next_state=[ 0.02314227 -0.55918271 -0.0775043   0.18247081]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 40 ] state=[ 0.02314227 -0.55918271 -0.0775043   0.18247081], action=0, reward=1.0, next_state=[ 0.01195862 -0.75311502 -0.07385489  0.44973257]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 41 ] state=[ 0.01195862 -0.75311502 -0.07385489  0.44973257], action=0, reward=1.0, next_state=[-0.00310368 -0.94711891 -0.06486023  0.7182515 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 42 ] state=[-0.00310368 -0.94711891 -0.06486023  0.7182515 ], action=1, reward=1.0, next_state=[-0.02204606 -0.75116223 -0.0504952   0.40587899]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 43 ] state=[-0.02204606 -0.75116223 -0.0504952   0.40587899], action=1, reward=1.0, next_state=[-0.0370693  -0.55536194 -0.04237762  0.09771364]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 44 ] state=[-0.0370693  -0.55536194 -0.04237762  0.09771364], action=0, reward=1.0, next_state=[-0.04817654 -0.74985172 -0.04042335  0.37673109]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 45 ] state=[-0.04817654 -0.74985172 -0.04042335  0.37673109], action=0, reward=1.0, next_state=[-0.06317358 -0.94437692 -0.03288873  0.6563993 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 46 ] state=[-0.06317358 -0.94437692 -0.03288873  0.6563993 ], action=1, reward=1.0, next_state=[-0.08206112 -0.74881293 -0.01976074  0.35354441]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 47 ] state=[-0.08206112 -0.74881293 -0.01976074  0.35354441], action=0, reward=1.0, next_state=[-0.09703737 -0.9436484  -0.01268986  0.63993127]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 48 ] state=[-0.09703737 -0.9436484  -0.01268986  0.63993127], action=1, reward=1.0, next_state=[-1.15910343e-01 -7.48351849e-01  1.08769349e-04  3.43279313e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 49 ] state=[-1.15910343e-01 -7.48351849e-01  1.08769349e-04  3.43279313e-01], action=0, reward=1.0, next_state=[-0.13087738 -0.94347535  0.00697436  0.63599654]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 50 ] state=[-0.13087738 -0.94347535  0.00697436  0.63599654], action=1, reward=1.0, next_state=[-0.14974689 -0.74845136  0.01969429  0.34551811]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 51 ] state=[-0.14974689 -0.74845136  0.01969429  0.34551811], action=0, reward=1.0, next_state=[-0.16471591 -0.94384784  0.02660465  0.64434574]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 52 ] state=[-0.16471591 -0.94384784  0.02660465  0.64434574], action=0, reward=1.0, next_state=[-0.18359287 -1.13933027  0.03949156  0.94528646]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 53 ] state=[-0.18359287 -1.13933027  0.03949156  0.94528646], action=1, reward=1.0, next_state=[-0.20637948 -0.94476187  0.05839729  0.66526891]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 54 ] state=[-0.20637948 -0.94476187  0.05839729  0.66526891], action=1, reward=1.0, next_state=[-0.22527471 -0.75049873  0.07170267  0.39152998]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 55 ] state=[-0.22527471 -0.75049873  0.07170267  0.39152998], action=1, reward=1.0, next_state=[-0.24028469 -0.55646374  0.07953327  0.12228789]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 56 ] state=[-0.24028469 -0.55646374  0.07953327  0.12228789], action=0, reward=1.0, next_state=[-0.25141396 -0.75262975  0.08197903  0.43896489]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 57 ] state=[-0.25141396 -0.75262975  0.08197903  0.43896489], action=1, reward=1.0, next_state=[-0.26646656 -0.55875796  0.09075833  0.17320871]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 58 ] state=[-0.26646656 -0.55875796  0.09075833  0.17320871], action=1, reward=1.0, next_state=[-0.27764172 -0.36504426  0.0942225  -0.08951961]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 214 ][ timestamp 59 ] state=[-0.27764172 -0.36504426  0.0942225  -0.08951961], action=0, reward=1.0, next_state=[-0.2849426  -0.56138162  0.09243211  0.23134054]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 60 ] state=[-0.2849426  -0.56138162  0.09243211  0.23134054], action=0, reward=1.0, next_state=[-0.29617023 -0.75769445  0.09705892  0.55168912]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 61 ] state=[-0.29617023 -0.75769445  0.09705892  0.55168912], action=1, reward=1.0, next_state=[-0.31132412 -0.56406002  0.1080927   0.29109503]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 62 ] state=[-0.31132412 -0.56406002  0.1080927   0.29109503], action=1, reward=1.0, next_state=[-0.32260532 -0.37063203  0.1139146   0.03436382]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 63 ] state=[-0.32260532 -0.37063203  0.1139146   0.03436382], action=1, reward=1.0, next_state=[-0.33001796 -0.17731232  0.11460188 -0.22031781]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 64 ] state=[-0.33001796 -0.17731232  0.11460188 -0.22031781], action=1, reward=1.0, next_state=[-0.33356421  0.01600088  0.11019552 -0.47476627]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 65 ] state=[-0.33356421  0.01600088  0.11019552 -0.47476627], action=1, reward=1.0, next_state=[-0.33324419  0.20940826  0.1007002  -0.73078575]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 66 ] state=[-0.33324419  0.20940826  0.1007002  -0.73078575], action=0, reward=1.0, next_state=[-0.32905603  0.01304945  0.08608448 -0.40818381]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 67 ] state=[-0.32905603  0.01304945  0.08608448 -0.40818381], action=1, reward=1.0, next_state=[-0.32879504  0.20685206  0.07792081 -0.67253367]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 68 ] state=[-0.32879504  0.20685206  0.07792081 -0.67253367], action=1, reward=1.0, next_state=[-0.324658    0.40080938  0.06447013 -0.93970133]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 69 ] state=[-0.324658    0.40080938  0.06447013 -0.93970133], action=1, reward=1.0, next_state=[-0.31664181  0.59500582  0.04567611 -1.21144974]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 70 ] state=[-0.31664181  0.59500582  0.04567611 -1.21144974], action=0, reward=1.0, next_state=[-0.30474169  0.39932495  0.02144711 -0.90481045]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 71 ] state=[-0.30474169  0.39932495  0.02144711 -0.90481045], action=0, reward=1.0, next_state=[-0.2967552   0.20391921  0.0033509  -0.60546429]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 72 ] state=[-0.2967552   0.20391921  0.0033509  -0.60546429], action=1, reward=1.0, next_state=[-0.29267681  0.39899415 -0.00875838 -0.89708989]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 73 ] state=[-0.29267681  0.39899415 -0.00875838 -0.89708989], action=0, reward=1.0, next_state=[-0.28469693  0.20399201 -0.02670018 -0.60717284]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 74 ] state=[-0.28469693  0.20399201 -0.02670018 -0.60717284], action=0, reward=1.0, next_state=[-0.28061709  0.00925335 -0.03884364 -0.32301789]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 75 ] state=[-0.28061709  0.00925335 -0.03884364 -0.32301789], action=1, reward=1.0, next_state=[-0.28043202  0.20490628 -0.045304   -0.62769305]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 76 ] state=[-0.28043202  0.20490628 -0.045304   -0.62769305], action=0, reward=1.0, next_state=[-0.27633389  0.01044494 -0.05785786 -0.34961515]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 77 ] state=[-0.27633389  0.01044494 -0.05785786 -0.34961515], action=1, reward=1.0, next_state=[-0.276125    0.20633998 -0.06485016 -0.65996675]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 78 ] state=[-0.276125    0.20633998 -0.06485016 -0.65996675], action=1, reward=1.0, next_state=[-0.2719982   0.40230162 -0.0780495  -0.97234391]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 79 ] state=[-0.2719982   0.40230162 -0.0780495  -0.97234391], action=0, reward=1.0, next_state=[-0.26395216  0.20830885 -0.09749637 -0.70516388]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 80 ] state=[-0.26395216  0.20830885 -0.09749637 -0.70516388], action=1, reward=1.0, next_state=[-0.25978599  0.40463685 -0.11159965 -1.02687589]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 81 ] state=[-0.25978599  0.40463685 -0.11159965 -1.02687589], action=0, reward=1.0, next_state=[-0.25169325  0.21116316 -0.13213717 -0.77121292]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 82 ] state=[-0.25169325  0.21116316 -0.13213717 -0.77121292], action=0, reward=1.0, next_state=[-0.24746999  0.01808306 -0.14756143 -0.52285288]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 83 ] state=[-0.24746999  0.01808306 -0.14756143 -0.52285288], action=1, reward=1.0, next_state=[-0.24710833  0.21494008 -0.15801848 -0.85815521]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 84 ] state=[-0.24710833  0.21494008 -0.15801848 -0.85815521], action=1, reward=1.0, next_state=[-0.24280952  0.41182093 -0.17518159 -1.19606142]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 85 ] state=[-0.24280952  0.41182093 -0.17518159 -1.19606142], action=0, reward=1.0, next_state=[-0.23457311  0.2193448  -0.19910282 -0.9630064 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ][ timestamp 86 ] state=[-0.23457311  0.2193448  -0.19910282 -0.9630064 ], action=0, reward=-1.0, next_state=[-0.23018621  0.02737398 -0.21836295 -0.73888914]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 214: Exploration_rate=0.01. Score=86.\n",
      "[ episode 215 ] state=[ 0.01938205  0.04911825 -0.02405234 -0.03285232]\n",
      "[ episode 215 ][ timestamp 1 ] state=[ 0.01938205  0.04911825 -0.02405234 -0.03285232], action=0, reward=1.0, next_state=[ 0.02036442 -0.14565067 -0.02470939  0.25214585]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 2 ] state=[ 0.02036442 -0.14565067 -0.02470939  0.25214585], action=1, reward=1.0, next_state=[ 0.0174514   0.04981524 -0.01966647 -0.04822734]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 3 ] state=[ 0.0174514   0.04981524 -0.01966647 -0.04822734], action=1, reward=1.0, next_state=[ 0.01844771  0.24521359 -0.02063102 -0.34704976]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 4 ] state=[ 0.01844771  0.24521359 -0.02063102 -0.34704976], action=1, reward=1.0, next_state=[ 0.02335198  0.44062283 -0.02757202 -0.64616632]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 5 ] state=[ 0.02335198  0.44062283 -0.02757202 -0.64616632], action=0, reward=1.0, next_state=[ 0.03216444  0.2458957  -0.04049534 -0.36229179]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 6 ] state=[ 0.03216444  0.2458957  -0.04049534 -0.36229179], action=0, reward=1.0, next_state=[ 0.03708235  0.05137202 -0.04774118 -0.08264787]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 7 ] state=[ 0.03708235  0.05137202 -0.04774118 -0.08264787], action=0, reward=1.0, next_state=[ 0.03810979 -0.14303422 -0.04939414  0.19459865]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 8 ] state=[ 0.03810979 -0.14303422 -0.04939414  0.19459865], action=0, reward=1.0, next_state=[ 0.03524911 -0.33741609 -0.04550216  0.47129987]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 9 ] state=[ 0.03524911 -0.33741609 -0.04550216  0.47129987], action=0, reward=1.0, next_state=[ 0.02850079 -0.53186679 -0.03607616  0.74930101]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 10 ] state=[ 0.02850079 -0.53186679 -0.03607616  0.74930101], action=0, reward=1.0, next_state=[ 0.01786345 -0.72647304 -0.02109014  1.03041635]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 11 ] state=[ 0.01786345 -0.72647304 -0.02109014  1.03041635], action=1, reward=1.0, next_state=[ 3.33398971e-03 -5.31076914e-01 -4.81817586e-04  7.31187305e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 12 ] state=[ 3.33398971e-03 -5.31076914e-01 -4.81817586e-04  7.31187305e-01], action=1, reward=1.0, next_state=[-0.00728755 -0.33594831  0.01414193  0.43835277]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 13 ] state=[-0.00728755 -0.33594831  0.01414193  0.43835277], action=1, reward=1.0, next_state=[-0.01400651 -0.14102935  0.02290898  0.15016116]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 14 ] state=[-0.01400651 -0.14102935  0.02290898  0.15016116], action=1, reward=1.0, next_state=[-0.0168271   0.0537572   0.02591221 -0.13520734]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 15 ] state=[-0.0168271   0.0537572   0.02591221 -0.13520734], action=0, reward=1.0, next_state=[-0.01575196 -0.14172614  0.02320806  0.16553657]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 16 ] state=[-0.01575196 -0.14172614  0.02320806  0.16553657], action=1, reward=1.0, next_state=[-0.01858648  0.05305604  0.02651879 -0.11973546]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 17 ] state=[-0.01858648  0.05305604  0.02651879 -0.11973546], action=1, reward=1.0, next_state=[-0.01752536  0.2477882   0.02412408 -0.40393539]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 18 ] state=[-0.01752536  0.2477882   0.02412408 -0.40393539], action=0, reward=1.0, next_state=[-0.0125696   0.05233257  0.01604537 -0.10374546]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 215 ][ timestamp 19 ] state=[-0.0125696   0.05233257  0.01604537 -0.10374546], action=1, reward=1.0, next_state=[-0.01152294  0.24722094  0.01397047 -0.39132325]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 20 ] state=[-0.01152294  0.24722094  0.01397047 -0.39132325], action=0, reward=1.0, next_state=[-0.00657853  0.05190354  0.006144   -0.09426855]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 21 ] state=[-0.00657853  0.05190354  0.006144   -0.09426855], action=0, reward=1.0, next_state=[-0.00554045 -0.14330593  0.00425863  0.20034645]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 22 ] state=[-0.00554045 -0.14330593  0.00425863  0.20034645], action=1, reward=1.0, next_state=[-0.00840657  0.05175485  0.00826556 -0.09099004]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 23 ] state=[-0.00840657  0.05175485  0.00826556 -0.09099004], action=0, reward=1.0, next_state=[-0.00737148 -0.14348459  0.00644576  0.20428917]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 24 ] state=[-0.00737148 -0.14348459  0.00644576  0.20428917], action=0, reward=1.0, next_state=[-0.01024117 -0.33869813  0.01053154  0.49899843]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 25 ] state=[-0.01024117 -0.33869813  0.01053154  0.49899843], action=1, reward=1.0, next_state=[-0.01701513 -0.14372623  0.02051151  0.20965301]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 26 ] state=[-0.01701513 -0.14372623  0.02051151  0.20965301], action=1, reward=1.0, next_state=[-0.01988966  0.05109653  0.02470457 -0.07648968]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 27 ] state=[-0.01988966  0.05109653  0.02470457 -0.07648968], action=1, reward=1.0, next_state=[-0.01886772  0.24585577  0.02317478 -0.36127699]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 28 ] state=[-0.01886772  0.24585577  0.02317478 -0.36127699], action=1, reward=1.0, next_state=[-0.01395061  0.44064077  0.01594924 -0.64656327]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 29 ] state=[-0.01395061  0.44064077  0.01594924 -0.64656327], action=1, reward=1.0, next_state=[-0.00513779  0.63553691  0.00301797 -0.93418141]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 30 ] state=[-0.00513779  0.63553691  0.00301797 -0.93418141], action=1, reward=1.0, next_state=[ 0.00757294  0.83061802 -0.01566566 -1.22591446]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 31 ] state=[ 0.00757294  0.83061802 -0.01566566 -1.22591446], action=0, reward=1.0, next_state=[ 0.0241853   0.63570123 -0.04018395 -0.93818067]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 32 ] state=[ 0.0241853   0.63570123 -0.04018395 -0.93818067], action=0, reward=1.0, next_state=[ 0.03689933  0.44114343 -0.05894756 -0.65839047]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 33 ] state=[ 0.03689933  0.44114343 -0.05894756 -0.65839047], action=0, reward=1.0, next_state=[ 0.0457222   0.24688933 -0.07211537 -0.38483597]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 34 ] state=[ 0.0457222   0.24688933 -0.07211537 -0.38483597], action=1, reward=1.0, next_state=[ 0.05065998  0.44295705 -0.07981209 -0.69935667]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 35 ] state=[ 0.05065998  0.44295705 -0.07981209 -0.69935667], action=0, reward=1.0, next_state=[ 0.05951913  0.24902702 -0.09379922 -0.43282748]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 36 ] state=[ 0.05951913  0.24902702 -0.09379922 -0.43282748], action=0, reward=1.0, next_state=[ 0.06449967  0.05534961 -0.10245577 -0.171125  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 37 ] state=[ 0.06449967  0.05534961 -0.10245577 -0.171125  ], action=0, reward=1.0, next_state=[ 0.06560666 -0.13816804 -0.10587827  0.08755994]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 38 ] state=[ 0.06560666 -0.13816804 -0.10587827  0.08755994], action=0, reward=1.0, next_state=[ 0.0628433  -0.33162556 -0.10412707  0.34505113]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 39 ] state=[ 0.0628433  -0.33162556 -0.10412707  0.34505113], action=1, reward=1.0, next_state=[ 0.05621079 -0.1351884  -0.09722605  0.02143327]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 40 ] state=[ 0.05621079 -0.1351884  -0.09722605  0.02143327], action=1, reward=1.0, next_state=[ 0.05350702  0.06118371 -0.09679738 -0.30027321]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 41 ] state=[ 0.05350702  0.06118371 -0.09679738 -0.30027321], action=0, reward=1.0, next_state=[ 0.05473069 -0.13243486 -0.10280285 -0.03961893]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 42 ] state=[ 0.05473069 -0.13243486 -0.10280285 -0.03961893], action=0, reward=1.0, next_state=[ 0.052082   -0.32594387 -0.10359523  0.2189413 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 43 ] state=[ 0.052082   -0.32594387 -0.10359523  0.2189413 ], action=1, reward=1.0, next_state=[ 0.04556312 -0.12950536 -0.0992164  -0.1045393 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 44 ] state=[ 0.04556312 -0.12950536 -0.0992164  -0.1045393 ], action=1, reward=1.0, next_state=[ 0.04297301  0.06688812 -0.10130719 -0.42680255]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 45 ] state=[ 0.04297301  0.06688812 -0.10130719 -0.42680255], action=1, reward=1.0, next_state=[ 0.04431077  0.26328809 -0.10984324 -0.74962542]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 46 ] state=[ 0.04431077  0.26328809 -0.10984324 -0.74962542], action=0, reward=1.0, next_state=[ 0.04957653  0.06983881 -0.12483575 -0.49342931]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 47 ] state=[ 0.04957653  0.06983881 -0.12483575 -0.49342931], action=1, reward=1.0, next_state=[ 0.05097331  0.26647991 -0.13470433 -0.82270205]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 48 ] state=[ 0.05097331  0.26647991 -0.13470433 -0.82270205], action=1, reward=1.0, next_state=[ 0.05630291  0.46316219 -0.15115837 -1.1545363 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 49 ] state=[ 0.05630291  0.46316219 -0.15115837 -1.1545363 ], action=1, reward=1.0, next_state=[ 0.06556615  0.65989686 -0.1742491  -1.49054487]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 50 ] state=[ 0.06556615  0.65989686 -0.1742491  -1.49054487], action=1, reward=1.0, next_state=[ 0.07876409  0.85665901 -0.20406    -1.83218912]\n",
      "[ Experience replay ] starts\n",
      "[ episode 215 ][ timestamp 51 ] state=[ 0.07876409  0.85665901 -0.20406    -1.83218912], action=0, reward=-1.0, next_state=[ 0.09589727  0.66429734 -0.24070378 -1.60921147]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 215: Exploration_rate=0.01. Score=51.\n",
      "[ episode 216 ] state=[ 0.03232636  0.01450196 -0.01931103 -0.04486163]\n",
      "[ episode 216 ][ timestamp 1 ] state=[ 0.03232636  0.01450196 -0.01931103 -0.04486163], action=1, reward=1.0, next_state=[ 0.0326164   0.20989543 -0.02020827 -0.34357426]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 2 ] state=[ 0.0326164   0.20989543 -0.02020827 -0.34357426], action=0, reward=1.0, next_state=[ 0.03681431  0.01506671 -0.02707975 -0.05733168]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 3 ] state=[ 0.03681431  0.01506671 -0.02707975 -0.05733168], action=0, reward=1.0, next_state=[ 0.03711564 -0.17965671 -0.02822638  0.22668589]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 4 ] state=[ 0.03711564 -0.17965671 -0.02822638  0.22668589], action=0, reward=1.0, next_state=[ 0.03352251 -0.37436414 -0.02369267  0.51033323]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 5 ] state=[ 0.03352251 -0.37436414 -0.02369267  0.51033323], action=0, reward=1.0, next_state=[ 0.02603522 -0.56914445 -0.013486    0.7954567 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 6 ] state=[ 0.02603522 -0.56914445 -0.013486    0.7954567 ], action=1, reward=1.0, next_state=[ 0.01465234 -0.37384004  0.00242313  0.49856196]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 7 ] state=[ 0.01465234 -0.37384004  0.00242313  0.49856196], action=1, reward=1.0, next_state=[ 0.00717553 -0.17875233  0.01239437  0.20664366]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 8 ] state=[ 0.00717553 -0.17875233  0.01239437  0.20664366], action=0, reward=1.0, next_state=[ 0.00360049 -0.37404931  0.01652724  0.50321048]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 9 ] state=[ 0.00360049 -0.37404931  0.01652724  0.50321048], action=0, reward=1.0, next_state=[-0.0038805  -0.56940025  0.02659145  0.80105567]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 10 ] state=[-0.0038805  -0.56940025  0.02659145  0.80105567], action=1, reward=1.0, next_state=[-0.0152685  -0.37465291  0.04261257  0.51685489]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 11 ] state=[-0.0152685  -0.37465291  0.04261257  0.51685489], action=1, reward=1.0, next_state=[-0.02276156 -0.18015607  0.05294966  0.23789878]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 216 ][ timestamp 12 ] state=[-0.02276156 -0.18015607  0.05294966  0.23789878], action=1, reward=1.0, next_state=[-0.02636468  0.01417102  0.05770764 -0.0376234 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 13 ] state=[-0.02636468  0.01417102  0.05770764 -0.0376234 ], action=1, reward=1.0, next_state=[-0.02608126  0.20841999  0.05695517 -0.31155521]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 14 ] state=[-0.02608126  0.20841999  0.05695517 -0.31155521], action=0, reward=1.0, next_state=[-0.02191286  0.01253482  0.05072407 -0.00146812]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 15 ] state=[-0.02191286  0.01253482  0.05072407 -0.00146812], action=1, reward=1.0, next_state=[-0.02166217  0.206894    0.05069471 -0.27772544]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 16 ] state=[-0.02166217  0.206894    0.05069471 -0.27772544], action=1, reward=1.0, next_state=[-0.01752429  0.40125746  0.0451402  -0.55399822]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 17 ] state=[-0.01752429  0.40125746  0.0451402  -0.55399822], action=1, reward=1.0, next_state=[-0.00949914  0.59571745  0.03406023 -0.83212436]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 18 ] state=[-0.00949914  0.59571745  0.03406023 -0.83212436], action=0, reward=1.0, next_state=[ 0.00241521  0.400147    0.01741775 -0.52892706]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 19 ] state=[ 0.00241521  0.400147    0.01741775 -0.52892706], action=0, reward=1.0, next_state=[ 0.01041815  0.20478439  0.0068392  -0.23080705]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 20 ] state=[ 0.01041815  0.20478439  0.0068392  -0.23080705], action=0, reward=1.0, next_state=[0.01451384 0.00956539 0.00222306 0.06402533]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 21 ] state=[0.01451384 0.00956539 0.00222306 0.06402533], action=0, reward=1.0, next_state=[ 0.01470515 -0.18558837  0.00350357  0.35740882]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 22 ] state=[ 0.01470515 -0.18558837  0.00350357  0.35740882], action=0, reward=1.0, next_state=[ 0.01099338 -0.38075995  0.01065175  0.65119444]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 23 ] state=[ 0.01099338 -0.38075995  0.01065175  0.65119444], action=0, reward=1.0, next_state=[ 0.00337818 -0.57602863  0.02367563  0.94721239]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 24 ] state=[ 0.00337818 -0.57602863  0.02367563  0.94721239], action=0, reward=1.0, next_state=[-0.00814239 -0.77146126  0.04261988  1.24723917]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 25 ] state=[-0.00814239 -0.77146126  0.04261988  1.24723917], action=0, reward=1.0, next_state=[-0.02357162 -0.96710305  0.06756467  1.55296182]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 26 ] state=[-0.02357162 -0.96710305  0.06756467  1.55296182], action=0, reward=1.0, next_state=[-0.04291368 -1.16296686  0.0986239   1.8659361 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 27 ] state=[-0.04291368 -1.16296686  0.0986239   1.8659361 ], action=1, reward=1.0, next_state=[-0.06617302 -0.96905326  0.13594262  1.60542761]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 28 ] state=[-0.06617302 -0.96905326  0.13594262  1.60542761], action=0, reward=1.0, next_state=[-0.08555408 -1.16549612  0.16805118  1.93721747]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 29 ] state=[-0.08555408 -1.16549612  0.16805118  1.93721747], action=0, reward=1.0, next_state=[-0.108864   -1.36196847  0.20679553  2.27694914]\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ][ timestamp 30 ] state=[-0.108864   -1.36196847  0.20679553  2.27694914], action=0, reward=-1.0, next_state=[-0.13610337 -1.55832808  0.25233451  2.62557855]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 216: Exploration_rate=0.01. Score=30.\n",
      "[ episode 217 ] state=[-0.00467696  0.02125767 -0.04370973  0.04548176]\n",
      "[ episode 217 ][ timestamp 1 ] state=[-0.00467696  0.02125767 -0.04370973  0.04548176], action=0, reward=1.0, next_state=[-0.0042518  -0.17321114 -0.0428001   0.32405979]\n",
      "[ Experience replay ] starts\n",
      "[ episode 217 ][ timestamp 2 ] state=[-0.0042518  -0.17321114 -0.0428001   0.32405979], action=0, reward=1.0, next_state=[-0.00771603 -0.36769836 -0.0363189   0.60294407]\n",
      "[ Experience replay ] starts\n",
      "[ episode 217 ][ timestamp 3 ] state=[-0.00771603 -0.36769836 -0.0363189   0.60294407], action=1, reward=1.0, next_state=[-0.01506999 -0.17208775 -0.02426002  0.29904624]\n",
      "[ Experience replay ] starts\n",
      "[ episode 217 ][ timestamp 4 ] state=[-0.01506999 -0.17208775 -0.02426002  0.29904624], action=0, reward=1.0, next_state=[-0.01851175 -0.36685564 -0.01827909  0.58398037]\n",
      "[ Experience replay ] starts\n",
      "[ episode 217 ][ timestamp 5 ] state=[-0.01851175 -0.36685564 -0.01827909  0.58398037], action=1, reward=1.0, next_state=[-0.02584886 -0.17148246 -0.00659949  0.28559579]\n",
      "[ Experience replay ] starts\n",
      "[ episode 217 ][ timestamp 6 ] state=[-0.02584886 -0.17148246 -0.00659949  0.28559579], action=1, reward=1.0, next_state=[-0.02927851  0.02373299 -0.00088757 -0.00916124]\n",
      "[ Experience replay ] starts\n",
      "[ episode 217 ][ timestamp 7 ] state=[-0.02927851  0.02373299 -0.00088757 -0.00916124], action=1, reward=1.0, next_state=[-0.02880385  0.21886766 -0.0010708  -0.30212407]\n",
      "[ Experience replay ] starts\n",
      "[ episode 217 ][ timestamp 8 ] state=[-0.02880385  0.21886766 -0.0010708  -0.30212407], action=1, reward=1.0, next_state=[-0.0244265   0.41400486 -0.00711328 -0.59514451]\n",
      "[ Experience replay ] starts\n",
      "[ episode 217 ][ timestamp 9 ] state=[-0.0244265   0.41400486 -0.00711328 -0.59514451], action=0, reward=1.0, next_state=[-0.0161464   0.21898318 -0.01901617 -0.30471068]\n",
      "[ Experience replay ] starts\n",
      "[ episode 217 ][ timestamp 10 ] state=[-0.0161464   0.21898318 -0.01901617 -0.30471068], action=1, reward=1.0, next_state=[-0.01176674  0.41437089 -0.02511038 -0.60332968]\n",
      "[ Experience replay ] starts\n",
      "[ episode 217 ][ timestamp 11 ] state=[-0.01176674  0.41437089 -0.02511038 -0.60332968], action=0, reward=1.0, next_state=[-0.00347932  0.21960898 -0.03717697 -0.31866058]\n",
      "[ Experience replay ] starts\n",
      "[ episode 217 ][ timestamp 12 ] state=[-0.00347932  0.21960898 -0.03717697 -0.31866058], action=1, reward=1.0, next_state=[ 0.00091286  0.41524015 -0.04355019 -0.62283209]\n",
      "[ Experience replay ] starts\n",
      "[ episode 217 ][ timestamp 13 ] state=[ 0.00091286  0.41524015 -0.04355019 -0.62283209], action=0, reward=1.0, next_state=[ 0.00921767  0.22075248 -0.05600683 -0.3441769 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 217 ][ timestamp 14 ] state=[ 0.00921767  0.22075248 -0.05600683 -0.3441769 ], action=1, reward=1.0, next_state=[ 0.01363271  0.4166246  -0.06289037 -0.65398178]\n",
      "[ Experience replay ] starts\n",
      "[ episode 217 ][ timestamp 15 ] state=[ 0.01363271  0.4166246  -0.06289037 -0.65398178], action=0, reward=1.0, next_state=[ 0.02196521  0.22243211 -0.07597    -0.3817465 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 217 ][ timestamp 16 ] state=[ 0.02196521  0.22243211 -0.07597    -0.3817465 ], action=1, reward=1.0, next_state=[ 0.02641385  0.41854595 -0.08360493 -0.69738247]\n",
      "[ Experience replay ] starts\n",
      "[ episode 217 ][ timestamp 17 ] state=[ 0.02641385  0.41854595 -0.08360493 -0.69738247], action=0, reward=1.0, next_state=[ 0.03478477  0.22467673 -0.09755258 -0.43214561]\n",
      "[ Experience replay ] starts\n",
      "[ episode 217 ][ timestamp 18 ] state=[ 0.03478477  0.22467673 -0.09755258 -0.43214561], action=1, reward=1.0, next_state=[ 0.0392783   0.42103483 -0.10619549 -0.75391737]\n",
      "[ Experience replay ] starts\n",
      "[ episode 217 ][ timestamp 19 ] state=[ 0.0392783   0.42103483 -0.10619549 -0.75391737], action=0, reward=1.0, next_state=[ 0.047699    0.22752476 -0.12127384 -0.49645028]\n",
      "[ Experience replay ] starts\n",
      "[ episode 217 ][ timestamp 20 ] state=[ 0.047699    0.22752476 -0.12127384 -0.49645028], action=0, reward=1.0, next_state=[ 0.05224949  0.03430287 -0.13120285 -0.24431336]\n",
      "[ Experience replay ] starts\n",
      "[ episode 217 ][ timestamp 21 ] state=[ 0.05224949  0.03430287 -0.13120285 -0.24431336], action=1, reward=1.0, next_state=[ 0.05293555  0.23103095 -0.13608911 -0.5753323 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 217 ][ timestamp 22 ] state=[ 0.05293555  0.23103095 -0.13608911 -0.5753323 ], action=1, reward=1.0, next_state=[ 0.05755617  0.42777183 -0.14759576 -0.90760188]\n",
      "[ Experience replay ] starts\n",
      "[ episode 217 ][ timestamp 23 ] state=[ 0.05755617  0.42777183 -0.14759576 -0.90760188], action=0, reward=1.0, next_state=[ 0.06611161  0.234923   -0.1657478  -0.66470952]\n",
      "[ Experience replay ] starts\n",
      "[ episode 217 ][ timestamp 24 ] state=[ 0.06611161  0.234923   -0.1657478  -0.66470952], action=1, reward=1.0, next_state=[ 0.07081007  0.43191522 -0.17904199 -1.00465531]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 217 ][ timestamp 25 ] state=[ 0.07081007  0.43191522 -0.17904199 -1.00465531], action=0, reward=1.0, next_state=[ 0.07944837  0.23957745 -0.19913509 -0.77311806]\n",
      "[ Experience replay ] starts\n",
      "[ episode 217 ][ timestamp 26 ] state=[ 0.07944837  0.23957745 -0.19913509 -0.77311806], action=1, reward=-1.0, next_state=[ 0.08423992  0.4368003  -0.21459745 -1.12126561]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 217: Exploration_rate=0.01. Score=26.\n",
      "[ episode 218 ] state=[-0.02431798 -0.00383046 -0.01255144  0.03690203]\n",
      "[ episode 218 ][ timestamp 1 ] state=[-0.02431798 -0.00383046 -0.01255144  0.03690203], action=1, reward=1.0, next_state=[-0.02439459  0.19146922 -0.0118134  -0.25971443]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 2 ] state=[-0.02439459  0.19146922 -0.0118134  -0.25971443], action=1, reward=1.0, next_state=[-0.02056521  0.3867578  -0.01700769 -0.55609993]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 3 ] state=[-0.02056521  0.3867578  -0.01700769 -0.55609993], action=0, reward=1.0, next_state=[-0.01283005  0.19187871 -0.02812969 -0.26882359]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 4 ] state=[-0.01283005  0.19187871 -0.02812969 -0.26882359], action=0, reward=1.0, next_state=[-0.00899248 -0.00283074 -0.03350616  0.01485601]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 5 ] state=[-0.00899248 -0.00283074 -0.03350616  0.01485601], action=1, reward=1.0, next_state=[-0.00904909  0.19275531 -0.03320904 -0.28820737]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 6 ] state=[-0.00904909  0.19275531 -0.03320904 -0.28820737], action=0, reward=1.0, next_state=[-0.00519399 -0.00187772 -0.03897319 -0.00618047]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 7 ] state=[-0.00519399 -0.00187772 -0.03897319 -0.00618047], action=1, reward=1.0, next_state=[-0.00523154  0.19378086 -0.0390968  -0.31090069]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 8 ] state=[-0.00523154  0.19378086 -0.0390968  -0.31090069], action=0, reward=1.0, next_state=[-0.00135592 -0.00076289 -0.04531481 -0.03079959]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 9 ] state=[-0.00135592 -0.00076289 -0.04531481 -0.03079959], action=0, reward=1.0, next_state=[-0.00137118 -0.1952067  -0.0459308   0.24724872]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 10 ] state=[-0.00137118 -0.1952067  -0.0459308   0.24724872], action=0, reward=1.0, next_state=[-0.00527532 -0.38964361 -0.04098583  0.52509758]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 11 ] state=[-0.00527532 -0.38964361 -0.04098583  0.52509758], action=1, reward=1.0, next_state=[-0.01306819 -0.19396958 -0.03048388  0.21978656]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 12 ] state=[-0.01306819 -0.19396958 -0.03048388  0.21978656], action=0, reward=1.0, next_state=[-0.01694758 -0.38864282 -0.02608815  0.50269989]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 13 ] state=[-0.01694758 -0.38864282 -0.02608815  0.50269989], action=0, reward=1.0, next_state=[-0.02472044 -0.58338753 -0.01603415  0.78704851]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 14 ] state=[-0.02472044 -0.58338753 -0.01603415  0.78704851], action=0, reward=1.0, next_state=[-3.63881865e-02 -7.78285593e-01 -2.93179725e-04  1.07464419e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 15 ] state=[-3.63881865e-02 -7.78285593e-01 -2.93179725e-04  1.07464419e+00], action=1, reward=1.0, next_state=[-0.0519539  -0.58315977  0.0211997   0.78186927]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 16 ] state=[-0.0519539  -0.58315977  0.0211997   0.78186927], action=1, reward=1.0, next_state=[-0.06361709 -0.38833552  0.03683709  0.49593081]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 17 ] state=[-0.06361709 -0.38833552  0.03683709  0.49593081], action=1, reward=1.0, next_state=[-0.0713838  -0.19375187  0.04675571  0.21508099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 18 ] state=[-0.0713838  -0.19375187  0.04675571  0.21508099], action=0, reward=1.0, next_state=[-0.07525884 -0.38950999  0.05105733  0.52213844]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 19 ] state=[-0.07525884 -0.38950999  0.05105733  0.52213844], action=0, reward=1.0, next_state=[-0.08304904 -0.58531201  0.06150009  0.83046308]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 20 ] state=[-0.08304904 -0.58531201  0.06150009  0.83046308], action=1, reward=1.0, next_state=[-0.09475528 -0.39108217  0.07810936  0.55773875]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 21 ] state=[-0.09475528 -0.39108217  0.07810936  0.55773875], action=1, reward=1.0, next_state=[-0.10257693 -0.1971386   0.08926413  0.29065119]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 22 ] state=[-0.10257693 -0.1971386   0.08926413  0.29065119], action=1, reward=1.0, next_state=[-0.1065197  -0.00339535  0.09507715  0.02740218]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 23 ] state=[-0.1065197  -0.00339535  0.09507715  0.02740218], action=1, reward=1.0, next_state=[-0.1065876   0.19024364  0.0956252  -0.23383387]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 24 ] state=[-0.1065876   0.19024364  0.0956252  -0.23383387], action=1, reward=1.0, next_state=[-0.10278273  0.38387846  0.09094852 -0.49488816]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 25 ] state=[-0.10278273  0.38387846  0.09094852 -0.49488816], action=0, reward=1.0, next_state=[-0.09510516  0.18759952  0.08105076 -0.17498456]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 26 ] state=[-0.09510516  0.18759952  0.08105076 -0.17498456], action=1, reward=1.0, next_state=[-0.09135317  0.3814736   0.07755107 -0.44103816]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 27 ] state=[-0.09135317  0.3814736   0.07755107 -0.44103816], action=1, reward=1.0, next_state=[-0.0837237   0.57541734  0.0687303  -0.70830223]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 28 ] state=[-0.0837237   0.57541734  0.0687303  -0.70830223], action=0, reward=1.0, next_state=[-0.07221535  0.37941405  0.05456426 -0.39480064]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 29 ] state=[-0.07221535  0.37941405  0.05456426 -0.39480064], action=0, reward=1.0, next_state=[-0.06462707  0.18356204  0.04666825 -0.0854259 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 30 ] state=[-0.06462707  0.18356204  0.04666825 -0.0854259 ], action=0, reward=1.0, next_state=[-0.06095583 -0.01219673  0.04495973  0.22160803]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 31 ] state=[-0.06095583 -0.01219673  0.04495973  0.22160803], action=1, reward=1.0, next_state=[-0.06119977  0.18225472  0.04939189 -0.05656069]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 32 ] state=[-0.06119977  0.18225472  0.04939189 -0.05656069], action=1, reward=1.0, next_state=[-0.05755467  0.37663495  0.04826067 -0.33326015]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 33 ] state=[-0.05755467  0.37663495  0.04826067 -0.33326015], action=1, reward=1.0, next_state=[-0.05002197  0.57103797  0.04159547 -0.61034202]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 34 ] state=[-0.05002197  0.57103797  0.04159547 -0.61034202], action=0, reward=1.0, next_state=[-0.03860121  0.37536004  0.02938863 -0.30485347]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 35 ] state=[-0.03860121  0.37536004  0.02938863 -0.30485347], action=0, reward=1.0, next_state=[-0.03109401  0.17983187  0.02329156 -0.00304885]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 36 ] state=[-0.03109401  0.17983187  0.02329156 -0.00304885], action=1, reward=1.0, next_state=[-0.02749737  0.37461218  0.02323058 -0.28829296]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 37 ] state=[-0.02749737  0.37461218  0.02323058 -0.28829296], action=0, reward=1.0, next_state=[-0.02000513  0.17916679  0.01746473  0.0116252 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 38 ] state=[-0.02000513  0.17916679  0.01746473  0.0116252 ], action=0, reward=1.0, next_state=[-0.0164218  -0.01620122  0.01769723  0.30976689]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 39 ] state=[-0.0164218  -0.01620122  0.01769723  0.30976689], action=0, reward=1.0, next_state=[-0.01674582 -0.21157079  0.02389257  0.60797807]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 40 ] state=[-0.01674582 -0.21157079  0.02389257  0.60797807], action=1, reward=1.0, next_state=[-0.02097724 -0.01679088  0.03605213  0.32291534]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 41 ] state=[-0.02097724 -0.01679088  0.03605213  0.32291534], action=1, reward=1.0, next_state=[-0.02131305  0.17779964  0.04251044  0.04181625]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 218 ][ timestamp 42 ] state=[-0.02131305  0.17779964  0.04251044  0.04181625], action=1, reward=1.0, next_state=[-0.01775706  0.37228704  0.04334676 -0.23715698]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 43 ] state=[-0.01775706  0.37228704  0.04334676 -0.23715698], action=1, reward=1.0, next_state=[-0.01031132  0.56676377  0.03860362 -0.5158581 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 44 ] state=[-0.01031132  0.56676377  0.03860362 -0.5158581 ], action=0, reward=1.0, next_state=[ 0.00102396  0.37112008  0.02828646 -0.21126456]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 45 ] state=[ 0.00102396  0.37112008  0.02828646 -0.21126456], action=1, reward=1.0, next_state=[ 0.00844636  0.56582641  0.02406117 -0.49489212]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 46 ] state=[ 0.00844636  0.56582641  0.02406117 -0.49489212], action=0, reward=1.0, next_state=[ 0.01976289  0.37037355  0.01416333 -0.19472438]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 47 ] state=[ 0.01976289  0.37037355  0.01416333 -0.19472438], action=1, reward=1.0, next_state=[ 0.02717036  0.56529007  0.01026884 -0.48290596]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 48 ] state=[ 0.02717036  0.56529007  0.01026884 -0.48290596], action=1, reward=1.0, next_state=[ 3.84761579e-02  7.60265591e-01  6.10718951e-04 -7.72334836e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 49 ] state=[ 3.84761579e-02  7.60265591e-01  6.10718951e-04 -7.72334836e-01], action=0, reward=1.0, next_state=[ 0.05368147  0.56513524 -0.01483598 -0.47945982]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 50 ] state=[ 0.05368147  0.56513524 -0.01483598 -0.47945982], action=1, reward=1.0, next_state=[ 0.06498417  0.76046346 -0.02442517 -0.77678151]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 51 ] state=[ 0.06498417  0.76046346 -0.02442517 -0.77678151], action=1, reward=1.0, next_state=[ 0.08019344  0.95591266 -0.0399608  -1.07704815]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 52 ] state=[ 0.08019344  0.95591266 -0.0399608  -1.07704815], action=1, reward=1.0, next_state=[ 0.0993117   1.15153903 -0.06150177 -1.38199881]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 53 ] state=[ 0.0993117   1.15153903 -0.06150177 -1.38199881], action=0, reward=1.0, next_state=[ 0.12234248  0.95723607 -0.08914174 -1.10916552]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 54 ] state=[ 0.12234248  0.95723607 -0.08914174 -1.10916552], action=0, reward=1.0, next_state=[ 0.1414872   0.76339134 -0.11132505 -0.84572589]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 55 ] state=[ 0.1414872   0.76339134 -0.11132505 -0.84572589], action=1, reward=1.0, next_state=[ 0.15675503  0.95984176 -0.12823957 -1.17123941]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 56 ] state=[ 0.15675503  0.95984176 -0.12823957 -1.17123941], action=0, reward=1.0, next_state=[ 0.17595186  0.76659896 -0.15166436 -0.92135459]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 57 ] state=[ 0.17595186  0.76659896 -0.15166436 -0.92135459], action=0, reward=1.0, next_state=[ 0.19128384  0.57381576 -0.17009145 -0.67991781]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 58 ] state=[ 0.19128384  0.57381576 -0.17009145 -0.67991781], action=1, reward=1.0, next_state=[ 0.20276016  0.77084087 -0.18368981 -1.02095677]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 59 ] state=[ 0.20276016  0.77084087 -0.18368981 -1.02095677], action=1, reward=1.0, next_state=[ 0.21817697  0.96787158 -0.20410894 -1.3652323 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ][ timestamp 60 ] state=[ 0.21817697  0.96787158 -0.20410894 -1.3652323 ], action=1, reward=-1.0, next_state=[ 0.2375344   1.1648793  -0.23141359 -1.71420188]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 218: Exploration_rate=0.01. Score=60.\n",
      "[ episode 219 ] state=[ 0.03384541 -0.0436589  -0.04551415  0.00712714]\n",
      "[ episode 219 ][ timestamp 1 ] state=[ 0.03384541 -0.0436589  -0.04551415  0.00712714], action=1, reward=1.0, next_state=[ 0.03297223  0.15208523 -0.04537161 -0.29956155]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 2 ] state=[ 0.03297223  0.15208523 -0.04537161 -0.29956155], action=0, reward=1.0, next_state=[ 0.03601394 -0.04236162 -0.05136284 -0.02152611]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 3 ] state=[ 0.03601394 -0.04236162 -0.05136284 -0.02152611], action=1, reward=1.0, next_state=[ 0.0351667   0.15345787 -0.05179336 -0.32996201]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 4 ] state=[ 0.0351667   0.15345787 -0.05179336 -0.32996201], action=0, reward=1.0, next_state=[ 0.03823586 -0.04089    -0.0583926  -0.05405157]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 5 ] state=[ 0.03823586 -0.04089    -0.0583926  -0.05405157], action=0, reward=1.0, next_state=[ 0.03741806 -0.23512818 -0.05947364  0.21965145]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 6 ] state=[ 0.03741806 -0.23512818 -0.05947364  0.21965145], action=0, reward=1.0, next_state=[ 0.0327155  -0.42935179 -0.05508061  0.49299684]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 7 ] state=[ 0.0327155  -0.42935179 -0.05508061  0.49299684], action=1, reward=1.0, next_state=[ 0.02412846 -0.23349799 -0.04522067  0.18347615]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 8 ] state=[ 0.02412846 -0.23349799 -0.04522067  0.18347615], action=0, reward=1.0, next_state=[ 0.0194585  -0.42794471 -0.04155115  0.46155771]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 9 ] state=[ 0.0194585  -0.42794471 -0.04155115  0.46155771], action=1, reward=1.0, next_state=[ 0.01089961 -0.23226088 -0.03231999  0.15607279]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 10 ] state=[ 0.01089961 -0.23226088 -0.03231999  0.15607279], action=0, reward=1.0, next_state=[ 0.00625439 -0.42690553 -0.02919854  0.43838687]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 11 ] state=[ 0.00625439 -0.42690553 -0.02919854  0.43838687], action=0, reward=1.0, next_state=[-0.00228372 -0.6216023  -0.0204308   0.7217244 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 12 ] state=[-0.00228372 -0.6216023  -0.0204308   0.7217244 ], action=1, reward=1.0, next_state=[-0.01471577 -0.42620377 -0.00599631  0.42268154]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 13 ] state=[-0.01471577 -0.42620377 -0.00599631  0.42268154], action=1, reward=1.0, next_state=[-0.02323984 -0.23099739  0.00245732  0.12811432]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 14 ] state=[-0.02323984 -0.23099739  0.00245732  0.12811432], action=0, reward=1.0, next_state=[-0.02785979 -0.42615445  0.00501961  0.42157148]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 15 ] state=[-0.02785979 -0.42615445  0.00501961  0.42157148], action=1, reward=1.0, next_state=[-0.03638288 -0.23110398  0.01345104  0.13047522]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 16 ] state=[-0.03638288 -0.23110398  0.01345104  0.13047522], action=0, reward=1.0, next_state=[-0.04100496 -0.42641601  0.01606054  0.42737124]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 17 ] state=[-0.04100496 -0.42641601  0.01606054  0.42737124], action=0, reward=1.0, next_state=[-0.04953328 -0.6217617   0.02460797  0.72507359]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 18 ] state=[-0.04953328 -0.6217617   0.02460797  0.72507359], action=0, reward=1.0, next_state=[-0.06196851 -0.81721514  0.03910944  1.025399  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 19 ] state=[-0.06196851 -0.81721514  0.03910944  1.025399  ], action=1, reward=1.0, next_state=[-0.07831282 -0.62263517  0.05961742  0.74524746]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 20 ] state=[-0.07831282 -0.62263517  0.05961742  0.74524746], action=1, reward=1.0, next_state=[-0.09076552 -0.42838436  0.07452237  0.47190606]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 21 ] state=[-0.09076552 -0.42838436  0.07452237  0.47190606], action=1, reward=1.0, next_state=[-0.09933321 -0.23438973  0.08396049  0.20361106]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 22 ] state=[-0.09933321 -0.23438973  0.08396049  0.20361106], action=1, reward=1.0, next_state=[-0.104021   -0.0405626   0.08803271 -0.06145009]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 23 ] state=[-0.104021   -0.0405626   0.08803271 -0.06145009], action=1, reward=1.0, next_state=[-0.10483225  0.15319409  0.08680371 -0.32511146]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 24 ] state=[-0.10483225  0.15319409  0.08680371 -0.32511146], action=1, reward=1.0, next_state=[-0.10176837  0.34697976  0.08030148 -0.58920729]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 25 ] state=[-0.10176837  0.34697976  0.08030148 -0.58920729], action=1, reward=1.0, next_state=[-0.09482878  0.54089088  0.06851733 -0.85555341]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 26 ] state=[-0.09482878  0.54089088  0.06851733 -0.85555341], action=1, reward=1.0, next_state=[-0.08401096  0.73501558  0.05140626 -1.12592888]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 219 ][ timestamp 27 ] state=[-0.08401096  0.73501558  0.05140626 -1.12592888], action=1, reward=1.0, next_state=[-0.06931065  0.92942758  0.02888769 -1.40205486]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 28 ] state=[-0.06931065  0.92942758  0.02888769 -1.40205486], action=1, reward=1.0, next_state=[-5.07220951e-02  1.12417898e+00  8.46588413e-04 -1.68556828e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 29 ] state=[-5.07220951e-02  1.12417898e+00  8.46588413e-04 -1.68556828e+00], action=1, reward=1.0, next_state=[-0.02823852  1.31929112 -0.03286478 -1.97798749]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 30 ] state=[-0.02823852  1.31929112 -0.03286478 -1.97798749], action=1, reward=1.0, next_state=[-1.85269305e-03  1.51474320e+00 -7.24245270e-02 -2.28066780e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 31 ] state=[-1.85269305e-03  1.51474320e+00 -7.24245270e-02 -2.28066780e+00], action=1, reward=1.0, next_state=[ 0.02844217  1.71045801 -0.11803788 -2.59474461]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 32 ] state=[ 0.02844217  1.71045801 -0.11803788 -2.59474461], action=0, reward=1.0, next_state=[ 0.06265133  1.51643657 -0.16993278 -2.34036017]\n",
      "[ Experience replay ] starts\n",
      "[ episode 219 ][ timestamp 33 ] state=[ 0.06265133  1.51643657 -0.16993278 -2.34036017], action=1, reward=-1.0, next_state=[ 0.09298006  1.71263467 -0.21673998 -2.68013844]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 219: Exploration_rate=0.01. Score=33.\n",
      "[ episode 220 ] state=[-0.02114179  0.02686959 -0.03888854  0.04580384]\n",
      "[ episode 220 ][ timestamp 1 ] state=[-0.02114179  0.02686959 -0.03888854  0.04580384], action=1, reward=1.0, next_state=[-0.02060439  0.22252698 -0.03797246 -0.25889069]\n",
      "[ Experience replay ] starts\n",
      "[ episode 220 ][ timestamp 2 ] state=[-0.02060439  0.22252698 -0.03797246 -0.25889069], action=1, reward=1.0, next_state=[-0.01615385  0.41816987 -0.04315027 -0.56330471]\n",
      "[ Experience replay ] starts\n",
      "[ episode 220 ][ timestamp 3 ] state=[-0.01615385  0.41816987 -0.04315027 -0.56330471], action=1, reward=1.0, next_state=[-0.00779046  0.61386989 -0.05441637 -0.86926374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 220 ][ timestamp 4 ] state=[-0.00779046  0.61386989 -0.05441637 -0.86926374], action=0, reward=1.0, next_state=[ 0.00448694  0.41952878 -0.07180164 -0.59417409]\n",
      "[ Experience replay ] starts\n",
      "[ episode 220 ][ timestamp 5 ] state=[ 0.00448694  0.41952878 -0.07180164 -0.59417409], action=1, reward=1.0, next_state=[ 0.01287752  0.6155784  -0.08368512 -0.90858234]\n",
      "[ Experience replay ] starts\n",
      "[ episode 220 ][ timestamp 6 ] state=[ 0.01287752  0.6155784  -0.08368512 -0.90858234], action=1, reward=1.0, next_state=[ 0.02518908  0.81172733 -0.10185677 -1.2263508 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 220 ][ timestamp 7 ] state=[ 0.02518908  0.81172733 -0.10185677 -1.2263508 ], action=0, reward=1.0, next_state=[ 0.04142363  0.61805339 -0.12638379 -0.96723973]\n",
      "[ Experience replay ] starts\n",
      "[ episode 220 ][ timestamp 8 ] state=[ 0.04142363  0.61805339 -0.12638379 -0.96723973], action=0, reward=1.0, next_state=[ 0.0537847   0.42483431 -0.14572858 -0.71678071]\n",
      "[ Experience replay ] starts\n",
      "[ episode 220 ][ timestamp 9 ] state=[ 0.0537847   0.42483431 -0.14572858 -0.71678071], action=0, reward=1.0, next_state=[ 0.06228138  0.2319976  -0.1600642  -0.47328437]\n",
      "[ Experience replay ] starts\n",
      "[ episode 220 ][ timestamp 10 ] state=[ 0.06228138  0.2319976  -0.1600642  -0.47328437], action=1, reward=1.0, next_state=[ 0.06692134  0.42897514 -0.16952988 -0.81183194]\n",
      "[ Experience replay ] starts\n",
      "[ episode 220 ][ timestamp 11 ] state=[ 0.06692134  0.42897514 -0.16952988 -0.81183194], action=0, reward=1.0, next_state=[ 0.07550084  0.23653053 -0.18576652 -0.57690668]\n",
      "[ Experience replay ] starts\n",
      "[ episode 220 ][ timestamp 12 ] state=[ 0.07550084  0.23653053 -0.18576652 -0.57690668], action=0, reward=1.0, next_state=[ 0.08023145  0.04443141 -0.19730466 -0.34801738]\n",
      "[ Experience replay ] starts\n",
      "[ episode 220 ][ timestamp 13 ] state=[ 0.08023145  0.04443141 -0.19730466 -0.34801738], action=0, reward=1.0, next_state=[ 0.08112008 -0.14741751 -0.204265   -0.12345917]\n",
      "[ Experience replay ] starts\n",
      "[ episode 220 ][ timestamp 14 ] state=[ 0.08112008 -0.14741751 -0.204265   -0.12345917], action=0, reward=1.0, next_state=[ 0.07817173 -0.33911667 -0.20673419  0.09847436]\n",
      "[ Experience replay ] starts\n",
      "[ episode 220 ][ timestamp 15 ] state=[ 0.07817173 -0.33911667 -0.20673419  0.09847436], action=1, reward=1.0, next_state=[ 0.07138939 -0.1417241  -0.2047647  -0.25165756]\n",
      "[ Experience replay ] starts\n",
      "[ episode 220 ][ timestamp 16 ] state=[ 0.07138939 -0.1417241  -0.2047647  -0.25165756], action=0, reward=-1.0, next_state=[ 0.06855491 -0.33342344 -0.20979785 -0.02989679]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 220: Exploration_rate=0.01. Score=16.\n",
      "[ episode 221 ] state=[-0.04764164 -0.00310302 -0.02114421 -0.03046382]\n",
      "[ episode 221 ][ timestamp 1 ] state=[-0.04764164 -0.00310302 -0.02114421 -0.03046382], action=1, reward=1.0, next_state=[-0.0477037   0.19231567 -0.02175349 -0.32974226]\n",
      "[ Experience replay ] starts\n",
      "[ episode 221 ][ timestamp 2 ] state=[-0.0477037   0.19231567 -0.02175349 -0.32974226], action=1, reward=1.0, next_state=[-0.04385739  0.38774043 -0.02834833 -0.62920506]\n",
      "[ Experience replay ] starts\n",
      "[ episode 221 ][ timestamp 3 ] state=[-0.04385739  0.38774043 -0.02834833 -0.62920506], action=1, reward=1.0, next_state=[-0.03610258  0.58324628 -0.04093243 -0.9306793 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 221 ][ timestamp 4 ] state=[-0.03610258  0.58324628 -0.04093243 -0.9306793 ], action=0, reward=1.0, next_state=[-0.02443765  0.38869996 -0.05954602 -0.65113503]\n",
      "[ Experience replay ] starts\n",
      "[ episode 221 ][ timestamp 5 ] state=[-0.02443765  0.38869996 -0.05954602 -0.65113503], action=0, reward=1.0, next_state=[-0.01666366  0.19445569 -0.07256872 -0.37778121]\n",
      "[ Experience replay ] starts\n",
      "[ episode 221 ][ timestamp 6 ] state=[-0.01666366  0.19445569 -0.07256872 -0.37778121], action=1, reward=1.0, next_state=[-0.01277454  0.39052921 -0.08012434 -0.6924339 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 221 ][ timestamp 7 ] state=[-0.01277454  0.39052921 -0.08012434 -0.6924339 ], action=0, reward=1.0, next_state=[-0.00496396  0.19660491 -0.09397302 -0.42601204]\n",
      "[ Experience replay ] starts\n",
      "[ episode 221 ][ timestamp 8 ] state=[-0.00496396  0.19660491 -0.09397302 -0.42601204], action=0, reward=1.0, next_state=[-0.00103186  0.00293091 -0.10249326 -0.16437026]\n",
      "[ Experience replay ] starts\n",
      "[ episode 221 ][ timestamp 9 ] state=[-0.00103186  0.00293091 -0.10249326 -0.16437026], action=1, reward=1.0, next_state=[-0.00097324  0.19935934 -0.10578067 -0.48754696]\n",
      "[ Experience replay ] starts\n",
      "[ episode 221 ][ timestamp 10 ] state=[-0.00097324  0.19935934 -0.10578067 -0.48754696], action=0, reward=1.0, next_state=[ 0.00301395  0.00587649 -0.11553161 -0.22998646]\n",
      "[ Experience replay ] starts\n",
      "[ episode 221 ][ timestamp 11 ] state=[ 0.00301395  0.00587649 -0.11553161 -0.22998646], action=0, reward=1.0, next_state=[ 0.00313148 -0.18742128 -0.12013134  0.02413652]\n",
      "[ Experience replay ] starts\n",
      "[ episode 221 ][ timestamp 12 ] state=[ 0.00313148 -0.18742128 -0.12013134  0.02413652], action=0, reward=1.0, next_state=[-0.00061695 -0.38063392 -0.11964861  0.27663301]\n",
      "[ Experience replay ] starts\n",
      "[ episode 221 ][ timestamp 13 ] state=[-0.00061695 -0.38063392 -0.11964861  0.27663301], action=1, reward=1.0, next_state=[-0.00822963 -0.18402624 -0.11411595 -0.0512629 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 221 ][ timestamp 14 ] state=[-0.00822963 -0.18402624 -0.11411595 -0.0512629 ], action=1, reward=1.0, next_state=[-0.01191015  0.01253136 -0.1151412  -0.37765895]\n",
      "[ Experience replay ] starts\n",
      "[ episode 221 ][ timestamp 15 ] state=[-0.01191015  0.01253136 -0.1151412  -0.37765895], action=0, reward=1.0, next_state=[-0.01165953 -0.18078314 -0.12269438 -0.123384  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 221 ][ timestamp 16 ] state=[-0.01165953 -0.18078314 -0.12269438 -0.123384  ], action=1, reward=1.0, next_state=[-0.01527519  0.01586348 -0.12516206 -0.4521182 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 221 ][ timestamp 17 ] state=[-0.01527519  0.01586348 -0.12516206 -0.4521182 ], action=1, reward=1.0, next_state=[-0.01495792  0.21251262 -0.13420443 -0.78148611]\n",
      "[ Experience replay ] starts\n",
      "[ episode 221 ][ timestamp 18 ] state=[-0.01495792  0.21251262 -0.13420443 -0.78148611], action=1, reward=1.0, next_state=[-0.01070767  0.40919885 -0.14983415 -1.11320035]\n",
      "[ Experience replay ] starts\n",
      "[ episode 221 ][ timestamp 19 ] state=[-0.01070767  0.40919885 -0.14983415 -1.11320035], action=1, reward=1.0, next_state=[-0.00252369  0.60593639 -0.17209816 -1.44888684]\n",
      "[ Experience replay ] starts\n",
      "[ episode 221 ][ timestamp 20 ] state=[-0.00252369  0.60593639 -0.17209816 -1.44888684], action=0, reward=1.0, next_state=[ 0.00959504  0.41329677 -0.20107589 -1.21454349]\n",
      "[ Experience replay ] starts\n",
      "[ episode 221 ][ timestamp 21 ] state=[ 0.00959504  0.41329677 -0.20107589 -1.21454349], action=1, reward=-1.0, next_state=[ 0.01786097  0.61036273 -0.22536676 -1.56290554]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 221: Exploration_rate=0.01. Score=21.\n",
      "[ episode 222 ] state=[ 0.01692828 -0.01809465 -0.03060311  0.00055529]\n",
      "[ episode 222 ][ timestamp 1 ] state=[ 0.01692828 -0.01809465 -0.03060311  0.00055529], action=0, reward=1.0, next_state=[ 0.01656639 -0.21276465 -0.030592    0.28342765]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 222 ][ timestamp 2 ] state=[ 0.01656639 -0.21276465 -0.030592    0.28342765], action=1, reward=1.0, next_state=[ 0.01231109 -0.01722002 -0.02492345 -0.0187447 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 3 ] state=[ 0.01231109 -0.01722002 -0.02492345 -0.0187447 ], action=0, reward=1.0, next_state=[ 0.01196669 -0.21197584 -0.02529834  0.26597157]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 4 ] state=[ 0.01196669 -0.21197584 -0.02529834  0.26597157], action=1, reward=1.0, next_state=[ 0.00772718 -0.01650212 -0.01997891 -0.0345821 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 5 ] state=[ 0.00772718 -0.01650212 -0.01997891 -0.0345821 ], action=0, reward=1.0, next_state=[ 0.00739713 -0.21133196 -0.02067055  0.25173092]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 6 ] state=[ 0.00739713 -0.21133196 -0.02067055  0.25173092], action=1, reward=1.0, next_state=[ 0.00317049 -0.01592103 -0.01563593 -0.04739956]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 7 ] state=[ 0.00317049 -0.01592103 -0.01563593 -0.04739956], action=1, reward=1.0, next_state=[ 0.00285207  0.1794216  -0.01658392 -0.34497447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 8 ] state=[ 0.00285207  0.1794216  -0.01658392 -0.34497447], action=0, reward=1.0, next_state=[ 0.00644051 -0.01546056 -0.02348341 -0.05756687]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 9 ] state=[ 0.00644051 -0.01546056 -0.02348341 -0.05756687], action=0, reward=1.0, next_state=[ 0.00613129 -0.21023807 -0.02463475  0.22761534]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 10 ] state=[ 0.00613129 -0.21023807 -0.02463475  0.22761534], action=0, reward=1.0, next_state=[ 0.00192653 -0.40499946 -0.02008244  0.51242691]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 11 ] state=[ 0.00192653 -0.40499946 -0.02008244  0.51242691], action=1, reward=1.0, next_state=[-0.00617346 -0.20960049 -0.00983391  0.2134837 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 12 ] state=[-0.00617346 -0.20960049 -0.00983391  0.2134837 ], action=1, reward=1.0, next_state=[-0.01036547 -0.01433933 -0.00556423 -0.08228499]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 13 ] state=[-0.01036547 -0.01433933 -0.00556423 -0.08228499], action=1, reward=1.0, next_state=[-0.01065225  0.18086194 -0.00720993 -0.37671824]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 14 ] state=[-0.01065225  0.18086194 -0.00720993 -0.37671824], action=0, reward=1.0, next_state=[-0.00703501 -0.01415687 -0.0147443  -0.08631733]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 15 ] state=[-0.00703501 -0.01415687 -0.0147443  -0.08631733], action=1, reward=1.0, next_state=[-0.00731815  0.18117329 -0.01647064 -0.38361539]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 16 ] state=[-0.00731815  0.18117329 -0.01647064 -0.38361539], action=0, reward=1.0, next_state=[-0.00369469 -0.01371099 -0.02414295 -0.09617077]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 17 ] state=[-0.00369469 -0.01371099 -0.02414295 -0.09617077], action=1, reward=1.0, next_state=[-0.00396891  0.18174852 -0.02606637 -0.39637193]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 18 ] state=[-0.00396891  0.18174852 -0.02606637 -0.39637193], action=0, reward=1.0, next_state=[-0.00033394 -0.01299408 -0.03399381 -0.1120199 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 19 ] state=[-0.00033394 -0.01299408 -0.03399381 -0.1120199 ], action=1, reward=1.0, next_state=[-0.00059382  0.18259806 -0.0362342  -0.41523088]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 20 ] state=[-0.00059382  0.18259806 -0.0362342  -0.41523088], action=1, reward=1.0, next_state=[ 0.00305814  0.37821433 -0.04453882 -0.71911321]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 21 ] state=[ 0.00305814  0.37821433 -0.04453882 -0.71911321], action=0, reward=1.0, next_state=[ 0.01062243  0.18373604 -0.05892109 -0.44077514]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 22 ] state=[ 0.01062243  0.18373604 -0.05892109 -0.44077514], action=0, reward=1.0, next_state=[ 0.01429715 -0.01050473 -0.06773659 -0.16723238]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 23 ] state=[ 0.01429715 -0.01050473 -0.06773659 -0.16723238], action=0, reward=1.0, next_state=[ 0.01408706 -0.20459499 -0.07108124  0.10333604]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 24 ] state=[ 0.01408706 -0.20459499 -0.07108124  0.10333604], action=0, reward=1.0, next_state=[ 0.00999516 -0.39863009 -0.06901452  0.37277343]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 25 ] state=[ 0.00999516 -0.39863009 -0.06901452  0.37277343], action=0, reward=1.0, next_state=[ 0.00202256 -0.59270723 -0.06155905  0.64292196]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 26 ] state=[ 0.00202256 -0.59270723 -0.06155905  0.64292196], action=1, reward=1.0, next_state=[-0.00983159 -0.39678373 -0.04870061  0.33150644]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 27 ] state=[-0.00983159 -0.39678373 -0.04870061  0.33150644], action=0, reward=1.0, next_state=[-0.01776726 -0.59117986 -0.04207048  0.60844258]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 28 ] state=[-0.01776726 -0.59117986 -0.04207048  0.60844258], action=1, reward=1.0, next_state=[-0.02959086 -0.39549578 -0.02990163  0.30281112]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 29 ] state=[-0.02959086 -0.39549578 -0.02990163  0.30281112], action=1, reward=1.0, next_state=[-0.03750078 -0.19996072 -0.0238454   0.00084986]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 30 ] state=[-0.03750078 -0.19996072 -0.0238454   0.00084986], action=1, reward=1.0, next_state=[-0.04149999 -0.00450505 -0.02382841 -0.29926018]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 31 ] state=[-0.04149999 -0.00450505 -0.02382841 -0.29926018], action=1, reward=1.0, next_state=[-0.04159009  0.19094831 -0.02981361 -0.59936187]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 32 ] state=[-0.04159009  0.19094831 -0.02981361 -0.59936187], action=1, reward=1.0, next_state=[-0.03777113  0.38647442 -0.04180085 -0.9012846 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 33 ] state=[-0.03777113  0.38647442 -0.04180085 -0.9012846 ], action=0, reward=1.0, next_state=[-0.03004164  0.19194299 -0.05982654 -0.62202823]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 34 ] state=[-0.03004164  0.19194299 -0.05982654 -0.62202823], action=1, reward=1.0, next_state=[-0.02620278  0.38784708 -0.0722671  -0.93293714]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 35 ] state=[-0.02620278  0.38784708 -0.0722671  -0.93293714], action=0, reward=1.0, next_state=[-0.01844584  0.19377067 -0.09092585 -0.66381041]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 36 ] state=[-0.01844584  0.19377067 -0.09092585 -0.66381041], action=1, reward=1.0, next_state=[-0.01457042  0.39003202 -0.10420206 -0.98368171]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 37 ] state=[-0.01457042  0.39003202 -0.10420206 -0.98368171], action=0, reward=1.0, next_state=[-0.00676978  0.19644873 -0.12387569 -0.72546181]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 38 ] state=[-0.00676978  0.19644873 -0.12387569 -0.72546181], action=0, reward=1.0, next_state=[-0.00284081  0.00323761 -0.13838493 -0.47419231]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 39 ] state=[-0.00284081  0.00323761 -0.13838493 -0.47419231], action=0, reward=1.0, next_state=[-0.00277606 -0.18968658 -0.14786877 -0.22812796]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 40 ] state=[-0.00277606 -0.18968658 -0.14786877 -0.22812796], action=1, reward=1.0, next_state=[-0.00656979  0.00720505 -0.15243133 -0.56355765]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 41 ] state=[-0.00656979  0.00720505 -0.15243133 -0.56355765], action=0, reward=1.0, next_state=[-0.00642569 -0.18548649 -0.16370248 -0.32251324]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 42 ] state=[-0.00642569 -0.18548649 -0.16370248 -0.32251324], action=1, reward=1.0, next_state=[-0.01013542  0.01154202 -0.17015275 -0.66201866]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 43 ] state=[-0.01013542  0.01154202 -0.17015275 -0.66201866], action=0, reward=1.0, next_state=[-0.00990458 -0.18085526 -0.18339312 -0.42737425]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 44 ] state=[-0.00990458 -0.18085526 -0.18339312 -0.42737425], action=0, reward=1.0, next_state=[-0.01352168 -0.37297064 -0.19194061 -0.19764952]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 45 ] state=[-0.01352168 -0.37297064 -0.19194061 -0.19764952], action=0, reward=1.0, next_state=[-0.02098109 -0.5649032  -0.1958936   0.02887763]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 46 ] state=[-0.02098109 -0.5649032  -0.1958936   0.02887763], action=1, reward=1.0, next_state=[-0.03227916 -0.36759031 -0.19531605 -0.31865612]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 222 ][ timestamp 47 ] state=[-0.03227916 -0.36759031 -0.19531605 -0.31865612], action=1, reward=1.0, next_state=[-0.03963096 -0.1703009  -0.20168917 -0.66602197]\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ][ timestamp 48 ] state=[-0.03963096 -0.1703009  -0.20168917 -0.66602197], action=0, reward=-1.0, next_state=[-0.04303698 -0.36213155 -0.21500961 -0.44300413]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 222: Exploration_rate=0.01. Score=48.\n",
      "[ episode 223 ] state=[ 0.00203269 -0.0416713   0.00070437 -0.01157174]\n",
      "[ episode 223 ][ timestamp 1 ] state=[ 0.00203269 -0.0416713   0.00070437 -0.01157174], action=1, reward=1.0, next_state=[ 0.00119926  0.15344054  0.00047294 -0.30403235]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 2 ] state=[ 0.00119926  0.15344054  0.00047294 -0.30403235], action=1, reward=1.0, next_state=[ 0.00426807  0.34855575 -0.00560771 -0.59656608]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 3 ] state=[ 0.00426807  0.34855575 -0.00560771 -0.59656608], action=0, reward=1.0, next_state=[ 0.01123919  0.15351272 -0.01753903 -0.3056548 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 4 ] state=[ 0.01123919  0.15351272 -0.01753903 -0.3056548 ], action=1, reward=1.0, next_state=[ 0.01430944  0.34888016 -0.02365212 -0.60381709]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 5 ] state=[ 0.01430944  0.34888016 -0.02365212 -0.60381709], action=0, reward=1.0, next_state=[ 0.02128705  0.15409685 -0.03572847 -0.31867692]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 6 ] state=[ 0.02128705  0.15409685 -0.03572847 -0.31867692], action=0, reward=1.0, next_state=[ 0.02436898 -0.04049851 -0.04210201 -0.0374721 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 7 ] state=[ 0.02436898 -0.04049851 -0.04210201 -0.0374721 ], action=1, reward=1.0, next_state=[ 0.02355901  0.15520111 -0.04285145 -0.34313573]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 8 ] state=[ 0.02355901  0.15520111 -0.04285145 -0.34313573], action=0, reward=1.0, next_state=[ 0.02666303 -0.03928585 -0.04971416 -0.06426757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 9 ] state=[ 0.02666303 -0.03928585 -0.04971416 -0.06426757], action=0, reward=1.0, next_state=[ 0.02587732 -0.23366108 -0.05099951  0.21232511]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 10 ] state=[ 0.02587732 -0.23366108 -0.05099951  0.21232511], action=1, reward=1.0, next_state=[ 0.0212041  -0.03784846 -0.04675301 -0.09599928]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 11 ] state=[ 0.0212041  -0.03784846 -0.04675301 -0.09599928], action=1, reward=1.0, next_state=[ 0.02044713  0.15791131 -0.048673   -0.40305846]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 12 ] state=[ 0.02044713  0.15791131 -0.048673   -0.40305846], action=0, reward=1.0, next_state=[ 0.02360535 -0.03648774 -0.05673417 -0.12610943]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 13 ] state=[ 0.02360535 -0.03648774 -0.05673417 -0.12610943], action=1, reward=1.0, next_state=[ 0.0228756   0.15939915 -0.05925635 -0.43613791]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 14 ] state=[ 0.0228756   0.15939915 -0.05925635 -0.43613791], action=0, reward=1.0, next_state=[ 0.02606358 -0.03483611 -0.06797911 -0.16270756]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 15 ] state=[ 0.02606358 -0.03483611 -0.06797911 -0.16270756], action=0, reward=1.0, next_state=[ 0.02536686 -0.22892237 -0.07123326  0.10777895]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 16 ] state=[ 0.02536686 -0.22892237 -0.07123326  0.10777895], action=1, reward=1.0, next_state=[ 0.02078841 -0.03285576 -0.06907768 -0.2065    ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 17 ] state=[ 0.02078841 -0.03285576 -0.06907768 -0.2065    ], action=1, reward=1.0, next_state=[ 0.0201313   0.16318251 -0.07320768 -0.52014879]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 18 ] state=[ 0.0201313   0.16318251 -0.07320768 -0.52014879], action=0, reward=1.0, next_state=[ 0.02339495 -0.03083664 -0.08361066 -0.25140342]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 19 ] state=[ 0.02339495 -0.03083664 -0.08361066 -0.25140342], action=0, reward=1.0, next_state=[ 0.02277821 -0.22467131 -0.08863873  0.01377998]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 20 ] state=[ 0.02277821 -0.22467131 -0.08863873  0.01377998], action=1, reward=1.0, next_state=[ 0.01828479 -0.02839735 -0.08836313 -0.30550082]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 21 ] state=[ 0.01828479 -0.02839735 -0.08836313 -0.30550082], action=1, reward=1.0, next_state=[ 0.01771684  0.1678654  -0.09447315 -0.62469134]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 22 ] state=[ 0.01771684  0.1678654  -0.09447315 -0.62469134], action=0, reward=1.0, next_state=[ 0.02107415 -0.02581952 -0.10696697 -0.36319331]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 23 ] state=[ 0.02107415 -0.02581952 -0.10696697 -0.36319331], action=0, reward=1.0, next_state=[ 0.02055776 -0.21927148 -0.11423084 -0.10606224]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 24 ] state=[ 0.02055776 -0.21927148 -0.11423084 -0.10606224], action=1, reward=1.0, next_state=[ 0.01617233 -0.02271361 -0.11635208 -0.43248839]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 25 ] state=[ 0.01617233 -0.02271361 -0.11635208 -0.43248839], action=1, reward=1.0, next_state=[ 0.01571806  0.17384697 -0.12500185 -0.75946614]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 26 ] state=[ 0.01571806  0.17384697 -0.12500185 -0.75946614], action=0, reward=1.0, next_state=[ 0.019195   -0.01935131 -0.14019117 -0.5085848 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 27 ] state=[ 0.019195   -0.01935131 -0.14019117 -0.5085848 ], action=0, reward=1.0, next_state=[ 0.01880797 -0.21224863 -0.15036287 -0.26315884]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 28 ] state=[ 0.01880797 -0.21224863 -0.15036287 -0.26315884], action=0, reward=1.0, next_state=[ 0.014563   -0.40494026 -0.15562605 -0.02142296]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 29 ] state=[ 0.014563   -0.40494026 -0.15562605 -0.02142296], action=0, reward=1.0, next_state=[ 0.00646419 -0.59752769 -0.15605451  0.21839737]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 30 ] state=[ 0.00646419 -0.59752769 -0.15605451  0.21839737], action=0, reward=1.0, next_state=[-0.00548636 -0.79011458 -0.15168656  0.45807326]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 31 ] state=[-0.00548636 -0.79011458 -0.15168656  0.45807326], action=1, reward=1.0, next_state=[-0.02128865 -0.59321034 -0.14252509  0.12168326]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 32 ] state=[-0.02128865 -0.59321034 -0.14252509  0.12168326], action=0, reward=1.0, next_state=[-0.03315286 -0.78603322 -0.14009143  0.36622423]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 33 ] state=[-0.03315286 -0.78603322 -0.14009143  0.36622423], action=0, reward=1.0, next_state=[-0.04887353 -0.97891534 -0.13276694  0.61166068]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 34 ] state=[-0.04887353 -0.97891534 -0.13276694  0.61166068], action=0, reward=1.0, next_state=[-0.06845183 -1.17195631 -0.12053373  0.85975492]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 35 ] state=[-0.06845183 -1.17195631 -0.12053373  0.85975492], action=0, reward=1.0, next_state=[-0.09189096 -1.36524855 -0.10333863  1.11223849]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 36 ] state=[-0.09189096 -1.36524855 -0.10333863  1.11223849], action=1, reward=1.0, next_state=[-0.11919593 -1.16893245 -0.08109386  0.78900775]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 37 ] state=[-0.11919593 -1.16893245 -0.08109386  0.78900775], action=1, reward=1.0, next_state=[-0.14257458 -0.97279596 -0.06531371  0.47195439]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 38 ] state=[-0.14257458 -0.97279596 -0.06531371  0.47195439], action=1, reward=1.0, next_state=[-0.1620305  -0.77681524 -0.05587462  0.15942153]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 39 ] state=[-0.1620305  -0.77681524 -0.05587462  0.15942153], action=1, reward=1.0, next_state=[-0.1775668  -0.58093972 -0.05268619 -0.15035183]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 40 ] state=[-0.1775668  -0.58093972 -0.05268619 -0.15035183], action=1, reward=1.0, next_state=[-0.1891856  -0.38510447 -0.05569323 -0.45917967]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 41 ] state=[-0.1891856  -0.38510447 -0.05569323 -0.45917967], action=1, reward=1.0, next_state=[-0.19688769 -0.1892413  -0.06487682 -0.76888424]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 223 ][ timestamp 42 ] state=[-0.19688769 -0.1892413  -0.06487682 -0.76888424], action=1, reward=1.0, next_state=[-0.20067251  0.00671081 -0.0802545  -1.08125447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 43 ] state=[-0.20067251  0.00671081 -0.0802545  -1.08125447], action=1, reward=1.0, next_state=[-0.2005383   0.20279514 -0.10187959 -1.39800377]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 44 ] state=[-0.2005383   0.20279514 -0.10187959 -1.39800377], action=0, reward=1.0, next_state=[-0.19648239  0.0090769  -0.12983967 -1.13883393]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 45 ] state=[-0.19648239  0.0090769  -0.12983967 -1.13883393], action=0, reward=1.0, next_state=[-0.19630086 -0.18413065 -0.15261635 -0.88952774]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 46 ] state=[-0.19630086 -0.18413065 -0.15261635 -0.88952774], action=0, reward=1.0, next_state=[-0.19998347 -0.37688908 -0.1704069  -0.64844606]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 47 ] state=[-0.19998347 -0.37688908 -0.1704069  -0.64844606], action=0, reward=1.0, next_state=[-0.20752125 -0.56927891 -0.18337582 -0.41389873]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 48 ] state=[-0.20752125 -0.56927891 -0.18337582 -0.41389873], action=0, reward=1.0, next_state=[-0.21890683 -0.7613926  -0.1916538  -0.18417059]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 49 ] state=[-0.21890683 -0.7613926  -0.1916538  -0.18417059], action=1, reward=1.0, next_state=[-0.23413468 -0.56411891 -0.19533721 -0.53066507]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 50 ] state=[-0.23413468 -0.56411891 -0.19533721 -0.53066507], action=0, reward=1.0, next_state=[-0.24541706 -0.75603445 -0.20595051 -0.30533108]\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ][ timestamp 51 ] state=[-0.24541706 -0.75603445 -0.20595051 -0.30533108], action=0, reward=-1.0, next_state=[-0.26053775 -0.94771764 -0.21205713 -0.08400486]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 223: Exploration_rate=0.01. Score=51.\n",
      "[ episode 224 ] state=[-0.04304758  0.0377657   0.02964109 -0.00928898]\n",
      "[ episode 224 ][ timestamp 1 ] state=[-0.04304758  0.0377657   0.02964109 -0.00928898], action=1, reward=1.0, next_state=[-0.04229226  0.23245029  0.02945531 -0.29247439]\n",
      "[ Experience replay ] starts\n",
      "[ episode 224 ][ timestamp 2 ] state=[-0.04229226  0.23245029  0.02945531 -0.29247439], action=0, reward=1.0, next_state=[-0.03764326  0.03692102  0.02360582  0.00935091]\n",
      "[ Experience replay ] starts\n",
      "[ episode 224 ][ timestamp 3 ] state=[-0.03764326  0.03692102  0.02360582  0.00935091], action=1, reward=1.0, next_state=[-0.03690484  0.23169662  0.02379284 -0.27579162]\n",
      "[ Experience replay ] starts\n",
      "[ episode 224 ][ timestamp 4 ] state=[-0.03690484  0.23169662  0.02379284 -0.27579162], action=1, reward=1.0, next_state=[-0.0322709   0.42647117  0.01827701 -0.56087633]\n",
      "[ Experience replay ] starts\n",
      "[ episode 224 ][ timestamp 5 ] state=[-0.0322709   0.42647117  0.01827701 -0.56087633], action=1, reward=1.0, next_state=[-0.02374148  0.62133191  0.00705948 -0.84774547]\n",
      "[ Experience replay ] starts\n",
      "[ episode 224 ][ timestamp 6 ] state=[-0.02374148  0.62133191  0.00705948 -0.84774547], action=1, reward=1.0, next_state=[-0.01131484  0.81635686 -0.00989543 -1.13820014]\n",
      "[ Experience replay ] starts\n",
      "[ episode 224 ][ timestamp 7 ] state=[-0.01131484  0.81635686 -0.00989543 -1.13820014], action=1, reward=1.0, next_state=[ 0.00501229  1.01160681 -0.03265943 -1.43396994]\n",
      "[ Experience replay ] starts\n",
      "[ episode 224 ][ timestamp 8 ] state=[ 0.00501229  1.01160681 -0.03265943 -1.43396994], action=1, reward=1.0, next_state=[ 0.02524443  1.20711605 -0.06133883 -1.73667757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 224 ][ timestamp 9 ] state=[ 0.02524443  1.20711605 -0.06133883 -1.73667757], action=1, reward=1.0, next_state=[ 0.04938675  1.40288129 -0.09607238 -2.04779549]\n",
      "[ Experience replay ] starts\n",
      "[ episode 224 ][ timestamp 10 ] state=[ 0.04938675  1.40288129 -0.09607238 -2.04779549], action=1, reward=1.0, next_state=[ 0.07744438  1.59884821 -0.13702829 -2.36859219]\n",
      "[ Experience replay ] starts\n",
      "[ episode 224 ][ timestamp 11 ] state=[ 0.07744438  1.59884821 -0.13702829 -2.36859219], action=1, reward=1.0, next_state=[ 0.10942134  1.7948955  -0.18440013 -2.70006697]\n",
      "[ Experience replay ] starts\n",
      "[ episode 224 ][ timestamp 12 ] state=[ 0.10942134  1.7948955  -0.18440013 -2.70006697], action=1, reward=-1.0, next_state=[ 0.14531925  1.99081638 -0.23840147 -3.04287287]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 224: Exploration_rate=0.01. Score=12.\n",
      "[ episode 225 ] state=[-0.0159128  -0.02188011 -0.02001048  0.04966652]\n",
      "[ episode 225 ][ timestamp 1 ] state=[-0.0159128  -0.02188011 -0.02001048  0.04966652], action=1, reward=1.0, next_state=[-0.0163504   0.17352298 -0.01901715 -0.24926211]\n",
      "[ Experience replay ] starts\n",
      "[ episode 225 ][ timestamp 2 ] state=[-0.0163504   0.17352298 -0.01901715 -0.24926211], action=1, reward=1.0, next_state=[-0.01287995  0.36891127 -0.02400239 -0.54788227]\n",
      "[ Experience replay ] starts\n",
      "[ episode 225 ][ timestamp 3 ] state=[-0.01287995  0.36891127 -0.02400239 -0.54788227], action=1, reward=1.0, next_state=[-0.00550172  0.56436205 -0.03496004 -0.84803002]\n",
      "[ Experience replay ] starts\n",
      "[ episode 225 ][ timestamp 4 ] state=[-0.00550172  0.56436205 -0.03496004 -0.84803002], action=1, reward=1.0, next_state=[ 0.00578552  0.75994297 -0.05192064 -1.15149829]\n",
      "[ Experience replay ] starts\n",
      "[ episode 225 ][ timestamp 5 ] state=[ 0.00578552  0.75994297 -0.05192064 -1.15149829], action=1, reward=1.0, next_state=[ 0.02098438  0.95570249 -0.0749506  -1.45999967]\n",
      "[ Experience replay ] starts\n",
      "[ episode 225 ][ timestamp 6 ] state=[ 0.02098438  0.95570249 -0.0749506  -1.45999967], action=1, reward=1.0, next_state=[ 0.04009843  1.1516592  -0.1041506  -1.77512438]\n",
      "[ Experience replay ] starts\n",
      "[ episode 225 ][ timestamp 7 ] state=[ 0.04009843  1.1516592  -0.1041506  -1.77512438], action=1, reward=1.0, next_state=[ 0.06313161  1.34778933 -0.13965309 -2.09829035]\n",
      "[ Experience replay ] starts\n",
      "[ episode 225 ][ timestamp 8 ] state=[ 0.06313161  1.34778933 -0.13965309 -2.09829035], action=1, reward=1.0, next_state=[ 0.0900874   1.54401205 -0.18161889 -2.43068357]\n",
      "[ Experience replay ] starts\n",
      "[ episode 225 ][ timestamp 9 ] state=[ 0.0900874   1.54401205 -0.18161889 -2.43068357], action=1, reward=-1.0, next_state=[ 0.12096764  1.74017239 -0.23023256 -2.77318747]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 225: Exploration_rate=0.01. Score=9.\n",
      "[ episode 226 ] state=[ 0.04817374 -0.02906303  0.01302784  0.04200233]\n",
      "[ episode 226 ][ timestamp 1 ] state=[ 0.04817374 -0.02906303  0.01302784  0.04200233], action=1, reward=1.0, next_state=[ 0.04759248  0.16586971  0.01386789 -0.24654189]\n",
      "[ Experience replay ] starts\n",
      "[ episode 226 ][ timestamp 2 ] state=[ 0.04759248  0.16586971  0.01386789 -0.24654189], action=1, reward=1.0, next_state=[ 0.05090988  0.36079088  0.00893705 -0.5348185 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 226 ][ timestamp 3 ] state=[ 0.05090988  0.36079088  0.00893705 -0.5348185 ], action=1, reward=1.0, next_state=[ 0.0581257   0.55578602 -0.00175932 -0.82467208]\n",
      "[ Experience replay ] starts\n",
      "[ episode 226 ][ timestamp 4 ] state=[ 0.0581257   0.55578602 -0.00175932 -0.82467208], action=1, reward=1.0, next_state=[ 0.06924142  0.75093199 -0.01825276 -1.11790782]\n",
      "[ Experience replay ] starts\n",
      "[ episode 226 ][ timestamp 5 ] state=[ 0.06924142  0.75093199 -0.01825276 -1.11790782], action=1, reward=1.0, next_state=[ 0.08426006  0.94628864 -0.04061092 -1.41625999]\n",
      "[ Experience replay ] starts\n",
      "[ episode 226 ][ timestamp 6 ] state=[ 0.08426006  0.94628864 -0.04061092 -1.41625999], action=1, reward=1.0, next_state=[ 0.10318583  1.14188933 -0.06893612 -1.72135545]\n",
      "[ Experience replay ] starts\n",
      "[ episode 226 ][ timestamp 7 ] state=[ 0.10318583  1.14188933 -0.06893612 -1.72135545], action=1, reward=1.0, next_state=[ 0.12602361  1.33772969 -0.10336323 -2.03466942]\n",
      "[ Experience replay ] starts\n",
      "[ episode 226 ][ timestamp 8 ] state=[ 0.12602361  1.33772969 -0.10336323 -2.03466942], action=1, reward=1.0, next_state=[ 0.15277821  1.53375405 -0.14405662 -2.35747134]\n",
      "[ Experience replay ] starts\n",
      "[ episode 226 ][ timestamp 9 ] state=[ 0.15277821  1.53375405 -0.14405662 -2.35747134], action=1, reward=1.0, next_state=[ 0.18345329  1.72983943 -0.19120604 -2.69075908]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 226 ][ timestamp 10 ] state=[ 0.18345329  1.72983943 -0.19120604 -2.69075908], action=1, reward=-1.0, next_state=[ 0.21805008  1.92577703 -0.24502123 -3.03518192]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 226: Exploration_rate=0.01. Score=10.\n",
      "[ episode 227 ] state=[-0.00799127 -0.04159598 -0.01770179 -0.00688262]\n",
      "[ episode 227 ][ timestamp 1 ] state=[-0.00799127 -0.04159598 -0.01770179 -0.00688262], action=1, reward=1.0, next_state=[-0.00882319  0.15377531 -0.01783944 -0.30509769]\n",
      "[ Experience replay ] starts\n",
      "[ episode 227 ][ timestamp 2 ] state=[-0.00882319  0.15377531 -0.01783944 -0.30509769], action=1, reward=1.0, next_state=[-0.00574769  0.34914688 -0.0239414  -0.60335294]\n",
      "[ Experience replay ] starts\n",
      "[ episode 227 ][ timestamp 3 ] state=[-0.00574769  0.34914688 -0.0239414  -0.60335294], action=1, reward=1.0, next_state=[ 0.00123525  0.54459536 -0.03600846 -0.90347973]\n",
      "[ Experience replay ] starts\n",
      "[ episode 227 ][ timestamp 4 ] state=[ 0.00123525  0.54459536 -0.03600846 -0.90347973], action=1, reward=1.0, next_state=[ 0.01212716  0.74018606 -0.05407805 -1.2072598 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 227 ][ timestamp 5 ] state=[ 0.01212716  0.74018606 -0.05407805 -1.2072598 ], action=1, reward=1.0, next_state=[ 0.02693088  0.93596335 -0.07822325 -1.51638763]\n",
      "[ Experience replay ] starts\n",
      "[ episode 227 ][ timestamp 6 ] state=[ 0.02693088  0.93596335 -0.07822325 -1.51638763], action=1, reward=1.0, next_state=[ 0.04565015  1.1319397  -0.108551   -1.83242744]\n",
      "[ Experience replay ] starts\n",
      "[ episode 227 ][ timestamp 7 ] state=[ 0.04565015  1.1319397  -0.108551   -1.83242744], action=1, reward=1.0, next_state=[ 0.06828894  1.32808288 -0.14519955 -2.15676185]\n",
      "[ Experience replay ] starts\n",
      "[ episode 227 ][ timestamp 8 ] state=[ 0.06828894  1.32808288 -0.14519955 -2.15676185], action=1, reward=1.0, next_state=[ 0.0948506   1.52430085 -0.18833479 -2.49053045]\n",
      "[ Experience replay ] starts\n",
      "[ episode 227 ][ timestamp 9 ] state=[ 0.0948506   1.52430085 -0.18833479 -2.49053045], action=1, reward=-1.0, next_state=[ 0.12533661  1.72042439 -0.23814539 -2.83455745]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 227: Exploration_rate=0.01. Score=9.\n",
      "[ episode 228 ] state=[-0.04757463 -0.00927954 -0.03012321 -0.00839117]\n",
      "[ episode 228 ][ timestamp 1 ] state=[-0.04757463 -0.00927954 -0.03012321 -0.00839117], action=1, reward=1.0, next_state=[-0.04776022  0.18626118 -0.03029103 -0.31042406]\n",
      "[ Experience replay ] starts\n",
      "[ episode 228 ][ timestamp 2 ] state=[-0.04776022  0.18626118 -0.03029103 -0.31042406], action=1, reward=1.0, next_state=[-0.044035    0.38180132 -0.03649951 -0.61250391]\n",
      "[ Experience replay ] starts\n",
      "[ episode 228 ][ timestamp 3 ] state=[-0.044035    0.38180132 -0.03649951 -0.61250391], action=1, reward=1.0, next_state=[-0.03639897  0.57741384 -0.04874959 -0.91645575]\n",
      "[ Experience replay ] starts\n",
      "[ episode 228 ][ timestamp 4 ] state=[-0.03639897  0.57741384 -0.04874959 -0.91645575], action=1, reward=1.0, next_state=[-0.0248507   0.77315989 -0.06707871 -1.22405269]\n",
      "[ Experience replay ] starts\n",
      "[ episode 228 ][ timestamp 5 ] state=[-0.0248507   0.77315989 -0.06707871 -1.22405269], action=1, reward=1.0, next_state=[-0.0093875   0.96907857 -0.09155976 -1.53697617]\n",
      "[ Experience replay ] starts\n",
      "[ episode 228 ][ timestamp 6 ] state=[-0.0093875   0.96907857 -0.09155976 -1.53697617], action=1, reward=1.0, next_state=[ 0.00999407  1.16517563 -0.12229929 -1.85677064]\n",
      "[ Experience replay ] starts\n",
      "[ episode 228 ][ timestamp 7 ] state=[ 0.00999407  1.16517563 -0.12229929 -1.85677064], action=1, reward=1.0, next_state=[ 0.03329759  1.36141007 -0.1594347  -2.18479015]\n",
      "[ Experience replay ] starts\n",
      "[ episode 228 ][ timestamp 8 ] state=[ 0.03329759  1.36141007 -0.1594347  -2.18479015], action=1, reward=1.0, next_state=[ 0.06052579  1.5576787  -0.2031305  -2.5221347 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 228 ][ timestamp 9 ] state=[ 0.06052579  1.5576787  -0.2031305  -2.5221347 ], action=1, reward=-1.0, next_state=[ 0.09167936  1.75379836 -0.2535732  -2.86957634]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 228: Exploration_rate=0.01. Score=9.\n",
      "[ episode 229 ] state=[-0.01808813  0.00075821  0.00255436 -0.03214828]\n",
      "[ episode 229 ][ timestamp 1 ] state=[-0.01808813  0.00075821  0.00255436 -0.03214828], action=1, reward=1.0, next_state=[-0.01807297  0.19584344  0.0019114  -0.32402418]\n",
      "[ Experience replay ] starts\n",
      "[ episode 229 ][ timestamp 2 ] state=[-0.01807297  0.19584344  0.0019114  -0.32402418], action=1, reward=1.0, next_state=[-0.0141561   0.39093812 -0.00456909 -0.61610372]\n",
      "[ Experience replay ] starts\n",
      "[ episode 229 ][ timestamp 3 ] state=[-0.0141561   0.39093812 -0.00456909 -0.61610372], action=1, reward=1.0, next_state=[-0.00633734  0.58612361 -0.01689116 -0.9102222 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 229 ][ timestamp 4 ] state=[-0.00633734  0.58612361 -0.01689116 -0.9102222 ], action=1, reward=1.0, next_state=[ 0.00538514  0.78147003 -0.0350956  -1.2081658 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 229 ][ timestamp 5 ] state=[ 0.00538514  0.78147003 -0.0350956  -1.2081658 ], action=1, reward=1.0, next_state=[ 0.02101454  0.9770273  -0.05925892 -1.51163707]\n",
      "[ Experience replay ] starts\n",
      "[ episode 229 ][ timestamp 6 ] state=[ 0.02101454  0.9770273  -0.05925892 -1.51163707], action=1, reward=1.0, next_state=[ 0.04055508  1.17281485 -0.08949166 -1.82221481]\n",
      "[ Experience replay ] starts\n",
      "[ episode 229 ][ timestamp 7 ] state=[ 0.04055508  1.17281485 -0.08949166 -1.82221481], action=1, reward=1.0, next_state=[ 0.06401138  1.36880933 -0.12593596 -2.14130551]\n",
      "[ Experience replay ] starts\n",
      "[ episode 229 ][ timestamp 8 ] state=[ 0.06401138  1.36880933 -0.12593596 -2.14130551], action=1, reward=1.0, next_state=[ 0.09138757  1.56493009 -0.16876207 -2.47008428]\n",
      "[ Experience replay ] starts\n",
      "[ episode 229 ][ timestamp 9 ] state=[ 0.09138757  1.56493009 -0.16876207 -2.47008428], action=1, reward=-1.0, next_state=[ 0.12268617  1.76102207 -0.21816375 -2.80942442]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 229: Exploration_rate=0.01. Score=9.\n",
      "[ episode 230 ] state=[-0.02226316 -0.02403012 -0.03711227  0.04422864]\n",
      "[ episode 230 ][ timestamp 1 ] state=[-0.02226316 -0.02403012 -0.03711227  0.04422864], action=1, reward=1.0, next_state=[-0.02274376  0.1716038  -0.0362277  -0.25992869]\n",
      "[ Experience replay ] starts\n",
      "[ episode 230 ][ timestamp 2 ] state=[-0.02274376  0.1716038  -0.0362277  -0.25992869], action=1, reward=1.0, next_state=[-0.01931168  0.36722369 -0.04142627 -0.5638146 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 230 ][ timestamp 3 ] state=[-0.01931168  0.36722369 -0.04142627 -0.5638146 ], action=1, reward=1.0, next_state=[-0.01196721  0.56290168 -0.05270257 -0.8692556 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 230 ][ timestamp 4 ] state=[-0.01196721  0.56290168 -0.05270257 -0.8692556 ], action=1, reward=1.0, next_state=[-7.09176915e-04  7.58699473e-01 -7.00876779e-02 -1.17803189e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 230 ][ timestamp 5 ] state=[-7.09176915e-04  7.58699473e-01 -7.00876779e-02 -1.17803189e+00], action=1, reward=1.0, next_state=[ 0.01446481  0.95465816 -0.09364832 -1.49183717]\n",
      "[ Experience replay ] starts\n",
      "[ episode 230 ][ timestamp 6 ] state=[ 0.01446481  0.95465816 -0.09364832 -1.49183717], action=1, reward=1.0, next_state=[ 0.03355798  1.1507868  -0.12348506 -1.81223341]\n",
      "[ Experience replay ] starts\n",
      "[ episode 230 ][ timestamp 7 ] state=[ 0.03355798  1.1507868  -0.12348506 -1.81223341], action=1, reward=1.0, next_state=[ 0.05657371  1.34704924 -0.15972973 -2.14059779]\n",
      "[ Experience replay ] starts\n",
      "[ episode 230 ][ timestamp 8 ] state=[ 0.05657371  1.34704924 -0.15972973 -2.14059779], action=1, reward=1.0, next_state=[ 0.0835147   1.5433488  -0.20254168 -2.47806   ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 230 ][ timestamp 9 ] state=[ 0.0835147   1.5433488  -0.20254168 -2.47806   ], action=1, reward=-1.0, next_state=[ 0.11438167  1.73951075 -0.25210288 -2.82542909]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 230: Exploration_rate=0.01. Score=9.\n",
      "[ episode 231 ] state=[ 0.04323152 -0.00927428  0.0379635   0.0385324 ]\n",
      "[ episode 231 ][ timestamp 1 ] state=[ 0.04323152 -0.00927428  0.0379635   0.0385324 ], action=1, reward=1.0, next_state=[ 0.04304603  0.18528329  0.03873415 -0.24193508]\n",
      "[ Experience replay ] starts\n",
      "[ episode 231 ][ timestamp 2 ] state=[ 0.04304603  0.18528329  0.03873415 -0.24193508], action=1, reward=1.0, next_state=[ 0.0467517   0.37983115  0.03389545 -0.522153  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 231 ][ timestamp 3 ] state=[ 0.0467517   0.37983115  0.03389545 -0.522153  ], action=1, reward=1.0, next_state=[ 0.05434832  0.57446003  0.02345239 -0.80396527]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 231 ][ timestamp 4 ] state=[ 0.05434832  0.57446003  0.02345239 -0.80396527], action=1, reward=1.0, next_state=[ 0.06583752  0.76925271  0.00737309 -1.08917957]\n",
      "[ Experience replay ] starts\n",
      "[ episode 231 ][ timestamp 5 ] state=[ 0.06583752  0.76925271  0.00737309 -1.08917957], action=1, reward=1.0, next_state=[ 0.08122258  0.96427668 -0.01441051 -1.37953991]\n",
      "[ Experience replay ] starts\n",
      "[ episode 231 ][ timestamp 6 ] state=[ 0.08122258  0.96427668 -0.01441051 -1.37953991], action=0, reward=1.0, next_state=[ 0.10050811  0.76933758 -0.0420013  -1.09139815]\n",
      "[ Experience replay ] starts\n",
      "[ episode 231 ][ timestamp 7 ] state=[ 0.10050811  0.76933758 -0.0420013  -1.09139815], action=0, reward=1.0, next_state=[ 0.11589486  0.57479358 -0.06382927 -0.81218427]\n",
      "[ Experience replay ] starts\n",
      "[ episode 231 ][ timestamp 8 ] state=[ 0.11589486  0.57479358 -0.06382927 -0.81218427], action=0, reward=1.0, next_state=[ 0.12739073  0.38060131 -0.08007295 -0.54024211]\n",
      "[ Experience replay ] starts\n",
      "[ episode 231 ][ timestamp 9 ] state=[ 0.12739073  0.38060131 -0.08007295 -0.54024211], action=1, reward=1.0, next_state=[ 0.13500276  0.57675212 -0.0908778  -0.85704189]\n",
      "[ Experience replay ] starts\n",
      "[ episode 231 ][ timestamp 10 ] state=[ 0.13500276  0.57675212 -0.0908778  -0.85704189], action=1, reward=1.0, next_state=[ 0.1465378   0.77298693 -0.10801863 -1.17686076]\n",
      "[ Experience replay ] starts\n",
      "[ episode 231 ][ timestamp 11 ] state=[ 0.1465378   0.77298693 -0.10801863 -1.17686076], action=0, reward=1.0, next_state=[ 0.16199754  0.57942105 -0.13155585 -0.91989994]\n",
      "[ Experience replay ] starts\n",
      "[ episode 231 ][ timestamp 12 ] state=[ 0.16199754  0.57942105 -0.13155585 -0.91989994], action=0, reward=1.0, next_state=[ 0.17358596  0.38629897 -0.14995385 -0.67128592]\n",
      "[ Experience replay ] starts\n",
      "[ episode 231 ][ timestamp 13 ] state=[ 0.17358596  0.38629897 -0.14995385 -0.67128592], action=0, reward=1.0, next_state=[ 0.18131194  0.19354458 -0.16337957 -0.42932037]\n",
      "[ Experience replay ] starts\n",
      "[ episode 231 ][ timestamp 14 ] state=[ 0.18131194  0.19354458 -0.16337957 -0.42932037], action=0, reward=1.0, next_state=[ 0.18518283  0.00106761 -0.17196597 -0.19226986]\n",
      "[ Experience replay ] starts\n",
      "[ episode 231 ][ timestamp 15 ] state=[ 0.18518283  0.00106761 -0.17196597 -0.19226986], action=0, reward=1.0, next_state=[ 0.18520419 -0.19123059 -0.17581137  0.04161374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 231 ][ timestamp 16 ] state=[ 0.18520419 -0.19123059 -0.17581137  0.04161374], action=1, reward=1.0, next_state=[ 0.18137957  0.0059195  -0.17497909 -0.30097546]\n",
      "[ Experience replay ] starts\n",
      "[ episode 231 ][ timestamp 17 ] state=[ 0.18137957  0.0059195  -0.17497909 -0.30097546], action=0, reward=1.0, next_state=[ 0.18149796 -0.18633296 -0.1809986  -0.068182  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 231 ][ timestamp 18 ] state=[ 0.18149796 -0.18633296 -0.1809986  -0.068182  ], action=1, reward=1.0, next_state=[ 0.1777713   0.01086008 -0.18236224 -0.41206319]\n",
      "[ Experience replay ] starts\n",
      "[ episode 231 ][ timestamp 19 ] state=[ 0.1777713   0.01086008 -0.18236224 -0.41206319], action=1, reward=1.0, next_state=[ 0.17798851  0.20803523 -0.19060351 -0.75623941]\n",
      "[ Experience replay ] starts\n",
      "[ episode 231 ][ timestamp 20 ] state=[ 0.17798851  0.20803523 -0.19060351 -0.75623941], action=0, reward=1.0, next_state=[ 0.18214921  0.01597977 -0.2057283  -0.52907212]\n",
      "[ Experience replay ] starts\n",
      "[ episode 231 ][ timestamp 21 ] state=[ 0.18214921  0.01597977 -0.2057283  -0.52907212], action=0, reward=-1.0, next_state=[ 0.18246881 -0.17574466 -0.21630974 -0.30760834]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 231: Exploration_rate=0.01. Score=21.\n",
      "[ episode 232 ] state=[ 0.02802768 -0.00989843 -0.02091588 -0.0185934 ]\n",
      "[ episode 232 ][ timestamp 1 ] state=[ 0.02802768 -0.00989843 -0.02091588 -0.0185934 ], action=1, reward=1.0, next_state=[ 0.02782972  0.18551714 -0.02128775 -0.31780145]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 2 ] state=[ 0.02782972  0.18551714 -0.02128775 -0.31780145], action=1, reward=1.0, next_state=[ 0.03154006  0.38093572 -0.02764377 -0.61712103]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 3 ] state=[ 0.03154006  0.38093572 -0.02764377 -0.61712103], action=0, reward=1.0, next_state=[ 0.03915877  0.18621063 -0.03998619 -0.33327124]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 4 ] state=[ 0.03915877  0.18621063 -0.03998619 -0.33327124], action=0, reward=1.0, next_state=[ 0.04288299 -0.00832005 -0.04665162 -0.05346126]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 5 ] state=[ 0.04288299 -0.00832005 -0.04665162 -0.05346126], action=1, reward=1.0, next_state=[ 0.04271658  0.1874387  -0.04772084 -0.36049051]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 6 ] state=[ 0.04271658  0.1874387  -0.04772084 -0.36049051], action=1, reward=1.0, next_state=[ 0.04646536  0.38320535 -0.05493066 -0.6678308 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 7 ] state=[ 0.04646536  0.38320535 -0.05493066 -0.6678308 ], action=0, reward=1.0, next_state=[ 0.05412947  0.18888857 -0.06828727 -0.39293676]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 8 ] state=[ 0.05412947  0.18888857 -0.06828727 -0.39293676], action=1, reward=1.0, next_state=[ 0.05790724  0.38490976 -0.07614601 -0.70634411]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 9 ] state=[ 0.05790724  0.38490976 -0.07614601 -0.70634411], action=1, reward=1.0, next_state=[ 0.06560543  0.58099948 -0.09027289 -1.02199167]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 10 ] state=[ 0.06560543  0.58099948 -0.09027289 -1.02199167], action=0, reward=1.0, next_state=[ 0.07722542  0.38718855 -0.11071272 -0.75896321]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 11 ] state=[ 0.07722542  0.38718855 -0.11071272 -0.75896321], action=0, reward=1.0, next_state=[ 0.08496919  0.19375212 -0.12589199 -0.50306811]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 12 ] state=[ 0.08496919  0.19375212 -0.12589199 -0.50306811], action=0, reward=1.0, next_state=[ 0.08884423  0.00060843 -0.13595335 -0.25255992]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 13 ] state=[ 0.08884423  0.00060843 -0.13595335 -0.25255992], action=0, reward=1.0, next_state=[ 0.0888564  -0.19233678 -0.14100455 -0.00565995]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 14 ] state=[ 0.0888564  -0.19233678 -0.14100455 -0.00565995], action=0, reward=1.0, next_state=[ 0.08500967 -0.38518452 -0.14111775  0.23942263]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 15 ] state=[ 0.08500967 -0.38518452 -0.14111775  0.23942263], action=1, reward=1.0, next_state=[ 0.07730598 -0.18835826 -0.13632929 -0.09423295]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 16 ] state=[ 0.07730598 -0.18835826 -0.13632929 -0.09423295], action=0, reward=1.0, next_state=[ 0.07353881 -0.38128962 -0.13821395  0.15252215]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 17 ] state=[ 0.07353881 -0.38128962 -0.13821395  0.15252215], action=0, reward=1.0, next_state=[ 0.06591302 -0.57418973 -0.13516351  0.39860733]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 18 ] state=[ 0.06591302 -0.57418973 -0.13516351  0.39860733], action=0, reward=1.0, next_state=[ 0.05442923 -0.76716126 -0.12719136  0.64580738]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 19 ] state=[ 0.05442923 -0.76716126 -0.12719136  0.64580738], action=0, reward=1.0, next_state=[ 0.039086   -0.96030293 -0.11427522  0.89588609]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 20 ] state=[ 0.039086   -0.96030293 -0.11427522  0.89588609], action=0, reward=1.0, next_state=[ 0.01987994 -1.15370552 -0.09635749  1.150574  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 21 ] state=[ 0.01987994 -1.15370552 -0.09635749  1.150574  ], action=1, reward=1.0, next_state=[-0.00319417 -0.95746737 -0.07334601  0.82929694]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 22 ] state=[-0.00319417 -0.95746737 -0.07334601  0.82929694], action=1, reward=1.0, next_state=[-0.02234352 -0.76142349 -0.05676007  0.51447736]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 23 ] state=[-0.02234352 -0.76142349 -0.05676007  0.51447736], action=1, reward=1.0, next_state=[-0.03757199 -0.56555004 -0.04647053  0.20446184]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 24 ] state=[-0.03757199 -0.56555004 -0.04647053  0.20446184], action=0, reward=1.0, next_state=[-0.04888299 -0.75997769 -0.04238129  0.48213105]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 232 ][ timestamp 25 ] state=[-0.04888299 -0.75997769 -0.04238129  0.48213105], action=0, reward=1.0, next_state=[-0.06408254 -0.95447662 -0.03273867  0.7611611 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 26 ] state=[-0.06408254 -0.95447662 -0.03273867  0.7611611 ], action=0, reward=1.0, next_state=[-0.08317207 -1.14913263 -0.01751545  1.0433652 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 27 ] state=[-0.08317207 -1.14913263 -0.01751545  1.0433652 ], action=1, reward=1.0, next_state=[-0.10615473 -0.95378252  0.00335186  0.7452357 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 28 ] state=[-0.10615473 -0.95378252  0.00335186  0.7452357 ], action=1, reward=1.0, next_state=[-0.12523038 -0.75870698  0.01825657  0.45360949]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 29 ] state=[-0.12523038 -0.75870698  0.01825657  0.45360949], action=1, reward=1.0, next_state=[-0.14040452 -0.56384788  0.02732876  0.16673668]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 30 ] state=[-0.14040452 -0.56384788  0.02732876  0.16673668], action=0, reward=1.0, next_state=[-0.15168147 -0.75935015  0.03066349  0.46791424]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 31 ] state=[-0.15168147 -0.75935015  0.03066349  0.46791424], action=1, reward=1.0, next_state=[-0.16686848 -0.56467453  0.04002178  0.18505172]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 32 ] state=[-0.16686848 -0.56467453  0.04002178  0.18505172], action=1, reward=1.0, next_state=[-0.17816197 -0.37014738  0.04372281 -0.09474207]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 33 ] state=[-0.17816197 -0.37014738  0.04372281 -0.09474207], action=0, reward=1.0, next_state=[-0.18556491 -0.56586784  0.04182797  0.21140845]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 34 ] state=[-0.18556491 -0.56586784  0.04182797  0.21140845], action=0, reward=1.0, next_state=[-0.19688227 -0.76156211  0.04605614  0.51698694]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 35 ] state=[-0.19688227 -0.76156211  0.04605614  0.51698694], action=0, reward=1.0, next_state=[-0.21211351 -0.95730127  0.05639588  0.82382007]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 36 ] state=[-0.21211351 -0.95730127  0.05639588  0.82382007], action=0, reward=1.0, next_state=[-0.23125954 -1.15314746  0.07287228  1.13369391]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 37 ] state=[-0.23125954 -1.15314746  0.07287228  1.13369391], action=1, reward=1.0, next_state=[-0.25432249 -0.9590509   0.09554616  0.86472726]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 38 ] state=[-0.25432249 -0.9590509   0.09554616  0.86472726], action=1, reward=1.0, next_state=[-0.27350351 -0.76535029  0.1128407   0.60354942]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 39 ] state=[-0.27350351 -0.76535029  0.1128407   0.60354942], action=1, reward=1.0, next_state=[-0.28881051 -0.57197233  0.12491169  0.34843204]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 40 ] state=[-0.28881051 -0.57197233  0.12491169  0.34843204], action=1, reward=1.0, next_state=[-0.30024996 -0.37882782  0.13188033  0.09760117]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 41 ] state=[-0.30024996 -0.37882782  0.13188033  0.09760117], action=1, reward=1.0, next_state=[-0.30782651 -0.18581836  0.13383236 -0.15073847]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 42 ] state=[-0.30782651 -0.18581836  0.13383236 -0.15073847], action=1, reward=1.0, next_state=[-0.31154288  0.00715863  0.13081759 -0.39838614]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 43 ] state=[-0.31154288  0.00715863  0.13081759 -0.39838614], action=1, reward=1.0, next_state=[-0.31139971  0.20020564  0.12284987 -0.64713167]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 44 ] state=[-0.31139971  0.20020564  0.12284987 -0.64713167], action=1, reward=1.0, next_state=[-0.3073956   0.39342119  0.10990723 -0.89874364]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 45 ] state=[-0.3073956   0.39342119  0.10990723 -0.89874364], action=1, reward=1.0, next_state=[-0.29952717  0.58689569  0.09193236 -1.15495662]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 46 ] state=[-0.29952717  0.58689569  0.09193236 -1.15495662], action=0, reward=1.0, next_state=[-0.28778926  0.39070317  0.06883323 -0.83492051]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 47 ] state=[-0.28778926  0.39070317  0.06883323 -0.83492051], action=1, reward=1.0, next_state=[-0.2799752   0.58482065  0.05213482 -1.10518621]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 48 ] state=[-0.2799752   0.58482065  0.05213482 -1.10518621], action=1, reward=1.0, next_state=[-0.26827878  0.77921973  0.03003109 -1.38106795]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 49 ] state=[-0.26827878  0.77921973  0.03003109 -1.38106795], action=0, reward=1.0, next_state=[-0.25269439  0.58373612  0.00240973 -1.07914693]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 50 ] state=[-0.25269439  0.58373612  0.00240973 -1.07914693], action=1, reward=1.0, next_state=[-0.24101967  0.77882617 -0.01917321 -1.37107269]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 51 ] state=[-0.24101967  0.77882617 -0.01917321 -1.37107269], action=0, reward=1.0, next_state=[-0.22544314  0.5839492  -0.04659466 -1.08444755]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 52 ] state=[-0.22544314  0.5839492  -0.04659466 -1.08444755], action=1, reward=1.0, next_state=[-0.21376416  0.77965392 -0.06828361 -1.39137988]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 53 ] state=[-0.21376416  0.77965392 -0.06828361 -1.39137988], action=0, reward=1.0, next_state=[-0.19817108  0.5854455  -0.09611121 -1.12080592]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 54 ] state=[-0.19817108  0.5854455  -0.09611121 -1.12080592], action=1, reward=1.0, next_state=[-0.18646217  0.78168751 -0.11852733 -1.44202363]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 55 ] state=[-0.18646217  0.78168751 -0.11852733 -1.44202363], action=0, reward=1.0, next_state=[-0.17082842  0.5882076  -0.1473678  -1.18860549]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 56 ] state=[-0.17082842  0.5882076  -0.1473678  -1.18860549], action=0, reward=1.0, next_state=[-0.15906427  0.39527064 -0.17113991 -0.9455064 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 57 ] state=[-0.15906427  0.39527064 -0.17113991 -0.9455064 ], action=0, reward=1.0, next_state=[-0.15115885  0.20281535 -0.19005004 -0.71111062]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 58 ] state=[-0.15115885  0.20281535 -0.19005004 -0.71111062], action=0, reward=1.0, next_state=[-0.14710255  0.01076212 -0.20427225 -0.48375669]\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ][ timestamp 59 ] state=[-0.14710255  0.01076212 -0.20427225 -0.48375669], action=0, reward=-1.0, next_state=[-0.14688731 -0.18098007 -0.21394738 -0.26176246]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 232: Exploration_rate=0.01. Score=59.\n",
      "[ episode 233 ] state=[-0.01423558  0.0090645   0.00763651 -0.00306171]\n",
      "[ episode 233 ][ timestamp 1 ] state=[-0.01423558  0.0090645   0.00763651 -0.00306171], action=0, reward=1.0, next_state=[-0.01405429 -0.18616613  0.00757527  0.29202081]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 2 ] state=[-0.01405429 -0.18616613  0.00757527  0.29202081], action=0, reward=1.0, next_state=[-0.01777762 -0.38139527  0.01341569  0.58708322]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 3 ] state=[-0.01777762 -0.38139527  0.01341569  0.58708322], action=1, reward=1.0, next_state=[-0.02540552 -0.18646375  0.02515735  0.29865635]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 4 ] state=[-0.02540552 -0.18646375  0.02515735  0.29865635], action=1, reward=1.0, next_state=[-0.0291348   0.00829073  0.03113048  0.01401255]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 5 ] state=[-0.0291348   0.00829073  0.03113048  0.01401255], action=1, reward=1.0, next_state=[-0.02896898  0.20295272  0.03141073 -0.26868808]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 6 ] state=[-0.02896898  0.20295272  0.03141073 -0.26868808], action=0, reward=1.0, next_state=[-0.02490993  0.00739692  0.02603697  0.03373417]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 7 ] state=[-0.02490993  0.00739692  0.02603697  0.03373417], action=0, reward=1.0, next_state=[-0.02476199 -0.18808855  0.02671165  0.33451699]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 8 ] state=[-0.02476199 -0.18808855  0.02671165  0.33451699], action=1, reward=1.0, next_state=[-0.02852376  0.00664325  0.03340199  0.05037578]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 233 ][ timestamp 9 ] state=[-0.02852376  0.00664325  0.03340199  0.05037578], action=1, reward=1.0, next_state=[-0.02839089  0.20127072  0.03440951 -0.23158422]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 10 ] state=[-0.02839089  0.20127072  0.03440951 -0.23158422], action=0, reward=1.0, next_state=[-0.02436548  0.00567442  0.02977782  0.07175096]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 11 ] state=[-0.02436548  0.00567442  0.02977782  0.07175096], action=1, reward=1.0, next_state=[-0.02425199  0.20035709  0.03121284 -0.21139019]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 12 ] state=[-0.02425199  0.20035709  0.03121284 -0.21139019], action=0, reward=1.0, next_state=[-0.02024485  0.00480308  0.02698504  0.09097302]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 13 ] state=[-0.02024485  0.00480308  0.02698504  0.09097302], action=0, reward=1.0, next_state=[-0.02014879 -0.19069505  0.0288045   0.3920461 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 14 ] state=[-0.02014879 -0.19069505  0.0288045   0.3920461 ], action=1, reward=1.0, next_state=[-0.02396269  0.00400653  0.03664542  0.10858222]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 15 ] state=[-0.02396269  0.00400653  0.03664542  0.10858222], action=1, reward=1.0, next_state=[-0.02388256  0.19858471  0.03881707 -0.17231776]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 16 ] state=[-0.02388256  0.19858471  0.03881707 -0.17231776], action=1, reward=1.0, next_state=[-0.01991086  0.39313022  0.03537071 -0.45250684]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 17 ] state=[-0.01991086  0.39313022  0.03537071 -0.45250684], action=1, reward=1.0, next_state=[-0.01204826  0.58773458  0.02632057 -0.73383398]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 18 ] state=[-0.01204826  0.58773458  0.02632057 -0.73383398], action=0, reward=1.0, next_state=[-2.93568140e-04  3.92259061e-01  1.16438942e-02 -4.32984905e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 19 ] state=[-2.93568140e-04  3.92259061e-01  1.16438942e-02 -4.32984905e-01], action=0, reward=1.0, next_state=[ 0.00755161  0.1969742   0.0029842  -0.13665425]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 20 ] state=[ 0.00755161  0.1969742   0.0029842  -0.13665425], action=1, reward=1.0, next_state=[ 1.14910971e-02  3.92053282e-01  2.51111164e-04 -4.28394212e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 21 ] state=[ 1.14910971e-02  3.92053282e-01  2.51111164e-04 -4.28394212e-01], action=1, reward=1.0, next_state=[ 0.01933216  0.58717168 -0.00831677 -0.72099797]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 22 ] state=[ 0.01933216  0.58717168 -0.00831677 -0.72099797], action=0, reward=1.0, next_state=[ 0.0310756   0.39216576 -0.02273673 -0.43094432]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 23 ] state=[ 0.0310756   0.39216576 -0.02273673 -0.43094432], action=0, reward=1.0, next_state=[ 0.03891891  0.19737303 -0.03135562 -0.14551475]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 24 ] state=[ 0.03891891  0.19737303 -0.03135562 -0.14551475], action=1, reward=1.0, next_state=[ 0.04286637  0.39292966 -0.03426591 -0.44792255]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 25 ] state=[ 0.04286637  0.39292966 -0.03426591 -0.44792255], action=0, reward=1.0, next_state=[ 0.05072497  0.19830875 -0.04322436 -0.16623477]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 26 ] state=[ 0.05072497  0.19830875 -0.04322436 -0.16623477], action=0, reward=1.0, next_state=[ 0.05469114  0.00383133 -0.04654906  0.11250487]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 27 ] state=[ 0.05469114  0.00383133 -0.04654906  0.11250487], action=1, reward=1.0, next_state=[ 0.05476777  0.19958831 -0.04429896 -0.19449301]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 28 ] state=[ 0.05476777  0.19958831 -0.04429896 -0.19449301], action=1, reward=1.0, next_state=[ 0.05875953  0.39531503 -0.04818882 -0.50081469]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 29 ] state=[ 0.05875953  0.39531503 -0.04818882 -0.50081469], action=0, reward=1.0, next_state=[ 0.06666583  0.20090432 -0.05820512 -0.22369919]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 30 ] state=[ 0.06666583  0.20090432 -0.05820512 -0.22369919], action=0, reward=1.0, next_state=[ 0.07068392  0.00666049 -0.0626791   0.0500705 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 31 ] state=[ 0.07068392  0.00666049 -0.0626791   0.0500705 ], action=0, reward=1.0, next_state=[ 0.07081713 -0.18750931 -0.06167769  0.32233768]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 32 ] state=[ 0.07081713 -0.18750931 -0.06167769  0.32233768], action=1, reward=1.0, next_state=[ 0.06706694  0.00843423 -0.05523094  0.0108595 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 33 ] state=[ 0.06706694  0.00843423 -0.05523094  0.0108595 ], action=0, reward=1.0, next_state=[ 0.06723563 -0.18585393 -0.05501375  0.2856177 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 34 ] state=[ 0.06723563 -0.18585393 -0.05501375  0.2856177 ], action=1, reward=1.0, next_state=[ 0.06351855  0.01000771 -0.04930139 -0.02389616]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 35 ] state=[ 0.06351855  0.01000771 -0.04930139 -0.02389616], action=0, reward=1.0, next_state=[ 0.0637187  -0.18437382 -0.04977932  0.25283311]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 36 ] state=[ 0.0637187  -0.18437382 -0.04977932  0.25283311], action=0, reward=1.0, next_state=[ 0.06003123 -0.37875093 -0.04472265  0.52940853]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 37 ] state=[ 0.06003123 -0.37875093 -0.04472265  0.52940853], action=1, reward=1.0, next_state=[ 0.05245621 -0.1830293  -0.03413448  0.22297555]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 38 ] state=[ 0.05245621 -0.1830293  -0.03413448  0.22297555], action=0, reward=1.0, next_state=[ 0.04879562 -0.37764716 -0.02967497  0.5046987 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 39 ] state=[ 0.04879562 -0.37764716 -0.02967497  0.5046987 ], action=1, reward=1.0, next_state=[ 0.04124268 -0.18211984 -0.019581    0.20281369]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 40 ] state=[ 0.04124268 -0.18211984 -0.019581    0.20281369], action=1, reward=1.0, next_state=[ 0.03760028  0.01327659 -0.01552473 -0.09598122]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 41 ] state=[ 0.03760028  0.01327659 -0.01552473 -0.09598122], action=0, reward=1.0, next_state=[ 0.03786581 -0.18161945 -0.01744435  0.19176353]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 42 ] state=[ 0.03786581 -0.18161945 -0.01744435  0.19176353], action=1, reward=1.0, next_state=[ 0.03423343  0.01374765 -0.01360908 -0.10637092]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 43 ] state=[ 0.03423343  0.01374765 -0.01360908 -0.10637092], action=0, reward=1.0, next_state=[ 0.03450838 -0.18117666 -0.0157365   0.18198753]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 44 ] state=[ 0.03450838 -0.18117666 -0.0157365   0.18198753], action=0, reward=1.0, next_state=[ 0.03088485 -0.37606994 -0.01209675  0.46966491]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 45 ] state=[ 0.03088485 -0.37606994 -0.01209675  0.46966491], action=0, reward=1.0, next_state=[ 0.02336345 -0.57101894 -0.00270345  0.75851066]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 46 ] state=[ 0.02336345 -0.57101894 -0.00270345  0.75851066], action=1, reward=1.0, next_state=[ 0.01194307 -0.37585984  0.01246676  0.46497827]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 47 ] state=[ 0.01194307 -0.37585984  0.01246676  0.46497827], action=1, reward=1.0, next_state=[ 0.00442587 -0.18091625  0.02176633  0.17625074]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 48 ] state=[ 0.00442587 -0.18091625  0.02176633  0.17625074], action=0, reward=1.0, next_state=[ 0.00080755 -0.37634283  0.02529134  0.47571997]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 49 ] state=[ 0.00080755 -0.37634283  0.02529134  0.47571997], action=1, reward=1.0, next_state=[-0.00671931 -0.18158697  0.03480574  0.19111446]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 50 ] state=[-0.00671931 -0.18158697  0.03480574  0.19111446], action=1, reward=1.0, next_state=[-0.01035105  0.01302022  0.03862803 -0.09038869]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 51 ] state=[-0.01035105  0.01302022  0.03862803 -0.09038869], action=0, reward=1.0, next_state=[-0.01009065 -0.18263351  0.03682026  0.21422678]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 52 ] state=[-0.01009065 -0.18263351  0.03682026  0.21422678], action=1, reward=1.0, next_state=[-0.01374332  0.01194322  0.0411048  -0.06661777]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 53 ] state=[-0.01374332  0.01194322  0.0411048  -0.06661777], action=1, reward=1.0, next_state=[-0.01350445  0.20645248  0.03977244 -0.3460538 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 54 ] state=[-0.01350445  0.20645248  0.03977244 -0.3460538 ], action=0, reward=1.0, next_state=[-0.0093754   0.01078801  0.03285136 -0.0410992 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 233 ][ timestamp 55 ] state=[-0.0093754   0.01078801  0.03285136 -0.0410992 ], action=1, reward=1.0, next_state=[-0.00915964  0.20542385  0.03202938 -0.32323887]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 56 ] state=[-0.00915964  0.20542385  0.03202938 -0.32323887], action=0, reward=1.0, next_state=[-0.00505116  0.00986081  0.0255646  -0.02062973]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 57 ] state=[-0.00505116  0.00986081  0.0255646  -0.02062973], action=1, reward=1.0, next_state=[-0.00485395  0.20460699  0.02515201 -0.30513837]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 58 ] state=[-0.00485395  0.20460699  0.02515201 -0.30513837], action=0, reward=1.0, next_state=[-0.00076181  0.0091358   0.01904924 -0.00463042]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 59 ] state=[-0.00076181  0.0091358   0.01904924 -0.00463042], action=1, reward=1.0, next_state=[-0.00057909  0.20397945  0.01895663 -0.29124273]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 60 ] state=[-0.00057909  0.20397945  0.01895663 -0.29124273], action=0, reward=1.0, next_state=[0.0035005  0.00859241 0.01313178 0.00735809]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 61 ] state=[0.0035005  0.00859241 0.01313178 0.00735809], action=1, reward=1.0, next_state=[ 0.00367234  0.20352359  0.01327894 -0.28115284]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 62 ] state=[ 0.00367234  0.20352359  0.01327894 -0.28115284], action=1, reward=1.0, next_state=[ 0.00774282  0.39845363  0.00765588 -0.56961823]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 63 ] state=[ 0.00774282  0.39845363  0.00765588 -0.56961823], action=0, reward=1.0, next_state=[ 0.01571189  0.20322515 -0.00373648 -0.27453328]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 64 ] state=[ 0.01571189  0.20322515 -0.00373648 -0.27453328], action=0, reward=1.0, next_state=[ 0.01977639  0.00815671 -0.00922715  0.01696881]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 65 ] state=[ 0.01977639  0.00815671 -0.00922715  0.01696881], action=1, reward=1.0, next_state=[ 0.01993953  0.20340977 -0.00888777 -0.27861105]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 66 ] state=[ 0.01993953  0.20340977 -0.00888777 -0.27861105], action=0, reward=1.0, next_state=[ 0.02400772  0.00841573 -0.01445999  0.01125549]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 67 ] state=[ 0.02400772  0.00841573 -0.01445999  0.01125549], action=1, reward=1.0, next_state=[ 0.02417604  0.20374204 -0.01423488 -0.28595443]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 68 ] state=[ 0.02417604  0.20374204 -0.01423488 -0.28595443], action=1, reward=1.0, next_state=[ 0.02825088  0.39906408 -0.01995397 -0.58309273]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 69 ] state=[ 0.02825088  0.39906408 -0.01995397 -0.58309273], action=1, reward=1.0, next_state=[ 0.03623216  0.59445981 -0.03161583 -0.88199406]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 70 ] state=[ 0.03623216  0.59445981 -0.03161583 -0.88199406], action=1, reward=1.0, next_state=[ 0.04812135  0.78999659 -0.04925571 -1.18444616]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 71 ] state=[ 0.04812135  0.78999659 -0.04925571 -1.18444616], action=0, reward=1.0, next_state=[ 0.06392129  0.595547   -0.07294463 -0.90760084]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 72 ] state=[ 0.06392129  0.595547   -0.07294463 -0.90760084], action=0, reward=1.0, next_state=[ 0.07583223  0.40148432 -0.09109665 -0.63870764]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 73 ] state=[ 0.07583223  0.40148432 -0.09109665 -0.63870764], action=0, reward=1.0, next_state=[ 0.08386191  0.20774273 -0.1038708  -0.37604563]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 74 ] state=[ 0.08386191  0.20774273 -0.1038708  -0.37604563], action=1, reward=1.0, next_state=[ 0.08801677  0.40417483 -0.11139171 -0.69958884]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 75 ] state=[ 0.08801677  0.40417483 -0.11139171 -0.69958884], action=0, reward=1.0, next_state=[ 0.09610026  0.21075901 -0.12538349 -0.44394469]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 76 ] state=[ 0.09610026  0.21075901 -0.12538349 -0.44394469], action=0, reward=1.0, next_state=[ 0.10031544  0.01761343 -0.13426238 -0.1932669 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 77 ] state=[ 0.10031544  0.01761343 -0.13426238 -0.1932669 ], action=0, reward=1.0, next_state=[ 0.10066771 -0.17535793 -0.13812772  0.05423048]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 78 ] state=[ 0.10066771 -0.17535793 -0.13812772  0.05423048], action=0, reward=1.0, next_state=[ 0.09716055 -0.36825684 -0.13704311  0.30034241]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 79 ] state=[ 0.09716055 -0.36825684 -0.13704311  0.30034241], action=1, reward=1.0, next_state=[ 0.08979542 -0.17147476 -0.13103626 -0.03222793]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 80 ] state=[ 0.08979542 -0.17147476 -0.13103626 -0.03222793], action=1, reward=1.0, next_state=[ 0.08636592  0.02525911 -0.13168082 -0.36321336]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 81 ] state=[ 0.08636592  0.02525911 -0.13168082 -0.36321336], action=0, reward=1.0, next_state=[ 0.0868711  -0.1677695  -0.13894509 -0.1147795 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 82 ] state=[ 0.0868711  -0.1677695  -0.13894509 -0.1147795 ], action=0, reward=1.0, next_state=[ 0.08351571 -0.36065538 -0.14124068  0.13104241]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 83 ] state=[ 0.08351571 -0.36065538 -0.14124068  0.13104241], action=0, reward=1.0, next_state=[ 0.07630261 -0.5535013  -0.13861983  0.37604394]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 84 ] state=[ 0.07630261 -0.5535013  -0.13861983  0.37604394], action=1, reward=1.0, next_state=[ 0.06523258 -0.35671069 -0.13109895  0.04306572]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 85 ] state=[ 0.06523258 -0.35671069 -0.13109895  0.04306572], action=0, reward=1.0, next_state=[ 0.05809837 -0.54973284 -0.13023764  0.29168163]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 86 ] state=[ 0.05809837 -0.54973284 -0.13023764  0.29168163], action=1, reward=1.0, next_state=[ 0.04710371 -0.35301769 -0.12440401 -0.03907385]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 87 ] state=[ 0.04710371 -0.35301769 -0.12440401 -0.03907385], action=0, reward=1.0, next_state=[ 0.04004336 -0.54615645 -0.12518548  0.21191486]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 88 ] state=[ 0.04004336 -0.54615645 -0.12518548  0.21191486], action=0, reward=1.0, next_state=[ 0.02912023 -0.73928692 -0.12094719  0.46263508]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 89 ] state=[ 0.02912023 -0.73928692 -0.12094719  0.46263508], action=1, reward=1.0, next_state=[ 0.01433449 -0.54268188 -0.11169448  0.13441004]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 90 ] state=[ 0.01433449 -0.54268188 -0.11169448  0.13441004], action=0, reward=1.0, next_state=[ 0.00348085 -0.73604145 -0.10900628  0.38987211]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 91 ] state=[ 0.00348085 -0.73604145 -0.10900628  0.38987211], action=0, reward=1.0, next_state=[-0.01123998 -0.92946108 -0.10120884  0.64629513]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 92 ] state=[-0.01123998 -0.92946108 -0.10120884  0.64629513], action=0, reward=1.0, next_state=[-0.0298292  -1.12303802 -0.08828294  0.90547004]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 93 ] state=[-0.0298292  -1.12303802 -0.08828294  0.90547004], action=1, reward=1.0, next_state=[-0.05228996 -0.92683865 -0.07017354  0.58639563]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 94 ] state=[-0.05228996 -0.92683865 -0.07017354  0.58639563], action=0, reward=1.0, next_state=[-0.07082673 -1.12091122 -0.05844562  0.85617393]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 95 ] state=[-0.07082673 -1.12091122 -0.05844562  0.85617393], action=1, reward=1.0, next_state=[-0.09324496 -0.92504366 -0.04132215  0.54570101]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 96 ] state=[-0.09324496 -0.92504366 -0.04132215  0.54570101], action=1, reward=1.0, next_state=[-0.11174583 -0.7293662  -0.03040813  0.24029012]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 233 ][ timestamp 97 ] state=[-0.11174583 -0.7293662  -0.03040813  0.24029012], action=0, reward=1.0, next_state=[-0.12633315 -0.92404086 -0.02560232  0.52322851]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 98 ] state=[-0.12633315 -0.92404086 -0.02560232  0.52322851], action=1, reward=1.0, next_state=[-0.14481397 -0.72856811 -0.01513775  0.22258921]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 99 ] state=[-0.14481397 -0.72856811 -0.01513775  0.22258921], action=0, reward=1.0, next_state=[-0.15938533 -0.92347046 -0.01068597  0.51045891]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 100 ] state=[-0.15938533 -0.92347046 -0.01068597  0.51045891], action=0, reward=1.0, next_state=[-1.77854743e-01 -1.11844026e+00 -4.76790286e-04  7.99755298e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 101 ] state=[-1.77854743e-01 -1.11844026e+00 -4.76790286e-04  7.99755298e-01], action=1, reward=1.0, next_state=[-0.20022355 -0.92331177  0.01551832  0.50692242]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 102 ] state=[-0.20022355 -0.92331177  0.01551832  0.50692242], action=1, reward=1.0, next_state=[-0.21868978 -0.72841188  0.02565676  0.21916999]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 103 ] state=[-0.21868978 -0.72841188  0.02565676  0.21916999], action=0, reward=1.0, next_state=[-0.23325802 -0.92389101  0.03004016  0.51983445]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 104 ] state=[-0.23325802 -0.92389101  0.03004016  0.51983445], action=1, reward=1.0, next_state=[-0.25173584 -0.72920455  0.04043685  0.23676699]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 105 ] state=[-0.25173584 -0.72920455  0.04043685  0.23676699], action=0, reward=1.0, next_state=[-0.26631993 -0.92488019  0.04517219  0.5419257 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 106 ] state=[-0.26631993 -0.92488019  0.04517219  0.5419257 ], action=1, reward=1.0, next_state=[-0.28481754 -0.73042127  0.05601071  0.26381098]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 107 ] state=[-0.28481754 -0.73042127  0.05601071  0.26381098], action=1, reward=1.0, next_state=[-0.29942596 -0.53614166  0.06128693 -0.01069289]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 108 ] state=[-0.29942596 -0.53614166  0.06128693 -0.01069289], action=1, reward=1.0, next_state=[-0.31014879 -0.34194975  0.06107307 -0.28342679]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 109 ] state=[-0.31014879 -0.34194975  0.06107307 -0.28342679], action=0, reward=1.0, next_state=[-0.31698779 -0.53788721  0.05540453  0.02787577]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 110 ] state=[-0.31698779 -0.53788721  0.05540453  0.02787577], action=1, reward=1.0, next_state=[-0.32774553 -0.34360177  0.05596205 -0.24682462]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 111 ] state=[-0.32774553 -0.34360177  0.05596205 -0.24682462], action=0, reward=1.0, next_state=[-0.33461757 -0.53947646  0.05102556  0.06297172]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 112 ] state=[-0.33461757 -0.53947646  0.05102556  0.06297172], action=1, reward=1.0, next_state=[-0.3454071  -0.34512182  0.05228499 -0.2131858 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 113 ] state=[-0.3454071  -0.34512182  0.05228499 -0.2131858 ], action=1, reward=1.0, next_state=[-0.35230953 -0.15078486  0.04802127 -0.48892809]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 114 ] state=[-0.35230953 -0.15078486  0.04802127 -0.48892809], action=1, reward=1.0, next_state=[-0.35532523  0.04362787  0.03824271 -0.76609818]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 115 ] state=[-0.35532523  0.04362787  0.03824271 -0.76609818], action=0, reward=1.0, next_state=[-0.35445267 -0.15199919  0.02292075 -0.46163153]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 116 ] state=[-0.35445267 -0.15199919  0.02292075 -0.46163153], action=1, reward=1.0, next_state=[-0.35749266  0.04279144  0.01368812 -0.74700261]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 117 ] state=[-0.35749266  0.04279144  0.01368812 -0.74700261], action=1, reward=1.0, next_state=[-0.35663683  0.23772188 -0.00125193 -1.03534671]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 118 ] state=[-0.35663683  0.23772188 -0.00125193 -1.03534671], action=1, reward=1.0, next_state=[-0.35188239  0.43286046 -0.02195887 -1.32842241]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 119 ] state=[-0.35188239  0.43286046 -0.02195887 -1.32842241], action=0, reward=1.0, next_state=[-0.34322518  0.2380224  -0.04852732 -1.04269117]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 120 ] state=[-0.34322518  0.2380224  -0.04852732 -1.04269117], action=0, reward=1.0, next_state=[-0.33846473  0.04357734 -0.06938114 -0.76562836]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 121 ] state=[-0.33846473  0.04357734 -0.06938114 -0.76562836], action=0, reward=1.0, next_state=[-0.33759319 -0.15052417 -0.08469371 -0.49555829]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 122 ] state=[-0.33759319 -0.15052417 -0.08469371 -0.49555829], action=1, reward=1.0, next_state=[-0.34060367  0.04568361 -0.09460487 -0.81368523]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 123 ] state=[-0.34060367  0.04568361 -0.09460487 -0.81368523], action=0, reward=1.0, next_state=[-0.33969    -0.14802419 -0.11087858 -0.5521952 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 124 ] state=[-0.33969    -0.14802419 -0.11087858 -0.5521952 ], action=0, reward=1.0, next_state=[-0.34265048 -0.34142863 -0.12192248 -0.29640156]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 125 ] state=[-0.34265048 -0.34142863 -0.12192248 -0.29640156], action=1, reward=1.0, next_state=[-0.34947906 -0.14479864 -0.12785051 -0.62491352]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 126 ] state=[-0.34947906 -0.14479864 -0.12785051 -0.62491352], action=1, reward=1.0, next_state=[-0.35237503  0.05185445 -0.14034878 -0.95497135]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 127 ] state=[-0.35237503  0.05185445 -0.14034878 -0.95497135], action=0, reward=1.0, next_state=[-0.35133794 -0.14112916 -0.15944821 -0.70946948]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 128 ] state=[-0.35133794 -0.14112916 -0.15944821 -0.70946948], action=0, reward=1.0, next_state=[-0.35416052 -0.33372566 -0.1736376  -0.47091874]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 129 ] state=[-0.35416052 -0.33372566 -0.1736376  -0.47091874], action=1, reward=1.0, next_state=[-0.36083504 -0.13663108 -0.18305597 -0.8129083 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 130 ] state=[-0.36083504 -0.13663108 -0.18305597 -0.8129083 ], action=0, reward=1.0, next_state=[-0.36356766 -0.32883732 -0.19931414 -0.5829344 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ][ timestamp 131 ] state=[-0.36356766 -0.32883732 -0.19931414 -0.5829344 ], action=1, reward=-1.0, next_state=[-0.3701444  -0.13156354 -0.21097283 -0.93119796]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 233: Exploration_rate=0.01. Score=131.\n",
      "[ episode 234 ] state=[ 0.01541073  0.02670262 -0.04253533 -0.03401008]\n",
      "[ episode 234 ][ timestamp 1 ] state=[ 0.01541073  0.02670262 -0.04253533 -0.03401008], action=0, reward=1.0, next_state=[ 0.01594478 -0.16778437 -0.04321553  0.24495491]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 2 ] state=[ 0.01594478 -0.16778437 -0.04321553  0.24495491], action=1, reward=1.0, next_state=[ 0.01258909  0.02792733 -0.03831643 -0.06103996]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 3 ] state=[ 0.01258909  0.02792733 -0.03831643 -0.06103996], action=1, reward=1.0, next_state=[ 0.01314764  0.22357711 -0.03953723 -0.3655615 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 4 ] state=[ 0.01314764  0.22357711 -0.03953723 -0.3655615 ], action=0, reward=1.0, next_state=[ 0.01761918  0.02903868 -0.04684846 -0.08560281]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 5 ] state=[ 0.01761918  0.02903868 -0.04684846 -0.08560281], action=0, reward=1.0, next_state=[ 0.01819995 -0.16538152 -0.04856052  0.1919391 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 6 ] state=[ 0.01819995 -0.16538152 -0.04856052  0.1919391 ], action=1, reward=1.0, next_state=[ 0.01489232  0.03040027 -0.04472173 -0.11565857]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 7 ] state=[ 0.01489232  0.03040027 -0.04472173 -0.11565857], action=0, reward=1.0, next_state=[ 0.01550033 -0.16405331 -0.0470349   0.16258635]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 8 ] state=[ 0.01550033 -0.16405331 -0.0470349   0.16258635], action=0, reward=1.0, next_state=[ 0.01221926 -0.35847147 -0.04378318  0.44006791]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 9 ] state=[ 0.01221926 -0.35847147 -0.04378318  0.44006791], action=1, reward=1.0, next_state=[ 0.00504983 -0.16275811 -0.03498182  0.13391106]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 10 ] state=[ 0.00504983 -0.16275811 -0.03498182  0.13391106], action=1, reward=1.0, next_state=[ 0.00179467  0.032847   -0.0323036  -0.16959966]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 234 ][ timestamp 11 ] state=[ 0.00179467  0.032847   -0.0323036  -0.16959966], action=1, reward=1.0, next_state=[ 0.00245161  0.22841608 -0.03569559 -0.47229583]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 12 ] state=[ 0.00245161  0.22841608 -0.03569559 -0.47229583], action=0, reward=1.0, next_state=[ 0.00701993  0.03381599 -0.04514151 -0.19107392]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 13 ] state=[ 0.00701993  0.03381599 -0.04514151 -0.19107392], action=0, reward=1.0, next_state=[ 0.00769625 -0.16063208 -0.04896299  0.08703396]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 14 ] state=[ 0.00769625 -0.16063208 -0.04896299  0.08703396], action=1, reward=1.0, next_state=[ 0.00448361  0.03515627 -0.04722231 -0.22068597]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 15 ] state=[ 0.00448361  0.03515627 -0.04722231 -0.22068597], action=0, reward=1.0, next_state=[ 0.00518674 -0.15925999 -0.05163603  0.05673514]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 16 ] state=[ 0.00518674 -0.15925999 -0.05163603  0.05673514], action=1, reward=1.0, next_state=[ 0.00200154  0.03656285 -0.05050132 -0.25178187]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 17 ] state=[ 0.00200154  0.03656285 -0.05050132 -0.25178187], action=0, reward=1.0, next_state=[ 0.00273279 -0.15780295 -0.05553696  0.02455405]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 18 ] state=[ 0.00273279 -0.15780295 -0.05553696  0.02455405], action=1, reward=1.0, next_state=[-0.00042327  0.03806965 -0.05504588 -0.28512134]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 19 ] state=[-0.00042327  0.03806965 -0.05504588 -0.28512134], action=0, reward=1.0, next_state=[ 0.00033813 -0.15622579 -0.06074831 -0.01029493]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 20 ] state=[ 0.00033813 -0.15622579 -0.06074831 -0.01029493], action=0, reward=1.0, next_state=[-0.00278639 -0.35042629 -0.06095421  0.26261947]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 21 ] state=[-0.00278639 -0.35042629 -0.06095421  0.26261947], action=0, reward=1.0, next_state=[-0.00979491 -0.5446276  -0.05570182  0.53547101]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 22 ] state=[-0.00979491 -0.5446276  -0.05570182  0.53547101], action=0, reward=1.0, next_state=[-0.02068747 -0.73892387 -0.0449924   0.81009552]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 23 ] state=[-0.02068747 -0.73892387 -0.0449924   0.81009552], action=1, reward=1.0, next_state=[-0.03546594 -0.5432153  -0.02879049  0.50360645]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 24 ] state=[-0.03546594 -0.5432153  -0.02879049  0.50360645], action=0, reward=1.0, next_state=[-0.04633025 -0.73791989 -0.01871836  0.78707908]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 25 ] state=[-0.04633025 -0.73791989 -0.01871836  0.78707908], action=1, reward=1.0, next_state=[-0.06108865 -0.54254588 -0.00297678  0.48856652]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 26 ] state=[-0.06108865 -0.54254588 -0.00297678  0.48856652], action=0, reward=1.0, next_state=[-0.07193956 -0.73762571  0.00679456  0.78030979]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 27 ] state=[-0.07193956 -0.73762571  0.00679456  0.78030979], action=1, reward=1.0, next_state=[-0.08669208 -0.54259782  0.02240075  0.4897723 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 28 ] state=[-0.08669208 -0.54259782  0.02240075  0.4897723 ], action=1, reward=1.0, next_state=[-0.09754404 -0.34779893  0.0321962   0.20423254]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 29 ] state=[-0.09754404 -0.34779893  0.0321962   0.20423254], action=1, reward=1.0, next_state=[-0.10450001 -0.15315185  0.03628085 -0.07812272]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 30 ] state=[-0.10450001 -0.15315185  0.03628085 -0.07812272], action=1, reward=1.0, next_state=[-0.10756305  0.04143172  0.03471839 -0.35914177]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 31 ] state=[-0.10756305  0.04143172  0.03471839 -0.35914177], action=0, reward=1.0, next_state=[-0.10673442 -0.15416613  0.02753556 -0.05571664]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 32 ] state=[-0.10673442 -0.15416613  0.02753556 -0.05571664], action=0, reward=1.0, next_state=[-0.10981774 -0.34967185  0.02642123  0.24552521]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 33 ] state=[-0.10981774 -0.34967185  0.02642123  0.24552521], action=1, reward=1.0, next_state=[-0.11681118 -0.15493703  0.03133173 -0.03870813]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 34 ] state=[-0.11681118 -0.15493703  0.03133173 -0.03870813], action=1, reward=1.0, next_state=[-0.11990992  0.03972194  0.03055757 -0.32134326]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 35 ] state=[-0.11990992  0.03972194  0.03055757 -0.32134326], action=1, reward=1.0, next_state=[-0.11911548  0.2343957   0.0241307  -0.60423506]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 36 ] state=[-0.11911548  0.2343957   0.0241307  -0.60423506], action=0, reward=1.0, next_state=[-0.11442756  0.03894473  0.012046   -0.30405022]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 37 ] state=[-0.11442756  0.03894473  0.012046   -0.30405022], action=1, reward=1.0, next_state=[-0.11364867  0.23389296  0.005965   -0.59290991]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 38 ] state=[-0.11364867  0.23389296  0.005965   -0.59290991], action=0, reward=1.0, next_state=[-0.10897081  0.03868802 -0.0058932  -0.29835401]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 39 ] state=[-0.10897081  0.03868802 -0.0058932  -0.29835401], action=0, reward=1.0, next_state=[-0.10819705 -0.15634943 -0.01186028 -0.0075355 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 40 ] state=[-0.10819705 -0.15634943 -0.01186028 -0.0075355 ], action=0, reward=1.0, next_state=[-0.11132404 -0.3512993  -0.01201099  0.28138189]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 41 ] state=[-0.11132404 -0.3512993  -0.01201099  0.28138189], action=1, reward=1.0, next_state=[-0.11835002 -0.1560081  -0.00638335 -0.01506493]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 42 ] state=[-0.11835002 -0.1560081  -0.00638335 -0.01506493], action=0, reward=1.0, next_state=[-0.12147019 -0.35103793 -0.00668465  0.27559716]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 43 ] state=[-0.12147019 -0.35103793 -0.00668465  0.27559716], action=0, reward=1.0, next_state=[-0.12849094 -0.54606387 -0.00117271  0.56616427]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 44 ] state=[-0.12849094 -0.54606387 -0.00117271  0.56616427], action=1, reward=1.0, next_state=[-0.13941222 -0.35092549  0.01015058  0.27311212]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 45 ] state=[-0.13941222 -0.35092549  0.01015058  0.27311212], action=1, reward=1.0, next_state=[-0.14643073 -0.15594983  0.01561282 -0.01635208]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 46 ] state=[-0.14643073 -0.15594983  0.01561282 -0.01635208], action=1, reward=1.0, next_state=[-0.14954973  0.03894477  0.01528578 -0.30406838]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 47 ] state=[-0.14954973  0.03894477  0.01528578 -0.30406838], action=1, reward=1.0, next_state=[-0.14877083  0.23384559  0.00920441 -0.5918916 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 48 ] state=[-0.14877083  0.23384559  0.00920441 -0.5918916 ], action=1, reward=1.0, next_state=[-0.14409392  0.42883748 -0.00263342 -0.88166099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 49 ] state=[-0.14409392  0.42883748 -0.00263342 -0.88166099], action=1, reward=1.0, next_state=[-0.13551717  0.6239951  -0.02026664 -1.17517063]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 50 ] state=[-0.13551717  0.6239951  -0.02026664 -1.17517063], action=1, reward=1.0, next_state=[-0.12303727  0.81937445 -0.04377006 -1.47413746]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 51 ] state=[-0.12303727  0.81937445 -0.04377006 -1.47413746], action=0, reward=1.0, next_state=[-0.10664978  0.62481391 -0.07325281 -1.19544045]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 52 ] state=[-0.10664978  0.62481391 -0.07325281 -1.19544045], action=1, reward=1.0, next_state=[-0.0941535   0.82080381 -0.09716161 -1.51015396]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 53 ] state=[-0.0941535   0.82080381 -0.09716161 -1.51015396], action=0, reward=1.0, next_state=[-0.07773743  0.62698417 -0.12736469 -1.24931631]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 54 ] state=[-0.07773743  0.62698417 -0.12736469 -1.24931631], action=1, reward=1.0, next_state=[-0.06519774  0.82348762 -0.15235102 -1.57902807]\n",
      "[ Experience replay ] starts\n",
      "[ episode 234 ][ timestamp 55 ] state=[-0.06519774  0.82348762 -0.15235102 -1.57902807], action=1, reward=1.0, next_state=[-0.04872799  1.02006042 -0.18393158 -1.91509004]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 234 ][ timestamp 56 ] state=[-0.04872799  1.02006042 -0.18393158 -1.91509004], action=1, reward=-1.0, next_state=[-0.02832678  1.21662558 -0.22223338 -2.25873585]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 234: Exploration_rate=0.01. Score=56.\n",
      "[ episode 235 ] state=[-0.00834208  0.02232874  0.0339267   0.04692574]\n",
      "[ episode 235 ][ timestamp 1 ] state=[-0.00834208  0.02232874  0.0339267   0.04692574], action=1, reward=1.0, next_state=[-0.0078955   0.2169482   0.03486521 -0.23486292]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 2 ] state=[-0.0078955   0.2169482   0.03486521 -0.23486292], action=1, reward=1.0, next_state=[-0.00355654  0.41155511  0.03016796 -0.51634759]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 3 ] state=[-0.00355654  0.41155511  0.03016796 -0.51634759], action=1, reward=1.0, next_state=[ 0.00467456  0.60623955  0.019841   -0.79937334]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 4 ] state=[ 0.00467456  0.60623955  0.019841   -0.79937334], action=1, reward=1.0, next_state=[ 0.01679935  0.80108379  0.00385354 -1.0857493 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 5 ] state=[ 0.01679935  0.80108379  0.00385354 -1.0857493 ], action=0, reward=1.0, next_state=[ 0.03282103  0.60591122 -0.01786145 -0.79185968]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 6 ] state=[ 0.03282103  0.60591122 -0.01786145 -0.79185968], action=0, reward=1.0, next_state=[ 0.04493925  0.41103899 -0.03369864 -0.50484895]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 7 ] state=[ 0.04493925  0.41103899 -0.03369864 -0.50484895], action=0, reward=1.0, next_state=[ 0.05316003  0.21640775 -0.04379562 -0.22297337]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 8 ] state=[ 0.05316003  0.21640775 -0.04379562 -0.22297337], action=0, reward=1.0, next_state=[ 0.05748819  0.02193824 -0.04825509  0.05557939]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 9 ] state=[ 0.05748819  0.02193824 -0.04825509  0.05557939], action=0, reward=1.0, next_state=[ 0.05792695 -0.17245978 -0.0471435   0.3326555 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 10 ] state=[ 0.05792695 -0.17245978 -0.0471435   0.3326555 ], action=1, reward=1.0, next_state=[ 0.05447776  0.02330037 -0.04049039  0.02548646]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 11 ] state=[ 0.05447776  0.02330037 -0.04049039  0.02548646], action=1, reward=1.0, next_state=[ 0.05494377  0.2189789  -0.03998066 -0.27969167]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 12 ] state=[ 0.05494377  0.2189789  -0.03998066 -0.27969167], action=0, reward=1.0, next_state=[ 0.05932334  0.02444941 -0.0455745   0.0001182 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 13 ] state=[ 0.05932334  0.02444941 -0.0455745   0.0001182 ], action=1, reward=1.0, next_state=[ 0.05981233  0.22019433 -0.04557213 -0.30658857]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 14 ] state=[ 0.05981233  0.22019433 -0.04557213 -0.30658857], action=1, reward=1.0, next_state=[ 0.06421622  0.41593504 -0.0517039  -0.61328838]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 15 ] state=[ 0.06421622  0.41593504 -0.0517039  -0.61328838], action=0, reward=1.0, next_state=[ 0.07253492  0.2215723  -0.06396967 -0.33732805]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 16 ] state=[ 0.07253492  0.2215723  -0.06396967 -0.33732805], action=1, reward=1.0, next_state=[ 0.07696637  0.41754348 -0.07071623 -0.64947782]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 17 ] state=[ 0.07696637  0.41754348 -0.07071623 -0.64947782], action=0, reward=1.0, next_state=[ 0.08531723  0.22347414 -0.08370579 -0.37987463]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 18 ] state=[ 0.08531723  0.22347414 -0.08370579 -0.37987463], action=0, reward=1.0, next_state=[ 0.08978672  0.02963443 -0.09130328 -0.11471387]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 19 ] state=[ 0.08978672  0.02963443 -0.09130328 -0.11471387], action=0, reward=1.0, next_state=[ 0.09037941 -0.16406871 -0.09359756  0.14782472]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 20 ] state=[ 0.09037941 -0.16406871 -0.09359756  0.14782472], action=1, reward=1.0, next_state=[ 0.08709803  0.03226026 -0.09064106 -0.17285725]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 21 ] state=[ 0.08709803  0.03226026 -0.09064106 -0.17285725], action=1, reward=1.0, next_state=[ 0.08774324  0.22855471 -0.09409821 -0.49270221]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 22 ] state=[ 0.08774324  0.22855471 -0.09409821 -0.49270221], action=1, reward=1.0, next_state=[ 0.09231433  0.42486915 -0.10395225 -0.8134952 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 23 ] state=[ 0.09231433  0.42486915 -0.10395225 -0.8134952 ], action=0, reward=1.0, next_state=[ 0.10081171  0.23131281 -0.12022216 -0.55523491]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 24 ] state=[ 0.10081171  0.23131281 -0.12022216 -0.55523491], action=1, reward=1.0, next_state=[ 0.10543797  0.42789938 -0.13132686 -0.88324656]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 25 ] state=[ 0.10543797  0.42789938 -0.13132686 -0.88324656], action=0, reward=1.0, next_state=[ 0.11399596  0.23478191 -0.14899179 -0.63456396]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 26 ] state=[ 0.11399596  0.23478191 -0.14899179 -0.63456396], action=0, reward=1.0, next_state=[ 0.1186916   0.04201772 -0.16168307 -0.39226276]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 27 ] state=[ 0.1186916   0.04201772 -0.16168307 -0.39226276], action=0, reward=1.0, next_state=[ 0.11953195 -0.15048481 -0.16952832 -0.15460296]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 28 ] state=[ 0.11953195 -0.15048481 -0.16952832 -0.15460296], action=0, reward=1.0, next_state=[ 0.11652225 -0.34282511 -0.17262038  0.08016861]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 29 ] state=[ 0.11652225 -0.34282511 -0.17262038  0.08016861], action=1, reward=1.0, next_state=[ 0.10966575 -0.14570303 -0.17101701 -0.26161881]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 30 ] state=[ 0.10966575 -0.14570303 -0.17101701 -0.26161881], action=0, reward=1.0, next_state=[ 0.10675169 -0.33802369 -0.17624938 -0.02738041]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 31 ] state=[ 0.10675169 -0.33802369 -0.17624938 -0.02738041], action=1, reward=1.0, next_state=[ 0.09999122 -0.14086971 -0.17679699 -0.37007943]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 32 ] state=[ 0.09999122 -0.14086971 -0.17679699 -0.37007943], action=0, reward=1.0, next_state=[ 0.09717382 -0.33309706 -0.18419858 -0.13794101]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 33 ] state=[ 0.09717382 -0.33309706 -0.18419858 -0.13794101], action=0, reward=1.0, next_state=[ 0.09051188 -0.52516861 -0.1869574   0.09144384]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 34 ] state=[ 0.09051188 -0.52516861 -0.1869574   0.09144384], action=0, reward=1.0, next_state=[ 0.08000851 -0.71718748 -0.18512852  0.31980722]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 35 ] state=[ 0.08000851 -0.71718748 -0.18512852  0.31980722], action=0, reward=1.0, next_state=[ 0.06566476 -0.90925685 -0.17873238  0.5488709 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 36 ] state=[ 0.06566476 -0.90925685 -0.17873238  0.5488709 ], action=0, reward=1.0, next_state=[ 0.04747962 -1.10147747 -0.16775496  0.78034068]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 37 ] state=[ 0.04747962 -1.10147747 -0.16775496  0.78034068], action=1, reward=1.0, next_state=[ 0.02545007 -0.90449556 -0.15214815  0.43992668]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 38 ] state=[ 0.02545007 -0.90449556 -0.15214815  0.43992668], action=0, reward=1.0, next_state=[ 0.00736016 -1.09717377 -0.14334961  0.68104603]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 39 ] state=[ 0.00736016 -1.09717377 -0.14334961  0.68104603], action=0, reward=1.0, next_state=[-0.01458331 -1.29004455 -0.12972869  0.9253842 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 40 ] state=[-0.01458331 -1.29004455 -0.12972869  0.9253842 ], action=0, reward=1.0, next_state=[-0.0403842  -1.48319836 -0.11122101  1.17464697]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 41 ] state=[-0.0403842  -1.48319836 -0.11122101  1.17464697], action=0, reward=1.0, next_state=[-0.07004817 -1.67671334 -0.08772807  1.43049433]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 235 ][ timestamp 42 ] state=[-0.07004817 -1.67671334 -0.08772807  1.43049433], action=0, reward=1.0, next_state=[-0.10358244 -1.87064957 -0.05911818  1.69452099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 43 ] state=[-0.10358244 -1.87064957 -0.05911818  1.69452099], action=0, reward=1.0, next_state=[-0.14099543 -2.06504152 -0.02522776  1.96822888]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 44 ] state=[-0.14099543 -2.06504152 -0.02522776  1.96822888], action=0, reward=1.0, next_state=[-0.18229626 -2.25988808  0.01413681  2.25298956]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 45 ] state=[-0.18229626 -2.25988808  0.01413681  2.25298956], action=1, reward=1.0, next_state=[-0.22749402 -2.06490169  0.05919661  1.96469528]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 46 ] state=[-0.22749402 -2.06490169  0.05919661  1.96469528], action=1, reward=1.0, next_state=[-0.26879205 -1.87045373  0.09849051  1.69092787]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 47 ] state=[-0.26879205 -1.87045373  0.09849051  1.69092787], action=1, reward=1.0, next_state=[-0.30620113 -1.67659801  0.13230907  1.43046293]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 48 ] state=[-0.30620113 -1.67659801  0.13230907  1.43046293], action=1, reward=1.0, next_state=[-0.33973309 -1.48333425  0.16091833  1.18188648]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 49 ] state=[-0.33973309 -1.48333425  0.16091833  1.18188648], action=1, reward=1.0, next_state=[-0.36939977 -1.29062411  0.18455606  0.9436619 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 50 ] state=[-0.36939977 -1.29062411  0.18455606  0.9436619 ], action=1, reward=1.0, next_state=[-0.39521226 -1.09840349  0.20342929  0.71417944]\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ][ timestamp 51 ] state=[-0.39521226 -1.09840349  0.20342929  0.71417944], action=0, reward=-1.0, next_state=[-0.41718033 -1.29567341  0.21771288  1.06337917]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 235: Exploration_rate=0.01. Score=51.\n",
      "[ episode 236 ] state=[-0.02177098  0.02255062 -0.02459229 -0.01149009]\n",
      "[ episode 236 ][ timestamp 1 ] state=[-0.02177098  0.02255062 -0.02459229 -0.01149009], action=1, reward=1.0, next_state=[-0.02131997  0.21801647 -0.02482209 -0.31182961]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 2 ] state=[-0.02131997  0.21801647 -0.02482209 -0.31182961], action=0, reward=1.0, next_state=[-0.01695964  0.02325678 -0.03105868 -0.02707701]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 3 ] state=[-0.01695964  0.02325678 -0.03105868 -0.02707701], action=1, reward=1.0, next_state=[-0.01649451  0.21881005 -0.03160022 -0.32939524]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 4 ] state=[-0.01649451  0.21881005 -0.03160022 -0.32939524], action=0, reward=1.0, next_state=[-0.01211831  0.02415187 -0.03818813 -0.04684265]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 5 ] state=[-0.01211831  0.02415187 -0.03818813 -0.04684265], action=1, reward=1.0, next_state=[-0.01163527  0.21980001 -0.03912498 -0.35132548]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 6 ] state=[-0.01163527  0.21980001 -0.03912498 -0.35132548], action=0, reward=1.0, next_state=[-0.00723927  0.02525566 -0.04615149 -0.0712321 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 7 ] state=[-0.00723927  0.02525566 -0.04615149 -0.0712321 ], action=0, reward=1.0, next_state=[-0.00673415 -0.16917529 -0.04757613  0.20654008]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 8 ] state=[-0.00673415 -0.16917529 -0.04757613  0.20654008], action=0, reward=1.0, next_state=[-0.01011766 -0.36358577 -0.04344533  0.48384371]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 9 ] state=[-0.01011766 -0.36358577 -0.04344533  0.48384371], action=1, reward=1.0, next_state=[-0.01738938 -0.16787846 -0.03376845  0.17779085]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 10 ] state=[-0.01738938 -0.16787846 -0.03376845  0.17779085], action=1, reward=1.0, next_state=[-0.02074695  0.02771006 -0.03021264 -0.12535071]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 11 ] state=[-0.02074695  0.02771006 -0.03021264 -0.12535071], action=0, reward=1.0, next_state=[-0.02019274 -0.16696633 -0.03271965  0.15764944]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 12 ] state=[-0.02019274 -0.16696633 -0.03271965  0.15764944], action=1, reward=1.0, next_state=[-0.02353207  0.02860843 -0.02956666 -0.14517354]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 13 ] state=[-0.02353207  0.02860843 -0.02956666 -0.14517354], action=0, reward=1.0, next_state=[-0.0229599  -0.1660779  -0.03247013  0.13803699]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 14 ] state=[-0.0229599  -0.1660779  -0.03247013  0.13803699], action=0, reward=1.0, next_state=[-0.02628146 -0.3607201  -0.02970939  0.42030185]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 15 ] state=[-0.02628146 -0.3607201  -0.02970939  0.42030185], action=1, reward=1.0, next_state=[-0.03349586 -0.16519007 -0.02130336  0.11840295]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 16 ] state=[-0.03349586 -0.16519007 -0.02130336  0.11840295], action=0, reward=1.0, next_state=[-0.03679966 -0.36000042 -0.0189353   0.40428945]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 17 ] state=[-0.03679966 -0.36000042 -0.0189353   0.40428945], action=1, reward=1.0, next_state=[-0.04399967 -0.16461512 -0.01084951  0.10569739]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 18 ] state=[-0.04399967 -0.16461512 -0.01084951  0.10569739], action=1, reward=1.0, next_state=[-0.04729197  0.03066062 -0.00873556 -0.19038866]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 19 ] state=[-0.04729197  0.03066062 -0.00873556 -0.19038866], action=1, reward=1.0, next_state=[-0.04667876  0.22590645 -0.01254333 -0.48581445]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 20 ] state=[-0.04667876  0.22590645 -0.01254333 -0.48581445], action=0, reward=1.0, next_state=[-0.04216063  0.03096372 -0.02225962 -0.19711101]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 21 ] state=[-0.04216063  0.03096372 -0.02225962 -0.19711101], action=1, reward=1.0, next_state=[-0.04154136  0.22639688 -0.02620184 -0.49673191]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 22 ] state=[-0.04154136  0.22639688 -0.02620184 -0.49673191], action=0, reward=1.0, next_state=[-0.03701342  0.031654   -0.03613648 -0.21242032]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 23 ] state=[-0.03701342  0.031654   -0.03613648 -0.21242032], action=0, reward=1.0, next_state=[-0.03638034 -0.16293316 -0.04038489  0.06864805]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 24 ] state=[-0.03638034 -0.16293316 -0.04038489  0.06864805], action=0, reward=1.0, next_state=[-0.039639   -0.35745355 -0.03901193  0.34832079]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 25 ] state=[-0.039639   -0.35745355 -0.03901193  0.34832079], action=1, reward=1.0, next_state=[-0.04678808 -0.16179907 -0.03204551  0.04359578]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 26 ] state=[-0.04678808 -0.16179907 -0.03204551  0.04359578], action=1, reward=1.0, next_state=[-0.05002406  0.0337674  -0.0311736  -0.25902308]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 27 ] state=[-0.05002406  0.0337674  -0.0311736  -0.25902308], action=1, reward=1.0, next_state=[-0.04934871  0.2293202  -0.03635406 -0.56137331]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 28 ] state=[-0.04934871  0.2293202  -0.03635406 -0.56137331], action=1, reward=1.0, next_state=[-0.0447623   0.42493298 -0.04758152 -0.86528435]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 29 ] state=[-0.0447623   0.42493298 -0.04758152 -0.86528435], action=1, reward=1.0, next_state=[-0.03626365  0.62066914 -0.06488721 -1.17253999]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 30 ] state=[-0.03626365  0.62066914 -0.06488721 -1.17253999], action=0, reward=1.0, next_state=[-0.02385026  0.42644794 -0.08833801 -0.90088473]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 31 ] state=[-0.02385026  0.42644794 -0.08833801 -0.90088473], action=0, reward=1.0, next_state=[-0.0153213   0.23262683 -0.10635571 -0.63722432]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 236 ][ timestamp 32 ] state=[-0.0153213   0.23262683 -0.10635571 -0.63722432], action=0, reward=1.0, next_state=[-0.01066877  0.03913619 -0.11910019 -0.37983798]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 33 ] state=[-0.01066877  0.03913619 -0.11910019 -0.37983798], action=1, reward=1.0, next_state=[-0.00988604  0.23573029 -0.12669695 -0.70757285]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 34 ] state=[-0.00988604  0.23573029 -0.12669695 -0.70757285], action=1, reward=1.0, next_state=[-0.00517144  0.43235845 -0.14084841 -1.03730035]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 35 ] state=[-0.00517144  0.43235845 -0.14084841 -1.03730035], action=1, reward=1.0, next_state=[ 0.00347573  0.62904277 -0.16159442 -1.37067792]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 36 ] state=[ 0.00347573  0.62904277 -0.16159442 -1.37067792], action=1, reward=1.0, next_state=[ 0.01605659  0.82577455 -0.18900797 -1.70923332]\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ][ timestamp 37 ] state=[ 0.01605659  0.82577455 -0.18900797 -1.70923332], action=0, reward=-1.0, next_state=[ 0.03257208  0.6332609  -0.22319264 -1.48084361]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 236: Exploration_rate=0.01. Score=37.\n",
      "[ episode 237 ] state=[-0.02140228  0.02868376  0.02616207  0.02768414]\n",
      "[ episode 237 ][ timestamp 1 ] state=[-0.02140228  0.02868376  0.02616207  0.02768414], action=1, reward=1.0, next_state=[-0.0208286   0.22342095  0.02671575 -0.25663091]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 2 ] state=[-0.0208286   0.22342095  0.02671575 -0.25663091], action=0, reward=1.0, next_state=[-0.01636018  0.02792796  0.02158313  0.04435742]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 3 ] state=[-0.01636018  0.02792796  0.02158313  0.04435742], action=1, reward=1.0, next_state=[-0.01580162  0.22273388  0.02247028 -0.24143844]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 4 ] state=[-0.01580162  0.22273388  0.02247028 -0.24143844], action=1, reward=1.0, next_state=[-0.01134695  0.41752777  0.01764151 -0.5269498 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 5 ] state=[-0.01134695  0.41752777  0.01764151 -0.5269498 ], action=0, reward=1.0, next_state=[-0.00299639  0.22216209  0.00710252 -0.22876055]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 6 ] state=[-0.00299639  0.22216209  0.00710252 -0.22876055], action=0, reward=1.0, next_state=[0.00144685 0.02693936 0.00252731 0.06615427]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 7 ] state=[0.00144685 0.02693936 0.00252731 0.06615427], action=1, reward=1.0, next_state=[ 0.00198564  0.22202499  0.00385039 -0.22573021]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 8 ] state=[ 0.00198564  0.22202499  0.00385039 -0.22573021], action=0, reward=1.0, next_state=[ 0.00642614  0.02684822 -0.00066421  0.06816479]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 9 ] state=[ 0.00642614  0.02684822 -0.00066421  0.06816479], action=0, reward=1.0, next_state=[ 0.0069631  -0.1682642   0.00069908  0.36063808]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 10 ] state=[ 0.0069631  -0.1682642   0.00069908  0.36063808], action=1, reward=1.0, next_state=[0.00359782 0.02684781 0.00791184 0.06817567]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 11 ] state=[0.00359782 0.02684781 0.00791184 0.06817567], action=1, reward=1.0, next_state=[ 0.00413477  0.22185544  0.00927536 -0.22200057]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 12 ] state=[ 0.00413477  0.22185544  0.00927536 -0.22200057], action=0, reward=1.0, next_state=[0.00857188 0.02660215 0.00483535 0.07359369]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 13 ] state=[0.00857188 0.02660215 0.00483535 0.07359369], action=1, reward=1.0, next_state=[ 0.00910393  0.22165445  0.00630722 -0.21755975]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 14 ] state=[ 0.00910393  0.22165445  0.00630722 -0.21755975], action=0, reward=1.0, next_state=[0.01353701 0.0264429  0.00195602 0.07710605]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 15 ] state=[0.01353701 0.0264429  0.00195602 0.07710605], action=0, reward=1.0, next_state=[ 0.01406587 -0.16870704  0.00349815  0.37040547]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 16 ] state=[ 0.01406587 -0.16870704  0.00349815  0.37040547], action=1, reward=1.0, next_state=[0.01069173 0.02636504 0.01090625 0.07882759]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 17 ] state=[0.01069173 0.02636504 0.01090625 0.07882759], action=0, reward=1.0, next_state=[ 0.01121903 -0.16891154  0.01248281  0.37493143]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 18 ] state=[ 0.01121903 -0.16891154  0.01248281  0.37493143], action=1, reward=1.0, next_state=[0.0078408  0.02603089 0.01998144 0.0862104 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 19 ] state=[0.0078408  0.02603089 0.01998144 0.0862104 ], action=1, reward=1.0, next_state=[ 0.00836142  0.22086081  0.02170564 -0.20010199]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 20 ] state=[ 0.00836142  0.22086081  0.02170564 -0.20010199], action=1, reward=1.0, next_state=[ 0.01277864  0.4156657   0.0177036  -0.48585953]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 21 ] state=[ 0.01277864  0.4156657   0.0177036  -0.48585953], action=1, reward=1.0, next_state=[ 0.02109195  0.61053342  0.00798641 -0.77291072]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 22 ] state=[ 0.02109195  0.61053342  0.00798641 -0.77291072], action=1, reward=1.0, next_state=[ 0.03330262  0.80554458 -0.0074718  -1.06307015]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 23 ] state=[ 0.03330262  0.80554458 -0.0074718  -1.06307015], action=0, reward=1.0, next_state=[ 0.04941351  0.61052234 -0.0287332  -0.77274165]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 24 ] state=[ 0.04941351  0.61052234 -0.0287332  -0.77274165], action=0, reward=1.0, next_state=[ 0.06162396  0.41580727 -0.04418804 -0.48923599]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 25 ] state=[ 0.06162396  0.41580727 -0.04418804 -0.48923599], action=1, reward=1.0, next_state=[ 0.0699401   0.61152385 -0.05397276 -0.79551136]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 26 ] state=[ 0.0699401   0.61152385 -0.05397276 -0.79551136], action=1, reward=1.0, next_state=[ 0.08217058  0.80734335 -0.06988298 -1.10467316]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 27 ] state=[ 0.08217058  0.80734335 -0.06988298 -1.10467316], action=0, reward=1.0, next_state=[ 0.09831745  0.61320649 -0.09197645 -0.83470753]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 28 ] state=[ 0.09831745  0.61320649 -0.09197645 -0.83470753], action=1, reward=1.0, next_state=[ 0.11058158  0.80945652 -0.1086706  -1.15484127]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 29 ] state=[ 0.11058158  0.80945652 -0.1086706  -1.15484127], action=0, reward=1.0, next_state=[ 0.12677071  0.6159063  -0.13176742 -0.89811482]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 30 ] state=[ 0.12677071  0.6159063  -0.13176742 -0.89811482], action=0, reward=1.0, next_state=[ 0.13908883  0.42279281 -0.14972972 -0.6495833 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 31 ] state=[ 0.13908883  0.42279281 -0.14972972 -0.6495833 ], action=0, reward=1.0, next_state=[ 0.14754469  0.23003869 -0.16272139 -0.40754332]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 32 ] state=[ 0.14754469  0.23003869 -0.16272139 -0.40754332], action=0, reward=1.0, next_state=[ 0.15214546  0.03755282 -0.17087225 -0.17025785]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 33 ] state=[ 0.15214546  0.03755282 -0.17087225 -0.17025785], action=0, reward=1.0, next_state=[ 0.15289652 -0.15476392 -0.17427741  0.06402381]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 34 ] state=[ 0.15289652 -0.15476392 -0.17427741  0.06402381], action=0, reward=1.0, next_state=[ 0.14980124 -0.34701453 -0.17299693  0.29705285]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 35 ] state=[ 0.14980124 -0.34701453 -0.17299693  0.29705285], action=1, reward=1.0, next_state=[ 0.14286095 -0.14990293 -0.16705588 -0.044809  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 36 ] state=[ 0.14286095 -0.14990293 -0.16705588 -0.044809  ], action=0, reward=1.0, next_state=[ 0.13986289 -0.3422845  -0.16795206  0.19085972]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 37 ] state=[ 0.13986289 -0.3422845  -0.16795206  0.19085972], action=0, reward=1.0, next_state=[ 0.1330172  -0.53465542 -0.16413486  0.42620979]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 38 ] state=[ 0.1330172  -0.53465542 -0.16413486  0.42620979], action=1, reward=1.0, next_state=[ 0.12232409 -0.33763544 -0.15561067  0.08661244]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 39 ] state=[ 0.12232409 -0.33763544 -0.15561067  0.08661244], action=0, reward=1.0, next_state=[ 0.11557138 -0.53022421 -0.15387842  0.32643992]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 40 ] state=[ 0.11557138 -0.53022421 -0.15387842  0.32643992], action=0, reward=1.0, next_state=[ 0.1049669  -0.7228588  -0.14734962  0.56691563]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 41 ] state=[ 0.1049669  -0.7228588  -0.14734962  0.56691563], action=0, reward=1.0, next_state=[ 0.09050972 -0.91563998 -0.13601131  0.80978964]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 42 ] state=[ 0.09050972 -0.91563998 -0.13601131  0.80978964], action=0, reward=1.0, next_state=[ 0.07219692 -1.10866238 -0.11981551  1.05678516]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 43 ] state=[ 0.07219692 -1.10866238 -0.11981551  1.05678516], action=1, reward=1.0, next_state=[ 0.05002368 -0.91217414 -0.09867981  0.72902429]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 237 ][ timestamp 44 ] state=[ 0.05002368 -0.91217414 -0.09867981  0.72902429], action=0, reward=1.0, next_state=[ 0.03178019 -1.10580361 -0.08409932  0.98909072]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 45 ] state=[ 0.03178019 -1.10580361 -0.08409932  0.98909072], action=1, reward=1.0, next_state=[ 0.00966412 -0.90966267 -0.06431751  0.67122306]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 46 ] state=[ 0.00966412 -0.90966267 -0.06431751  0.67122306], action=1, reward=1.0, next_state=[-0.00852913 -0.71370834 -0.05089305  0.359003  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 47 ] state=[-0.00852913 -0.71370834 -0.05089305  0.359003  ], action=0, reward=1.0, next_state=[-0.0228033  -0.90807126 -0.04371299  0.6352138 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 48 ] state=[-0.0228033  -0.90807126 -0.04371299  0.6352138 ], action=1, reward=1.0, next_state=[-0.04096472 -0.71236775 -0.03100871  0.32909143]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 49 ] state=[-0.04096472 -0.71236775 -0.03100871  0.32909143], action=1, reward=1.0, next_state=[-0.05521208 -0.5168184  -0.02442688  0.02679331]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 50 ] state=[-0.05521208 -0.5168184  -0.02442688  0.02679331], action=0, reward=1.0, next_state=[-0.06554845 -0.71158169 -0.02389102  0.3116703 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 51 ] state=[-0.06554845 -0.71158169 -0.02389102  0.3116703 ], action=1, reward=1.0, next_state=[-0.07978008 -0.51612766 -0.01765761  0.01154964]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 52 ] state=[-0.07978008 -0.51612766 -0.01765761  0.01154964], action=0, reward=1.0, next_state=[-0.09010263 -0.71099199 -0.01742662  0.29860949]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 53 ] state=[-0.09010263 -0.71099199 -0.01742662  0.29860949], action=1, reward=1.0, next_state=[-1.04322473e-01 -5.15626018e-01 -1.14544291e-02  4.81871273e-04]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 54 ] state=[-1.04322473e-01 -5.15626018e-01 -1.14544291e-02  4.81871273e-04], action=0, reward=1.0, next_state=[-0.11463499 -0.71058184 -0.01144479  0.28952889]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 55 ] state=[-0.11463499 -0.71058184 -0.01144479  0.28952889], action=1, reward=1.0, next_state=[-0.12884663 -0.51529857 -0.00565421 -0.00674152]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 56 ] state=[-0.12884663 -0.51529857 -0.00565421 -0.00674152], action=1, reward=1.0, next_state=[-0.1391526  -0.32009599 -0.00578904 -0.30120304]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 57 ] state=[-0.1391526  -0.32009599 -0.00578904 -0.30120304], action=0, reward=1.0, next_state=[-0.14555452 -0.51513496 -0.01181311 -0.01035147]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 58 ] state=[-0.14555452 -0.51513496 -0.01181311 -0.01035147], action=1, reward=1.0, next_state=[-0.15585722 -0.3198456  -0.01202013 -0.30673804]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 59 ] state=[-0.15585722 -0.3198456  -0.01202013 -0.30673804], action=1, reward=1.0, next_state=[-0.16225413 -0.12455445 -0.0181549  -0.60318744]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 60 ] state=[-0.16225413 -0.12455445 -0.0181549  -0.60318744], action=0, reward=1.0, next_state=[-0.16474522 -0.31941783 -0.03021864 -0.31627777]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 61 ] state=[-0.16474522 -0.31941783 -0.03021864 -0.31627777], action=0, reward=1.0, next_state=[-0.17113358 -0.51409661 -0.0365442  -0.03327585]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 62 ] state=[-0.17113358 -0.51409661 -0.0365442  -0.03327585], action=0, reward=1.0, next_state=[-0.18141551 -0.70867597 -0.03720972  0.2476567 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 63 ] state=[-0.18141551 -0.70867597 -0.03720972  0.2476567 ], action=0, reward=1.0, next_state=[-0.19558903 -0.90324729 -0.03225658  0.52837453]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 64 ] state=[-0.19558903 -0.90324729 -0.03225658  0.52837453], action=1, reward=1.0, next_state=[-0.21365398 -0.70768672 -0.02168909  0.22570449]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 65 ] state=[-0.21365398 -0.70768672 -0.02168909  0.22570449], action=1, reward=1.0, next_state=[-0.22780771 -0.51226162 -0.017175   -0.07374032]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 66 ] state=[-0.22780771 -0.51226162 -0.017175   -0.07374032], action=0, reward=1.0, next_state=[-0.23805294 -0.70713319 -0.01864981  0.21347472]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 67 ] state=[-0.23805294 -0.70713319 -0.01864981  0.21347472], action=1, reward=1.0, next_state=[-0.25219561 -0.51174963 -0.01438031 -0.08503236]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 68 ] state=[-0.25219561 -0.51174963 -0.01438031 -0.08503236], action=0, reward=1.0, next_state=[-0.2624306  -0.70666253 -0.01608096  0.20307908]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 69 ] state=[-0.2624306  -0.70666253 -0.01608096  0.20307908], action=1, reward=1.0, next_state=[-0.27656385 -0.51131434 -0.01201938 -0.09463292]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 70 ] state=[-0.27656385 -0.51131434 -0.01201938 -0.09463292], action=0, reward=1.0, next_state=[-0.28679014 -0.70626198 -0.01391204  0.1942338 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 71 ] state=[-0.28679014 -0.70626198 -0.01391204  0.1942338 ], action=0, reward=1.0, next_state=[-0.30091538 -0.90118219 -0.01002736  0.48249582]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 72 ] state=[-0.30091538 -0.90118219 -0.01002736  0.48249582], action=1, reward=1.0, next_state=[-3.18939020e-01 -7.05920151e-01 -3.77445316e-04  1.86669495e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 73 ] state=[-3.18939020e-01 -7.05920151e-01 -3.77445316e-04  1.86669495e-01], action=1, reward=1.0, next_state=[-0.33305742 -0.5107928   0.00335594 -0.10613248]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 74 ] state=[-0.33305742 -0.5107928   0.00335594 -0.10613248], action=0, reward=1.0, next_state=[-0.34327328 -0.70596268  0.0012333   0.18760734]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 75 ] state=[-0.34327328 -0.70596268  0.0012333   0.18760734], action=0, reward=1.0, next_state=[-0.35739253 -0.90110226  0.00498544  0.48067907]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 76 ] state=[-0.35739253 -0.90110226  0.00498544  0.48067907], action=1, reward=1.0, next_state=[-0.37541458 -0.70605104  0.01459902  0.18957159]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 77 ] state=[-0.37541458 -0.70605104  0.01459902  0.18957159], action=0, reward=1.0, next_state=[-0.3895356  -0.90137877  0.01839046  0.48682393]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 78 ] state=[-0.3895356  -0.90137877  0.01839046  0.48682393], action=1, reward=1.0, next_state=[-0.40756317 -0.70652108  0.02812693  0.1999933 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 79 ] state=[-0.40756317 -0.70652108  0.02812693  0.1999933 ], action=1, reward=1.0, next_state=[-0.4216936  -0.51181246  0.0321268  -0.08368587]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 80 ] state=[-0.4216936  -0.51181246  0.0321268  -0.08368587], action=1, reward=1.0, next_state=[-0.43192984 -0.31716542  0.03045308 -0.36606212]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 81 ] state=[-0.43192984 -0.31716542  0.03045308 -0.36606212], action=0, reward=1.0, next_state=[-0.43827315 -0.51270659  0.02313184 -0.06393453]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 82 ] state=[-0.43827315 -0.51270659  0.02313184 -0.06393453], action=1, reward=1.0, next_state=[-0.44852728 -0.3179238   0.02185315 -0.3492304 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 83 ] state=[-0.44852728 -0.3179238   0.02185315 -0.3492304 ], action=1, reward=1.0, next_state=[-0.45488576 -0.12311936  0.01486854 -0.63494297]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 84 ] state=[-0.45488576 -0.12311936  0.01486854 -0.63494297], action=1, reward=1.0, next_state=[-0.45734815  0.07179208  0.00216968 -0.92290663]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 237 ][ timestamp 85 ] state=[-0.45734815  0.07179208  0.00216968 -0.92290663], action=0, reward=1.0, next_state=[-0.45591231 -0.12335912 -0.01628845 -0.62954264]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 86 ] state=[-0.45591231 -0.12335912 -0.01628845 -0.62954264], action=1, reward=1.0, next_state=[-0.45837949  0.0719863  -0.0288793  -0.92731049]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 87 ] state=[-0.45837949  0.0719863  -0.0288793  -0.92731049], action=1, reward=1.0, next_state=[-0.45693976  0.26748604 -0.04742551 -1.22892716]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 88 ] state=[-0.45693976  0.26748604 -0.04742551 -1.22892716], action=0, reward=1.0, next_state=[-0.45159004  0.07300535 -0.07200406 -0.951472  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 89 ] state=[-0.45159004  0.07300535 -0.07200406 -0.951472  ], action=1, reward=1.0, next_state=[-0.45012993  0.26901861 -0.0910335  -1.26588094]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 90 ] state=[-0.45012993  0.26901861 -0.0910335  -1.26588094], action=0, reward=1.0, next_state=[-0.44474956  0.07517011 -0.11635112 -1.00303908]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 91 ] state=[-0.44474956  0.07517011 -0.11635112 -1.00303908], action=1, reward=1.0, next_state=[-0.44324616  0.27163801 -0.1364119  -1.3298785 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 92 ] state=[-0.44324616  0.27163801 -0.1364119  -1.3298785 ], action=0, reward=1.0, next_state=[-0.4378134   0.07847496 -0.16300947 -1.0828064 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 93 ] state=[-0.4378134   0.07847496 -0.16300947 -1.0828064 ], action=0, reward=1.0, next_state=[-0.4362439  -0.11416479 -0.18466559 -0.84539024]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 94 ] state=[-0.4362439  -0.11416479 -0.18466559 -0.84539024], action=0, reward=1.0, next_state=[-0.4385272  -0.30635207 -0.2015734  -0.6159944 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ][ timestamp 95 ] state=[-0.4385272  -0.30635207 -0.2015734  -0.6159944 ], action=0, reward=-1.0, next_state=[-0.44465424 -0.49817234 -0.21389329 -0.3929518 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 237: Exploration_rate=0.01. Score=95.\n",
      "[ episode 238 ] state=[-0.00862332  0.03673552  0.0017195  -0.00299467]\n",
      "[ episode 238 ][ timestamp 1 ] state=[-0.00862332  0.03673552  0.0017195  -0.00299467], action=0, reward=1.0, next_state=[-0.00788861 -0.15841104  0.0016596   0.29023029]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 2 ] state=[-0.00788861 -0.15841104  0.0016596   0.29023029], action=1, reward=1.0, next_state=[-0.01105683  0.0366872   0.00746421 -0.00192876]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 3 ] state=[-0.01105683  0.0366872   0.00746421 -0.00192876], action=0, reward=1.0, next_state=[-0.01032309 -0.158541    0.00742564  0.29309984]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 4 ] state=[-0.01032309 -0.158541    0.00742564  0.29309984], action=1, reward=1.0, next_state=[-0.01349391  0.0364743   0.01328763  0.00276808]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 5 ] state=[-0.01349391  0.0364743   0.01328763  0.00276808], action=0, reward=1.0, next_state=[-0.01276442 -0.15883567  0.01334299  0.29961362]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 6 ] state=[-0.01276442 -0.15883567  0.01334299  0.29961362], action=1, reward=1.0, next_state=[-0.01594114  0.03609358  0.01933527  0.0111685 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 7 ] state=[-0.01594114  0.03609358  0.01933527  0.0111685 ], action=1, reward=1.0, next_state=[-0.01521927  0.23093297  0.01955864 -0.27535175]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 8 ] state=[-0.01521927  0.23093297  0.01955864 -0.27535175], action=1, reward=1.0, next_state=[-0.01060061  0.42577049  0.0140516  -0.56180226]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 9 ] state=[-0.01060061  0.42577049  0.0140516  -0.56180226], action=1, reward=1.0, next_state=[-0.0020852   0.62069246  0.00281556 -0.85002531]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 10 ] state=[-0.0020852   0.62069246  0.00281556 -0.85002531], action=0, reward=1.0, next_state=[ 0.01032865  0.42553223 -0.01418495 -0.55645835]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 11 ] state=[ 0.01032865  0.42553223 -0.01418495 -0.55645835], action=0, reward=1.0, next_state=[ 0.0188393   0.23061226 -0.02531412 -0.26827806]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 12 ] state=[ 0.0188393   0.23061226 -0.02531412 -0.26827806], action=0, reward=1.0, next_state=[ 0.02345154  0.03586055 -0.03067968  0.01631436]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 13 ] state=[ 0.02345154  0.03586055 -0.03067968  0.01631436], action=0, reward=1.0, next_state=[ 0.02416875 -0.15880829 -0.03035339  0.2991618 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 14 ] state=[ 0.02416875 -0.15880829 -0.03035339  0.2991618 ], action=1, reward=1.0, next_state=[ 0.02099259  0.03673288 -0.02437015 -0.00293737]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 15 ] state=[ 0.02099259  0.03673288 -0.02437015 -0.00293737], action=1, reward=1.0, next_state=[ 0.02172725  0.2321957  -0.0244289  -0.30320866]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 16 ] state=[ 0.02172725  0.2321957  -0.0244289  -0.30320866], action=0, reward=1.0, next_state=[ 0.02637116  0.03743027 -0.03049308 -0.01832906]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 17 ] state=[ 0.02637116  0.03743027 -0.03049308 -0.01832906], action=0, reward=1.0, next_state=[ 0.02711976 -0.15724141 -0.03085966  0.26457913]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 18 ] state=[ 0.02711976 -0.15724141 -0.03085966  0.26457913], action=0, reward=1.0, next_state=[ 0.02397494 -0.35190961 -0.02556807  0.54737111]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 19 ] state=[ 0.02397494 -0.35190961 -0.02556807  0.54737111], action=1, reward=1.0, next_state=[ 0.01693674 -0.15643796 -0.01462065  0.24674327]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 20 ] state=[ 0.01693674 -0.15643796 -0.01462065  0.24674327], action=1, reward=1.0, next_state=[ 0.01380799  0.03888972 -0.00968579 -0.05051526]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 21 ] state=[ 0.01380799  0.03888972 -0.00968579 -0.05051526], action=0, reward=1.0, next_state=[ 0.01458578 -0.15609202 -0.01069609  0.23909605]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 22 ] state=[ 0.01458578 -0.15609202 -0.01069609  0.23909605], action=0, reward=1.0, next_state=[ 0.01146394 -0.35105955 -0.00591417  0.52838603]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 23 ] state=[ 0.01146394 -0.35105955 -0.00591417  0.52838603], action=0, reward=1.0, next_state=[ 0.00444275 -0.54609779  0.00465355  0.81919953]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 24 ] state=[ 0.00444275 -0.54609779  0.00465355  0.81919953], action=1, reward=1.0, next_state=[-0.00647921 -0.35103984  0.02103754  0.52798391]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 25 ] state=[-0.00647921 -0.35103984  0.02103754  0.52798391], action=1, reward=1.0, next_state=[-0.0135     -0.1562201   0.03159722  0.24200354]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 26 ] state=[-0.0135     -0.1562201   0.03159722  0.24200354], action=0, reward=1.0, next_state=[-0.01662441 -0.35177881  0.03643729  0.54448322]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 27 ] state=[-0.01662441 -0.35177881  0.03643729  0.54448322], action=1, reward=1.0, next_state=[-0.02365998 -0.15718732  0.04732695  0.26349992]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 28 ] state=[-0.02365998 -0.15718732  0.04732695  0.26349992], action=1, reward=1.0, next_state=[-0.02680373  0.03722827  0.05259695 -0.013888  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 29 ] state=[-0.02680373  0.03722827  0.05259695 -0.013888  ], action=0, reward=1.0, next_state=[-0.02605916 -0.15860699  0.05231919  0.29491504]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 30 ] state=[-0.02605916 -0.15860699  0.05231919  0.29491504], action=0, reward=1.0, next_state=[-0.0292313  -0.35443428  0.05821749  0.60362887]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 31 ] state=[-0.0292313  -0.35443428  0.05821749  0.60362887], action=1, reward=1.0, next_state=[-0.03631999 -0.16017281  0.07029007  0.3298366 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 238 ][ timestamp 32 ] state=[-0.03631999 -0.16017281  0.07029007  0.3298366 ], action=1, reward=1.0, next_state=[-0.03952344  0.03388181  0.0768868   0.06012171]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 33 ] state=[-0.03952344  0.03388181  0.0768868   0.06012171], action=0, reward=1.0, next_state=[-0.03884581 -0.16225352  0.07808924  0.376038  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 34 ] state=[-0.03884581 -0.16225352  0.07808924  0.376038  ], action=1, reward=1.0, next_state=[-0.04209088  0.03167747  0.08561     0.1089629 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 35 ] state=[-0.04209088  0.03167747  0.08561     0.1089629 ], action=1, reward=1.0, next_state=[-0.04145733  0.22547495  0.08778925 -0.15553011]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 36 ] state=[-0.04145733  0.22547495  0.08778925 -0.15553011], action=0, reward=1.0, next_state=[-0.03694783  0.02921289  0.08467865  0.16350617]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 37 ] state=[-0.03694783  0.02921289  0.08467865  0.16350617], action=1, reward=1.0, next_state=[-0.03636357  0.22302699  0.08794878 -0.1013075 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 38 ] state=[-0.03636357  0.22302699  0.08794878 -0.1013075 ], action=1, reward=1.0, next_state=[-0.03190303  0.41678562  0.08592263 -0.36499851]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 39 ] state=[-0.03190303  0.41678562  0.08592263 -0.36499851], action=1, reward=1.0, next_state=[-0.02356732  0.61058809  0.07862266 -0.62939961]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 40 ] state=[-0.02356732  0.61058809  0.07862266 -0.62939961], action=1, reward=1.0, next_state=[-0.01135556  0.80452993  0.06603466 -0.89632244]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 41 ] state=[-0.01135556  0.80452993  0.06603466 -0.89632244], action=1, reward=1.0, next_state=[ 0.00473504  0.99869746  0.04810821 -1.16753887]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 42 ] state=[ 0.00473504  0.99869746  0.04810821 -1.16753887], action=0, reward=1.0, next_state=[ 0.02470899  0.8029837   0.02475744 -0.86016953]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 43 ] state=[ 0.02470899  0.8029837   0.02475744 -0.86016953], action=1, reward=1.0, next_state=[ 0.04076866  0.99775987  0.00755405 -1.14496631]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 44 ] state=[ 0.04076866  0.99775987  0.00755405 -1.14496631], action=1, reward=1.0, next_state=[ 0.06072386  1.19278234 -0.01534528 -1.43527079]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 45 ] state=[ 0.06072386  1.19278234 -0.01534528 -1.43527079], action=1, reward=1.0, next_state=[ 0.08457951  1.38809013 -0.0440507  -1.73270932]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 46 ] state=[ 0.08457951  1.38809013 -0.0440507  -1.73270932], action=1, reward=1.0, next_state=[ 0.11234131  1.58368627 -0.07870488 -2.03876564]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 47 ] state=[ 0.11234131  1.58368627 -0.07870488 -2.03876564], action=1, reward=1.0, next_state=[ 0.14401504  1.7795249  -0.11948019 -2.35472956]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 48 ] state=[ 0.14401504  1.7795249  -0.11948019 -2.35472956], action=1, reward=1.0, next_state=[ 0.17960553  1.97549559 -0.16657479 -2.68163356]\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ][ timestamp 49 ] state=[ 0.17960553  1.97549559 -0.16657479 -2.68163356], action=0, reward=-1.0, next_state=[ 0.21911544  1.78194471 -0.22020746 -2.44407263]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 238: Exploration_rate=0.01. Score=49.\n",
      "[ episode 239 ] state=[-0.03989988  0.01565507  0.01961039  0.00860984]\n",
      "[ episode 239 ][ timestamp 1 ] state=[-0.03989988  0.01565507  0.01961039  0.00860984], action=0, reward=1.0, next_state=[-0.03958678 -0.17974256  0.01978259  0.307415  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 2 ] state=[-0.03958678 -0.17974256  0.01978259  0.307415  ], action=1, reward=1.0, next_state=[-0.04318163  0.015092    0.02593089  0.02103605]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 3 ] state=[-0.04318163  0.015092    0.02593089  0.02103605], action=0, reward=1.0, next_state=[-0.04287979 -0.18039204  0.02635161  0.32178636]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 4 ] state=[-0.04287979 -0.18039204  0.02635161  0.32178636], action=1, reward=1.0, next_state=[-0.04648763  0.01434493  0.03278734  0.03752879]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 5 ] state=[-0.04648763  0.01434493  0.03278734  0.03752879], action=1, reward=1.0, next_state=[-0.04620073  0.20898174  0.03353792 -0.24463176]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 6 ] state=[-0.04620073  0.20898174  0.03353792 -0.24463176], action=1, reward=1.0, next_state=[-0.0420211   0.40360902  0.02864528 -0.5265502 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 7 ] state=[-0.0420211   0.40360902  0.02864528 -0.5265502 ], action=0, reward=1.0, next_state=[-0.03394892  0.20809596  0.01811428 -0.22498036]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 8 ] state=[-0.03394892  0.20809596  0.01811428 -0.22498036], action=1, reward=1.0, next_state=[-0.029787    0.4029544   0.01361467 -0.51189476]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 9 ] state=[-0.029787    0.4029544   0.01361467 -0.51189476], action=0, reward=1.0, next_state=[-0.02172791  0.20764335  0.00337677 -0.21495275]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 10 ] state=[-0.02172791  0.20764335  0.00337677 -0.21495275], action=1, reward=1.0, next_state=[-0.01757504  0.40271686 -0.00092228 -0.50656858]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 11 ] state=[-0.01757504  0.40271686 -0.00092228 -0.50656858], action=1, reward=1.0, next_state=[-0.00952071  0.59785179 -0.01105365 -0.79954201]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 12 ] state=[-0.00952071  0.59785179 -0.01105365 -0.79954201], action=0, reward=1.0, next_state=[ 0.00243633  0.40288321 -0.02704449 -0.5103567 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 13 ] state=[ 0.00243633  0.40288321 -0.02704449 -0.5103567 ], action=0, reward=1.0, next_state=[ 0.01049399  0.20815247 -0.03725163 -0.22631752]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 14 ] state=[ 0.01049399  0.20815247 -0.03725163 -0.22631752], action=1, reward=1.0, next_state=[ 0.01465704  0.40378645 -0.04177798 -0.53051436]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 15 ] state=[ 0.01465704  0.40378645 -0.04177798 -0.53051436], action=0, reward=1.0, next_state=[ 0.02273277  0.20927632 -0.05238826 -0.2512829 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 16 ] state=[ 0.02273277  0.20927632 -0.05238826 -0.2512829 ], action=0, reward=1.0, next_state=[ 0.0269183   0.01494009 -0.05741392  0.02442642]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 17 ] state=[ 0.0269183   0.01494009 -0.05741392  0.02442642], action=0, reward=1.0, next_state=[ 0.0272171  -0.1793135  -0.05692539  0.29845627]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 18 ] state=[ 0.0272171  -0.1793135  -0.05692539  0.29845627], action=1, reward=1.0, next_state=[ 0.02363083  0.01657174 -0.05095627 -0.01162267]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 19 ] state=[ 0.02363083  0.01657174 -0.05095627 -0.01162267], action=0, reward=1.0, next_state=[ 0.02396227 -0.1777838  -0.05118872  0.26455757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 20 ] state=[ 0.02396227 -0.1777838  -0.05118872  0.26455757], action=1, reward=1.0, next_state=[ 0.02040659  0.01802998 -0.04589757 -0.04382129]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 21 ] state=[ 0.02040659  0.01802998 -0.04589757 -0.04382129], action=0, reward=1.0, next_state=[ 0.02076719 -0.17640479 -0.046774    0.23403458]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 22 ] state=[ 0.02076719 -0.17640479 -0.046774    0.23403458], action=1, reward=1.0, next_state=[ 0.01723909  0.01935318 -0.04209331 -0.07302776]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 23 ] state=[ 0.01723909  0.01935318 -0.04209331 -0.07302776], action=0, reward=1.0, next_state=[ 0.01762616 -0.17514083 -0.04355386  0.20608304]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 24 ] state=[ 0.01762616 -0.17514083 -0.04355386  0.20608304], action=1, reward=1.0, next_state=[ 0.01412334  0.02057601 -0.0394322  -0.10001459]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 25 ] state=[ 0.01412334  0.02057601 -0.0394322  -0.10001459], action=0, reward=1.0, next_state=[ 0.01453486 -0.17395928 -0.04143249  0.17997144]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 26 ] state=[ 0.01453486 -0.17395928 -0.04143249  0.17997144], action=1, reward=1.0, next_state=[ 0.01105567  0.02173032 -0.03783306 -0.12548872]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 27 ] state=[ 0.01105567  0.02173032 -0.03783306 -0.12548872], action=1, reward=1.0, next_state=[ 0.01149028  0.21737328 -0.04034284 -0.42986341]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 239 ][ timestamp 28 ] state=[ 0.01149028  0.21737328 -0.04034284 -0.42986341], action=1, reward=1.0, next_state=[ 0.01583775  0.41304261 -0.04894011 -0.73498619]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 29 ] state=[ 0.01583775  0.41304261 -0.04894011 -0.73498619], action=0, reward=1.0, next_state=[ 0.0240986   0.21862968 -0.06363983 -0.45809859]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 30 ] state=[ 0.0240986   0.21862968 -0.06363983 -0.45809859], action=0, reward=1.0, next_state=[ 0.02847119  0.02446239 -0.0728018  -0.18613474]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 31 ] state=[ 0.02847119  0.02446239 -0.0728018  -0.18613474], action=1, reward=1.0, next_state=[ 0.02896044  0.22054638 -0.0765245  -0.50086644]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 32 ] state=[ 0.02896044  0.22054638 -0.0765245  -0.50086644], action=0, reward=1.0, next_state=[ 0.03337137  0.02658186 -0.08654182 -0.23324738]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 33 ] state=[ 0.03337137  0.02658186 -0.08654182 -0.23324738], action=0, reward=1.0, next_state=[ 0.033903   -0.1672038  -0.09120677  0.03093172]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 34 ] state=[ 0.033903   -0.1672038  -0.09120677  0.03093172], action=0, reward=1.0, next_state=[ 0.03055893 -0.36090746 -0.09058814  0.2935019 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 35 ] state=[ 0.03055893 -0.36090746 -0.09058814  0.2935019 ], action=0, reward=1.0, next_state=[ 0.02334078 -0.55462894 -0.0847181   0.55629615]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 36 ] state=[ 0.02334078 -0.55462894 -0.0847181   0.55629615], action=1, reward=1.0, next_state=[ 0.0122482  -0.35842615 -0.07359218  0.23817013]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 37 ] state=[ 0.0122482  -0.35842615 -0.07359218  0.23817013], action=1, reward=1.0, next_state=[ 0.00507968 -0.16233423 -0.06882877 -0.0767882 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 38 ] state=[ 0.00507968 -0.16233423 -0.06882877 -0.0767882 ], action=1, reward=1.0, next_state=[ 0.00183299  0.03370347 -0.07036454 -0.39036818]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 39 ] state=[ 0.00183299  0.03370347 -0.07036454 -0.39036818], action=1, reward=1.0, next_state=[ 0.00250706  0.22974987 -0.0781719  -0.70438019]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 40 ] state=[ 0.00250706  0.22974987 -0.0781719  -0.70438019], action=0, reward=1.0, next_state=[ 0.00710206  0.0357932  -0.09225951 -0.43729281]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 41 ] state=[ 0.00710206  0.0357932  -0.09225951 -0.43729281], action=1, reward=1.0, next_state=[ 0.00781792  0.2320917  -0.10100536 -0.75757412]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 42 ] state=[ 0.00781792  0.2320917  -0.10100536 -0.75757412], action=0, reward=1.0, next_state=[ 0.01245976  0.03849605 -0.11615684 -0.49830581]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 43 ] state=[ 0.01245976  0.03849605 -0.11615684 -0.49830581], action=0, reward=1.0, next_state=[ 0.01322968 -0.15481308 -0.12612296 -0.24436945]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 44 ] state=[ 0.01322968 -0.15481308 -0.12612296 -0.24436945], action=0, reward=1.0, next_state=[ 0.01013342 -0.34792911 -0.13101035  0.0060218 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 45 ] state=[ 0.01013342 -0.34792911 -0.13101035  0.0060218 ], action=0, reward=1.0, next_state=[ 0.00317484 -0.54095258 -0.13088991  0.25466886]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 46 ] state=[ 0.00317484 -0.54095258 -0.13088991  0.25466886], action=1, reward=1.0, next_state=[-0.00764422 -0.3442283  -0.12579654 -0.07626528]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 47 ] state=[-0.00764422 -0.3442283  -0.12579654 -0.07626528], action=1, reward=1.0, next_state=[-0.01452878 -0.14754847 -0.12732184 -0.40584051]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 48 ] state=[-0.01452878 -0.14754847 -0.12732184 -0.40584051], action=0, reward=1.0, next_state=[-0.01747975 -0.34065669 -0.13543865 -0.15585441]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 49 ] state=[-0.01747975 -0.34065669 -0.13543865 -0.15585441], action=0, reward=1.0, next_state=[-0.02429289 -0.53360572 -0.13855574  0.09122131]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 50 ] state=[-0.02429289 -0.53360572 -0.13855574  0.09122131], action=0, reward=1.0, next_state=[-0.034965   -0.72649781 -0.13673131  0.33718139]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 51 ] state=[-0.034965   -0.72649781 -0.13673131  0.33718139], action=0, reward=1.0, next_state=[-0.04949496 -0.91943601 -0.12998769  0.58381374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 52 ] state=[-0.04949496 -0.91943601 -0.12998769  0.58381374], action=0, reward=1.0, next_state=[-0.06788368 -1.1125204  -0.11831141  0.83288804]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 53 ] state=[-0.06788368 -1.1125204  -0.11831141  0.83288804], action=0, reward=1.0, next_state=[-0.09013408 -1.30584418 -0.10165365  1.08614407]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 54 ] state=[-0.09013408 -1.30584418 -0.10165365  1.08614407], action=1, reward=1.0, next_state=[-0.11625097 -1.1095391  -0.07993077  0.76337179]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 55 ] state=[-0.11625097 -1.1095391  -0.07993077  0.76337179], action=1, reward=1.0, next_state=[-0.13844175 -0.91341259 -0.06466333  0.44664667]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 56 ] state=[-0.13844175 -0.91341259 -0.06466333  0.44664667], action=0, reward=1.0, next_state=[-0.15671    -1.10756301 -0.0557304   0.71826589]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 57 ] state=[-0.15671    -1.10756301 -0.0557304   0.71826589], action=1, reward=1.0, next_state=[-0.17886126 -0.91171596 -0.04136508  0.40857514]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 58 ] state=[-0.17886126 -0.91171596 -0.04136508  0.40857514], action=0, reward=1.0, next_state=[-0.19709558 -1.10622775 -0.03319358  0.68793537]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 59 ] state=[-0.19709558 -1.10622775 -0.03319358  0.68793537], action=1, reward=1.0, next_state=[-0.21922014 -0.91066118 -0.01943487  0.38499   ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 60 ] state=[-0.21922014 -0.91066118 -0.01943487  0.38499   ], action=1, reward=1.0, next_state=[-0.23743336 -0.71526879 -0.01173507  0.08624326]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 61 ] state=[-0.23743336 -0.71526879 -0.01173507  0.08624326], action=1, reward=1.0, next_state=[-0.25173874 -0.51998061 -0.01001021 -0.21011887]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 62 ] state=[-0.25173874 -0.51998061 -0.01001021 -0.21011887], action=1, reward=1.0, next_state=[-0.26213835 -0.32471697 -0.01421258 -0.50594261]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 63 ] state=[-0.26213835 -0.32471697 -0.01421258 -0.50594261], action=0, reward=1.0, next_state=[-0.26863269 -0.51963578 -0.02433144 -0.21777227]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 64 ] state=[-0.26863269 -0.51963578 -0.02433144 -0.21777227], action=0, reward=1.0, next_state=[-0.2790254  -0.71440162 -0.02868688  0.06713727]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 65 ] state=[-0.2790254  -0.71440162 -0.02868688  0.06713727], action=1, reward=1.0, next_state=[-0.29331343 -0.51888037 -0.02734414 -0.23445671]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 66 ] state=[-0.29331343 -0.51888037 -0.02734414 -0.23445671], action=0, reward=1.0, next_state=[-0.30369104 -0.71360118 -0.03203327  0.04947714]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 67 ] state=[-0.30369104 -0.71360118 -0.03203327  0.04947714], action=1, reward=1.0, next_state=[-0.31796307 -0.5180349  -0.03104373 -0.25313797]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 68 ] state=[-0.31796307 -0.5180349  -0.03104373 -0.25313797], action=0, reward=1.0, next_state=[-0.32832376 -0.71270014 -0.03610649  0.02959381]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 69 ] state=[-0.32832376 -0.71270014 -0.03610649  0.02959381], action=1, reward=1.0, next_state=[-0.34257777 -0.5170795  -0.03551461 -0.2742589 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 70 ] state=[-0.34257777 -0.5170795  -0.03551461 -0.2742589 ], action=1, reward=1.0, next_state=[-0.35291936 -0.3214693  -0.04099979 -0.57792828]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 71 ] state=[-0.35291936 -0.3214693  -0.04099979 -0.57792828], action=1, reward=1.0, next_state=[-0.35934874 -0.12579742 -0.05255835 -0.88324   ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 72 ] state=[-0.35934874 -0.12579742 -0.05255835 -0.88324   ], action=1, reward=1.0, next_state=[-0.36186469  0.06999739 -0.07022315 -1.19197171]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 239 ][ timestamp 73 ] state=[-0.36186469  0.06999739 -0.07022315 -1.19197171], action=0, reward=1.0, next_state=[-0.36046474 -0.12414809 -0.09406259 -0.92209988]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 74 ] state=[-0.36046474 -0.12414809 -0.09406259 -0.92209988], action=1, reward=1.0, next_state=[-0.3629477   0.07211032 -0.11250459 -1.24279975]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 75 ] state=[-0.3629477   0.07211032 -0.11250459 -1.24279975], action=0, reward=1.0, next_state=[-0.3615055  -0.12140244 -0.13736058 -0.9873723 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 76 ] state=[-0.3615055  -0.12140244 -0.13736058 -0.9873723 ], action=0, reward=1.0, next_state=[-0.36393355 -0.31444452 -0.15710803 -0.74079375]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 77 ] state=[-0.36393355 -0.31444452 -0.15710803 -0.74079375], action=0, reward=1.0, next_state=[-0.37022244 -0.5070889  -0.1719239  -0.5013861 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 78 ] state=[-0.37022244 -0.5070889  -0.1719239  -0.5013861 ], action=0, reward=1.0, next_state=[-0.38036422 -0.69942358 -0.18195162 -0.26743433]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 79 ] state=[-0.38036422 -0.69942358 -0.18195162 -0.26743433], action=1, reward=1.0, next_state=[-0.39435269 -0.50223444 -0.18730031 -0.61153447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 80 ] state=[-0.39435269 -0.50223444 -0.18730031 -0.61153447], action=1, reward=1.0, next_state=[-0.40439738 -0.3050569  -0.199531   -0.95687287]\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ][ timestamp 81 ] state=[-0.40439738 -0.3050569  -0.199531   -0.95687287], action=0, reward=-1.0, next_state=[-0.41049851 -0.49701787 -0.21866846 -0.73291792]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 239: Exploration_rate=0.01. Score=81.\n",
      "[ episode 240 ] state=[-0.01466157  0.02685285 -0.01967169 -0.02940872]\n",
      "[ episode 240 ][ timestamp 1 ] state=[-0.01466157  0.02685285 -0.01967169 -0.02940872], action=1, reward=1.0, next_state=[-0.01412451  0.2222513  -0.02025987 -0.32823279]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 2 ] state=[-0.01412451  0.2222513  -0.02025987 -0.32823279], action=0, reward=1.0, next_state=[-0.00967949  0.02742355 -0.02682452 -0.04200713]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 3 ] state=[-0.00967949  0.02742355 -0.02682452 -0.04200713], action=0, reward=1.0, next_state=[-0.00913102 -0.16730368 -0.02766467  0.24209317]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 4 ] state=[-0.00913102 -0.16730368 -0.02766467  0.24209317], action=1, reward=1.0, next_state=[-0.01247709  0.0282023  -0.0228228  -0.05918595]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 5 ] state=[-0.01247709  0.0282023  -0.0228228  -0.05918595], action=1, reward=1.0, next_state=[-0.01191305  0.22364392 -0.02400652 -0.35898136]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 6 ] state=[-0.01191305  0.22364392 -0.02400652 -0.35898136], action=0, reward=1.0, next_state=[-0.00744017  0.02887132 -0.03118615 -0.07396388]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 7 ] state=[-0.00744017  0.02887132 -0.03118615 -0.07396388], action=0, reward=1.0, next_state=[-0.00686274 -0.16578998 -0.03266543  0.20871885]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 8 ] state=[-0.00686274 -0.16578998 -0.03266543  0.20871885], action=1, reward=1.0, next_state=[-0.01017854  0.02978345 -0.02849105 -0.09408673]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 9 ] state=[-0.01017854  0.02978345 -0.02849105 -0.09408673], action=1, reward=1.0, next_state=[-0.00958287  0.22530193 -0.03037278 -0.39562066]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 10 ] state=[-0.00958287  0.22530193 -0.03037278 -0.39562066], action=0, reward=1.0, next_state=[-0.00507683  0.0306238  -0.0382852  -0.11266637]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 11 ] state=[-0.00507683  0.0306238  -0.0382852  -0.11266637], action=0, reward=1.0, next_state=[-0.00446436 -0.16392924 -0.04053852  0.16769624]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 12 ] state=[-0.00446436 -0.16392924 -0.04053852  0.16769624], action=0, reward=1.0, next_state=[-0.00774294 -0.35844818 -0.0371846   0.44731987]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 13 ] state=[-0.00774294 -0.35844818 -0.0371846   0.44731987], action=1, reward=1.0, next_state=[-0.01491191 -0.16282048 -0.0282382   0.14315142]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 14 ] state=[-0.01491191 -0.16282048 -0.0282382   0.14315142], action=0, reward=1.0, next_state=[-0.01816831 -0.35752688 -0.02537517  0.42679365]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 15 ] state=[-0.01816831 -0.35752688 -0.02537517  0.42679365], action=1, reward=1.0, next_state=[-0.02531885 -0.16205488 -0.0168393   0.12622055]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 16 ] state=[-0.02531885 -0.16205488 -0.0168393   0.12622055], action=1, reward=1.0, next_state=[-0.02855995  0.03330421 -0.01431489 -0.17172707]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 17 ] state=[-0.02855995  0.03330421 -0.01431489 -0.17172707], action=1, reward=1.0, next_state=[-0.02789387  0.22862809 -0.01774943 -0.4688913 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 18 ] state=[-0.02789387  0.22862809 -0.01774943 -0.4688913 ], action=1, reward=1.0, next_state=[-0.0233213   0.42399623 -0.02712726 -0.76711541]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 19 ] state=[-0.0233213   0.42399623 -0.02712726 -0.76711541], action=0, reward=1.0, next_state=[-0.01484138  0.22925804 -0.04246956 -0.48309004]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 20 ] state=[-0.01484138  0.22925804 -0.04246956 -0.48309004], action=0, reward=1.0, next_state=[-0.01025622  0.03476042 -0.05213137 -0.20408897]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 21 ] state=[-0.01025622  0.03476042 -0.05213137 -0.20408897], action=0, reward=1.0, next_state=[-0.00956101 -0.15957875 -0.05621314  0.07170408]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 22 ] state=[-0.00956101 -0.15957875 -0.05621314  0.07170408], action=0, reward=1.0, next_state=[-0.01275259 -0.35385163 -0.05477906  0.34613515]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 23 ] state=[-0.01275259 -0.35385163 -0.05477906  0.34613515], action=1, reward=1.0, next_state=[-0.01982962 -0.157995   -0.04785636  0.03669389]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 24 ] state=[-0.01982962 -0.157995   -0.04785636  0.03669389], action=0, reward=1.0, next_state=[-0.02298952 -0.35239918 -0.04712248  0.31390189]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 25 ] state=[-0.02298952 -0.35239918 -0.04712248  0.31390189], action=1, reward=1.0, next_state=[-0.0300375  -0.15663873 -0.04084444  0.0067383 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 26 ] state=[-0.0300375  -0.15663873 -0.04084444  0.0067383 ], action=1, reward=1.0, next_state=[-0.03317028  0.03904446 -0.04070968 -0.29854661]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 27 ] state=[-0.03317028  0.03904446 -0.04070968 -0.29854661], action=0, reward=1.0, next_state=[-0.03238939 -0.15547426 -0.04668061 -0.01897561]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 28 ] state=[-0.03238939 -0.15547426 -0.04668061 -0.01897561], action=1, reward=1.0, next_state=[-0.03549887  0.04028498 -0.04706012 -0.32601372]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 29 ] state=[-0.03549887  0.04028498 -0.04706012 -0.32601372], action=0, reward=1.0, next_state=[-0.03469317 -0.15413645 -0.0535804  -0.04853501]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 30 ] state=[-0.03469317 -0.15413645 -0.0535804  -0.04853501], action=0, reward=1.0, next_state=[-0.0377759  -0.3484508  -0.0545511   0.22677312]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 31 ] state=[-0.0377759  -0.3484508  -0.0545511   0.22677312], action=0, reward=1.0, next_state=[-0.04474492 -0.54275243 -0.05001564  0.50176194]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 32 ] state=[-0.04474492 -0.54275243 -0.05001564  0.50176194], action=0, reward=1.0, next_state=[-0.05559997 -0.737135   -0.0399804   0.77827273]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 33 ] state=[-0.05559997 -0.737135   -0.0399804   0.77827273], action=0, reward=1.0, next_state=[-0.07034267 -0.93168507 -0.02441494  1.05811351]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 240 ][ timestamp 34 ] state=[-0.07034267 -0.93168507 -0.02441494  1.05811351], action=0, reward=1.0, next_state=[-0.08897637 -1.12647518 -0.00325267  1.34303433]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 35 ] state=[-0.08897637 -1.12647518 -0.00325267  1.34303433], action=1, reward=1.0, next_state=[-0.11150587 -0.93131246  0.02360802  1.04933551]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 36 ] state=[-0.11150587 -0.93131246  0.02360802  1.04933551], action=0, reward=1.0, next_state=[-0.13013212 -1.12673953  0.04459473  1.34933454]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 37 ] state=[-0.13013212 -1.12673953  0.04459473  1.34933454], action=0, reward=1.0, next_state=[-0.15266691 -1.32239255  0.07158142  1.6556288 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 38 ] state=[-0.15266691 -1.32239255  0.07158142  1.6556288 ], action=0, reward=1.0, next_state=[-0.17911476 -1.518273    0.10469399  1.96972402]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 39 ] state=[-0.17911476 -1.518273    0.10469399  1.96972402], action=0, reward=1.0, next_state=[-0.20948022 -1.71433324  0.14408847  2.29292794]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 40 ] state=[-0.20948022 -1.71433324  0.14408847  2.29292794], action=0, reward=1.0, next_state=[-0.24376689 -1.91046074  0.18994703  2.62628614]\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ][ timestamp 41 ] state=[-0.24376689 -1.91046074  0.18994703  2.62628614], action=0, reward=-1.0, next_state=[-0.2819761  -2.10646001  0.24247275  2.97050646]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 240: Exploration_rate=0.01. Score=41.\n",
      "[ episode 241 ] state=[-0.00067885  0.01611022  0.01757745  0.04706055]\n",
      "[ episode 241 ][ timestamp 1 ] state=[-0.00067885  0.01611022  0.01757745  0.04706055], action=0, reward=1.0, next_state=[-0.00035664 -0.17925931  0.01851866  0.34523708]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 2 ] state=[-0.00035664 -0.17925931  0.01851866  0.34523708], action=1, reward=1.0, next_state=[-0.00394183  0.01559438  0.0254234   0.05845083]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 3 ] state=[-0.00394183  0.01559438  0.0254234   0.05845083], action=0, reward=1.0, next_state=[-0.00362994 -0.17988269  0.02659241  0.35904536]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 4 ] state=[-0.00362994 -0.17988269  0.02659241  0.35904536], action=1, reward=1.0, next_state=[-0.00722759  0.01485133  0.03377332  0.07486484]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 5 ] state=[-0.00722759  0.01485133  0.03377332  0.07486484], action=1, reward=1.0, next_state=[-0.00693057  0.20947324  0.03527062 -0.20697407]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 6 ] state=[-0.00693057  0.20947324  0.03527062 -0.20697407], action=0, reward=1.0, next_state=[-0.0027411   0.01386515  0.03113114  0.09662299]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 7 ] state=[-0.0027411   0.01386515  0.03113114  0.09662299], action=1, reward=1.0, next_state=[-0.0024638   0.20852741  0.0330636  -0.18607783]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 8 ] state=[-0.0024638   0.20852741  0.0330636  -0.18607783], action=1, reward=1.0, next_state=[ 0.00170675  0.40316108  0.02934204 -0.46814985]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 9 ] state=[ 0.00170675  0.40316108  0.02934204 -0.46814985], action=1, reward=1.0, next_state=[ 0.00976997  0.59785647  0.01997904 -0.75144191]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 10 ] state=[ 0.00976997  0.59785647  0.01997904 -0.75144191], action=0, reward=1.0, next_state=[ 0.0217271   0.40246478  0.00495021 -0.45253942]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 11 ] state=[ 0.0217271   0.40246478  0.00495021 -0.45253942], action=1, reward=1.0, next_state=[ 0.0297764   0.59751638 -0.00410058 -0.74365788]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 12 ] state=[ 0.0297764   0.59751638 -0.00410058 -0.74365788], action=0, reward=1.0, next_state=[ 0.04172672  0.40245126 -0.01897374 -0.45226823]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 13 ] state=[ 0.04172672  0.40245126 -0.01897374 -0.45226823], action=0, reward=1.0, next_state=[ 0.04977575  0.2076027  -0.02801911 -0.16562594]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 14 ] state=[ 0.04977575  0.2076027  -0.02801911 -0.16562594], action=1, reward=1.0, next_state=[ 0.0539278   0.4031143  -0.03133162 -0.46701477]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 15 ] state=[ 0.0539278   0.4031143  -0.03133162 -0.46701477], action=0, reward=1.0, next_state=[ 0.06199009  0.20844871 -0.04067192 -0.18436968]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 16 ] state=[ 0.06199009  0.20844871 -0.04067192 -0.18436968], action=1, reward=1.0, next_state=[ 0.06615906  0.40412829 -0.04435931 -0.48960057]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 17 ] state=[ 0.06615906  0.40412829 -0.04435931 -0.48960057], action=0, reward=1.0, next_state=[ 0.07424163  0.2096593  -0.05415132 -0.21122139]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 18 ] state=[ 0.07424163  0.2096593  -0.05415132 -0.21122139], action=0, reward=1.0, next_state=[ 0.07843481  0.01535173 -0.05837575  0.06390002]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 19 ] state=[ 0.07843481  0.01535173 -0.05837575  0.06390002], action=1, reward=1.0, next_state=[ 0.07874185  0.21125995 -0.05709775 -0.24661447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 20 ] state=[ 0.07874185  0.21125995 -0.05709775 -0.24661447], action=0, reward=1.0, next_state=[ 0.08296705  0.01699799 -0.06203004  0.02752598]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 21 ] state=[ 0.08296705  0.01699799 -0.06203004  0.02752598], action=1, reward=1.0, next_state=[ 0.08330701  0.21295212 -0.06147952 -0.28406505]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 22 ] state=[ 0.08330701  0.21295212 -0.06147952 -0.28406505], action=0, reward=1.0, next_state=[ 0.08756605  0.01875846 -0.06716082 -0.01138848]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 23 ] state=[ 0.08756605  0.01875846 -0.06716082 -0.01138848], action=0, reward=1.0, next_state=[ 0.08794122 -0.17533925 -0.06738859  0.25937127]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 24 ] state=[ 0.08794122 -0.17533925 -0.06738859  0.25937127], action=1, reward=1.0, next_state=[ 0.08443443  0.02067678 -0.06220117 -0.05378266]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 25 ] state=[ 0.08443443  0.02067678 -0.06220117 -0.05378266], action=0, reward=1.0, next_state=[ 0.08484797 -0.17350069 -0.06327682  0.21864492]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 26 ] state=[ 0.08484797 -0.17350069 -0.06327682  0.21864492], action=1, reward=1.0, next_state=[ 0.08137796  0.02246603 -0.05890392 -0.09330786]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 27 ] state=[ 0.08137796  0.02246603 -0.05890392 -0.09330786], action=0, reward=1.0, next_state=[ 0.08182728 -0.17176435 -0.06077008  0.18022468]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 28 ] state=[ 0.08182728 -0.17176435 -0.06077008  0.18022468], action=0, reward=1.0, next_state=[ 0.07839199 -0.36596642 -0.05716559  0.45313466]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 29 ] state=[ 0.07839199 -0.36596642 -0.05716559  0.45313466], action=1, reward=1.0, next_state=[ 0.07107266 -0.17008465 -0.04810289  0.14299443]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 30 ] state=[ 0.07107266 -0.17008465 -0.04810289  0.14299443], action=0, reward=1.0, next_state=[ 0.06767097 -0.36448587 -0.045243    0.42012216]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 31 ] state=[ 0.06767097 -0.36448587 -0.045243    0.42012216], action=1, reward=1.0, next_state=[ 0.06038125 -0.16875304 -0.03684056  0.11352645]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 32 ] state=[ 0.06038125 -0.16875304 -0.03684056  0.11352645], action=0, reward=1.0, next_state=[ 0.05700619 -0.36332827 -0.03457003  0.39436258]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 33 ] state=[ 0.05700619 -0.36332827 -0.03457003  0.39436258], action=1, reward=1.0, next_state=[ 0.04973962 -0.16773327 -0.02668278  0.09098381]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 34 ] state=[ 0.04973962 -0.16773327 -0.02668278  0.09098381], action=0, reward=1.0, next_state=[ 0.04638496 -0.3624628  -0.0248631   0.37513033]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 35 ] state=[ 0.04638496 -0.3624628  -0.0248631   0.37513033], action=1, reward=1.0, next_state=[ 0.0391357  -0.16699668 -0.0173605   0.07471277]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 36 ] state=[ 0.0391357  -0.16699668 -0.0173605   0.07471277], action=1, reward=1.0, next_state=[ 0.03579577  0.02836979 -0.01586624 -0.22339652]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 37 ] state=[ 0.03579577  0.02836979 -0.01586624 -0.22339652], action=1, reward=1.0, next_state=[ 0.03636317  0.22371488 -0.02033417 -0.52104175]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 38 ] state=[ 0.03636317  0.22371488 -0.02033417 -0.52104175], action=0, reward=1.0, next_state=[ 0.04083746  0.02888498 -0.03075501 -0.23483514]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 39 ] state=[ 0.04083746  0.02888498 -0.03075501 -0.23483514], action=0, reward=1.0, next_state=[ 0.04141516 -0.16578436 -0.03545171  0.04799024]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 241 ][ timestamp 40 ] state=[ 0.04141516 -0.16578436 -0.03545171  0.04799024], action=0, reward=1.0, next_state=[ 0.03809948 -0.3603805  -0.0344919   0.32928041]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 41 ] state=[ 0.03809948 -0.3603805  -0.0344919   0.32928041], action=1, reward=1.0, next_state=[ 0.03089187 -0.16478495 -0.0279063   0.02592298]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 42 ] state=[ 0.03089187 -0.16478495 -0.0279063   0.02592298], action=0, reward=1.0, next_state=[ 0.02759617 -0.35949581 -0.02738784  0.30967218]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 43 ] state=[ 0.02759617 -0.35949581 -0.02738784  0.30967218], action=1, reward=1.0, next_state=[ 0.02040625 -0.16399457 -0.02119439  0.00847926]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 44 ] state=[ 0.02040625 -0.16399457 -0.02119439  0.00847926], action=0, reward=1.0, next_state=[ 0.01712636 -0.35880625 -0.02102481  0.29440047]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 45 ] state=[ 0.01712636 -0.35880625 -0.02102481  0.29440047], action=0, reward=1.0, next_state=[ 0.00995023 -0.55362224 -0.0151368   0.58037904]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 46 ] state=[ 0.00995023 -0.55362224 -0.0151368   0.58037904], action=1, reward=1.0, next_state=[-0.00112221 -0.35829149 -0.00352922  0.28296642]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 47 ] state=[-0.00112221 -0.35829149 -0.00352922  0.28296642], action=0, reward=1.0, next_state=[-0.00828804 -0.55336292  0.00213011  0.57453417]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 48 ] state=[-0.00828804 -0.55336292  0.00213011  0.57453417], action=0, reward=1.0, next_state=[-0.0193553  -0.74851467  0.01362079  0.86788738]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 49 ] state=[-0.0193553  -0.74851467  0.01362079  0.86788738], action=1, reward=1.0, next_state=[-0.03432559 -0.55358068  0.03097854  0.5795179 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 50 ] state=[-0.03432559 -0.55358068  0.03097854  0.5795179 ], action=1, reward=1.0, next_state=[-0.04539721 -0.35890624  0.0425689   0.29675258]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 51 ] state=[-0.04539721 -0.35890624  0.0425689   0.29675258], action=1, reward=1.0, next_state=[-0.05257533 -0.16441616  0.04850395  0.01779323]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 52 ] state=[-0.05257533 -0.16441616  0.04850395  0.01779323], action=1, reward=1.0, next_state=[-0.05586365  0.02997784  0.04885982 -0.25920026]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 53 ] state=[-0.05586365  0.02997784  0.04885982 -0.25920026], action=1, reward=1.0, next_state=[-0.0552641   0.22436945  0.04367581 -0.53608063]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 54 ] state=[-0.0552641   0.22436945  0.04367581 -0.53608063], action=0, reward=1.0, next_state=[-0.05077671  0.02866147  0.0329542  -0.229962  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 55 ] state=[-0.05077671  0.02866147  0.0329542  -0.229962  ], action=0, reward=1.0, next_state=[-0.05020348 -0.16691552  0.02835496  0.07293098]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 56 ] state=[-0.05020348 -0.16691552  0.02835496  0.07293098], action=0, reward=1.0, next_state=[-0.05354179 -0.36243226  0.02981358  0.37442344]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 57 ] state=[-0.05354179 -0.36243226  0.02981358  0.37442344], action=1, reward=1.0, next_state=[-0.06079043 -0.16774621  0.03730205  0.09128803]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 58 ] state=[-0.06079043 -0.16774621  0.03730205  0.09128803], action=0, reward=1.0, next_state=[-0.06414536 -0.36338241  0.03912781  0.39550246]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 59 ] state=[-0.06414536 -0.36338241  0.03912781  0.39550246], action=1, reward=1.0, next_state=[-0.07141301 -0.16883685  0.04703786  0.11540812]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 60 ] state=[-0.07141301 -0.16883685  0.04703786  0.11540812], action=0, reward=1.0, next_state=[-0.07478974 -0.36460012  0.04934602  0.42255226]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 61 ] state=[-0.07478974 -0.36460012  0.04934602  0.42255226], action=0, reward=1.0, next_state=[-0.08208175 -0.56038517  0.05779706  0.73037419]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 62 ] state=[-0.08208175 -0.56038517  0.05779706  0.73037419], action=1, reward=1.0, next_state=[-0.09328945 -0.36610764  0.07240455  0.45642737]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 63 ] state=[-0.09328945 -0.36610764  0.07240455  0.45642737], action=1, reward=1.0, next_state=[-0.1006116  -0.17208004  0.0815331   0.18741687]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 64 ] state=[-0.1006116  -0.17208004  0.0815331   0.18741687], action=1, reward=1.0, next_state=[-0.1040532   0.02178648  0.08528143 -0.0784727 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 65 ] state=[-0.1040532   0.02178648  0.08528143 -0.0784727 ], action=1, reward=1.0, next_state=[-0.10361747  0.21558892  0.08371198 -0.34307751]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 66 ] state=[-0.10361747  0.21558892  0.08371198 -0.34307751], action=1, reward=1.0, next_state=[-0.0993057   0.40942636  0.07685043 -0.60823291]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 67 ] state=[-0.0993057   0.40942636  0.07685043 -0.60823291], action=0, reward=1.0, next_state=[-0.09111717  0.21331888  0.06468577 -0.29236814]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 68 ] state=[-0.09111717  0.21331888  0.06468577 -0.29236814], action=0, reward=1.0, next_state=[-0.08685079  0.01733714  0.05883841  0.01999402]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 69 ] state=[-0.08685079  0.01733714  0.05883841  0.01999402], action=1, reward=1.0, next_state=[-0.08650405  0.21156809  0.05923829 -0.25355971]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 70 ] state=[-0.08650405  0.21156809  0.05923829 -0.25355971], action=1, reward=1.0, next_state=[-0.08227269  0.40579636  0.05416709 -0.52698522]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 71 ] state=[-0.08227269  0.40579636  0.05416709 -0.52698522], action=1, reward=1.0, next_state=[-0.07415676  0.60011599  0.04362739 -0.80211981]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 72 ] state=[-0.07415676  0.60011599  0.04362739 -0.80211981], action=0, reward=1.0, next_state=[-0.06215444  0.40442376  0.02758499 -0.4960384 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 73 ] state=[-0.06215444  0.40442376  0.02758499 -0.4960384 ], action=1, reward=1.0, next_state=[-0.05406596  0.59914609  0.01766423 -0.7799018 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 74 ] state=[-0.05406596  0.59914609  0.01766423 -0.7799018 ], action=0, reward=1.0, next_state=[-0.04208304  0.4037858   0.00206619 -0.48171408]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 75 ] state=[-0.04208304  0.4037858   0.00206619 -0.48171408], action=0, reward=1.0, next_state=[-0.03400733  0.20863474 -0.00756809 -0.18838066]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 76 ] state=[-0.03400733  0.20863474 -0.00756809 -0.18838066], action=0, reward=1.0, next_state=[-0.02983463  0.01362188 -0.01133571  0.10190526]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 77 ] state=[-0.02983463  0.01362188 -0.01133571  0.10190526], action=1, reward=1.0, next_state=[-0.02956219  0.20890444 -0.0092976  -0.19433238]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 78 ] state=[-0.02956219  0.20890444 -0.0092976  -0.19433238], action=1, reward=1.0, next_state=[-0.02538411  0.40415814 -0.01318425 -0.48993374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 79 ] state=[-0.02538411  0.40415814 -0.01318425 -0.48993374], action=0, reward=1.0, next_state=[-0.01730094  0.20922464 -0.02298292 -0.20143496]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 80 ] state=[-0.01730094  0.20922464 -0.02298292 -0.20143496], action=1, reward=1.0, next_state=[-0.01311645  0.40466763 -0.02701162 -0.50127839]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 81 ] state=[-0.01311645  0.40466763 -0.02701162 -0.50127839], action=1, reward=1.0, next_state=[-0.0050231   0.60015972 -0.03703719 -0.80235001]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 82 ] state=[-0.0050231   0.60015972 -0.03703719 -0.80235001], action=1, reward=1.0, next_state=[ 0.0069801   0.79576947 -0.05308419 -1.10644986]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 241 ][ timestamp 83 ] state=[ 0.0069801   0.79576947 -0.05308419 -1.10644986], action=0, reward=1.0, next_state=[ 0.02289549  0.60138407 -0.07521319 -0.8308819 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 84 ] state=[ 0.02289549  0.60138407 -0.07521319 -0.8308819 ], action=0, reward=1.0, next_state=[ 0.03492317  0.40736627 -0.09183083 -0.56276982]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 85 ] state=[ 0.03492317  0.40736627 -0.09183083 -0.56276982], action=1, reward=1.0, next_state=[ 0.04307049  0.60364871 -0.10308622 -0.88291326]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 86 ] state=[ 0.04307049  0.60364871 -0.10308622 -0.88291326], action=1, reward=1.0, next_state=[ 0.05514347  0.80000821 -0.12074449 -1.2061426 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 87 ] state=[ 0.05514347  0.80000821 -0.12074449 -1.2061426 ], action=1, reward=1.0, next_state=[ 0.07114363  0.99646552 -0.14486734 -1.53409571]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 88 ] state=[ 0.07114363  0.99646552 -0.14486734 -1.53409571], action=1, reward=1.0, next_state=[ 0.09107294  1.19300497 -0.17554925 -1.86825897]\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ][ timestamp 89 ] state=[ 0.09107294  1.19300497 -0.17554925 -1.86825897], action=0, reward=-1.0, next_state=[ 0.11493304  1.00018487 -0.21291443 -1.63482086]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 241: Exploration_rate=0.01. Score=89.\n",
      "[ episode 242 ] state=[-0.01610625  0.00269935 -0.01082219 -0.02564152]\n",
      "[ episode 242 ][ timestamp 1 ] state=[-0.01610625  0.00269935 -0.01082219 -0.02564152], action=1, reward=1.0, next_state=[-0.01605226  0.19797481 -0.01133502 -0.32171923]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 2 ] state=[-0.01605226  0.19797481 -0.01133502 -0.32171923], action=1, reward=1.0, next_state=[-0.01209277  0.39325633 -0.01776941 -0.61795511]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 3 ] state=[-0.01209277  0.39325633 -0.01776941 -0.61795511], action=1, reward=1.0, next_state=[-0.00422764  0.58862194 -0.03012851 -0.91618119]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 4 ] state=[-0.00422764  0.58862194 -0.03012851 -0.91618119], action=0, reward=1.0, next_state=[ 0.0075448   0.39392007 -0.04845213 -0.63311737]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 5 ] state=[ 0.0075448   0.39392007 -0.04845213 -0.63311737], action=0, reward=1.0, next_state=[ 0.0154232   0.19950634 -0.06111448 -0.35607836]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 6 ] state=[ 0.0154232   0.19950634 -0.06111448 -0.35607836], action=0, reward=1.0, next_state=[ 0.01941333  0.00530413 -0.06823605 -0.08327536]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 7 ] state=[ 0.01941333  0.00530413 -0.06823605 -0.08327536], action=1, reward=1.0, next_state=[ 0.01951941  0.2013345  -0.06990155 -0.39668246]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 8 ] state=[ 0.01951941  0.2013345  -0.06990155 -0.39668246], action=0, reward=1.0, next_state=[ 0.0235461   0.00727033 -0.0778352  -0.12683141]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 9 ] state=[ 0.0235461   0.00727033 -0.0778352  -0.12683141], action=1, reward=1.0, next_state=[ 0.02369151  0.20341605 -0.08037183 -0.44301967]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 10 ] state=[ 0.02369151  0.20341605 -0.08037183 -0.44301967], action=0, reward=1.0, next_state=[ 0.02775983  0.00951787 -0.08923222 -0.17671516]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 11 ] state=[ 0.02775983  0.00951787 -0.08923222 -0.17671516], action=1, reward=1.0, next_state=[ 0.02795018  0.20579599 -0.09276653 -0.49616046]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 12 ] state=[ 0.02795018  0.20579599 -0.09276653 -0.49616046], action=0, reward=1.0, next_state=[ 0.0320661   0.0120962  -0.10268974 -0.23409432]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 13 ] state=[ 0.0320661   0.0120962  -0.10268974 -0.23409432], action=0, reward=1.0, next_state=[ 0.03230803 -0.18142005 -0.10737162  0.02451315]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 14 ] state=[ 0.03230803 -0.18142005 -0.10737162  0.02451315], action=0, reward=1.0, next_state=[ 0.02867963 -0.37485146 -0.10688136  0.28148273]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 15 ] state=[ 0.02867963 -0.37485146 -0.10688136  0.28148273], action=1, reward=1.0, next_state=[ 0.0211826  -0.17838016 -0.10125171 -0.04290582]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 16 ] state=[ 0.0211826  -0.17838016 -0.10125171 -0.04290582], action=0, reward=1.0, next_state=[ 0.01761499 -0.37191542 -0.10210982  0.21619309]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 17 ] state=[ 0.01761499 -0.37191542 -0.10210982  0.21619309], action=0, reward=1.0, next_state=[ 0.01017669 -0.56544064 -0.09778596  0.47500076]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 18 ] state=[ 0.01017669 -0.56544064 -0.09778596  0.47500076], action=1, reward=1.0, next_state=[-0.00113213 -0.36908366 -0.08828594  0.15316907]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 19 ] state=[-0.00113213 -0.36908366 -0.08828594  0.15316907], action=0, reward=1.0, next_state=[-0.0085138  -0.56283783 -0.08522256  0.41674606]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 20 ] state=[-0.0085138  -0.56283783 -0.08522256  0.41674606], action=0, reward=1.0, next_state=[-0.01977056 -0.75665515 -0.07688764  0.6813918 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 21 ] state=[-0.01977056 -0.75665515 -0.07688764  0.6813918 ], action=1, reward=1.0, next_state=[-0.03490366 -0.56055431 -0.06325981  0.36552688]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 22 ] state=[-0.03490366 -0.56055431 -0.06325981  0.36552688], action=0, reward=1.0, next_state=[-0.04611475 -0.7547229  -0.05594927  0.63761121]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 23 ] state=[-0.04611475 -0.7547229  -0.05594927  0.63761121], action=1, reward=1.0, next_state=[-0.0612092  -0.55886722 -0.04319704  0.32784689]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 24 ] state=[-0.0612092  -0.55886722 -0.04319704  0.32784689], action=1, reward=1.0, next_state=[-0.07238655 -0.36315777 -0.03664011  0.02186057]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 25 ] state=[-0.07238655 -0.36315777 -0.03664011  0.02186057], action=1, reward=1.0, next_state=[-0.0796497  -0.16753004 -0.0362029  -0.28215385]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 26 ] state=[-0.0796497  -0.16753004 -0.0362029  -0.28215385], action=1, reward=1.0, next_state=[-0.0830003   0.0280891  -0.04184597 -0.58603161]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 27 ] state=[-0.0830003   0.0280891  -0.04184597 -0.58603161], action=1, reward=1.0, next_state=[-0.08243852  0.22377141 -0.0535666  -0.89159725]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 28 ] state=[-0.08243852  0.22377141 -0.0535666  -0.89159725], action=1, reward=1.0, next_state=[-0.07796309  0.41957752 -0.07139855 -1.20062618]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 29 ] state=[-0.07796309  0.41957752 -0.07139855 -1.20062618], action=0, reward=1.0, next_state=[-0.06957154  0.22544801 -0.09541107 -0.93114717]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 30 ] state=[-0.06957154  0.22544801 -0.09541107 -0.93114717], action=1, reward=1.0, next_state=[-0.06506258  0.42171909 -0.11403402 -1.25222308]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 31 ] state=[-0.06506258  0.42171909 -0.11403402 -1.25222308], action=0, reward=1.0, next_state=[-0.0566282   0.22822765 -0.13907848 -0.99732435]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 32 ] state=[-0.0566282   0.22822765 -0.13907848 -0.99732435], action=0, reward=1.0, next_state=[-0.05206365  0.03521156 -0.15902497 -0.7513532 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 33 ] state=[-0.05206365  0.03521156 -0.15902497 -0.7513532 ], action=0, reward=1.0, next_state=[-0.05135942 -0.15740179 -0.17405203 -0.51263525]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 34 ] state=[-0.05135942 -0.15740179 -0.17405203 -0.51263525], action=0, reward=1.0, next_state=[-0.05450745 -0.34970012 -0.18430473 -0.27945919]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 35 ] state=[-0.05450745 -0.34970012 -0.18430473 -0.27945919], action=0, reward=1.0, next_state=[-0.06150146 -0.54178026 -0.18989392 -0.05009797]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 36 ] state=[-0.06150146 -0.54178026 -0.18989392 -0.05009797], action=1, reward=1.0, next_state=[-0.07233706 -0.34451467 -0.19089588 -0.39617124]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 37 ] state=[-0.07233706 -0.34451467 -0.19089588 -0.39617124], action=0, reward=1.0, next_state=[-0.07922735 -0.53648828 -0.1988193  -0.16922487]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 38 ] state=[-0.07922735 -0.53648828 -0.1988193  -0.16922487], action=0, reward=1.0, next_state=[-0.08995712 -0.72829145 -0.2022038   0.05474369]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 242 ][ timestamp 39 ] state=[-0.08995712 -0.72829145 -0.2022038   0.05474369], action=0, reward=1.0, next_state=[-0.10452295 -0.92002665 -0.20110893  0.27744334]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 40 ] state=[-0.10452295 -0.92002665 -0.20110893  0.27744334], action=0, reward=1.0, next_state=[-0.12292348 -1.11179664 -0.19556006  0.50057255]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 41 ] state=[-0.12292348 -1.11179664 -0.19556006  0.50057255], action=0, reward=1.0, next_state=[-0.14515942 -1.30370223 -0.18554861  0.72581518]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 42 ] state=[-0.14515942 -1.30370223 -0.18554861  0.72581518], action=1, reward=1.0, next_state=[-0.17123346 -1.10656566 -0.1710323   0.38094723]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 43 ] state=[-0.17123346 -1.10656566 -0.1710323   0.38094723], action=0, reward=1.0, next_state=[-0.19336477 -1.29889874 -0.16341336  0.61519882]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 44 ] state=[-0.19336477 -1.29889874 -0.16341336  0.61519882], action=1, reward=1.0, next_state=[-0.21934275 -1.10191625 -0.15110938  0.27583148]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 45 ] state=[-0.21934275 -1.10191625 -0.15110938  0.27583148], action=1, reward=1.0, next_state=[-0.24138107 -0.90499769 -0.14559275 -0.06043771]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 46 ] state=[-0.24138107 -0.90499769 -0.14559275 -0.06043771], action=0, reward=1.0, next_state=[-0.25948103 -1.09776465 -0.14680151  0.18300033]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 47 ] state=[-0.25948103 -1.09776465 -0.14680151  0.18300033], action=0, reward=1.0, next_state=[-0.28143632 -1.29051439 -0.1431415   0.42601032]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 48 ] state=[-0.28143632 -1.29051439 -0.1431415   0.42601032], action=0, reward=1.0, next_state=[-0.30724661 -1.48334953 -0.1346213   0.67036473]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 49 ] state=[-0.30724661 -1.48334953 -0.1346213   0.67036473], action=0, reward=1.0, next_state=[-0.3369136  -1.67636849 -0.121214    0.91781435]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 50 ] state=[-0.3369136  -1.67636849 -0.121214    0.91781435], action=1, reward=1.0, next_state=[-0.37044097 -1.47983476 -0.10285771  0.58962713]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 51 ] state=[-0.37044097 -1.47983476 -0.10285771  0.58962713], action=0, reward=1.0, next_state=[-0.40003766 -1.67337749 -0.09106517  0.84821998]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 52 ] state=[-0.40003766 -1.67337749 -0.09106517  0.84821998], action=0, reward=1.0, next_state=[-0.43350521 -1.86714722 -0.07410077  1.11093406]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 53 ] state=[-0.43350521 -1.86714722 -0.07410077  1.11093406], action=1, reward=1.0, next_state=[-0.47084816 -1.67113419 -0.05188209  0.79595567]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 54 ] state=[-0.47084816 -1.67113419 -0.05188209  0.79595567], action=0, reward=1.0, next_state=[-0.50427084 -1.86550722 -0.03596298  1.07187641]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 55 ] state=[-0.50427084 -1.86550722 -0.03596298  1.07187641], action=1, reward=1.0, next_state=[-0.54158099 -1.66992875 -0.01452545  0.76812756]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 56 ] state=[-0.54158099 -1.66992875 -0.01452545  0.76812756], action=0, reward=1.0, next_state=[-5.74979560e-01 -1.86484777e+00  8.37102610e-04  1.05620491e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 57 ] state=[-5.74979560e-01 -1.86484777e+00  8.37102610e-04  1.05620491e+00], action=0, reward=1.0, next_state=[-0.61227652 -2.0599808   0.0219612   1.34915047]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 58 ] state=[-0.61227652 -2.0599808   0.0219612   1.34915047], action=1, reward=1.0, next_state=[-0.65347613 -1.86514158  0.04894421  1.06341819]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 59 ] state=[-0.65347613 -1.86514158  0.04894421  1.06341819], action=0, reward=1.0, next_state=[-0.69077896 -2.06087609  0.07021257  1.3710522 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 60 ] state=[-0.69077896 -2.06087609  0.07021257  1.3710522 ], action=0, reward=1.0, next_state=[-0.73199648 -2.25680247  0.09763362  1.6848432 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 61 ] state=[-0.73199648 -2.25680247  0.09763362  1.6848432 ], action=0, reward=1.0, next_state=[-0.77713253 -2.45290943  0.13133048  2.00626145]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 62 ] state=[-0.77713253 -2.45290943  0.13133048  2.00626145], action=0, reward=1.0, next_state=[-0.82619072 -2.64913281  0.17145571  2.33656213]\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ][ timestamp 63 ] state=[-0.82619072 -2.64913281  0.17145571  2.33656213], action=1, reward=-1.0, next_state=[-0.87917338 -2.45592465  0.21818695  2.10116064]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 242: Exploration_rate=0.01. Score=63.\n",
      "[ episode 243 ] state=[-0.01493549  0.04803839  0.04733765  0.04621163]\n",
      "[ episode 243 ][ timestamp 1 ] state=[-0.01493549  0.04803839  0.04733765  0.04621163], action=0, reward=1.0, next_state=[-0.01397472 -0.14772927  0.04826188  0.35344623]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 2 ] state=[-0.01397472 -0.14772927  0.04826188  0.35344623], action=1, reward=1.0, next_state=[-0.0169293   0.04667439  0.05533081  0.07636377]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 3 ] state=[-0.0169293   0.04667439  0.05533081  0.07636377], action=1, reward=1.0, next_state=[-0.01599582  0.24096127  0.05685808 -0.1983616 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 4 ] state=[-0.01599582  0.24096127  0.05685808 -0.1983616 ], action=0, reward=1.0, next_state=[-0.01117659  0.04507411  0.05289085  0.11170158]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 5 ] state=[-0.01117659  0.04507411  0.05289085  0.11170158], action=1, reward=1.0, next_state=[-0.01027511  0.23939985  0.05512488 -0.16383675]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 6 ] state=[-0.01027511  0.23939985  0.05512488 -0.16383675], action=1, reward=1.0, next_state=[-0.00548711  0.43369112  0.05184815 -0.43863245]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 7 ] state=[-0.00548711  0.43369112  0.05184815 -0.43863245], action=1, reward=1.0, next_state=[ 0.00318671  0.62804236  0.0430755  -0.71453103]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 8 ] state=[ 0.00318671  0.62804236  0.0430755  -0.71453103], action=1, reward=1.0, next_state=[ 0.01574756  0.82254236  0.02878488 -0.99335013]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 9 ] state=[ 0.01574756  0.82254236  0.02878488 -0.99335013], action=1, reward=1.0, next_state=[ 0.03219841  1.01726763  0.00891787 -1.27685545]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 10 ] state=[ 0.03219841  1.01726763  0.00891787 -1.27685545], action=0, reward=1.0, next_state=[ 0.05254376  0.82203311 -0.01661924 -0.98139349]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 11 ] state=[ 0.05254376  0.82203311 -0.01661924 -0.98139349], action=0, reward=1.0, next_state=[ 0.06898442  0.62713778 -0.03624711 -0.6939767 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 12 ] state=[ 0.06898442  0.62713778 -0.03624711 -0.6939767 ], action=1, reward=1.0, next_state=[ 0.08152718  0.82274329 -0.05012664 -0.99784655]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 13 ] state=[ 0.08152718  0.82274329 -0.05012664 -0.99784655], action=0, reward=1.0, next_state=[ 0.09798204  0.62832607 -0.07008357 -0.72131808]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 14 ] state=[ 0.09798204  0.62832607 -0.07008357 -0.72131808], action=0, reward=1.0, next_state=[ 0.11054856  0.43424002 -0.08450993 -0.4514914 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 15 ] state=[ 0.11054856  0.43424002 -0.08450993 -0.4514914 ], action=0, reward=1.0, next_state=[ 0.11923336  0.24040857 -0.09353976 -0.18659821]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 16 ] state=[ 0.11923336  0.24040857 -0.09353976 -0.18659821], action=0, reward=1.0, next_state=[ 0.12404154  0.04674076 -0.09727172  0.07517293]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 17 ] state=[ 0.12404154  0.04674076 -0.09727172  0.07517293], action=1, reward=1.0, next_state=[ 0.12497635  0.24311289 -0.09576827 -0.24654564]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 18 ] state=[ 0.12497635  0.24311289 -0.09576827 -0.24654564], action=1, reward=1.0, next_state=[ 0.12983861  0.43946288 -0.10069918 -0.56783388]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 19 ] state=[ 0.12983861  0.43946288 -0.10069918 -0.56783388], action=0, reward=1.0, next_state=[ 0.13862787  0.24588688 -0.11205586 -0.30849638]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 20 ] state=[ 0.13862787  0.24588688 -0.11205586 -0.30849638], action=0, reward=1.0, next_state=[ 0.1435456   0.05252508 -0.11822578 -0.05314827]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 243 ][ timestamp 21 ] state=[ 0.1435456   0.05252508 -0.11822578 -0.05314827], action=1, reward=1.0, next_state=[ 0.1445961   0.24912635 -0.11928875 -0.38066906]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 22 ] state=[ 0.1445961   0.24912635 -0.11928875 -0.38066906], action=0, reward=1.0, next_state=[ 0.14957863  0.05588242 -0.12690213 -0.12785086]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 23 ] state=[ 0.14957863  0.05588242 -0.12690213 -0.12785086], action=0, reward=1.0, next_state=[ 0.15069628 -0.13721475 -0.12945915  0.1222566 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 24 ] state=[ 0.15069628 -0.13721475 -0.12945915  0.1222566 ], action=0, reward=1.0, next_state=[ 0.14795199 -0.3302672  -0.12701402  0.37145727]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 25 ] state=[ 0.14795199 -0.3302672  -0.12701402  0.37145727], action=0, reward=1.0, next_state=[ 0.14134664 -0.52337743 -0.11958487  0.62154745]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 26 ] state=[ 0.14134664 -0.52337743 -0.11958487  0.62154745], action=1, reward=1.0, next_state=[ 0.13087909 -0.32680644 -0.10715392  0.29372253]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 27 ] state=[ 0.13087909 -0.32680644 -0.10715392  0.29372253], action=0, reward=1.0, next_state=[ 0.12434296 -0.52025048 -0.10127947  0.55078135]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 28 ] state=[ 0.12434296 -0.52025048 -0.10127947  0.55078135], action=1, reward=1.0, next_state=[ 0.11393795 -0.32386277 -0.09026384  0.22798404]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 29 ] state=[ 0.11393795 -0.32386277 -0.09026384  0.22798404], action=0, reward=1.0, next_state=[ 0.1074607  -0.51758662 -0.08570416  0.49088529]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 30 ] state=[ 0.1074607  -0.51758662 -0.08570416  0.49088529], action=1, reward=1.0, next_state=[ 0.09710897 -0.32136688 -0.07588646  0.17246978]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 31 ] state=[ 0.09710897 -0.32136688 -0.07588646  0.17246978], action=0, reward=1.0, next_state=[ 0.09068163 -0.51532531 -0.07243706  0.4402809 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 32 ] state=[ 0.09068163 -0.51532531 -0.07243706  0.4402809 ], action=1, reward=1.0, next_state=[ 0.08037512 -0.31925697 -0.06363144  0.12567177]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 33 ] state=[ 0.08037512 -0.31925697 -0.06363144  0.12567177], action=1, reward=1.0, next_state=[ 0.07398998 -0.12328387 -0.06111801 -0.18638798]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 34 ] state=[ 0.07398998 -0.12328387 -0.06111801 -0.18638798], action=0, reward=1.0, next_state=[ 0.07152431 -0.31748054 -0.06484577  0.08640563]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 35 ] state=[ 0.07152431 -0.31748054 -0.06484577  0.08640563], action=0, reward=1.0, next_state=[ 0.0651747  -0.51161593 -0.06311766  0.35794539]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 36 ] state=[ 0.0651747  -0.51161593 -0.06311766  0.35794539], action=1, reward=1.0, next_state=[ 0.05494238 -0.31565612 -0.05595875  0.0460467 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 37 ] state=[ 0.05494238 -0.31565612 -0.05595875  0.0460467 ], action=0, reward=1.0, next_state=[ 0.04862925 -0.50993286 -0.05503781  0.32056238]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 38 ] state=[ 0.04862925 -0.50993286 -0.05503781  0.32056238], action=1, reward=1.0, next_state=[ 0.0384306  -0.31407206 -0.04862657  0.01104309]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 39 ] state=[ 0.0384306  -0.31407206 -0.04862657  0.01104309], action=1, reward=1.0, next_state=[ 0.03214916 -0.11828769 -0.0484057  -0.29657691]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 40 ] state=[ 0.03214916 -0.11828769 -0.0484057  -0.29657691], action=0, reward=1.0, next_state=[ 0.0297834  -0.31268736 -0.05433724 -0.01954468]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 41 ] state=[ 0.0297834  -0.31268736 -0.05433724 -0.01954468], action=1, reward=1.0, next_state=[ 0.02352965 -0.11682996 -0.05472814 -0.32886447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 42 ] state=[ 0.02352965 -0.11682996 -0.05472814 -0.32886447], action=0, reward=1.0, next_state=[ 0.02119306 -0.31113183 -0.06130543 -0.05393008]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 43 ] state=[ 0.02119306 -0.31113183 -0.06130543 -0.05393008], action=0, reward=1.0, next_state=[ 0.01497042 -0.50532361 -0.06238403  0.21879788]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 44 ] state=[ 0.01497042 -0.50532361 -0.06238403  0.21879788], action=0, reward=1.0, next_state=[ 0.00486395 -0.69950089 -0.05800807  0.49116821]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 45 ] state=[ 0.00486395 -0.69950089 -0.05800807  0.49116821], action=0, reward=1.0, next_state=[-0.00912607 -0.89375865 -0.04818471  0.76501993]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 46 ] state=[-0.00912607 -0.89375865 -0.04818471  0.76501993], action=1, reward=1.0, next_state=[-0.02700124 -0.69800746 -0.03288431  0.45757313]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 47 ] state=[-0.02700124 -0.69800746 -0.03288431  0.45757313], action=1, reward=1.0, next_state=[-0.04096139 -0.50243643 -0.02373284  0.15470893]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 48 ] state=[-0.04096139 -0.50243643 -0.02373284  0.15470893], action=1, reward=1.0, next_state=[-0.05101012 -0.30698285 -0.02063867 -0.14536568]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 49 ] state=[-0.05101012 -0.30698285 -0.02063867 -0.14536568], action=1, reward=1.0, next_state=[-0.05714978 -0.11157151 -0.02354598 -0.4444876 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 50 ] state=[-0.05714978 -0.11157151 -0.02354598 -0.4444876 ], action=1, reward=1.0, next_state=[-0.05938121  0.08387554 -0.03243573 -0.74449878]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 51 ] state=[-0.05938121  0.08387554 -0.03243573 -0.74449878], action=0, reward=1.0, next_state=[-0.0577037  -0.11078412 -0.04732571 -0.46219731]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 52 ] state=[-0.0577037  -0.11078412 -0.04732571 -0.46219731], action=0, reward=1.0, next_state=[-0.05991938 -0.30520639 -0.05656965 -0.184799  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 53 ] state=[-0.05991938 -0.30520639 -0.05656965 -0.184799  ], action=0, reward=1.0, next_state=[-0.06602351 -0.49947522 -0.06026563  0.0895155 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 54 ] state=[-0.06602351 -0.49947522 -0.06026563  0.0895155 ], action=0, reward=1.0, next_state=[-0.07601301 -0.69368389 -0.05847532  0.36259227]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 55 ] state=[-0.07601301 -0.69368389 -0.05847532  0.36259227], action=0, reward=1.0, next_state=[-0.08988669 -0.88792808 -0.05122348  0.6362786 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 56 ] state=[-0.08988669 -0.88792808 -0.05122348  0.6362786 ], action=1, reward=1.0, next_state=[-0.10764525 -0.69213057 -0.03849791  0.32791445]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 57 ] state=[-0.10764525 -0.69213057 -0.03849791  0.32791445], action=0, reward=1.0, next_state=[-0.12148786 -0.8866839  -0.03193962  0.60821263]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 58 ] state=[-0.12148786 -0.8866839  -0.03193962  0.60821263], action=1, reward=1.0, next_state=[-0.13922154 -0.69113032 -0.01977536  0.30564321]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 59 ] state=[-0.13922154 -0.69113032 -0.01977536  0.30564321], action=1, reward=1.0, next_state=[-0.15304415 -0.49573223 -0.0136625   0.0067898 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 60 ] state=[-0.15304415 -0.49573223 -0.0136625   0.0067898 ], action=1, reward=1.0, next_state=[-0.16295879 -0.30041703 -0.0135267  -0.29017231]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 61 ] state=[-0.16295879 -0.30041703 -0.0135267  -0.29017231], action=0, reward=1.0, next_state=[-0.16896713 -0.49534351 -0.01933015 -0.00178606]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 62 ] state=[-0.16896713 -0.49534351 -0.01933015 -0.00178606], action=0, reward=1.0, next_state=[-0.178874   -0.69018299 -0.01936587  0.28473584]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 63 ] state=[-0.178874   -0.69018299 -0.01936587  0.28473584], action=0, reward=1.0, next_state=[-0.19267766 -0.88502346 -0.01367115  0.57124853]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 64 ] state=[-0.19267766 -0.88502346 -0.01367115  0.57124853], action=0, reward=1.0, next_state=[-0.21037813 -1.07995105 -0.00224618  0.85959341]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 65 ] state=[-0.21037813 -1.07995105 -0.00224618  0.85959341], action=0, reward=1.0, next_state=[-0.23197715 -1.27504234  0.01494568  1.15156922]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 66 ] state=[-0.23197715 -1.27504234  0.01494568  1.15156922], action=1, reward=1.0, next_state=[-0.257478   -1.08011855  0.03797707  0.86361006]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 67 ] state=[-0.257478   -1.08011855  0.03797707  0.86361006], action=1, reward=1.0, next_state=[-0.27908037 -0.88553362  0.05524927  0.58310569]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 68 ] state=[-0.27908037 -0.88553362  0.05524927  0.58310569], action=0, reward=1.0, next_state=[-0.29679104 -1.0813843   0.06691138  0.89266847]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 243 ][ timestamp 69 ] state=[-0.29679104 -1.0813843   0.06691138  0.89266847], action=1, reward=1.0, next_state=[-0.31841873 -0.88723062  0.08476475  0.62174693]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 70 ] state=[-0.31841873 -0.88723062  0.08476475  0.62174693], action=1, reward=1.0, next_state=[-0.33616334 -0.69338824  0.09719969  0.35691832]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 71 ] state=[-0.33616334 -0.69338824  0.09719969  0.35691832], action=1, reward=1.0, next_state=[-0.35003111 -0.49977288  0.10433806  0.09639785]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 72 ] state=[-0.35003111 -0.49977288  0.10433806  0.09639785], action=1, reward=1.0, next_state=[-0.36002656 -0.30628907  0.10626601 -0.16162977]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 73 ] state=[-0.36002656 -0.30628907  0.10626601 -0.16162977], action=0, reward=1.0, next_state=[-0.36615235 -0.50275915  0.10303342  0.16259638]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 74 ] state=[-0.36615235 -0.50275915  0.10303342  0.16259638], action=1, reward=1.0, next_state=[-0.37620753 -0.30925154  0.10628535 -0.09588744]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 75 ] state=[-0.37620753 -0.30925154  0.10628535 -0.09588744], action=1, reward=1.0, next_state=[-0.38239256 -0.11580078  0.1043676  -0.35323705]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 76 ] state=[-0.38239256 -0.11580078  0.1043676  -0.35323705], action=1, reward=1.0, next_state=[-0.38470858  0.07769426  0.09730286 -0.61127189]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 77 ] state=[-0.38470858  0.07769426  0.09730286 -0.61127189], action=1, reward=1.0, next_state=[-0.38315469  0.27133123  0.08507742 -0.87179151]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 78 ] state=[-0.38315469  0.27133123  0.08507742 -0.87179151], action=1, reward=1.0, next_state=[-0.37772807  0.4651995   0.06764159 -1.13655952]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 79 ] state=[-0.37772807  0.4651995   0.06764159 -1.13655952], action=1, reward=1.0, next_state=[-0.36842408  0.6593746   0.0449104  -1.40728464]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 80 ] state=[-0.36842408  0.6593746   0.0449104  -1.40728464], action=1, reward=1.0, next_state=[-0.35523658  0.85391139  0.01676471 -1.68559638]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 81 ] state=[-0.35523658  0.85391139  0.01676471 -1.68559638], action=0, reward=1.0, next_state=[-0.33815836  0.65859954 -0.01694722 -1.38774118]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 82 ] state=[-0.33815836  0.65859954 -0.01694722 -1.38774118], action=0, reward=1.0, next_state=[-0.32498637  0.46369285 -0.04470204 -1.10040536]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 83 ] state=[-0.32498637  0.46369285 -0.04470204 -1.10040536], action=0, reward=1.0, next_state=[-0.31571251  0.26918676 -0.06671015 -0.82207572]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 84 ] state=[-0.31571251  0.26918676 -0.06671015 -0.82207572], action=0, reward=1.0, next_state=[-0.31032877  0.07503787 -0.08315167 -0.55109839]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 85 ] state=[-0.31032877  0.07503787 -0.08315167 -0.55109839], action=0, reward=1.0, next_state=[-0.30882802 -0.11882381 -0.09417363 -0.28572902]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 86 ] state=[-0.30882802 -0.11882381 -0.09417363 -0.28572902], action=1, reward=1.0, next_state=[-0.31120449  0.07750624 -0.09988821 -0.60656531]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 87 ] state=[-0.31120449  0.07750624 -0.09988821 -0.60656531], action=0, reward=1.0, next_state=[-0.30965437 -0.1160876  -0.11201952 -0.34694037]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 88 ] state=[-0.30965437 -0.1160876  -0.11201952 -0.34694037], action=0, reward=1.0, next_state=[-0.31197612 -0.30945276 -0.11895833 -0.09157545]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 89 ] state=[-0.31197612 -0.30945276 -0.11895833 -0.09157545], action=0, reward=1.0, next_state=[-0.31816517 -0.50268656 -0.12078984  0.16133549]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 90 ] state=[-0.31816517 -0.50268656 -0.12078984  0.16133549], action=1, reward=1.0, next_state=[-0.32821891 -0.30606105 -0.11756313 -0.1668797 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 91 ] state=[-0.32821891 -0.30606105 -0.11756313 -0.1668797 ], action=0, reward=1.0, next_state=[-0.33434013 -0.49932111 -0.12090072  0.0865254 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 92 ] state=[-0.33434013 -0.49932111 -0.12090072  0.0865254 ], action=0, reward=1.0, next_state=[-0.34432655 -0.69252127 -0.11917021  0.33875193]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 93 ] state=[-0.34432655 -0.69252127 -0.11917021  0.33875193], action=1, reward=1.0, next_state=[-0.35817697 -0.49592301 -0.11239518  0.01099289]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 94 ] state=[-0.35817697 -0.49592301 -0.11239518  0.01099289], action=0, reward=1.0, next_state=[-0.36809543 -0.68926864 -0.11217532  0.26620676]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 95 ] state=[-0.36809543 -0.68926864 -0.11217532  0.26620676], action=0, reward=1.0, next_state=[-0.38188081 -0.88262575 -0.10685118  0.52150911]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 96 ] state=[-0.38188081 -0.88262575 -0.10685118  0.52150911], action=0, reward=1.0, next_state=[-0.39953332 -1.07609423 -0.096421    0.77870224]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 97 ] state=[-0.39953332 -1.07609423 -0.096421    0.77870224], action=1, reward=1.0, next_state=[-0.42105521 -0.87978809 -0.08084696  0.45730691]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 98 ] state=[-0.42105521 -0.87978809 -0.08084696  0.45730691], action=1, reward=1.0, next_state=[-0.43865097 -0.68362182 -0.07170082  0.14027549]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 99 ] state=[-0.43865097 -0.68362182 -0.07170082  0.14027549], action=1, reward=1.0, next_state=[-0.4523234  -0.4875501  -0.06889531 -0.17413839]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 100 ] state=[-0.4523234  -0.4875501  -0.06889531 -0.17413839], action=1, reward=1.0, next_state=[-0.46207441 -0.29151322 -0.07237808 -0.4877353 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 101 ] state=[-0.46207441 -0.29151322 -0.07237808 -0.4877353 ], action=0, reward=1.0, next_state=[-0.46790467 -0.48554333 -0.08213278 -0.21871273]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 102 ] state=[-0.46790467 -0.48554333 -0.08213278 -0.21871273], action=0, reward=1.0, next_state=[-0.47761554 -0.67940102 -0.08650704  0.04697367]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 103 ] state=[-0.47761554 -0.67940102 -0.08650704  0.04697367], action=0, reward=1.0, next_state=[-0.49120356 -0.87318286 -0.08556756  0.31115813]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 104 ] state=[-0.49120356 -0.87318286 -0.08556756  0.31115813], action=0, reward=1.0, next_state=[-0.50866722 -1.06698812 -0.0793444   0.57567624]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 105 ] state=[-0.50866722 -1.06698812 -0.0793444   0.57567624], action=1, reward=1.0, next_state=[-0.53000698 -0.87084881 -0.06783087  0.25909011]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 106 ] state=[-0.53000698 -0.87084881 -0.06783087  0.25909011], action=0, reward=1.0, next_state=[-0.54742395 -1.06494015 -0.06264907  0.52963062]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 107 ] state=[-0.54742395 -1.06494015 -0.06264907  0.52963062], action=0, reward=1.0, next_state=[-0.56872276 -1.25912741 -0.05205646  0.80193329]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 108 ] state=[-0.56872276 -1.25912741 -0.05205646  0.80193329], action=0, reward=1.0, next_state=[-0.5939053  -1.45349828 -0.03601779  1.07779696]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 109 ] state=[-0.5939053  -1.45349828 -0.03601779  1.07779696], action=1, reward=1.0, next_state=[-0.62297527 -1.2579196  -0.01446186  0.77403226]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 110 ] state=[-0.62297527 -1.2579196  -0.01446186  0.77403226], action=0, reward=1.0, next_state=[-6.48133662e-01 -1.45283964e+00  1.01878979e-03  1.06213012e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 111 ] state=[-6.48133662e-01 -1.45283964e+00  1.01878979e-03  1.06213012e+00], action=0, reward=1.0, next_state=[-0.67719046 -1.64797507  0.02226139  1.35513263]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 112 ] state=[-0.67719046 -1.64797507  0.02226139  1.35513263], action=0, reward=1.0, next_state=[-0.71014996 -1.84336921  0.04936404  1.65469553]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 243 ][ timestamp 113 ] state=[-0.71014996 -1.84336921  0.04936404  1.65469553], action=0, reward=1.0, next_state=[-0.74701734 -2.0390313   0.08245796  1.96233827]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 114 ] state=[-0.74701734 -2.0390313   0.08245796  1.96233827], action=1, reward=1.0, next_state=[-0.78779797 -1.84487351  0.12170472  1.69630631]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 115 ] state=[-0.78779797 -1.84487351  0.12170472  1.69630631], action=1, reward=1.0, next_state=[-0.82469544 -1.65134769  0.15563085  1.44385774]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 116 ] state=[-0.82469544 -1.65134769  0.15563085  1.44385774], action=1, reward=1.0, next_state=[-0.85772239 -1.45844571  0.184508    1.20357289]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 117 ] state=[-0.85772239 -1.45844571  0.184508    1.20357289], action=1, reward=1.0, next_state=[-0.8868913  -1.2661246   0.20857946  0.97392582]\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ][ timestamp 118 ] state=[-0.8868913  -1.2661246   0.20857946  0.97392582], action=1, reward=-1.0, next_state=[-0.9122138  -1.07431813  0.22805798  0.75333061]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 243: Exploration_rate=0.01. Score=118.\n",
      "[ episode 244 ] state=[-0.04246039 -0.02197169  0.04029732 -0.02889988]\n",
      "[ episode 244 ][ timestamp 1 ] state=[-0.04246039 -0.02197169  0.04029732 -0.02889988], action=1, reward=1.0, next_state=[-0.04289983  0.1725499   0.03971932 -0.30860118]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 2 ] state=[-0.04289983  0.1725499   0.03971932 -0.30860118], action=0, reward=1.0, next_state=[-0.03944883 -0.02311482  0.0335473  -0.00366117]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 3 ] state=[-0.03944883 -0.02311482  0.0335473  -0.00366117], action=1, reward=1.0, next_state=[-0.03991112  0.17151035  0.03347408 -0.28557361]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 4 ] state=[-0.03991112  0.17151035  0.03347408 -0.28557361], action=0, reward=1.0, next_state=[-0.03648092 -0.02407262  0.0277626   0.01747603]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 5 ] state=[-0.03648092 -0.02407262  0.0277626   0.01747603], action=1, reward=1.0, next_state=[-0.03696237  0.17064041  0.02811213 -0.26631981]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 6 ] state=[-0.03696237  0.17064041  0.02811213 -0.26631981], action=0, reward=1.0, next_state=[-0.03354956 -0.02487125  0.02278573  0.03509568]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 7 ] state=[-0.03354956 -0.02487125  0.02278573  0.03509568], action=0, reward=1.0, next_state=[-0.03404699 -0.22031242  0.02348764  0.33487976]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 8 ] state=[-0.03404699 -0.22031242  0.02348764  0.33487976], action=0, reward=1.0, next_state=[-0.03845323 -0.41576063  0.03018524  0.63487596]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 9 ] state=[-0.03845323 -0.41576063  0.03018524  0.63487596], action=1, reward=1.0, next_state=[-0.04676845 -0.22107243  0.04288276  0.35184979]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 10 ] state=[-0.04676845 -0.22107243  0.04288276  0.35184979], action=1, reward=1.0, next_state=[-0.0511899  -0.0265857   0.04991975  0.07299156]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 11 ] state=[-0.0511899  -0.0265857   0.04991975  0.07299156], action=1, reward=1.0, next_state=[-0.05172161  0.16778637  0.05137958 -0.20353302]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 12 ] state=[-0.05172161  0.16778637  0.05137958 -0.20353302], action=0, reward=1.0, next_state=[-0.04836588 -0.02803128  0.04730892  0.10490479]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 13 ] state=[-0.04836588 -0.02803128  0.04730892  0.10490479], action=0, reward=1.0, next_state=[-0.04892651 -0.22379816  0.04940702  0.41213018]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 14 ] state=[-0.04892651 -0.22379816  0.04940702  0.41213018], action=1, reward=1.0, next_state=[-0.05340247 -0.02941013  0.05764962  0.13542371]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 15 ] state=[-0.05340247 -0.02941013  0.05764962  0.13542371], action=1, reward=1.0, next_state=[-0.05399067  0.16484072  0.0603581  -0.1385289 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 16 ] state=[-0.05399067  0.16484072  0.0603581  -0.1385289 ], action=1, reward=1.0, next_state=[-0.05069386  0.35904857  0.05758752 -0.41157569]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 17 ] state=[-0.05069386  0.35904857  0.05758752 -0.41157569], action=1, reward=1.0, next_state=[-0.04351289  0.55330888  0.04935601 -0.68556175]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 18 ] state=[-0.04351289  0.55330888  0.04935601 -0.68556175], action=0, reward=1.0, next_state=[-0.03244671  0.35753773  0.03564477 -0.37775785]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 19 ] state=[-0.03244671  0.35753773  0.03564477 -0.37775785], action=0, reward=1.0, next_state=[-0.02529596  0.16192815  0.02808961 -0.07405252]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 20 ] state=[-0.02529596  0.16192815  0.02808961 -0.07405252], action=0, reward=1.0, next_state=[-0.02205739 -0.033585    0.02660856  0.22735878]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 21 ] state=[-0.02205739 -0.033585    0.02660856  0.22735878], action=1, reward=1.0, next_state=[-0.02272909  0.16114678  0.03115574 -0.0568135 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 22 ] state=[-0.02272909  0.16114678  0.03115574 -0.0568135 ], action=0, reward=1.0, next_state=[-0.01950616 -0.03440772  0.03001947  0.2455342 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 23 ] state=[-0.01950616 -0.03440772  0.03001947  0.2455342 ], action=1, reward=1.0, next_state=[-0.02019431  0.1602729   0.03493015 -0.03753076]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 24 ] state=[-0.02019431  0.1602729   0.03493015 -0.03753076], action=1, reward=1.0, next_state=[-0.01698885  0.35487699  0.03417954 -0.31899146]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 25 ] state=[-0.01698885  0.35487699  0.03417954 -0.31899146], action=1, reward=1.0, next_state=[-0.00989131  0.54949591  0.02779971 -0.60070249]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 26 ] state=[-0.00989131  0.54949591  0.02779971 -0.60070249], action=1, reward=1.0, next_state=[ 0.0010986   0.74421815  0.01578566 -0.88450094]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 27 ] state=[ 0.0010986   0.74421815  0.01578566 -0.88450094], action=1, reward=1.0, next_state=[ 0.01598297  0.93912224 -0.00190436 -1.17217987]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 28 ] state=[ 0.01598297  0.93912224 -0.00190436 -1.17217987], action=1, reward=1.0, next_state=[ 0.03476541  1.1342689  -0.02534796 -1.4654592 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 29 ] state=[ 0.03476541  1.1342689  -0.02534796 -1.4654592 ], action=0, reward=1.0, next_state=[ 0.05745079  0.93946638 -0.05465714 -1.18080078]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 30 ] state=[ 0.05745079  0.93946638 -0.05465714 -1.18080078], action=0, reward=1.0, next_state=[ 0.07624012  0.74509486 -0.07827316 -0.9057401 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 31 ] state=[ 0.07624012  0.74509486 -0.07827316 -0.9057401 ], action=0, reward=1.0, next_state=[ 0.09114202  0.55111508 -0.09638796 -0.63865014]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 32 ] state=[ 0.09114202  0.55111508 -0.09638796 -0.63865014], action=0, reward=1.0, next_state=[ 0.10216432  0.35745987 -0.10916096 -0.37780987]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 33 ] state=[ 0.10216432  0.35745987 -0.10916096 -0.37780987], action=0, reward=1.0, next_state=[ 0.10931351  0.16404384 -0.11671716 -0.12144231]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 34 ] state=[ 0.10931351  0.16404384 -0.11671716 -0.12144231], action=0, reward=1.0, next_state=[ 0.11259439 -0.02922927 -0.11914601  0.13225791]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 35 ] state=[ 0.11259439 -0.02922927 -0.11914601  0.13225791], action=0, reward=1.0, next_state=[ 0.11200981 -0.22246088 -0.11650085  0.38510435]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 36 ] state=[ 0.11200981 -0.22246088 -0.11650085  0.38510435], action=0, reward=1.0, next_state=[ 0.10756059 -0.41575284 -0.10879876  0.63890311]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 37 ] state=[ 0.10756059 -0.41575284 -0.10879876  0.63890311], action=0, reward=1.0, next_state=[ 0.09924553 -0.60920309 -0.0960207   0.89543899]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 38 ] state=[ 0.09924553 -0.60920309 -0.0960207   0.89543899], action=0, reward=1.0, next_state=[ 0.08706147 -0.80290114 -0.07811192  1.15646094]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 39 ] state=[ 0.08706147 -0.80290114 -0.07811192  1.15646094], action=0, reward=1.0, next_state=[ 0.07100345 -0.99692277 -0.0549827   1.42366442]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 40 ] state=[ 0.07100345 -0.99692277 -0.0549827   1.42366442], action=0, reward=1.0, next_state=[ 0.05106499 -1.19132349 -0.02650941  1.69866807]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 41 ] state=[ 0.05106499 -1.19132349 -0.02650941  1.69866807], action=1, reward=1.0, next_state=[ 0.02723852 -0.99590619  0.00746395  1.39785227]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 42 ] state=[ 0.02723852 -0.99590619  0.00746395  1.39785227], action=1, reward=1.0, next_state=[ 0.0073204  -0.80087785  0.035421    1.10751228]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 244 ][ timestamp 43 ] state=[ 0.0073204  -0.80087785  0.035421    1.10751228], action=0, reward=1.0, next_state=[-0.00869716 -0.99644704  0.05757124  1.41109365]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 44 ] state=[-0.00869716 -0.99644704  0.05757124  1.41109365], action=0, reward=1.0, next_state=[-0.0286261  -1.19223362  0.08579311  1.72120355]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 45 ] state=[-0.0286261  -1.19223362  0.08579311  1.72120355], action=1, reward=1.0, next_state=[-0.05247077 -0.99819262  0.12021719  1.45640482]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 46 ] state=[-0.05247077 -0.99819262  0.12021719  1.45640482], action=1, reward=1.0, next_state=[-0.07243462 -0.80473362  0.14934528  1.2035695 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 47 ] state=[-0.07243462 -0.80473362  0.14934528  1.2035695 ], action=1, reward=1.0, next_state=[-0.0885293  -0.61182401  0.17341667  0.96117056]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 48 ] state=[-0.0885293  -0.61182401  0.17341667  0.96117056], action=0, reward=1.0, next_state=[-0.10076578 -0.80879881  0.19264008  1.30293046]\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ][ timestamp 49 ] state=[-0.10076578 -0.80879881  0.19264008  1.30293046], action=1, reward=-1.0, next_state=[-0.11694175 -0.61657022  0.21869869  1.07620781]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 244: Exploration_rate=0.01. Score=49.\n",
      "[ episode 245 ] state=[ 0.00109929 -0.03127259 -0.01830105 -0.02658056]\n",
      "[ episode 245 ][ timestamp 1 ] state=[ 0.00109929 -0.03127259 -0.01830105 -0.02658056], action=0, reward=1.0, next_state=[ 0.00047384 -0.22612737 -0.01883266  0.26027246]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 2 ] state=[ 0.00047384 -0.22612737 -0.01883266  0.26027246], action=0, reward=1.0, next_state=[-0.00404871 -0.42097549 -0.01362721  0.54695633]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 3 ] state=[-0.00404871 -0.42097549 -0.01362721  0.54695633], action=0, reward=1.0, next_state=[-0.01246822 -0.61590336 -0.00268808  0.83531471]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 4 ] state=[-0.01246822 -0.61590336 -0.00268808  0.83531471], action=1, reward=1.0, next_state=[-0.02478628 -0.42074479  0.01401821  0.54178762]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 5 ] state=[-0.02478628 -0.42074479  0.01401821  0.54178762], action=1, reward=1.0, next_state=[-0.03320118 -0.22582264  0.02485396  0.25355435]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 6 ] state=[-0.03320118 -0.22582264  0.02485396  0.25355435], action=0, reward=1.0, next_state=[-0.03771763 -0.42129049  0.02992505  0.55397189]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 7 ] state=[-0.03771763 -0.42129049  0.02992505  0.55397189], action=1, reward=1.0, next_state=[-0.04614344 -0.22660125  0.04100449  0.27086542]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 8 ] state=[-0.04614344 -0.22660125  0.04100449  0.27086542], action=0, reward=1.0, next_state=[-0.05067547 -0.42228361  0.0464218   0.57619418]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 9 ] state=[-0.05067547 -0.42228361  0.0464218   0.57619418], action=1, reward=1.0, next_state=[-0.05912114 -0.22784207  0.05794568  0.29848918]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 10 ] state=[-0.05912114 -0.22784207  0.05794568  0.29848918], action=1, reward=1.0, next_state=[-0.06367798 -0.03359192  0.06391546  0.0246295 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 11 ] state=[-0.06367798 -0.03359192  0.06391546  0.0246295 ], action=0, reward=1.0, next_state=[-0.06434982 -0.22956948  0.06440805  0.33677394]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 12 ] state=[-0.06434982 -0.22956948  0.06440805  0.33677394], action=1, reward=1.0, next_state=[-0.06894121 -0.03542041  0.07114353  0.06507706]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 13 ] state=[-0.06894121 -0.03542041  0.07114353  0.06507706], action=1, reward=1.0, next_state=[-0.06964962  0.15861323  0.07244507 -0.20433859]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 14 ] state=[-0.06964962  0.15861323  0.07244507 -0.20433859], action=1, reward=1.0, next_state=[-0.06647735  0.35262842  0.0683583  -0.4733178 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 15 ] state=[-0.06647735  0.35262842  0.0683583  -0.4733178 ], action=0, reward=1.0, next_state=[-0.05942478  0.15661101  0.05889195 -0.15989669]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 16 ] state=[-0.05942478  0.15661101  0.05889195 -0.15989669], action=0, reward=1.0, next_state=[-0.05629256 -0.03930246  0.05569401  0.15076827]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 17 ] state=[-0.05629256 -0.03930246  0.05569401  0.15076827], action=0, reward=1.0, next_state=[-0.05707861 -0.23517585  0.05870938  0.46048838]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 18 ] state=[-0.05707861 -0.23517585  0.05870938  0.46048838], action=0, reward=1.0, next_state=[-0.06178213 -0.43107636  0.06791914  0.77108351]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 19 ] state=[-0.06178213 -0.43107636  0.06791914  0.77108351], action=1, reward=1.0, next_state=[-0.07040366 -0.23695153  0.08334081  0.50052051]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 20 ] state=[-0.07040366 -0.23695153  0.08334081  0.50052051], action=1, reward=1.0, next_state=[-0.07514269 -0.04309723  0.09335122  0.23522216]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 21 ] state=[-0.07514269 -0.04309723  0.09335122  0.23522216], action=0, reward=1.0, next_state=[-0.07600463 -0.23942036  0.09805567  0.55583006]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 22 ] state=[-0.07600463 -0.23942036  0.09805567  0.55583006], action=1, reward=1.0, next_state=[-0.08079304 -0.04580194  0.10917227  0.29557973]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 23 ] state=[-0.08079304 -0.04580194  0.10917227  0.29557973], action=1, reward=1.0, next_state=[-0.08170908  0.14760801  0.11508386  0.03922489]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 24 ] state=[-0.08170908  0.14760801  0.11508386  0.03922489], action=1, reward=1.0, next_state=[-0.07875692  0.3409077   0.11586836 -0.21504665]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 25 ] state=[-0.07875692  0.3409077   0.11586836 -0.21504665], action=1, reward=1.0, next_state=[-0.07193876  0.53419895  0.11156743 -0.4690503 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 26 ] state=[-0.07193876  0.53419895  0.11156743 -0.4690503 ], action=1, reward=1.0, next_state=[-0.06125478  0.7275826   0.10218642 -0.72458951]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 27 ] state=[-0.06125478  0.7275826   0.10218642 -0.72458951], action=1, reward=1.0, next_state=[-0.04670313  0.9211541   0.08769463 -0.98344155]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 28 ] state=[-0.04670313  0.9211541   0.08769463 -0.98344155], action=1, reward=1.0, next_state=[-0.02828005  1.11499866  0.0680258  -1.24734187]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 29 ] state=[-0.02828005  1.11499866  0.0680258  -1.24734187], action=0, reward=1.0, next_state=[-0.00598008  0.91907354  0.04307896 -0.93414975]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 30 ] state=[-0.00598008  0.91907354  0.04307896 -0.93414975], action=1, reward=1.0, next_state=[ 0.01240139  1.11358871  0.02439597 -1.21299051]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 31 ] state=[ 0.01240139  1.11358871  0.02439597 -1.21299051], action=0, reward=1.0, next_state=[ 3.46731682e-02  9.18160544e-01  1.36158471e-04 -9.12763796e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 32 ] state=[ 3.46731682e-02  9.18160544e-01  1.36158471e-04 -9.12763796e-01], action=0, reward=1.0, next_state=[ 0.05303638  0.72303675 -0.01811912 -0.62003808]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 33 ] state=[ 0.05303638  0.72303675 -0.01811912 -0.62003808], action=0, reward=1.0, next_state=[ 0.06749711  0.52817248 -0.03051988 -0.33311638]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 34 ] state=[ 0.06749711  0.52817248 -0.03051988 -0.33311638], action=0, reward=1.0, next_state=[ 0.07806056  0.33349792 -0.03718221 -0.05021198]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 35 ] state=[ 0.07806056  0.33349792 -0.03718221 -0.05021198], action=0, reward=1.0, next_state=[ 0.08473052  0.13892831 -0.03818645  0.23051167]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 245 ][ timestamp 36 ] state=[ 0.08473052  0.13892831 -0.03818645  0.23051167], action=0, reward=1.0, next_state=[ 0.08750909 -0.05562776 -0.03357621  0.51090892]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 37 ] state=[ 0.08750909 -0.05562776 -0.03357621  0.51090892], action=1, reward=1.0, next_state=[ 0.08639653  0.13995069 -0.02335803  0.20783705]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 38 ] state=[ 0.08639653  0.13995069 -0.02335803  0.20783705], action=0, reward=1.0, next_state=[ 0.08919555 -0.05482961 -0.01920129  0.49306116]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 39 ] state=[ 0.08919555 -0.05482961 -0.01920129  0.49306116], action=1, reward=1.0, next_state=[ 0.08809895  0.14055783 -0.00934007  0.1943892 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 40 ] state=[ 0.08809895  0.14055783 -0.00934007  0.1943892 ], action=0, reward=1.0, next_state=[ 0.09091011 -0.05442928 -0.00545229  0.48411116]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 41 ] state=[ 0.09091011 -0.05442928 -0.00545229  0.48411116], action=1, reward=1.0, next_state=[0.08982153 0.14076919 0.00422994 0.18971484]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 42 ] state=[0.08982153 0.14076919 0.00422994 0.18971484], action=0, reward=1.0, next_state=[ 0.09263691 -0.05441302  0.00802423  0.48372914]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 43 ] state=[ 0.09263691 -0.05441302  0.00802423  0.48372914], action=1, reward=1.0, next_state=[0.09154865 0.14059477 0.01769882 0.19358597]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 44 ] state=[0.09154865 0.14059477 0.01769882 0.19358597], action=0, reward=1.0, next_state=[ 0.09436054 -0.05477583  0.02157054  0.49179915]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 45 ] state=[ 0.09436054 -0.05477583  0.02157054  0.49179915], action=1, reward=1.0, next_state=[0.09326503 0.14003532 0.03140652 0.20599165]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 46 ] state=[0.09326503 0.14003532 0.03140652 0.20599165], action=1, reward=1.0, next_state=[ 0.09606573  0.33469441  0.03552635 -0.07662099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 47 ] state=[ 0.09606573  0.33469441  0.03552635 -0.07662099], action=1, reward=1.0, next_state=[ 0.10275962  0.52928953  0.03399393 -0.35788694]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 48 ] state=[ 0.10275962  0.52928953  0.03399393 -0.35788694], action=0, reward=1.0, next_state=[ 0.11334541  0.33370121  0.02683619 -0.05468166]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 49 ] state=[ 0.11334541  0.33370121  0.02683619 -0.05468166], action=0, reward=1.0, next_state=[0.12001944 0.13820495 0.02574256 0.24634603]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 50 ] state=[0.12001944 0.13820495 0.02574256 0.24634603], action=1, reward=1.0, next_state=[ 0.12278354  0.33294996  0.03066948 -0.03810723]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 51 ] state=[ 0.12278354  0.33294996  0.03066948 -0.03810723], action=1, reward=1.0, next_state=[ 0.12944254  0.52761899  0.02990734 -0.32095804]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 52 ] state=[ 0.12944254  0.52761899  0.02990734 -0.32095804], action=1, reward=1.0, next_state=[ 0.13999492  0.72230255  0.02348818 -0.60406134]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 53 ] state=[ 0.13999492  0.72230255  0.02348818 -0.60406134], action=1, reward=1.0, next_state=[ 0.15444097  0.91708827  0.01140695 -0.88925444]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 54 ] state=[ 0.15444097  0.91708827  0.01140695 -0.88925444], action=1, reward=1.0, next_state=[ 0.17278273  1.11205359 -0.00637814 -1.17832982]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 55 ] state=[ 0.17278273  1.11205359 -0.00637814 -1.17832982], action=1, reward=1.0, next_state=[ 0.1950238   1.30725779 -0.02994474 -1.47300532]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 56 ] state=[ 0.1950238   1.30725779 -0.02994474 -1.47300532], action=0, reward=1.0, next_state=[ 0.22116896  1.11251443 -0.05940484 -1.18982367]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 57 ] state=[ 0.22116896  1.11251443 -0.05940484 -1.18982367], action=1, reward=1.0, next_state=[ 0.24341925  1.30835382 -0.08320132 -1.50051934]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 58 ] state=[ 0.24341925  1.30835382 -0.08320132 -1.50051934], action=1, reward=1.0, next_state=[ 0.26958632  1.50438189 -0.1132117  -1.81797725]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 59 ] state=[ 0.26958632  1.50438189 -0.1132117  -1.81797725], action=1, reward=1.0, next_state=[ 0.29967396  1.70056616 -0.14957125 -2.143583  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 60 ] state=[ 0.29967396  1.70056616 -0.14957125 -2.143583  ], action=0, reward=1.0, next_state=[ 0.33368529  1.50720358 -0.19244291 -1.90058764]\n",
      "[ Experience replay ] starts\n",
      "[ episode 245 ][ timestamp 61 ] state=[ 0.33368529  1.50720358 -0.19244291 -1.90058764], action=1, reward=-1.0, next_state=[ 0.36382936  1.70381755 -0.23045466 -2.24629397]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 245: Exploration_rate=0.01. Score=61.\n",
      "[ episode 246 ] state=[-0.04219169  0.04438108 -0.00594248 -0.03747589]\n",
      "[ episode 246 ][ timestamp 1 ] state=[-0.04219169  0.04438108 -0.00594248 -0.03747589], action=1, reward=1.0, next_state=[-0.04130407  0.23958774 -0.006692   -0.33202779]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 2 ] state=[-0.04130407  0.23958774 -0.006692   -0.33202779], action=1, reward=1.0, next_state=[-0.03651232  0.4348043  -0.01333255 -0.62681351]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 3 ] state=[-0.03651232  0.4348043  -0.01333255 -0.62681351], action=0, reward=1.0, next_state=[-0.02781623  0.23987096 -0.02586882 -0.33835914]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 4 ] state=[-0.02781623  0.23987096 -0.02586882 -0.33835914], action=0, reward=1.0, next_state=[-0.02301881  0.04512648 -0.03263601 -0.05394475]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 5 ] state=[-0.02301881  0.04512648 -0.03263601 -0.05394475], action=0, reward=1.0, next_state=[-0.02211628 -0.14951268 -0.0337149   0.22826525]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 6 ] state=[-0.02211628 -0.14951268 -0.0337149   0.22826525], action=0, reward=1.0, next_state=[-0.02510653 -0.34413701 -0.0291496   0.51012553]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 7 ] state=[-0.02510653 -0.34413701 -0.0291496   0.51012553], action=0, reward=1.0, next_state=[-0.03198927 -0.53883645 -0.01894709  0.79348185]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 8 ] state=[-0.03198927 -0.53883645 -0.01894709  0.79348185], action=0, reward=1.0, next_state=[-0.042766   -0.73369325 -0.00307745  1.08014449]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 9 ] state=[-0.042766   -0.73369325 -0.00307745  1.08014449], action=1, reward=1.0, next_state=[-0.05743987 -0.53853081  0.01852544  0.78649744]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 10 ] state=[-0.05743987 -0.53853081  0.01852544  0.78649744], action=1, reward=1.0, next_state=[-0.06821048 -0.34366819  0.03425539  0.49969983]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 11 ] state=[-0.06821048 -0.34366819  0.03425539  0.49969983], action=1, reward=1.0, next_state=[-0.07508385 -0.14904549  0.04424939  0.21800616]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 12 ] state=[-0.07508385 -0.14904549  0.04424939  0.21800616], action=1, reward=1.0, next_state=[-0.07806476  0.0454169   0.04860951 -0.06039682]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 13 ] state=[-0.07806476  0.0454169   0.04860951 -0.06039682], action=0, reward=1.0, next_state=[-0.07715642 -0.15036709  0.04740157  0.24721784]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 14 ] state=[-0.07715642 -0.15036709  0.04740157  0.24721784], action=1, reward=1.0, next_state=[-0.08016376  0.04404696  0.05234593 -0.03014483]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 15 ] state=[-0.08016376  0.04404696  0.05234593 -0.03014483], action=0, reward=1.0, next_state=[-0.07928282 -0.15178507  0.05174303  0.27858353]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 16 ] state=[-0.07928282 -0.15178507  0.05174303  0.27858353], action=0, reward=1.0, next_state=[-0.08231852 -0.34760552  0.0573147   0.58712675]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 17 ] state=[-0.08231852 -0.34760552  0.0573147   0.58712675], action=1, reward=1.0, next_state=[-0.08927063 -0.15333113  0.06905724  0.31303497]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 18 ] state=[-0.08927063 -0.15333113  0.06905724  0.31303497], action=1, reward=1.0, next_state=[-0.09233726  0.04074257  0.07531794  0.04290498]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 246 ][ timestamp 19 ] state=[-0.09233726  0.04074257  0.07531794  0.04290498], action=1, reward=1.0, next_state=[-0.09152241  0.23470821  0.07617604 -0.22509608]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 20 ] state=[-0.09152241  0.23470821  0.07617604 -0.22509608], action=0, reward=1.0, next_state=[-0.08682824  0.03858488  0.07167412  0.09060989]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 21 ] state=[-0.08682824  0.03858488  0.07167412  0.09060989], action=1, reward=1.0, next_state=[-0.08605654  0.2326102   0.07348631 -0.1786267 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 22 ] state=[-0.08605654  0.2326102   0.07348631 -0.1786267 ], action=1, reward=1.0, next_state=[-0.08140434  0.42660781  0.06991378 -0.44725221]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 23 ] state=[-0.08140434  0.42660781  0.06991378 -0.44725221], action=1, reward=1.0, next_state=[-0.07287218  0.62067469  0.06096874 -0.71710348]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 24 ] state=[-0.07287218  0.62067469  0.06096874 -0.71710348], action=0, reward=1.0, next_state=[-0.06045869  0.42476431  0.04662667 -0.40587021]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 25 ] state=[-0.06045869  0.42476431  0.04662667 -0.40587021], action=1, reward=1.0, next_state=[-0.0519634   0.61919512  0.03850926 -0.68349618]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 26 ] state=[-0.0519634   0.61919512  0.03850926 -0.68349618], action=0, reward=1.0, next_state=[-0.0395795   0.4235602   0.02483934 -0.37894244]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 27 ] state=[-0.0395795   0.4235602   0.02483934 -0.37894244], action=0, reward=1.0, next_state=[-0.0311083   0.22809447  0.01726049 -0.07853227]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 28 ] state=[-0.0311083   0.22809447  0.01726049 -0.07853227], action=0, reward=1.0, next_state=[-0.02654641  0.03272939  0.01568984  0.21954603]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 29 ] state=[-0.02654641  0.03272939  0.01568984  0.21954603], action=1, reward=1.0, next_state=[-0.02589182  0.22762359  0.02008077 -0.06814666]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 30 ] state=[-0.02589182  0.22762359  0.02008077 -0.06814666], action=1, reward=1.0, next_state=[-0.02133935  0.42245197  0.01871783 -0.35442697]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 31 ] state=[-0.02133935  0.42245197  0.01871783 -0.35442697], action=1, reward=1.0, next_state=[-0.01289031  0.61730285  0.01162929 -0.64114936]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 32 ] state=[-0.01289031  0.61730285  0.01162929 -0.64114936], action=0, reward=1.0, next_state=[-0.00054425  0.42202072 -0.00119369 -0.34482705]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 33 ] state=[-0.00054425  0.42202072 -0.00119369 -0.34482705], action=1, reward=1.0, next_state=[ 0.00789616  0.61715964 -0.00809024 -0.63788615]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 34 ] state=[ 0.00789616  0.61715964 -0.00809024 -0.63788615], action=1, reward=1.0, next_state=[ 0.02023936  0.81239346 -0.02084796 -0.93310581]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 35 ] state=[ 0.02023936  0.81239346 -0.02084796 -0.93310581], action=0, reward=1.0, next_state=[ 0.03648722  0.6175589  -0.03951007 -0.64704634]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 36 ] state=[ 0.03648722  0.6175589  -0.03951007 -0.64704634], action=1, reward=1.0, next_state=[ 0.0488384   0.81320843 -0.052451   -0.95190453]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 37 ] state=[ 0.0488384   0.81320843 -0.052451   -0.95190453], action=1, reward=1.0, next_state=[ 0.06510257  1.0089955  -0.07148909 -1.26059478]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 38 ] state=[ 0.06510257  1.0089955  -0.07148909 -1.26059478], action=0, reward=1.0, next_state=[ 0.08528248  0.81485705 -0.09670099 -0.99113083]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 39 ] state=[ 0.08528248  0.81485705 -0.09670099 -0.99113083], action=0, reward=1.0, next_state=[ 0.10157962  0.6211529  -0.1165236  -0.73031785]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 40 ] state=[ 0.10157962  0.6211529  -0.1165236  -0.73031785], action=0, reward=1.0, next_state=[ 0.11400268  0.42781769 -0.13112996 -0.47646208]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 41 ] state=[ 0.11400268  0.42781769 -0.13112996 -0.47646208], action=0, reward=1.0, next_state=[ 0.12255903  0.2347674  -0.1406592  -0.22781452]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 42 ] state=[ 0.12255903  0.2347674  -0.1406592  -0.22781452], action=0, reward=1.0, next_state=[ 0.12725438  0.04190647 -0.14521549  0.0174022 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 43 ] state=[ 0.12725438  0.04190647 -0.14521549  0.0174022 ], action=1, reward=1.0, next_state=[ 0.12809251  0.23878018 -0.14486745 -0.31734361]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 44 ] state=[ 0.12809251  0.23878018 -0.14486745 -0.31734361], action=0, reward=1.0, next_state=[ 0.13286812  0.0459867  -0.15121432 -0.07362486]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 45 ] state=[ 0.13286812  0.0459867  -0.15121432 -0.07362486], action=1, reward=1.0, next_state=[ 0.13378785  0.2429166  -0.15268682 -0.40993671]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 46 ] state=[ 0.13378785  0.2429166  -0.15268682 -0.40993671], action=0, reward=1.0, next_state=[ 0.13864618  0.0502517  -0.16088555 -0.16901726]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 47 ] state=[ 0.13864618  0.0502517  -0.16088555 -0.16901726], action=0, reward=1.0, next_state=[ 0.13965122 -0.14224554 -0.1642659   0.06890313]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 48 ] state=[ 0.13965122 -0.14224554 -0.1642659   0.06890313], action=0, reward=1.0, next_state=[ 0.1368063  -0.33467806 -0.16288784  0.30558902]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 49 ] state=[ 0.1368063  -0.33467806 -0.16288784  0.30558902], action=0, reward=1.0, next_state=[ 0.13011274 -0.52714946 -0.15677605  0.54279701]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 50 ] state=[ 0.13011274 -0.52714946 -0.15677605  0.54279701], action=0, reward=1.0, next_state=[ 0.11956975 -0.71976099 -0.14592011  0.78226737]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 51 ] state=[ 0.11956975 -0.71976099 -0.14592011  0.78226737], action=0, reward=1.0, next_state=[ 0.10517453 -0.91260829 -0.13027477  1.02571567]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 52 ] state=[ 0.10517453 -0.91260829 -0.13027477  1.02571567], action=1, reward=1.0, next_state=[ 0.08692237 -0.71601517 -0.10976045  0.69513228]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 53 ] state=[ 0.08692237 -0.71601517 -0.10976045  0.69513228], action=0, reward=1.0, next_state=[ 0.07260207 -0.90945741 -0.09585781  0.95134473]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 54 ] state=[ 0.07260207 -0.90945741 -0.09585781  0.95134473], action=0, reward=1.0, next_state=[ 0.05441292 -1.10316769 -0.07683091  1.21243716]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 55 ] state=[ 0.05441292 -1.10316769 -0.07683091  1.21243716], action=1, reward=1.0, next_state=[ 0.03234956 -0.90714278 -0.05258217  0.89670115]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 56 ] state=[ 0.03234956 -0.90714278 -0.05258217  0.89670115], action=1, reward=1.0, next_state=[ 0.01420671 -0.71134892 -0.03464815  0.58796423]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 57 ] state=[ 0.01420671 -0.71134892 -0.03464815  0.58796423], action=0, reward=1.0, next_state=[-2.02708269e-05 -9.05968959e-01 -2.28888630e-02  8.69534561e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 58 ] state=[-2.02708269e-05 -9.05968959e-01 -2.28888630e-02  8.69534561e-01], action=1, reward=1.0, next_state=[-0.01813965 -0.71054324 -0.00549817  0.56974402]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 59 ] state=[-0.01813965 -0.71054324 -0.00549817  0.56974402], action=1, reward=1.0, next_state=[-0.03235051 -0.51534461  0.00589671  0.27533405]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 60 ] state=[-0.03235051 -0.51534461  0.00589671  0.27533405], action=0, reward=1.0, next_state=[-0.04265741 -0.71055019  0.01140339  0.56987096]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 246 ][ timestamp 61 ] state=[-0.04265741 -0.71055019  0.01140339  0.56987096], action=1, reward=1.0, next_state=[-0.05686841 -0.51559001  0.02280081  0.28080222]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 62 ] state=[-0.05686841 -0.51559001  0.02280081  0.28080222], action=0, reward=1.0, next_state=[-0.06718021 -0.71102966  0.02841685  0.58058835]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 63 ] state=[-0.06718021 -0.71102966  0.02841685  0.58058835], action=0, reward=1.0, next_state=[-0.0814008  -0.90653804  0.04002862  0.88208595]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 64 ] state=[-0.0814008  -0.90653804  0.04002862  0.88208595], action=0, reward=1.0, next_state=[-0.09953156 -1.10218014  0.05767034  1.1870793 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 65 ] state=[-0.09953156 -1.10218014  0.05767034  1.1870793 ], action=0, reward=1.0, next_state=[-0.12157517 -1.29800049  0.08141193  1.4972672 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 66 ] state=[-0.12157517 -1.29800049  0.08141193  1.4972672 ], action=0, reward=1.0, next_state=[-0.14753518 -1.49401215  0.11135727  1.81421953]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 67 ] state=[-0.14753518 -1.49401215  0.11135727  1.81421953], action=1, reward=1.0, next_state=[-0.17741542 -1.30029226  0.14764166  1.55811091]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 68 ] state=[-0.17741542 -1.30029226  0.14764166  1.55811091], action=1, reward=1.0, next_state=[-0.20342127 -1.10721437  0.17880388  1.31489402]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 69 ] state=[-0.20342127 -1.10721437  0.17880388  1.31489402], action=1, reward=1.0, next_state=[-0.22556555 -0.91474789  0.20510176  1.08308568]\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ][ timestamp 70 ] state=[-0.22556555 -0.91474789  0.20510176  1.08308568], action=1, reward=-1.0, next_state=[-0.24386051 -0.72283499  0.22676347  0.86112803]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 246: Exploration_rate=0.01. Score=70.\n",
      "[ episode 247 ] state=[ 0.02595947 -0.04071938  0.02071329 -0.01532814]\n",
      "[ episode 247 ][ timestamp 1 ] state=[ 0.02595947 -0.04071938  0.02071329 -0.01532814], action=1, reward=1.0, next_state=[ 0.02514508  0.15409949  0.02040672 -0.30140449]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 2 ] state=[ 0.02514508  0.15409949  0.02040672 -0.30140449], action=1, reward=1.0, next_state=[ 0.02822707  0.34892473  0.01437863 -0.58758234]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 3 ] state=[ 0.02822707  0.34892473  0.01437863 -0.58758234], action=0, reward=1.0, next_state=[ 0.03520557  0.15360439  0.00262699 -0.29040495]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 4 ] state=[ 0.03520557  0.15360439  0.00262699 -0.29040495], action=0, reward=1.0, next_state=[ 0.03827766 -0.04155492 -0.00318111  0.00310534]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 5 ] state=[ 0.03827766 -0.04155492 -0.00318111  0.00310534], action=0, reward=1.0, next_state=[ 0.03744656 -0.2366311  -0.003119    0.29478289]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 6 ] state=[ 0.03744656 -0.2366311  -0.003119    0.29478289], action=1, reward=1.0, next_state=[ 0.03271394 -0.04146482  0.00277665  0.00111791]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 7 ] state=[ 0.03271394 -0.04146482  0.00277665  0.00111791], action=0, reward=1.0, next_state=[ 0.03188464 -0.23662649  0.00279901  0.29467561]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 8 ] state=[ 0.03188464 -0.23662649  0.00279901  0.29467561], action=0, reward=1.0, next_state=[ 0.02715211 -0.43178823  0.00869252  0.58823999]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 9 ] state=[ 0.02715211 -0.43178823  0.00869252  0.58823999], action=1, reward=1.0, next_state=[ 0.01851634 -0.23678908  0.02045732  0.29830788]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 10 ] state=[ 0.01851634 -0.23678908  0.02045732  0.29830788], action=1, reward=1.0, next_state=[ 0.01378056 -0.04196462  0.02642348  0.01214638]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 11 ] state=[ 0.01378056 -0.04196462  0.02642348  0.01214638], action=0, reward=1.0, next_state=[ 0.01294127 -0.23745536  0.02666641  0.31304772]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 12 ] state=[ 0.01294127 -0.23745536  0.02666641  0.31304772], action=0, reward=1.0, next_state=[ 0.00819216 -0.43294685  0.03292736  0.61401969]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 13 ] state=[ 0.00819216 -0.43294685  0.03292736  0.61401969], action=0, reward=1.0, next_state=[-4.66774015e-04 -6.28513067e-01  4.52077571e-02  9.16888906e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 14 ] state=[-4.66774015e-04 -6.28513067e-01  4.52077571e-02  9.16888906e-01], action=1, reward=1.0, next_state=[-0.01303704 -0.43403058  0.06354554  0.63874978]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 15 ] state=[-0.01303704 -0.43403058  0.06354554  0.63874978], action=1, reward=1.0, next_state=[-0.02171765 -0.23984953  0.07632053  0.36673591]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 16 ] state=[-0.02171765 -0.23984953  0.07632053  0.36673591], action=1, reward=1.0, next_state=[-0.02651464 -0.04589038  0.08365525  0.09906057]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 17 ] state=[-0.02651464 -0.04589038  0.08365525  0.09906057], action=1, reward=1.0, next_state=[-0.02743245  0.1479392   0.08563646 -0.16610109]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 18 ] state=[-0.02743245  0.1479392   0.08563646 -0.16610109], action=1, reward=1.0, next_state=[-0.02447366  0.34173756  0.08231444 -0.43058699]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 19 ] state=[-0.02447366  0.34173756  0.08231444 -0.43058699], action=1, reward=1.0, next_state=[-0.01763891  0.5356033   0.0737027  -0.69622785]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 20 ] state=[-0.01763891  0.5356033   0.0737027  -0.69622785], action=1, reward=1.0, next_state=[-0.00692684  0.72962991  0.05977814 -0.96482867]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 21 ] state=[-0.00692684  0.72962991  0.05977814 -0.96482867], action=1, reward=1.0, next_state=[ 0.00766575  0.92390012  0.04048157 -1.23814917]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 22 ] state=[ 0.00766575  0.92390012  0.04048157 -1.23814917], action=0, reward=1.0, next_state=[ 0.02614376  0.72828221  0.01571858 -0.93306437]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 23 ] state=[ 0.02614376  0.72828221  0.01571858 -0.93306437], action=1, reward=1.0, next_state=[ 0.0407094   0.9231886  -0.0029427  -1.22076676]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 24 ] state=[ 0.0407094   0.9231886  -0.0029427  -1.22076676], action=0, reward=1.0, next_state=[ 0.05917317  0.72810469 -0.02735804 -0.92900733]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 25 ] state=[ 0.05917317  0.72810469 -0.02735804 -0.92900733], action=0, reward=1.0, next_state=[ 0.07373527  0.53336253 -0.04593818 -0.64504566]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 26 ] state=[ 0.07373527  0.53336253 -0.04593818 -0.64504566], action=0, reward=1.0, next_state=[ 0.08440252  0.33890984 -0.0588391  -0.36717541]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 27 ] state=[ 0.08440252  0.33890984 -0.0588391  -0.36717541], action=0, reward=1.0, next_state=[ 0.09118071  0.14467119 -0.06618261 -0.09361035]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 28 ] state=[ 0.09118071  0.14467119 -0.06618261 -0.09361035], action=0, reward=1.0, next_state=[ 0.09407414 -0.04944282 -0.06805481  0.17747972]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 29 ] state=[ 0.09407414 -0.04944282 -0.06805481  0.17747972], action=1, reward=1.0, next_state=[ 0.09308528  0.14658371 -0.06450522 -0.13587209]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 30 ] state=[ 0.09308528  0.14658371 -0.06450522 -0.13587209], action=0, reward=1.0, next_state=[ 0.09601696 -0.04755784 -0.06722266  0.13578321]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 31 ] state=[ 0.09601696 -0.04755784 -0.06722266  0.13578321], action=1, reward=1.0, next_state=[ 0.0950658   0.14845936 -0.064507   -0.17732709]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 247 ][ timestamp 32 ] state=[ 0.0950658   0.14845936 -0.064507   -0.17732709], action=0, reward=1.0, next_state=[ 0.09803499 -0.04568298 -0.06805354  0.09432883]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 33 ] state=[ 0.09803499 -0.04568298 -0.06805354  0.09432883], action=0, reward=1.0, next_state=[ 0.09712133 -0.23976688 -0.06616696  0.36478849]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 34 ] state=[ 0.09712133 -0.23976688 -0.06616696  0.36478849], action=0, reward=1.0, next_state=[ 0.09232599 -0.43388915 -0.05887119  0.63589583]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 35 ] state=[ 0.09232599 -0.43388915 -0.05887119  0.63589583], action=1, reward=1.0, next_state=[ 0.08364821 -0.23799768 -0.04615327  0.32526954]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 36 ] state=[ 0.08364821 -0.23799768 -0.04615327  0.32526954], action=0, reward=1.0, next_state=[ 0.07888825 -0.43243315 -0.03964788  0.60304792]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 37 ] state=[ 0.07888825 -0.43243315 -0.03964788  0.60304792], action=1, reward=1.0, next_state=[ 0.07023959 -0.23677974 -0.02758693  0.29814502]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 38 ] state=[ 0.07023959 -0.23677974 -0.02758693  0.29814502], action=0, reward=1.0, next_state=[ 0.06550399 -0.43149781 -0.02162402  0.58200146]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 39 ] state=[ 0.06550399 -0.43149781 -0.02162402  0.58200146], action=0, reward=1.0, next_state=[ 0.05687404 -0.62631021 -0.009984    0.86779479]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 40 ] state=[ 0.05687404 -0.62631021 -0.009984    0.86779479], action=1, reward=1.0, next_state=[ 0.04434783 -0.43105385  0.0073719   0.57198959]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 41 ] state=[ 0.04434783 -0.43105385  0.0073719   0.57198959], action=1, reward=1.0, next_state=[ 0.03572676 -0.23603604  0.01881169  0.28163814]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 42 ] state=[ 0.03572676 -0.23603604  0.01881169  0.28163814], action=1, reward=1.0, next_state=[ 0.03100604 -0.0411874   0.02444445 -0.00505279]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 43 ] state=[ 0.03100604 -0.0411874   0.02444445 -0.00505279], action=0, reward=1.0, next_state=[ 0.03018229 -0.23665124  0.0243434   0.29524132]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 44 ] state=[ 0.03018229 -0.23665124  0.0243434   0.29524132], action=1, reward=1.0, next_state=[ 0.02544926 -0.04188464  0.03024823  0.01033424]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 45 ] state=[ 0.02544926 -0.04188464  0.03024823  0.01033424], action=0, reward=1.0, next_state=[ 0.02461157 -0.23742704  0.03045491  0.31240529]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 46 ] state=[ 0.02461157 -0.23742704  0.03045491  0.31240529], action=0, reward=1.0, next_state=[ 0.01986303 -0.43296932  0.03670302  0.61453506]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 47 ] state=[ 0.01986303 -0.43296932  0.03670302  0.61453506], action=1, reward=1.0, next_state=[ 0.01120364 -0.23837893  0.04899372  0.33363431]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 48 ] state=[ 0.01120364 -0.23837893  0.04899372  0.33363431], action=1, reward=1.0, next_state=[ 0.00643607 -0.04398729  0.0556664   0.05679514]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 49 ] state=[ 0.00643607 -0.04398729  0.0556664   0.05679514], action=1, reward=1.0, next_state=[ 0.00555632  0.15029414  0.05680231 -0.21781813]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 50 ] state=[ 0.00555632  0.15029414  0.05680231 -0.21781813], action=0, reward=1.0, next_state=[ 0.0085622  -0.04559186  0.05244594  0.09222788]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 51 ] state=[ 0.0085622  -0.04559186  0.05244594  0.09222788], action=1, reward=1.0, next_state=[ 0.00765036  0.14874067  0.0542905  -0.18345808]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 52 ] state=[ 0.00765036  0.14874067  0.0542905  -0.18345808], action=1, reward=1.0, next_state=[ 0.01062518  0.34304547  0.05062134 -0.45853228]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 53 ] state=[ 0.01062518  0.34304547  0.05062134 -0.45853228], action=0, reward=1.0, next_state=[ 0.01748609  0.14724583  0.04145069 -0.15033274]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 54 ] state=[ 0.01748609  0.14724583  0.04145069 -0.15033274], action=1, reward=1.0, next_state=[ 0.020431    0.34175048  0.03844404 -0.42965608]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 55 ] state=[ 0.020431    0.34175048  0.03844404 -0.42965608], action=0, reward=1.0, next_state=[ 0.02726601  0.14610579  0.02985092 -0.12510613]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 56 ] state=[ 0.02726601  0.14610579  0.02985092 -0.12510613], action=1, reward=1.0, next_state=[ 0.03018813  0.34078766  0.02734879 -0.40822396]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 57 ] state=[ 0.03018813  0.34078766  0.02734879 -0.40822396], action=0, reward=1.0, next_state=[ 0.03700388  0.14528882  0.01918432 -0.10704583]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 58 ] state=[ 0.03700388  0.14528882  0.01918432 -0.10704583], action=0, reward=1.0, next_state=[ 0.03990966 -0.05010272  0.0170434   0.19162739]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 59 ] state=[ 0.03990966 -0.05010272  0.0170434   0.19162739], action=0, reward=1.0, next_state=[ 0.03890761 -0.24546428  0.02087595  0.4896377 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 60 ] state=[ 0.03890761 -0.24546428  0.02087595  0.4896377 ], action=1, reward=1.0, next_state=[ 0.03399832 -0.05064297  0.0306687   0.20360648]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 61 ] state=[ 0.03399832 -0.05064297  0.0306687   0.20360648], action=1, reward=1.0, next_state=[ 0.03298546  0.14402727  0.03474083 -0.07924638]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 62 ] state=[ 0.03298546  0.14402727  0.03474083 -0.07924638], action=1, reward=1.0, next_state=[ 0.03586601  0.33863442  0.0331559  -0.36076922]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 63 ] state=[ 0.03586601  0.33863442  0.0331559  -0.36076922], action=1, reward=1.0, next_state=[ 0.04263869  0.53326978  0.02594052 -0.64281574]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 64 ] state=[ 0.04263869  0.53326978  0.02594052 -0.64281574], action=0, reward=1.0, next_state=[ 0.05330409  0.33779605  0.0130842  -0.34207813]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 65 ] state=[ 0.05330409  0.33779605  0.0130842  -0.34207813], action=0, reward=1.0, next_state=[ 0.06006001  0.14249041  0.00624264 -0.04529811]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 66 ] state=[ 0.06006001  0.14249041  0.00624264 -0.04529811], action=0, reward=1.0, next_state=[ 0.06290982 -0.0527205   0.00533668  0.24934787]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 67 ] state=[ 0.06290982 -0.0527205   0.00533668  0.24934787], action=0, reward=1.0, next_state=[ 0.06185541 -0.24791825  0.01032364  0.54370931]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 68 ] state=[ 0.06185541 -0.24791825  0.01032364  0.54370931], action=1, reward=1.0, next_state=[ 0.05689704 -0.05294289  0.02119782  0.25429695]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 69 ] state=[ 0.05689704 -0.05294289  0.02119782  0.25429695], action=1, reward=1.0, next_state=[ 0.05583819  0.14187008  0.02628376 -0.03162516]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 70 ] state=[ 0.05583819  0.14187008  0.02628376 -0.03162516], action=0, reward=1.0, next_state=[ 0.05867559 -0.05361874  0.02565126  0.26923332]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 71 ] state=[ 0.05867559 -0.05361874  0.02565126  0.26923332], action=1, reward=1.0, next_state=[ 0.05760321  0.14112793  0.03103592 -0.01524995]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 72 ] state=[ 0.05760321  0.14112793  0.03103592 -0.01524995], action=0, reward=1.0, next_state=[ 0.06042577 -0.05442505  0.03073092  0.28706136]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 247 ][ timestamp 73 ] state=[ 0.06042577 -0.05442505  0.03073092  0.28706136], action=1, reward=1.0, next_state=[0.05933727 0.14024548 0.03647215 0.00422692]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 74 ] state=[0.05933727 0.14024548 0.03647215 0.00422692], action=0, reward=1.0, next_state=[ 0.06214218 -0.05538004  0.03655669  0.30819048]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 75 ] state=[ 0.06214218 -0.05538004  0.03655669  0.30819048], action=0, reward=1.0, next_state=[ 0.06103458 -0.25100329  0.0427205   0.61217459]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 76 ] state=[ 0.06103458 -0.25100329  0.0427205   0.61217459], action=1, reward=1.0, next_state=[ 0.05601451 -0.05650362  0.05496399  0.33324726]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 77 ] state=[ 0.05601451 -0.05650362  0.05496399  0.33324726], action=1, reward=1.0, next_state=[0.05488444 0.1377947  0.06162894 0.05839119]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 78 ] state=[0.05488444 0.1377947  0.06162894 0.05839119], action=1, reward=1.0, next_state=[ 0.05764034  0.33198135  0.06279676 -0.21422836]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 79 ] state=[ 0.05764034  0.33198135  0.06279676 -0.21422836], action=1, reward=1.0, next_state=[ 0.06427996  0.52615192  0.05851219 -0.48646002]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 80 ] state=[ 0.06427996  0.52615192  0.05851219 -0.48646002], action=0, reward=1.0, next_state=[ 0.074803    0.33025526  0.04878299 -0.17592512]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 81 ] state=[ 0.074803    0.33025526  0.04878299 -0.17592512], action=1, reward=1.0, next_state=[ 0.08140811  0.52464635  0.04526449 -0.45282836]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 82 ] state=[ 0.08140811  0.52464635  0.04526449 -0.45282836], action=0, reward=1.0, next_state=[ 0.09190103  0.3289145   0.03620792 -0.14622809]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 83 ] state=[ 0.09190103  0.3289145   0.03620792 -0.14622809], action=1, reward=1.0, next_state=[ 0.09847932  0.52349973  0.03328336 -0.42727182]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 84 ] state=[ 0.09847932  0.52349973  0.03328336 -0.42727182], action=0, reward=1.0, next_state=[ 0.10894932  0.32792257  0.02473793 -0.12428506]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 85 ] state=[ 0.10894932  0.32792257  0.02473793 -0.12428506], action=0, reward=1.0, next_state=[0.11550777 0.13245511 0.02225222 0.17609863]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 86 ] state=[0.11550777 0.13245511 0.02225222 0.17609863], action=1, reward=1.0, next_state=[ 0.11815687  0.32725165  0.0257742  -0.10948223]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 87 ] state=[ 0.11815687  0.32725165  0.0257742  -0.10948223], action=1, reward=1.0, next_state=[ 0.1247019   0.52199497  0.02358455 -0.3939234 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 88 ] state=[ 0.1247019   0.52199497  0.02358455 -0.3939234 ], action=1, reward=1.0, next_state=[ 0.1351418   0.71677445  0.01570608 -0.67907816]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 89 ] state=[ 0.1351418   0.71677445  0.01570608 -0.67907816], action=1, reward=1.0, next_state=[ 0.14947729  0.91167474  0.00212452 -0.96677514]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 90 ] state=[ 0.14947729  0.91167474  0.00212452 -0.96677514], action=1, reward=1.0, next_state=[ 0.16771079  1.10676809 -0.01721098 -1.2587899 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 91 ] state=[ 0.16771079  1.10676809 -0.01721098 -1.2587899 ], action=0, reward=1.0, next_state=[ 0.18984615  0.91187054 -0.04238678 -0.97154665]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 92 ] state=[ 0.18984615  0.91187054 -0.04238678 -0.97154665], action=1, reward=1.0, next_state=[ 0.20808356  1.10753493 -0.06181771 -1.27723759]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 93 ] state=[ 0.20808356  1.10753493 -0.06181771 -1.27723759], action=0, reward=1.0, next_state=[ 0.23023426  0.91325321 -0.08736246 -1.0045345 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 94 ] state=[ 0.23023426  0.91325321 -0.08736246 -1.0045345 ], action=0, reward=1.0, next_state=[ 0.24849932  0.71939988 -0.10745315 -0.74051535]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 95 ] state=[ 0.24849932  0.71939988 -0.10745315 -0.74051535], action=0, reward=1.0, next_state=[ 0.26288732  0.52591259 -0.12226346 -0.48348881]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 96 ] state=[ 0.26288732  0.52591259 -0.12226346 -0.48348881], action=1, reward=1.0, next_state=[ 0.27340557  0.72252876 -0.13193324 -0.81206746]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 97 ] state=[ 0.27340557  0.72252876 -0.13193324 -0.81206746], action=0, reward=1.0, next_state=[ 0.28785615  0.52943691 -0.14817459 -0.56362275]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 98 ] state=[ 0.28785615  0.52943691 -0.14817459 -0.56362275], action=0, reward=1.0, next_state=[ 0.29844489  0.3366707  -0.15944704 -0.32104596]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 99 ] state=[ 0.29844489  0.3366707  -0.15944704 -0.32104596], action=0, reward=1.0, next_state=[ 0.3051783   0.14413607 -0.16586796 -0.08258644]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 100 ] state=[ 0.3051783   0.14413607 -0.16586796 -0.08258644], action=0, reward=1.0, next_state=[ 0.30806102 -0.0482678  -0.16751969  0.15351648]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 101 ] state=[ 0.30806102 -0.0482678  -0.16751969  0.15351648], action=0, reward=1.0, next_state=[ 0.30709566 -0.24064446 -0.16444936  0.38902119]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 102 ] state=[ 0.30709566 -0.24064446 -0.16444936  0.38902119], action=0, reward=1.0, next_state=[ 0.30228278 -0.43309703 -0.15666894  0.62567489]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 103 ] state=[ 0.30228278 -0.43309703 -0.15666894  0.62567489], action=1, reward=1.0, next_state=[ 0.29362083 -0.23617517 -0.14415544  0.28803734]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 104 ] state=[ 0.29362083 -0.23617517 -0.14415544  0.28803734], action=0, reward=1.0, next_state=[ 0.28889733 -0.42897874 -0.13839469  0.53200786]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 105 ] state=[ 0.28889733 -0.42897874 -0.13839469  0.53200786], action=0, reward=1.0, next_state=[ 0.28031776 -0.62191058 -0.12775453  0.77808034]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 106 ] state=[ 0.28031776 -0.62191058 -0.12775453  0.77808034], action=1, reward=1.0, next_state=[ 0.26787955 -0.42528511 -0.11219293  0.44808798]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 107 ] state=[ 0.26787955 -0.42528511 -0.11219293  0.44808798], action=0, reward=1.0, next_state=[ 0.25937384 -0.6186561  -0.10323117  0.70340531]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 108 ] state=[ 0.25937384 -0.6186561  -0.10323117  0.70340531], action=1, reward=1.0, next_state=[ 0.24700072 -0.42226649 -0.08916306  0.38009306]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 109 ] state=[ 0.24700072 -0.42226649 -0.08916306  0.38009306], action=0, reward=1.0, next_state=[ 0.23855539 -0.61601662 -0.0815612   0.64338456]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 110 ] state=[ 0.23855539 -0.61601662 -0.0815612   0.64338456], action=0, reward=1.0, next_state=[ 0.22623506 -0.80991278 -0.06869351  0.90930953]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 111 ] state=[ 0.22623506 -0.80991278 -0.06869351  0.90930953], action=0, reward=1.0, next_state=[ 0.2100368  -1.00404111 -0.05050732  1.17963525]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 112 ] state=[ 0.2100368  -1.00404111 -0.05050732  1.17963525], action=0, reward=1.0, next_state=[ 0.18995598 -1.19847222 -0.02691461  1.45606716]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 113 ] state=[ 0.18995598 -1.19847222 -0.02691461  1.45606716], action=1, reward=1.0, next_state=[ 0.16598654 -1.00303048  0.00220673  1.15509879]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 247 ][ timestamp 114 ] state=[ 0.16598654 -1.00303048  0.00220673  1.15509879], action=0, reward=1.0, next_state=[ 0.14592593 -1.19818114  0.0253087   1.44847284]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 115 ] state=[ 0.14592593 -1.19818114  0.0253087   1.44847284], action=1, reward=1.0, next_state=[ 0.1219623  -1.00337932  0.05427816  1.16380366]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 116 ] state=[ 0.1219623  -1.00337932  0.05427816  1.16380366], action=1, reward=1.0, next_state=[ 0.10189472 -0.80900444  0.07755423  0.88862067]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 117 ] state=[ 0.10189472 -0.80900444  0.07755423  0.88862067], action=1, reward=1.0, next_state=[ 0.08571463 -0.6150158   0.09532665  0.62129043]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 118 ] state=[ 0.08571463 -0.6150158   0.09532665  0.62129043], action=1, reward=1.0, next_state=[ 0.07341431 -0.42134524  0.10775246  0.36008715]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 119 ] state=[ 0.07341431 -0.42134524  0.10775246  0.36008715], action=0, reward=1.0, next_state=[ 0.06498741 -0.61782071  0.1149542   0.68470907]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 120 ] state=[ 0.06498741 -0.61782071  0.1149542   0.68470907], action=1, reward=1.0, next_state=[ 0.05263099 -0.42446656  0.12864838  0.43031418]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 121 ] state=[ 0.05263099 -0.42446656  0.12864838  0.43031418], action=0, reward=1.0, next_state=[ 0.04414166 -0.62115317  0.13725466  0.76062441]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 122 ] state=[ 0.04414166 -0.62115317  0.13725466  0.76062441], action=1, reward=1.0, next_state=[ 0.0317186  -0.42816216  0.15246715  0.5140867 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 123 ] state=[ 0.0317186  -0.42816216  0.15246715  0.5140867 ], action=0, reward=1.0, next_state=[ 0.02315536 -0.62506549  0.16274889  0.85066727]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 124 ] state=[ 0.02315536 -0.62506549  0.16274889  0.85066727], action=1, reward=1.0, next_state=[ 0.01065405 -0.43249214  0.17976223  0.61326159]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 125 ] state=[ 0.01065405 -0.43249214  0.17976223  0.61326159], action=1, reward=1.0, next_state=[ 0.0020042  -0.24027707  0.19202746  0.38215087]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 126 ] state=[ 0.0020042  -0.24027707  0.19202746  0.38215087], action=1, reward=1.0, next_state=[-0.00280134 -0.04832626  0.19967048  0.15562667]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 127 ] state=[-0.00280134 -0.04832626  0.19967048  0.15562667], action=0, reward=1.0, next_state=[-0.00376786 -0.24566352  0.20278301  0.50406536]\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ][ timestamp 128 ] state=[-0.00376786 -0.24566352  0.20278301  0.50406536], action=1, reward=-1.0, next_state=[-0.00868113 -0.05388993  0.21286432  0.28150962]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 247: Exploration_rate=0.01. Score=128.\n",
      "[ episode 248 ] state=[-0.00591278  0.04275419  0.01792983  0.01180421]\n",
      "[ episode 248 ][ timestamp 1 ] state=[-0.00591278  0.04275419  0.01792983  0.01180421], action=1, reward=1.0, next_state=[-0.00505769  0.23761448  0.01816592 -0.27516815]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 2 ] state=[-0.00505769  0.23761448  0.01816592 -0.27516815], action=0, reward=1.0, next_state=[-0.0003054   0.04223812  0.01266255  0.02318852]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 3 ] state=[-0.0003054   0.04223812  0.01266255  0.02318852], action=0, reward=1.0, next_state=[ 0.00053936 -0.15306312  0.01312632  0.31983957]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 4 ] state=[ 0.00053936 -0.15306312  0.01312632  0.31983957], action=1, reward=1.0, next_state=[-0.0025219   0.04186946  0.01952311  0.03132493]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 5 ] state=[-0.0025219   0.04186946  0.01952311  0.03132493], action=1, reward=1.0, next_state=[-0.00168452  0.23670607  0.02014961 -0.25513487]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 6 ] state=[-0.00168452  0.23670607  0.02014961 -0.25513487], action=1, reward=1.0, next_state=[ 0.00304961  0.43153462  0.01504692 -0.54139478]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 7 ] state=[ 0.00304961  0.43153462  0.01504692 -0.54139478], action=1, reward=1.0, next_state=[ 0.0116803   0.62644188  0.00421902 -0.82929895]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 8 ] state=[ 0.0116803   0.62644188  0.00421902 -0.82929895], action=0, reward=1.0, next_state=[ 0.02420914  0.43126251 -0.01236696 -0.53529211]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 9 ] state=[ 0.02420914  0.43126251 -0.01236696 -0.53529211], action=0, reward=1.0, next_state=[ 0.03283439  0.23631663 -0.0230728  -0.24653144]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 10 ] state=[ 0.03283439  0.23631663 -0.0230728  -0.24653144], action=0, reward=1.0, next_state=[ 0.03756072  0.04153167 -0.02800343  0.03878542]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 11 ] state=[ 0.03756072  0.04153167 -0.02800343  0.03878542], action=0, reward=1.0, next_state=[ 0.03839135 -0.15317775 -0.02722772  0.32250311]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 12 ] state=[ 0.03839135 -0.15317775 -0.02722772  0.32250311], action=1, reward=1.0, next_state=[ 0.0353278   0.04232113 -0.02077766  0.02135953]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 13 ] state=[ 0.0353278   0.04232113 -0.02077766  0.02135953], action=0, reward=1.0, next_state=[ 0.03617422 -0.15249678 -0.02035047  0.30741512]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 14 ] state=[ 0.03617422 -0.15249678 -0.02035047  0.30741512], action=0, reward=1.0, next_state=[ 0.03312428 -0.34732293 -0.01420217  0.59361121]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 15 ] state=[ 0.03312428 -0.34732293 -0.01420217  0.59361121], action=1, reward=1.0, next_state=[ 0.02617783 -0.15200509 -0.00232994  0.2964887 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 16 ] state=[ 0.02617783 -0.15200509 -0.00232994  0.2964887 ], action=0, reward=1.0, next_state=[ 0.02313772 -0.34709375  0.00359983  0.5884359 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 17 ] state=[ 0.02313772 -0.34709375  0.00359983  0.5884359 ], action=1, reward=1.0, next_state=[ 0.01619585 -0.15202239  0.01536855  0.29688911]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 18 ] state=[ 0.01619585 -0.15202239  0.01536855  0.29688911], action=1, reward=1.0, next_state=[0.0131554  0.04287714 0.02130633 0.00909251]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 19 ] state=[0.0131554  0.04287714 0.02130633 0.00909251], action=0, reward=1.0, next_state=[ 0.01401294 -0.15254379  0.02148818  0.30842096]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 20 ] state=[ 0.01401294 -0.15254379  0.02148818  0.30842096], action=1, reward=1.0, next_state=[0.01096207 0.04226549 0.0276566  0.02259153]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 21 ] state=[0.01096207 0.04226549 0.0276566  0.02259153], action=0, reward=1.0, next_state=[ 0.01180738 -0.15324194  0.02810843  0.32387053]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 22 ] state=[ 0.01180738 -0.15324194  0.02810843  0.32387053], action=0, reward=1.0, next_state=[ 0.00874254 -0.34875262  0.03458584  0.6252835 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 23 ] state=[ 0.00874254 -0.34875262  0.03458584  0.6252835 ], action=0, reward=1.0, next_state=[ 0.00176749 -0.54433988  0.04709151  0.92865516]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 24 ] state=[ 0.00176749 -0.54433988  0.04709151  0.92865516], action=1, reward=1.0, next_state=[-0.00911931 -0.34988422  0.06566462  0.65113481]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 25 ] state=[-0.00911931 -0.34988422  0.06566462  0.65113481], action=0, reward=1.0, next_state=[-0.016117   -0.54585631  0.07868731  0.96375095]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 26 ] state=[-0.016117   -0.54585631  0.07868731  0.96375095], action=1, reward=1.0, next_state=[-0.02703412 -0.35187467  0.09796233  0.69678903]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 27 ] state=[-0.02703412 -0.35187467  0.09796233  0.69678903], action=1, reward=1.0, next_state=[-0.03407162 -0.15823788  0.11189811  0.43648131]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 248 ][ timestamp 28 ] state=[-0.03407162 -0.15823788  0.11189811  0.43648131], action=1, reward=1.0, next_state=[-0.03723637  0.03513698  0.12062774  0.18106253]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 29 ] state=[-0.03723637  0.03513698  0.12062774  0.18106253], action=1, reward=1.0, next_state=[-0.03653363  0.22834482  0.12424899 -0.07126465]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 30 ] state=[-0.03653363  0.22834482  0.12424899 -0.07126465], action=1, reward=1.0, next_state=[-0.03196674  0.4214867   0.1228237  -0.3223088 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 31 ] state=[-0.03196674  0.4214867   0.1228237  -0.3223088 ], action=1, reward=1.0, next_state=[-0.023537    0.6146651   0.11637752 -0.57387402]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 32 ] state=[-0.023537    0.6146651   0.11637752 -0.57387402], action=1, reward=1.0, next_state=[-0.0112437   0.80797965  0.10490004 -0.8277466 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 33 ] state=[-0.0112437   0.80797965  0.10490004 -0.8277466 ], action=1, reward=1.0, next_state=[ 0.00491589  1.00152292  0.08834511 -1.08568158]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 34 ] state=[ 0.00491589  1.00152292  0.08834511 -1.08568158], action=1, reward=1.0, next_state=[ 0.02494635  1.1953755   0.06663148 -1.34938675]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 35 ] state=[ 0.02494635  1.1953755   0.06663148 -1.34938675], action=1, reward=1.0, next_state=[ 0.04885386  1.38959996  0.03964374 -1.62050179]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 36 ] state=[ 0.04885386  1.38959996  0.03964374 -1.62050179], action=1, reward=1.0, next_state=[ 0.07664586  1.58423312  0.0072337  -1.90056994]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 37 ] state=[ 0.07664586  1.58423312  0.0072337  -1.90056994], action=0, reward=1.0, next_state=[ 0.10833052  1.38903367 -0.03077769 -1.60565174]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 38 ] state=[ 0.10833052  1.38903367 -0.03077769 -1.60565174], action=1, reward=1.0, next_state=[ 0.1361112   1.5845058  -0.06289073 -1.90776827]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 39 ] state=[ 0.1361112   1.5845058  -0.06289073 -1.90776827], action=0, reward=1.0, next_state=[ 0.16780131  1.39011643 -0.10104609 -1.63523837]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 40 ] state=[ 0.16780131  1.39011643 -0.10104609 -1.63523837], action=0, reward=1.0, next_state=[ 0.19560364  1.19631492 -0.13375086 -1.37567595]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 41 ] state=[ 0.19560364  1.19631492 -0.13375086 -1.37567595], action=0, reward=1.0, next_state=[ 0.21952994  1.00309357 -0.16126438 -1.12763811]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 42 ] state=[ 0.21952994  1.00309357 -0.16126438 -1.12763811], action=0, reward=1.0, next_state=[ 0.23959181  0.81040879 -0.18381714 -0.88956755]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 43 ] state=[ 0.23959181  0.81040879 -0.18381714 -0.88956755], action=0, reward=1.0, next_state=[ 0.25579999  0.61819285 -0.20160849 -0.6598394 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 248 ][ timestamp 44 ] state=[ 0.25579999  0.61819285 -0.20160849 -0.6598394 ], action=0, reward=-1.0, next_state=[ 0.26816384  0.42636232 -0.21480528 -0.43679386]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 248: Exploration_rate=0.01. Score=44.\n",
      "[ episode 249 ] state=[ 0.03831255 -0.00997742 -0.01093975 -0.01371889]\n",
      "[ episode 249 ][ timestamp 1 ] state=[ 0.03831255 -0.00997742 -0.01093975 -0.01371889], action=1, reward=1.0, next_state=[ 0.038113    0.1852997  -0.01121413 -0.30983326]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 2 ] state=[ 0.038113    0.1852997  -0.01121413 -0.30983326], action=0, reward=1.0, next_state=[ 0.041819   -0.00966069 -0.01741079 -0.02070794]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 3 ] state=[ 0.041819   -0.00966069 -0.01741079 -0.02070794], action=0, reward=1.0, next_state=[ 0.04162578 -0.20452868 -0.01782495  0.26643123]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 4 ] state=[ 0.04162578 -0.20452868 -0.01782495  0.26643123], action=1, reward=1.0, next_state=[ 0.03753521 -0.00915693 -0.01249633 -0.03182011]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 5 ] state=[ 0.03753521 -0.00915693 -0.01249633 -0.03182011], action=1, reward=1.0, next_state=[ 0.03735207  0.18614198 -0.01313273 -0.32841941]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 6 ] state=[ 0.03735207  0.18614198 -0.01313273 -0.32841941], action=0, reward=1.0, next_state=[ 0.04107491 -0.00879058 -0.01970112 -0.03990671]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 7 ] state=[ 0.04107491 -0.00879058 -0.01970112 -0.03990671], action=1, reward=1.0, next_state=[ 0.0408991   0.18660827 -0.02049925 -0.33873985]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 8 ] state=[ 0.0408991   0.18660827 -0.02049925 -0.33873985], action=0, reward=1.0, next_state=[ 0.04463127 -0.00821608 -0.02727405 -0.05259108]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 9 ] state=[ 0.04463127 -0.00821608 -0.02727405 -0.05259108], action=0, reward=1.0, next_state=[ 0.04446694 -0.20293656 -0.02832587  0.23136342]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 10 ] state=[ 0.04446694 -0.20293656 -0.02832587  0.23136342], action=0, reward=1.0, next_state=[ 0.04040821 -0.39764254 -0.0236986   0.51497855]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 11 ] state=[ 0.04040821 -0.39764254 -0.0236986   0.51497855], action=0, reward=1.0, next_state=[ 0.03245536 -0.59242288 -0.01339903  0.80010027]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 12 ] state=[ 0.03245536 -0.59242288 -0.01339903  0.80010027], action=1, reward=1.0, next_state=[ 0.0206069  -0.39711972  0.00260298  0.50323264]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 13 ] state=[ 0.0206069  -0.39711972  0.00260298  0.50323264], action=0, reward=1.0, next_state=[ 0.01266451 -0.59227826  0.01266763  0.79673474]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 14 ] state=[ 0.01266451 -0.59227826  0.01266763  0.79673474], action=1, reward=1.0, next_state=[ 0.00081894 -0.39733241  0.02860232  0.5080636 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 15 ] state=[ 0.00081894 -0.39733241  0.02860232  0.5080636 ], action=0, reward=1.0, next_state=[-0.0071277  -0.59284543  0.0387636   0.80962112]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 16 ] state=[-0.0071277  -0.59284543  0.0387636   0.80962112], action=1, reward=1.0, next_state=[-0.01898461 -0.39827545  0.05495602  0.52937904]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 17 ] state=[-0.01898461 -0.39827545  0.05495602  0.52937904], action=1, reward=1.0, next_state=[-0.02695012 -0.20396794  0.0655436   0.25450673]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 18 ] state=[-0.02695012 -0.20396794  0.0655436   0.25450673], action=1, reward=1.0, next_state=[-0.03102948 -0.00984009  0.07063373 -0.01680378]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 19 ] state=[-0.03102948 -0.00984009  0.07063373 -0.01680378], action=1, reward=1.0, next_state=[-0.03122628  0.18420154  0.07029766 -0.28639139]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 20 ] state=[-0.03122628  0.18420154  0.07029766 -0.28639139], action=1, reward=1.0, next_state=[-0.02754225  0.37825421  0.06456983 -0.55610097]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 21 ] state=[-0.02754225  0.37825421  0.06456983 -0.55610097], action=1, reward=1.0, next_state=[-0.01997717  0.57241302  0.05344781 -0.82776194]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 22 ] state=[-0.01997717  0.57241302  0.05344781 -0.82776194], action=1, reward=1.0, next_state=[-0.00852891  0.76676504  0.03689257 -1.10316749]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 23 ] state=[-0.00852891  0.76676504  0.03689257 -1.10316749], action=0, reward=1.0, next_state=[ 0.00680639  0.57117774  0.01482922 -0.79914221]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 24 ] state=[ 0.00680639  0.57117774  0.01482922 -0.79914221], action=0, reward=1.0, next_state=[ 0.01822995  0.37585553 -0.00115362 -0.50183148]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 249 ][ timestamp 25 ] state=[ 0.01822995  0.37585553 -0.00115362 -0.50183148], action=1, reward=1.0, next_state=[ 0.02574706  0.57099372 -0.01119025 -0.79487774]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 26 ] state=[ 0.02574706  0.57099372 -0.01119025 -0.79487774], action=0, reward=1.0, next_state=[ 0.03716693  0.37602713 -0.02708781 -0.50573602]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 27 ] state=[ 0.03716693  0.37602713 -0.02708781 -0.50573602], action=0, reward=1.0, next_state=[ 0.04468748  0.18129716 -0.03720253 -0.22171107]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 28 ] state=[ 0.04468748  0.18129716 -0.03720253 -0.22171107], action=0, reward=1.0, next_state=[ 0.04831342 -0.01327383 -0.04163675  0.05900846]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 29 ] state=[ 0.04831342 -0.01327383 -0.04163675  0.05900846], action=0, reward=1.0, next_state=[ 0.04804794 -0.20777483 -0.04045658  0.33826942]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 30 ] state=[ 0.04804794 -0.20777483 -0.04045658  0.33826942], action=0, reward=1.0, next_state=[ 0.04389245 -0.40229844 -0.03369119  0.61792509]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 31 ] state=[ 0.04389245 -0.40229844 -0.03369119  0.61792509], action=0, reward=1.0, next_state=[ 0.03584648 -0.59693396 -0.02133269  0.89980936]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 32 ] state=[ 0.03584648 -0.59693396 -0.02133269  0.89980936], action=0, reward=1.0, next_state=[ 0.0239078  -0.79176043 -0.0033365   1.18571123]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 33 ] state=[ 0.0239078  -0.79176043 -0.0033365   1.18571123], action=0, reward=1.0, next_state=[ 0.00807259 -0.98683894  0.02037772  1.47734645]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 34 ] state=[ 0.00807259 -0.98683894  0.02037772  1.47734645], action=1, reward=1.0, next_state=[-0.01166419 -0.79197169  0.04992465  1.19109689]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 35 ] state=[-0.01166419 -0.79197169  0.04992465  1.19109689], action=0, reward=1.0, next_state=[-0.02750362 -0.98770371  0.07374659  1.49900085]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 36 ] state=[-0.02750362 -0.98770371  0.07374659  1.49900085], action=0, reward=1.0, next_state=[-0.0472577  -1.18364011  0.10372661  1.81376846]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 37 ] state=[-0.0472577  -1.18364011  0.10372661  1.81376846], action=1, reward=1.0, next_state=[-0.0709305  -0.98981483  0.14000198  1.55503416]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 38 ] state=[-0.0709305  -0.98981483  0.14000198  1.55503416], action=1, reward=1.0, next_state=[-0.0907268  -0.79662064  0.17110266  1.30910452]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 39 ] state=[-0.0907268  -0.79662064  0.17110266  1.30910452], action=1, reward=1.0, next_state=[-0.10665921 -0.60402881  0.19728475  1.07449431]\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ][ timestamp 40 ] state=[-0.10665921 -0.60402881  0.19728475  1.07449431], action=1, reward=-1.0, next_state=[-0.11873978 -0.41198246  0.21877464  0.84963884]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 249: Exploration_rate=0.01. Score=40.\n",
      "[ episode 250 ] state=[-0.04366089  0.01380097 -0.04067373 -0.02860283]\n",
      "[ episode 250 ][ timestamp 1 ] state=[-0.04366089  0.01380097 -0.04067373 -0.02860283], action=0, reward=1.0, next_state=[-0.04338487 -0.18071481 -0.04124579  0.25097474]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 2 ] state=[-0.04338487 -0.18071481 -0.04124579  0.25097474], action=0, reward=1.0, next_state=[-0.04699917 -0.37522424 -0.0362263   0.53036792]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 3 ] state=[-0.04699917 -0.37522424 -0.0362263   0.53036792], action=0, reward=1.0, next_state=[-0.05450365 -0.56981837 -0.02561894  0.8114194 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 4 ] state=[-0.05450365 -0.56981837 -0.02561894  0.8114194 ], action=1, reward=1.0, next_state=[-0.06590002 -0.374355   -0.00939055  0.51078942]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 5 ] state=[-0.06590002 -0.374355   -0.00939055  0.51078942], action=1, reward=1.0, next_state=[-0.07338712 -0.17910204  0.00082524  0.2151621 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 6 ] state=[-0.07338712 -0.17910204  0.00082524  0.2151621 ], action=0, reward=1.0, next_state=[-0.07696916 -0.37423577  0.00512848  0.50810523]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 7 ] state=[-0.07696916 -0.37423577  0.00512848  0.50810523], action=1, reward=1.0, next_state=[-0.08445388 -0.17918646  0.01529059  0.21704287]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 8 ] state=[-0.08445388 -0.17918646  0.01529059  0.21704287], action=0, reward=1.0, next_state=[-0.08803761 -0.37452362  0.01963144  0.51450962]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 9 ] state=[-0.08803761 -0.37452362  0.01963144  0.51450962], action=0, reward=1.0, next_state=[-0.09552808 -0.56991646  0.02992164  0.81331368]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 10 ] state=[-0.09552808 -0.56991646  0.02992164  0.81331368], action=1, reward=1.0, next_state=[-0.10692641 -0.37521682  0.04618791  0.53019059]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 11 ] state=[-0.10692641 -0.37521682  0.04618791  0.53019059], action=1, reward=1.0, next_state=[-0.11443074 -0.180774    0.05679172  0.25241183]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 12 ] state=[-0.11443074 -0.180774    0.05679172  0.25241183], action=1, reward=1.0, next_state=[-0.11804622  0.01349296  0.06183996 -0.02183102]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 13 ] state=[-0.11804622  0.01349296  0.06183996 -0.02183102], action=1, reward=1.0, next_state=[-0.11777636  0.20767606  0.06140334 -0.29437953]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 14 ] state=[-0.11777636  0.20767606  0.06140334 -0.29437953], action=1, reward=1.0, next_state=[-0.11362284  0.40187129  0.05551575 -0.56708217]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 15 ] state=[-0.11362284  0.40187129  0.05551575 -0.56708217], action=1, reward=1.0, next_state=[-0.10558542  0.59617233  0.0441741  -0.84177148]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 16 ] state=[-0.10558542  0.59617233  0.0441741  -0.84177148], action=0, reward=1.0, next_state=[-0.09366197  0.40047614  0.02733867 -0.53553058]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 17 ] state=[-0.09366197  0.40047614  0.02733867 -0.53553058], action=0, reward=1.0, next_state=[-0.08565245  0.20498064  0.01662806 -0.23436035]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 18 ] state=[-0.08565245  0.20498064  0.01662806 -0.23436035], action=0, reward=1.0, next_state=[-0.08155284  0.0096251   0.01194086  0.06352087]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 19 ] state=[-0.08155284  0.0096251   0.01194086  0.06352087], action=1, reward=1.0, next_state=[-0.08136033  0.20457384  0.01321127 -0.22537085]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 20 ] state=[-0.08136033  0.20457384  0.01321127 -0.22537085], action=1, reward=1.0, next_state=[-0.07726886  0.39950451  0.00870386 -0.51385734]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 21 ] state=[-0.07726886  0.39950451  0.00870386 -0.51385734], action=0, reward=1.0, next_state=[-0.06927877  0.20426106 -0.00157329 -0.21844436]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 22 ] state=[-0.06927877  0.20426106 -0.00157329 -0.21844436], action=0, reward=1.0, next_state=[-0.06519355  0.00916163 -0.00594218  0.07374187]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 23 ] state=[-0.06519355  0.00916163 -0.00594218  0.07374187], action=0, reward=1.0, next_state=[-0.06501031 -0.18587463 -0.00446734  0.36454411]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 24 ] state=[-0.06501031 -0.18587463 -0.00446734  0.36454411], action=0, reward=1.0, next_state=[-0.06872781 -0.38093281  0.00282354  0.65581506]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 25 ] state=[-0.06872781 -0.38093281  0.00282354  0.65581506], action=1, reward=1.0, next_state=[-0.07634646 -0.18585028  0.01593984  0.36402255]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 26 ] state=[-0.07634646 -0.18585028  0.01593984  0.36402255], action=1, reward=1.0, next_state=[-0.08006347  0.00904155  0.02322029  0.07640807]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 27 ] state=[-0.08006347  0.00904155  0.02322029  0.07640807], action=1, reward=1.0, next_state=[-0.07988264  0.20382306  0.02474845 -0.20885928]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 28 ] state=[-0.07988264  0.20382306  0.02474845 -0.20885928], action=1, reward=1.0, next_state=[-0.07580617  0.39858255  0.02057127 -0.49363375]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 250 ][ timestamp 29 ] state=[-0.07580617  0.39858255  0.02057127 -0.49363375], action=0, reward=1.0, next_state=[-0.06783452  0.2031766   0.01069859 -0.19453932]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 30 ] state=[-0.06783452  0.2031766   0.01069859 -0.19453932], action=0, reward=1.0, next_state=[-0.06377099  0.00790326  0.00680781  0.10149926]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 31 ] state=[-0.06377099  0.00790326  0.00680781  0.10149926], action=0, reward=1.0, next_state=[-0.06361293 -0.18731559  0.00883779  0.39632223]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 32 ] state=[-0.06361293 -0.18731559  0.00883779  0.39632223], action=0, reward=1.0, next_state=[-0.06735924 -0.38256182  0.01676424  0.6917784 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 33 ] state=[-0.06735924 -0.38256182  0.01676424  0.6917784 ], action=1, reward=1.0, next_state=[-0.07501047 -0.18767642  0.03059981  0.40441985]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 34 ] state=[-0.07501047 -0.18767642  0.03059981  0.40441985], action=1, reward=1.0, next_state=[-0.078764   0.0069985  0.0386882  0.1215391]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 35 ] state=[-0.078764   0.0069985  0.0386882  0.1215391], action=1, reward=1.0, next_state=[-0.07862403  0.20154542  0.04111898 -0.15869142]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 36 ] state=[-0.07862403  0.20154542  0.04111898 -0.15869142], action=0, reward=1.0, next_state=[-0.07459312  0.00585964  0.03794516  0.14667473]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 37 ] state=[-0.07459312  0.00585964  0.03794516  0.14667473], action=1, reward=1.0, next_state=[-0.07447593  0.20041823  0.04087865 -0.13379989]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 38 ] state=[-0.07447593  0.20041823  0.04087865 -0.13379989], action=1, reward=1.0, next_state=[-0.07046757  0.39493152  0.03820265 -0.4133111 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 39 ] state=[-0.07046757  0.39493152  0.03820265 -0.4133111 ], action=1, reward=1.0, next_state=[-0.06256894  0.58949172  0.02993643 -0.69370961]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 40 ] state=[-0.06256894  0.58949172  0.02993643 -0.69370961], action=1, reward=1.0, next_state=[-0.0507791   0.78418588  0.01606224 -0.97682001]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 41 ] state=[-0.0507791   0.78418588  0.01606224 -0.97682001], action=0, reward=1.0, next_state=[-0.03509539  0.58885225 -0.00347416 -0.67913527]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 42 ] state=[-0.03509539  0.58885225 -0.00347416 -0.67913527], action=1, reward=1.0, next_state=[-0.02331834  0.78402229 -0.01705687 -0.97290996]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 43 ] state=[-0.02331834  0.78402229 -0.01705687 -0.97290996], action=0, reward=1.0, next_state=[-0.00763789  0.58913331 -0.03651507 -0.68563349]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 44 ] state=[-0.00763789  0.58913331 -0.03651507 -0.68563349], action=0, reward=1.0, next_state=[ 0.00414477  0.39453681 -0.05022774 -0.40466636]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 45 ] state=[ 0.00414477  0.39453681 -0.05022774 -0.40466636], action=0, reward=1.0, next_state=[ 0.01203551  0.20016182 -0.05832106 -0.12823232]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 46 ] state=[ 0.01203551  0.20016182 -0.05832106 -0.12823232], action=1, reward=1.0, next_state=[ 0.01603874  0.39606865 -0.06088571 -0.43872962]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 47 ] state=[ 0.01603874  0.39606865 -0.06088571 -0.43872962], action=0, reward=1.0, next_state=[ 0.02396012  0.20185892 -0.0696603  -0.16584416]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 48 ] state=[ 0.02396012  0.20185892 -0.0696603  -0.16584416], action=1, reward=1.0, next_state=[ 0.0279973   0.3979053  -0.07297719 -0.4796641 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 49 ] state=[ 0.0279973   0.3979053  -0.07297719 -0.4796641 ], action=0, reward=1.0, next_state=[ 0.0359554   0.20388535 -0.08257047 -0.21084504]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 50 ] state=[ 0.0359554   0.20388535 -0.08257047 -0.21084504], action=0, reward=1.0, next_state=[ 0.04003311  0.01003511 -0.08678737  0.0546915 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 51 ] state=[ 0.04003311  0.01003511 -0.08678737  0.0546915 ], action=0, reward=1.0, next_state=[ 0.04023381 -0.18374215 -0.08569354  0.31877995]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 52 ] state=[ 0.04023381 -0.18374215 -0.08569354  0.31877995], action=0, reward=1.0, next_state=[ 0.03655897 -0.37754574 -0.07931794  0.58325554]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 53 ] state=[ 0.03655897 -0.37754574 -0.07931794  0.58325554], action=1, reward=1.0, next_state=[ 0.02900805 -0.18140742 -0.06765283  0.26667801]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 54 ] state=[ 0.02900805 -0.18140742 -0.06765283  0.26667801], action=0, reward=1.0, next_state=[ 0.0253799  -0.37550188 -0.06231927  0.53727893]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 55 ] state=[ 0.0253799  -0.37550188 -0.06231927  0.53727893], action=0, reward=1.0, next_state=[ 0.01786987 -0.56969482 -0.05157369  0.80969287]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 56 ] state=[ 0.01786987 -0.56969482 -0.05157369  0.80969287], action=1, reward=1.0, next_state=[ 0.00647597 -0.37390558 -0.03537983  0.50124356]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 57 ] state=[ 0.00647597 -0.37390558 -0.03537983  0.50124356], action=1, reward=1.0, next_state=[-0.00100214 -0.17830323 -0.02535496  0.19762415]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 58 ] state=[-0.00100214 -0.17830323 -0.02535496  0.19762415], action=0, reward=1.0, next_state=[-0.00456821 -0.37305352 -0.02140248  0.48220213]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 59 ] state=[-0.00456821 -0.37305352 -0.02140248  0.48220213], action=0, reward=1.0, next_state=[-0.01202928 -0.56786694 -0.01175844  0.7680635 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 60 ] state=[-0.01202928 -0.56786694 -0.01175844  0.7680635 ], action=1, reward=1.0, next_state=[-0.02338661 -0.37258512  0.00360283  0.47170411]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 61 ] state=[-0.02338661 -0.37258512  0.00360283  0.47170411], action=1, reward=1.0, next_state=[-0.03083832 -0.17751424  0.01303692  0.18015892]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 62 ] state=[-0.03083832 -0.17751424  0.01303692  0.18015892], action=1, reward=1.0, next_state=[-0.0343886   0.01741875  0.01664009 -0.10838298]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 63 ] state=[-0.0343886   0.01741875  0.01664009 -0.10838298], action=1, reward=1.0, next_state=[-0.03404023  0.21229835  0.01447244 -0.39576994]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 64 ] state=[-0.03404023  0.21229835  0.01447244 -0.39576994], action=0, reward=1.0, next_state=[-0.02979426  0.01697407  0.00655704 -0.09855946]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 65 ] state=[-0.02979426  0.01697407  0.00655704 -0.09855946], action=0, reward=1.0, next_state=[-0.02945478 -0.17824124  0.00458585  0.19618496]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 66 ] state=[-0.02945478 -0.17824124  0.00458585  0.19618496], action=0, reward=1.0, next_state=[-0.0330196  -0.37342848  0.00850955  0.49031099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 67 ] state=[-0.0330196  -0.37342848  0.00850955  0.49031099], action=0, reward=1.0, next_state=[-0.04048817 -0.56866944  0.01831577  0.78566359]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 68 ] state=[-0.04048817 -0.56866944  0.01831577  0.78566359], action=1, reward=1.0, next_state=[-0.05186156 -0.37380385  0.03402904  0.49879878]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 69 ] state=[-0.05186156 -0.37380385  0.03402904  0.49879878], action=0, reward=1.0, next_state=[-0.05933764 -0.56938863  0.04400501  0.80200871]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 70 ] state=[-0.05933764 -0.56938863  0.04400501  0.80200871], action=1, reward=1.0, next_state=[-0.07072541 -0.3748969   0.06004519  0.52348682]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 250 ][ timestamp 71 ] state=[-0.07072541 -0.3748969   0.06004519  0.52348682], action=1, reward=1.0, next_state=[-0.07822335 -0.18066915  0.07051492  0.25031292]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 72 ] state=[-0.07822335 -0.18066915  0.07051492  0.25031292], action=1, reward=1.0, next_state=[-0.08183673  0.01337869  0.07552118 -0.01932126]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 73 ] state=[-0.08183673  0.01337869  0.07552118 -0.01932126], action=1, reward=1.0, next_state=[-0.08156916  0.20734091  0.07513476 -0.28725316]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 74 ] state=[-0.08156916  0.20734091  0.07513476 -0.28725316], action=1, reward=1.0, next_state=[-0.07742234  0.40131544  0.06938969 -0.55532523]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 75 ] state=[-0.07742234  0.40131544  0.06938969 -0.55532523], action=1, reward=1.0, next_state=[-0.06939603  0.59539802  0.05828319 -0.82536431]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 76 ] state=[-0.06939603  0.59539802  0.05828319 -0.82536431], action=0, reward=1.0, next_state=[-0.05748807  0.39952944  0.0417759  -0.51493475]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 77 ] state=[-0.05748807  0.39952944  0.0417759  -0.51493475], action=0, reward=1.0, next_state=[-0.04949748  0.20384483  0.03147721 -0.2093854 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 78 ] state=[-0.04949748  0.20384483  0.03147721 -0.2093854 ], action=1, reward=1.0, next_state=[-0.04542059  0.39850289  0.0272895  -0.49197508]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 79 ] state=[-0.04542059  0.39850289  0.0272895  -0.49197508], action=0, reward=1.0, next_state=[-0.03745053  0.20300686  0.01745    -0.19081809]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 80 ] state=[-0.03745053  0.20300686  0.01745    -0.19081809], action=1, reward=1.0, next_state=[-0.03339039  0.39787488  0.01363364 -0.47794559]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 81 ] state=[-0.03339039  0.39787488  0.01363364 -0.47794559], action=1, reward=1.0, next_state=[-0.02543289  0.59280172  0.00407473 -0.7663005 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 82 ] state=[-0.02543289  0.59280172  0.00407473 -0.7663005 ], action=0, reward=1.0, next_state=[-0.01357686  0.3976239  -0.01125128 -0.47233824]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 83 ] state=[-0.01357686  0.3976239  -0.01125128 -0.47233824], action=0, reward=1.0, next_state=[-0.00562438  0.20266265 -0.02069805 -0.18322269]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 84 ] state=[-0.00562438  0.20266265 -0.02069805 -0.18322269], action=0, reward=1.0, next_state=[-0.00157113  0.00784288 -0.0243625   0.10285958]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 85 ] state=[-0.00157113  0.00784288 -0.0243625   0.10285958], action=0, reward=1.0, next_state=[-0.00141427 -0.18692161 -0.02230531  0.38775775]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 86 ] state=[-0.00141427 -0.18692161 -0.02230531  0.38775775], action=1, reward=1.0, next_state=[-0.0051527   0.00850974 -0.01455016  0.08812643]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 87 ] state=[-0.0051527   0.00850974 -0.01455016  0.08812643], action=1, reward=1.0, next_state=[-0.00498251  0.2038372  -0.01278763 -0.20911134]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 88 ] state=[-0.00498251  0.2038372  -0.01278763 -0.20911134], action=0, reward=1.0, next_state=[-0.00090576  0.00890041 -0.01696986  0.07951048]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 89 ] state=[-0.00090576  0.00890041 -0.01696986  0.07951048], action=0, reward=1.0, next_state=[-0.00072775 -0.18597421 -0.01537965  0.36679143]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 90 ] state=[-0.00072775 -0.18597421 -0.01537965  0.36679143], action=1, reward=1.0, next_state=[-0.00444724  0.00936287 -0.00804382  0.06929902]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 91 ] state=[-0.00444724  0.00936287 -0.00804382  0.06929902], action=0, reward=1.0, next_state=[-0.00425998 -0.18564284 -0.00665784  0.35943326]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 92 ] state=[-0.00425998 -0.18564284 -0.00665784  0.35943326], action=0, reward=1.0, next_state=[-7.97283841e-03 -3.80669519e-01  5.30828646e-04  6.50009407e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 93 ] state=[-7.97283841e-03 -3.80669519e-01  5.30828646e-04  6.50009407e-01], action=0, reward=1.0, next_state=[-0.01558623 -0.57579886  0.01353102  0.94285944]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 94 ] state=[-0.01558623 -0.57579886  0.01353102  0.94285944], action=0, reward=1.0, next_state=[-0.02710221 -0.77110049  0.03238821  1.23976307]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 95 ] state=[-0.02710221 -0.77110049  0.03238821  1.23976307], action=1, reward=1.0, next_state=[-0.04252422 -0.57640909  0.05718347  0.95739959]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 96 ] state=[-0.04252422 -0.57640909  0.05718347  0.95739959], action=1, reward=1.0, next_state=[-0.0540524  -0.38210078  0.07633146  0.68321631]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 97 ] state=[-0.0540524  -0.38210078  0.07633146  0.68321631], action=1, reward=1.0, next_state=[-0.06169441 -0.1881171   0.08999578  0.41550772]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 98 ] state=[-0.06169441 -0.1881171   0.08999578  0.41550772], action=1, reward=1.0, next_state=[-0.06545676  0.00562177  0.09830594  0.15249853]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 99 ] state=[-0.06545676  0.00562177  0.09830594  0.15249853], action=1, reward=1.0, next_state=[-0.06534432  0.19920872  0.10135591 -0.10762447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 100 ] state=[-0.06534432  0.19920872  0.10135591 -0.10762447], action=1, reward=1.0, next_state=[-0.06136015  0.39274317  0.09920342 -0.36668865]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 101 ] state=[-0.06136015  0.39274317  0.09920342 -0.36668865], action=1, reward=1.0, next_state=[-0.05350528  0.58632577  0.09186965 -0.62651689]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 102 ] state=[-0.05350528  0.58632577  0.09186965 -0.62651689], action=1, reward=1.0, next_state=[-0.04177877  0.78005341  0.07933931 -0.88891121]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 103 ] state=[-0.04177877  0.78005341  0.07933931 -0.88891121], action=0, reward=1.0, next_state=[-0.0261777   0.58394961  0.06156109 -0.57237956]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 104 ] state=[-0.0261777   0.58394961  0.06156109 -0.57237956], action=0, reward=1.0, next_state=[-0.01449871  0.38802094  0.05011349 -0.26095574]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 105 ] state=[-0.01449871  0.38802094  0.05011349 -0.26095574], action=1, reward=1.0, next_state=[-0.00673829  0.58239304  0.04489438 -0.53742065]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 106 ] state=[-0.00673829  0.58239304  0.04489438 -0.53742065], action=1, reward=1.0, next_state=[ 0.00490957  0.77685599  0.03414597 -0.81562665]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 107 ] state=[ 0.00490957  0.77685599  0.03414597 -0.81562665], action=1, reward=1.0, next_state=[ 0.02044669  0.97149417  0.01783343 -1.09737678]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 108 ] state=[ 0.02044669  0.97149417  0.01783343 -1.09737678], action=0, reward=1.0, next_state=[ 0.03987658  0.77614202 -0.0041141  -0.79915239]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 109 ] state=[ 0.03987658  0.77614202 -0.0041141  -0.79915239], action=1, reward=1.0, next_state=[ 0.05539942  0.97132016 -0.02009715 -1.09312668]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 110 ] state=[ 0.05539942  0.97132016 -0.02009715 -1.09312668], action=1, reward=1.0, next_state=[ 0.07482582  1.16670106 -0.04195968 -1.392047  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 111 ] state=[ 0.07482582  1.16670106 -0.04195968 -1.392047  ], action=0, reward=1.0, next_state=[ 0.09815984  0.97212591 -0.06980062 -1.1127737 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 112 ] state=[ 0.09815984  0.97212591 -0.06980062 -1.1127737 ], action=0, reward=1.0, next_state=[ 0.11760236  0.7779866  -0.0920561  -0.84277857]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 113 ] state=[ 0.11760236  0.7779866  -0.0920561  -0.84277857], action=0, reward=1.0, next_state=[ 0.13316209  0.58423352 -0.10891167 -0.58040581]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 114 ] state=[ 0.13316209  0.58423352 -0.10891167 -0.58040581], action=0, reward=1.0, next_state=[ 0.14484676  0.39079271 -0.12051978 -0.32392057]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 115 ] state=[ 0.14484676  0.39079271 -0.12051978 -0.32392057], action=0, reward=1.0, next_state=[ 0.15266262  0.19757455 -0.1269982  -0.07154276]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 250 ][ timestamp 116 ] state=[ 0.15266262  0.19757455 -0.1269982  -0.07154276], action=0, reward=1.0, next_state=[ 0.15661411  0.00448045 -0.12842905  0.17852861]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 117 ] state=[ 0.15661411  0.00448045 -0.12842905  0.17852861], action=0, reward=1.0, next_state=[ 0.15670372 -0.18859208 -0.12485848  0.42809784]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 118 ] state=[ 0.15670372 -0.18859208 -0.12485848  0.42809784], action=0, reward=1.0, next_state=[ 0.15293188 -0.38174501 -0.11629652  0.67895869]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 119 ] state=[ 0.15293188 -0.38174501 -0.11629652  0.67895869], action=0, reward=1.0, next_state=[ 0.14529697 -0.57507581 -0.10271735  0.93288187]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 120 ] state=[ 0.14529697 -0.57507581 -0.10271735  0.93288187], action=1, reward=1.0, next_state=[ 0.13379546 -0.37872919 -0.08405971  0.60976846]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 121 ] state=[ 0.13379546 -0.37872919 -0.08405971  0.60976846], action=1, reward=1.0, next_state=[ 0.12622087 -0.18253903 -0.07186434  0.29183786]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 122 ] state=[ 0.12622087 -0.18253903 -0.07186434  0.29183786], action=0, reward=1.0, next_state=[ 0.12257009 -0.37656666 -0.06602759  0.56101815]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 123 ] state=[ 0.12257009 -0.37656666 -0.06602759  0.56101815], action=0, reward=1.0, next_state=[ 0.11503876 -0.57070285 -0.05480722  0.83218988]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 124 ] state=[ 0.11503876 -0.57070285 -0.05480722  0.83218988], action=0, reward=1.0, next_state=[ 0.1036247  -0.7650347  -0.03816342  1.1071447 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 125 ] state=[ 0.1036247  -0.7650347  -0.03816342  1.1071447 ], action=1, reward=1.0, next_state=[ 0.08832401 -0.56943242 -0.01602053  0.8027376 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 126 ] state=[ 0.08832401 -0.56943242 -0.01602053  0.8027376 ], action=1, reward=1.0, next_state=[ 7.69353618e-02 -3.74094489e-01  3.42214439e-05  5.05058473e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 127 ] state=[ 7.69353618e-02 -3.74094489e-01  3.42214439e-05  5.05058473e-01], action=0, reward=1.0, next_state=[ 0.06945347 -0.56921692  0.01013539  0.79775218]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 128 ] state=[ 0.06945347 -0.56921692  0.01013539  0.79775218], action=0, reward=1.0, next_state=[ 0.05806913 -0.76447646  0.02609043  1.0936062 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 129 ] state=[ 0.05806913 -0.76447646  0.02609043  1.0936062 ], action=1, reward=1.0, next_state=[ 0.0427796  -0.56970777  0.04796256  0.80922232]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 130 ] state=[ 0.0427796  -0.56970777  0.04796256  0.80922232], action=1, reward=1.0, next_state=[ 0.03138545 -0.37527469  0.064147    0.53200368]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 131 ] state=[ 0.03138545 -0.37527469  0.064147    0.53200368], action=0, reward=1.0, next_state=[ 0.02387996 -0.57123746  0.07478708  0.84418956]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 132 ] state=[ 0.02387996 -0.57123746  0.07478708  0.84418956], action=1, reward=1.0, next_state=[ 0.01245521 -0.37721138  0.09167087  0.57593088]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 133 ] state=[ 0.01245521 -0.37721138  0.09167087  0.57593088], action=1, reward=1.0, next_state=[ 0.00491098 -0.18348594  0.10318949  0.31347634]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 134 ] state=[ 0.00491098 -0.18348594  0.10318949  0.31347634], action=1, reward=1.0, next_state=[0.00124126 0.01002625 0.10945901 0.05503599]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 135 ] state=[0.00124126 0.01002625 0.10945901 0.05503599], action=1, reward=1.0, next_state=[ 0.00144178  0.20342236  0.11055973 -0.20120533]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 136 ] state=[ 0.00144178  0.20342236  0.11055973 -0.20120533], action=0, reward=1.0, next_state=[0.00551023 0.00690711 0.10653563 0.12420618]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 137 ] state=[0.00551023 0.00690711 0.10653563 0.12420618], action=1, reward=1.0, next_state=[ 0.00564837  0.20035428  0.10901975 -0.13305717]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 138 ] state=[ 0.00564837  0.20035428  0.10901975 -0.13305717], action=1, reward=1.0, next_state=[ 0.00965546  0.39375943  0.10635861 -0.38945424]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 139 ] state=[ 0.00965546  0.39375943  0.10635861 -0.38945424], action=1, reward=1.0, next_state=[ 0.01753065  0.5872237   0.09856952 -0.64680031]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 140 ] state=[ 0.01753065  0.5872237   0.09856952 -0.64680031], action=1, reward=1.0, next_state=[ 0.02927512  0.78084412  0.08563352 -0.90688864]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 141 ] state=[ 0.02927512  0.78084412  0.08563352 -0.90688864], action=1, reward=1.0, next_state=[ 0.044892    0.97470882  0.06749574 -1.17147462]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 142 ] state=[ 0.044892    0.97470882  0.06749574 -1.17147462], action=1, reward=1.0, next_state=[ 0.06438618  1.16889139  0.04406625 -1.44225657]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 143 ] state=[ 0.06438618  1.16889139  0.04406625 -1.44225657], action=0, reward=1.0, next_state=[ 0.08776401  0.97325546  0.01522112 -1.13613626]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 144 ] state=[ 0.08776401  0.97325546  0.01522112 -1.13613626], action=0, reward=1.0, next_state=[ 0.10722912  0.77793772 -0.00750161 -0.83871876]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 145 ] state=[ 0.10722912  0.77793772 -0.00750161 -0.83871876], action=0, reward=1.0, next_state=[ 0.12278787  0.58291901 -0.02427598 -0.54840437]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 146 ] state=[ 0.12278787  0.58291901 -0.02427598 -0.54840437], action=1, reward=1.0, next_state=[ 0.13444625  0.77837343 -0.03524407 -0.84863605]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 147 ] state=[ 0.13444625  0.77837343 -0.03524407 -0.84863605], action=0, reward=1.0, next_state=[ 0.15001372  0.58374943 -0.05221679 -0.56724096]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 148 ] state=[ 0.15001372  0.58374943 -0.05221679 -0.56724096], action=1, reward=1.0, next_state=[ 0.16168871  0.77956347 -0.06356161 -0.87590645]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 149 ] state=[ 0.16168871  0.77956347 -0.06356161 -0.87590645], action=1, reward=1.0, next_state=[ 0.17727998  0.97548915 -0.08107974 -1.18787603]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 150 ] state=[ 0.17727998  0.97548915 -0.08107974 -1.18787603], action=0, reward=1.0, next_state=[ 0.19678976  0.78150653 -0.10483726 -0.92166932]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 151 ] state=[ 0.19678976  0.78150653 -0.10483726 -0.92166932], action=1, reward=1.0, next_state=[ 0.21241989  0.97787693 -0.12327064 -1.24537343]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 152 ] state=[ 0.21241989  0.97787693 -0.12327064 -1.24537343], action=0, reward=1.0, next_state=[ 0.23197743  0.78453284 -0.14817811 -0.99370786]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 153 ] state=[ 0.23197743  0.78453284 -0.14817811 -0.99370786], action=0, reward=1.0, next_state=[ 0.24766809  0.59167038 -0.16805227 -0.75098944]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 154 ] state=[ 0.24766809  0.59167038 -0.16805227 -0.75098944], action=0, reward=1.0, next_state=[ 0.2595015   0.39921536 -0.18307206 -0.51554889]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 155 ] state=[ 0.2595015   0.39921536 -0.18307206 -0.51554889], action=0, reward=1.0, next_state=[ 0.2674858   0.20707939 -0.19338304 -0.28568413]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 156 ] state=[ 0.2674858   0.20707939 -0.19338304 -0.28568413], action=0, reward=1.0, next_state=[ 0.27162739  0.01516554 -0.19909672 -0.05968026]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 157 ] state=[ 0.27162739  0.01516554 -0.19909672 -0.05968026], action=0, reward=1.0, next_state=[ 0.2719307  -0.17662765 -0.20029032  0.1641779 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 158 ] state=[ 0.2719307  -0.17662765 -0.20029032  0.1641779 ], action=0, reward=1.0, next_state=[ 0.26839815 -0.36840315 -0.19700677  0.38759799]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 159 ] state=[ 0.26839815 -0.36840315 -0.19700677  0.38759799], action=0, reward=1.0, next_state=[ 0.26103009 -0.56026311 -0.18925481  0.61227513]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 160 ] state=[ 0.26103009 -0.56026311 -0.18925481  0.61227513], action=0, reward=1.0, next_state=[ 0.24982482 -0.7523067  -0.1770093   0.83988766]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 250 ][ timestamp 161 ] state=[ 0.24982482 -0.7523067  -0.1770093   0.83988766], action=1, reward=1.0, next_state=[ 0.23477869 -0.55526709 -0.16021155  0.49717705]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 162 ] state=[ 0.23477869 -0.55526709 -0.16021155  0.49717705], action=0, reward=1.0, next_state=[ 0.22367335 -0.74781043 -0.15026801  0.7353924 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 163 ] state=[ 0.22367335 -0.74781043 -0.15026801  0.7353924 ], action=0, reward=1.0, next_state=[ 0.20871714 -0.9405725  -0.13556016  0.97726443]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 164 ] state=[ 0.20871714 -0.9405725  -0.13556016  0.97726443], action=1, reward=1.0, next_state=[ 0.18990569 -0.74391888 -0.11601487  0.64525748]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 165 ] state=[ 0.18990569 -0.74391888 -0.11601487  0.64525748], action=1, reward=1.0, next_state=[ 0.17502731 -0.5473877  -0.10310972  0.31841047]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 166 ] state=[ 0.17502731 -0.5473877  -0.10310972  0.31841047], action=1, reward=1.0, next_state=[ 0.16407956 -0.3509599  -0.09674151 -0.00492691]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 167 ] state=[ 0.16407956 -0.3509599  -0.09674151 -0.00492691], action=0, reward=1.0, next_state=[ 0.15706036 -0.54457091 -0.09684005  0.255734  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 168 ] state=[ 0.15706036 -0.54457091 -0.09684005  0.255734  ], action=1, reward=1.0, next_state=[ 0.14616894 -0.34820929 -0.09172537 -0.0658549 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 169 ] state=[ 0.14616894 -0.34820929 -0.09172537 -0.0658549 ], action=0, reward=1.0, next_state=[ 0.13920476 -0.54190461 -0.09304247  0.19653723]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 170 ] state=[ 0.13920476 -0.54190461 -0.09304247  0.19653723], action=0, reward=1.0, next_state=[ 0.12836666 -0.73558106 -0.08911173  0.4584803 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 171 ] state=[ 0.12836666 -0.73558106 -0.08911173  0.4584803 ], action=0, reward=1.0, next_state=[ 0.11365504 -0.92933774 -0.07994212  0.72179794]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 172 ] state=[ 0.11365504 -0.92933774 -0.07994212  0.72179794], action=0, reward=1.0, next_state=[ 0.09506829 -1.12326818 -0.06550616  0.98828662]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 173 ] state=[ 0.09506829 -1.12326818 -0.06550616  0.98828662], action=0, reward=1.0, next_state=[ 0.07260292 -1.31745485 -0.04574043  1.25969686]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 174 ] state=[ 0.07260292 -1.31745485 -0.04574043  1.25969686], action=0, reward=1.0, next_state=[ 0.04625383 -1.51196277 -0.02054649  1.53771058]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 175 ] state=[ 0.04625383 -1.51196277 -0.02054649  1.53771058], action=0, reward=1.0, next_state=[ 0.01601457 -1.70683151  0.01020772  1.82391176]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 176 ] state=[ 0.01601457 -1.70683151  0.01020772  1.82391176], action=1, reward=1.0, next_state=[-0.01812206 -1.5118243   0.04668596  1.5344172 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 177 ] state=[-0.01812206 -1.5118243   0.04668596  1.5344172 ], action=1, reward=1.0, next_state=[-0.04835854 -1.31729472  0.0773743   1.25666145]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 178 ] state=[-0.04835854 -1.31729472  0.0773743   1.25666145], action=0, reward=1.0, next_state=[-0.07470444 -1.51331712  0.10250753  1.57254069]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 179 ] state=[-0.07470444 -1.51331712  0.10250753  1.57254069], action=0, reward=1.0, next_state=[-0.10497078 -1.70950172  0.13395834  1.8953573 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 180 ] state=[-0.10497078 -1.70950172  0.13395834  1.8953573 ], action=0, reward=1.0, next_state=[-0.13916082 -1.90579764  0.17186549  2.22642932]\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ][ timestamp 181 ] state=[-0.13916082 -1.90579764  0.17186549  2.22642932], action=0, reward=-1.0, next_state=[-0.17727677 -2.10208898  0.21639407  2.5668086 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 250: Exploration_rate=0.01. Score=181.\n",
      "[ episode 251 ] state=[ 0.01982677  0.04921568 -0.03419026  0.03515414]\n",
      "[ episode 251 ][ timestamp 1 ] state=[ 0.01982677  0.04921568 -0.03419026  0.03515414], action=0, reward=1.0, next_state=[ 0.02081108 -0.14539972 -0.03348717  0.31685665]\n",
      "[ Experience replay ] starts\n",
      "[ episode 251 ][ timestamp 2 ] state=[ 0.02081108 -0.14539972 -0.03348717  0.31685665], action=0, reward=1.0, next_state=[ 0.01790308 -0.34002909 -0.02715004  0.59879364]\n",
      "[ Experience replay ] starts\n",
      "[ episode 251 ][ timestamp 3 ] state=[ 0.01790308 -0.34002909 -0.02715004  0.59879364], action=0, reward=1.0, next_state=[ 0.0111025  -0.53476086 -0.01517417  0.88280251]\n",
      "[ Experience replay ] starts\n",
      "[ episode 251 ][ timestamp 4 ] state=[ 0.0111025  -0.53476086 -0.01517417  0.88280251], action=0, reward=1.0, next_state=[ 4.07285979e-04 -7.29673475e-01  2.48188224e-03  1.17067674e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 251 ][ timestamp 5 ] state=[ 4.07285979e-04 -7.29673475e-01  2.48188224e-03  1.17067674e+00], action=0, reward=1.0, next_state=[-0.01418618 -0.92482761  0.02589542  1.46413672]\n",
      "[ Experience replay ] starts\n",
      "[ episode 251 ][ timestamp 6 ] state=[-0.01418618 -0.92482761  0.02589542  1.46413672], action=0, reward=1.0, next_state=[-0.03268274 -1.12025704  0.05517815  1.76479498]\n",
      "[ Experience replay ] starts\n",
      "[ episode 251 ][ timestamp 7 ] state=[-0.03268274 -1.12025704  0.05517815  1.76479498], action=1, reward=1.0, next_state=[-0.05508788 -0.92580052  0.09047405  1.48976827]\n",
      "[ Experience replay ] starts\n",
      "[ episode 251 ][ timestamp 8 ] state=[-0.05508788 -0.92580052  0.09047405  1.48976827], action=1, reward=1.0, next_state=[-0.07360389 -0.73188922  0.12026942  1.22665406]\n",
      "[ Experience replay ] starts\n",
      "[ episode 251 ][ timestamp 9 ] state=[-0.07360389 -0.73188922  0.12026942  1.22665406], action=1, reward=1.0, next_state=[-0.08824167 -0.53850309  0.1448025   0.97394432]\n",
      "[ Experience replay ] starts\n",
      "[ episode 251 ][ timestamp 10 ] state=[-0.08824167 -0.53850309  0.1448025   0.97394432], action=0, reward=1.0, next_state=[-0.09901173 -0.73523954  0.16428138  1.30838388]\n",
      "[ Experience replay ] starts\n",
      "[ episode 251 ][ timestamp 11 ] state=[-0.09901173 -0.73523954  0.16428138  1.30838388], action=1, reward=1.0, next_state=[-0.11371652 -0.54253546  0.19044906  1.07130133]\n",
      "[ Experience replay ] starts\n",
      "[ episode 251 ][ timestamp 12 ] state=[-0.11371652 -0.54253546  0.19044906  1.07130133], action=1, reward=-1.0, next_state=[-0.12456723 -0.35037116  0.21187509  0.84392074]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 251: Exploration_rate=0.01. Score=12.\n",
      "[ episode 252 ] state=[ 0.04332091 -0.02265626 -0.00345676  0.04958407]\n",
      "[ episode 252 ][ timestamp 1 ] state=[ 0.04332091 -0.02265626 -0.00345676  0.04958407], action=1, reward=1.0, next_state=[ 0.04286778  0.17251508 -0.00246508 -0.24418749]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 2 ] state=[ 0.04286778  0.17251508 -0.00246508 -0.24418749], action=0, reward=1.0, next_state=[ 0.04631808 -0.02257157 -0.00734883  0.04771687]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 3 ] state=[ 0.04631808 -0.02257157 -0.00734883  0.04771687], action=1, reward=1.0, next_state=[ 0.04586665  0.17265498 -0.0063945  -0.24727559]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 4 ] state=[ 0.04586665  0.17265498 -0.0063945  -0.24727559], action=1, reward=1.0, next_state=[ 0.04931975  0.36786767 -0.01134001 -0.5419686 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 5 ] state=[ 0.04931975  0.36786767 -0.01134001 -0.5419686 ], action=1, reward=1.0, next_state=[ 0.0566771   0.56314715 -0.02217938 -0.83820289]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 6 ] state=[ 0.0566771   0.56314715 -0.02217938 -0.83820289], action=1, reward=1.0, next_state=[ 0.06794005  0.75856485 -0.03894344 -1.13777754]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 7 ] state=[ 0.06794005  0.75856485 -0.03894344 -1.13777754], action=0, reward=1.0, next_state=[ 0.08311134  0.56397325 -0.06169899 -0.85755793]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 8 ] state=[ 0.08311134  0.56397325 -0.06169899 -0.85755793], action=1, reward=1.0, next_state=[ 0.09439081  0.75987908 -0.07885015 -1.16898551]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 9 ] state=[ 0.09439081  0.75987908 -0.07885015 -1.16898551], action=0, reward=1.0, next_state=[ 0.10958839  0.56586633 -0.10222986 -0.90202854]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 10 ] state=[ 0.10958839  0.56586633 -0.10222986 -0.90202854], action=0, reward=1.0, next_state=[ 0.12090572  0.37226684 -0.12027043 -0.6431487 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 11 ] state=[ 0.12090572  0.37226684 -0.12027043 -0.6431487 ], action=0, reward=1.0, next_state=[ 0.12835105  0.1790083  -0.1331334  -0.3906293 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 12 ] state=[ 0.12835105  0.1790083  -0.1331334  -0.3906293 ], action=0, reward=1.0, next_state=[ 0.13193122 -0.01399773 -0.14094599 -0.14270787]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 252 ][ timestamp 13 ] state=[ 0.13193122 -0.01399773 -0.14094599 -0.14270787], action=0, reward=1.0, next_state=[ 0.13165127 -0.20684929 -0.14380014  0.10239981]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 14 ] state=[ 0.13165127 -0.20684929 -0.14380014  0.10239981], action=0, reward=1.0, next_state=[ 0.12751428 -0.399649   -0.14175215  0.34648274]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 15 ] state=[ 0.12751428 -0.399649   -0.14175215  0.34648274], action=0, reward=1.0, next_state=[ 0.1195213  -0.59250004 -0.13482249  0.59132214]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 16 ] state=[ 0.1195213  -0.59250004 -0.13482249  0.59132214], action=1, reward=1.0, next_state=[ 0.1076713  -0.39577381 -0.12299605  0.25939283]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 17 ] state=[ 0.1076713  -0.39577381 -0.12299605  0.25939283], action=0, reward=1.0, next_state=[ 0.09975582 -0.58894486 -0.11780819  0.5108907 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 18 ] state=[ 0.09975582 -0.58894486 -0.11780819  0.5108907 ], action=1, reward=1.0, next_state=[ 0.08797693 -0.39237758 -0.10759038  0.18352796]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 19 ] state=[ 0.08797693 -0.39237758 -0.10759038  0.18352796], action=0, reward=1.0, next_state=[ 0.08012937 -0.58580873 -0.10391982  0.4404264 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 20 ] state=[ 0.08012937 -0.58580873 -0.10391982  0.4404264 ], action=0, reward=1.0, next_state=[ 0.0684132  -0.77931829 -0.09511129  0.69862736]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 21 ] state=[ 0.0684132  -0.77931829 -0.09511129  0.69862736], action=1, reward=1.0, next_state=[ 0.05282683 -0.58301529 -0.08113875  0.37758311]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 22 ] state=[ 0.05282683 -0.58301529 -0.08113875  0.37758311], action=0, reward=1.0, next_state=[ 0.04116653 -0.77689677 -0.07358708  0.64361992]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 23 ] state=[ 0.04116653 -0.77689677 -0.07358708  0.64361992], action=1, reward=1.0, next_state=[ 0.02562859 -0.58083054 -0.06071469  0.32870141]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 24 ] state=[ 0.02562859 -0.58083054 -0.06071469  0.32870141], action=1, reward=1.0, next_state=[ 0.01401198 -0.38489917 -0.05414066  0.01750673]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 25 ] state=[ 0.01401198 -0.38489917 -0.05414066  0.01750673], action=1, reward=1.0, next_state=[ 0.006314   -0.18904426 -0.05379052 -0.29175475]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 26 ] state=[ 0.006314   -0.18904426 -0.05379052 -0.29175475], action=0, reward=1.0, next_state=[ 0.00253311 -0.38335963 -0.05962562 -0.01651006]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 27 ] state=[ 0.00253311 -0.38335963 -0.05962562 -0.01651006], action=0, reward=1.0, next_state=[-0.00513408 -0.57757804 -0.05995582  0.2567803 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 28 ] state=[-0.00513408 -0.57757804 -0.05995582  0.2567803 ], action=1, reward=1.0, next_state=[-0.01668564 -0.38165362 -0.05482021 -0.05419473]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 29 ] state=[-0.01668564 -0.38165362 -0.05482021 -0.05419473], action=0, reward=1.0, next_state=[-0.02431871 -0.57594841 -0.05590411  0.22070058]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 30 ] state=[-0.02431871 -0.57594841 -0.05590411  0.22070058], action=0, reward=1.0, next_state=[-0.03583768 -0.77022856 -0.0514901   0.49523829]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 31 ] state=[-0.03583768 -0.77022856 -0.0514901   0.49523829], action=1, reward=1.0, next_state=[-0.05124225 -0.57441974 -0.04158533  0.18678292]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 32 ] state=[-0.05124225 -0.57441974 -0.04158533  0.18678292], action=0, reward=1.0, next_state=[-0.06273065 -0.7689228  -0.03784967  0.46606271]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 33 ] state=[-0.06273065 -0.7689228  -0.03784967  0.46606271], action=0, reward=1.0, next_state=[-0.0781091  -0.96349008 -0.02852842  0.74657946]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 34 ] state=[-0.0781091  -0.96349008 -0.02852842  0.74657946], action=1, reward=1.0, next_state=[-0.0973789  -0.76798637 -0.01359683  0.445057  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 35 ] state=[-0.0973789  -0.76798637 -0.01359683  0.445057  ], action=0, reward=1.0, next_state=[-0.11273863 -0.96291333 -0.00469569  0.73342308]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 36 ] state=[-0.11273863 -0.96291333 -0.00469569  0.73342308], action=0, reward=1.0, next_state=[-0.1319969  -1.15797009  0.00997277  1.02462446]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 37 ] state=[-0.1319969  -1.15797009  0.00997277  1.02462446], action=0, reward=1.0, next_state=[-0.1551563  -1.35322342  0.03046526  1.32042184]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 38 ] state=[-0.1551563  -1.35322342  0.03046526  1.32042184], action=0, reward=1.0, next_state=[-0.18222077 -1.54871693  0.0568737   1.62248144]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 39 ] state=[-0.18222077 -1.54871693  0.0568737   1.62248144], action=1, reward=1.0, next_state=[-0.21319511 -1.35430885  0.08932333  1.34805267]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 40 ] state=[-0.21319511 -1.35430885  0.08932333  1.34805267], action=1, reward=1.0, next_state=[-0.24028128 -1.16041587  0.11628438  1.08459883]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 41 ] state=[-0.24028128 -1.16041587  0.11628438  1.08459883], action=0, reward=1.0, next_state=[-0.2634896  -1.35686387  0.13797636  1.41139139]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 42 ] state=[-0.2634896  -1.35686387  0.13797636  1.41139139], action=1, reward=1.0, next_state=[-0.29062688 -1.16369576  0.16620419  1.1648294 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 43 ] state=[-0.29062688 -1.16369576  0.16620419  1.1648294 ], action=0, reward=1.0, next_state=[-0.31390079 -1.36054444  0.18950077  1.50467288]\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ][ timestamp 44 ] state=[-0.31390079 -1.36054444  0.18950077  1.50467288], action=1, reward=-1.0, next_state=[-0.34111168 -1.16815902  0.21959423  1.27664115]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 252: Exploration_rate=0.01. Score=44.\n",
      "[ episode 253 ] state=[-0.00242921  0.01143942  0.00143047  0.02567206]\n",
      "[ episode 253 ][ timestamp 1 ] state=[-0.00242921  0.01143942  0.00143047  0.02567206], action=0, reward=1.0, next_state=[-0.00220042 -0.18370302  0.00194391  0.31880597]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 2 ] state=[-0.00220042 -0.18370302  0.00194391  0.31880597], action=1, reward=1.0, next_state=[-0.00587448  0.01139119  0.00832003  0.02673671]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 3 ] state=[-0.00587448  0.01139119  0.00832003  0.02673671], action=1, reward=1.0, next_state=[-0.00564666  0.20639285  0.00885477 -0.26330958]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 4 ] state=[-0.00564666  0.20639285  0.00885477 -0.26330958], action=0, reward=1.0, next_state=[-0.0015188   0.01114563  0.00358858  0.03215303]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 5 ] state=[-0.0015188   0.01114563  0.00358858  0.03215303], action=0, reward=1.0, next_state=[-0.00129589 -0.1840276   0.00423164  0.32596603]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 6 ] state=[-0.00129589 -0.1840276   0.00423164  0.32596603], action=1, reward=1.0, next_state=[-0.00497644  0.01103385  0.01075096  0.03462057]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 7 ] state=[-0.00497644  0.01103385  0.01075096  0.03462057], action=1, reward=1.0, next_state=[-0.00475576  0.20599999  0.01144337 -0.25465102]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 8 ] state=[-0.00475576  0.20599999  0.01144337 -0.25465102], action=0, reward=1.0, next_state=[-0.00063576  0.01071654  0.00635035  0.04161926]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 9 ] state=[-0.00063576  0.01071654  0.00635035  0.04161926], action=1, reward=1.0, next_state=[-0.00042143  0.20574685  0.00718273 -0.24905332]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 10 ] state=[-0.00042143  0.20574685  0.00718273 -0.24905332], action=0, reward=1.0, next_state=[0.0036935  0.01052307 0.00220167 0.04588651]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 253 ][ timestamp 11 ] state=[0.0036935  0.01052307 0.00220167 0.04588651], action=1, reward=1.0, next_state=[ 0.00390397  0.20561338  0.0031194  -0.24610096]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 12 ] state=[ 0.00390397  0.20561338  0.0031194  -0.24610096], action=0, reward=1.0, next_state=[ 0.00801623  0.01044701 -0.00180262  0.04756426]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 13 ] state=[ 0.00801623  0.01044701 -0.00180262  0.04756426], action=1, reward=1.0, next_state=[ 0.00822517  0.20559477 -0.00085134 -0.24568686]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 14 ] state=[ 0.00822517  0.20559477 -0.00085134 -0.24568686], action=0, reward=1.0, next_state=[ 0.01233707  0.01048498 -0.00576507  0.04672741]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 15 ] state=[ 0.01233707  0.01048498 -0.00576507  0.04672741], action=1, reward=1.0, next_state=[ 0.01254677  0.20568913 -0.00483053 -0.24776886]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 16 ] state=[ 0.01254677  0.20568913 -0.00483053 -0.24776886], action=0, reward=1.0, next_state=[ 0.01666055  0.01063649 -0.0097859   0.04338651]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 17 ] state=[ 0.01666055  0.01063649 -0.0097859   0.04338651], action=1, reward=1.0, next_state=[ 0.01687328  0.20589739 -0.00891817 -0.25236783]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 18 ] state=[ 0.01687328  0.20589739 -0.00891817 -0.25236783], action=0, reward=1.0, next_state=[ 0.02099123  0.01090392 -0.01396553  0.03748885]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 19 ] state=[ 0.02099123  0.01090392 -0.01396553  0.03748885], action=0, reward=1.0, next_state=[ 0.02120931 -0.18401501 -0.01321575  0.325733  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 20 ] state=[ 0.02120931 -0.18401501 -0.01321575  0.325733  ], action=1, reward=1.0, next_state=[ 0.01752901  0.01129259 -0.00670109  0.02891186]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 21 ] state=[ 0.01752901  0.01129259 -0.00670109  0.02891186], action=0, reward=1.0, next_state=[ 0.01775486 -0.18373263 -0.00612286  0.31947301]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 22 ] state=[ 0.01775486 -0.18373263 -0.00612286  0.31947301], action=1, reward=1.0, next_state=[0.01408021 0.01147599 0.0002666  0.02486547]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 23 ] state=[0.01408021 0.01147599 0.0002666  0.02486547], action=0, reward=1.0, next_state=[ 0.01430973 -0.18364979  0.00076391  0.3176325 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 24 ] state=[ 0.01430973 -0.18364979  0.00076391  0.3176325 ], action=1, reward=1.0, next_state=[0.01063673 0.01146128 0.00711656 0.02519058]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 25 ] state=[0.01063673 0.01146128 0.00711656 0.02519058], action=1, reward=1.0, next_state=[ 0.01086596  0.20648045  0.00762038 -0.26523852]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 26 ] state=[ 0.01086596  0.20648045  0.00762038 -0.26523852], action=0, reward=1.0, next_state=[0.01499556 0.01125057 0.0023156  0.02983817]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 27 ] state=[0.01499556 0.01125057 0.0023156  0.02983817], action=1, reward=1.0, next_state=[ 0.01522058  0.20633924  0.00291237 -0.26211326]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 28 ] state=[ 0.01522058  0.20633924  0.00291237 -0.26211326], action=0, reward=1.0, next_state=[ 0.01934736  0.01117583 -0.0023299   0.03148684]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 29 ] state=[ 0.01934736  0.01117583 -0.0023299   0.03148684], action=1, reward=1.0, next_state=[ 0.01957088  0.20633112 -0.00170016 -0.26193028]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 30 ] state=[ 0.01957088  0.20633112 -0.00170016 -0.26193028], action=1, reward=1.0, next_state=[ 0.0236975   0.4014773  -0.00693877 -0.55514898]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 31 ] state=[ 0.0236975   0.4014773  -0.00693877 -0.55514898], action=0, reward=1.0, next_state=[ 0.03172705  0.20645346 -0.01804175 -0.26466024]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 32 ] state=[ 0.03172705  0.20645346 -0.01804175 -0.26466024], action=1, reward=1.0, next_state=[ 0.03585611  0.40182821 -0.02333495 -0.56297866]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 33 ] state=[ 0.03585611  0.40182821 -0.02333495 -0.56297866], action=1, reward=1.0, next_state=[ 0.04389268  0.5972697  -0.03459452 -0.86292093]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 34 ] state=[ 0.04389268  0.5972697  -0.03459452 -0.86292093], action=0, reward=1.0, next_state=[ 0.05583807  0.4026354  -0.05185294 -0.58131292]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 35 ] state=[ 0.05583807  0.4026354  -0.05185294 -0.58131292], action=0, reward=1.0, next_state=[ 0.06389078  0.20827688 -0.0634792  -0.30540492]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 36 ] state=[ 0.06389078  0.20827688 -0.0634792  -0.30540492], action=0, reward=1.0, next_state=[ 0.06805632  0.01411426 -0.0695873  -0.03339796]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 37 ] state=[ 0.06805632  0.01411426 -0.0695873  -0.03339796], action=1, reward=1.0, next_state=[ 0.0683386   0.21016155 -0.07025526 -0.34719934]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 38 ] state=[ 0.0683386   0.21016155 -0.07025526 -0.34719934], action=0, reward=1.0, next_state=[ 0.07254183  0.01610557 -0.07719924 -0.07747149]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 39 ] state=[ 0.07254183  0.01610557 -0.07719924 -0.07747149], action=1, reward=1.0, next_state=[ 0.07286395  0.21224446 -0.07874867 -0.3934776 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 40 ] state=[ 0.07286395  0.21224446 -0.07874867 -0.3934776 ], action=0, reward=1.0, next_state=[ 0.07710883  0.01832315 -0.08661823 -0.12662529]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 41 ] state=[ 0.07710883  0.01832315 -0.08661823 -0.12662529], action=1, reward=1.0, next_state=[ 0.0774753   0.21457231 -0.08915073 -0.44532934]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 42 ] state=[ 0.0774753   0.21457231 -0.08915073 -0.44532934], action=1, reward=1.0, next_state=[ 0.08176674  0.41083498 -0.09805732 -0.76472984]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 43 ] state=[ 0.08176674  0.41083498 -0.09805732 -0.76472984], action=1, reward=1.0, next_state=[ 0.08998344  0.60716068 -0.11335192 -1.08658641]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 44 ] state=[ 0.08998344  0.60716068 -0.11335192 -1.08658641], action=0, reward=1.0, next_state=[ 0.10212666  0.41370129 -0.13508364 -0.83151374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 45 ] state=[ 0.10212666  0.41370129 -0.13508364 -0.83151374], action=0, reward=1.0, next_state=[ 0.11040068  0.2206585  -0.15171392 -0.58418138]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 46 ] state=[ 0.11040068  0.2206585  -0.15171392 -0.58418138], action=1, reward=1.0, next_state=[ 0.11481385  0.41754364 -0.16339755 -0.92054979]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 47 ] state=[ 0.11481385  0.41754364 -0.16339755 -0.92054979], action=0, reward=1.0, next_state=[ 0.12316473  0.22496196 -0.18180854 -0.68335037]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 48 ] state=[ 0.12316473  0.22496196 -0.18180854 -0.68335037], action=0, reward=1.0, next_state=[ 0.12766396  0.0327677  -0.19547555 -0.45296824]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 49 ] state=[ 0.12766396  0.0327677  -0.19547555 -0.45296824], action=1, reward=1.0, next_state=[ 0.12831932  0.23003875 -0.20453491 -0.80034391]\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ][ timestamp 50 ] state=[ 0.12831932  0.23003875 -0.20453491 -0.80034391], action=1, reward=-1.0, next_state=[ 0.13292009  0.42729054 -0.22054179 -1.14976906]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 253: Exploration_rate=0.01. Score=50.\n",
      "[ episode 254 ] state=[0.02376596 0.02253119 0.03985849 0.02595064]\n",
      "[ episode 254 ][ timestamp 1 ] state=[0.02376596 0.02253119 0.03985849 0.02595064], action=1, reward=1.0, next_state=[ 0.02421659  0.21705954  0.0403775  -0.25389484]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 2 ] state=[ 0.02421659  0.21705954  0.0403775  -0.25389484], action=1, reward=1.0, next_state=[ 0.02855778  0.41158239  0.03529961 -0.53357354]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 3 ] state=[ 0.02855778  0.41158239  0.03529961 -0.53357354], action=1, reward=1.0, next_state=[ 0.03678943  0.60619058  0.02462814 -0.81492804]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 254 ][ timestamp 4 ] state=[ 0.03678943  0.60619058  0.02462814 -0.81492804], action=1, reward=1.0, next_state=[ 0.04891324  0.80096679  0.00832958 -1.0997638 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 5 ] state=[ 0.04891324  0.80096679  0.00832958 -1.0997638 ], action=0, reward=1.0, next_state=[ 0.06493257  0.6057362  -0.0136657  -0.80447922]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 6 ] state=[ 0.06493257  0.6057362  -0.0136657  -0.80447922], action=0, reward=1.0, next_state=[ 0.0770473   0.41080425 -0.02975529 -0.51612618]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 7 ] state=[ 0.0770473   0.41080425 -0.02975529 -0.51612618], action=0, reward=1.0, next_state=[ 0.08526338  0.21611366 -0.04007781 -0.23296633]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 8 ] state=[ 0.08526338  0.21611366 -0.04007781 -0.23296633], action=0, reward=1.0, next_state=[ 0.08958565  0.02158659 -0.04473714  0.04681023]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 9 ] state=[ 0.08958565  0.02158659 -0.04473714  0.04681023], action=0, reward=1.0, next_state=[ 0.09001739 -0.17286626 -0.04380093  0.32504934]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 10 ] state=[ 0.09001739 -0.17286626 -0.04380093  0.32504934], action=0, reward=1.0, next_state=[ 0.08656006 -0.36733807 -0.03729994  0.60360393]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 11 ] state=[ 0.08656006 -0.36733807 -0.03729994  0.60360393], action=1, reward=1.0, next_state=[ 0.0792133  -0.17171484 -0.02522787  0.29940954]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 12 ] state=[ 0.0792133  -0.17171484 -0.02522787  0.29940954], action=1, reward=1.0, next_state=[ 0.075779    0.02375745 -0.01923967 -0.0011218 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 13 ] state=[ 0.075779    0.02375745 -0.01923967 -0.0011218 ], action=0, reward=1.0, next_state=[ 0.07625415 -0.17108336 -0.01926211  0.28542922]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 14 ] state=[ 0.07625415 -0.17108336 -0.01926211  0.28542922], action=0, reward=1.0, next_state=[ 0.07283249 -0.36592538 -0.01355353  0.57197531]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 15 ] state=[ 0.07283249 -0.36592538 -0.01355353  0.57197531], action=1, reward=1.0, next_state=[ 0.06551398 -0.17061602 -0.00211402  0.27505357]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 16 ] state=[ 0.06551398 -0.17061602 -0.00211402  0.27505357], action=0, reward=1.0, next_state=[ 0.06210166 -0.36570775  0.00338705  0.56706899]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 17 ] state=[ 0.06210166 -0.36570775  0.00338705  0.56706899], action=0, reward=1.0, next_state=[ 0.0547875  -0.56087705  0.01472843  0.86081705]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 18 ] state=[ 0.0547875  -0.56087705  0.01472843  0.86081705], action=1, reward=1.0, next_state=[ 0.04356996 -0.36595874  0.03194477  0.5728013 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 19 ] state=[ 0.04356996 -0.36595874  0.03194477  0.5728013 ], action=1, reward=1.0, next_state=[ 0.03625079 -0.17129892  0.0434008   0.2903507 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 20 ] state=[ 0.03625079 -0.17129892  0.0434008   0.2903507 ], action=1, reward=1.0, next_state=[0.03282481 0.02317816 0.04920781 0.01166561]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 21 ] state=[0.03282481 0.02317816 0.04920781 0.01166561], action=1, reward=1.0, next_state=[ 0.03328837  0.21756114  0.04944112 -0.26509465]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 22 ] state=[ 0.03328837  0.21756114  0.04944112 -0.26509465], action=1, reward=1.0, next_state=[ 0.03763959  0.41194383  0.04413923 -0.54178264]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 23 ] state=[ 0.03763959  0.41194383  0.04413923 -0.54178264], action=1, reward=1.0, next_state=[ 0.04587847  0.60641852  0.03330358 -0.82023782]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 24 ] state=[ 0.04587847  0.60641852  0.03330358 -0.82023782], action=0, reward=1.0, next_state=[ 0.05800684  0.41085702  0.01689882 -0.51726879]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 25 ] state=[ 0.05800684  0.41085702  0.01689882 -0.51726879], action=0, reward=1.0, next_state=[ 0.06622398  0.21550125  0.00655345 -0.21930896]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 26 ] state=[ 0.06622398  0.21550125  0.00655345 -0.21930896], action=0, reward=1.0, next_state=[0.07053401 0.02028624 0.00216727 0.07543397]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 27 ] state=[0.07053401 0.02028624 0.00216727 0.07543397], action=0, reward=1.0, next_state=[ 0.07093973 -0.17486672  0.00367595  0.36879989]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 28 ] state=[ 0.07093973 -0.17486672  0.00367595  0.36879989], action=1, reward=1.0, next_state=[0.0674424  0.02020281 0.01105194 0.0772783 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 29 ] state=[0.0674424  0.02020281 0.01105194 0.0772783 ], action=0, reward=1.0, next_state=[ 0.06784645 -0.17507582  0.01259751  0.37342756]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 30 ] state=[ 0.06784645 -0.17507582  0.01259751  0.37342756], action=0, reward=1.0, next_state=[ 0.06434494 -0.37037444  0.02006606  0.67005581]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 31 ] state=[ 0.06434494 -0.37037444  0.02006606  0.67005581], action=0, reward=1.0, next_state=[ 0.05693745 -0.56576954  0.03346718  0.96898849]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 32 ] state=[ 0.05693745 -0.56576954  0.03346718  0.96898849], action=1, reward=1.0, next_state=[ 0.04562206 -0.3711125   0.05284695  0.68700394]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 33 ] state=[ 0.04562206 -0.3711125   0.05284695  0.68700394], action=1, reward=1.0, next_state=[ 0.03819981 -0.17676239  0.06658703  0.41141555]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 34 ] state=[ 0.03819981 -0.17676239  0.06658703  0.41141555], action=0, reward=1.0, next_state=[ 0.03466456 -0.372762    0.07481534  0.72432555]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 35 ] state=[ 0.03466456 -0.372762    0.07481534  0.72432555], action=0, reward=1.0, next_state=[ 0.02720932 -0.5688345   0.08930185  1.03958677]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 36 ] state=[ 0.02720932 -0.5688345   0.08930185  1.03958677], action=0, reward=1.0, next_state=[ 0.01583263 -0.76502216  0.11009358  1.35891548]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 37 ] state=[ 0.01583263 -0.76502216  0.11009358  1.35891548], action=1, reward=1.0, next_state=[ 5.32185778e-04 -5.71439433e-01  1.37271894e-01  1.10260153e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 38 ] state=[ 5.32185778e-04 -5.71439433e-01  1.37271894e-01  1.10260153e+00], action=1, reward=1.0, next_state=[-0.0108966  -0.37836376  0.15932392  0.85594373]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 39 ] state=[-0.0108966  -0.37836376  0.15932392  0.85594373], action=0, reward=1.0, next_state=[-0.01846388 -0.57525616  0.1764428   1.19418511]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 40 ] state=[-0.01846388 -0.57525616  0.1764428   1.19418511], action=1, reward=1.0, next_state=[-0.029969   -0.38280213  0.2003265   0.96159147]\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ][ timestamp 41 ] state=[-0.029969   -0.38280213  0.2003265   0.96159147], action=1, reward=-1.0, next_state=[-0.03762504 -0.19085353  0.21955833  0.73792941]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 254: Exploration_rate=0.01. Score=41.\n",
      "[ episode 255 ] state=[ 0.03863275  0.00581702  0.0200389  -0.01618577]\n",
      "[ episode 255 ][ timestamp 1 ] state=[ 0.03863275  0.00581702  0.0200389  -0.01618577], action=1, reward=1.0, next_state=[ 0.03874909  0.20064594  0.01971518 -0.30247944]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 2 ] state=[ 0.03874909  0.20064594  0.01971518 -0.30247944], action=0, reward=1.0, next_state=[ 0.04276201  0.00524863  0.01366559 -0.00364455]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 3 ] state=[ 0.04276201  0.00524863  0.01366559 -0.00364455], action=0, reward=1.0, next_state=[ 0.04286698 -0.19006661  0.0135927   0.29331851]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 4 ] state=[ 0.04286698 -0.19006661  0.0135927   0.29331851], action=1, reward=1.0, next_state=[0.03906565 0.00485893 0.01945907 0.00495334]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 5 ] state=[0.03906565 0.00485893 0.01945907 0.00495334], action=1, reward=1.0, next_state=[ 0.03916283  0.19969649  0.01955814 -0.28152705]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 6 ] state=[ 0.03916283  0.19969649  0.01955814 -0.28152705], action=0, reward=1.0, next_state=[0.04315676 0.0043011  0.0139276  0.01725971]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 7 ] state=[0.04315676 0.0043011  0.0139276  0.01725971], action=1, reward=1.0, next_state=[ 0.04324278  0.19922057  0.01427279 -0.27099656]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 255 ][ timestamp 8 ] state=[ 0.04324278  0.19922057  0.01427279 -0.27099656], action=0, reward=1.0, next_state=[0.04722719 0.00389789 0.00885286 0.02615368]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 9 ] state=[0.04722719 0.00389789 0.00885286 0.02615368], action=1, reward=1.0, next_state=[ 0.04730515  0.19889177  0.00937593 -0.26372298]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 10 ] state=[ 0.04730515  0.19889177  0.00937593 -0.26372298], action=0, reward=1.0, next_state=[0.05128298 0.00363725 0.00410148 0.03190241]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 11 ] state=[0.05128298 0.00363725 0.00410148 0.03190241], action=1, reward=1.0, next_state=[ 0.05135573  0.19870015  0.00473952 -0.25948364]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 12 ] state=[ 0.05135573  0.19870015  0.00473952 -0.25948364], action=0, reward=1.0, next_state=[ 0.05532973  0.00351086 -0.00045015  0.03469042]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 13 ] state=[ 0.05532973  0.00351086 -0.00045015  0.03469042], action=1, reward=1.0, next_state=[ 5.53999489e-02  1.98639261e-01  2.43658676e-04 -2.58134502e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 14 ] state=[ 5.53999489e-02  1.98639261e-01  2.43658676e-04 -2.58134502e-01], action=0, reward=1.0, next_state=[ 0.05937273  0.00351383 -0.00491903  0.03462527]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 15 ] state=[ 0.05937273  0.00351383 -0.00491903  0.03462527], action=0, reward=1.0, next_state=[ 0.05944301 -0.19153724 -0.00422653  0.32575214]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 16 ] state=[ 0.05944301 -0.19153724 -0.00422653  0.32575214], action=1, reward=1.0, next_state=[0.05561227 0.00364464 0.00228852 0.03173935]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 17 ] state=[0.05561227 0.00364464 0.00228852 0.03173935], action=0, reward=1.0, next_state=[ 0.05568516 -0.19151006  0.0029233   0.32514345]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 18 ] state=[ 0.05568516 -0.19151006  0.0029233   0.32514345], action=1, reward=1.0, next_state=[0.05185496 0.00357015 0.00942617 0.03338384]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 19 ] state=[0.05185496 0.00357015 0.00942617 0.03338384], action=1, reward=1.0, next_state=[ 0.05192636  0.19855567  0.01009385 -0.25631019]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 20 ] state=[ 0.05192636  0.19855567  0.01009385 -0.25631019], action=0, reward=1.0, next_state=[0.05589747 0.00329107 0.00496765 0.03953933]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 21 ] state=[0.05589747 0.00329107 0.00496765 0.03953933], action=1, reward=1.0, next_state=[ 0.0559633   0.19834143  0.00575843 -0.25157212]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 22 ] state=[ 0.0559633   0.19834143  0.00575843 -0.25157212], action=1, reward=1.0, next_state=[ 0.05993012  0.39338068  0.00072699 -0.54243318]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 23 ] state=[ 0.05993012  0.39338068  0.00072699 -0.54243318], action=0, reward=1.0, next_state=[ 0.06779774  0.19824852 -0.01012167 -0.24952128]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 24 ] state=[ 0.06779774  0.19824852 -0.01012167 -0.24952128], action=1, reward=1.0, next_state=[ 0.07176271  0.39351354 -0.0151121  -0.54537953]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 25 ] state=[ 0.07176271  0.39351354 -0.0151121  -0.54537953], action=1, reward=1.0, next_state=[ 0.07963298  0.58884454 -0.02601969 -0.84278536]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 26 ] state=[ 0.07963298  0.58884454 -0.02601969 -0.84278536], action=0, reward=1.0, next_state=[ 0.09140987  0.3940872  -0.0428754  -0.55839716]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 27 ] state=[ 0.09140987  0.3940872  -0.0428754  -0.55839716], action=1, reward=1.0, next_state=[ 0.09929161  0.58978395 -0.05404334 -0.86427402]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 28 ] state=[ 0.09929161  0.58978395 -0.05404334 -0.86427402], action=0, reward=1.0, next_state=[ 0.11108729  0.39543768 -0.07132882 -0.58906123]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 29 ] state=[ 0.11108729  0.39543768 -0.07132882 -0.58906123], action=0, reward=1.0, next_state=[ 0.11899605  0.20138321 -0.08311005 -0.3196726 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 30 ] state=[ 0.11899605  0.20138321 -0.08311005 -0.3196726 ], action=0, reward=1.0, next_state=[ 0.12302371  0.00753717 -0.0895035  -0.0543134 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 31 ] state=[ 0.12302371  0.00753717 -0.0895035  -0.0543134 ], action=1, reward=1.0, next_state=[ 0.12317445  0.20382088 -0.09058977 -0.37383937]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 32 ] state=[ 0.12317445  0.20382088 -0.09058977 -0.37383937], action=0, reward=1.0, next_state=[ 0.12725087  0.0100947  -0.09806655 -0.11103862]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 33 ] state=[ 0.12725087  0.0100947  -0.09806655 -0.11103862], action=0, reward=1.0, next_state=[ 0.12745277 -0.18349522 -0.10028733  0.14916568]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 34 ] state=[ 0.12745277 -0.18349522 -0.10028733  0.14916568], action=0, reward=1.0, next_state=[ 0.12378286 -0.37704874 -0.09730401  0.4086021 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 35 ] state=[ 0.12378286 -0.37704874 -0.09730401  0.4086021 ], action=0, reward=1.0, next_state=[ 0.11624189 -0.57066611 -0.08913197  0.6690921 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 36 ] state=[ 0.11624189 -0.57066611 -0.08913197  0.6690921 ], action=1, reward=1.0, next_state=[ 0.10482856 -0.37442529 -0.07575013  0.34972926]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 37 ] state=[ 0.10482856 -0.37442529 -0.07575013  0.34972926], action=1, reward=1.0, next_state=[ 0.09734006 -0.17831233 -0.06875554  0.03415416]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 38 ] state=[ 0.09734006 -0.17831233 -0.06875554  0.03415416], action=1, reward=1.0, next_state=[ 0.09377381  0.01772479 -0.06807246 -0.27940495]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 39 ] state=[ 0.09377381  0.01772479 -0.06807246 -0.27940495], action=0, reward=1.0, next_state=[ 0.09412831 -0.17636339 -0.07366056 -0.00894481]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 40 ] state=[ 0.09412831 -0.17636339 -0.07366056 -0.00894481], action=0, reward=1.0, next_state=[ 0.09060104 -0.37035589 -0.07383945  0.25961823]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 41 ] state=[ 0.09060104 -0.37035589 -0.07383945  0.25961823], action=0, reward=1.0, next_state=[ 0.08319392 -0.56435032 -0.06864709  0.52812788]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 42 ] state=[ 0.08319392 -0.56435032 -0.06864709  0.52812788], action=1, reward=1.0, next_state=[ 0.07190692 -0.3683331  -0.05808453  0.21462817]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 43 ] state=[ 0.07190692 -0.3683331  -0.05808453  0.21462817], action=0, reward=1.0, next_state=[ 0.06454025 -0.56257862 -0.05379197  0.48843783]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 44 ] state=[ 0.06454025 -0.56257862 -0.05379197  0.48843783], action=0, reward=1.0, next_state=[ 0.05328868 -0.75690201 -0.04402321  0.76369409]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 45 ] state=[ 0.05328868 -0.75690201 -0.04402321  0.76369409], action=0, reward=1.0, next_state=[ 0.03815064 -0.9513909  -0.02874933  1.04220613]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 46 ] state=[ 0.03815064 -0.9513909  -0.02874933  1.04220613], action=1, reward=1.0, next_state=[ 0.01912282 -0.75589915 -0.00790521  0.74063854]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 47 ] state=[ 0.01912282 -0.75589915 -0.00790521  0.74063854], action=1, reward=1.0, next_state=[ 0.00400484 -0.56066896  0.00690756  0.44547829]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 48 ] state=[ 0.00400484 -0.56066896  0.00690756  0.44547829], action=0, reward=1.0, next_state=[-0.00720854 -0.75588795  0.01581713  0.7403306 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 255 ][ timestamp 49 ] state=[-0.00720854 -0.75588795  0.01581713  0.7403306 ], action=1, reward=1.0, next_state=[-0.0223263  -0.56098791  0.03062374  0.45266715]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 50 ] state=[-0.0223263  -0.56098791  0.03062374  0.45266715], action=0, reward=1.0, next_state=[-0.03354606 -0.75652924  0.03967708  0.7548436 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 51 ] state=[-0.03354606 -0.75652924  0.03967708  0.7548436 ], action=1, reward=1.0, next_state=[-0.04867664 -0.56197607  0.05477396  0.47490552]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 52 ] state=[-0.04867664 -0.56197607  0.05477396  0.47490552], action=1, reward=1.0, next_state=[-0.05991616 -0.36766865  0.06427207  0.199977  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 53 ] state=[-0.05991616 -0.36766865  0.06427207  0.199977  ], action=1, reward=1.0, next_state=[-0.06726954 -0.17352202  0.06827161 -0.07175868]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 54 ] state=[-0.06726954 -0.17352202  0.06827161 -0.07175868], action=1, reward=1.0, next_state=[-0.07073998  0.02055811  0.06683643 -0.34214442]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 55 ] state=[-0.07073998  0.02055811  0.06683643 -0.34214442], action=1, reward=1.0, next_state=[-0.07032881  0.21466866  0.05999354 -0.61302486]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 56 ] state=[-0.07032881  0.21466866  0.05999354 -0.61302486], action=0, reward=1.0, next_state=[-0.06603544  0.01876189  0.04773305 -0.30206586]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 57 ] state=[-0.06603544  0.01876189  0.04773305 -0.30206586], action=1, reward=1.0, next_state=[-0.0656602   0.21317218  0.04169173 -0.57932096]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 58 ] state=[-0.0656602   0.21317218  0.04169173 -0.57932096], action=1, reward=1.0, next_state=[-0.06139676  0.40768582  0.03010531 -0.85858407]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 59 ] state=[-0.06139676  0.40768582  0.03010531 -0.85858407], action=0, reward=1.0, next_state=[-0.05324304  0.21216699  0.01293363 -0.55658908]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 60 ] state=[-0.05324304  0.21216699  0.01293363 -0.55658908], action=0, reward=1.0, next_state=[-0.0489997   0.01686587  0.00180185 -0.25985953]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 61 ] state=[-0.0489997   0.01686587  0.00180185 -0.25985953], action=0, reward=1.0, next_state=[-0.04866239 -0.17828176 -0.00339534  0.03339118]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 62 ] state=[-0.04866239 -0.17828176 -0.00339534  0.03339118], action=1, reward=1.0, next_state=[-0.05222802  0.01688872 -0.00272752 -0.26036108]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 63 ] state=[-0.05222802  0.01688872 -0.00272752 -0.26036108], action=0, reward=1.0, next_state=[-0.05189025 -0.17819419 -0.00793474  0.03146031]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 64 ] state=[-0.05189025 -0.17819419 -0.00793474  0.03146031], action=0, reward=1.0, next_state=[-0.05545413 -0.37320146 -0.00730553  0.32162922]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 65 ] state=[-0.05545413 -0.37320146 -0.00730553  0.32162922], action=1, reward=1.0, next_state=[-0.06291816 -0.17797624 -0.00087295  0.02665139]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 66 ] state=[-0.06291816 -0.17797624 -0.00087295  0.02665139], action=1, reward=1.0, next_state=[-0.06647768  0.01715822 -0.00033992 -0.26630683]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 67 ] state=[-0.06647768  0.01715822 -0.00033992 -0.26630683], action=1, reward=1.0, next_state=[-0.06613452  0.21228502 -0.00566606 -0.55909695]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 68 ] state=[-0.06613452  0.21228502 -0.00566606 -0.55909695], action=0, reward=1.0, next_state=[-0.06188882  0.01724306 -0.016848   -0.26820452]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 69 ] state=[-0.06188882  0.01724306 -0.016848   -0.26820452], action=0, reward=1.0, next_state=[-0.06154396 -0.17763445 -0.02221209  0.01911718]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 70 ] state=[-0.06154396 -0.17763445 -0.02221209  0.01911718], action=1, reward=1.0, next_state=[-0.06509665  0.01779889 -0.02182975 -0.28049033]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 71 ] state=[-0.06509665  0.01779889 -0.02182975 -0.28049033], action=0, reward=1.0, next_state=[-0.06474067 -0.17700497 -0.02743955  0.00522841]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 72 ] state=[-0.06474067 -0.17700497 -0.02743955  0.00522841], action=0, reward=1.0, next_state=[-0.06828077 -0.37172288 -0.02733498  0.28912909]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 73 ] state=[-0.06828077 -0.37172288 -0.02733498  0.28912909], action=0, reward=1.0, next_state=[-0.07571523 -0.56644458 -0.0215524   0.57306706]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 74 ] state=[-0.07571523 -0.56644458 -0.0215524   0.57306706], action=0, reward=1.0, next_state=[-0.08704412 -0.76125782 -0.01009106  0.85888313]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 75 ] state=[-0.08704412 -0.76125782 -0.01009106  0.85888313], action=0, reward=1.0, next_state=[-0.10226927 -0.95624087  0.0070866   1.1483761 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 76 ] state=[-0.10226927 -0.95624087  0.0070866   1.1483761 ], action=1, reward=1.0, next_state=[-0.12139409 -0.76121215  0.03005412  0.8579238 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 77 ] state=[-0.12139409 -0.76121215  0.03005412  0.8579238 ], action=0, reward=1.0, next_state=[-0.13661833 -0.95673036  0.0472126   1.15990326]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 78 ] state=[-0.13661833 -0.95673036  0.0472126   1.15990326], action=1, reward=1.0, next_state=[-0.15575294 -0.76225425  0.07041067  0.88238951]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 79 ] state=[-0.15575294 -0.76225425  0.07041067  0.88238951], action=1, reward=1.0, next_state=[-0.17099803 -0.56815561  0.08805846  0.61264659]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 80 ] state=[-0.17099803 -0.56815561  0.08805846  0.61264659], action=1, reward=1.0, next_state=[-0.18236114 -0.3743675   0.10031139  0.34894644]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 81 ] state=[-0.18236114 -0.3743675   0.10031139  0.34894644], action=1, reward=1.0, next_state=[-0.18984849 -0.18080467  0.10729032  0.08950386]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 82 ] state=[-0.18984849 -0.18080467  0.10729032  0.08950386], action=1, reward=1.0, next_state=[-0.19346458  0.0126289   0.10908039 -0.16749523]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 83 ] state=[-0.19346458  0.0126289   0.10908039 -0.16749523], action=1, reward=1.0, next_state=[-0.193212    0.20603412  0.10573049 -0.42387276]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 84 ] state=[-0.193212    0.20603412  0.10573049 -0.42387276], action=1, reward=1.0, next_state=[-0.18909132  0.39951185  0.09725303 -0.68144183]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 85 ] state=[-0.18909132  0.39951185  0.09725303 -0.68144183], action=1, reward=1.0, next_state=[-0.18110109  0.59315822  0.0836242  -0.94199148]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 86 ] state=[-0.18110109  0.59315822  0.0836242  -0.94199148], action=1, reward=1.0, next_state=[-0.16923792  0.78705979  0.06478437 -1.20727058]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 87 ] state=[-0.16923792  0.78705979  0.06478437 -1.20727058], action=1, reward=1.0, next_state=[-0.15349673  0.98128773  0.04063896 -1.47896804]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 88 ] state=[-0.15349673  0.98128773  0.04063896 -1.47896804], action=0, reward=1.0, next_state=[-0.13387097  0.78569392  0.01105959 -1.173875  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 89 ] state=[-0.13387097  0.78569392  0.01105959 -1.173875  ], action=1, reward=1.0, next_state=[-0.11815709  0.98067039 -0.01241791 -1.46307037]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 90 ] state=[-0.11815709  0.98067039 -0.01241791 -1.46307037], action=0, reward=1.0, next_state=[-0.09854368  0.78570278 -0.04167931 -1.17429227]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 255 ][ timestamp 91 ] state=[-0.09854368  0.78570278 -0.04167931 -1.17429227], action=1, reward=1.0, next_state=[-0.08282963  0.98134087 -0.06516516 -1.47974473]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 92 ] state=[-0.08282963  0.98134087 -0.06516516 -1.47974473], action=0, reward=1.0, next_state=[-0.06320281  0.78707201 -0.09476005 -1.20810493]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 93 ] state=[-0.06320281  0.78707201 -0.09476005 -1.20810493], action=1, reward=1.0, next_state=[-0.04746137  0.98328158 -0.11892215 -1.52891666]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 94 ] state=[-0.04746137  0.98328158 -0.11892215 -1.52891666], action=1, reward=1.0, next_state=[-0.02779574  1.17962022 -0.14950048 -1.85622531]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 95 ] state=[-0.02779574  1.17962022 -0.14950048 -1.85622531], action=1, reward=1.0, next_state=[-0.00420334  1.37603492 -0.18662499 -2.19135061]\n",
      "[ Experience replay ] starts\n",
      "[ episode 255 ][ timestamp 96 ] state=[-0.00420334  1.37603492 -0.18662499 -2.19135061], action=0, reward=-1.0, next_state=[ 0.02331736  1.18314437 -0.230452   -1.9615886 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 255: Exploration_rate=0.01. Score=96.\n",
      "[ episode 256 ] state=[ 0.00640603 -0.0373547  -0.04859114  0.0163435 ]\n",
      "[ episode 256 ][ timestamp 1 ] state=[ 0.00640603 -0.0373547  -0.04859114  0.0163435 ], action=1, reward=1.0, next_state=[ 0.00565894  0.15842921 -0.04826427 -0.29126592]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 2 ] state=[ 0.00565894  0.15842921 -0.04826427 -0.29126592], action=0, reward=1.0, next_state=[ 0.00882752 -0.03597252 -0.05408959 -0.01418708]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 3 ] state=[ 0.00882752 -0.03597252 -0.05408959 -0.01418708], action=0, reward=1.0, next_state=[ 0.00810807 -0.23027871 -0.05437333  0.26095137]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 4 ] state=[ 0.00810807 -0.23027871 -0.05437333  0.26095137], action=1, reward=1.0, next_state=[ 0.0035025  -0.03442444 -0.0491543  -0.04837375]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 5 ] state=[ 0.0035025  -0.03442444 -0.0491543  -0.04837375], action=0, reward=1.0, next_state=[ 0.00281401 -0.22880836 -0.05012178  0.22840441]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 6 ] state=[ 0.00281401 -0.22880836 -0.05012178  0.22840441], action=1, reward=1.0, next_state=[-0.00176216 -0.0330073  -0.04555369 -0.07965797]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 7 ] state=[-0.00176216 -0.0330073  -0.04555369 -0.07965797], action=0, reward=1.0, next_state=[-0.0024223  -0.22744763 -0.04714685  0.1983118 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 8 ] state=[-0.0024223  -0.22744763 -0.04714685  0.1983118 ], action=1, reward=1.0, next_state=[-0.00697125 -0.03168415 -0.04318061 -0.10886315]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 9 ] state=[-0.00697125 -0.03168415 -0.04318061 -0.10886315], action=1, reward=1.0, next_state=[-0.00760494  0.16402912 -0.04535787 -0.41485057]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 10 ] state=[-0.00760494  0.16402912 -0.04535787 -0.41485057], action=0, reward=1.0, next_state=[-0.00432436 -0.03042158 -0.05365489 -0.13680514]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 11 ] state=[-0.00432436 -0.03042158 -0.05365489 -0.13680514], action=0, reward=1.0, next_state=[-0.00493279 -0.22473561 -0.05639099  0.13847948]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 12 ] state=[-0.00493279 -0.22473561 -0.05639099  0.13847948], action=0, reward=1.0, next_state=[-0.0094275  -0.41900644 -0.0536214   0.41285236]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 13 ] state=[-0.0094275  -0.41900644 -0.0536214   0.41285236], action=1, reward=1.0, next_state=[-0.01780763 -0.22316703 -0.04536435  0.10375832]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 14 ] state=[-0.01780763 -0.22316703 -0.04536435  0.10375832], action=0, reward=1.0, next_state=[-0.02227097 -0.4176105  -0.04328919  0.38179092]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 15 ] state=[-0.02227097 -0.4176105  -0.04328919  0.38179092], action=1, reward=1.0, next_state=[-0.03062318 -0.22190147 -0.03565337  0.07577934]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 16 ] state=[-0.03062318 -0.22190147 -0.03565337  0.07577934], action=1, reward=1.0, next_state=[-0.03506121 -0.02628701 -0.03413778 -0.22793574]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 17 ] state=[-0.03506121 -0.02628701 -0.03413778 -0.22793574], action=0, reward=1.0, next_state=[-0.03558695 -0.2209049  -0.0386965   0.05378645]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 18 ] state=[-0.03558695 -0.2209049  -0.0386965   0.05378645], action=0, reward=1.0, next_state=[-0.04000505 -0.41545124 -0.03762077  0.33401357]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 19 ] state=[-0.04000505 -0.41545124 -0.03762077  0.33401357], action=1, reward=1.0, next_state=[-0.04831407 -0.21981461 -0.03094049  0.02970837]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 20 ] state=[-0.04831407 -0.21981461 -0.03094049  0.02970837], action=0, reward=1.0, next_state=[-0.05271036 -0.41447951 -0.03034633  0.31247091]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 21 ] state=[-0.05271036 -0.41447951 -0.03034633  0.31247091], action=1, reward=1.0, next_state=[-0.06099995 -0.21893867 -0.02409691  0.01037425]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 22 ] state=[-0.06099995 -0.21893867 -0.02409691  0.01037425], action=1, reward=1.0, next_state=[-0.06537873 -0.02347958 -0.02388942 -0.28981308]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 23 ] state=[-0.06537873 -0.02347958 -0.02388942 -0.28981308], action=0, reward=1.0, next_state=[-0.06584832 -0.21825288 -0.02968569 -0.00475932]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 24 ] state=[-0.06584832 -0.21825288 -0.02968569 -0.00475932], action=0, reward=1.0, next_state=[-0.07021338 -0.41293679 -0.02978087  0.27841158]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 25 ] state=[-0.07021338 -0.41293679 -0.02978087  0.27841158], action=0, reward=1.0, next_state=[-0.07847211 -0.60762152 -0.02421264  0.56155489]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 26 ] state=[-0.07847211 -0.60762152 -0.02421264  0.56155489], action=0, reward=1.0, next_state=[-0.09062454 -0.80239545 -0.01298154  0.84651234]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 27 ] state=[-0.09062454 -0.80239545 -0.01298154  0.84651234], action=1, reward=1.0, next_state=[-0.10667245 -0.60709883  0.0039487   0.54977563]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 28 ] state=[-0.10667245 -0.60709883  0.0039487   0.54977563], action=0, reward=1.0, next_state=[-0.11881443 -0.80227602  0.01494422  0.84370005]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 29 ] state=[-0.11881443 -0.80227602  0.01494422  0.84370005], action=0, reward=1.0, next_state=[-0.13485995 -0.9975987   0.03181822  1.14104478]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 30 ] state=[-0.13485995 -0.9975987   0.03181822  1.14104478], action=1, reward=1.0, next_state=[-0.15481192 -0.80290677  0.05463911  0.85850768]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 31 ] state=[-0.15481192 -0.80290677  0.05463911  0.85850768], action=0, reward=1.0, next_state=[-0.17087006 -0.99872876  0.07180927  1.16785822]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 32 ] state=[-0.17087006 -0.99872876  0.07180927  1.16785822], action=0, reward=1.0, next_state=[-0.19084463 -1.19470774  0.09516643  1.48216286]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 33 ] state=[-0.19084463 -1.19470774  0.09516643  1.48216286], action=1, reward=1.0, next_state=[-0.21473879 -1.0008668   0.12480969  1.22065384]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 34 ] state=[-0.21473879 -1.0008668   0.12480969  1.22065384], action=1, reward=1.0, next_state=[-0.23475612 -0.80755451  0.14922277  0.96953982]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 35 ] state=[-0.23475612 -0.80755451  0.14922277  0.96953982], action=0, reward=1.0, next_state=[-0.25090721 -1.00433032  0.16861356  1.30513222]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 36 ] state=[-0.25090721 -1.00433032  0.16861356  1.30513222], action=1, reward=1.0, next_state=[-0.27099382 -0.81169903  0.19471621  1.06962084]\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ][ timestamp 37 ] state=[-0.27099382 -0.81169903  0.19471621  1.06962084], action=1, reward=-1.0, next_state=[-0.2872278  -0.61960977  0.21610862  0.84381744]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 256: Exploration_rate=0.01. Score=37.\n",
      "[ episode 257 ] state=[-0.03337512  0.02041422 -0.01963308  0.02161838]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 257 ][ timestamp 1 ] state=[-0.03337512  0.02041422 -0.01963308  0.02161838], action=0, reward=1.0, next_state=[-0.03296683 -0.17442075 -0.01920071  0.30804276]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 2 ] state=[-0.03296683 -0.17442075 -0.01920071  0.30804276], action=0, reward=1.0, next_state=[-0.03645525 -0.36926393 -0.01303986  0.59460898]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 3 ] state=[-0.03645525 -0.36926393 -0.01303986  0.59460898], action=1, reward=1.0, next_state=[-0.04384053 -0.17396192 -0.00114768  0.29784726]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 4 ] state=[-0.04384053 -0.17396192 -0.00114768  0.29784726], action=1, reward=1.0, next_state=[-0.04731977  0.02117638  0.00480927  0.0048026 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 5 ] state=[-0.04731977  0.02117638  0.00480927  0.0048026 ], action=0, reward=1.0, next_state=[-0.04689624 -0.17401421  0.00490532  0.29899902]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 6 ] state=[-0.04689624 -0.17401421  0.00490532  0.29899902], action=1, reward=1.0, next_state=[-0.05037652  0.02103747  0.0108853   0.00786717]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 7 ] state=[-0.05037652  0.02103747  0.0108853   0.00786717], action=0, reward=1.0, next_state=[-0.04995577 -0.17423888  0.01104264  0.30396456]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 8 ] state=[-0.04995577 -0.17423888  0.01104264  0.30396456], action=0, reward=1.0, next_state=[-0.05344055 -0.36951645  0.01712194  0.60010953]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 9 ] state=[-0.05344055 -0.36951645  0.01712194  0.60010953], action=0, reward=1.0, next_state=[-0.06083088 -0.5648737   0.02912413  0.89813606]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 10 ] state=[-0.06083088 -0.5648737   0.02912413  0.89813606], action=1, reward=1.0, next_state=[-0.07212835 -0.37015836  0.04708685  0.61474819]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 11 ] state=[-0.07212835 -0.37015836  0.04708685  0.61474819], action=0, reward=1.0, next_state=[-0.07953152 -0.56590552  0.05938181  0.92188191]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 12 ] state=[-0.07953152 -0.56590552  0.05938181  0.92188191], action=1, reward=1.0, next_state=[-0.09084963 -0.37163405  0.07781945  0.64843632]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 13 ] state=[-0.09084963 -0.37163405  0.07781945  0.64843632], action=1, reward=1.0, next_state=[-0.09828231 -0.17767753  0.09078818  0.38123785]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 14 ] state=[-0.09828231 -0.17767753  0.09078818  0.38123785], action=1, reward=1.0, next_state=[-0.10183586  0.01604587  0.09841293  0.11850458]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 15 ] state=[-0.10183586  0.01604587  0.09841293  0.11850458], action=1, reward=1.0, next_state=[-0.10151495  0.20963014  0.10078302 -0.14158007]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 16 ] state=[-0.10151495  0.20963014  0.10078302 -0.14158007], action=1, reward=1.0, next_state=[-0.09732234  0.40317509  0.09795142 -0.40084426]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 17 ] state=[-0.09732234  0.40317509  0.09795142 -0.40084426], action=0, reward=1.0, next_state=[-0.08925884  0.20681008  0.08993454 -0.07895695]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 18 ] state=[-0.08925884  0.20681008  0.08993454 -0.07895695], action=1, reward=1.0, next_state=[-0.08512264  0.40053539  0.0883554  -0.34196541]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 19 ] state=[-0.08512264  0.40053539  0.0883554  -0.34196541], action=0, reward=1.0, next_state=[-0.07711193  0.20427476  0.08151609 -0.02278012]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 20 ] state=[-0.07711193  0.20427476  0.08151609 -0.02278012], action=1, reward=1.0, next_state=[-0.07302644  0.39813881  0.08106049 -0.28867139]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 21 ] state=[-0.07302644  0.39813881  0.08106049 -0.28867139], action=1, reward=1.0, next_state=[-0.06506366  0.5920169   0.07528706 -0.55472789]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 22 ] state=[-0.06506366  0.5920169   0.07528706 -0.55472789], action=0, reward=1.0, next_state=[-0.05322332  0.39592305  0.0641925  -0.23930685]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 23 ] state=[-0.05322332  0.39592305  0.0641925  -0.23930685], action=1, reward=1.0, next_state=[-0.04530486  0.59007204  0.05940637 -0.51107089]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 24 ] state=[-0.04530486  0.59007204  0.05940637 -0.51107089], action=1, reward=1.0, next_state=[-0.03350342  0.78430905  0.04918495 -0.78445724]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 25 ] state=[-0.03350342  0.78430905  0.04918495 -0.78445724], action=0, reward=1.0, next_state=[-0.01781724  0.58854699  0.0334958  -0.47671472]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 26 ] state=[-0.01781724  0.58854699  0.0334958  -0.47671472], action=1, reward=1.0, next_state=[-0.0060463   0.78318038  0.02396151 -0.75865511]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 27 ] state=[-0.0060463   0.78318038  0.02396151 -0.75865511], action=0, reward=1.0, next_state=[ 0.00961731  0.58773658  0.00878841 -0.45852956]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 28 ] state=[ 0.00961731  0.58773658  0.00878841 -0.45852956], action=1, reward=1.0, next_state=[ 2.13720394e-02  7.82733198e-01 -3.82185047e-04 -7.48429434e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 29 ] state=[ 2.13720394e-02  7.82733198e-01 -3.82185047e-04 -7.48429434e-01], action=0, reward=1.0, next_state=[ 0.0370267   0.58761652 -0.01535077 -0.4558668 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 30 ] state=[ 0.0370267   0.58761652 -0.01535077 -0.4558668 ], action=0, reward=1.0, next_state=[ 0.04877903  0.39271494 -0.02446811 -0.16806182]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 31 ] state=[ 0.04877903  0.39271494 -0.02446811 -0.16806182], action=0, reward=1.0, next_state=[ 0.05663333  0.19795161 -0.02782935  0.11680282]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 32 ] state=[ 0.05663333  0.19795161 -0.02782935  0.11680282], action=1, reward=1.0, next_state=[ 0.06059236  0.39346102 -0.02549329 -0.18452852]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 33 ] state=[ 0.06059236  0.39346102 -0.02549329 -0.18452852], action=0, reward=1.0, next_state=[ 0.06846159  0.19871293 -0.02918386  0.10000447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 34 ] state=[ 0.06846159  0.19871293 -0.02918386  0.10000447], action=0, reward=1.0, next_state=[ 0.07243584  0.00402113 -0.02718377  0.38333899]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 35 ] state=[ 0.07243584  0.00402113 -0.02718377  0.38333899], action=0, reward=1.0, next_state=[ 0.07251627 -0.19070453 -0.01951699  0.66732852]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 36 ] state=[ 0.07251627 -0.19070453 -0.01951699  0.66732852], action=1, reward=1.0, next_state=[ 0.06870218  0.00468333 -0.00617042  0.36856492]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 37 ] state=[ 0.06870218  0.00468333 -0.00617042  0.36856492], action=0, reward=1.0, next_state=[ 0.06879584 -0.19035041  0.00120088  0.65929586]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 38 ] state=[ 0.06879584 -0.19035041  0.00120088  0.65929586], action=1, reward=1.0, next_state=[0.06498883 0.00475481 0.01438679 0.3669913 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 39 ] state=[0.06498883 0.00475481 0.01438679 0.3669913 ], action=1, reward=1.0, next_state=[0.06508393 0.1996694  0.02172662 0.07887924]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 40 ] state=[0.06508393 0.1996694  0.02172662 0.07887924], action=1, reward=1.0, next_state=[ 0.06907732  0.39447327  0.02330421 -0.20687046]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 41 ] state=[ 0.06907732  0.39447327  0.02330421 -0.20687046], action=1, reward=1.0, next_state=[ 0.07696678  0.58925436  0.0191668  -0.49211194]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 42 ] state=[ 0.07696678  0.58925436  0.0191668  -0.49211194], action=0, reward=1.0, next_state=[ 0.08875187  0.39386737  0.00932456 -0.19345061]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 257 ][ timestamp 43 ] state=[ 0.08875187  0.39386737  0.00932456 -0.19345061], action=1, reward=1.0, next_state=[ 0.09662922  0.5888547   0.00545555 -0.48317751]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 44 ] state=[ 0.09662922  0.5888547   0.00545555 -0.48317751], action=0, reward=1.0, next_state=[ 0.10840631  0.39365618 -0.004208   -0.18878016]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 45 ] state=[ 0.10840631  0.39365618 -0.004208   -0.18878016], action=1, reward=1.0, next_state=[ 0.11627944  0.58883808 -0.00798361 -0.48278757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 46 ] state=[ 0.11627944  0.58883808 -0.00798361 -0.48278757], action=0, reward=1.0, next_state=[ 0.1280562   0.39382971 -0.01763936 -0.1926315 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 47 ] state=[ 0.1280562   0.39382971 -0.01763936 -0.1926315 ], action=1, reward=1.0, next_state=[ 0.13593279  0.5891995  -0.02149199 -0.49082629]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 48 ] state=[ 0.13593279  0.5891995  -0.02149199 -0.49082629], action=0, reward=1.0, next_state=[ 0.14771678  0.39438721 -0.03130851 -0.2049935 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 49 ] state=[ 0.14771678  0.39438721 -0.03130851 -0.2049935 ], action=1, reward=1.0, next_state=[ 0.15560453  0.58994258 -0.03540838 -0.50738599]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 50 ] state=[ 0.15560453  0.58994258 -0.03540838 -0.50738599], action=0, reward=1.0, next_state=[ 0.16740338  0.39533696 -0.0455561  -0.22606843]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 51 ] state=[ 0.16740338  0.39533696 -0.0455561  -0.22606843], action=0, reward=1.0, next_state=[ 0.17531012  0.20089469 -0.05007747  0.05190352]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 52 ] state=[ 0.17531012  0.20089469 -0.05007747  0.05190352], action=1, reward=1.0, next_state=[ 0.17932801  0.39669759 -0.0490394  -0.25614927]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 53 ] state=[ 0.17932801  0.39669759 -0.0490394  -0.25614927], action=0, reward=1.0, next_state=[ 0.18726196  0.20230885 -0.05416239  0.0206715 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 54 ] state=[ 0.18726196  0.20230885 -0.05416239  0.0206715 ], action=0, reward=1.0, next_state=[ 0.19130814  0.00800379 -0.05374896  0.29578572]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 55 ] state=[ 0.19130814  0.00800379 -0.05374896  0.29578572], action=0, reward=1.0, next_state=[ 0.19146822 -0.18631235 -0.04783324  0.57104443]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 56 ] state=[ 0.19146822 -0.18631235 -0.04783324  0.57104443], action=0, reward=1.0, next_state=[ 0.18774197 -0.38073204 -0.03641236  0.84828278]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 57 ] state=[ 0.18774197 -0.38073204 -0.03641236  0.84828278], action=1, reward=1.0, next_state=[ 0.18012733 -0.18513287 -0.0194467   0.54437564]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 58 ] state=[ 0.18012733 -0.18513287 -0.0194467   0.54437564], action=1, reward=1.0, next_state=[ 0.17642467  0.01025688 -0.00855919  0.24562947]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 59 ] state=[ 0.17642467  0.01025688 -0.00855919  0.24562947], action=1, reward=1.0, next_state=[ 0.17662981  0.20550003 -0.0036466  -0.0497409 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 60 ] state=[ 0.17662981  0.20550003 -0.0036466  -0.0497409 ], action=0, reward=1.0, next_state=[ 0.18073981  0.01043055 -0.00464142  0.24178927]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 61 ] state=[ 0.18073981  0.01043055 -0.00464142  0.24178927], action=0, reward=1.0, next_state=[ 1.80948420e-01 -1.84624792e-01  1.94369797e-04  5.33004566e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 62 ] state=[ 1.80948420e-01 -1.84624792e-01  1.94369797e-04  5.33004566e-01], action=0, reward=1.0, next_state=[ 0.17725592 -0.37974948  0.01085446  0.82574873]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 63 ] state=[ 0.17725592 -0.37974948  0.01085446  0.82574873], action=1, reward=1.0, next_state=[ 0.16966093 -0.18477764  0.02736944  0.53649936]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 64 ] state=[ 0.16966093 -0.18477764  0.02736944  0.53649936], action=0, reward=1.0, next_state=[ 0.16596538 -0.38027352  0.03809942  0.83767895]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 65 ] state=[ 0.16596538 -0.38027352  0.03809942  0.83767895], action=0, reward=1.0, next_state=[ 0.15835991 -0.5758945   0.054853    1.142096  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 66 ] state=[ 0.15835991 -0.5758945   0.054853    1.142096  ], action=1, reward=1.0, next_state=[ 0.14684202 -0.38153062  0.07769492  0.86710738]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 67 ] state=[ 0.14684202 -0.38153062  0.07769492  0.86710738], action=0, reward=1.0, next_state=[ 0.13921141 -0.57761898  0.09503707  1.18317194]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 68 ] state=[ 0.13921141 -0.57761898  0.09503707  1.18317194], action=0, reward=1.0, next_state=[ 0.12765903 -0.77383682  0.11870051  1.50406937]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 69 ] state=[ 0.12765903 -0.77383682  0.11870051  1.50406937], action=0, reward=1.0, next_state=[ 0.11218229 -0.97018233  0.1487819   1.83133127]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 70 ] state=[ 0.11218229 -0.97018233  0.1487819   1.83133127], action=1, reward=1.0, next_state=[ 0.09277865 -0.77698838  0.18540852  1.58832252]\n",
      "[ Experience replay ] starts\n",
      "[ episode 257 ][ timestamp 71 ] state=[ 0.09277865 -0.77698838  0.18540852  1.58832252], action=0, reward=-1.0, next_state=[ 0.07723888 -0.9737658   0.21717497  1.93262813]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 257: Exploration_rate=0.01. Score=71.\n",
      "[ episode 258 ] state=[0.0225866  0.04703465 0.02325517 0.03599301]\n",
      "[ episode 258 ][ timestamp 1 ] state=[0.0225866  0.04703465 0.02325517 0.03599301], action=0, reward=1.0, next_state=[ 0.02352729 -0.14841294  0.02397503  0.33592152]\n",
      "[ Experience replay ] starts\n",
      "[ episode 258 ][ timestamp 2 ] state=[ 0.02352729 -0.14841294  0.02397503  0.33592152], action=0, reward=1.0, next_state=[ 0.02055903 -0.34386773  0.03069346  0.63606744]\n",
      "[ Experience replay ] starts\n",
      "[ episode 258 ][ timestamp 3 ] state=[ 0.02055903 -0.34386773  0.03069346  0.63606744], action=0, reward=1.0, next_state=[ 0.01368168 -0.53940401  0.04341481  0.93825617]\n",
      "[ Experience replay ] starts\n",
      "[ episode 258 ][ timestamp 4 ] state=[ 0.01368168 -0.53940401  0.04341481  0.93825617], action=0, reward=1.0, next_state=[ 0.00289359 -0.73508357  0.06217993  1.24425887]\n",
      "[ Experience replay ] starts\n",
      "[ episode 258 ][ timestamp 5 ] state=[ 0.00289359 -0.73508357  0.06217993  1.24425887], action=1, reward=1.0, next_state=[-0.01180808 -0.5408121   0.08706511  0.97168395]\n",
      "[ Experience replay ] starts\n",
      "[ episode 258 ][ timestamp 6 ] state=[-0.01180808 -0.5408121   0.08706511  0.97168395], action=0, reward=1.0, next_state=[-0.02262432 -0.73698776  0.10649879  1.29039765]\n",
      "[ Experience replay ] starts\n",
      "[ episode 258 ][ timestamp 7 ] state=[-0.02262432 -0.73698776  0.10649879  1.29039765], action=0, reward=1.0, next_state=[-0.03736407 -0.93329056  0.13230674  1.61443507]\n",
      "[ Experience replay ] starts\n",
      "[ episode 258 ][ timestamp 8 ] state=[-0.03736407 -0.93329056  0.13230674  1.61443507], action=0, reward=1.0, next_state=[-0.05602988 -1.12970235  0.16459544  1.94526266]\n",
      "[ Experience replay ] starts\n",
      "[ episode 258 ][ timestamp 9 ] state=[-0.05602988 -1.12970235  0.16459544  1.94526266], action=0, reward=1.0, next_state=[-0.07862393 -1.32615161  0.20350069  2.28412681]\n",
      "[ Experience replay ] starts\n",
      "[ episode 258 ][ timestamp 10 ] state=[-0.07862393 -1.32615161  0.20350069  2.28412681], action=0, reward=-1.0, next_state=[-0.10514696 -1.52249667  0.24918323  2.63198417]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 258: Exploration_rate=0.01. Score=10.\n",
      "[ episode 259 ] state=[ 0.00276766 -0.00293615  0.00732109  0.01373958]\n",
      "[ episode 259 ][ timestamp 1 ] state=[ 0.00276766 -0.00293615  0.00732109  0.01373958], action=0, reward=1.0, next_state=[ 0.00270894 -0.19816232  0.00759588  0.30872338]\n",
      "[ Experience replay ] starts\n",
      "[ episode 259 ][ timestamp 2 ] state=[ 0.00270894 -0.19816232  0.00759588  0.30872338], action=0, reward=1.0, next_state=[-0.00125431 -0.39339167  0.01377035  0.60379212]\n",
      "[ Experience replay ] starts\n",
      "[ episode 259 ][ timestamp 3 ] state=[-0.00125431 -0.39339167  0.01377035  0.60379212], action=0, reward=1.0, next_state=[-0.00912214 -0.58870348  0.02584619  0.90078041]\n",
      "[ Experience replay ] starts\n",
      "[ episode 259 ][ timestamp 4 ] state=[-0.00912214 -0.58870348  0.02584619  0.90078041], action=0, reward=1.0, next_state=[-0.02089621 -0.78416593  0.0438618   1.20147409]\n",
      "[ Experience replay ] starts\n",
      "[ episode 259 ][ timestamp 5 ] state=[-0.02089621 -0.78416593  0.0438618   1.20147409], action=0, reward=1.0, next_state=[-0.03657953 -0.97982684  0.06789128  1.50757442]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 259 ][ timestamp 6 ] state=[-0.03657953 -0.97982684  0.06789128  1.50757442], action=0, reward=1.0, next_state=[-0.05617607 -1.17570308  0.09804277  1.82065662]\n",
      "[ Experience replay ] starts\n",
      "[ episode 259 ][ timestamp 7 ] state=[-0.05617607 -1.17570308  0.09804277  1.82065662], action=1, reward=1.0, next_state=[-0.07969013 -0.9817976   0.1344559   1.55997362]\n",
      "[ Experience replay ] starts\n",
      "[ episode 259 ][ timestamp 8 ] state=[-0.07969013 -0.9817976   0.1344559   1.55997362], action=1, reward=1.0, next_state=[-0.09932608 -0.78851666  0.16565537  1.31207996]\n",
      "[ Experience replay ] starts\n",
      "[ episode 259 ][ timestamp 9 ] state=[-0.09932608 -0.78851666  0.16565537  1.31207996], action=0, reward=1.0, next_state=[-0.11509642 -0.98530264  0.19189697  1.65169831]\n",
      "[ Experience replay ] starts\n",
      "[ episode 259 ][ timestamp 10 ] state=[-0.11509642 -0.98530264  0.19189697  1.65169831], action=1, reward=-1.0, next_state=[-0.13480247 -0.79287026  0.22493094  1.4244202 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 259: Exploration_rate=0.01. Score=10.\n",
      "[ episode 260 ] state=[ 0.01822661 -0.00647644  0.01049385 -0.04673153]\n",
      "[ episode 260 ][ timestamp 1 ] state=[ 0.01822661 -0.00647644  0.01049385 -0.04673153], action=0, reward=1.0, next_state=[ 0.01809708 -0.20174729  0.00955922  0.24924374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 2 ] state=[ 0.01809708 -0.20174729  0.00955922  0.24924374], action=1, reward=1.0, next_state=[ 0.01406214 -0.00676314  0.0145441  -0.04040874]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 3 ] state=[ 0.01406214 -0.00676314  0.0145441  -0.04040874], action=0, reward=1.0, next_state=[ 0.01392687 -0.2020906   0.01373592  0.25682727]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 4 ] state=[ 0.01392687 -0.2020906   0.01373592  0.25682727], action=0, reward=1.0, next_state=[ 0.00988506 -0.39740594  0.01887247  0.55381088]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 5 ] state=[ 0.00988506 -0.39740594  0.01887247  0.55381088], action=0, reward=1.0, next_state=[ 0.00193694 -0.59278775  0.02994869  0.85237957]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 6 ] state=[ 0.00193694 -0.59278775  0.02994869  0.85237957], action=0, reward=1.0, next_state=[-0.00991881 -0.7883049   0.04699628  1.15432738]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 7 ] state=[-0.00991881 -0.7883049   0.04699628  1.15432738], action=1, reward=1.0, next_state=[-0.02568491 -0.59382629  0.07008282  0.87674337]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 8 ] state=[-0.02568491 -0.59382629  0.07008282  0.87674337], action=1, reward=1.0, next_state=[-0.03756143 -0.39972327  0.08761769  0.60689106]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 9 ] state=[-0.03756143 -0.39972327  0.08761769  0.60689106], action=1, reward=1.0, next_state=[-0.0455559  -0.2059286   0.09975551  0.3430408 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 10 ] state=[-0.0455559  -0.2059286   0.09975551  0.3430408 ], action=1, reward=1.0, next_state=[-0.04967447 -0.01235686  0.10661633  0.08340619]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 11 ] state=[-0.04967447 -0.01235686  0.10661633  0.08340619], action=1, reward=1.0, next_state=[-0.04992161  0.18108805  0.10828445 -0.17382772]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 12 ] state=[-0.04992161  0.18108805  0.10828445 -0.17382772], action=1, reward=1.0, next_state=[-0.04629985  0.37450704  0.1048079  -0.43048346]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 13 ] state=[-0.04629985  0.37450704  0.1048079  -0.43048346], action=1, reward=1.0, next_state=[-0.03880971  0.56800084  0.09619823 -0.68837437]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 14 ] state=[-0.03880971  0.56800084  0.09619823 -0.68837437], action=1, reward=1.0, next_state=[-0.02744969  0.76166534  0.08243074 -0.94928933]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 15 ] state=[-0.02744969  0.76166534  0.08243074 -0.94928933], action=1, reward=1.0, next_state=[-0.01221638  0.95558665  0.06344496 -1.21497641]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 16 ] state=[-0.01221638  0.95558665  0.06344496 -1.21497641], action=1, reward=1.0, next_state=[ 0.00689535  1.14983532  0.03914543 -1.48712288]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 17 ] state=[ 0.00689535  1.14983532  0.03914543 -1.48712288], action=0, reward=1.0, next_state=[ 0.02989206  0.9542589   0.00940297 -1.18247717]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 18 ] state=[ 0.02989206  0.9542589   0.00940297 -1.18247717], action=0, reward=1.0, next_state=[ 0.04897723  0.75901619 -0.01424657 -0.88686163]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 19 ] state=[ 0.04897723  0.75901619 -0.01424657 -0.88686163], action=0, reward=1.0, next_state=[ 0.06415756  0.56409049 -0.03198381 -0.5986911 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 20 ] state=[ 0.06415756  0.56409049 -0.03198381 -0.5986911 ], action=0, reward=1.0, next_state=[ 0.07543937  0.36943031 -0.04395763 -0.31625179]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 21 ] state=[ 0.07543937  0.36943031 -0.04395763 -0.31625179], action=1, reward=1.0, next_state=[ 0.08282797  0.56514992 -0.05028266 -0.622467  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 22 ] state=[ 0.08282797  0.56514992 -0.05028266 -0.622467  ], action=0, reward=1.0, next_state=[ 0.09413097  0.37076481 -0.062732   -0.34603474]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 23 ] state=[ 0.09413097  0.37076481 -0.062732   -0.34603474], action=1, reward=1.0, next_state=[ 0.10154627  0.5667204  -0.0696527  -0.65782107]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 24 ] state=[ 0.10154627  0.5667204  -0.0696527  -0.65782107], action=0, reward=1.0, next_state=[ 0.11288068  0.37263355 -0.08280912 -0.38785805]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 25 ] state=[ 0.11288068  0.37263355 -0.08280912 -0.38785805], action=0, reward=1.0, next_state=[ 0.12033335  0.17877869 -0.09056628 -0.12239025]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 26 ] state=[ 0.12033335  0.17877869 -0.09056628 -0.12239025], action=0, reward=1.0, next_state=[ 0.12390892 -0.01493688 -0.09301409  0.14040213]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 27 ] state=[ 0.12390892 -0.01493688 -0.09301409  0.14040213], action=0, reward=1.0, next_state=[ 0.12361018 -0.20861209 -0.09020604  0.40235242]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 28 ] state=[ 0.12361018 -0.20861209 -0.09020604  0.40235242], action=0, reward=1.0, next_state=[ 0.11943794 -0.40234655 -0.08215899  0.66528796]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 29 ] state=[ 0.11943794 -0.40234655 -0.08215899  0.66528796], action=0, reward=1.0, next_state=[ 0.11139101 -0.59623541 -0.06885324  0.93101264]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 30 ] state=[ 0.11139101 -0.59623541 -0.06885324  0.93101264], action=1, reward=1.0, next_state=[ 0.0994663  -0.40025517 -0.05023298  0.61751197]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 31 ] state=[ 0.0994663  -0.40025517 -0.05023298  0.61751197], action=1, reward=1.0, next_state=[ 0.0914612  -0.20446882 -0.03788274  0.3094406 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 32 ] state=[ 0.0914612  -0.20446882 -0.03788274  0.3094406 ], action=0, reward=1.0, next_state=[ 0.08737182 -0.39903111 -0.03169393  0.58993979]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 33 ] state=[ 0.08737182 -0.39903111 -0.03169393  0.58993979], action=0, reward=1.0, next_state=[ 0.0793912  -0.59369528 -0.01989514  0.87247295]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 34 ] state=[ 0.0793912  -0.59369528 -0.01989514  0.87247295], action=1, reward=1.0, next_state=[ 0.06751729 -0.39830852 -0.00244568  0.57360202]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 35 ] state=[ 0.06751729 -0.39830852 -0.00244568  0.57360202], action=0, reward=1.0, next_state=[ 0.05955112 -0.59339609  0.00902636  0.86551348]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 36 ] state=[ 0.05955112 -0.59339609  0.00902636  0.86551348], action=1, reward=1.0, next_state=[ 0.0476832  -0.39839815  0.02633663  0.5756822 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 37 ] state=[ 0.0476832  -0.39839815  0.02633663  0.5756822 ], action=0, reward=1.0, next_state=[ 0.03971524 -0.5938792   0.03785028  0.87654416]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 38 ] state=[ 0.03971524 -0.5938792   0.03785028  0.87654416], action=1, reward=1.0, next_state=[ 0.02783766 -0.39929158  0.05538116  0.59599711]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 39 ] state=[ 0.02783766 -0.39929158  0.05538116  0.59599711], action=1, reward=1.0, next_state=[ 0.01985182 -0.20498663  0.0673011   0.32126028]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 260 ][ timestamp 40 ] state=[ 0.01985182 -0.20498663  0.0673011   0.32126028], action=0, reward=1.0, next_state=[ 0.01575209 -0.40099923  0.07372631  0.63438515]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 41 ] state=[ 0.01575209 -0.40099923  0.07372631  0.63438515], action=1, reward=1.0, next_state=[ 0.00773211 -0.2069789   0.08641401  0.36580116]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 42 ] state=[ 0.00773211 -0.2069789   0.08641401  0.36580116], action=1, reward=1.0, next_state=[ 0.00359253 -0.01318445  0.09373003  0.10156827]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 43 ] state=[ 0.00359253 -0.01318445  0.09373003  0.10156827], action=1, reward=1.0, next_state=[ 0.00332884  0.18047795  0.0957614  -0.16013392]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 44 ] state=[ 0.00332884  0.18047795  0.0957614  -0.16013392], action=1, reward=1.0, next_state=[ 0.0069384   0.37410778  0.09255872 -0.42113713]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 45 ] state=[ 0.0069384   0.37410778  0.09255872 -0.42113713], action=0, reward=1.0, next_state=[ 0.01442055  0.17780467  0.08413598 -0.10076945]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 46 ] state=[ 0.01442055  0.17780467  0.08413598 -0.10076945], action=1, reward=1.0, next_state=[ 0.01797665  0.37162635  0.08212059 -0.36576675]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 47 ] state=[ 0.01797665  0.37162635  0.08212059 -0.36576675], action=1, reward=1.0, next_state=[ 0.02540917  0.56549112  0.07480525 -0.63146759]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 48 ] state=[ 0.02540917  0.56549112  0.07480525 -0.63146759], action=0, reward=1.0, next_state=[ 0.036719    0.36940956  0.0621759  -0.31619555]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 49 ] state=[ 0.036719    0.36940956  0.0621759  -0.31619555], action=1, reward=1.0, next_state=[ 0.04410719  0.56359332  0.05585199 -0.58864042]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 50 ] state=[ 0.04410719  0.56359332  0.05585199 -0.58864042], action=1, reward=1.0, next_state=[ 0.05537905  0.7578905   0.04407918 -0.86321979]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 51 ] state=[ 0.05537905  0.7578905   0.04407918 -0.86321979], action=1, reward=1.0, next_state=[ 0.07053686  0.95238551  0.02681479 -1.14172384]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 52 ] state=[ 0.07053686  0.95238551  0.02681479 -1.14172384], action=0, reward=1.0, next_state=[ 0.08958457  0.75692356  0.00398031 -0.84075371]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 53 ] state=[ 0.08958457  0.75692356  0.00398031 -0.84075371], action=0, reward=1.0, next_state=[ 0.10472305  0.5617475  -0.01283476 -0.54682173]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 54 ] state=[ 0.10472305  0.5617475  -0.01283476 -0.54682173], action=0, reward=1.0, next_state=[ 0.115958    0.3668082  -0.0237712  -0.25821018]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 55 ] state=[ 0.115958    0.3668082  -0.0237712  -0.25821018], action=0, reward=1.0, next_state=[ 0.12329416  0.17203354 -0.0289354   0.02688119]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 56 ] state=[ 0.12329416  0.17203354 -0.0289354   0.02688119], action=0, reward=1.0, next_state=[ 0.12673483 -0.02266176 -0.02839778  0.31029607]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 57 ] state=[ 0.12673483 -0.02266176 -0.02839778  0.31029607], action=1, reward=1.0, next_state=[ 0.1262816   0.17285304 -0.02219186  0.0087943 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 58 ] state=[ 0.1262816   0.17285304 -0.02219186  0.0087943 ], action=0, reward=1.0, next_state=[ 0.12973866 -0.02194374 -0.02201597  0.29439364]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 59 ] state=[ 0.12973866 -0.02194374 -0.02201597  0.29439364], action=1, reward=1.0, next_state=[ 0.12929978  0.17348506 -0.0161281  -0.00515069]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 60 ] state=[ 0.12929978  0.17348506 -0.0161281  -0.00515069], action=1, reward=1.0, next_state=[ 0.13276948  0.36883455 -0.01623111 -0.30287827]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 61 ] state=[ 0.13276948  0.36883455 -0.01623111 -0.30287827], action=0, reward=1.0, next_state=[ 0.14014617  0.17394764 -0.02228868 -0.01535815]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 62 ] state=[ 0.14014617  0.17394764 -0.02228868 -0.01535815], action=0, reward=1.0, next_state=[ 0.14362513 -0.02084769 -0.02259584  0.27020994]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 63 ] state=[ 0.14362513 -0.02084769 -0.02259584  0.27020994], action=0, reward=1.0, next_state=[ 0.14320817 -0.21564003 -0.01719164  0.55568125]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 64 ] state=[ 0.14320817 -0.21564003 -0.01719164  0.55568125], action=1, reward=1.0, next_state=[ 0.13889537 -0.02028097 -0.00607802  0.25763188]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 65 ] state=[ 0.13889537 -0.02028097 -0.00607802  0.25763188], action=1, reward=1.0, next_state=[ 0.13848975  0.17492722 -0.00092538 -0.03696193]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 66 ] state=[ 0.13848975  0.17492722 -0.00092538 -0.03696193], action=1, reward=1.0, next_state=[ 0.1419883   0.37006243 -0.00166462 -0.32993668]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 67 ] state=[ 0.1419883   0.37006243 -0.00166462 -0.32993668], action=1, reward=1.0, next_state=[ 0.14938955  0.56520804 -0.00826335 -0.62314408]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 68 ] state=[ 0.14938955  0.56520804 -0.00826335 -0.62314408], action=1, reward=1.0, next_state=[ 0.16069371  0.76044439 -0.02072623 -0.91841801]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 69 ] state=[ 0.16069371  0.76044439 -0.02072623 -0.91841801], action=1, reward=1.0, next_state=[ 0.17590259  0.9558403  -0.03909459 -1.217542  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 70 ] state=[ 0.17590259  0.9558403  -0.03909459 -1.217542  ], action=0, reward=1.0, next_state=[ 0.1950194   0.76124368 -0.06344543 -0.93736099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 71 ] state=[ 0.1950194   0.76124368 -0.06344543 -0.93736099], action=0, reward=1.0, next_state=[ 0.21024427  0.56703196 -0.08219265 -0.66526999]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 72 ] state=[ 0.21024427  0.56703196 -0.08219265 -0.66526999], action=0, reward=1.0, next_state=[ 0.22158491  0.37314365 -0.09549805 -0.39955678]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 73 ] state=[ 0.22158491  0.37314365 -0.09549805 -0.39955678], action=0, reward=1.0, next_state=[ 0.22904779  0.17949695 -0.10348919 -0.13844402]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 74 ] state=[ 0.22904779  0.17949695 -0.10348919 -0.13844402], action=1, reward=1.0, next_state=[ 0.23263773  0.37593719 -0.10625807 -0.46189942]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 75 ] state=[ 0.23263773  0.37593719 -0.10625807 -0.46189942], action=0, reward=1.0, next_state=[ 0.24015647  0.18246485 -0.11549606 -0.20450883]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 76 ] state=[ 0.24015647  0.18246485 -0.11549606 -0.20450883], action=1, reward=1.0, next_state=[ 0.24380577  0.37903278 -0.11958623 -0.53127674]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 77 ] state=[ 0.24380577  0.37903278 -0.11958623 -0.53127674], action=0, reward=1.0, next_state=[ 0.25138642  0.18577797 -0.13021177 -0.27853946]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 78 ] state=[ 0.25138642  0.18577797 -0.13021177 -0.27853946], action=0, reward=1.0, next_state=[ 0.25510198 -0.00726927 -0.13578256 -0.02959416]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 79 ] state=[ 0.25510198 -0.00726927 -0.13578256 -0.02959416], action=0, reward=1.0, next_state=[ 0.2549566  -0.20020917 -0.13637444  0.21735437]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 80 ] state=[ 0.2549566  -0.20020917 -0.13637444  0.21735437], action=0, reward=1.0, next_state=[ 0.25095241 -0.39314481 -0.13202735  0.46410091]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 260 ][ timestamp 81 ] state=[ 0.25095241 -0.39314481 -0.13202735  0.46410091], action=0, reward=1.0, next_state=[ 0.24308952 -0.58617811 -0.12274533  0.71242755]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 82 ] state=[ 0.24308952 -0.58617811 -0.12274533  0.71242755], action=1, reward=1.0, next_state=[ 0.23136595 -0.3895897  -0.10849678  0.38376698]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 83 ] state=[ 0.23136595 -0.3895897  -0.10849678  0.38376698], action=1, reward=1.0, next_state=[ 0.22357416 -0.19310806 -0.10082144  0.05894198]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 84 ] state=[ 0.22357416 -0.19310806 -0.10082144  0.05894198], action=1, reward=1.0, next_state=[ 0.219712    0.0033041  -0.0996426  -0.26377146]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 85 ] state=[ 0.219712    0.0033041  -0.0996426  -0.26377146], action=0, reward=1.0, next_state=[ 0.21977808 -0.19026486 -0.10491803 -0.0041047 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 86 ] state=[ 0.21977808 -0.19026486 -0.10491803 -0.0041047 ], action=0, reward=1.0, next_state=[ 0.21597278 -0.38373789 -0.10500013  0.25371968]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 87 ] state=[ 0.21597278 -0.38373789 -0.10500013  0.25371968], action=0, reward=1.0, next_state=[ 0.20829803 -0.5772161  -0.09992573  0.5115253 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 88 ] state=[ 0.20829803 -0.5772161  -0.09992573  0.5115253 ], action=0, reward=1.0, next_state=[ 0.1967537  -0.770799   -0.08969523  0.77112183]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 89 ] state=[ 0.1967537  -0.770799   -0.08969523  0.77112183], action=1, reward=1.0, next_state=[ 0.18133772 -0.57456475 -0.07427279  0.45161867]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 90 ] state=[ 0.18133772 -0.57456475 -0.07427279  0.45161867], action=1, reward=1.0, next_state=[ 0.16984643 -0.3784753  -0.06524042  0.13647928]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 91 ] state=[ 0.16984643 -0.3784753  -0.06524042  0.13647928], action=0, reward=1.0, next_state=[ 0.16227692 -0.57260507 -0.06251083  0.40788738]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 92 ] state=[ 0.16227692 -0.57260507 -0.06251083  0.40788738], action=1, reward=1.0, next_state=[ 0.15082482 -0.37665505 -0.05435308  0.09617021]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 93 ] state=[ 0.15082482 -0.37665505 -0.05435308  0.09617021], action=1, reward=1.0, next_state=[ 0.14329172 -0.18079791 -0.05242968 -0.21315358]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 94 ] state=[ 0.14329172 -0.18079791 -0.05242968 -0.21315358], action=0, reward=1.0, next_state=[ 0.13967576 -0.3751326  -0.05669275  0.06254062]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 95 ] state=[ 0.13967576 -0.3751326  -0.05669275  0.06254062], action=0, reward=1.0, next_state=[ 0.13217311 -0.56939781 -0.05544194  0.33681154]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 96 ] state=[ 0.13217311 -0.56939781 -0.05544194  0.33681154], action=0, reward=1.0, next_state=[ 0.12078515 -0.76368875 -0.04870571  0.61150857]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 97 ] state=[ 0.12078515 -0.76368875 -0.04870571  0.61150857], action=0, reward=1.0, next_state=[ 0.10551138 -0.95809734 -0.03647554  0.88846182]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 98 ] state=[ 0.10551138 -0.95809734 -0.03647554  0.88846182], action=0, reward=1.0, next_state=[ 0.08634943 -1.15270579 -0.0187063   1.16945889]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 99 ] state=[ 0.08634943 -1.15270579 -0.0187063   1.16945889], action=0, reward=1.0, next_state=[ 0.06329532 -1.34757949  0.00468288  1.45621898]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 100 ] state=[ 0.06329532 -1.34757949  0.00468288  1.45621898], action=0, reward=1.0, next_state=[ 0.03634373 -1.5427586   0.03380726  1.75036119]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 101 ] state=[ 0.03634373 -1.5427586   0.03380726  1.75036119], action=0, reward=1.0, next_state=[ 0.00548855 -1.73824766  0.06881448  2.05336465]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 102 ] state=[ 0.00548855 -1.73824766  0.06881448  2.05336465], action=0, reward=1.0, next_state=[-0.0292764  -1.93400283  0.10988177  2.36651794]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 103 ] state=[-0.0292764  -1.93400283  0.10988177  2.36651794], action=1, reward=1.0, next_state=[-0.06795645 -1.74001562  0.15721213  2.10953228]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 104 ] state=[-0.06795645 -1.74001562  0.15721213  2.10953228], action=1, reward=1.0, next_state=[-0.10275677 -1.5467782   0.19940278  1.86928099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 260 ][ timestamp 105 ] state=[-0.10275677 -1.5467782   0.19940278  1.86928099], action=1, reward=-1.0, next_state=[-0.13369233 -1.35431806  0.2367884   1.64454785]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 260: Exploration_rate=0.01. Score=105.\n",
      "[ episode 261 ] state=[ 0.03795989 -0.0144076  -0.01236538 -0.01228039]\n",
      "[ episode 261 ][ timestamp 1 ] state=[ 0.03795989 -0.0144076  -0.01236538 -0.01228039], action=0, reward=1.0, next_state=[ 0.03767173 -0.20935006 -0.01261099  0.27647561]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 2 ] state=[ 0.03767173 -0.20935006 -0.01261099  0.27647561], action=0, reward=1.0, next_state=[ 0.03348473 -0.40428984 -0.00708148  0.56515449]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 3 ] state=[ 0.03348473 -0.40428984 -0.00708148  0.56515449], action=1, reward=1.0, next_state=[ 0.02539894 -0.20906925  0.00422161  0.27024902]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 4 ] state=[ 0.02539894 -0.20906925  0.00422161  0.27024902], action=0, reward=1.0, next_state=[ 0.02121755 -0.40425119  0.00962659  0.56426047]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 5 ] state=[ 0.02121755 -0.40425119  0.00962659  0.56426047], action=1, reward=1.0, next_state=[ 0.01313253 -0.20926563  0.0209118   0.27462585]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 6 ] state=[ 0.01313253 -0.20926563  0.0209118   0.27462585], action=1, reward=1.0, next_state=[ 0.00894721 -0.01444819  0.02640432 -0.0113888 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 7 ] state=[ 0.00894721 -0.01444819  0.02640432 -0.0113888 ], action=0, reward=1.0, next_state=[ 0.00865825 -0.20993866  0.02617654  0.28950667]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 8 ] state=[ 0.00865825 -0.20993866  0.02617654  0.28950667], action=1, reward=1.0, next_state=[ 0.00445948 -0.01519957  0.03196668  0.00519313]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 9 ] state=[ 0.00445948 -0.01519957  0.03196668  0.00519313], action=1, reward=1.0, next_state=[ 0.00415549  0.17944969  0.03207054 -0.277235  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 10 ] state=[ 0.00415549  0.17944969  0.03207054 -0.277235  ], action=0, reward=1.0, next_state=[ 0.00774448 -0.01611476  0.02652584  0.02538797]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 11 ] state=[ 0.00774448 -0.01611476  0.02652584  0.02538797], action=0, reward=1.0, next_state=[ 0.00742218 -0.21160688  0.0270336   0.32632066]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 12 ] state=[ 0.00742218 -0.21160688  0.0270336   0.32632066], action=1, reward=1.0, next_state=[ 0.00319005 -0.01688004  0.03356001  0.04228404]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 13 ] state=[ 0.00319005 -0.01688004  0.03356001  0.04228404], action=1, reward=1.0, next_state=[ 0.00285245  0.177745    0.03440569 -0.23962434]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 14 ] state=[ 0.00285245  0.177745    0.03440569 -0.23962434], action=0, reward=1.0, next_state=[ 0.00640735 -0.01785113  0.0296132   0.06370949]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 15 ] state=[ 0.00640735 -0.01785113  0.0296132   0.06370949], action=1, reward=1.0, next_state=[ 0.00605032  0.176834    0.03088739 -0.21948516]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 16 ] state=[ 0.00605032  0.176834    0.03088739 -0.21948516], action=0, reward=1.0, next_state=[ 0.009587   -0.01871554  0.02649769  0.0827787 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 17 ] state=[ 0.009587   -0.01871554  0.02649769  0.0827787 ], action=1, reward=1.0, next_state=[ 0.00921269  0.17601675  0.02815327 -0.20142779]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 261 ][ timestamp 18 ] state=[ 0.00921269  0.17601675  0.02815327 -0.20142779], action=0, reward=1.0, next_state=[ 0.01273303 -0.0194963   0.02412471  0.10000153]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 19 ] state=[ 0.01273303 -0.0194963   0.02412471  0.10000153], action=1, reward=1.0, next_state=[ 0.0123431   0.17527175  0.02612474 -0.18497354]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 20 ] state=[ 0.0123431   0.17527175  0.02612474 -0.18497354], action=1, reward=1.0, next_state=[ 0.01584854  0.37001035  0.02242527 -0.46930197]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 21 ] state=[ 0.01584854  0.37001035  0.02242527 -0.46930197], action=1, reward=1.0, next_state=[ 0.02324874  0.56480845  0.01303923 -0.75483318]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 22 ] state=[ 0.02324874  0.56480845  0.01303923 -0.75483318], action=1, reward=1.0, next_state=[ 0.03454491  0.75974824 -0.00205743 -1.04338458]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 23 ] state=[ 0.03454491  0.75974824 -0.00205743 -1.04338458], action=0, reward=1.0, next_state=[ 0.04973988  0.56465367 -0.02292512 -0.75134823]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 24 ] state=[ 0.04973988  0.56465367 -0.02292512 -0.75134823], action=1, reward=1.0, next_state=[ 0.06103295  0.76008415 -0.03795209 -1.05115632]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 25 ] state=[ 0.06103295  0.76008415 -0.03795209 -1.05115632], action=0, reward=1.0, next_state=[ 0.07623463  0.56548556 -0.05897522 -0.77062386]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 26 ] state=[ 0.07623463  0.56548556 -0.05897522 -0.77062386], action=1, reward=1.0, next_state=[ 0.08754434  0.7613674  -0.07438769 -1.08126447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 27 ] state=[ 0.08754434  0.7613674  -0.07438769 -1.08126447], action=0, reward=1.0, next_state=[ 0.10277169  0.56730201 -0.09601298 -0.81282122]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 28 ] state=[ 0.10277169  0.56730201 -0.09601298 -0.81282122], action=0, reward=1.0, next_state=[ 0.11411773  0.37361703 -0.11226941 -0.55181631]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 29 ] state=[ 0.11411773  0.37361703 -0.11226941 -0.55181631], action=1, reward=1.0, next_state=[ 0.12159007  0.57012184 -0.12330573 -0.87765576]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 30 ] state=[ 0.12159007  0.57012184 -0.12330573 -0.87765576], action=0, reward=1.0, next_state=[ 0.13299251  0.37687188 -0.14085885 -0.62614179]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 31 ] state=[ 0.13299251  0.37687188 -0.14085885 -0.62614179], action=0, reward=1.0, next_state=[ 0.14052995  0.18396795 -0.15338168 -0.38092743]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 32 ] state=[ 0.14052995  0.18396795 -0.15338168 -0.38092743], action=0, reward=1.0, next_state=[ 0.14420931 -0.00868122 -0.16100023 -0.14026382]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 33 ] state=[ 0.14420931 -0.00868122 -0.16100023 -0.14026382], action=1, reward=1.0, next_state=[ 0.14403568  0.18833648 -0.16380551 -0.4790983 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 34 ] state=[ 0.14403568  0.18833648 -0.16380551 -0.4790983 ], action=1, reward=1.0, next_state=[ 0.14780241  0.38534598 -0.17338748 -0.81860048]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 35 ] state=[ 0.14780241  0.38534598 -0.17338748 -0.81860048], action=1, reward=1.0, next_state=[ 0.15550933  0.58236317 -0.18975948 -1.16041605]\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ][ timestamp 36 ] state=[ 0.15550933  0.58236317 -0.18975948 -1.16041605], action=1, reward=-1.0, next_state=[ 0.16715659  0.77938099 -0.21296781 -1.50609304]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 261: Exploration_rate=0.01. Score=36.\n",
      "[ episode 262 ] state=[0.0415863  0.03573438 0.00589904 0.01535078]\n",
      "[ episode 262 ][ timestamp 1 ] state=[0.0415863  0.03573438 0.00589904 0.01535078], action=1, reward=1.0, next_state=[ 0.04230098  0.23077124  0.00620606 -0.27546511]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 2 ] state=[ 0.04230098  0.23077124  0.00620606 -0.27546511], action=1, reward=1.0, next_state=[ 0.04691641  0.4258041   0.00069676 -0.5661842 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 3 ] state=[ 0.04691641  0.4258041   0.00069676 -0.5661842 ], action=1, reward=1.0, next_state=[ 0.05543249  0.62091627 -0.01062693 -0.85864753]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 4 ] state=[ 0.05543249  0.62091627 -0.01062693 -0.85864753], action=0, reward=1.0, next_state=[ 0.06785082  0.42594068 -0.02779988 -0.56932492]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 5 ] state=[ 0.06785082  0.42594068 -0.02779988 -0.56932492], action=0, reward=1.0, next_state=[ 0.07636963  0.23121943 -0.03918637 -0.28552802]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 6 ] state=[ 0.07636963  0.23121943 -0.03918637 -0.28552802], action=0, reward=1.0, next_state=[ 0.08099402  0.03667762 -0.04489693 -0.00545718]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 7 ] state=[ 0.08099402  0.03667762 -0.04489693 -0.00545718], action=1, reward=1.0, next_state=[ 0.08172757  0.23241375 -0.04500608 -0.31196076]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 8 ] state=[ 0.08172757  0.23241375 -0.04500608 -0.31196076], action=1, reward=1.0, next_state=[ 0.08637585  0.42814702 -0.05124529 -0.61849069]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 9 ] state=[ 0.08637585  0.42814702 -0.05124529 -0.61849069], action=0, reward=1.0, next_state=[ 0.09493879  0.23377691 -0.06361511 -0.34237779]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 10 ] state=[ 0.09493879  0.23377691 -0.06361511 -0.34237779], action=1, reward=1.0, next_state=[ 0.09961432  0.42974352 -0.07046266 -0.65442334]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 11 ] state=[ 0.09961432  0.42974352 -0.07046266 -0.65442334], action=1, reward=1.0, next_state=[ 0.10820919  0.62577215 -0.08355113 -0.96843552]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 12 ] state=[ 0.10820919  0.62577215 -0.08355113 -0.96843552], action=0, reward=1.0, next_state=[ 0.12072464  0.43186533 -0.10291984 -0.70312538]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 13 ] state=[ 0.12072464  0.43186533 -0.10291984 -0.70312538], action=0, reward=1.0, next_state=[ 0.12936194  0.23830894 -0.11698235 -0.44453215]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 14 ] state=[ 0.12936194  0.23830894 -0.11698235 -0.44453215], action=0, reward=1.0, next_state=[ 0.13412812  0.04501958 -0.12587299 -0.19089414]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 15 ] state=[ 0.13412812  0.04501958 -0.12587299 -0.19089414], action=0, reward=1.0, next_state=[ 0.13502851 -0.14809793 -0.12969087  0.05958133]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 16 ] state=[ 0.13502851 -0.14809793 -0.12969087  0.05958133], action=0, reward=1.0, next_state=[ 0.13206656 -0.3411449  -0.12849925  0.30869763]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 17 ] state=[ 0.13206656 -0.3411449  -0.12849925  0.30869763], action=0, reward=1.0, next_state=[ 0.12524366 -0.53422412 -0.12232529  0.55825375]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 18 ] state=[ 0.12524366 -0.53422412 -0.12232529  0.55825375], action=0, reward=1.0, next_state=[ 0.11455918 -0.72743581 -0.11116022  0.81003163]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 19 ] state=[ 0.11455918 -0.72743581 -0.11116022  0.81003163], action=0, reward=1.0, next_state=[ 0.10001046 -0.92087349 -0.09495959  1.06578349]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 20 ] state=[ 0.10001046 -0.92087349 -0.09495959  1.06578349], action=1, reward=1.0, next_state=[ 0.08159299 -0.72463202 -0.07364392  0.74487129]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 21 ] state=[ 0.08159299 -0.72463202 -0.07364392  0.74487129], action=1, reward=1.0, next_state=[ 0.06710035 -0.52857521 -0.05874649  0.42995144]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 22 ] state=[ 0.06710035 -0.52857521 -0.05874649  0.42995144], action=1, reward=1.0, next_state=[ 0.05652884 -0.33267269 -0.05014746  0.11934304]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 262 ][ timestamp 23 ] state=[ 0.05652884 -0.33267269 -0.05014746  0.11934304], action=0, reward=1.0, next_state=[ 0.04987539 -0.52704162 -0.0477606   0.39579274]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 24 ] state=[ 0.04987539 -0.52704162 -0.0477606   0.39579274], action=0, reward=1.0, next_state=[ 0.03933456 -0.72145453 -0.03984475  0.67304329]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 25 ] state=[ 0.03933456 -0.72145453 -0.03984475  0.67304329], action=1, reward=1.0, next_state=[ 0.02490547 -0.52580207 -0.02638388  0.36808628]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 26 ] state=[ 0.02490547 -0.52580207 -0.02638388  0.36808628], action=0, reward=1.0, next_state=[ 0.01438943 -0.72053938 -0.01902216  0.65233462]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 27 ] state=[ 0.01438943 -0.72053938 -0.01902216  0.65233462], action=0, reward=1.0, next_state=[-2.13609627e-05 -9.15391334e-01 -5.97546328e-03  9.38967493e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 28 ] state=[-2.13609627e-05 -9.15391334e-01 -5.97546328e-03  9.38967493e-01], action=0, reward=1.0, next_state=[-0.01832919 -1.11043222  0.01280389  1.22976682]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 29 ] state=[-0.01832919 -1.11043222  0.01280389  1.22976682], action=0, reward=1.0, next_state=[-0.04053783 -1.30571654  0.03739922  1.52643354]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 30 ] state=[-0.04053783 -1.30571654  0.03739922  1.52643354], action=1, reward=1.0, next_state=[-0.06665216 -1.11106537  0.06792789  1.24565376]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 31 ] state=[-0.06665216 -1.11106537  0.06792789  1.24565376], action=1, reward=1.0, next_state=[-0.08887347 -0.91687732  0.09284097  0.97499889]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 32 ] state=[-0.08887347 -0.91687732  0.09284097  0.97499889], action=1, reward=1.0, next_state=[-0.10721102 -0.72311508  0.11234095  0.71286327]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 33 ] state=[-0.10721102 -0.72311508  0.11234095  0.71286327], action=0, reward=1.0, next_state=[-0.12167332 -0.91959838  0.12659821  1.0386892 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 34 ] state=[-0.12167332 -0.91959838  0.12659821  1.0386892 ], action=1, reward=1.0, next_state=[-0.14006529 -0.7263652   0.147372    0.78827959]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 35 ] state=[-0.14006529 -0.7263652   0.147372    0.78827959], action=1, reward=1.0, next_state=[-0.15459259 -0.53354151  0.16313759  0.54534995]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 36 ] state=[-0.15459259 -0.53354151  0.16313759  0.54534995], action=1, reward=1.0, next_state=[-0.16526342 -0.34104233  0.17404459  0.30818501]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 37 ] state=[-0.16526342 -0.34104233  0.17404459  0.30818501], action=1, reward=1.0, next_state=[-0.17208427 -0.14877215  0.18020829  0.07504802]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 38 ] state=[-0.17208427 -0.14877215  0.18020829  0.07504802], action=1, reward=1.0, next_state=[-0.17505971  0.04337027  0.18170925 -0.15580345]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 39 ] state=[-0.17505971  0.04337027  0.18170925 -0.15580345], action=1, reward=1.0, next_state=[-0.1741923   0.23548845  0.17859318 -0.38610722]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 40 ] state=[-0.1741923   0.23548845  0.17859318 -0.38610722], action=1, reward=1.0, next_state=[-0.16948254  0.42768526  0.17087103 -0.61758926]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 41 ] state=[-0.16948254  0.42768526  0.17087103 -0.61758926], action=1, reward=1.0, next_state=[-0.16092883  0.62006037  0.15851925 -0.85195761]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 42 ] state=[-0.16092883  0.62006037  0.15851925 -0.85195761], action=1, reward=1.0, next_state=[-0.14852762  0.81270734  0.1414801  -1.09089527]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 43 ] state=[-0.14852762  0.81270734  0.1414801  -1.09089527], action=0, reward=1.0, next_state=[-0.13227348  0.61603318  0.11966219 -0.75737515]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 44 ] state=[-0.13227348  0.61603318  0.11966219 -0.75737515], action=0, reward=1.0, next_state=[-0.11995281  0.41948319  0.10451469 -0.42956166]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 45 ] state=[-0.11995281  0.41948319  0.10451469 -0.42956166], action=1, reward=1.0, next_state=[-0.11156315  0.61298183  0.09592346 -0.68755442]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 46 ] state=[-0.11156315  0.61298183  0.09592346 -0.68755442], action=0, reward=1.0, next_state=[-0.09930351  0.41666855  0.08217237 -0.36627996]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 47 ] state=[-0.09930351  0.41666855  0.08217237 -0.36627996], action=1, reward=1.0, next_state=[-0.09097014  0.61053251  0.07484677 -0.63196317]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 48 ] state=[-0.09097014  0.61053251  0.07484677 -0.63196317], action=1, reward=1.0, next_state=[-0.07875949  0.80453481  0.0622075  -0.90016749]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 49 ] state=[-0.07875949  0.80453481  0.0622075  -0.90016749], action=1, reward=1.0, next_state=[-0.06266879  0.99876114  0.04420415 -1.17266624]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 50 ] state=[-0.06266879  0.99876114  0.04420415 -1.17266624], action=0, reward=1.0, next_state=[-0.04269357  0.8030933   0.02075083 -0.8664594 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 51 ] state=[-0.04269357  0.8030933   0.02075083 -0.8664594 ], action=0, reward=1.0, next_state=[-0.02663171  0.60769519  0.00342164 -0.56732503]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 52 ] state=[-0.02663171  0.60769519  0.00342164 -0.56732503], action=0, reward=1.0, next_state=[-0.0144778   0.41252541 -0.00792486 -0.27356611]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 53 ] state=[-0.0144778   0.41252541 -0.00792486 -0.27356611], action=1, reward=1.0, next_state=[-0.00622729  0.60775954 -0.01339618 -0.56873799]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 54 ] state=[-0.00622729  0.60775954 -0.01339618 -0.56873799], action=1, reward=1.0, next_state=[ 0.0059279   0.80306679 -0.02477094 -0.86561095]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 55 ] state=[ 0.0059279   0.80306679 -0.02477094 -0.86561095], action=0, reward=1.0, next_state=[ 0.02198923  0.60829059 -0.04208316 -0.58081818]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 56 ] state=[ 0.02198923  0.60829059 -0.04208316 -0.58081818], action=0, reward=1.0, next_state=[ 0.03415504  0.4137828  -0.05369952 -0.30168361]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 57 ] state=[ 0.03415504  0.4137828  -0.05369952 -0.30168361], action=0, reward=1.0, next_state=[ 0.0424307   0.21946569 -0.0597332  -0.02640818]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 58 ] state=[ 0.0424307   0.21946569 -0.0597332  -0.02640818], action=0, reward=1.0, next_state=[ 0.04682001  0.02524897 -0.06026136  0.2468462 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 59 ] state=[ 0.04682001  0.02524897 -0.06026136  0.2468462 ], action=0, reward=1.0, next_state=[ 0.04732499 -0.16896287 -0.05532443  0.51992906]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 60 ] state=[ 0.04732499 -0.16896287 -0.05532443  0.51992906], action=0, reward=1.0, next_state=[ 0.04394574 -0.36326412 -0.04492585  0.79467792]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 61 ] state=[ 0.04394574 -0.36326412 -0.04492585  0.79467792], action=0, reward=1.0, next_state=[ 0.03668045 -0.5577416  -0.0290323   1.07289605]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 62 ] state=[ 0.03668045 -0.5577416  -0.0290323   1.07289605], action=1, reward=1.0, next_state=[ 0.02552562 -0.36224817 -0.00757437  0.77124518]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 63 ] state=[ 0.02552562 -0.36224817 -0.00757437  0.77124518], action=0, reward=1.0, next_state=[ 0.01828066 -0.55726508  0.00785053  1.0615353 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 64 ] state=[ 0.01828066 -0.55726508  0.00785053  1.0615353 ], action=1, reward=1.0, next_state=[ 0.00713536 -0.36224796  0.02908124  0.77132667]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 65 ] state=[ 0.00713536 -0.36224796  0.02908124  0.77132667], action=1, reward=1.0, next_state=[-1.09602267e-04 -1.67538008e-01  4.45077688e-02  4.87933916e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 66 ] state=[-1.09602267e-04 -1.67538008e-01  4.45077688e-02  4.87933916e-01], action=0, reward=1.0, next_state=[-0.00346036 -0.36325874  0.05426645  0.79430524]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 262 ][ timestamp 67 ] state=[-0.00346036 -0.36325874  0.05426645  0.79430524], action=1, reward=1.0, next_state=[-0.01072554 -0.16892197  0.07015255  0.5191757 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 68 ] state=[-0.01072554 -0.16892197  0.07015255  0.5191757 ], action=1, reward=1.0, next_state=[-0.01410398  0.02514585  0.08053607  0.24939793]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 69 ] state=[-0.01410398  0.02514585  0.08053607  0.24939793], action=1, reward=1.0, next_state=[-0.01360106  0.21903085  0.08552402 -0.0168349 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 70 ] state=[-0.01360106  0.21903085  0.08552402 -0.0168349 ], action=1, reward=1.0, next_state=[-0.00922044  0.41282879  0.08518733 -0.28135591]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 71 ] state=[-0.00922044  0.41282879  0.08518733 -0.28135591], action=1, reward=1.0, next_state=[-0.00096387  0.60663884  0.07956021 -0.54600199]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 72 ] state=[-0.00096387  0.60663884  0.07956021 -0.54600199], action=0, reward=1.0, next_state=[ 0.01116891  0.41049446  0.06864017 -0.22935006]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 73 ] state=[ 0.01116891  0.41049446  0.06864017 -0.22935006], action=0, reward=1.0, next_state=[0.0193788  0.21446219 0.06405317 0.08417028]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 74 ] state=[0.0193788  0.21446219 0.06405317 0.08417028], action=1, reward=1.0, next_state=[ 0.02366804  0.40861028  0.06573657 -0.18763588]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 75 ] state=[ 0.02366804  0.40861028  0.06573657 -0.18763588], action=1, reward=1.0, next_state=[ 0.03184025  0.60273315  0.06198386 -0.45887863]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 76 ] state=[ 0.03184025  0.60273315  0.06198386 -0.45887863], action=1, reward=1.0, next_state=[ 0.04389491  0.79692664  0.05280628 -0.73139789]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 77 ] state=[ 0.04389491  0.79692664  0.05280628 -0.73139789], action=1, reward=1.0, next_state=[ 0.05983344  0.9912806   0.03817833 -1.00700463]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 78 ] state=[ 0.05983344  0.9912806   0.03817833 -1.00700463], action=1, reward=1.0, next_state=[ 0.07965906  1.18587257  0.01803823 -1.28745818]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 79 ] state=[ 0.07965906  1.18587257  0.01803823 -1.28745818], action=0, reward=1.0, next_state=[ 0.10337651  0.9905258  -0.00771093 -0.98918274]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 80 ] state=[ 0.10337651  0.9905258  -0.00771093 -0.98918274], action=0, reward=1.0, next_state=[ 0.12318702  0.79550792 -0.02749459 -0.69893161]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 81 ] state=[ 0.12318702  0.79550792 -0.02749459 -0.69893161], action=1, reward=1.0, next_state=[ 0.13909718  0.99100007 -0.04147322 -1.00014139]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 82 ] state=[ 0.13909718  0.99100007 -0.04147322 -1.00014139], action=1, reward=1.0, next_state=[ 0.15891718  1.18665106 -0.06147605 -1.30555515]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 83 ] state=[ 0.15891718  1.18665106 -0.06147605 -1.30555515], action=0, reward=1.0, next_state=[ 0.18265021  0.99236004 -0.08758715 -1.03273173]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 84 ] state=[ 0.18265021  0.99236004 -0.08758715 -1.03273173], action=0, reward=1.0, next_state=[ 0.20249741  0.79850532 -0.10824178 -0.76878202]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 85 ] state=[ 0.20249741  0.79850532 -0.10824178 -0.76878202], action=0, reward=1.0, next_state=[ 0.21846751  0.60502655 -0.12361742 -0.51202332]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 86 ] state=[ 0.21846751  0.60502655 -0.12361742 -0.51202332], action=0, reward=1.0, next_state=[ 0.23056804  0.41184281 -0.13385789 -0.26071   ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 87 ] state=[ 0.23056804  0.41184281 -0.13385789 -0.26071   ], action=0, reward=1.0, next_state=[ 0.2388049   0.21886039 -0.13907209 -0.01306268]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 88 ] state=[ 0.2388049   0.21886039 -0.13907209 -0.01306268], action=0, reward=1.0, next_state=[ 0.24318211  0.0259785  -0.13933334  0.23271123]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 89 ] state=[ 0.24318211  0.0259785  -0.13933334  0.23271123], action=1, reward=1.0, next_state=[ 0.24370168  0.22278783 -0.13467912 -0.10047338]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 90 ] state=[ 0.24370168  0.22278783 -0.13467912 -0.10047338], action=0, reward=1.0, next_state=[ 0.24815743  0.02982734 -0.13668859  0.14687026]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 91 ] state=[ 0.24815743  0.02982734 -0.13668859  0.14687026], action=1, reward=1.0, next_state=[ 0.24875398  0.22661505 -0.13375118 -0.18561947]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 92 ] state=[ 0.24875398  0.22661505 -0.13375118 -0.18561947], action=0, reward=1.0, next_state=[ 0.25328628  0.03363511 -0.13746357  0.06205937]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 93 ] state=[ 0.25328628  0.03363511 -0.13746357  0.06205937], action=1, reward=1.0, next_state=[ 0.25395898  0.23043284 -0.13622238 -0.2706397 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 94 ] state=[ 0.25395898  0.23043284 -0.13622238 -0.2706397 ], action=0, reward=1.0, next_state=[ 0.25856764  0.0374911  -0.14163518 -0.02383381]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 95 ] state=[ 0.25856764  0.0374911  -0.14163518 -0.02383381], action=1, reward=1.0, next_state=[ 0.25931746  0.23433017 -0.14211185 -0.3576375 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 96 ] state=[ 0.25931746  0.23433017 -0.14211185 -0.3576375 ], action=1, reward=1.0, next_state=[ 0.26400407  0.4311563  -0.1492646  -0.69154082]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 97 ] state=[ 0.26400407  0.4311563  -0.1492646  -0.69154082], action=0, reward=1.0, next_state=[ 0.27262719  0.23838592 -0.16309542 -0.44932147]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 98 ] state=[ 0.27262719  0.23838592 -0.16309542 -0.44932147], action=0, reward=1.0, next_state=[ 0.27739491  0.04590109 -0.17208185 -0.21216356]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 99 ] state=[ 0.27739491  0.04590109 -0.17208185 -0.21216356], action=0, reward=1.0, next_state=[ 0.27831293 -0.14639635 -0.17632512  0.02167962]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 100 ] state=[ 0.27831293 -0.14639635 -0.17632512  0.02167962], action=0, reward=1.0, next_state=[ 0.27538501 -0.33860895 -0.17589153  0.25395675]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 101 ] state=[ 0.27538501 -0.33860895 -0.17589153  0.25395675], action=0, reward=1.0, next_state=[ 0.26861283 -0.53084039 -0.17081239  0.48640909]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 102 ] state=[ 0.26861283 -0.53084039 -0.17081239  0.48640909], action=0, reward=1.0, next_state=[ 0.25799602 -0.72319257 -0.16108421  0.72076342]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 103 ] state=[ 0.25799602 -0.72319257 -0.16108421  0.72076342], action=0, reward=1.0, next_state=[ 0.24353217 -0.91576292 -0.14666894  0.95872519]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 104 ] state=[ 0.24353217 -0.91576292 -0.14666894  0.95872519], action=0, reward=1.0, next_state=[ 0.22521691 -1.10864108 -0.12749444  1.20196992]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 105 ] state=[ 0.22521691 -1.10864108 -0.12749444  1.20196992], action=0, reward=1.0, next_state=[ 0.20304409 -1.30190492 -0.10345504  1.45213086]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 106 ] state=[ 0.20304409 -1.30190492 -0.10345504  1.45213086], action=1, reward=1.0, next_state=[ 0.17700599 -1.10567542 -0.07441242  1.12899883]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 107 ] state=[ 0.17700599 -1.10567542 -0.07441242  1.12899883], action=0, reward=1.0, next_state=[ 0.15489248 -1.29974809 -0.05183245  1.39744516]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 262 ][ timestamp 108 ] state=[ 0.15489248 -1.29974809 -0.05183245  1.39744516], action=0, reward=1.0, next_state=[ 0.12889752 -1.49418854 -0.02388354  1.67348222]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 109 ] state=[ 0.12889752 -1.49418854 -0.02388354  1.67348222], action=1, reward=1.0, next_state=[ 0.09901375 -1.29879759  0.0095861   1.3734583 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 110 ] state=[ 0.09901375 -1.29879759  0.0095861   1.3734583 ], action=1, reward=1.0, next_state=[ 0.0730378  -1.10379678  0.03705527  1.08378879]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 111 ] state=[ 0.0730378  -1.10379678  0.03705527  1.08378879], action=1, reward=1.0, next_state=[ 0.05096186 -0.90918286  0.05873104  0.80296006]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 112 ] state=[ 0.05096186 -0.90918286  0.05873104  0.80296006], action=1, reward=1.0, next_state=[ 0.0327782  -0.71491332  0.07479024  0.52931518]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 113 ] state=[ 0.0327782  -0.71491332  0.07479024  0.52931518], action=0, reward=1.0, next_state=[ 0.01847994 -0.91100334  0.08537655  0.8445958 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 114 ] state=[ 0.01847994 -0.91100334  0.08537655  0.8445958 ], action=1, reward=1.0, next_state=[ 2.59869714e-04 -7.17143675e-01  1.02268463e-01  5.79935682e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 115 ] state=[ 2.59869714e-04 -7.17143675e-01  1.02268463e-01  5.79935682e-01], action=1, reward=1.0, next_state=[-0.014083   -0.52359231  0.11386718  0.32114009]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 116 ] state=[-0.014083   -0.52359231  0.11386718  0.32114009], action=1, reward=1.0, next_state=[-0.02455485 -0.33026049  0.12028998  0.06642501]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 117 ] state=[-0.02455485 -0.33026049  0.12028998  0.06642501], action=1, reward=1.0, next_state=[-0.03116006 -0.13705014  0.12161848 -0.18601624]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 118 ] state=[-0.03116006 -0.13705014  0.12161848 -0.18601624], action=1, reward=1.0, next_state=[-0.03390106  0.05614081  0.11789815 -0.43799444]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 119 ] state=[-0.03390106  0.05614081  0.11789815 -0.43799444], action=0, reward=1.0, next_state=[-0.03277825 -0.14043533  0.10913826 -0.11059535]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 120 ] state=[-0.03277825 -0.14043533  0.10913826 -0.11059535], action=1, reward=1.0, next_state=[-0.03558695  0.05296723  0.10692636 -0.36695017]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 121 ] state=[-0.03558695  0.05296723  0.10692636 -0.36695017], action=0, reward=1.0, next_state=[-0.03452761 -0.14349879  0.09958735 -0.04255774]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 122 ] state=[-0.03452761 -0.14349879  0.09958735 -0.04255774], action=0, reward=1.0, next_state=[-0.03739758 -0.33989731  0.0987362   0.27981071]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 123 ] state=[-0.03739758 -0.33989731  0.0987362   0.27981071], action=1, reward=1.0, next_state=[-0.04419553 -0.14631231  0.10433241  0.01982877]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 124 ] state=[-0.04419553 -0.14631231  0.10433241  0.01982877], action=1, reward=1.0, next_state=[-0.04712178  0.04717069  0.10472899 -0.23819946]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 125 ] state=[-0.04712178  0.04717069  0.10472899 -0.23819946], action=0, reward=1.0, next_state=[-0.04617836 -0.1492794   0.099965    0.08559519]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 126 ] state=[-0.04617836 -0.1492794   0.099965    0.08559519], action=1, reward=1.0, next_state=[-0.04916395  0.0442781   0.1016769  -0.17395081]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 127 ] state=[-0.04916395  0.0442781   0.1016769  -0.17395081], action=0, reward=1.0, next_state=[-0.04827839 -0.15214096  0.09819789  0.14899765]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 128 ] state=[-0.04827839 -0.15214096  0.09819789  0.14899765], action=0, reward=1.0, next_state=[-0.05132121 -0.34852193  0.10117784  0.47097381]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 129 ] state=[-0.05132121 -0.34852193  0.10117784  0.47097381], action=0, reward=1.0, next_state=[-0.05829165 -0.54491657  0.11059732  0.79375475]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 130 ] state=[-0.05829165 -0.54491657  0.11059732  0.79375475], action=1, reward=1.0, next_state=[-0.06918998 -0.35147243  0.12647241  0.53781072]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 131 ] state=[-0.06918998 -0.35147243  0.12647241  0.53781072], action=0, reward=1.0, next_state=[-0.07621943 -0.54812438  0.13722863  0.86751649]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 132 ] state=[-0.07621943 -0.54812438  0.13722863  0.86751649], action=0, reward=1.0, next_state=[-0.08718191 -0.74482     0.15457896  1.2000049 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 133 ] state=[-0.08718191 -0.74482     0.15457896  1.2000049 ], action=1, reward=1.0, next_state=[-0.10207831 -0.55199794  0.17857905  0.95948594]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 134 ] state=[-0.10207831 -0.55199794  0.17857905  0.95948594], action=0, reward=1.0, next_state=[-0.11311827 -0.74901247  0.19776877  1.30253172]\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ][ timestamp 135 ] state=[-0.11311827 -0.74901247  0.19776877  1.30253172], action=1, reward=-1.0, next_state=[-0.12809852 -0.55687097  0.22381941  1.0777032 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 262: Exploration_rate=0.01. Score=135.\n",
      "[ episode 263 ] state=[ 0.02190356 -0.01730752 -0.03711091 -0.0082241 ]\n",
      "[ episode 263 ][ timestamp 1 ] state=[ 0.02190356 -0.01730752 -0.03711091 -0.0082241 ], action=0, reward=1.0, next_state=[ 0.02155741 -0.21187814 -0.03727539  0.27252278]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 2 ] state=[ 0.02155741 -0.21187814 -0.03727539  0.27252278], action=0, reward=1.0, next_state=[ 0.01731984 -0.40644893 -0.03182493  0.5532198 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 3 ] state=[ 0.01731984 -0.40644893 -0.03182493  0.5532198 ], action=1, reward=1.0, next_state=[ 0.00919086 -0.21089486 -0.02076054  0.25068228]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 4 ] state=[ 0.00919086 -0.21089486 -0.02076054  0.25068228], action=0, reward=1.0, next_state=[ 0.00497297 -0.40571429 -0.01574689  0.53674529]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 5 ] state=[ 0.00497297 -0.40571429 -0.01574689  0.53674529], action=0, reward=1.0, next_state=[-0.00314132 -0.60061134 -0.00501199  0.82442522]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 6 ] state=[-0.00314132 -0.60061134 -0.00501199  0.82442522], action=1, reward=1.0, next_state=[-0.01515355 -0.40542119  0.01147652  0.53017016]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 7 ] state=[-0.01515355 -0.40542119  0.01147652  0.53017016], action=0, reward=1.0, next_state=[-0.02326197 -0.60070269  0.02207992  0.82644714]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 8 ] state=[-0.02326197 -0.60070269  0.02207992  0.82644714], action=1, reward=1.0, next_state=[-0.03527602 -0.40588953  0.03860887  0.54078959]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 9 ] state=[-0.03527602 -0.40588953  0.03860887  0.54078959], action=1, reward=1.0, next_state=[-0.04339381 -0.21133093  0.04942466  0.26051737]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 10 ] state=[-0.04339381 -0.21133093  0.04942466  0.26051737], action=1, reward=1.0, next_state=[-0.04762043 -0.01694809  0.054635   -0.0161759 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 11 ] state=[-0.04762043 -0.01694809  0.054635   -0.0161759 ], action=0, reward=1.0, next_state=[-0.04795939 -0.21280928  0.05431149  0.29323222]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 12 ] state=[-0.04795939 -0.21280928  0.05431149  0.29323222], action=1, reward=1.0, next_state=[-0.05221558 -0.01850205  0.06017613  0.01816086]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 13 ] state=[-0.05221558 -0.01850205  0.06017613  0.01816086], action=0, reward=1.0, next_state=[-0.05258562 -0.21443306  0.06053935  0.32920652]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 14 ] state=[-0.05258562 -0.21443306  0.06053935  0.32920652], action=1, reward=1.0, next_state=[-0.05687428 -0.02022283  0.06712348  0.05621255]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 15 ] state=[-0.05687428 -0.02022283  0.06712348  0.05621255], action=1, reward=1.0, next_state=[-0.05727874  0.17387567  0.06824773 -0.21456007]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 16 ] state=[-0.05727874  0.17387567  0.06824773 -0.21456007], action=1, reward=1.0, next_state=[-0.05380123  0.36795891  0.06395653 -0.48495794]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 17 ] state=[-0.05380123  0.36795891  0.06395653 -0.48495794], action=0, reward=1.0, next_state=[-0.04644205  0.17199546  0.05425737 -0.17282334]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 18 ] state=[-0.04644205  0.17199546  0.05425737 -0.17282334], action=0, reward=1.0, next_state=[-0.04300214 -0.02385937  0.0508009   0.13647042]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 19 ] state=[-0.04300214 -0.02385937  0.0508009   0.13647042], action=0, reward=1.0, next_state=[-0.04347933 -0.21967076  0.05353031  0.44473763]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 263 ][ timestamp 20 ] state=[-0.04347933 -0.21967076  0.05353031  0.44473763], action=1, reward=1.0, next_state=[-0.04787274 -0.02534543  0.06242506  0.16939755]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 21 ] state=[-0.04787274 -0.02534543  0.06242506  0.16939755], action=0, reward=1.0, next_state=[-0.04837965 -0.22130277  0.06581301  0.48110209]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 22 ] state=[-0.04837965 -0.22130277  0.06581301  0.48110209], action=0, reward=1.0, next_state=[-0.0528057  -0.41728898  0.07543506  0.79378003]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 23 ] state=[-0.0528057  -0.41728898  0.07543506  0.79378003], action=0, reward=1.0, next_state=[-0.06115148 -0.61336087  0.09131066  1.10920833]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 24 ] state=[-0.06115148 -0.61336087  0.09131066  1.10920833], action=1, reward=1.0, next_state=[-0.0734187  -0.41954964  0.11349482  0.84651064]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 25 ] state=[-0.0734187  -0.41954964  0.11349482  0.84651064], action=0, reward=1.0, next_state=[-0.08180969 -0.61602177  0.13042504  1.17261868]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 26 ] state=[-0.08180969 -0.61602177  0.13042504  1.17261868], action=0, reward=1.0, next_state=[-0.09413013 -0.81257534  0.15387741  1.50318129]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 27 ] state=[-0.09413013 -0.81257534  0.15387741  1.50318129], action=1, reward=1.0, next_state=[-0.11038164 -0.61961934  0.18394104  1.26222882]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 28 ] state=[-0.11038164 -0.61961934  0.18394104  1.26222882], action=1, reward=1.0, next_state=[-0.12277402 -0.42726284  0.20918561  1.03233573]\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ][ timestamp 29 ] state=[-0.12277402 -0.42726284  0.20918561  1.03233573], action=1, reward=-1.0, next_state=[-0.13131928 -0.23544351  0.22983233  0.81193216]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 263: Exploration_rate=0.01. Score=29.\n",
      "[ episode 264 ] state=[-0.02327585  0.04242669  0.03918376 -0.04543086]\n",
      "[ episode 264 ][ timestamp 1 ] state=[-0.02327585  0.04242669  0.03918376 -0.04543086], action=0, reward=1.0, next_state=[-0.02242732 -0.15323459  0.03827514  0.25935286]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 2 ] state=[-0.02242732 -0.15323459  0.03827514  0.25935286], action=1, reward=1.0, next_state=[-0.02549201  0.04132064  0.0434622  -0.02101611]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 3 ] state=[-0.02549201  0.04132064  0.0434622  -0.02101611], action=1, reward=1.0, next_state=[-0.0246656   0.23579322  0.04304187 -0.29967564]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 4 ] state=[-0.0246656   0.23579322  0.04304187 -0.29967564], action=0, reward=1.0, next_state=[-0.01994974  0.04008503  0.03704836  0.00626516]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 5 ] state=[-0.01994974  0.04008503  0.03704836  0.00626516], action=0, reward=1.0, next_state=[-0.01914803 -0.15554812  0.03717366  0.31040325]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 6 ] state=[-0.01914803 -0.15554812  0.03717366  0.31040325], action=1, reward=1.0, next_state=[-0.022259    0.03902503  0.04338173  0.0296717 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 7 ] state=[-0.022259    0.03902503  0.04338173  0.0296717 ], action=1, reward=1.0, next_state=[-0.0214785   0.23349888  0.04397516 -0.24901439]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 8 ] state=[-0.0214785   0.23349888  0.04397516 -0.24901439], action=1, reward=1.0, next_state=[-0.01680852  0.42796613  0.03899488 -0.52750874]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 9 ] state=[-0.01680852  0.42796613  0.03899488 -0.52750874], action=1, reward=1.0, next_state=[-0.0082492   0.62251835  0.0284447  -0.80765364]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 10 ] state=[-0.0082492   0.62251835  0.0284447  -0.80765364], action=0, reward=1.0, next_state=[ 0.00420117  0.42701836  0.01229163 -0.50616065]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 11 ] state=[ 0.00420117  0.42701836  0.01229163 -0.50616065], action=1, reward=1.0, next_state=[ 0.01274154  0.62196496  0.00216842 -0.79494482]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 12 ] state=[ 0.01274154  0.62196496  0.00216842 -0.79494482], action=0, reward=1.0, next_state=[ 0.02518084  0.42681332 -0.01373048 -0.50158053]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 13 ] state=[ 0.02518084  0.42681332 -0.01373048 -0.50158053], action=0, reward=1.0, next_state=[ 0.0337171   0.23188757 -0.02376209 -0.21325611]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 14 ] state=[ 0.0337171   0.23188757 -0.02376209 -0.21325611], action=0, reward=1.0, next_state=[ 0.03835486  0.03711327 -0.02802721  0.07183747]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 15 ] state=[ 0.03835486  0.03711327 -0.02802721  0.07183747], action=0, reward=1.0, next_state=[ 0.03909712 -0.15759589 -0.02659046  0.35554759]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 16 ] state=[ 0.03909712 -0.15759589 -0.02659046  0.35554759], action=0, reward=1.0, next_state=[ 0.0359452  -0.35232989 -0.01947951  0.63972864]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 17 ] state=[ 0.0359452  -0.35232989 -0.01947951  0.63972864], action=1, reward=1.0, next_state=[ 0.0288986  -0.15694184 -0.00668494  0.34097557]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 18 ] state=[ 0.0288986  -0.15694184 -0.00668494  0.34097557], action=0, reward=1.0, next_state=[ 2.57597681e-02 -3.51968044e-01  1.34571783e-04  6.31542976e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 19 ] state=[ 2.57597681e-02 -3.51968044e-01  1.34571783e-04  6.31542976e-01], action=1, reward=1.0, next_state=[ 0.01872041 -0.15684797  0.01276543  0.33890243]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 20 ] state=[ 0.01872041 -0.15684797  0.01276543  0.33890243], action=0, reward=1.0, next_state=[ 0.01558345 -0.35214922  0.01954348  0.63558337]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 21 ] state=[ 0.01558345 -0.35214922  0.01954348  0.63558337], action=1, reward=1.0, next_state=[ 0.00854046 -0.15730522  0.03225515  0.3491186 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 22 ] state=[ 0.00854046 -0.15730522  0.03225515  0.3491186 ], action=1, reward=1.0, next_state=[0.00539436 0.03734349 0.03923752 0.06677878]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 23 ] state=[0.00539436 0.03734349 0.03923752 0.06677878], action=1, reward=1.0, next_state=[ 0.00614123  0.23188156  0.0405731  -0.21327085]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 24 ] state=[ 0.00614123  0.23188156  0.0405731  -0.21327085], action=0, reward=1.0, next_state=[0.01077886 0.03620373 0.03630768 0.09192956]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 25 ] state=[0.01077886 0.03620373 0.03630768 0.09192956], action=1, reward=1.0, next_state=[ 0.01150293  0.23078697  0.03814627 -0.18908083]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 26 ] state=[ 0.01150293  0.23078697  0.03814627 -0.18908083], action=0, reward=1.0, next_state=[0.01611867 0.03514063 0.03436465 0.11538747]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 27 ] state=[0.01611867 0.03514063 0.03436465 0.11538747], action=1, reward=1.0, next_state=[ 0.01682149  0.22975376  0.0366724  -0.16625866]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 28 ] state=[ 0.01682149  0.22975376  0.0366724  -0.16625866], action=0, reward=1.0, next_state=[0.02141656 0.03412658 0.03334723 0.13776409]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 29 ] state=[0.02141656 0.03412658 0.03334723 0.13776409], action=0, reward=1.0, next_state=[ 0.02209909 -0.16145674  0.03610251  0.44077823]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 30 ] state=[ 0.02209909 -0.16145674  0.03610251  0.44077823], action=1, reward=1.0, next_state=[0.01886996 0.03313619 0.04491807 0.15969087]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 31 ] state=[0.01886996 0.03313619 0.04491807 0.15969087], action=1, reward=1.0, next_state=[ 0.01953268  0.22758724  0.04811189 -0.11849004]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 32 ] state=[ 0.01953268  0.22758724  0.04811189 -0.11849004], action=1, reward=1.0, next_state=[ 0.02408443  0.42198802  0.04574209 -0.39561434]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 264 ][ timestamp 33 ] state=[ 0.02408443  0.42198802  0.04574209 -0.39561434], action=1, reward=1.0, next_state=[ 0.03252419  0.61643211  0.0378298  -0.67353192]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 34 ] state=[ 0.03252419  0.61643211  0.0378298  -0.67353192], action=1, reward=1.0, next_state=[ 0.04485283  0.81100842  0.02435917 -0.95406825]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 35 ] state=[ 0.04485283  0.81100842  0.02435917 -0.95406825], action=1, reward=1.0, next_state=[ 0.061073    1.00579434  0.0052778  -1.23899956]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 36 ] state=[ 0.061073    1.00579434  0.0052778  -1.23899956], action=1, reward=1.0, next_state=[ 0.08118888  1.20084811 -0.01950219 -1.53002447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 37 ] state=[ 0.08118888  1.20084811 -0.01950219 -1.53002447], action=1, reward=1.0, next_state=[ 0.10520585  1.3961997  -0.05010268 -1.82872942]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 38 ] state=[ 0.10520585  1.3961997  -0.05010268 -1.82872942], action=0, reward=1.0, next_state=[ 0.13312984  1.2016674  -0.08667727 -1.55202116]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 39 ] state=[ 0.13312984  1.2016674  -0.08667727 -1.55202116], action=0, reward=1.0, next_state=[ 0.15716319  1.00768524 -0.11771769 -1.28759149]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 40 ] state=[ 0.15716319  1.00768524 -0.11771769 -1.28759149], action=0, reward=1.0, next_state=[ 0.17731689  0.81424126 -0.14346952 -1.0339628 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 41 ] state=[ 0.17731689  0.81424126 -0.14346952 -1.0339628 ], action=0, reward=1.0, next_state=[ 0.19360172  0.62128824 -0.16414878 -0.7895424 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 42 ] state=[ 0.19360172  0.62128824 -0.16414878 -0.7895424 ], action=1, reward=1.0, next_state=[ 0.20602748  0.81823806 -0.17993962 -1.12903927]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 43 ] state=[ 0.20602748  0.81823806 -0.17993962 -1.12903927], action=0, reward=1.0, next_state=[ 0.22239224  0.62586964 -0.20252041 -0.8977627 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ][ timestamp 44 ] state=[ 0.22239224  0.62586964 -0.20252041 -0.8977627 ], action=0, reward=-1.0, next_state=[ 0.23490964  0.43398323 -0.22047566 -0.67495037]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 264: Exploration_rate=0.01. Score=44.\n",
      "[ episode 265 ] state=[ 0.0254334  -0.02071791 -0.02186442 -0.03282677]\n",
      "[ episode 265 ][ timestamp 1 ] state=[ 0.0254334  -0.02071791 -0.02186442 -0.03282677], action=1, reward=1.0, next_state=[ 0.02501904  0.17471065 -0.02252095 -0.33232717]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 2 ] state=[ 0.02501904  0.17471065 -0.02252095 -0.33232717], action=0, reward=1.0, next_state=[ 0.02851325 -0.02008362 -0.0291675  -0.04683045]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 3 ] state=[ 0.02851325 -0.02008362 -0.0291675  -0.04683045], action=1, reward=1.0, next_state=[ 0.02811158  0.17544417 -0.03010411 -0.34857142]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 4 ] state=[ 0.02811158  0.17544417 -0.03010411 -0.34857142], action=1, reward=1.0, next_state=[ 0.03162046  0.37098106 -0.03707553 -0.65059314]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 5 ] state=[ 0.03162046  0.37098106 -0.03707553 -0.65059314], action=0, reward=1.0, next_state=[ 0.03904008  0.1763946  -0.0500874  -0.36981173]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 6 ] state=[ 0.03904008  0.1763946  -0.0500874  -0.36981173], action=0, reward=1.0, next_state=[ 0.04256798 -0.01798126 -0.05748363 -0.09333314]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 7 ] state=[ 0.04256798 -0.01798126 -0.05748363 -0.09333314], action=0, reward=1.0, next_state=[ 0.04220835 -0.2122342  -0.05935029  0.1806741 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 8 ] state=[ 0.04220835 -0.2122342  -0.05935029  0.1806741 ], action=1, reward=1.0, next_state=[ 0.03796367 -0.0163154  -0.05573681 -0.1301254 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 9 ] state=[ 0.03796367 -0.0163154  -0.05573681 -0.1301254 ], action=1, reward=1.0, next_state=[ 0.03763736  0.17955885 -0.05833932 -0.43985865]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 10 ] state=[ 0.03763736  0.17955885 -0.05833932 -0.43985865], action=0, reward=1.0, next_state=[ 0.04122854 -0.01469102 -0.06713649 -0.1661216 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 11 ] state=[ 0.04122854 -0.01469102 -0.06713649 -0.1661216 ], action=0, reward=1.0, next_state=[ 0.04093472 -0.20879091 -0.07045893  0.10464904]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 12 ] state=[ 0.04093472 -0.20879091 -0.07045893  0.10464904], action=0, reward=1.0, next_state=[ 0.0367589  -0.4028361  -0.06836594  0.37429683]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 13 ] state=[ 0.0367589  -0.4028361  -0.06836594  0.37429683], action=0, reward=1.0, next_state=[ 0.02870218 -0.5969237  -0.06088001  0.6446642 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 14 ] state=[ 0.02870218 -0.5969237  -0.06088001  0.6446642 ], action=1, reward=1.0, next_state=[ 0.0167637  -0.40100853 -0.04798672  0.33344822]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 15 ] state=[ 0.0167637  -0.40100853 -0.04798672  0.33344822], action=0, reward=1.0, next_state=[ 0.00874353 -0.59541581 -0.04131776  0.61062077]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 16 ] state=[ 0.00874353 -0.59541581 -0.04131776  0.61062077], action=1, reward=1.0, next_state=[-0.00316479 -0.39974143 -0.02910534  0.30521573]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 17 ] state=[-0.00316479 -0.39974143 -0.02910534  0.30521573], action=1, reward=1.0, next_state=[-0.01115961 -0.20421706 -0.02300103  0.00349763]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 18 ] state=[-0.01115961 -0.20421706 -0.02300103  0.00349763], action=1, reward=1.0, next_state=[-0.01524396 -0.00877292 -0.02293108 -0.29635274]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 19 ] state=[-0.01524396 -0.00877292 -0.02293108 -0.29635274], action=1, reward=1.0, next_state=[-0.01541941  0.18666829 -0.02885813 -0.59617864]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 20 ] state=[-0.01541941  0.18666829 -0.02885813 -0.59617864], action=1, reward=1.0, next_state=[-0.01168605  0.38218197 -0.0407817  -0.89781015]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 21 ] state=[-0.01168605  0.38218197 -0.0407817  -0.89781015], action=0, reward=1.0, next_state=[-0.00404241  0.18763584 -0.05873791 -0.61822009]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 22 ] state=[-0.00404241  0.18763584 -0.05873791 -0.61822009], action=0, reward=1.0, next_state=[-2.89691896e-04 -6.61856800e-03 -7.11023096e-02 -3.44600006e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 23 ] state=[-2.89691896e-04 -6.61856800e-03 -7.11023096e-02 -3.44600006e-01], action=1, reward=1.0, next_state=[-4.22063257e-04  1.89439028e-01 -7.79943097e-02 -6.58829800e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 24 ] state=[-4.22063257e-04  1.89439028e-01 -7.79943097e-02 -6.58829800e-01], action=1, reward=1.0, next_state=[ 0.00336672  0.38555489 -0.09117091 -0.97501639]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 25 ] state=[ 0.00336672  0.38555489 -0.09117091 -0.97501639], action=0, reward=1.0, next_state=[ 0.01107782  0.19176633 -0.11067123 -0.71230793]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 26 ] state=[ 0.01107782  0.19176633 -0.11067123 -0.71230793], action=1, reward=1.0, next_state=[ 0.01491314  0.38823251 -0.12491739 -1.03767525]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 27 ] state=[ 0.01491314  0.38823251 -0.12491739 -1.03767525], action=0, reward=1.0, next_state=[ 0.02267779  0.19497211 -0.1456709  -0.78667376]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 28 ] state=[ 0.02267779  0.19497211 -0.1456709  -0.78667376], action=0, reward=1.0, next_state=[ 0.02657723  0.00211955 -0.16140437 -0.54313469]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 29 ] state=[ 0.02657723  0.00211955 -0.16140437 -0.54313469], action=0, reward=1.0, next_state=[ 0.02661962 -0.19041005 -0.17226707 -0.305341  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 30 ] state=[ 0.02661962 -0.19041005 -0.17226707 -0.305341  ], action=0, reward=1.0, next_state=[ 0.02281142 -0.38271217 -0.17837389 -0.0715537 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 31 ] state=[ 0.02281142 -0.38271217 -0.17837389 -0.0715537 ], action=0, reward=1.0, next_state=[ 0.01515718 -0.5748881  -0.17980496  0.1599722 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 265 ][ timestamp 32 ] state=[ 0.01515718 -0.5748881  -0.17980496  0.1599722 ], action=0, reward=1.0, next_state=[ 0.00365942 -0.76704139 -0.17660552  0.39097718]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 33 ] state=[ 0.00365942 -0.76704139 -0.17660552  0.39097718], action=0, reward=1.0, next_state=[-0.01168141 -0.95927493 -0.16878597  0.62318988]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 34 ] state=[-0.01168141 -0.95927493 -0.16878597  0.62318988], action=0, reward=1.0, next_state=[-0.03086691 -1.15168831 -0.15632218  0.85832069]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 35 ] state=[-0.03086691 -1.15168831 -0.15632218  0.85832069], action=0, reward=1.0, next_state=[-0.05390067 -1.34437488 -0.13915576  1.09805451]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 36 ] state=[-0.05390067 -1.34437488 -0.13915576  1.09805451], action=0, reward=1.0, next_state=[-0.08078817 -1.5374182  -0.11719467  1.34404051]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 37 ] state=[-0.08078817 -1.5374182  -0.11719467  1.34404051], action=1, reward=1.0, next_state=[-0.11153654 -1.34103338 -0.09031386  1.0171075 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 38 ] state=[-0.11153654 -1.34103338 -0.09031386  1.0171075 ], action=0, reward=1.0, next_state=[-0.1383572  -1.5348428  -0.06997171  1.28012063]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 39 ] state=[-0.1383572  -1.5348428  -0.06997171  1.28012063], action=1, reward=1.0, next_state=[-0.16905406 -1.33890248 -0.0443693   0.96637445]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 40 ] state=[-0.16905406 -1.33890248 -0.0443693   0.96637445], action=1, reward=1.0, next_state=[-0.19583211 -1.14321363 -0.02504181  0.66008976]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 41 ] state=[-0.19583211 -1.14321363 -0.02504181  0.66008976], action=1, reward=1.0, next_state=[-0.21869638 -0.9477523  -0.01184001  0.35962817]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 42 ] state=[-0.21869638 -0.9477523  -0.01184001  0.35962817], action=1, reward=1.0, next_state=[-0.23765143 -0.75246406 -0.00464745  0.06323546]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 43 ] state=[-0.23765143 -0.75246406 -0.00464745  0.06323546], action=0, reward=1.0, next_state=[-0.25270071 -0.94751907 -0.00338274  0.35444847]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 44 ] state=[-0.25270071 -0.94751907 -0.00338274  0.35444847], action=0, reward=1.0, next_state=[-0.27165109 -1.14259276  0.00370623  0.6460628 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 45 ] state=[-0.27165109 -1.14259276  0.00370623  0.6460628 ], action=1, reward=1.0, next_state=[-0.29450295 -0.94752265  0.01662748  0.35454927]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 46 ] state=[-0.29450295 -0.94752265  0.01662748  0.35454927], action=0, reward=1.0, next_state=[-0.3134534  -1.14287703  0.02371847  0.65242859]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 47 ] state=[-0.3134534  -1.14287703  0.02371847  0.65242859], action=1, reward=1.0, next_state=[-0.33631094 -0.94809327  0.03676704  0.36730771]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 48 ] state=[-0.33631094 -0.94809327  0.03676704  0.36730771], action=1, reward=1.0, next_state=[-0.3552728  -0.75351254  0.0441132   0.08644094]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 49 ] state=[-0.3552728  -0.75351254  0.0441132   0.08644094], action=1, reward=1.0, next_state=[-0.37034305 -0.55904976  0.04584201 -0.19200437]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 50 ] state=[-0.37034305 -0.55904976  0.04584201 -0.19200437], action=1, reward=1.0, next_state=[-0.38152405 -0.36461256  0.04200193 -0.46988094]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 51 ] state=[-0.38152405 -0.36461256  0.04200193 -0.46988094], action=0, reward=1.0, next_state=[-0.3888163  -0.56030188  0.03260431 -0.16426091]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 52 ] state=[-0.3888163  -0.56030188  0.03260431 -0.16426091], action=0, reward=1.0, next_state=[-0.40002234 -0.75587503  0.02931909  0.13852687]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 53 ] state=[-0.40002234 -0.75587503  0.02931909  0.13852687], action=0, reward=1.0, next_state=[-0.41513984 -0.95140438  0.03208963  0.44031342]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 54 ] state=[-0.41513984 -0.95140438  0.03208963  0.44031342], action=1, reward=1.0, next_state=[-0.43416793 -0.75675092  0.0408959   0.15791627]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 55 ] state=[-0.43416793 -0.75675092  0.0408959   0.15791627], action=1, reward=1.0, next_state=[-0.44930295 -0.56223761  0.04405422 -0.12158968]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 56 ] state=[-0.44930295 -0.56223761  0.04405422 -0.12158968], action=1, reward=1.0, next_state=[-0.4605477  -0.36777361  0.04162243 -0.40005492]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 57 ] state=[-0.4605477  -0.36777361  0.04162243 -0.40005492], action=0, reward=1.0, next_state=[-0.46790317 -0.56346051  0.03362133 -0.09454534]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 58 ] state=[-0.46790317 -0.56346051  0.03362133 -0.09454534], action=0, reward=1.0, next_state=[-0.47917238 -0.75904781  0.03173042  0.20855262]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 59 ] state=[-0.47917238 -0.75904781  0.03173042  0.20855262], action=1, reward=1.0, next_state=[-0.49435334 -0.56439361  0.03590147 -0.07395453]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 60 ] state=[-0.49435334 -0.56439361  0.03590147 -0.07395453], action=0, reward=1.0, next_state=[-0.50564121 -0.76001136  0.03442238  0.22983578]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 61 ] state=[-0.50564121 -0.76001136  0.03442238  0.22983578], action=0, reward=1.0, next_state=[-0.52084144 -0.95560787  0.0390191   0.53317492]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 62 ] state=[-0.52084144 -0.95560787  0.0390191   0.53317492], action=1, reward=1.0, next_state=[-0.53995359 -0.76105578  0.0496826   0.25303762]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 63 ] state=[-0.53995359 -0.76105578  0.0496826   0.25303762], action=0, reward=1.0, next_state=[-0.55517471 -0.95685065  0.05474335  0.5609682 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 64 ] state=[-0.55517471 -0.95685065  0.05474335  0.5609682 ], action=0, reward=1.0, next_state=[-0.57431172 -1.15269643  0.06596271  0.8703833 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 65 ] state=[-0.57431172 -1.15269643  0.06596271  0.8703833 ], action=1, reward=1.0, next_state=[-0.59736565 -0.95853075  0.08337038  0.59914715]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 66 ] state=[-0.59736565 -0.95853075  0.08337038  0.59914715], action=1, reward=1.0, next_state=[-0.61653627 -0.76466812  0.09535332  0.33384573]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 67 ] state=[-0.61653627 -0.76466812  0.09535332  0.33384573], action=1, reward=1.0, next_state=[-0.63182963 -0.57102349  0.10203024  0.07268969]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 68 ] state=[-0.63182963 -0.57102349  0.10203024  0.07268969], action=1, reward=1.0, next_state=[-0.6432501  -0.37750104  0.10348403 -0.18613947]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 69 ] state=[-0.6432501  -0.37750104  0.10348403 -0.18613947], action=0, reward=1.0, next_state=[-0.65080012 -0.57393966  0.09976124  0.13731217]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 70 ] state=[-0.65080012 -0.57393966  0.09976124  0.13731217], action=1, reward=1.0, next_state=[-0.66227891 -0.38037761  0.10250749 -0.12230613]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 71 ] state=[-0.66227891 -0.38037761  0.10250749 -0.12230613], action=1, reward=1.0, next_state=[-0.66988646 -0.18686227  0.10006136 -0.38097097]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 72 ] state=[-0.66988646 -0.18686227  0.10006136 -0.38097097], action=0, reward=1.0, next_state=[-0.67362371 -0.38325214  0.09244194 -0.05849068]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 73 ] state=[-0.67362371 -0.38325214  0.09244194 -0.05849068], action=0, reward=1.0, next_state=[-0.68128875 -0.57956959  0.09127213  0.2618674 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 74 ] state=[-0.68128875 -0.57956959  0.09127213  0.2618674 ], action=0, reward=1.0, next_state=[-0.69288014 -0.77586784  0.09650948  0.58188593]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 75 ] state=[-0.69288014 -0.77586784  0.09650948  0.58188593], action=1, reward=1.0, next_state=[-0.7083975  -0.58222113  0.1081472   0.32109731]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 76 ] state=[-0.7083975  -0.58222113  0.1081472   0.32109731], action=0, reward=1.0, next_state=[-0.72004192 -0.77870366  0.11456914  0.64583259]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 265 ][ timestamp 77 ] state=[-0.72004192 -0.77870366  0.11456914  0.64583259], action=0, reward=1.0, next_state=[-0.735616   -0.97521992  0.12748579  0.97228417]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 78 ] state=[-0.735616   -0.97521992  0.12748579  0.97228417], action=0, reward=1.0, next_state=[-0.75512039 -1.17180069  0.14693148  1.30214173]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 79 ] state=[-0.75512039 -1.17180069  0.14693148  1.30214173], action=1, reward=1.0, next_state=[-0.77855641 -0.97881623  0.17297431  1.05882675]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 80 ] state=[-0.77855641 -0.97881623  0.17297431  1.05882675], action=1, reward=1.0, next_state=[-0.79813273 -0.78635471  0.19415085  0.8250438 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ][ timestamp 81 ] state=[-0.79813273 -0.78635471  0.19415085  0.8250438 ], action=1, reward=-1.0, next_state=[-0.81385983 -0.59434257  0.21065172  0.59915934]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 265: Exploration_rate=0.01. Score=81.\n",
      "[ episode 266 ] state=[ 0.03780572 -0.03652694 -0.00262734  0.0240608 ]\n",
      "[ episode 266 ][ timestamp 1 ] state=[ 0.03780572 -0.03652694 -0.00262734  0.0240608 ], action=0, reward=1.0, next_state=[ 0.03707518 -0.23161111 -0.00214612  0.31591361]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 2 ] state=[ 0.03707518 -0.23161111 -0.00214612  0.31591361], action=1, reward=1.0, next_state=[ 0.03244296 -0.03645865  0.00417215  0.02255465]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 3 ] state=[ 0.03244296 -0.03645865  0.00417215  0.02255465], action=0, reward=1.0, next_state=[ 0.03171378 -0.23164019  0.00462324  0.31655101]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 4 ] state=[ 0.03171378 -0.23164019  0.00462324  0.31655101], action=1, reward=1.0, next_state=[ 0.02708098 -0.03658439  0.01095426  0.02532967]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 5 ] state=[ 0.02708098 -0.03658439  0.01095426  0.02532967], action=0, reward=1.0, next_state=[ 0.02634929 -0.23186171  0.01146086  0.32144857]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 6 ] state=[ 0.02634929 -0.23186171  0.01146086  0.32144857], action=1, reward=1.0, next_state=[ 0.02171206 -0.03690483  0.01788983  0.03240187]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 7 ] state=[ 0.02171206 -0.03690483  0.01788983  0.03240187], action=0, reward=1.0, next_state=[ 0.02097396 -0.2322787   0.01853787  0.33067511]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 8 ] state=[ 0.02097396 -0.2322787   0.01853787  0.33067511], action=1, reward=1.0, next_state=[ 0.01632839 -0.03742547  0.02515137  0.0438953 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 9 ] state=[ 0.01632839 -0.03742547  0.02515137  0.0438953 ], action=1, reward=1.0, next_state=[ 0.01557988  0.15732696  0.02602927 -0.24074723]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 10 ] state=[ 0.01557988  0.15732696  0.02602927 -0.24074723], action=1, reward=1.0, next_state=[ 0.01872642  0.3520676   0.02121433 -0.5251075 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 11 ] state=[ 0.01872642  0.3520676   0.02121433 -0.5251075 ], action=1, reward=1.0, next_state=[ 0.02576777  0.54688469  0.01071218 -0.81103083]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 12 ] state=[ 0.02576777  0.54688469  0.01071218 -0.81103083], action=0, reward=1.0, next_state=[ 0.03670546  0.35161764 -0.00550844 -0.51499774]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 13 ] state=[ 0.03670546  0.35161764 -0.00550844 -0.51499774], action=1, reward=1.0, next_state=[ 0.04373782  0.54681673 -0.01580839 -0.8094114 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 14 ] state=[ 0.04373782  0.54681673 -0.01580839 -0.8094114 ], action=0, reward=1.0, next_state=[ 0.05467415  0.35191491 -0.03199662 -0.52174268]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 15 ] state=[ 0.05467415  0.35191491 -0.03199662 -0.52174268], action=0, reward=1.0, next_state=[ 0.06171245  0.15725761 -0.04243147 -0.23931158]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 16 ] state=[ 0.06171245  0.15725761 -0.04243147 -0.23931158], action=0, reward=1.0, next_state=[ 0.0648576  -0.03723331 -0.0472177   0.0396911 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 17 ] state=[ 0.0648576  -0.03723331 -0.0472177   0.0396911 ], action=0, reward=1.0, next_state=[ 0.06411293 -0.23164747 -0.04642388  0.31711048]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 18 ] state=[ 0.06411293 -0.23164747 -0.04642388  0.31711048], action=0, reward=1.0, next_state=[ 0.05947998 -0.42607851 -0.04008167  0.59479909]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 19 ] state=[ 0.05947998 -0.42607851 -0.04008167  0.59479909], action=0, reward=1.0, next_state=[ 0.05095841 -0.62061722 -0.02818569  0.87459193]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 20 ] state=[ 0.05095841 -0.62061722 -0.02818569  0.87459193], action=1, reward=1.0, next_state=[ 0.03854607 -0.42512365 -0.01069385  0.57318255]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 21 ] state=[ 0.03854607 -0.42512365 -0.01069385  0.57318255], action=0, reward=1.0, next_state=[ 3.00435971e-02 -6.20094043e-01  7.69798596e-04  8.62477488e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 22 ] state=[ 3.00435971e-02 -6.20094043e-01  7.69798596e-04  8.62477488e-01], action=1, reward=1.0, next_state=[ 0.01764172 -0.42498258  0.01801935  0.5700367 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 23 ] state=[ 0.01764172 -0.42498258  0.01801935  0.5700367 ], action=1, reward=1.0, next_state=[ 0.00914206 -0.23011792  0.02942008  0.28308456]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 24 ] state=[ 0.00914206 -0.23011792  0.02942008  0.28308456], action=0, reward=1.0, next_state=[ 0.00453971 -0.42564687  0.03508177  0.58489933]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 25 ] state=[ 0.00453971 -0.42564687  0.03508177  0.58489933], action=1, reward=1.0, next_state=[-0.00397323 -0.23103344  0.04677976  0.30347073]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 26 ] state=[-0.00397323 -0.23103344  0.04677976  0.30347073], action=1, reward=1.0, next_state=[-0.0085939  -0.03660831  0.05284917  0.02590031]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 27 ] state=[-0.0085939  -0.03660831  0.05284917  0.02590031], action=0, reward=1.0, next_state=[-0.00932607 -0.23244677  0.05336718  0.33477828]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 28 ] state=[-0.00932607 -0.23244677  0.05336718  0.33477828], action=1, reward=1.0, next_state=[-0.013975   -0.03812335  0.06006275  0.05939064]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 29 ] state=[-0.013975   -0.03812335  0.06006275  0.05939064], action=0, reward=1.0, next_state=[-0.01473747 -0.23405276  0.06125056  0.37040263]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 30 ] state=[-0.01473747 -0.23405276  0.06125056  0.37040263], action=0, reward=1.0, next_state=[-0.01941852 -0.42998902  0.06865861  0.68175229]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 31 ] state=[-0.01941852 -0.42998902  0.06865861  0.68175229], action=1, reward=1.0, next_state=[-0.0280183  -0.23588438  0.08229366  0.4114511 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 32 ] state=[-0.0280183  -0.23588438  0.08229366  0.4114511 ], action=1, reward=1.0, next_state=[-0.03273599 -0.04201959  0.09052268  0.14580507]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 33 ] state=[-0.03273599 -0.04201959  0.09052268  0.14580507], action=1, reward=1.0, next_state=[-0.03357638  0.15169725  0.09343878 -0.11700313]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 34 ] state=[-0.03357638  0.15169725  0.09343878 -0.11700313], action=1, reward=1.0, next_state=[-0.03054244  0.34536482  0.09109872 -0.37880621]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 35 ] state=[-0.03054244  0.34536482  0.09109872 -0.37880621], action=0, reward=1.0, next_state=[-0.02363514  0.1490752   0.08352259 -0.05884669]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 36 ] state=[-0.02363514  0.1490752   0.08352259 -0.05884669], action=1, reward=1.0, next_state=[-0.02065364  0.34290645  0.08234566 -0.32405293]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 37 ] state=[-0.02065364  0.34290645  0.08234566 -0.32405293], action=1, reward=1.0, next_state=[-0.01379551  0.53676524  0.0758646  -0.5896735 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 38 ] state=[-0.01379551  0.53676524  0.0758646  -0.5896735 ], action=1, reward=1.0, next_state=[-0.0030602   0.73074752  0.06407113 -0.85752719]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 39 ] state=[-0.0030602   0.73074752  0.06407113 -0.85752719], action=1, reward=1.0, next_state=[ 0.01155475  0.9249408   0.04692059 -1.12939539]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 266 ][ timestamp 40 ] state=[ 0.01155475  0.9249408   0.04692059 -1.12939539], action=0, reward=1.0, next_state=[ 0.03005356  0.72923679  0.02433268 -0.82237287]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 41 ] state=[ 0.03005356  0.72923679  0.02433268 -0.82237287], action=1, reward=1.0, next_state=[ 0.0446383   0.92401753  0.00788522 -1.10730439]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 42 ] state=[ 0.0446383   0.92401753  0.00788522 -1.10730439], action=0, reward=1.0, next_state=[ 0.06311865  0.72879282 -0.01426086 -0.81215819]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 43 ] state=[ 0.06311865  0.72879282 -0.01426086 -0.81215819], action=0, reward=1.0, next_state=[ 0.07769451  0.53386909 -0.03050403 -0.52399487]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 44 ] state=[ 0.07769451  0.53386909 -0.03050403 -0.52399487], action=1, reward=1.0, next_state=[ 0.08837189  0.72940676 -0.04098393 -0.82613173]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 45 ] state=[ 0.08837189  0.72940676 -0.04098393 -0.82613173], action=1, reward=1.0, next_state=[ 0.10296002  0.92506451 -0.05750656 -1.1314178 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 46 ] state=[ 0.10296002  0.92506451 -0.05750656 -1.1314178 ], action=1, reward=1.0, next_state=[ 0.12146131  1.12089026 -0.08013492 -1.44156848]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 47 ] state=[ 0.12146131  1.12089026 -0.08013492 -1.44156848], action=1, reward=1.0, next_state=[ 0.14387912  1.31690235 -0.10896629 -1.75817754]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 48 ] state=[ 0.14387912  1.31690235 -0.10896629 -1.75817754], action=1, reward=1.0, next_state=[ 0.17021716  1.51307696 -0.14412984 -2.08266694]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 49 ] state=[ 0.17021716  1.51307696 -0.14412984 -2.08266694], action=0, reward=1.0, next_state=[ 0.2004787   1.31967767 -0.18578318 -1.83780357]\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ][ timestamp 50 ] state=[ 0.2004787   1.31967767 -0.18578318 -1.83780357], action=0, reward=-1.0, next_state=[ 0.22687226  1.12703153 -0.22253925 -1.60811356]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 266: Exploration_rate=0.01. Score=50.\n",
      "[ episode 267 ] state=[-0.01839154  0.02767723 -0.00143249 -0.03258242]\n",
      "[ episode 267 ][ timestamp 1 ] state=[-0.01839154  0.02767723 -0.00143249 -0.03258242], action=1, reward=1.0, next_state=[-0.017838    0.22281969 -0.00208414 -0.32571696]\n",
      "[ Experience replay ] starts\n",
      "[ episode 267 ][ timestamp 2 ] state=[-0.017838    0.22281969 -0.00208414 -0.32571696], action=1, reward=1.0, next_state=[-0.01338161  0.41797125 -0.00859848 -0.61905641]\n",
      "[ Experience replay ] starts\n",
      "[ episode 267 ][ timestamp 3 ] state=[-0.01338161  0.41797125 -0.00859848 -0.61905641], action=1, reward=1.0, next_state=[-0.00502218  0.61321224 -0.02097961 -0.91443499]\n",
      "[ Experience replay ] starts\n",
      "[ episode 267 ][ timestamp 4 ] state=[-0.00502218  0.61321224 -0.02097961 -0.91443499], action=1, reward=1.0, next_state=[ 0.00724206  0.80861158 -0.03926831 -1.21363704]\n",
      "[ Experience replay ] starts\n",
      "[ episode 267 ][ timestamp 5 ] state=[ 0.00724206  0.80861158 -0.03926831 -1.21363704], action=0, reward=1.0, next_state=[ 0.0234143   0.61401774 -0.06354105 -0.93351322]\n",
      "[ Experience replay ] starts\n",
      "[ episode 267 ][ timestamp 6 ] state=[ 0.0234143   0.61401774 -0.06354105 -0.93351322], action=0, reward=1.0, next_state=[ 0.03569465  0.41980792 -0.08221131 -0.66145488]\n",
      "[ Experience replay ] starts\n",
      "[ episode 267 ][ timestamp 7 ] state=[ 0.03569465  0.41980792 -0.08221131 -0.66145488], action=0, reward=1.0, next_state=[ 0.04409081  0.22592031 -0.09544041 -0.39574865]\n",
      "[ Experience replay ] starts\n",
      "[ episode 267 ][ timestamp 8 ] state=[ 0.04409081  0.22592031 -0.09544041 -0.39574865], action=0, reward=1.0, next_state=[ 0.04860922  0.03227294 -0.10335538 -0.13461641]\n",
      "[ Experience replay ] starts\n",
      "[ episode 267 ][ timestamp 9 ] state=[ 0.04860922  0.03227294 -0.10335538 -0.13461641], action=0, reward=1.0, next_state=[ 0.04925467 -0.1612284  -0.10604771  0.12375427]\n",
      "[ Experience replay ] starts\n",
      "[ episode 267 ][ timestamp 10 ] state=[ 0.04925467 -0.1612284  -0.10604771  0.12375427], action=0, reward=1.0, next_state=[ 0.04603011 -0.35468383 -0.10357262  0.38118762]\n",
      "[ Experience replay ] starts\n",
      "[ episode 267 ][ timestamp 11 ] state=[ 0.04603011 -0.35468383 -0.10357262  0.38118762], action=0, reward=1.0, next_state=[ 0.03893643 -0.54819431 -0.09594887  0.6395019 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 267 ][ timestamp 12 ] state=[ 0.03893643 -0.54819431 -0.09594887  0.6395019 ], action=0, reward=1.0, next_state=[ 0.02797254 -0.74185681 -0.08315883  0.90049381]\n",
      "[ Experience replay ] starts\n",
      "[ episode 267 ][ timestamp 13 ] state=[ 0.02797254 -0.74185681 -0.08315883  0.90049381], action=0, reward=1.0, next_state=[ 0.01313541 -0.93575945 -0.06514896  1.16592214]\n",
      "[ Experience replay ] starts\n",
      "[ episode 267 ][ timestamp 14 ] state=[ 0.01313541 -0.93575945 -0.06514896  1.16592214], action=0, reward=1.0, next_state=[-0.00557978 -1.12997582 -0.04183051  1.43748842]\n",
      "[ Experience replay ] starts\n",
      "[ episode 267 ][ timestamp 15 ] state=[-0.00557978 -1.12997582 -0.04183051  1.43748842], action=0, reward=1.0, next_state=[-0.0281793  -1.32455797 -0.01308075  1.71681173]\n",
      "[ Experience replay ] starts\n",
      "[ episode 267 ][ timestamp 16 ] state=[-0.0281793  -1.32455797 -0.01308075  1.71681173], action=0, reward=1.0, next_state=[-0.05467046 -1.51952752  0.02125549  2.00539541]\n",
      "[ Experience replay ] starts\n",
      "[ episode 267 ][ timestamp 17 ] state=[-0.05467046 -1.51952752  0.02125549  2.00539541], action=1, reward=1.0, next_state=[-0.08506101 -1.32463336  0.0613634   1.71936885]\n",
      "[ Experience replay ] starts\n",
      "[ episode 267 ][ timestamp 18 ] state=[-0.08506101 -1.32463336  0.0613634   1.71936885], action=1, reward=1.0, next_state=[-0.11155368 -1.13026586  0.09575077  1.44639586]\n",
      "[ Experience replay ] starts\n",
      "[ episode 267 ][ timestamp 19 ] state=[-0.11155368 -1.13026586  0.09575077  1.44639586], action=0, reward=1.0, next_state=[-0.13415899 -1.32642632  0.12467869  1.76739647]\n",
      "[ Experience replay ] starts\n",
      "[ episode 267 ][ timestamp 20 ] state=[-0.13415899 -1.32642632  0.12467869  1.76739647], action=1, reward=1.0, next_state=[-0.16068752 -1.13291396  0.16002662  1.51594174]\n",
      "[ Experience replay ] starts\n",
      "[ episode 267 ][ timestamp 21 ] state=[-0.16068752 -1.13291396  0.16002662  1.51594174], action=1, reward=1.0, next_state=[-0.1833458  -0.9400491   0.19034546  1.27718807]\n",
      "[ Experience replay ] starts\n",
      "[ episode 267 ][ timestamp 22 ] state=[-0.1833458  -0.9400491   0.19034546  1.27718807], action=1, reward=-1.0, next_state=[-0.20214678 -0.74779399  0.21588922  1.04963814]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 267: Exploration_rate=0.01. Score=22.\n",
      "[ episode 268 ] state=[-0.04235456  0.04385872 -0.02321895 -0.0468943 ]\n",
      "[ episode 268 ][ timestamp 1 ] state=[-0.04235456  0.04385872 -0.02321895 -0.0468943 ], action=0, reward=1.0, next_state=[-0.04147739 -0.15092272 -0.02415683  0.23837336]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 2 ] state=[-0.04147739 -0.15092272 -0.02415683  0.23837336], action=1, reward=1.0, next_state=[-0.04449584  0.04453586 -0.01938937 -0.06183039]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 3 ] state=[-0.04449584  0.04453586 -0.01938937 -0.06183039], action=1, reward=1.0, next_state=[-0.04360512  0.23993036 -0.02062597 -0.36056717]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 4 ] state=[-0.04360512  0.23993036 -0.02062597 -0.36056717], action=0, reward=1.0, next_state=[-0.03880652  0.04510758 -0.02783732 -0.07445877]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 5 ] state=[-0.03880652  0.04510758 -0.02783732 -0.07445877], action=1, reward=1.0, next_state=[-0.03790437  0.24061732 -0.02932649 -0.37579287]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 6 ] state=[-0.03790437  0.24061732 -0.02932649 -0.37579287], action=0, reward=1.0, next_state=[-0.03309202  0.04592392 -0.03684235 -0.0924991 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 7 ] state=[-0.03309202  0.04592392 -0.03684235 -0.0924991 ], action=1, reward=1.0, next_state=[-0.03217354  0.24155404 -0.03869233 -0.39657435]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 268 ][ timestamp 8 ] state=[-0.03217354  0.24155404 -0.03869233 -0.39657435], action=0, reward=1.0, next_state=[-0.02734246  0.04700181 -0.04662382 -0.11633712]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 9 ] state=[-0.02734246  0.04700181 -0.04662382 -0.11633712], action=1, reward=1.0, next_state=[-0.02640242  0.24275971 -0.04895056 -0.42335733]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 10 ] state=[-0.02640242  0.24275971 -0.04895056 -0.42335733], action=0, reward=1.0, next_state=[-0.02154723  0.04836416 -0.05741771 -0.14649901]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 11 ] state=[-0.02154723  0.04836416 -0.05741771 -0.14649901], action=0, reward=1.0, next_state=[-0.02057995 -0.14589054 -0.06034769  0.12753133]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 12 ] state=[-0.02057995 -0.14589054 -0.06034769  0.12753133], action=0, reward=1.0, next_state=[-0.02349776 -0.34009838 -0.05779706  0.40058135]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 13 ] state=[-0.02349776 -0.34009838 -0.05779706  0.40058135], action=1, reward=1.0, next_state=[-0.03029972 -0.14420625 -0.04978544  0.09025092]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 14 ] state=[-0.03029972 -0.14420625 -0.04978544  0.09025092], action=1, reward=1.0, next_state=[-0.03318385  0.05159264 -0.04798042 -0.21771438]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 15 ] state=[-0.03318385  0.05159264 -0.04798042 -0.21771438], action=0, reward=1.0, next_state=[-0.032152   -0.14281175 -0.05233471  0.05945578]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 16 ] state=[-0.032152   -0.14281175 -0.05233471  0.05945578], action=1, reward=1.0, next_state=[-0.03500823  0.05301999 -0.05114559 -0.24926903]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 17 ] state=[-0.03500823  0.05301999 -0.05114559 -0.24926903], action=0, reward=1.0, next_state=[-0.03394783 -0.14133566 -0.05613097  0.02685298]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 18 ] state=[-0.03394783 -0.14133566 -0.05613097  0.02685298], action=0, reward=1.0, next_state=[-0.03677454 -0.3356096  -0.05559391  0.3013111 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 19 ] state=[-0.03677454 -0.3356096  -0.05559391  0.3013111 ], action=0, reward=1.0, next_state=[-0.04348674 -0.52989693 -0.04956769  0.57595565]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 20 ] state=[-0.04348674 -0.52989693 -0.04956769  0.57595565], action=1, reward=1.0, next_state=[-0.05408468 -0.33411647 -0.03804858  0.26807872]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 21 ] state=[-0.05408468 -0.33411647 -0.03804858  0.26807872], action=1, reward=1.0, next_state=[-0.060767   -0.13847275 -0.032687   -0.03635803]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 22 ] state=[-0.060767   -0.13847275 -0.032687   -0.03635803], action=0, reward=1.0, next_state=[-0.06353646 -0.33311109 -0.03341416  0.24583525]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 23 ] state=[-0.06353646 -0.33311109 -0.03341416  0.24583525], action=0, reward=1.0, next_state=[-0.07019868 -0.52774027 -0.02849746  0.52779411]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 24 ] state=[-0.07019868 -0.52774027 -0.02849746  0.52779411], action=0, reward=1.0, next_state=[-0.08075349 -0.72244992 -0.01794157  0.81136289]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 25 ] state=[-0.08075349 -0.72244992 -0.01794157  0.81136289], action=0, reward=1.0, next_state=[-0.09520249 -0.91732155 -0.00171432  1.09834875]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 26 ] state=[-0.09520249 -0.91732155 -0.00171432  1.09834875], action=0, reward=1.0, next_state=[-0.11354892 -1.11242089  0.02025266  1.39049332]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 27 ] state=[-0.11354892 -1.11242089  0.02025266  1.39049332], action=0, reward=1.0, next_state=[-0.13579733 -1.30778915  0.04806252  1.68943949]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 28 ] state=[-0.13579733 -1.30778915  0.04806252  1.68943949], action=0, reward=1.0, next_state=[-0.16195312 -1.5034325   0.08185131  1.99669057]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 29 ] state=[-0.16195312 -1.5034325   0.08185131  1.99669057], action=1, reward=1.0, next_state=[-0.19202177 -1.30925615  0.12178513  1.73043861]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 30 ] state=[-0.19202177 -1.30925615  0.12178513  1.73043861], action=1, reward=1.0, next_state=[-0.21820689 -1.11571766  0.1563939   1.47799746]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 31 ] state=[-0.21820689 -1.11571766  0.1563939   1.47799746], action=1, reward=1.0, next_state=[-0.24052124 -0.92281271  0.18595385  1.23796414]\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ][ timestamp 32 ] state=[-0.24052124 -0.92281271  0.18595385  1.23796414], action=1, reward=-1.0, next_state=[-0.2589775  -0.73050112  0.21071313  1.00882573]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 268: Exploration_rate=0.01. Score=32.\n",
      "[ episode 269 ] state=[ 0.0456173   0.02993018  0.0205983  -0.02933782]\n",
      "[ episode 269 ][ timestamp 1 ] state=[ 0.0456173   0.02993018  0.0205983  -0.02933782], action=1, reward=1.0, next_state=[ 0.0462159   0.22475078  0.02001154 -0.31545125]\n",
      "[ Experience replay ] starts\n",
      "[ episode 269 ][ timestamp 2 ] state=[ 0.0462159   0.22475078  0.02001154 -0.31545125], action=1, reward=1.0, next_state=[ 0.05071092  0.41958205  0.01370251 -0.60175664]\n",
      "[ Experience replay ] starts\n",
      "[ episode 269 ][ timestamp 3 ] state=[ 0.05071092  0.41958205  0.01370251 -0.60175664], action=0, reward=1.0, next_state=[ 0.05910256  0.22427113  0.00166738 -0.30478935]\n",
      "[ Experience replay ] starts\n",
      "[ episode 269 ][ timestamp 4 ] state=[ 0.05910256  0.22427113  0.00166738 -0.30478935], action=0, reward=1.0, next_state=[ 0.06358798  0.02912546 -0.00442841 -0.01158104]\n",
      "[ Experience replay ] starts\n",
      "[ episode 269 ][ timestamp 5 ] state=[ 0.06358798  0.02912546 -0.00442841 -0.01158104], action=1, reward=1.0, next_state=[ 0.06417049  0.22431064 -0.00466003 -0.30565788]\n",
      "[ Experience replay ] starts\n",
      "[ episode 269 ][ timestamp 6 ] state=[ 0.06417049  0.22431064 -0.00466003 -0.30565788], action=1, reward=1.0, next_state=[ 0.0686567   0.41949868 -0.01077318 -0.59980682]\n",
      "[ Experience replay ] starts\n",
      "[ episode 269 ][ timestamp 7 ] state=[ 0.0686567   0.41949868 -0.01077318 -0.59980682], action=1, reward=1.0, next_state=[ 0.07704668  0.61476969 -0.02276932 -0.89586358]\n",
      "[ Experience replay ] starts\n",
      "[ episode 269 ][ timestamp 8 ] state=[ 0.07704668  0.61476969 -0.02276932 -0.89586358], action=0, reward=1.0, next_state=[ 0.08934207  0.41996373 -0.04068659 -0.61042399]\n",
      "[ Experience replay ] starts\n",
      "[ episode 269 ][ timestamp 9 ] state=[ 0.08934207  0.41996373 -0.04068659 -0.61042399], action=0, reward=1.0, next_state=[ 0.09774135  0.2254334  -0.05289507 -0.33082854]\n",
      "[ Experience replay ] starts\n",
      "[ episode 269 ][ timestamp 10 ] state=[ 0.09774135  0.2254334  -0.05289507 -0.33082854], action=1, reward=1.0, next_state=[ 0.10225001  0.42126683 -0.05951164 -0.63971174]\n",
      "[ Experience replay ] starts\n",
      "[ episode 269 ][ timestamp 11 ] state=[ 0.10225001  0.42126683 -0.05951164 -0.63971174], action=0, reward=1.0, next_state=[ 0.11067535  0.22702288 -0.07230588 -0.36634772]\n",
      "[ Experience replay ] starts\n",
      "[ episode 269 ][ timestamp 12 ] state=[ 0.11067535  0.22702288 -0.07230588 -0.36634772], action=1, reward=1.0, next_state=[ 0.11521581  0.42309386 -0.07963283 -0.68092512]\n",
      "[ Experience replay ] starts\n",
      "[ episode 269 ][ timestamp 13 ] state=[ 0.11521581  0.42309386 -0.07963283 -0.68092512], action=1, reward=1.0, next_state=[ 0.12367768  0.61922624 -0.09325133 -0.99757869]\n",
      "[ Experience replay ] starts\n",
      "[ episode 269 ][ timestamp 14 ] state=[ 0.12367768  0.61922624 -0.09325133 -0.99757869], action=1, reward=1.0, next_state=[ 0.13606221  0.81546291 -0.11320291 -1.31803096]\n",
      "[ Experience replay ] starts\n",
      "[ episode 269 ][ timestamp 15 ] state=[ 0.13606221  0.81546291 -0.11320291 -1.31803096], action=1, reward=1.0, next_state=[ 0.15237147  1.01181974 -0.13956353 -1.64389162]\n",
      "[ Experience replay ] starts\n",
      "[ episode 269 ][ timestamp 16 ] state=[ 0.15237147  1.01181974 -0.13956353 -1.64389162], action=1, reward=1.0, next_state=[ 0.17260786  1.20827241 -0.17244136 -1.97660401]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 269 ][ timestamp 17 ] state=[ 0.17260786  1.20827241 -0.17244136 -1.97660401], action=0, reward=-1.0, next_state=[ 0.19677331  1.01533639 -0.21197344 -1.74193907]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 269: Exploration_rate=0.01. Score=17.\n",
      "[ episode 270 ] state=[-0.04793079 -0.01765611  0.02067728 -0.00844789]\n",
      "[ episode 270 ][ timestamp 1 ] state=[-0.04793079 -0.01765611  0.02067728 -0.00844789], action=0, reward=1.0, next_state=[-0.04828391 -0.2130684   0.02050832  0.29068658]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 2 ] state=[-0.04828391 -0.2130684   0.02050832  0.29068658], action=0, reward=1.0, next_state=[-0.05254528 -0.40847669  0.02632205  0.5897664 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 3 ] state=[-0.05254528 -0.40847669  0.02632205  0.5897664 ], action=1, reward=1.0, next_state=[-0.06071481 -0.213733    0.03811738  0.30548984]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 4 ] state=[-0.06071481 -0.213733    0.03811738  0.30548984], action=1, reward=1.0, next_state=[-0.06498947 -0.01917438  0.04422718  0.0250677 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 5 ] state=[-0.06498947 -0.01917438  0.04422718  0.0250677 ], action=0, reward=1.0, next_state=[-0.06537296 -0.21490177  0.04472853  0.33137023]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 6 ] state=[-0.06537296 -0.21490177  0.04472853  0.33137023], action=0, reward=1.0, next_state=[-0.06967099 -0.41063091  0.05135593  0.63781611]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 7 ] state=[-0.06967099 -0.41063091  0.05135593  0.63781611], action=1, reward=1.0, next_state=[-0.07788361 -0.21626129  0.06411226  0.36173808]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 8 ] state=[-0.07788361 -0.21626129  0.06411226  0.36173808], action=1, reward=1.0, next_state=[-0.08220884 -0.02210642  0.07134702  0.0899402 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 9 ] state=[-0.08220884 -0.02210642  0.07134702  0.0899402 ], action=1, reward=1.0, next_state=[-0.08265097  0.17192419  0.07314582 -0.17940703]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 10 ] state=[-0.08265097  0.17192419  0.07314582 -0.17940703], action=0, reward=1.0, next_state=[-0.07921248 -0.02416411  0.06955768  0.13542463]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 11 ] state=[-0.07921248 -0.02416411  0.06955768  0.13542463], action=1, reward=1.0, next_state=[-0.07969576  0.16989615  0.07226617 -0.13452838]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 12 ] state=[-0.07969576  0.16989615  0.07226617 -0.13452838], action=0, reward=1.0, next_state=[-0.07629784 -0.02618253  0.06957561  0.18004973]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 13 ] state=[-0.07629784 -0.02618253  0.06957561  0.18004973], action=1, reward=1.0, next_state=[-0.07682149  0.16787839  0.0731766  -0.08989866]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 14 ] state=[-0.07682149  0.16787839  0.0731766  -0.08989866], action=1, reward=1.0, next_state=[-0.07346392  0.36187933  0.07137863 -0.35862655]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 15 ] state=[-0.07346392  0.36187933  0.07137863 -0.35862655], action=0, reward=1.0, next_state=[-0.06622634  0.16581909  0.0642061  -0.04431756]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 16 ] state=[-0.06622634  0.16581909  0.0642061  -0.04431756], action=1, reward=1.0, next_state=[-0.06290996  0.3599644   0.06331975 -0.31607184]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 17 ] state=[-0.06290996  0.3599644   0.06331975 -0.31607184], action=1, reward=1.0, next_state=[-0.05571067  0.55412996  0.05699831 -0.58813294]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 18 ] state=[-0.05571067  0.55412996  0.05699831 -0.58813294], action=1, reward=1.0, next_state=[-0.04462807  0.74840934  0.04523565 -0.86233032]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 19 ] state=[-0.04462807  0.74840934  0.04523565 -0.86233032], action=1, reward=1.0, next_state=[-0.02965988  0.94288713  0.02798904 -1.14045386]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 20 ] state=[-0.02965988  0.94288713  0.02798904 -1.14045386], action=0, reward=1.0, next_state=[-0.01080214  0.7474107   0.00517997 -0.83912634]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 21 ] state=[-0.01080214  0.7474107   0.00517997 -0.83912634], action=0, reward=1.0, next_state=[ 0.00414607  0.5522184  -0.01160256 -0.54481892]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 22 ] state=[ 0.00414607  0.5522184  -0.01160256 -0.54481892], action=1, reward=1.0, next_state=[ 0.01519044  0.74750145 -0.02249894 -0.84113486]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 23 ] state=[ 0.01519044  0.74750145 -0.02249894 -0.84113486], action=1, reward=1.0, next_state=[ 0.03014047  0.9429232  -0.03932164 -1.14080742]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 24 ] state=[ 0.03014047  0.9429232  -0.03932164 -1.14080742], action=1, reward=1.0, next_state=[ 0.04899894  1.13853647 -0.06213778 -1.44555809]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 25 ] state=[ 0.04899894  1.13853647 -0.06213778 -1.44555809], action=1, reward=1.0, next_state=[ 0.07176967  1.33436542 -0.09104895 -1.75699137]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 26 ] state=[ 0.07176967  1.33436542 -0.09104895 -1.75699137], action=0, reward=1.0, next_state=[ 0.09845697  1.14038554 -0.12618877 -1.4939582 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 27 ] state=[ 0.09845697  1.14038554 -0.12618877 -1.4939582 ], action=0, reward=1.0, next_state=[ 0.12126468  0.94700425 -0.15606794 -1.24319381]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 28 ] state=[ 0.12126468  0.94700425 -0.15606794 -1.24319381], action=0, reward=1.0, next_state=[ 0.14020477  0.75419086 -0.18093181 -1.00318681]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 29 ] state=[ 0.14020477  0.75419086 -0.18093181 -1.00318681], action=0, reward=1.0, next_state=[ 0.15528859  0.56188643 -0.20099555 -0.772343  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ][ timestamp 30 ] state=[ 0.15528859  0.56188643 -0.20099555 -0.772343  ], action=0, reward=-1.0, next_state=[ 0.16652632  0.37001344 -0.21644241 -0.54902322]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 270: Exploration_rate=0.01. Score=30.\n",
      "[ episode 271 ] state=[-0.0458072   0.02028931 -0.02819912  0.03360974]\n",
      "[ episode 271 ][ timestamp 1 ] state=[-0.0458072   0.02028931 -0.02819912  0.03360974], action=1, reward=1.0, next_state=[-0.04540142  0.21580406 -0.02752693 -0.26783524]\n",
      "[ Experience replay ] starts\n",
      "[ episode 271 ][ timestamp 2 ] state=[-0.04540142  0.21580406 -0.02752693 -0.26783524], action=0, reward=1.0, next_state=[-0.04108534  0.02108555 -0.03288363  0.01603998]\n",
      "[ Experience replay ] starts\n",
      "[ episode 271 ][ timestamp 3 ] state=[-0.04108534  0.02108555 -0.03288363  0.01603998], action=0, reward=1.0, next_state=[-0.04066363 -0.17354975 -0.03256283  0.29816906]\n",
      "[ Experience replay ] starts\n",
      "[ episode 271 ][ timestamp 4 ] state=[-0.04066363 -0.17354975 -0.03256283  0.29816906], action=0, reward=1.0, next_state=[-0.04413462 -0.36819276 -0.02659945  0.58040701]\n",
      "[ Experience replay ] starts\n",
      "[ episode 271 ][ timestamp 5 ] state=[-0.04413462 -0.36819276 -0.02659945  0.58040701], action=1, reward=1.0, next_state=[-0.05149848 -0.17270837 -0.01499131  0.27946484]\n",
      "[ Experience replay ] starts\n",
      "[ episode 271 ][ timestamp 6 ] state=[-0.05149848 -0.17270837 -0.01499131  0.27946484], action=1, reward=1.0, next_state=[-0.05495264  0.02262419 -0.00940201 -0.01790836]\n",
      "[ Experience replay ] starts\n",
      "[ episode 271 ][ timestamp 7 ] state=[-0.05495264  0.02262419 -0.00940201 -0.01790836], action=1, reward=1.0, next_state=[-0.05450016  0.2178797  -0.00976018 -0.31354284]\n",
      "[ Experience replay ] starts\n",
      "[ episode 271 ][ timestamp 8 ] state=[-0.05450016  0.2178797  -0.00976018 -0.31354284], action=0, reward=1.0, next_state=[-0.05014257  0.02289814 -0.01603104 -0.02395387]\n",
      "[ Experience replay ] starts\n",
      "[ episode 271 ][ timestamp 9 ] state=[-0.05014257  0.02289814 -0.01603104 -0.02395387], action=1, reward=1.0, next_state=[-0.0496846   0.21824628 -0.01651012 -0.32165135]\n",
      "[ Experience replay ] starts\n",
      "[ episode 271 ][ timestamp 10 ] state=[-0.0496846   0.21824628 -0.01651012 -0.32165135], action=1, reward=1.0, next_state=[-0.04531968  0.41359941 -0.02294314 -0.61949486]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 271 ][ timestamp 11 ] state=[-0.04531968  0.41359941 -0.02294314 -0.61949486], action=1, reward=1.0, next_state=[-0.03704769  0.60903416 -0.03533304 -0.91931454]\n",
      "[ Experience replay ] starts\n",
      "[ episode 271 ][ timestamp 12 ] state=[-0.03704769  0.60903416 -0.03533304 -0.91931454], action=0, reward=1.0, next_state=[-0.02486701  0.41440716 -0.05371933 -0.637942  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 271 ][ timestamp 13 ] state=[-0.02486701  0.41440716 -0.05371933 -0.637942  ], action=1, reward=1.0, next_state=[-0.01657886  0.61023542 -0.06647817 -0.94704654]\n",
      "[ Experience replay ] starts\n",
      "[ episode 271 ][ timestamp 14 ] state=[-0.01657886  0.61023542 -0.06647817 -0.94704654], action=1, reward=1.0, next_state=[-0.00437415  0.80618656 -0.0854191  -1.2598542 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 271 ][ timestamp 15 ] state=[-0.00437415  0.80618656 -0.0854191  -1.2598542 ], action=1, reward=1.0, next_state=[ 0.01174958  1.00229104 -0.11061619 -1.57802111]\n",
      "[ Experience replay ] starts\n",
      "[ episode 271 ][ timestamp 16 ] state=[ 0.01174958  1.00229104 -0.11061619 -1.57802111], action=0, reward=1.0, next_state=[ 0.0317954   0.80864706 -0.14217661 -1.32178527]\n",
      "[ Experience replay ] starts\n",
      "[ episode 271 ][ timestamp 17 ] state=[ 0.0317954   0.80864706 -0.14217661 -1.32178527], action=0, reward=1.0, next_state=[ 0.04796834  0.61557888 -0.16861231 -1.07676435]\n",
      "[ Experience replay ] starts\n",
      "[ episode 271 ][ timestamp 18 ] state=[ 0.04796834  0.61557888 -0.16861231 -1.07676435], action=0, reward=1.0, next_state=[ 0.06027992  0.42303644 -0.1901476  -0.84138394]\n",
      "[ Experience replay ] starts\n",
      "[ episode 271 ][ timestamp 19 ] state=[ 0.06027992  0.42303644 -0.1901476  -0.84138394], action=0, reward=1.0, next_state=[ 0.06874064  0.23094778 -0.20697528 -0.61401128]\n",
      "[ Experience replay ] starts\n",
      "[ episode 271 ][ timestamp 20 ] state=[ 0.06874064  0.23094778 -0.20697528 -0.61401128], action=0, reward=-1.0, next_state=[ 0.0733596   0.03922694 -0.2192555  -0.39298508]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 271: Exploration_rate=0.01. Score=20.\n",
      "[ episode 272 ] state=[ 0.0316299  -0.03945546 -0.03299657  0.02389158]\n",
      "[ episode 272 ][ timestamp 1 ] state=[ 0.0316299  -0.03945546 -0.03299657  0.02389158], action=0, reward=1.0, next_state=[ 0.03084079 -0.23408906 -0.03251874  0.30598382]\n",
      "[ Experience replay ] starts\n",
      "[ episode 272 ][ timestamp 2 ] state=[ 0.03084079 -0.23408906 -0.03251874  0.30598382], action=0, reward=1.0, next_state=[ 0.02615901 -0.42873289 -0.02639906  0.58823638]\n",
      "[ Experience replay ] starts\n",
      "[ episode 272 ][ timestamp 3 ] state=[ 0.02615901 -0.42873289 -0.02639906  0.58823638], action=1, reward=1.0, next_state=[ 0.01758435 -0.23325139 -0.01463433  0.28735588]\n",
      "[ Experience replay ] starts\n",
      "[ episode 272 ][ timestamp 4 ] state=[ 0.01758435 -0.23325139 -0.01463433  0.28735588], action=1, reward=1.0, next_state=[ 0.01291932 -0.03792383 -0.00888722 -0.00990643]\n",
      "[ Experience replay ] starts\n",
      "[ episode 272 ][ timestamp 5 ] state=[ 0.01291932 -0.03792383 -0.00888722 -0.00990643], action=0, reward=1.0, next_state=[ 0.01216084 -0.2329172  -0.00908535  0.27995928]\n",
      "[ Experience replay ] starts\n",
      "[ episode 272 ][ timestamp 6 ] state=[ 0.01216084 -0.2329172  -0.00908535  0.27995928], action=1, reward=1.0, next_state=[ 0.0075025  -0.03766684 -0.00348616 -0.01557524]\n",
      "[ Experience replay ] starts\n",
      "[ episode 272 ][ timestamp 7 ] state=[ 0.0075025  -0.03766684 -0.00348616 -0.01557524], action=0, reward=1.0, next_state=[ 0.00674916 -0.23273862 -0.00379767  0.27600572]\n",
      "[ Experience replay ] starts\n",
      "[ episode 272 ][ timestamp 8 ] state=[ 0.00674916 -0.23273862 -0.00379767  0.27600572], action=0, reward=1.0, next_state=[ 0.00209439 -0.42780618  0.00172245  0.56748845]\n",
      "[ Experience replay ] starts\n",
      "[ episode 272 ][ timestamp 9 ] state=[ 0.00209439 -0.42780618  0.00172245  0.56748845], action=0, reward=1.0, next_state=[-0.00646173 -0.62295225  0.01307222  0.86071352]\n",
      "[ Experience replay ] starts\n",
      "[ episode 272 ][ timestamp 10 ] state=[-0.00646173 -0.62295225  0.01307222  0.86071352], action=0, reward=1.0, next_state=[-0.01892078 -0.81824977  0.03028649  1.15747789]\n",
      "[ Experience replay ] starts\n",
      "[ episode 272 ][ timestamp 11 ] state=[-0.01892078 -0.81824977  0.03028649  1.15747789], action=0, reward=1.0, next_state=[-0.03528577 -1.01375311  0.05343605  1.45950127]\n",
      "[ Experience replay ] starts\n",
      "[ episode 272 ][ timestamp 12 ] state=[-0.03528577 -1.01375311  0.05343605  1.45950127], action=0, reward=1.0, next_state=[-0.05556083 -1.2094881   0.08262607  1.7683874 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 272 ][ timestamp 13 ] state=[-0.05556083 -1.2094881   0.08262607  1.7683874 ], action=1, reward=1.0, next_state=[-0.0797506  -1.01539068  0.11799382  1.50249898]\n",
      "[ Experience replay ] starts\n",
      "[ episode 272 ][ timestamp 14 ] state=[-0.0797506  -1.01539068  0.11799382  1.50249898], action=0, reward=1.0, next_state=[-0.10005841 -1.21173081  0.1480438   1.82957114]\n",
      "[ Experience replay ] starts\n",
      "[ episode 272 ][ timestamp 15 ] state=[-0.10005841 -1.21173081  0.1480438   1.82957114], action=0, reward=1.0, next_state=[-0.12429303 -1.40815064  0.18463522  2.16434414]\n",
      "[ Experience replay ] starts\n",
      "[ episode 272 ][ timestamp 16 ] state=[-0.12429303 -1.40815064  0.18463522  2.16434414], action=0, reward=-1.0, next_state=[-0.15245604 -1.60453741  0.22792211  2.50789226]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 272: Exploration_rate=0.01. Score=16.\n",
      "[ episode 273 ] state=[-0.02752263 -0.04833874 -0.00116412 -0.00384441]\n",
      "[ episode 273 ][ timestamp 1 ] state=[-0.02752263 -0.04833874 -0.00116412 -0.00384441], action=0, reward=1.0, next_state=[-0.02848941 -0.24344398 -0.00124101  0.288471  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 2 ] state=[-0.02848941 -0.24344398 -0.00124101  0.288471  ], action=0, reward=1.0, next_state=[-0.03335829 -0.43854821  0.00452841  0.58076227]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 3 ] state=[-0.03335829 -0.43854821  0.00452841  0.58076227], action=1, reward=1.0, next_state=[-0.04212925 -0.24349001  0.01614366  0.28950931]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 4 ] state=[-0.04212925 -0.24349001  0.01614366  0.28950931], action=0, reward=1.0, next_state=[-0.04699905 -0.4388384   0.02193385  0.58723974]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 5 ] state=[-0.04699905 -0.4388384   0.02193385  0.58723974], action=0, reward=1.0, next_state=[-0.05577582 -0.63426055  0.03367864  0.8867505 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 6 ] state=[-0.05577582 -0.63426055  0.03367864  0.8867505 ], action=1, reward=1.0, next_state=[-0.06846103 -0.43961156  0.05141365  0.60484222]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 7 ] state=[-0.06846103 -0.43961156  0.05141365  0.60484222], action=1, reward=1.0, next_state=[-0.07725326 -0.24524487  0.06351049  0.3287864 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 8 ] state=[-0.07725326 -0.24524487  0.06351049  0.3287864 ], action=1, reward=1.0, next_state=[-0.08215816 -0.05108184  0.07008622  0.05678857]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 9 ] state=[-0.08215816 -0.05108184  0.07008622  0.05678857], action=1, reward=1.0, next_state=[-0.0831798   0.14296885  0.07122199 -0.21298437]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 10 ] state=[-0.0831798   0.14296885  0.07122199 -0.21298437], action=0, reward=1.0, next_state=[-0.08032042 -0.05309528  0.06696231  0.10128779]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 11 ] state=[-0.08032042 -0.05309528  0.06696231  0.10128779], action=0, reward=1.0, next_state=[-0.08138233 -0.24910982  0.06898806  0.41432286]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 12 ] state=[-0.08138233 -0.24910982  0.06898806  0.41432286], action=1, reward=1.0, next_state=[-0.08636452 -0.05503005  0.07727452  0.14416212]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 13 ] state=[-0.08636452 -0.05503005  0.07727452  0.14416212], action=1, reward=1.0, next_state=[-0.08746512  0.13890507  0.08015776 -0.12317636]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 14 ] state=[-0.08746512  0.13890507  0.08015776 -0.12317636], action=0, reward=1.0, next_state=[-0.08468702 -0.05726832  0.07769424  0.19368005]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 15 ] state=[-0.08468702 -0.05726832  0.07769424  0.19368005], action=0, reward=1.0, next_state=[-0.08583239 -0.25341075  0.08156784  0.50982526]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 273 ][ timestamp 16 ] state=[-0.08583239 -0.25341075  0.08156784  0.50982526], action=1, reward=1.0, next_state=[-0.0909006  -0.05952693  0.09176434  0.24392085]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 17 ] state=[-0.0909006  -0.05952693  0.09176434  0.24392085], action=1, reward=1.0, next_state=[-0.09209114  0.13417267  0.09664276 -0.01846523]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 18 ] state=[-0.09209114  0.13417267  0.09664276 -0.01846523], action=0, reward=1.0, next_state=[-0.08940769 -0.06219286  0.09627345  0.30307738]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 19 ] state=[-0.08940769 -0.06219286  0.09627345  0.30307738], action=1, reward=1.0, next_state=[-0.09065155  0.13143461  0.102335    0.04224182]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 20 ] state=[-0.09065155  0.13143461  0.102335    0.04224182], action=1, reward=1.0, next_state=[-0.08802285  0.32495156  0.10317984 -0.21648098]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 21 ] state=[-0.08802285  0.32495156  0.10317984 -0.21648098], action=1, reward=1.0, next_state=[-0.08152382  0.51845874  0.09885022 -0.47491698]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 22 ] state=[-0.08152382  0.51845874  0.09885022 -0.47491698], action=1, reward=1.0, next_state=[-0.07115465  0.712056    0.08935188 -0.73488059]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 23 ] state=[-0.07115465  0.712056    0.08935188 -0.73488059], action=1, reward=1.0, next_state=[-0.05691353  0.90583744  0.07465427 -0.99815868]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 24 ] state=[-0.05691353  0.90583744  0.07465427 -0.99815868], action=1, reward=1.0, next_state=[-0.03879678  1.09988623  0.05469109 -1.26649314]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 25 ] state=[-0.03879678  1.09988623  0.05469109 -1.26649314], action=1, reward=1.0, next_state=[-0.01679905  1.29426843  0.02936123 -1.54155933]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 26 ] state=[-0.01679905  1.29426843  0.02936123 -1.54155933], action=1, reward=1.0, next_state=[ 9.08631524e-03  1.48902532e+00 -1.46995626e-03 -1.82493779e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 27 ] state=[ 9.08631524e-03  1.48902532e+00 -1.46995626e-03 -1.82493779e+00], action=0, reward=1.0, next_state=[ 0.03886682  1.29391971 -0.03796871 -1.53271185]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 28 ] state=[ 0.03886682  1.29391971 -0.03796871 -1.53271185], action=1, reward=1.0, next_state=[ 0.06474522  1.48947805 -0.06862295 -1.83699806]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 29 ] state=[ 0.06474522  1.48947805 -0.06862295 -1.83699806], action=0, reward=1.0, next_state=[ 0.09453478  1.29517826 -0.10536291 -1.56639366]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 30 ] state=[ 0.09453478  1.29517826 -0.10536291 -1.56639366], action=0, reward=1.0, next_state=[ 0.12043834  1.10146124 -0.13669078 -1.30834894]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 31 ] state=[ 0.12043834  1.10146124 -0.13669078 -1.30834894], action=0, reward=1.0, next_state=[ 0.14246757  0.90831016 -0.16285776 -1.06138686]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 32 ] state=[ 0.14246757  0.90831016 -0.16285776 -1.06138686], action=0, reward=1.0, next_state=[ 0.16063377  0.7156751  -0.1840855  -0.82392651]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 33 ] state=[ 0.16063377  0.7156751  -0.1840855  -0.82392651], action=0, reward=1.0, next_state=[ 0.17494727  0.52348392 -0.20056403 -0.5943266 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ][ timestamp 34 ] state=[ 0.17494727  0.52348392 -0.20056403 -0.5943266 ], action=0, reward=-1.0, next_state=[ 0.18541695  0.33165029 -0.21245056 -0.37091558]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 273: Exploration_rate=0.01. Score=34.\n",
      "[ episode 274 ] state=[-0.03945875  0.01359409  0.03689304 -0.01022964]\n",
      "[ episode 274 ][ timestamp 1 ] state=[-0.03945875  0.01359409  0.03689304 -0.01022964], action=1, reward=1.0, next_state=[-0.03918687  0.20816805  0.03668845 -0.29104789]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 2 ] state=[-0.03918687  0.20816805  0.03668845 -0.29104789], action=0, reward=1.0, next_state=[-0.03502351  0.0125427   0.03086749  0.01297666]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 3 ] state=[-0.03502351  0.0125427   0.03086749  0.01297666], action=0, reward=1.0, next_state=[-0.03477265 -0.18300802  0.03112703  0.31523662]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 4 ] state=[-0.03477265 -0.18300802  0.03112703  0.31523662], action=1, reward=1.0, next_state=[-0.03843281  0.01165703  0.03743176  0.03253035]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 5 ] state=[-0.03843281  0.01165703  0.03743176  0.03253035], action=1, reward=1.0, next_state=[-0.03819967  0.20622276  0.03808237 -0.24811144]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 6 ] state=[-0.03819967  0.20622276  0.03808237 -0.24811144], action=0, reward=1.0, next_state=[-0.03407522  0.01057822  0.03312014  0.05633611]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 7 ] state=[-0.03407522  0.01057822  0.03312014  0.05633611], action=1, reward=1.0, next_state=[-0.03386365  0.20521001  0.03424686 -0.22571593]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 8 ] state=[-0.03386365  0.20521001  0.03424686 -0.22571593], action=1, reward=1.0, next_state=[-0.02975945  0.3998262   0.02973254 -0.50740243]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 9 ] state=[-0.02975945  0.3998262   0.02973254 -0.50740243], action=0, reward=1.0, next_state=[-0.02176293  0.2042982   0.01958449 -0.20549999]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 10 ] state=[-0.02176293  0.2042982   0.01958449 -0.20549999], action=1, reward=1.0, next_state=[-0.01767697  0.3991347   0.01547449 -0.49194121]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 11 ] state=[-0.01767697  0.3991347   0.01547449 -0.49194121], action=1, reward=1.0, next_state=[-0.00969427  0.59403499  0.00563567 -0.77970733]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 12 ] state=[-0.00969427  0.59403499  0.00563567 -0.77970733], action=0, reward=1.0, next_state=[ 0.00218643  0.39883602 -0.00995848 -0.48525664]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 13 ] state=[ 0.00218643  0.39883602 -0.00995848 -0.48525664], action=0, reward=1.0, next_state=[ 0.01016315  0.203856   -0.01966361 -0.19572886]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 14 ] state=[ 0.01016315  0.203856   -0.01966361 -0.19572886], action=0, reward=1.0, next_state=[ 0.01424027  0.00902076 -0.02357819  0.09068677]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 15 ] state=[ 0.01424027  0.00902076 -0.02357819  0.09068677], action=0, reward=1.0, next_state=[ 0.01442068 -0.18575544 -0.02176445  0.37583852]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 16 ] state=[ 0.01442068 -0.18575544 -0.02176445  0.37583852], action=0, reward=1.0, next_state=[ 0.01070558 -0.3805616  -0.01424768  0.66158031]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 17 ] state=[ 0.01070558 -0.3805616  -0.01424768  0.66158031], action=0, reward=1.0, next_state=[ 0.00309434 -0.57548244 -0.00101608  0.94974321]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 18 ] state=[ 0.00309434 -0.57548244 -0.00101608  0.94974321], action=1, reward=1.0, next_state=[-0.00841531 -0.38034682  0.01797879  0.65674122]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 19 ] state=[-0.00841531 -0.38034682  0.01797879  0.65674122], action=0, reward=1.0, next_state=[-0.01602224 -0.57571437  0.03111361  0.95503066]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 20 ] state=[-0.01602224 -0.57571437  0.03111361  0.95503066], action=0, reward=1.0, next_state=[-0.02753653 -0.77124073  0.05021423  1.25732417]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 21 ] state=[-0.02753653 -0.77124073  0.05021423  1.25732417], action=1, reward=1.0, next_state=[-0.04296134 -0.57679614  0.07536071  0.98078171]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 22 ] state=[-0.04296134 -0.57679614  0.07536071  0.98078171], action=0, reward=1.0, next_state=[-0.05449727 -0.77284282  0.09497634  1.29615215]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 23 ] state=[-0.05449727 -0.77284282  0.09497634  1.29615215], action=1, reward=1.0, next_state=[-0.06995412 -0.57904688  0.12089939  1.03464945]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 24 ] state=[-0.06995412 -0.57904688  0.12089939  1.03464945], action=1, reward=1.0, next_state=[-0.08153506 -0.38572176  0.14159238  0.7822364 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 25 ] state=[-0.08153506 -0.38572176  0.14159238  0.7822364 ], action=1, reward=1.0, next_state=[-0.0892495  -0.19280029  0.1572371   0.53723939]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 26 ] state=[-0.0892495  -0.19280029  0.1572371   0.53723939], action=1, reward=1.0, next_state=[-9.31055016e-02 -1.97837003e-04  1.67981891e-01  2.97937168e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 27 ] state=[-9.31055016e-02 -1.97837003e-04  1.67981891e-01  2.97937168e-01], action=1, reward=1.0, next_state=[-0.09310946  0.19218106  0.17394063  0.06258539]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 274 ][ timestamp 28 ] state=[-0.09310946  0.19218106  0.17394063  0.06258539], action=1, reward=1.0, next_state=[-0.08926584  0.38443778  0.17519234 -0.17056701]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 29 ] state=[-0.08926584  0.38443778  0.17519234 -0.17056701], action=1, reward=1.0, next_state=[-0.08157708  0.57667594  0.171781   -0.40326689]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 30 ] state=[-0.08157708  0.57667594  0.171781   -0.40326689], action=1, reward=1.0, next_state=[-0.07004356  0.76899841  0.16371566 -0.63724907]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 31 ] state=[-0.07004356  0.76899841  0.16371566 -0.63724907], action=1, reward=1.0, next_state=[-0.05466359  0.96150456  0.15097068 -0.87422947]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 32 ] state=[-0.05466359  0.96150456  0.15097068 -0.87422947], action=1, reward=1.0, next_state=[-0.0354335   1.15428719  0.13348609 -1.11589726]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 33 ] state=[-0.0354335   1.15428719  0.13348609 -1.11589726], action=1, reward=1.0, next_state=[-0.01234776  1.34742877  0.11116815 -1.36390387]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 34 ] state=[-0.01234776  1.34742877  0.11116815 -1.36390387], action=0, reward=1.0, next_state=[ 0.01460082  1.15110375  0.08389007 -1.038618  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 35 ] state=[ 0.01460082  1.15110375  0.08389007 -1.038618  ], action=1, reward=1.0, next_state=[ 0.03762289  1.3450168   0.06311771 -1.3038299 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 36 ] state=[ 0.03762289  1.3450168   0.06311771 -1.3038299 ], action=1, reward=1.0, next_state=[ 0.06452323  1.53928401  0.03704111 -1.57610617]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 37 ] state=[ 0.06452323  1.53928401  0.03704111 -1.57610617], action=0, reward=1.0, next_state=[ 0.09530891  1.34374069  0.00551899 -1.2721048 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 38 ] state=[ 0.09530891  1.34374069  0.00551899 -1.2721048 ], action=1, reward=1.0, next_state=[ 0.12218372  1.53879177 -0.01992311 -1.56305439]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 39 ] state=[ 0.12218372  1.53879177 -0.01992311 -1.56305439], action=0, reward=1.0, next_state=[ 0.15295956  1.34391365 -0.05118419 -1.27665222]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 40 ] state=[ 0.15295956  1.34391365 -0.05118419 -1.27665222], action=0, reward=1.0, next_state=[ 0.17983783  1.14948036 -0.07671724 -1.00042582]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 41 ] state=[ 0.17983783  1.14948036 -0.07671724 -1.00042582], action=0, reward=1.0, next_state=[ 0.20282744  0.95546289 -0.09672576 -0.73278836]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 42 ] state=[ 0.20282744  0.95546289 -0.09672576 -0.73278836], action=1, reward=1.0, next_state=[ 0.22193669  1.15177882 -0.11138152 -1.05427886]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 43 ] state=[ 0.22193669  1.15177882 -0.11138152 -1.05427886], action=0, reward=1.0, next_state=[ 0.24497227  0.95829544 -0.1324671  -0.79853068]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 44 ] state=[ 0.24497227  0.95829544 -0.1324671  -0.79853068], action=0, reward=1.0, next_state=[ 0.26413818  0.76521541 -0.14843771 -0.55027951]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 45 ] state=[ 0.26413818  0.76521541 -0.14843771 -0.55027951], action=0, reward=1.0, next_state=[ 0.27944249  0.57245595 -0.1594433  -0.30780048]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 46 ] state=[ 0.27944249  0.57245595 -0.1594433  -0.30780048], action=0, reward=1.0, next_state=[ 0.29089161  0.37992253 -0.16559931 -0.06934151]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 47 ] state=[ 0.29089161  0.37992253 -0.16559931 -0.06934151], action=0, reward=1.0, next_state=[ 0.29849006  0.18751413 -0.16698614  0.16685882]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 48 ] state=[ 0.29849006  0.18751413 -0.16698614  0.16685882], action=0, reward=1.0, next_state=[ 0.30224034 -0.00487289 -0.16364897  0.40255915]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 49 ] state=[ 0.30224034 -0.00487289 -0.16364897  0.40255915], action=0, reward=1.0, next_state=[ 0.30214288 -0.19734151 -0.15559778  0.63950649]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 50 ] state=[ 0.30214288 -0.19734151 -0.15559778  0.63950649], action=0, reward=1.0, next_state=[ 0.29819605 -0.3899911  -0.14280765  0.87942842]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 51 ] state=[ 0.29819605 -0.3899911  -0.14280765  0.87942842], action=1, reward=1.0, next_state=[ 0.29039623 -0.1932478  -0.12521909  0.54547476]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 52 ] state=[ 0.29039623 -0.1932478  -0.12521909  0.54547476], action=1, reward=1.0, next_state=[ 0.28653127  0.00339058 -0.11430959  0.21610833]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 53 ] state=[ 0.28653127  0.00339058 -0.11430959  0.21610833], action=0, reward=1.0, next_state=[ 0.28659909 -0.18992741 -0.10998742  0.47065899]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 54 ] state=[ 0.28659909 -0.18992741 -0.10998742  0.47065899], action=0, reward=1.0, next_state=[ 0.28280054 -0.38333789 -0.10057424  0.72675052]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 55 ] state=[ 0.28280054 -0.38333789 -0.10057424  0.72675052], action=0, reward=1.0, next_state=[ 0.27513378 -0.57693609 -0.08603923  0.98616135]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 56 ] state=[ 0.27513378 -0.57693609 -0.08603923  0.98616135], action=0, reward=1.0, next_state=[ 0.26359506 -0.77080697 -0.06631601  1.25062762]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 57 ] state=[ 0.26359506 -0.77080697 -0.06631601  1.25062762], action=0, reward=1.0, next_state=[ 0.24817892 -0.96501936 -0.04130345  1.52182323]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 58 ] state=[ 0.24817892 -0.96501936 -0.04130345  1.52182323], action=1, reward=1.0, next_state=[ 0.22887853 -0.76942343 -0.01086699  1.2165398 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 59 ] state=[ 0.22887853 -0.76942343 -0.01086699  1.2165398 ], action=0, reward=1.0, next_state=[ 0.21349006 -0.96440355  0.01346381  1.50579788]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 60 ] state=[ 0.21349006 -0.96440355  0.01346381  1.50579788], action=1, reward=1.0, next_state=[ 0.19420199 -0.76944747  0.04357976  1.2173485 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 61 ] state=[ 0.19420199 -0.76944747  0.04357976  1.2173485 ], action=1, reward=1.0, next_state=[ 0.17881304 -0.57491375  0.06792673  0.93863337]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 62 ] state=[ 0.17881304 -0.57491375  0.06792673  0.93863337], action=1, reward=1.0, next_state=[ 0.16731477 -0.38077008  0.0866994   0.66804455]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 63 ] state=[ 0.16731477 -0.38077008  0.0866994   0.66804455], action=1, reward=1.0, next_state=[ 0.15969937 -0.18695393  0.10006029  0.40387001]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 64 ] state=[ 0.15969937 -0.18695393  0.10006029  0.40387001], action=1, reward=1.0, next_state=[0.15596029 0.00661713 0.10813769 0.1443344 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 65 ] state=[0.15596029 0.00661713 0.10813769 0.1443344 ], action=1, reward=1.0, next_state=[ 0.15609263  0.20003763  0.11102438 -0.11237109]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 66 ] state=[ 0.15609263  0.20003763  0.11102438 -0.11237109], action=1, reward=1.0, next_state=[ 0.16009338  0.39340805  0.10877696 -0.36806673]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 67 ] state=[ 0.16009338  0.39340805  0.10877696 -0.36806673], action=0, reward=1.0, next_state=[ 0.16796154  0.19692215  0.10141562 -0.04316245]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 68 ] state=[ 0.16796154  0.19692215  0.10141562 -0.04316245], action=1, reward=1.0, next_state=[ 0.17189999  0.39045464  0.10055238 -0.30220447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 69 ] state=[ 0.17189999  0.39045464  0.10055238 -0.30220447], action=0, reward=1.0, next_state=[0.17970908 0.19405408 0.09450829 0.02042091]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 70 ] state=[0.17970908 0.19405408 0.09450829 0.02042091], action=1, reward=1.0, next_state=[ 0.18359016  0.38770255  0.0949167  -0.24101145]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 71 ] state=[ 0.18359016  0.38770255  0.0949167  -0.24101145], action=1, reward=1.0, next_state=[ 0.19134421  0.58134952  0.09009647 -0.50231081]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 72 ] state=[ 0.19134421  0.58134952  0.09009647 -0.50231081], action=1, reward=1.0, next_state=[ 0.2029712   0.77509372  0.08005026 -0.76529584]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 73 ] state=[ 0.2029712   0.77509372  0.08005026 -0.76529584], action=1, reward=1.0, next_state=[ 0.21847308  0.96902749  0.06474434 -1.03175529]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 74 ] state=[ 0.21847308  0.96902749  0.06474434 -1.03175529], action=1, reward=1.0, next_state=[ 0.23785363  1.16323121  0.04410924 -1.303429  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 75 ] state=[ 0.23785363  1.16323121  0.04410924 -1.303429  ], action=1, reward=1.0, next_state=[ 0.26111825  1.3577668   0.01804066 -1.58198465]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 274 ][ timestamp 76 ] state=[ 0.26111825  1.3577668   0.01804066 -1.58198465], action=1, reward=1.0, next_state=[ 0.28827359  1.55266949 -0.01359904 -1.86898743]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 77 ] state=[ 0.28827359  1.55266949 -0.01359904 -1.86898743], action=1, reward=1.0, next_state=[ 0.31932698  1.74793746 -0.05097879 -2.1658603 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 78 ] state=[ 0.31932698  1.74793746 -0.05097879 -2.1658603 ], action=1, reward=1.0, next_state=[ 0.35428573  1.94351889 -0.09429599 -2.47383259]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 79 ] state=[ 0.35428573  1.94351889 -0.09429599 -2.47383259], action=1, reward=1.0, next_state=[ 0.3931561   2.13929601 -0.14377264 -2.7938756 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 80 ] state=[ 0.3931561   2.13929601 -0.14377264 -2.7938756 ], action=1, reward=1.0, next_state=[ 0.43594202  2.3350664  -0.19965016 -3.12662508]\n",
      "[ Experience replay ] starts\n",
      "[ episode 274 ][ timestamp 81 ] state=[ 0.43594202  2.3350664  -0.19965016 -3.12662508], action=1, reward=-1.0, next_state=[ 0.48264335  2.53052211 -0.26218266 -3.47229282]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 274: Exploration_rate=0.01. Score=81.\n",
      "[ episode 275 ] state=[ 0.0299835  -0.01159394  0.0491974  -0.0016413 ]\n",
      "[ episode 275 ][ timestamp 1 ] state=[ 0.0299835  -0.01159394  0.0491974  -0.0016413 ], action=1, reward=1.0, next_state=[ 0.02975162  0.18278919  0.04916457 -0.278405  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 275 ][ timestamp 2 ] state=[ 0.02975162  0.18278919  0.04916457 -0.278405  ], action=1, reward=1.0, next_state=[ 0.0334074   0.37717654  0.04359647 -0.55518515]\n",
      "[ Experience replay ] starts\n",
      "[ episode 275 ][ timestamp 3 ] state=[ 0.0334074   0.37717654  0.04359647 -0.55518515], action=1, reward=1.0, next_state=[ 0.04095093  0.57166013  0.03249277 -0.83382003]\n",
      "[ Experience replay ] starts\n",
      "[ episode 275 ][ timestamp 4 ] state=[ 0.04095093  0.57166013  0.03249277 -0.83382003], action=1, reward=1.0, next_state=[ 0.05238413  0.76632341  0.01581637 -1.11610964]\n",
      "[ Experience replay ] starts\n",
      "[ episode 275 ][ timestamp 5 ] state=[ 0.05238413  0.76632341  0.01581637 -1.11610964], action=1, reward=1.0, next_state=[ 0.0677106   0.96123423 -0.00650582 -1.40378947]\n",
      "[ Experience replay ] starts\n",
      "[ episode 275 ][ timestamp 6 ] state=[ 0.0677106   0.96123423 -0.00650582 -1.40378947], action=1, reward=1.0, next_state=[ 0.08693529  1.15643637 -0.03458161 -1.69849919]\n",
      "[ Experience replay ] starts\n",
      "[ episode 275 ][ timestamp 7 ] state=[ 0.08693529  1.15643637 -0.03458161 -1.69849919], action=1, reward=1.0, next_state=[ 0.11006401  1.35193946 -0.0685516  -2.00174346]\n",
      "[ Experience replay ] starts\n",
      "[ episode 275 ][ timestamp 8 ] state=[ 0.11006401  1.35193946 -0.0685516  -2.00174346], action=1, reward=1.0, next_state=[ 0.1371028   1.54770648 -0.10858647 -2.31484267]\n",
      "[ Experience replay ] starts\n",
      "[ episode 275 ][ timestamp 9 ] state=[ 0.1371028   1.54770648 -0.10858647 -2.31484267], action=1, reward=1.0, next_state=[ 0.16805693  1.74363858 -0.15488332 -2.63887156]\n",
      "[ Experience replay ] starts\n",
      "[ episode 275 ][ timestamp 10 ] state=[ 0.16805693  1.74363858 -0.15488332 -2.63887156], action=1, reward=1.0, next_state=[ 0.2029297   1.93955723 -0.20766075 -2.97458554]\n",
      "[ Experience replay ] starts\n",
      "[ episode 275 ][ timestamp 11 ] state=[ 0.2029297   1.93955723 -0.20766075 -2.97458554], action=1, reward=-1.0, next_state=[ 0.24172085  2.13518427 -0.26715246 -3.3223362 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 275: Exploration_rate=0.01. Score=11.\n",
      "[ episode 276 ] state=[-0.0166539   0.04490049 -0.0113331   0.02243085]\n",
      "[ episode 276 ][ timestamp 1 ] state=[-0.0166539   0.04490049 -0.0113331   0.02243085], action=1, reward=1.0, next_state=[-0.01575589  0.24018312 -0.01088448 -0.27380615]\n",
      "[ Experience replay ] starts\n",
      "[ episode 276 ][ timestamp 2 ] state=[-0.01575589  0.24018312 -0.01088448 -0.27380615], action=1, reward=1.0, next_state=[-0.01095222  0.43545867 -0.01636061 -0.5699021 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 276 ][ timestamp 3 ] state=[-0.01095222  0.43545867 -0.01636061 -0.5699021 ], action=1, reward=1.0, next_state=[-0.00224305  0.6308062  -0.02775865 -0.86769399]\n",
      "[ Experience replay ] starts\n",
      "[ episode 276 ][ timestamp 4 ] state=[-0.00224305  0.6308062  -0.02775865 -0.86769399], action=1, reward=1.0, next_state=[ 0.01037307  0.82629464 -0.04511253 -1.16897368]\n",
      "[ Experience replay ] starts\n",
      "[ episode 276 ][ timestamp 5 ] state=[ 0.01037307  0.82629464 -0.04511253 -1.16897368], action=1, reward=1.0, next_state=[ 0.02689897  1.02197345 -0.068492   -1.47545185]\n",
      "[ Experience replay ] starts\n",
      "[ episode 276 ][ timestamp 6 ] state=[ 0.02689897  1.02197345 -0.068492   -1.47545185], action=1, reward=1.0, next_state=[ 0.04733843  1.21786212 -0.09800104 -1.78871683]\n",
      "[ Experience replay ] starts\n",
      "[ episode 276 ][ timestamp 7 ] state=[ 0.04733843  1.21786212 -0.09800104 -1.78871683], action=0, reward=1.0, next_state=[ 0.07169568  1.02396707 -0.13377538 -1.528036  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 276 ][ timestamp 8 ] state=[ 0.07169568  1.02396707 -0.13377538 -1.528036  ], action=0, reward=1.0, next_state=[ 0.09217502  0.83068861 -0.1643361  -1.27992135]\n",
      "[ Experience replay ] starts\n",
      "[ episode 276 ][ timestamp 9 ] state=[ 0.09217502  0.83068861 -0.1643361  -1.27992135], action=0, reward=1.0, next_state=[ 0.10878879  0.63799716 -0.18993452 -1.04287596]\n",
      "[ Experience replay ] starts\n",
      "[ episode 276 ][ timestamp 10 ] state=[ 0.10878879  0.63799716 -0.18993452 -1.04287596], action=0, reward=-1.0, next_state=[ 0.12154873  0.44583496 -0.21079204 -0.81532186]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 276: Exploration_rate=0.01. Score=10.\n",
      "[ episode 277 ] state=[ 0.04830466 -0.00226298  0.01032555 -0.00675793]\n",
      "[ episode 277 ][ timestamp 1 ] state=[ 0.04830466 -0.00226298  0.01032555 -0.00675793], action=0, reward=1.0, next_state=[ 0.0482594  -0.19753148  0.01019039  0.28916486]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 2 ] state=[ 0.0482594  -0.19753148  0.01019039  0.28916486], action=0, reward=1.0, next_state=[ 0.04430877 -0.39279725  0.01597369  0.58504424]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 3 ] state=[ 0.04430877 -0.39279725  0.01597369  0.58504424], action=0, reward=1.0, next_state=[ 0.03645282 -0.58813927  0.02767458  0.88271595]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 4 ] state=[ 0.03645282 -0.58813927  0.02767458  0.88271595], action=1, reward=1.0, next_state=[ 0.02469004 -0.39340389  0.0453289   0.59886001]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 5 ] state=[ 0.02469004 -0.39340389  0.0453289   0.59886001], action=1, reward=1.0, next_state=[ 0.01682196 -0.19894449  0.0573061   0.32079266]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 6 ] state=[ 0.01682196 -0.19894449  0.0573061   0.32079266], action=1, reward=1.0, next_state=[ 0.01284307 -0.00468347  0.06372195  0.04671824]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 7 ] state=[ 0.01284307 -0.00468347  0.06372195  0.04671824], action=1, reward=1.0, next_state=[ 0.0127494   0.18946961  0.06465631 -0.22519875]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 8 ] state=[ 0.0127494   0.18946961  0.06465631 -0.22519875], action=1, reward=1.0, next_state=[ 0.01653879  0.38361078  0.06015234 -0.49680629]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 9 ] state=[ 0.01653879  0.38361078  0.06015234 -0.49680629], action=1, reward=1.0, next_state=[ 0.02421101  0.57783525  0.05021621 -0.76994196]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 10 ] state=[ 0.02421101  0.57783525  0.05021621 -0.76994196], action=1, reward=1.0, next_state=[ 0.03576771  0.77223143  0.03481737 -1.0464113 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 11 ] state=[ 0.03576771  0.77223143  0.03481737 -1.0464113 ], action=0, reward=1.0, next_state=[ 0.05121234  0.57666508  0.01388915 -0.74300532]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 12 ] state=[ 0.05121234  0.57666508  0.01388915 -0.74300532], action=0, reward=1.0, next_state=[ 0.06274564  0.3813542  -0.00097096 -0.44598398]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 13 ] state=[ 0.06274564  0.3813542  -0.00097096 -0.44598398], action=0, reward=1.0, next_state=[ 0.07037273  0.186246   -0.00989064 -0.15360727]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 14 ] state=[ 0.07037273  0.186246   -0.00989064 -0.15360727], action=0, reward=1.0, next_state=[ 0.07409765 -0.00873295 -0.01296278  0.13593904]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 15 ] state=[ 0.07409765 -0.00873295 -0.01296278  0.13593904], action=1, reward=1.0, next_state=[ 0.07392299  0.18657225 -0.010244   -0.1608051 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 16 ] state=[ 0.07392299  0.18657225 -0.010244   -0.1608051 ], action=1, reward=1.0, next_state=[ 0.07765443  0.38183935 -0.0134601  -0.45670206]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 17 ] state=[ 0.07765443  0.38183935 -0.0134601  -0.45670206], action=0, reward=1.0, next_state=[ 0.08529122  0.18691026 -0.02259415 -0.16829206]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 18 ] state=[ 0.08529122  0.18691026 -0.02259415 -0.16829206], action=0, reward=1.0, next_state=[ 0.08902943 -0.00788112 -0.02595999  0.11717831]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 277 ][ timestamp 19 ] state=[ 0.08902943 -0.00788112 -0.02595999  0.11717831], action=0, reward=1.0, next_state=[ 0.0888718  -0.20262168 -0.02361642  0.40155935]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 20 ] state=[ 0.0888718  -0.20262168 -0.02361642  0.40155935], action=0, reward=1.0, next_state=[ 0.08481937 -0.39740083 -0.01558523  0.68670402]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 21 ] state=[ 0.08481937 -0.39740083 -0.01558523  0.68670402], action=0, reward=1.0, next_state=[ 0.07687135 -0.59230301 -0.00185115  0.97443991]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 22 ] state=[ 0.07687135 -0.59230301 -0.00185115  0.97443991], action=0, reward=1.0, next_state=[ 0.06502529 -0.78740008  0.01763764  1.26654078]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 23 ] state=[ 0.06502529 -0.78740008  0.01763764  1.26654078], action=0, reward=1.0, next_state=[ 0.04927729 -0.98274288  0.04296846  1.5646946 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 24 ] state=[ 0.04927729 -0.98274288  0.04296846  1.5646946 ], action=0, reward=1.0, next_state=[ 0.02962243 -1.1783513   0.07426235  1.87046524]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 25 ] state=[ 0.02962243 -1.1783513   0.07426235  1.87046524], action=0, reward=1.0, next_state=[ 0.00605541 -1.37420223  0.11167166  2.18524501]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 26 ] state=[ 0.00605541 -1.37420223  0.11167166  2.18524501], action=0, reward=1.0, next_state=[-0.02142864 -1.57021514  0.15537656  2.51019626]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 27 ] state=[-0.02142864 -1.57021514  0.15537656  2.51019626], action=0, reward=1.0, next_state=[-0.05283294 -1.76623492  0.20558048  2.84618098]\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ][ timestamp 28 ] state=[-0.05283294 -1.76623492  0.20558048  2.84618098], action=0, reward=-1.0, next_state=[-0.08815764 -1.96201256  0.2625041   3.19367942]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 277: Exploration_rate=0.01. Score=28.\n",
      "[ episode 278 ] state=[ 0.02486152  0.01258941  0.01766504 -0.00914656]\n",
      "[ episode 278 ][ timestamp 1 ] state=[ 0.02486152  0.01258941  0.01766504 -0.00914656], action=0, reward=1.0, next_state=[ 0.02511331 -0.18278137  0.01748211  0.28905713]\n",
      "[ Experience replay ] starts\n",
      "[ episode 278 ][ timestamp 2 ] state=[ 0.02511331 -0.18278137  0.01748211  0.28905713], action=0, reward=1.0, next_state=[ 0.02145768 -0.37814819  0.02326326  0.58720207]\n",
      "[ Experience replay ] starts\n",
      "[ episode 278 ][ timestamp 3 ] state=[ 0.02145768 -0.37814819  0.02326326  0.58720207], action=0, reward=1.0, next_state=[ 0.01389472 -0.57358809  0.0350073   0.88712138]\n",
      "[ Experience replay ] starts\n",
      "[ episode 278 ][ timestamp 4 ] state=[ 0.01389472 -0.57358809  0.0350073   0.88712138], action=0, reward=1.0, next_state=[ 0.00242296 -0.76916729  0.05274972  1.19060046]\n",
      "[ Experience replay ] starts\n",
      "[ episode 278 ][ timestamp 5 ] state=[ 0.00242296 -0.76916729  0.05274972  1.19060046], action=0, reward=1.0, next_state=[-0.01296039 -0.9649316   0.07656173  1.49933973]\n",
      "[ Experience replay ] starts\n",
      "[ episode 278 ][ timestamp 6 ] state=[-0.01296039 -0.9649316   0.07656173  1.49933973], action=0, reward=1.0, next_state=[-0.03225902 -1.16089564  0.10654853  1.81491186]\n",
      "[ Experience replay ] starts\n",
      "[ episode 278 ][ timestamp 7 ] state=[-0.03225902 -1.16089564  0.10654853  1.81491186], action=0, reward=1.0, next_state=[-0.05547693 -1.35703005  0.14284677  2.13871111]\n",
      "[ Experience replay ] starts\n",
      "[ episode 278 ][ timestamp 8 ] state=[-0.05547693 -1.35703005  0.14284677  2.13871111], action=0, reward=1.0, next_state=[-0.08261754 -1.55324661  0.18562099  2.47189244]\n",
      "[ Experience replay ] starts\n",
      "[ episode 278 ][ timestamp 9 ] state=[-0.08261754 -1.55324661  0.18562099  2.47189244], action=0, reward=-1.0, next_state=[-0.11368247 -1.74938091  0.23505884  2.81529976]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 278: Exploration_rate=0.01. Score=9.\n",
      "[ episode 279 ] state=[ 0.01219859  0.03773277 -0.02692827  0.03049198]\n",
      "[ episode 279 ][ timestamp 1 ] state=[ 0.01219859  0.03773277 -0.02692827  0.03049198], action=0, reward=1.0, next_state=[ 0.01295325 -0.15699287 -0.02631843  0.31455859]\n",
      "[ Experience replay ] starts\n",
      "[ episode 279 ][ timestamp 2 ] state=[ 0.01295325 -0.15699287 -0.02631843  0.31455859], action=0, reward=1.0, next_state=[ 0.00981339 -0.35173023 -0.02002726  0.59882674]\n",
      "[ Experience replay ] starts\n",
      "[ episode 279 ][ timestamp 3 ] state=[ 0.00981339 -0.35173023 -0.02002726  0.59882674], action=0, reward=1.0, next_state=[ 0.00277879 -0.54656632 -0.00805073  0.88513465]\n",
      "[ Experience replay ] starts\n",
      "[ episode 279 ][ timestamp 4 ] state=[ 0.00277879 -0.54656632 -0.00805073  0.88513465], action=0, reward=1.0, next_state=[-0.00815254 -0.74157805  0.00965197  1.17527587]\n",
      "[ Experience replay ] starts\n",
      "[ episode 279 ][ timestamp 5 ] state=[-0.00815254 -0.74157805  0.00965197  1.17527587], action=0, reward=1.0, next_state=[-0.0229841  -0.93682408  0.03315748  1.47096891]\n",
      "[ Experience replay ] starts\n",
      "[ episode 279 ][ timestamp 6 ] state=[-0.0229841  -0.93682408  0.03315748  1.47096891], action=0, reward=1.0, next_state=[-0.04172058 -1.1323355   0.06257686  1.77382136]\n",
      "[ Experience replay ] starts\n",
      "[ episode 279 ][ timestamp 7 ] state=[-0.04172058 -1.1323355   0.06257686  1.77382136], action=0, reward=1.0, next_state=[-0.06436729 -1.32810457  0.09805329  2.08528579]\n",
      "[ Experience replay ] starts\n",
      "[ episode 279 ][ timestamp 8 ] state=[-0.06436729 -1.32810457  0.09805329  2.08528579], action=0, reward=1.0, next_state=[-0.09092938 -1.52407102  0.13975901  2.40660502]\n",
      "[ Experience replay ] starts\n",
      "[ episode 279 ][ timestamp 9 ] state=[-0.09092938 -1.52407102  0.13975901  2.40660502], action=0, reward=1.0, next_state=[-0.1214108  -1.72010583  0.18789111  2.73874561]\n",
      "[ Experience replay ] starts\n",
      "[ episode 279 ][ timestamp 10 ] state=[-0.1214108  -1.72010583  0.18789111  2.73874561], action=0, reward=-1.0, next_state=[-0.15581292 -1.91599249  0.24266602  3.08231983]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 279: Exploration_rate=0.01. Score=10.\n",
      "[ episode 280 ] state=[-0.02392808  0.00211522  0.04313152  0.01873915]\n",
      "[ episode 280 ][ timestamp 1 ] state=[-0.02392808  0.00211522  0.04313152  0.01873915], action=0, reward=1.0, next_state=[-0.02388578 -0.19359789  0.0435063   0.32471253]\n",
      "[ Experience replay ] starts\n",
      "[ episode 280 ][ timestamp 2 ] state=[-0.02388578 -0.19359789  0.0435063   0.32471253], action=0, reward=1.0, next_state=[-0.02775773 -0.38931144  0.05000055  0.63079187]\n",
      "[ Experience replay ] starts\n",
      "[ episode 280 ][ timestamp 3 ] state=[-0.02775773 -0.38931144  0.05000055  0.63079187], action=0, reward=1.0, next_state=[-0.03554396 -0.58509409  0.06261639  0.93879286]\n",
      "[ Experience replay ] starts\n",
      "[ episode 280 ][ timestamp 4 ] state=[-0.03554396 -0.58509409  0.06261639  0.93879286], action=0, reward=1.0, next_state=[-0.04724584 -0.78100177  0.08139224  1.25047567]\n",
      "[ Experience replay ] starts\n",
      "[ episode 280 ][ timestamp 5 ] state=[-0.04724584 -0.78100177  0.08139224  1.25047567], action=0, reward=1.0, next_state=[-0.06286588 -0.977067    0.10640176  1.5675028 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 280 ][ timestamp 6 ] state=[-0.06286588 -0.977067    0.10640176  1.5675028 ], action=0, reward=1.0, next_state=[-0.08240722 -1.1732869   0.13775181  1.89139125]\n",
      "[ Experience replay ] starts\n",
      "[ episode 280 ][ timestamp 7 ] state=[-0.08240722 -1.1732869   0.13775181  1.89139125], action=0, reward=1.0, next_state=[-0.10587296 -1.3696094   0.17557964  2.22345649]\n",
      "[ Experience replay ] starts\n",
      "[ episode 280 ][ timestamp 8 ] state=[-0.10587296 -1.3696094   0.17557964  2.22345649], action=0, reward=-1.0, next_state=[-0.13326515 -1.5659172   0.22004877  2.56474656]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 280: Exploration_rate=0.01. Score=8.\n",
      "[ episode 281 ] state=[ 0.02962943  0.02891227  0.01148868 -0.02581117]\n",
      "[ episode 281 ][ timestamp 1 ] state=[ 0.02962943  0.02891227  0.01148868 -0.02581117], action=0, reward=1.0, next_state=[ 0.03020767 -0.16637253  0.01097245  0.27047431]\n",
      "[ Experience replay ] starts\n",
      "[ episode 281 ][ timestamp 2 ] state=[ 0.03020767 -0.16637253  0.01097245  0.27047431], action=0, reward=1.0, next_state=[ 0.02688022 -0.36164933  0.01638194  0.56659771]\n",
      "[ Experience replay ] starts\n",
      "[ episode 281 ][ timestamp 3 ] state=[ 0.02688022 -0.36164933  0.01638194  0.56659771], action=0, reward=1.0, next_state=[ 0.01964724 -0.55699722  0.02771389  0.86439629]\n",
      "[ Experience replay ] starts\n",
      "[ episode 281 ][ timestamp 4 ] state=[ 0.01964724 -0.55699722  0.02771389  0.86439629], action=0, reward=1.0, next_state=[ 0.00850729 -0.75248524  0.04500182  1.16566256]\n",
      "[ Experience replay ] starts\n",
      "[ episode 281 ][ timestamp 5 ] state=[ 0.00850729 -0.75248524  0.04500182  1.16566256], action=0, reward=1.0, next_state=[-0.00654241 -0.9481631   0.06831507  1.47210826]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 281 ][ timestamp 6 ] state=[-0.00654241 -0.9481631   0.06831507  1.47210826], action=0, reward=1.0, next_state=[-0.02550567 -1.14405063  0.09775724  1.78532319]\n",
      "[ Experience replay ] starts\n",
      "[ episode 281 ][ timestamp 7 ] state=[-0.02550567 -1.14405063  0.09775724  1.78532319], action=0, reward=1.0, next_state=[-0.04838669 -1.34012547  0.1334637   2.10672611]\n",
      "[ Experience replay ] starts\n",
      "[ episode 281 ][ timestamp 8 ] state=[-0.04838669 -1.34012547  0.1334637   2.10672611], action=0, reward=1.0, next_state=[-0.0751892  -1.53630847  0.17559822  2.43750554]\n",
      "[ Experience replay ] starts\n",
      "[ episode 281 ][ timestamp 9 ] state=[-0.0751892  -1.53630847  0.17559822  2.43750554], action=0, reward=-1.0, next_state=[-0.10591537 -1.73244667  0.22434833  2.77854956]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 281: Exploration_rate=0.01. Score=9.\n",
      "[ episode 282 ] state=[ 0.02009807  0.00879812 -0.03012464  0.02945343]\n",
      "[ episode 282 ][ timestamp 1 ] state=[ 0.02009807  0.00879812 -0.03012464  0.02945343], action=0, reward=1.0, next_state=[ 0.02027404 -0.18587916 -0.02953558  0.31248156]\n",
      "[ Experience replay ] starts\n",
      "[ episode 282 ][ timestamp 2 ] state=[ 0.02027404 -0.18587916 -0.02953558  0.31248156], action=0, reward=1.0, next_state=[ 0.01655645 -0.38056817 -0.02328594  0.5957055 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 282 ][ timestamp 3 ] state=[ 0.01655645 -0.38056817 -0.02328594  0.5957055 ], action=0, reward=1.0, next_state=[ 0.00894509 -0.57535662 -0.01137183  0.88096351]\n",
      "[ Experience replay ] starts\n",
      "[ episode 282 ][ timestamp 4 ] state=[ 0.00894509 -0.57535662 -0.01137183  0.88096351], action=0, reward=1.0, next_state=[-0.00256204 -0.77032226  0.00624744  1.17004982]\n",
      "[ Experience replay ] starts\n",
      "[ episode 282 ][ timestamp 5 ] state=[-0.00256204 -0.77032226  0.00624744  1.17004982], action=0, reward=1.0, next_state=[-0.01796849 -0.96552491  0.02964843  1.46468481]\n",
      "[ Experience replay ] starts\n",
      "[ episode 282 ][ timestamp 6 ] state=[-0.01796849 -0.96552491  0.02964843  1.46468481], action=0, reward=1.0, next_state=[-0.03727899 -1.1609972   0.05894213  1.76647974]\n",
      "[ Experience replay ] starts\n",
      "[ episode 282 ][ timestamp 7 ] state=[-0.03727899 -1.1609972   0.05894213  1.76647974], action=0, reward=1.0, next_state=[-0.06049893 -1.35673347  0.09427172  2.07689324]\n",
      "[ Experience replay ] starts\n",
      "[ episode 282 ][ timestamp 8 ] state=[-0.06049893 -1.35673347  0.09427172  2.07689324], action=0, reward=1.0, next_state=[-0.0876336  -1.55267625  0.13580959  2.39717719]\n",
      "[ Experience replay ] starts\n",
      "[ episode 282 ][ timestamp 9 ] state=[-0.0876336  -1.55267625  0.13580959  2.39717719], action=0, reward=1.0, next_state=[-0.11868712 -1.74870007  0.18375313  2.72831085]\n",
      "[ Experience replay ] starts\n",
      "[ episode 282 ][ timestamp 10 ] state=[-0.11868712 -1.74870007  0.18375313  2.72831085], action=0, reward=-1.0, next_state=[-0.15366113 -1.94459289  0.23831935  3.07092316]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 282: Exploration_rate=0.01. Score=10.\n",
      "[ episode 283 ] state=[ 0.04109828 -0.03567806  0.01142942 -0.00190164]\n",
      "[ episode 283 ][ timestamp 1 ] state=[ 0.04109828 -0.03567806  0.01142942 -0.00190164], action=0, reward=1.0, next_state=[ 0.04038472 -0.23096205  0.01139139  0.29436539]\n",
      "[ Experience replay ] starts\n",
      "[ episode 283 ][ timestamp 2 ] state=[ 0.04038472 -0.23096205  0.01139139  0.29436539], action=0, reward=1.0, next_state=[ 0.03576548 -0.42624453  0.0172787   0.59061911]\n",
      "[ Experience replay ] starts\n",
      "[ episode 283 ][ timestamp 3 ] state=[ 0.03576548 -0.42624453  0.0172787   0.59061911], action=0, reward=1.0, next_state=[ 0.02724059 -0.62160409  0.02909108  0.88869439]\n",
      "[ Experience replay ] starts\n",
      "[ episode 283 ][ timestamp 4 ] state=[ 0.02724059 -0.62160409  0.02909108  0.88869439], action=0, reward=1.0, next_state=[ 0.01480851 -0.8171085   0.04686497  1.19037849]\n",
      "[ Experience replay ] starts\n",
      "[ episode 283 ][ timestamp 5 ] state=[ 0.01480851 -0.8171085   0.04686497  1.19037849], action=0, reward=1.0, next_state=[-0.00153366 -1.01280539  0.07067254  1.49737478]\n",
      "[ Experience replay ] starts\n",
      "[ episode 283 ][ timestamp 6 ] state=[-0.00153366 -1.01280539  0.07067254  1.49737478], action=0, reward=1.0, next_state=[-0.02178977 -1.20871158  0.10062003  1.81126095]\n",
      "[ Experience replay ] starts\n",
      "[ episode 283 ][ timestamp 7 ] state=[-0.02178977 -1.20871158  0.10062003  1.81126095], action=0, reward=1.0, next_state=[-0.045964   -1.40480056  0.13684525  2.13343912]\n",
      "[ Experience replay ] starts\n",
      "[ episode 283 ][ timestamp 8 ] state=[-0.045964   -1.40480056  0.13684525  2.13343912], action=0, reward=1.0, next_state=[-0.07406001 -1.60098774  0.17951404  2.4650758 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 283 ][ timestamp 9 ] state=[-0.07406001 -1.60098774  0.17951404  2.4650758 ], action=0, reward=-1.0, next_state=[-0.10607977 -1.79711321  0.22881555  2.80703068]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 283: Exploration_rate=0.01. Score=9.\n",
      "[ episode 284 ] state=[-0.04173717 -0.04654033  0.03300641  0.03538015]\n",
      "[ episode 284 ][ timestamp 1 ] state=[-0.04173717 -0.04654033  0.03300641  0.03538015], action=0, reward=1.0, next_state=[-0.04266798 -0.24211968  0.03371401  0.3382915 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 284 ][ timestamp 2 ] state=[-0.04266798 -0.24211968  0.03371401  0.3382915 ], action=0, reward=1.0, next_state=[-0.04751037 -0.43770475  0.04047984  0.64141243]\n",
      "[ Experience replay ] starts\n",
      "[ episode 284 ][ timestamp 3 ] state=[-0.04751037 -0.43770475  0.04047984  0.64141243], action=0, reward=1.0, next_state=[-0.05626447 -0.63336691  0.05330809  0.94656307]\n",
      "[ Experience replay ] starts\n",
      "[ episode 284 ][ timestamp 4 ] state=[-0.05626447 -0.63336691  0.05330809  0.94656307], action=0, reward=1.0, next_state=[-0.0689318  -0.82916468  0.07223935  1.25550767]\n",
      "[ Experience replay ] starts\n",
      "[ episode 284 ][ timestamp 5 ] state=[-0.0689318  -0.82916468  0.07223935  1.25550767], action=0, reward=1.0, next_state=[-0.0855151  -1.02513335  0.0973495   1.56991392]\n",
      "[ Experience replay ] starts\n",
      "[ episode 284 ][ timestamp 6 ] state=[-0.0855151  -1.02513335  0.0973495   1.56991392], action=0, reward=1.0, next_state=[-0.10601776 -1.22127335  0.12874778  1.89130647]\n",
      "[ Experience replay ] starts\n",
      "[ episode 284 ][ timestamp 7 ] state=[-0.10601776 -1.22127335  0.12874778  1.89130647], action=0, reward=1.0, next_state=[-0.13044323 -1.4175366   0.16657391  2.22101214]\n",
      "[ Experience replay ] starts\n",
      "[ episode 284 ][ timestamp 8 ] state=[-0.13044323 -1.4175366   0.16657391  2.22101214], action=0, reward=-1.0, next_state=[-0.15879396 -1.61381075  0.21099415  2.56009488]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 284: Exploration_rate=0.01. Score=8.\n",
      "[ episode 285 ] state=[0.03600277 0.01561071 0.03360163 0.03455977]\n",
      "[ episode 285 ][ timestamp 1 ] state=[0.03600277 0.01561071 0.03360163 0.03455977], action=0, reward=1.0, next_state=[ 0.03631498 -0.17997658  0.03429282  0.33765213]\n",
      "[ Experience replay ] starts\n",
      "[ episode 285 ][ timestamp 2 ] state=[ 0.03631498 -0.17997658  0.03429282  0.33765213], action=0, reward=1.0, next_state=[ 0.03271545 -0.37556932  0.04104586  0.64094885]\n",
      "[ Experience replay ] starts\n",
      "[ episode 285 ][ timestamp 3 ] state=[ 0.03271545 -0.37556932  0.04104586  0.64094885], action=0, reward=1.0, next_state=[ 0.02520407 -0.57123872  0.05386484  0.94626983]\n",
      "[ Experience replay ] starts\n",
      "[ episode 285 ][ timestamp 4 ] state=[ 0.02520407 -0.57123872  0.05386484  0.94626983], action=0, reward=1.0, next_state=[ 0.01377929 -0.76704311  0.07279024  1.25537905]\n",
      "[ Experience replay ] starts\n",
      "[ episode 285 ][ timestamp 5 ] state=[ 0.01377929 -0.76704311  0.07279024  1.25537905], action=0, reward=1.0, next_state=[-1.56157063e-03 -9.63017636e-01  9.78978188e-02  1.56994385e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 285 ][ timestamp 6 ] state=[-1.56157063e-03 -9.63017636e-01  9.78978188e-02  1.56994385e+00], action=0, reward=1.0, next_state=[-0.02082192 -1.15916249  0.1292967   1.89148838]\n",
      "[ Experience replay ] starts\n",
      "[ episode 285 ][ timestamp 7 ] state=[-0.02082192 -1.15916249  0.1292967   1.89148838], action=0, reward=1.0, next_state=[-0.04400517 -1.35542935  0.16712646  2.22133866]\n",
      "[ Experience replay ] starts\n",
      "[ episode 285 ][ timestamp 8 ] state=[-0.04400517 -1.35542935  0.16712646  2.22133866], action=0, reward=-1.0, next_state=[-0.07111376 -1.55170553  0.21155324  2.56055758]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 285: Exploration_rate=0.01. Score=8.\n",
      "[ episode 286 ] state=[-0.01789238  0.02631775  0.01686206  0.02487838]\n",
      "[ episode 286 ][ timestamp 1 ] state=[-0.01789238  0.02631775  0.01686206  0.02487838], action=0, reward=1.0, next_state=[-0.01736603 -0.16904191  0.01735963  0.32283342]\n",
      "[ Experience replay ] starts\n",
      "[ episode 286 ][ timestamp 2 ] state=[-0.01736603 -0.16904191  0.01735963  0.32283342], action=0, reward=1.0, next_state=[-0.02074687 -0.3644067   0.0238163   0.62093993]\n",
      "[ Experience replay ] starts\n",
      "[ episode 286 ][ timestamp 3 ] state=[-0.02074687 -0.3644067   0.0238163   0.62093993], action=0, reward=1.0, next_state=[-0.028035   -0.55985301  0.0362351   0.92102759]\n",
      "[ Experience replay ] starts\n",
      "[ episode 286 ][ timestamp 4 ] state=[-0.028035   -0.55985301  0.0362351   0.92102759], action=0, reward=1.0, next_state=[-0.03923206 -0.75544541  0.05465565  1.22487438]\n",
      "[ Experience replay ] starts\n",
      "[ episode 286 ][ timestamp 5 ] state=[-0.03923206 -0.75544541  0.05465565  1.22487438], action=0, reward=1.0, next_state=[-0.05434097 -0.95122693  0.07915314  1.5341689 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 286 ][ timestamp 6 ] state=[-0.05434097 -0.95122693  0.07915314  1.5341689 ], action=0, reward=1.0, next_state=[-0.07336551 -1.14720809  0.10983652  1.85046696]\n",
      "[ Experience replay ] starts\n",
      "[ episode 286 ][ timestamp 7 ] state=[-0.07336551 -1.14720809  0.10983652  1.85046696], action=0, reward=1.0, next_state=[-0.09630967 -1.34335398  0.14684585  2.17513988]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 286 ][ timestamp 8 ] state=[-0.09630967 -1.34335398  0.14684585  2.17513988], action=0, reward=1.0, next_state=[-0.12317675 -1.53956901  0.19034865  2.50931247]\n",
      "[ Experience replay ] starts\n",
      "[ episode 286 ][ timestamp 9 ] state=[-0.12317675 -1.53956901  0.19034865  2.50931247], action=0, reward=-1.0, next_state=[-0.15396813 -1.73567943  0.2405349   2.85379014]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 286: Exploration_rate=0.01. Score=9.\n",
      "[ episode 287 ] state=[ 0.00681531 -0.01760415 -0.04047668  0.00628327]\n",
      "[ episode 287 ][ timestamp 1 ] state=[ 0.00681531 -0.01760415 -0.04047668  0.00628327], action=0, reward=1.0, next_state=[ 0.00646322 -0.21212294 -0.04035102  0.28592557]\n",
      "[ Experience replay ] starts\n",
      "[ episode 287 ][ timestamp 2 ] state=[ 0.00646322 -0.21212294 -0.04035102  0.28592557], action=0, reward=1.0, next_state=[ 0.00222076 -0.40664688 -0.0346325   0.56561399]\n",
      "[ Experience replay ] starts\n",
      "[ episode 287 ][ timestamp 3 ] state=[ 0.00222076 -0.40664688 -0.0346325   0.56561399], action=0, reward=1.0, next_state=[-0.00591217 -0.60126628 -0.02332022  0.84718812]\n",
      "[ Experience replay ] starts\n",
      "[ episode 287 ][ timestamp 4 ] state=[-0.00591217 -0.60126628 -0.02332022  0.84718812], action=0, reward=1.0, next_state=[-0.0179375  -0.79606248 -0.00637646  1.13244745]\n",
      "[ Experience replay ] starts\n",
      "[ episode 287 ][ timestamp 5 ] state=[-0.0179375  -0.79606248 -0.00637646  1.13244745], action=0, reward=1.0, next_state=[-0.03385875 -0.99110039  0.01627249  1.42312369]\n",
      "[ Experience replay ] starts\n",
      "[ episode 287 ][ timestamp 6 ] state=[-0.03385875 -0.99110039  0.01627249  1.42312369], action=0, reward=1.0, next_state=[-0.05368076 -1.18641973  0.04473496  1.72084782]\n",
      "[ Experience replay ] starts\n",
      "[ episode 287 ][ timestamp 7 ] state=[-0.05368076 -1.18641973  0.04473496  1.72084782], action=0, reward=1.0, next_state=[-0.07740915 -1.38202457  0.07915192  2.02710923]\n",
      "[ Experience replay ] starts\n",
      "[ episode 287 ][ timestamp 8 ] state=[-0.07740915 -1.38202457  0.07915192  2.02710923], action=0, reward=1.0, next_state=[-0.10504964 -1.57787037  0.1196941   2.34320454]\n",
      "[ Experience replay ] starts\n",
      "[ episode 287 ][ timestamp 9 ] state=[-0.10504964 -1.57787037  0.1196941   2.34320454], action=0, reward=1.0, next_state=[-0.13660705 -1.77384846  0.16655819  2.67017451]\n",
      "[ Experience replay ] starts\n",
      "[ episode 287 ][ timestamp 10 ] state=[-0.13660705 -1.77384846  0.16655819  2.67017451], action=0, reward=-1.0, next_state=[-0.17208402 -1.96976792  0.21996168  3.00872878]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 287: Exploration_rate=0.01. Score=10.\n",
      "[ episode 288 ] state=[-0.04095196  0.02241386  0.01124837  0.02435368]\n",
      "[ episode 288 ][ timestamp 1 ] state=[-0.04095196  0.02241386  0.01124837  0.02435368], action=0, reward=1.0, next_state=[-0.04050368 -0.17286758  0.01173544  0.32056427]\n",
      "[ Experience replay ] starts\n",
      "[ episode 288 ][ timestamp 2 ] state=[-0.04050368 -0.17286758  0.01173544  0.32056427], action=0, reward=1.0, next_state=[-0.04396103 -0.36815468  0.01814673  0.61692488]\n",
      "[ Experience replay ] starts\n",
      "[ episode 288 ][ timestamp 3 ] state=[-0.04396103 -0.36815468  0.01814673  0.61692488], action=0, reward=1.0, next_state=[-0.05132413 -0.56352538  0.03048523  0.91526752]\n",
      "[ Experience replay ] starts\n",
      "[ episode 288 ][ timestamp 4 ] state=[-0.05132413 -0.56352538  0.03048523  0.91526752], action=0, reward=1.0, next_state=[-0.06259464 -0.75904606  0.04879058  1.21737354]\n",
      "[ Experience replay ] starts\n",
      "[ episode 288 ][ timestamp 5 ] state=[-0.06259464 -0.75904606  0.04879058  1.21737354], action=0, reward=1.0, next_state=[-0.07777556 -0.95476205  0.07313805  1.52493691]\n",
      "[ Experience replay ] starts\n",
      "[ episode 288 ][ timestamp 6 ] state=[-0.07777556 -0.95476205  0.07313805  1.52493691], action=0, reward=1.0, next_state=[-0.0968708  -1.15068684  0.10363679  1.83952184]\n",
      "[ Experience replay ] starts\n",
      "[ episode 288 ][ timestamp 7 ] state=[-0.0968708  -1.15068684  0.10363679  1.83952184], action=0, reward=1.0, next_state=[-0.11988453 -1.34678938  0.14042722  2.16251208]\n",
      "[ Experience replay ] starts\n",
      "[ episode 288 ][ timestamp 8 ] state=[-0.11988453 -1.34678938  0.14042722  2.16251208], action=0, reward=1.0, next_state=[-0.14682032 -1.54297909  0.18367747  2.49504983]\n",
      "[ Experience replay ] starts\n",
      "[ episode 288 ][ timestamp 9 ] state=[-0.14682032 -1.54297909  0.18367747  2.49504983], action=0, reward=-1.0, next_state=[-0.1776799  -1.73908841  0.23357846  2.83796362]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 288: Exploration_rate=0.01. Score=9.\n",
      "[ episode 289 ] state=[-0.0232944  -0.01329455 -0.03407638  0.0224213 ]\n",
      "[ episode 289 ][ timestamp 1 ] state=[-0.0232944  -0.01329455 -0.03407638  0.0224213 ], action=0, reward=1.0, next_state=[-0.0235603  -0.20791166 -0.03362795  0.30416098]\n",
      "[ Experience replay ] starts\n",
      "[ episode 289 ][ timestamp 2 ] state=[-0.0235603  -0.20791166 -0.03362795  0.30416098], action=0, reward=1.0, next_state=[-0.02771853 -0.40253864 -0.02754473  0.58605163]\n",
      "[ Experience replay ] starts\n",
      "[ episode 289 ][ timestamp 3 ] state=[-0.02771853 -0.40253864 -0.02754473  0.58605163], action=0, reward=1.0, next_state=[-0.0357693  -0.59726418 -0.0158237   0.86993202]\n",
      "[ Experience replay ] starts\n",
      "[ episode 289 ][ timestamp 4 ] state=[-0.0357693  -0.59726418 -0.0158237   0.86993202], action=0, reward=1.0, next_state=[-0.04771459 -0.79216735  0.00157494  1.15759819]\n",
      "[ Experience replay ] starts\n",
      "[ episode 289 ][ timestamp 5 ] state=[-0.04771459 -0.79216735  0.00157494  1.15759819], action=0, reward=1.0, next_state=[-0.06355793 -0.98730979  0.0247269   1.45077453]\n",
      "[ Experience replay ] starts\n",
      "[ episode 289 ][ timestamp 6 ] state=[-0.06355793 -0.98730979  0.0247269   1.45077453], action=0, reward=1.0, next_state=[-0.08330413 -1.18272671  0.05374239  1.75107926]\n",
      "[ Experience replay ] starts\n",
      "[ episode 289 ][ timestamp 7 ] state=[-0.08330413 -1.18272671  0.05374239  1.75107926], action=0, reward=1.0, next_state=[-0.10695866 -1.37841591  0.08876398  2.05998193]\n",
      "[ Experience replay ] starts\n",
      "[ episode 289 ][ timestamp 8 ] state=[-0.10695866 -1.37841591  0.08876398  2.05998193], action=0, reward=1.0, next_state=[-0.13452698 -1.57432454  0.12996362  2.37875031]\n",
      "[ Experience replay ] starts\n",
      "[ episode 289 ][ timestamp 9 ] state=[-0.13452698 -1.57432454  0.12996362  2.37875031], action=0, reward=1.0, next_state=[-0.16601347 -1.77033312  0.17753862  2.70838549]\n",
      "[ Experience replay ] starts\n",
      "[ episode 289 ][ timestamp 10 ] state=[-0.16601347 -1.77033312  0.17753862  2.70838549], action=0, reward=-1.0, next_state=[-0.20142013 -1.96623709  0.23170633  3.04954502]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 289: Exploration_rate=0.01. Score=10.\n",
      "[ episode 290 ] state=[0.0332761  0.01614172 0.01836639 0.0319052 ]\n",
      "[ episode 290 ][ timestamp 1 ] state=[0.0332761  0.01614172 0.01836639 0.0319052 ], action=0, reward=1.0, next_state=[ 0.03359893 -0.17923873  0.01900449  0.33032587]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 2 ] state=[ 0.03359893 -0.17923873  0.01900449  0.33032587], action=0, reward=1.0, next_state=[ 0.03001416 -0.37462598  0.02561101  0.62894081]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 3 ] state=[ 0.03001416 -0.37462598  0.02561101  0.62894081], action=0, reward=1.0, next_state=[ 0.02252164 -0.57009581  0.03818983  0.92957821]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 4 ] state=[ 0.02252164 -0.57009581  0.03818983  0.92957821], action=1, reward=1.0, next_state=[ 0.01111972 -0.37550959  0.05678139  0.64913679]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 5 ] state=[ 0.01111972 -0.37550959  0.05678139  0.64913679], action=1, reward=1.0, next_state=[ 0.00360953 -0.18122267  0.06976413  0.37486085]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 6 ] state=[ 0.00360953 -0.18122267  0.06976413  0.37486085], action=1, reward=1.0, next_state=[-1.49223787e-05  1.28425575e-02  7.72613453e-02  1.04965126e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 7 ] state=[-1.49223787e-05  1.28425575e-02  7.72613453e-02  1.04965126e-01], action=1, reward=1.0, next_state=[ 0.00024193  0.20677716  0.07936065 -0.16237673]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 8 ] state=[ 0.00024193  0.20677716  0.07936065 -0.16237673], action=1, reward=1.0, next_state=[ 0.00437747  0.40067861  0.07611311 -0.42900592]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 9 ] state=[ 0.00437747  0.40067861  0.07611311 -0.42900592], action=1, reward=1.0, next_state=[ 0.01239104  0.59464479  0.06753299 -0.69675718]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 10 ] state=[ 0.01239104  0.59464479  0.06753299 -0.69675718], action=0, reward=1.0, next_state=[ 0.02428394  0.39865452  0.05359785 -0.3836023 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 11 ] state=[ 0.02428394  0.39865452  0.05359785 -0.3836023 ], action=0, reward=1.0, next_state=[ 0.03225703  0.20281419  0.04592581 -0.07451342]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 12 ] state=[ 0.03225703  0.20281419  0.04592581 -0.07451342], action=0, reward=1.0, next_state=[0.03631331 0.00706496 0.04443554 0.23229827]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 290 ][ timestamp 13 ] state=[0.03631331 0.00706496 0.04443554 0.23229827], action=1, reward=1.0, next_state=[ 0.03645461  0.20152474  0.0490815  -0.04604373]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 14 ] state=[ 0.03645461  0.20152474  0.0490815  -0.04604373], action=0, reward=1.0, next_state=[0.04048511 0.0057346  0.04816063 0.26171197]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 15 ] state=[0.04048511 0.0057346  0.04816063 0.26171197], action=1, reward=1.0, next_state=[ 0.0405998   0.20013718  0.05339487 -0.01540003]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 16 ] state=[ 0.0405998   0.20013718  0.05339487 -0.01540003], action=1, reward=1.0, next_state=[ 0.04460254  0.39445434  0.05308687 -0.29076973]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 17 ] state=[ 0.04460254  0.39445434  0.05308687 -0.29076973], action=1, reward=1.0, next_state=[ 0.05249163  0.58878072  0.04727147 -0.56624845]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 18 ] state=[ 0.05249163  0.58878072  0.04727147 -0.56624845], action=0, reward=1.0, next_state=[ 0.06426724  0.3930286   0.0359465  -0.25905564]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 19 ] state=[ 0.06426724  0.3930286   0.0359465  -0.25905564], action=0, reward=1.0, next_state=[0.07212782 0.1974124  0.03076539 0.04474509]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 20 ] state=[0.07212782 0.1974124  0.03076539 0.04474509], action=0, reward=1.0, next_state=[0.07607606 0.00186311 0.03166029 0.34697382]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 21 ] state=[0.07607606 0.00186311 0.03166029 0.34697382], action=1, reward=1.0, next_state=[0.07611333 0.19652076 0.03859977 0.06444026]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 22 ] state=[0.07611333 0.19652076 0.03859977 0.06444026], action=1, reward=1.0, next_state=[ 0.08004374  0.39106864  0.03988857 -0.21581868]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 23 ] state=[ 0.08004374  0.39106864  0.03988857 -0.21581868], action=1, reward=1.0, next_state=[ 0.08786512  0.58559831  0.0355722  -0.49565695]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 24 ] state=[ 0.08786512  0.58559831  0.0355722  -0.49565695], action=0, reward=1.0, next_state=[ 0.09957708  0.38999326  0.02565906 -0.19197897]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 25 ] state=[ 0.09957708  0.38999326  0.02565906 -0.19197897], action=0, reward=1.0, next_state=[0.10737695 0.19451382 0.02181948 0.1086866 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 26 ] state=[0.10737695 0.19451382 0.02181948 0.1086866 ], action=1, reward=1.0, next_state=[ 0.11126722  0.38931641  0.02399321 -0.17703331]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 27 ] state=[ 0.11126722  0.38931641  0.02399321 -0.17703331], action=1, reward=1.0, next_state=[ 0.11905355  0.58408693  0.02045255 -0.46205167]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 28 ] state=[ 0.11905355  0.58408693  0.02045255 -0.46205167], action=1, reward=1.0, next_state=[ 0.13073529  0.77891394  0.01121151 -0.74821844]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 29 ] state=[ 0.13073529  0.77891394  0.01121151 -0.74821844], action=0, reward=1.0, next_state=[ 0.14631357  0.58363913 -0.00375286 -0.45202852]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 30 ] state=[ 0.14631357  0.58363913 -0.00375286 -0.45202852], action=1, reward=1.0, next_state=[ 0.15798635  0.77881395 -0.01279343 -0.74589203]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 31 ] state=[ 0.15798635  0.77881395 -0.01279343 -0.74589203], action=0, reward=1.0, next_state=[ 0.17356263  0.58387085 -0.02771127 -0.45726247]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 32 ] state=[ 0.17356263  0.58387085 -0.02771127 -0.45726247], action=1, reward=1.0, next_state=[ 0.18524005  0.77937338 -0.03685652 -0.75854975]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 33 ] state=[ 0.18524005  0.77937338 -0.03685652 -0.75854975], action=1, reward=1.0, next_state=[ 0.20082751  0.97498331 -0.05202751 -1.06259874]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 34 ] state=[ 0.20082751  0.97498331 -0.05202751 -1.06259874], action=0, reward=1.0, next_state=[ 0.22032718  0.78058735 -0.07327949 -0.78668854]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 35 ] state=[ 0.22032718  0.78058735 -0.07327949 -0.78668854], action=0, reward=1.0, next_state=[ 0.23593893  0.58654448 -0.08901326 -0.51793027]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 36 ] state=[ 0.23593893  0.58654448 -0.08901326 -0.51793027], action=1, reward=1.0, next_state=[ 0.24766982  0.78279953 -0.09937186 -0.83728272]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 37 ] state=[ 0.24766982  0.78279953 -0.09937186 -0.83728272], action=1, reward=1.0, next_state=[ 0.26332581  0.97912801 -0.11611752 -1.15948989]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 38 ] state=[ 0.26332581  0.97912801 -0.11611752 -1.15948989], action=0, reward=1.0, next_state=[ 0.28290837  0.78569445 -0.13930731 -0.90535532]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 39 ] state=[ 0.28290837  0.78569445 -0.13930731 -0.90535532], action=0, reward=1.0, next_state=[ 0.29862226  0.59270585 -0.15741442 -0.65950082]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 40 ] state=[ 0.29862226  0.59270585 -0.15741442 -0.65950082], action=0, reward=1.0, next_state=[ 0.31047637  0.4000842  -0.17060444 -0.42022967]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 41 ] state=[ 0.31047637  0.4000842  -0.17060444 -0.42022967], action=0, reward=1.0, next_state=[ 0.31847806  0.20773819 -0.17900903 -0.18581403]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 42 ] state=[ 0.31847806  0.20773819 -0.17900903 -0.18581403], action=0, reward=1.0, next_state=[ 0.32263282  0.01556879 -0.18272531  0.04548592]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 43 ] state=[ 0.32263282  0.01556879 -0.18272531  0.04548592], action=0, reward=1.0, next_state=[ 0.3229442  -0.17652682 -0.18181559  0.27540957]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 44 ] state=[ 0.3229442  -0.17652682 -0.18181559  0.27540957], action=0, reward=1.0, next_state=[ 0.31941366 -0.36865212 -0.1763074   0.50568756]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 45 ] state=[ 0.31941366 -0.36865212 -0.1763074   0.50568756], action=1, reward=1.0, next_state=[ 0.31204062 -0.17154119 -0.16619365  0.16303831]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 46 ] state=[ 0.31204062 -0.17154119 -0.16619365  0.16303831], action=0, reward=1.0, next_state=[ 0.3086098  -0.36394234 -0.16293288  0.39902725]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 47 ] state=[ 0.3086098  -0.36394234 -0.16293288  0.39902725], action=0, reward=1.0, next_state=[ 0.30133095 -0.55642334 -0.15495234  0.63623425]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 48 ] state=[ 0.30133095 -0.55642334 -0.15495234  0.63623425], action=0, reward=1.0, next_state=[ 0.29020248 -0.74908362 -0.14222765  0.87638834]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 49 ] state=[ 0.29020248 -0.74908362 -0.14222765  0.87638834], action=0, reward=1.0, next_state=[ 0.27522081 -0.94201579 -0.12469989  1.12119035]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 50 ] state=[ 0.27522081 -0.94201579 -0.12469989  1.12119035], action=0, reward=1.0, next_state=[ 0.25638049 -1.13530159 -0.10227608  1.37230095]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 51 ] state=[ 0.25638049 -1.13530159 -0.10227608  1.37230095], action=0, reward=1.0, next_state=[ 0.23367446 -1.3290068  -0.07483006  1.63132363]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 52 ] state=[ 0.23367446 -1.3290068  -0.07483006  1.63132363], action=0, reward=1.0, next_state=[ 0.20709433 -1.52317428 -0.04220359  1.89978028]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 53 ] state=[ 0.20709433 -1.52317428 -0.04220359  1.89978028], action=1, reward=1.0, next_state=[ 0.17663084 -1.32762181 -0.00420798  1.59430861]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 290 ][ timestamp 54 ] state=[ 0.17663084 -1.32762181 -0.00420798  1.59430861], action=1, reward=1.0, next_state=[ 0.1500784  -1.1324502   0.02767819  1.30031664]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 55 ] state=[ 0.1500784  -1.1324502   0.02767819  1.30031664], action=1, reward=1.0, next_state=[ 0.1274294  -0.93769026  0.05368452  1.01642496]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 56 ] state=[ 0.1274294  -0.93769026  0.05368452  1.01642496], action=1, reward=1.0, next_state=[ 0.10867559 -0.74332361  0.07401302  0.74107068]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 57 ] state=[ 0.10867559 -0.74332361  0.07401302  0.74107068], action=1, reward=1.0, next_state=[ 0.09380912 -0.54929725  0.08883444  0.4725679 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 58 ] state=[ 0.09380912 -0.54929725  0.08883444  0.4725679 ], action=1, reward=1.0, next_state=[ 0.08282318 -0.35553488  0.09828579  0.20915339]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 59 ] state=[ 0.08282318 -0.35553488  0.09828579  0.20915339], action=0, reward=1.0, next_state=[ 0.07571248 -0.55191478  0.10246886  0.53115113]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 60 ] state=[ 0.07571248 -0.55191478  0.10246886  0.53115113], action=1, reward=1.0, next_state=[ 0.06467418 -0.35837214  0.11309188  0.27243312]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 61 ] state=[ 0.06467418 -0.35837214  0.11309188  0.27243312], action=1, reward=1.0, next_state=[ 0.05750674 -0.16503029  0.11854055  0.01745115]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 62 ] state=[ 0.05750674 -0.16503029  0.11854055  0.01745115], action=1, reward=1.0, next_state=[ 0.05420614  0.02820977  0.11888957 -0.23560543]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 63 ] state=[ 0.05420614  0.02820977  0.11888957 -0.23560543], action=1, reward=1.0, next_state=[ 0.05477033  0.2214502   0.11417746 -0.48854868]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 64 ] state=[ 0.05477033  0.2214502   0.11417746 -0.48854868], action=1, reward=1.0, next_state=[ 0.05919934  0.41479177  0.10440649 -0.74317744]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 65 ] state=[ 0.05919934  0.41479177  0.10440649 -0.74317744], action=1, reward=1.0, next_state=[ 0.06749517  0.60832959  0.08954294 -1.00126356]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 66 ] state=[ 0.06749517  0.60832959  0.08954294 -1.00126356], action=1, reward=1.0, next_state=[ 0.07966176  0.80214827  0.06951767 -1.26453639]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 67 ] state=[ 0.07966176  0.80214827  0.06951767 -1.26453639], action=0, reward=1.0, next_state=[ 0.09570473  0.60621008  0.04422694 -0.95091726]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 68 ] state=[ 0.09570473  0.60621008  0.04422694 -0.95091726], action=0, reward=1.0, next_state=[ 0.10782893  0.41052167  0.02520859 -0.6446732 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 69 ] state=[ 0.10782893  0.41052167  0.02520859 -0.6446732 ], action=1, reward=1.0, next_state=[ 0.11603936  0.60528342  0.01231513 -0.92931245]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 70 ] state=[ 0.11603936  0.60528342  0.01231513 -0.92931245], action=1, reward=1.0, next_state=[ 0.12814503  0.80023698 -0.00627112 -1.21810007]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 71 ] state=[ 0.12814503  0.80023698 -0.00627112 -1.21810007], action=0, reward=1.0, next_state=[ 0.14414977  0.60519645 -0.03063312 -0.92738872]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 72 ] state=[ 0.14414977  0.60519645 -0.03063312 -0.92738872], action=0, reward=1.0, next_state=[ 0.1562537   0.41050121 -0.04918089 -0.6444876 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 73 ] state=[ 0.1562537   0.41050121 -0.04918089 -0.6444876 ], action=1, reward=1.0, next_state=[ 0.16446372  0.60627281 -0.06207065 -0.95224328]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 74 ] state=[ 0.16446372  0.60627281 -0.06207065 -0.95224328], action=0, reward=1.0, next_state=[ 0.17658918  0.41203857 -0.08111551 -0.67969005]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 75 ] state=[ 0.17658918  0.41203857 -0.08111551 -0.67969005], action=0, reward=1.0, next_state=[ 0.18482995  0.21813147 -0.09470931 -0.41360758]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 76 ] state=[ 0.18482995  0.21813147 -0.09470931 -0.41360758], action=1, reward=1.0, next_state=[ 0.18919258  0.41445933 -0.10298146 -0.73458251]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 77 ] state=[ 0.18919258  0.41445933 -0.10298146 -0.73458251], action=0, reward=1.0, next_state=[ 0.19748177  0.22089941 -0.11767311 -0.4760039 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 78 ] state=[ 0.19748177  0.22089941 -0.11767311 -0.4760039 ], action=1, reward=1.0, next_state=[ 0.20189976  0.41746923 -0.12719319 -0.80333567]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 79 ] state=[ 0.20189976  0.41746923 -0.12719319 -0.80333567], action=0, reward=1.0, next_state=[ 0.21024914  0.22429938 -0.14325991 -0.55321563]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 80 ] state=[ 0.21024914  0.22429938 -0.14325991 -0.55321563], action=1, reward=1.0, next_state=[ 0.21473513  0.42111172 -0.15432422 -0.88738437]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 81 ] state=[ 0.21473513  0.42111172 -0.15432422 -0.88738437], action=0, reward=1.0, next_state=[ 0.22315736  0.22838314 -0.17207191 -0.64691862]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 82 ] state=[ 0.22315736  0.22838314 -0.17207191 -0.64691862], action=0, reward=1.0, next_state=[ 0.22772503  0.03602327 -0.18501028 -0.4129798 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 83 ] state=[ 0.22772503  0.03602327 -0.18501028 -0.4129798 ], action=0, reward=1.0, next_state=[ 0.22844549 -0.15606049 -0.19326987 -0.18385446]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 84 ] state=[ 0.22844549 -0.15606049 -0.19326987 -0.18385446], action=0, reward=1.0, next_state=[ 0.22532428 -0.34796749 -0.19694696  0.04217824]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 85 ] state=[ 0.22532428 -0.34796749 -0.19694696  0.04217824], action=0, reward=1.0, next_state=[ 0.21836493 -0.5398003  -0.1961034   0.26683603]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 86 ] state=[ 0.21836493 -0.5398003  -0.1961034   0.26683603], action=0, reward=1.0, next_state=[ 0.20756893 -0.73166194 -0.19076668  0.49182687]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 87 ] state=[ 0.20756893 -0.73166194 -0.19076668  0.49182687], action=0, reward=1.0, next_state=[ 0.19293569 -0.92365359 -0.18093014  0.71884416]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 88 ] state=[ 0.19293569 -0.92365359 -0.18093014  0.71884416], action=0, reward=1.0, next_state=[ 0.17446261 -1.11587229 -0.16655326  0.94956205]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 89 ] state=[ 0.17446261 -1.11587229 -0.16655326  0.94956205], action=0, reward=1.0, next_state=[ 0.15214517 -1.30840819 -0.14756202  1.18562886]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 90 ] state=[ 0.15214517 -1.30840819 -0.14756202  1.18562886], action=1, reward=1.0, next_state=[ 0.12597701 -1.11171333 -0.12384944  0.85056698]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 91 ] state=[ 0.12597701 -1.11171333 -0.12384944  0.85056698], action=1, reward=1.0, next_state=[ 0.10374274 -0.91513999 -0.1068381   0.52164675]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 92 ] state=[ 0.10374274 -0.91513999 -0.1068381   0.52164675], action=0, reward=1.0, next_state=[ 0.08543994 -1.1086087  -0.09640516  0.77884446]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 93 ] state=[ 0.08543994 -1.1086087  -0.09640516  0.77884446], action=0, reward=1.0, next_state=[ 0.06326776 -1.30228227 -0.08082828  1.03970664]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 94 ] state=[ 0.06326776 -1.30228227 -0.08082828  1.03970664], action=1, reward=1.0, next_state=[ 0.03722212 -1.10618486 -0.06003414  0.7227832 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 95 ] state=[ 0.03722212 -1.10618486 -0.06003414  0.7227832 ], action=1, reward=1.0, next_state=[ 0.01509842 -0.91028618 -0.04557848  0.41182511]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 96 ] state=[ 0.01509842 -0.91028618 -0.04557848  0.41182511], action=1, reward=1.0, next_state=[-0.0031073  -0.71454874 -0.03734198  0.10512844]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 97 ] state=[-0.0031073  -0.71454874 -0.03734198  0.10512844], action=1, reward=1.0, next_state=[-0.01739828 -0.5189121  -0.03523941 -0.19909793]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 98 ] state=[-0.01739828 -0.5189121  -0.03523941 -0.19909793], action=1, reward=1.0, next_state=[-0.02777652 -0.32330431 -0.03922137 -0.50268569]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 290 ][ timestamp 99 ] state=[-0.02777652 -0.32330431 -0.03922137 -0.50268569], action=0, reward=1.0, next_state=[-0.0342426  -0.51785213 -0.04927508 -0.22261652]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 100 ] state=[-0.0342426  -0.51785213 -0.04927508 -0.22261652], action=0, reward=1.0, next_state=[-0.04459965 -0.71223642 -0.05372741  0.054125  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 101 ] state=[-0.04459965 -0.71223642 -0.05372741  0.054125  ], action=0, reward=1.0, next_state=[-0.05884438 -0.90654847 -0.05264491  0.32938424]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 102 ] state=[-0.05884438 -0.90654847 -0.05264491  0.32938424], action=0, reward=1.0, next_state=[-0.07697534 -1.10088301 -0.04605723  0.60501173]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 103 ] state=[-0.07697534 -1.10088301 -0.04605723  0.60501173], action=0, reward=1.0, next_state=[-0.09899301 -1.29533165 -0.03395699  0.88283935]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 104 ] state=[-0.09899301 -1.29533165 -0.03395699  0.88283935], action=0, reward=1.0, next_state=[-0.12489964 -1.48997638 -0.0163002   1.1646567 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 105 ] state=[-0.12489964 -1.48997638 -0.0163002   1.1646567 ], action=0, reward=1.0, next_state=[-0.15469917 -1.68488239  0.00699293  1.45218482]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 106 ] state=[-0.15469917 -1.68488239  0.00699293  1.45218482], action=0, reward=1.0, next_state=[-0.18839681 -1.88008954  0.03603663  1.74704429]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 107 ] state=[-0.18839681 -1.88008954  0.03603663  1.74704429], action=1, reward=1.0, next_state=[-0.2259986  -1.68539517  0.07097751  1.46578482]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 108 ] state=[-0.2259986  -1.68539517  0.07097751  1.46578482], action=1, reward=1.0, next_state=[-0.25970651 -1.49121053  0.10029321  1.19609112]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 109 ] state=[-0.25970651 -1.49121053  0.10029321  1.19609112], action=1, reward=1.0, next_state=[-0.28953072 -1.29751963  0.12421503  0.93645156]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 110 ] state=[-0.28953072 -1.29751963  0.12421503  0.93645156], action=1, reward=1.0, next_state=[-0.31548111 -1.10427189  0.14294406  0.68523873]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 111 ] state=[-0.31548111 -1.10427189  0.14294406  0.68523873], action=1, reward=1.0, next_state=[-0.33756655 -0.91139331  0.15664884  0.44075423]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 112 ] state=[-0.33756655 -0.91139331  0.15664884  0.44075423], action=1, reward=1.0, next_state=[-0.35579442 -0.71879479  0.16546392  0.20126045]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 113 ] state=[-0.35579442 -0.71879479  0.16546392  0.20126045], action=1, reward=1.0, next_state=[-0.37017031 -0.52637821  0.16948913 -0.03499768]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 114 ] state=[-0.37017031 -0.52637821  0.16948913 -0.03499768], action=1, reward=1.0, next_state=[-0.38069788 -0.33404092  0.16878918 -0.26977806]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 115 ] state=[-0.38069788 -0.33404092  0.16878918 -0.26977806], action=1, reward=1.0, next_state=[-0.38737869 -0.14167922  0.16339362 -0.50483138]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 116 ] state=[-0.38737869 -0.14167922  0.16339362 -0.50483138], action=1, reward=1.0, next_state=[-0.39021228  0.05080867  0.15329699 -0.74189332]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 117 ] state=[-0.39021228  0.05080867  0.15329699 -0.74189332], action=1, reward=1.0, next_state=[-0.3891961   0.24351962  0.13845912 -0.98267686]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 118 ] state=[-0.3891961   0.24351962  0.13845912 -0.98267686], action=1, reward=1.0, next_state=[-0.38432571  0.43654213  0.11880558 -1.22886271]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 119 ] state=[-0.38432571  0.43654213  0.11880558 -1.22886271], action=0, reward=1.0, next_state=[-0.37559487  0.24010887  0.09422833 -0.90144309]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 120 ] state=[-0.37559487  0.24010887  0.09422833 -0.90144309], action=1, reward=1.0, next_state=[-0.37079269  0.43383653  0.07619947 -1.16308331]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 121 ] state=[-0.37079269  0.43383653  0.07619947 -1.16308331], action=0, reward=1.0, next_state=[-0.36211596  0.23780959  0.0529378  -0.84751516]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 122 ] state=[-0.36211596  0.23780959  0.0529378  -0.84751516], action=0, reward=1.0, next_state=[-0.35735977  0.04200704  0.0359875  -0.53866634]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 123 ] state=[-0.35735977  0.04200704  0.0359875  -0.53866634], action=0, reward=1.0, next_state=[-0.35651963 -0.15360187  0.02521417 -0.23486492]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 124 ] state=[-0.35651963 -0.15360187  0.02521417 -0.23486492], action=1, reward=1.0, next_state=[-0.35959167  0.04115093  0.02051687 -0.51948907]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 125 ] state=[-0.35959167  0.04115093  0.02051687 -0.51948907], action=1, reward=1.0, next_state=[-0.35876865  0.23597812  0.01012709 -0.80563682]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 126 ] state=[-0.35876865  0.23597812  0.01012709 -0.80563682], action=0, reward=1.0, next_state=[-0.35404909  0.04071882 -0.00598564 -0.50978557]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 127 ] state=[-0.35404909  0.04071882 -0.00598564 -0.50978557], action=0, reward=1.0, next_state=[-0.35323471 -0.15431829 -0.01618136 -0.21899491]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 128 ] state=[-0.35323471 -0.15431829 -0.01618136 -0.21899491], action=0, reward=1.0, next_state=[-0.35632107 -0.34920525 -0.02056125  0.06854014]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 129 ] state=[-0.35632107 -0.34920525 -0.02056125  0.06854014], action=1, reward=1.0, next_state=[-0.36330518 -0.15379464 -0.01919045 -0.2305584 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 130 ] state=[-0.36330518 -0.15379464 -0.01919045 -0.2305584 ], action=0, reward=1.0, next_state=[-0.36638107 -0.34863718 -0.02380162  0.05600995]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 131 ] state=[-0.36638107 -0.34863718 -0.02380162  0.05600995], action=0, reward=1.0, next_state=[-0.37335382 -0.54340991 -0.02268142  0.34108928]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 132 ] state=[-0.37335382 -0.54340991 -0.02268142  0.34108928], action=0, reward=1.0, next_state=[-0.38422201 -0.73820193 -0.01585963  0.62653439]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 133 ] state=[-0.38422201 -0.73820193 -0.01585963  0.62653439], action=1, reward=1.0, next_state=[-0.39898605 -0.54286224 -0.00332895  0.32889916]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 134 ] state=[-0.39898605 -0.54286224 -0.00332895  0.32889916], action=1, reward=1.0, next_state=[-0.4098433  -0.34769306  0.00324904  0.0351683 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 135 ] state=[-0.4098433  -0.34769306  0.00324904  0.0351683 ], action=0, reward=1.0, next_state=[-0.41679716 -0.54286145  0.0039524   0.32887456]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 136 ] state=[-0.41679716 -0.54286145  0.0039524   0.32887456], action=0, reward=1.0, next_state=[-0.42765439 -0.73803944  0.01052989  0.62280127]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 137 ] state=[-0.42765439 -0.73803944  0.01052989  0.62280127], action=1, reward=1.0, next_state=[-0.44241518 -0.54306609  0.02298592  0.33345318]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 138 ] state=[-0.44241518 -0.54306609  0.02298592  0.33345318], action=1, reward=1.0, next_state=[-0.4532765  -0.34827871  0.02965498  0.04810656]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 139 ] state=[-0.4532765  -0.34827871  0.02965498  0.04810656], action=1, reward=1.0, next_state=[-0.46024207 -0.15359426  0.03061711 -0.23507442]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 290 ][ timestamp 140 ] state=[-0.46024207 -0.15359426  0.03061711 -0.23507442], action=1, reward=1.0, next_state=[-0.46331396  0.04107717  0.02591563 -0.51794469]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 141 ] state=[-0.46331396  0.04107717  0.02591563 -0.51794469], action=0, reward=1.0, next_state=[-0.46249241 -0.1543999   0.01555673 -0.21720921]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 142 ] state=[-0.46249241 -0.1543999   0.01555673 -0.21720921], action=0, reward=1.0, next_state=[-0.46558041 -0.34974074  0.01121255  0.0803401 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 143 ] state=[-0.46558041 -0.34974074  0.01121255  0.0803401 ], action=1, reward=1.0, next_state=[-0.47257523 -0.15478131  0.01281935 -0.20878426]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 144 ] state=[-0.47257523 -0.15478131  0.01281935 -0.20878426], action=0, reward=1.0, next_state=[-0.47567085 -0.35008419  0.00864366  0.08791479]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 145 ] state=[-0.47567085 -0.35008419  0.00864366  0.08791479], action=1, reward=1.0, next_state=[-0.48267254 -0.1550872   0.01040196 -0.20202857]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 146 ] state=[-0.48267254 -0.1550872   0.01040196 -0.20202857], action=1, reward=1.0, next_state=[-0.48577428  0.03988446  0.00636139 -0.49141211]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 147 ] state=[-0.48577428  0.03988446  0.00636139 -0.49141211], action=1, reward=1.0, next_state=[-0.48497659  0.2349161  -0.00346685 -0.78208342]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 148 ] state=[-0.48497659  0.2349161  -0.00346685 -0.78208342], action=0, reward=1.0, next_state=[-0.48027827  0.03984197 -0.01910852 -0.49049324]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 149 ] state=[-0.48027827  0.03984197 -0.01910852 -0.49049324], action=0, reward=1.0, next_state=[-0.47948143 -0.15500528 -0.02891839 -0.20389328]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 150 ] state=[-0.47948143 -0.15500528 -0.02891839 -0.20389328], action=0, reward=1.0, next_state=[-0.48258154 -0.349702   -0.03299625  0.07952886]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 151 ] state=[-0.48258154 -0.349702   -0.03299625  0.07952886], action=0, reward=1.0, next_state=[-0.48957558 -0.54433578 -0.03140568  0.36162148]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 152 ] state=[-0.48957558 -0.54433578 -0.03140568  0.36162148], action=0, reward=1.0, next_state=[-0.50046229 -0.73899759 -0.02417325  0.64423846]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 153 ] state=[-0.50046229 -0.73899759 -0.02417325  0.64423846], action=1, reward=1.0, next_state=[-0.51524224 -0.54354723 -0.01128848  0.34404234]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 154 ] state=[-0.51524224 -0.54354723 -0.01128848  0.34404234], action=1, reward=1.0, next_state=[-0.52611319 -0.34826653 -0.00440763  0.0478212 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 155 ] state=[-0.52611319 -0.34826653 -0.00440763  0.0478212 ], action=1, reward=1.0, next_state=[-0.53307852 -0.15308165 -0.00345121 -0.24624911]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 156 ] state=[-0.53307852 -0.15308165 -0.00345121 -0.24624911], action=0, reward=1.0, next_state=[-0.53614015 -0.34815414 -0.00837619  0.04534323]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 157 ] state=[-0.53614015 -0.34815414 -0.00837619  0.04534323], action=0, reward=1.0, next_state=[-0.54310323 -0.54315499 -0.00746932  0.33537167]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 158 ] state=[-0.54310323 -0.54315499 -0.00746932  0.33537167], action=0, reward=1.0, next_state=[-0.55396633 -0.73816984 -0.00076189  0.62568983]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 159 ] state=[-0.55396633 -0.73816984 -0.00076189  0.62568983], action=1, reward=1.0, next_state=[-0.56872973 -0.54303727  0.01175191  0.33276706]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 160 ] state=[-0.56872973 -0.54303727  0.01175191  0.33276706], action=1, reward=1.0, next_state=[-0.57959048 -0.34808454  0.01840725  0.04381314]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 161 ] state=[-0.57959048 -0.34808454  0.01840725  0.04381314], action=1, reward=1.0, next_state=[-0.58655217 -0.15323131  0.01928351 -0.24300576]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 162 ] state=[-0.58655217 -0.15323131  0.01928351 -0.24300576], action=0, reward=1.0, next_state=[-0.58961679 -0.34862332  0.0144234   0.05569676]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 163 ] state=[-0.58961679 -0.34862332  0.0144234   0.05569676], action=0, reward=1.0, next_state=[-0.59658926 -0.54394908  0.01553733  0.35289526]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 164 ] state=[-0.59658926 -0.54394908  0.01553733  0.35289526], action=1, reward=1.0, next_state=[-0.60746824 -0.34905148  0.02259524  0.06515193]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 165 ] state=[-0.60746824 -0.34905148  0.02259524  0.06515193], action=0, reward=1.0, next_state=[-0.61444927 -0.54448997  0.02389827  0.36487727]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 166 ] state=[-0.61444927 -0.54448997  0.02389827  0.36487727], action=1, reward=1.0, next_state=[-0.62533907 -0.34971566  0.03119582  0.07982466]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 167 ] state=[-0.62533907 -0.34971566  0.03119582  0.07982466], action=1, reward=1.0, next_state=[-0.63233338 -0.15505448  0.03279231 -0.20285497]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 168 ] state=[-0.63233338 -0.15505448  0.03279231 -0.20285497], action=0, reward=1.0, next_state=[-0.63543447 -0.35062968  0.02873521  0.09998933]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 169 ] state=[-0.63543447 -0.35062968  0.02873521  0.09998933], action=0, reward=1.0, next_state=[-0.64244707 -0.54615142  0.030735    0.40159786]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 170 ] state=[-0.64244707 -0.54615142  0.030735    0.40159786], action=0, reward=1.0, next_state=[-0.65337009 -0.74169553  0.03876696  0.70381016]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 171 ] state=[-0.65337009 -0.74169553  0.03876696  0.70381016], action=1, reward=1.0, next_state=[-0.66820401 -0.54713165  0.05284316  0.42357825]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 172 ] state=[-0.66820401 -0.54713165  0.05284316  0.42357825], action=1, reward=1.0, next_state=[-0.67914664 -0.35279656  0.06131473  0.14801118]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 173 ] state=[-0.67914664 -0.35279656  0.06131473  0.14801118], action=1, reward=1.0, next_state=[-0.68620257 -0.1586038   0.06427495 -0.12471535]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 174 ] state=[-0.68620257 -0.1586038   0.06427495 -0.12471535], action=1, reward=1.0, next_state=[-0.68937465  0.03554126  0.06178064 -0.39644777]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 175 ] state=[-0.68937465  0.03554126  0.06178064 -0.39644777], action=1, reward=1.0, next_state=[-0.68866382  0.22973474  0.05385169 -0.66903031]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 176 ] state=[-0.68866382  0.22973474  0.05385169 -0.66903031], action=0, reward=1.0, next_state=[-0.68406913  0.03390699  0.04047108 -0.35988976]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 177 ] state=[-0.68406913  0.03390699  0.04047108 -0.35988976], action=1, reward=1.0, next_state=[-0.68339099  0.22843097  0.03327329 -0.63954156]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 178 ] state=[-0.68339099  0.22843097  0.03327329 -0.63954156], action=0, reward=1.0, next_state=[-0.67882237  0.0328613   0.02048245 -0.33656888]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 179 ] state=[-0.67882237  0.0328613   0.02048245 -0.33656888], action=1, reward=1.0, next_state=[-0.67816514  0.22768587  0.01375108 -0.62272301]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 180 ] state=[-0.67816514  0.22768587  0.01375108 -0.62272301], action=0, reward=1.0, next_state=[-0.67361142  0.03237463  0.00129662 -0.32574117]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 181 ] state=[-0.67361142  0.03237463  0.00129662 -0.32574117], action=1, reward=1.0, next_state=[-0.67296393  0.2274781  -0.00521821 -0.61801492]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 182 ] state=[-0.67296393  0.2274781  -0.00521821 -0.61801492], action=1, reward=1.0, next_state=[-0.66841437  0.42267255 -0.01757851 -0.91233676]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 183 ] state=[-0.66841437  0.42267255 -0.01757851 -0.91233676], action=0, reward=1.0, next_state=[-0.65996092  0.22779278 -0.03582524 -0.62523008]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 184 ] state=[-0.65996092  0.22779278 -0.03582524 -0.62523008], action=0, reward=1.0, next_state=[-0.65540506  0.03318879 -0.04832984 -0.34404176]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 185 ] state=[-0.65540506  0.03318879 -0.04832984 -0.34404176], action=0, reward=1.0, next_state=[-0.65474129 -0.1612135  -0.05521068 -0.06698226]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 290 ][ timestamp 186 ] state=[-0.65474129 -0.1612135  -0.05521068 -0.06698226], action=1, reward=1.0, next_state=[-0.65796556  0.03465476 -0.05655032 -0.37656067]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 187 ] state=[-0.65796556  0.03465476 -0.05655032 -0.37656067], action=1, reward=1.0, next_state=[-0.65727246  0.23053239 -0.06408154 -0.68652438]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 188 ] state=[-0.65727246  0.23053239 -0.06408154 -0.68652438], action=0, reward=1.0, next_state=[-0.65266181  0.03635577 -0.07781202 -0.41468435]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 189 ] state=[-0.65266181  0.03635577 -0.07781202 -0.41468435], action=0, reward=1.0, next_state=[-0.6519347  -0.15758204 -0.08610571 -0.14751153]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 190 ] state=[-0.6519347  -0.15758204 -0.08610571 -0.14751153], action=0, reward=1.0, next_state=[-0.65508634 -0.35137216 -0.08905594  0.1168129 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 191 ] state=[-0.65508634 -0.35137216 -0.08905594  0.1168129 ], action=0, reward=1.0, next_state=[-0.66211378 -0.54511271 -0.08671968  0.38012423]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 192 ] state=[-0.66211378 -0.54511271 -0.08671968  0.38012423], action=0, reward=1.0, next_state=[-0.67301604 -0.73890304 -0.0791172   0.64425374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 193 ] state=[-0.67301604 -0.73890304 -0.0791172   0.64425374], action=1, reward=1.0, next_state=[-0.6877941  -0.54277281 -0.06623212  0.32774248]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 194 ] state=[-0.6877941  -0.54277281 -0.06623212  0.32774248], action=1, reward=1.0, next_state=[-0.69864955 -0.34677352 -0.05967727  0.01493015]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 195 ] state=[-0.69864955 -0.34677352 -0.05967727  0.01493015], action=0, reward=1.0, next_state=[-0.70558502 -0.5409911  -0.05937867  0.28820321]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 196 ] state=[-0.70558502 -0.5409911  -0.05937867  0.28820321], action=0, reward=1.0, next_state=[-0.71640484 -0.73521824 -0.05361461  0.56158338]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 197 ] state=[-0.71640484 -0.73521824 -0.05361461  0.56158338], action=0, reward=1.0, next_state=[-0.73110921 -0.9295484  -0.04238294  0.83690463]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 198 ] state=[-0.73110921 -0.9295484  -0.04238294  0.83690463], action=0, reward=1.0, next_state=[-0.74970018 -1.12406665 -0.02564485  1.11596312]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 199 ] state=[-0.74970018 -1.12406665 -0.02564485  1.11596312], action=0, reward=1.0, next_state=[-0.77218151 -1.31884276 -0.00332558  1.40049247]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 200 ] state=[-0.77218151 -1.31884276 -0.00332558  1.40049247], action=0, reward=1.0, next_state=[-0.79855837 -1.51392323  0.02468426  1.69213382]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 201 ] state=[-0.79855837 -1.51392323  0.02468426  1.69213382], action=1, reward=1.0, next_state=[-0.82883683 -1.31909488  0.05852694  1.40723676]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 202 ] state=[-0.82883683 -1.31909488  0.05852694  1.40723676], action=1, reward=1.0, next_state=[-0.85521873 -1.12474602  0.08667168  1.13340972]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 203 ] state=[-0.85521873 -1.12474602  0.08667168  1.13340972], action=1, reward=1.0, next_state=[-0.87771365 -0.93085867  0.10933987  0.86911995]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 204 ] state=[-0.87771365 -0.93085867  0.10933987  0.86911995], action=1, reward=1.0, next_state=[-0.89633082 -0.7373805   0.12672227  0.61271769]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 205 ] state=[-0.89633082 -0.7373805   0.12672227  0.61271769], action=1, reward=1.0, next_state=[-0.91107843 -0.54423598  0.13897662  0.36248073]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 206 ] state=[-0.91107843 -0.54423598  0.13897662  0.36248073], action=1, reward=1.0, next_state=[-0.92196315 -0.3513347   0.14622624  0.11664639]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 207 ] state=[-0.92196315 -0.3513347   0.14622624  0.11664639], action=1, reward=1.0, next_state=[-0.92898985 -0.15857758  0.14855917 -0.12656616]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 208 ] state=[-0.92898985 -0.15857758  0.14855917 -0.12656616], action=1, reward=1.0, next_state=[-0.9321614   0.03413841  0.14602784 -0.36894018]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 209 ] state=[-0.9321614   0.03413841  0.14602784 -0.36894018], action=1, reward=1.0, next_state=[-0.93147863  0.22691641  0.13864904 -0.61224978]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 210 ] state=[-0.93147863  0.22691641  0.13864904 -0.61224978], action=1, reward=1.0, next_state=[-0.9269403   0.41985621  0.12640404 -0.85824985]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 211 ] state=[-0.9269403   0.41985621  0.12640404 -0.85824985], action=0, reward=1.0, next_state=[-0.91854318  0.2232599   0.10923905 -0.52864426]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 212 ] state=[-0.91854318  0.2232599   0.10923905 -0.52864426], action=0, reward=1.0, next_state=[-0.91407798  0.02678434  0.09866616 -0.20363515]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 213 ] state=[-0.91407798  0.02678434  0.09866616 -0.20363515], action=1, reward=1.0, next_state=[-0.91354229  0.22036698  0.09459346 -0.46363605]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 214 ] state=[-0.91354229  0.22036698  0.09459346 -0.46363605], action=0, reward=1.0, next_state=[-0.90913495  0.02404446  0.08532074 -0.14269978]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 215 ] state=[-0.90913495  0.02404446  0.08532074 -0.14269978], action=0, reward=1.0, next_state=[-0.90865406 -0.17218922  0.08246674  0.17563389]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 216 ] state=[-0.90865406 -0.17218922  0.08246674  0.17563389], action=0, reward=1.0, next_state=[-0.91209785 -0.36838862  0.08597942  0.49315057]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 217 ] state=[-0.91209785 -0.36838862  0.08597942  0.49315057], action=1, reward=1.0, next_state=[-0.91946562 -0.17457788  0.09584243  0.22875516]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 218 ] state=[-0.91946562 -0.17457788  0.09584243  0.22875516], action=1, reward=1.0, next_state=[-0.92295718  0.01905309  0.10041753 -0.03222377]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 219 ] state=[-0.92295718  0.01905309  0.10041753 -0.03222377], action=1, reward=1.0, next_state=[-0.92257612  0.21260235  0.09977306 -0.29161195]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 220 ] state=[-0.92257612  0.21260235  0.09977306 -0.29161195], action=1, reward=1.0, next_state=[-0.91832407  0.40617062  0.09394082 -0.55123574]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 221 ] state=[-0.91832407  0.40617062  0.09394082 -0.55123574], action=1, reward=1.0, next_state=[-0.91020066  0.59985634  0.0829161  -0.81290533]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 222 ] state=[-0.91020066  0.59985634  0.0829161  -0.81290533], action=0, reward=1.0, next_state=[-0.89820353  0.40370254  0.066658   -0.49533607]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 223 ] state=[-0.89820353  0.40370254  0.066658   -0.49533607], action=1, reward=1.0, next_state=[-0.89012948  0.59782427  0.05675128 -0.76628906]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 224 ] state=[-0.89012948  0.59782427  0.05675128 -0.76628906], action=1, reward=1.0, next_state=[-0.87817299  0.79212083  0.0414255  -1.04058877]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 225 ] state=[-0.87817299  0.79212083  0.0414255  -1.04058877], action=0, reward=1.0, next_state=[-0.86233058  0.59647376  0.02061372 -0.73519432]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 226 ] state=[-0.86233058  0.59647376  0.02061372 -0.73519432], action=1, reward=1.0, next_state=[-0.8504011   0.79130498  0.00590983 -1.02131906]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 227 ] state=[-0.8504011   0.79130498  0.00590983 -1.02131906], action=0, reward=1.0, next_state=[-0.834575    0.59610478 -0.01451655 -0.7267864 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 228 ] state=[-0.834575    0.59610478 -0.01451655 -0.7267864 ], action=0, reward=1.0, next_state=[-0.82265291  0.40118652 -0.02905228 -0.43870752]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 229 ] state=[-0.82265291  0.40118652 -0.02905228 -0.43870752], action=0, reward=1.0, next_state=[-0.81462918  0.20648755 -0.03782643 -0.15532248]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 230 ] state=[-0.81462918  0.20648755 -0.03782643 -0.15532248], action=1, reward=1.0, next_state=[-0.81049942  0.40213011 -0.04093288 -0.45969471]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 290 ][ timestamp 231 ] state=[-0.81049942  0.40213011 -0.04093288 -0.45969471], action=1, reward=1.0, next_state=[-0.80245682  0.59780603 -0.05012677 -0.76499363]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 232 ] state=[-0.80245682  0.59780603 -0.05012677 -0.76499363], action=0, reward=1.0, next_state=[-0.7905007   0.40340887 -0.06542664 -0.48849526]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 233 ] state=[-0.7905007   0.40340887 -0.06542664 -0.48849526], action=0, reward=1.0, next_state=[-0.78243252  0.20926805 -0.07519655 -0.21712882]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 234 ] state=[-0.78243252  0.20926805 -0.07519655 -0.21712882], action=0, reward=1.0, next_state=[-0.77824716  0.01529712 -0.07953912  0.0509184 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 235 ] state=[-0.77824716  0.01529712 -0.07953912  0.0509184 ], action=0, reward=1.0, next_state=[-0.77794122 -0.17859956 -0.07852076  0.31748405]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 236 ] state=[-0.77794122 -0.17859956 -0.07852076  0.31748405], action=0, reward=1.0, next_state=[-0.78151321 -0.37252044 -0.07217107  0.58440772]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 237 ] state=[-0.78151321 -0.37252044 -0.07217107  0.58440772], action=0, reward=1.0, next_state=[-0.78896362 -0.56656115 -0.06048292  0.85351121]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 238 ] state=[-0.78896362 -0.56656115 -0.06048292  0.85351121], action=1, reward=1.0, next_state=[-0.80029484 -0.37066923 -0.0434127   0.54243948]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 239 ] state=[-0.80029484 -0.37066923 -0.0434127   0.54243948], action=1, reward=1.0, next_state=[-0.80770823 -0.17496489 -0.03256391  0.23640023]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 240 ] state=[-0.80770823 -0.17496489 -0.03256391  0.23640023], action=1, reward=1.0, next_state=[-0.81120753  0.0206068  -0.0278359  -0.06637388]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 241 ] state=[-0.81120753  0.0206068  -0.0278359  -0.06637388], action=0, reward=1.0, next_state=[-0.81079539 -0.17410523 -0.02916338  0.21739833]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 242 ] state=[-0.81079539 -0.17410523 -0.02916338  0.21739833], action=1, reward=1.0, next_state=[-0.8142775   0.02142122 -0.02481541 -0.08433945]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 243 ] state=[-0.8142775   0.02142122 -0.02481541 -0.08433945], action=0, reward=1.0, next_state=[-0.81384907 -0.17333638 -0.0265022   0.20041203]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 244 ] state=[-0.81384907 -0.17333638 -0.0265022   0.20041203], action=1, reward=1.0, next_state=[-0.8173158   0.02215439 -0.02249396 -0.10051189]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 245 ] state=[-0.8173158   0.02215439 -0.02249396 -0.10051189], action=0, reward=1.0, next_state=[-0.81687271 -0.17263809 -0.0245042   0.18499024]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 246 ] state=[-0.81687271 -0.17263809 -0.0245042   0.18499024], action=0, reward=1.0, next_state=[-0.82032547 -0.36740102 -0.02080439  0.46984341]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 247 ] state=[-0.82032547 -0.36740102 -0.02080439  0.46984341], action=1, reward=1.0, next_state=[-0.82767349 -0.17199145 -0.01140753  0.17067645]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 248 ] state=[-0.82767349 -0.17199145 -0.01140753  0.17067645], action=1, reward=1.0, next_state=[-0.83111332  0.0232919  -0.007994   -0.12558326]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 249 ] state=[-0.83111332  0.0232919  -0.007994   -0.12558326], action=0, reward=1.0, next_state=[-0.83064748 -0.17171462 -0.01050566  0.16456697]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 250 ] state=[-0.83064748 -0.17171462 -0.01050566  0.16456697], action=1, reward=1.0, next_state=[-0.83408178  0.02355613 -0.00721432 -0.13141161]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 251 ] state=[-0.83408178  0.02355613 -0.00721432 -0.13141161], action=1, reward=1.0, next_state=[-0.83361065  0.21878068 -0.00984255 -0.4263618 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 252 ] state=[-0.83361065  0.21878068 -0.00984255 -0.4263618 ], action=0, reward=1.0, next_state=[-0.82923504  0.02379951 -0.01836979 -0.13679788]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 253 ] state=[-0.82923504  0.02379951 -0.01836979 -0.13679788], action=1, reward=1.0, next_state=[-0.82875905  0.21917969 -0.02110575 -0.43521912]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 254 ] state=[-0.82875905  0.21917969 -0.02110575 -0.43521912], action=0, reward=1.0, next_state=[-0.82437546  0.02436279 -0.02981013 -0.14926348]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 255 ] state=[-0.82437546  0.02436279 -0.02981013 -0.14926348], action=0, reward=1.0, next_state=[-0.8238882  -0.17031989 -0.0327954   0.13386791]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 256 ] state=[-0.8238882  -0.17031989 -0.0327954   0.13386791], action=1, reward=1.0, next_state=[-0.8272946   0.0252561  -0.03011804 -0.16897844]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 257 ] state=[-0.8272946   0.0252561  -0.03011804 -0.16897844], action=0, reward=1.0, next_state=[-0.82678948 -0.1694221  -0.03349761  0.11405305]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 258 ] state=[-0.82678948 -0.1694221  -0.03349761  0.11405305], action=1, reward=1.0, next_state=[-0.83017792  0.02616342 -0.03121655 -0.1890071 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 259 ] state=[-0.83017792  0.02616342 -0.03121655 -0.1890071 ], action=0, reward=1.0, next_state=[-0.82965465 -0.16849834 -0.03499669  0.09366711]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 260 ] state=[-0.82965465 -0.16849834 -0.03499669  0.09366711], action=0, reward=1.0, next_state=[-0.83302462 -0.36310166 -0.03312335  0.37510643]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 261 ] state=[-0.83302462 -0.36310166 -0.03312335  0.37510643], action=1, reward=1.0, next_state=[-0.84028665 -0.16752526 -0.02562122  0.07216627]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 262 ] state=[-0.84028665 -0.16752526 -0.02562122  0.07216627], action=0, reward=1.0, next_state=[-0.84363716 -0.36227071 -0.0241779   0.35665674]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 263 ] state=[-0.84363716 -0.36227071 -0.0241779   0.35665674], action=1, reward=1.0, next_state=[-0.85088257 -0.16681351 -0.01704476  0.05644902]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 264 ] state=[-0.85088257 -0.16681351 -0.01704476  0.05644902], action=1, reward=1.0, next_state=[-0.85421884  0.02854864 -0.01591578 -0.24156255]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 265 ] state=[-0.85421884  0.02854864 -0.01591578 -0.24156255], action=1, reward=1.0, next_state=[-0.85364787  0.22389428 -0.02074703 -0.53922294]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 266 ] state=[-0.85364787  0.22389428 -0.02074703 -0.53922294], action=0, reward=1.0, next_state=[-0.84916998  0.02907004 -0.03153149 -0.25314866]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 267 ] state=[-0.84916998  0.02907004 -0.03153149 -0.25314866], action=1, reward=1.0, next_state=[-0.84858858  0.2246277  -0.03659446 -0.55560807]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 268 ] state=[-0.84858858  0.2246277  -0.03659446 -0.55560807], action=0, reward=1.0, next_state=[-0.84409603  0.03003814 -0.04770662 -0.27467551]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 269 ] state=[-0.84409603  0.03003814 -0.04770662 -0.27467551], action=0, reward=1.0, next_state=[-0.84349526 -0.16437182 -0.05320013  0.00258722]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 270 ] state=[-0.84349526 -0.16437182 -0.05320013  0.00258722], action=0, reward=1.0, next_state=[-0.8467827  -0.35869204 -0.05314839  0.2780217 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 271 ] state=[-0.8467827  -0.35869204 -0.05314839  0.2780217 ], action=1, reward=1.0, next_state=[-0.85395654 -0.16285374 -0.04758796 -0.03093922]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 272 ] state=[-0.85395654 -0.16285374 -0.04758796 -0.03093922], action=1, reward=1.0, next_state=[-0.85721362  0.0329172  -0.04820674 -0.33824876]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 273 ] state=[-0.85721362  0.0329172  -0.04820674 -0.33824876], action=0, reward=1.0, next_state=[-0.85655527 -0.16148682 -0.05497172 -0.0611488 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 290 ][ timestamp 274 ] state=[-0.85655527 -0.16148682 -0.05497172 -0.0611488 ], action=0, reward=1.0, next_state=[-0.85978501 -0.35577926 -0.05619469  0.21369609]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 275 ] state=[-0.85978501 -0.35577926 -0.05619469  0.21369609], action=1, reward=1.0, next_state=[-0.86690059 -0.15990081 -0.05192077 -0.09617035]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 276 ] state=[-0.86690059 -0.15990081 -0.05192077 -0.09617035], action=0, reward=1.0, next_state=[-0.87009861 -0.35424165 -0.05384418  0.17969023]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 277 ] state=[-0.87009861 -0.35424165 -0.05384418  0.17969023], action=1, reward=1.0, next_state=[-0.87718344 -0.15839218 -0.05025037 -0.12948075]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 278 ] state=[-0.87718344 -0.15839218 -0.05025037 -0.12948075], action=1, reward=1.0, next_state=[-0.88035129  0.03741225 -0.05283999 -0.43758406]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 279 ] state=[-0.88035129  0.03741225 -0.05283999 -0.43758406], action=0, reward=1.0, next_state=[-0.87960304 -0.15692351 -0.06159167 -0.162015  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 280 ] state=[-0.87960304 -0.15692351 -0.06159167 -0.162015  ], action=0, reward=1.0, next_state=[-0.88274151 -0.35111212 -0.06483197  0.11061909]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 281 ] state=[-0.88274151 -0.35111212 -0.06483197  0.11061909], action=1, reward=1.0, next_state=[-0.88976375 -0.15512395 -0.06261959 -0.2017928 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 282 ] state=[-0.88976375 -0.15512395 -0.06261959 -0.2017928 ], action=1, reward=1.0, next_state=[-0.89286623  0.04083507 -0.06665544 -0.51355335]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 283 ] state=[-0.89286623  0.04083507 -0.06665544 -0.51355335], action=0, reward=1.0, next_state=[-0.89204953 -0.15328789 -0.07692651 -0.24259771]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 284 ] state=[-0.89204953 -0.15328789 -0.07692651 -0.24259771], action=0, reward=1.0, next_state=[-0.89511529 -0.34723155 -0.08177846  0.02486333]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 285 ] state=[-0.89511529 -0.34723155 -0.08177846  0.02486333], action=0, reward=1.0, next_state=[-0.90205992 -0.5410913  -0.0812812   0.29066506]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 286 ] state=[-0.90205992 -0.5410913  -0.0812812   0.29066506], action=1, reward=1.0, next_state=[-0.91288175 -0.34491011 -0.0754679  -0.02650557]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 287 ] state=[-0.91288175 -0.34491011 -0.0754679  -0.02650557], action=1, reward=1.0, next_state=[-0.91977995 -0.14879157 -0.07599801 -0.34201255]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 288 ] state=[-0.91977995 -0.14879157 -0.07599801 -0.34201255], action=0, reward=1.0, next_state=[-0.92275578 -0.34275464 -0.08283826 -0.07422965]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 289 ] state=[-0.92275578 -0.34275464 -0.08283826 -0.07422965], action=1, reward=1.0, next_state=[-0.92961087 -0.14654883 -0.08432285 -0.39185575]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 290 ] state=[-0.92961087 -0.14654883 -0.08432285 -0.39185575], action=0, reward=1.0, next_state=[-0.93254185 -0.3403792  -0.09215997 -0.12690478]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 291 ] state=[-0.93254185 -0.3403792  -0.09215997 -0.12690478], action=0, reward=1.0, next_state=[-0.93934943 -0.53406833 -0.09469806  0.13533928]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 292 ] state=[-0.93934943 -0.53406833 -0.09469806  0.13533928], action=0, reward=1.0, next_state=[-0.9500308  -0.72771528 -0.09199128  0.39670861]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 293 ] state=[-0.9500308  -0.72771528 -0.09199128  0.39670861], action=0, reward=1.0, next_state=[-0.9645851  -0.92141987 -0.0840571   0.65902966]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 294 ] state=[-0.9645851  -0.92141987 -0.0840571   0.65902966], action=1, reward=1.0, next_state=[-0.9830135  -0.72523485 -0.07087651  0.34110744]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 295 ] state=[-0.9830135  -0.72523485 -0.07087651  0.34110744], action=0, reward=1.0, next_state=[-0.9975182  -0.91928056 -0.06405436  0.61062497]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 296 ] state=[-0.9975182  -0.91928056 -0.06405436  0.61062497], action=0, reward=1.0, next_state=[-1.01590381 -1.11345146 -0.05184186  0.88246491]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 297 ] state=[-1.01590381 -1.11345146 -0.05184186  0.88246491], action=1, reward=1.0, next_state=[-1.03817284 -0.91766519 -0.03419256  0.57394538]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 298 ] state=[-1.03817284 -0.91766519 -0.03419256  0.57394538], action=0, reward=1.0, next_state=[-1.05652614 -1.1122915  -0.02271366  0.85566354]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 299 ] state=[-1.05652614 -1.1122915  -0.02271366  0.85566354], action=1, reward=1.0, next_state=[-1.07877197 -0.91686751 -0.00560039  0.55592593]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 300 ] state=[-1.07877197 -0.91686751 -0.00560039  0.55592593], action=0, reward=1.0, next_state=[-1.09710932 -1.11191039  0.00551813  0.84683915]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 301 ] state=[-1.09710932 -1.11191039  0.00551813  0.84683915], action=1, reward=1.0, next_state=[-1.11934753 -0.91686415  0.02245492  0.55589657]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 302 ] state=[-1.11934753 -0.91686415  0.02245492  0.55589657], action=1, reward=1.0, next_state=[-1.13768481 -0.72206454  0.03357285  0.27037201]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 303 ] state=[-1.13768481 -0.72206454  0.03357285  0.27037201], action=1, reward=1.0, next_state=[-1.15212611 -0.52743736  0.03898029 -0.01153568]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 304 ] state=[-1.15212611 -0.52743736  0.03898029 -0.01153568], action=1, reward=1.0, next_state=[-1.16267485 -0.33289549  0.03874957 -0.29166951]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 305 ] state=[-1.16267485 -0.33289549  0.03874957 -0.29166951], action=1, reward=1.0, next_state=[-1.16933276 -0.13834686  0.03291618 -0.57188387]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 306 ] state=[-1.16933276 -0.13834686  0.03291618 -0.57188387], action=1, reward=1.0, next_state=[-1.1720997   0.05629844  0.02147851 -0.85401806]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 307 ] state=[-1.1720997   0.05629844  0.02147851 -0.85401806], action=1, reward=1.0, next_state=[-1.17097373  0.25112116  0.00439815 -1.13987053]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 308 ] state=[-1.17097373  0.25112116  0.00439815 -1.13987053], action=0, reward=1.0, next_state=[-1.16595131  0.05594199 -0.01839927 -0.84581155]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 309 ] state=[-1.16595131  0.05594199 -0.01839927 -0.84581155], action=1, reward=1.0, next_state=[-1.16483247  0.25131007 -0.0353155  -1.14422316]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 310 ] state=[-1.16483247  0.25131007 -0.0353155  -1.14422316], action=0, reward=1.0, next_state=[-1.15980627  0.05666683 -0.05819996 -0.86282094]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 311 ] state=[-1.15980627  0.05666683 -0.05819996 -0.86282094], action=1, reward=1.0, next_state=[-1.15867293  0.25253083 -0.07545638 -1.17322064]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 312 ] state=[-1.15867293  0.25253083 -0.07545638 -1.17322064], action=1, reward=1.0, next_state=[-1.15362231  0.4485481  -0.09892079 -1.48857302]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 313 ] state=[-1.15362231  0.4485481  -0.09892079 -1.48857302], action=0, reward=1.0, next_state=[-1.14465135  0.25476038 -0.12869225 -1.22834779]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 314 ] state=[-1.14465135  0.25476038 -0.12869225 -1.22834779], action=0, reward=1.0, next_state=[-1.13955614  0.06150773 -0.15325921 -0.97859713]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 315 ] state=[-1.13955614  0.06150773 -0.15325921 -0.97859713], action=0, reward=1.0, next_state=[-1.13832599 -0.13126442 -0.17283115 -0.73771022]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 316 ] state=[-1.13832599 -0.13126442 -0.17283115 -0.73771022], action=0, reward=1.0, next_state=[-1.14095128 -0.32363181 -0.18758535 -0.50401778]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 317 ] state=[-1.14095128 -0.32363181 -0.18758535 -0.50401778], action=0, reward=1.0, next_state=[-1.14742391 -0.51568369 -0.19766571 -0.27582082]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 318 ] state=[-1.14742391 -0.51568369 -0.19766571 -0.27582082], action=0, reward=1.0, next_state=[-1.15773759 -0.70751733 -0.20318213 -0.05140955]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 290 ][ timestamp 319 ] state=[-1.15773759 -0.70751733 -0.20318213 -0.05140955], action=0, reward=1.0, next_state=[-1.17188793 -0.89923422 -0.20421032  0.17092481]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 320 ] state=[-1.17188793 -0.89923422 -0.20421032  0.17092481], action=0, reward=1.0, next_state=[-1.18987262 -1.09093715 -0.20079182  0.39288282]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 321 ] state=[-1.18987262 -1.09093715 -0.20079182  0.39288282], action=1, reward=1.0, next_state=[-1.21169136 -0.89361669 -0.19293416  0.04421177]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 322 ] state=[-1.21169136 -0.89361669 -0.19293416  0.04421177], action=1, reward=1.0, next_state=[-1.2295637  -0.69632702 -0.19204993 -0.30260333]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 323 ] state=[-1.2295637  -0.69632702 -0.19204993 -0.30260333], action=1, reward=1.0, next_state=[-1.24349024 -0.49906093 -0.198102   -0.64917859]\n",
      "[ Experience replay ] starts\n",
      "[ episode 290 ][ timestamp 324 ] state=[-1.24349024 -0.49906093 -0.198102   -0.64917859], action=0, reward=-1.0, next_state=[-1.25347145 -0.69095263 -0.21108557 -0.4248324 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 290: Exploration_rate=0.01. Score=324.\n",
      "[ episode 291 ] state=[0.04948893 0.01331399 0.04851025 0.0095579 ]\n",
      "[ episode 291 ][ timestamp 1 ] state=[0.04948893 0.01331399 0.04851025 0.0095579 ], action=0, reward=1.0, next_state=[ 0.04975521 -0.18246889  0.04870141  0.31714316]\n",
      "[ Experience replay ] starts\n",
      "[ episode 291 ][ timestamp 2 ] state=[ 0.04975521 -0.18246889  0.04870141  0.31714316], action=0, reward=1.0, next_state=[ 0.04610584 -0.37824945  0.05504427  0.62477837]\n",
      "[ Experience replay ] starts\n",
      "[ episode 291 ][ timestamp 3 ] state=[ 0.04610584 -0.37824945  0.05504427  0.62477837], action=1, reward=1.0, next_state=[ 0.03854085 -0.18393741  0.06753984  0.34992659]\n",
      "[ Experience replay ] starts\n",
      "[ episode 291 ][ timestamp 4 ] state=[ 0.03854085 -0.18393741  0.06753984  0.34992659], action=0, reward=1.0, next_state=[ 0.0348621  -0.37995165  0.07453837  0.66311922]\n",
      "[ Experience replay ] starts\n",
      "[ episode 291 ][ timestamp 5 ] state=[ 0.0348621  -0.37995165  0.07453837  0.66311922], action=0, reward=1.0, next_state=[ 0.02726307 -0.57602712  0.08780075  0.97830976]\n",
      "[ Experience replay ] starts\n",
      "[ episode 291 ][ timestamp 6 ] state=[ 0.02726307 -0.57602712  0.08780075  0.97830976], action=1, reward=1.0, next_state=[ 0.01574252 -0.38218508  0.10736695  0.71444699]\n",
      "[ Experience replay ] starts\n",
      "[ episode 291 ][ timestamp 7 ] state=[ 0.01574252 -0.38218508  0.10736695  0.71444699], action=0, reward=1.0, next_state=[ 0.00809882 -0.57861661  0.12165589  1.03890289]\n",
      "[ Experience replay ] starts\n",
      "[ episode 291 ][ timestamp 8 ] state=[ 0.00809882 -0.57861661  0.12165589  1.03890289], action=0, reward=1.0, next_state=[-0.00347351 -0.77512657  0.14243395  1.36716791]\n",
      "[ Experience replay ] starts\n",
      "[ episode 291 ][ timestamp 9 ] state=[-0.00347351 -0.77512657  0.14243395  1.36716791], action=1, reward=1.0, next_state=[-0.01897604 -0.58204564  0.1697773   1.12221352]\n",
      "[ Experience replay ] starts\n",
      "[ episode 291 ][ timestamp 10 ] state=[-0.01897604 -0.58204564  0.1697773   1.12221352], action=1, reward=1.0, next_state=[-0.03061696 -0.38950663  0.19222158  0.88723245]\n",
      "[ Experience replay ] starts\n",
      "[ episode 291 ][ timestamp 11 ] state=[-0.03061696 -0.38950663  0.19222158  0.88723245], action=1, reward=-1.0, next_state=[-0.03840709 -0.1974402   0.20996622  0.66060472]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 291: Exploration_rate=0.01. Score=11.\n",
      "[ episode 292 ] state=[-0.0092259  -0.02391969 -0.00314329  0.03793566]\n",
      "[ episode 292 ][ timestamp 1 ] state=[-0.0092259  -0.02391969 -0.00314329  0.03793566], action=1, reward=1.0, next_state=[-0.0097043   0.1712472  -0.00238458 -0.25573735]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 2 ] state=[-0.0097043   0.1712472  -0.00238458 -0.25573735], action=0, reward=1.0, next_state=[-0.00627935 -0.02384063 -0.00749933  0.03619249]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 3 ] state=[-0.00627935 -0.02384063 -0.00749933  0.03619249], action=1, reward=1.0, next_state=[-0.00675617  0.17138806 -0.00677548 -0.25884708]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 4 ] state=[-0.00675617  0.17138806 -0.00677548 -0.25884708], action=1, reward=1.0, next_state=[-0.00332841  0.36660608 -0.01195242 -0.55365937]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 5 ] state=[-0.00332841  0.36660608 -0.01195242 -0.55365937], action=1, reward=1.0, next_state=[ 0.00400372  0.56189381 -0.0230256  -0.85008397]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 6 ] state=[ 0.00400372  0.56189381 -0.0230256  -0.85008397], action=1, reward=1.0, next_state=[ 0.01524159  0.75732205 -0.04002728 -1.14991756]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 7 ] state=[ 0.01524159  0.75732205 -0.04002728 -1.14991756], action=1, reward=1.0, next_state=[ 0.03038803  0.9529429  -0.06302564 -1.45487867]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 8 ] state=[ 0.03038803  0.9529429  -0.06302564 -1.45487867], action=0, reward=1.0, next_state=[ 0.04944689  0.75864877 -0.09212321 -1.18253339]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 9 ] state=[ 0.04944689  0.75864877 -0.09212321 -1.18253339], action=0, reward=1.0, next_state=[ 0.06461987  0.56483504 -0.11577388 -0.92009149]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 10 ] state=[ 0.06461987  0.56483504 -0.11577388 -0.92009149], action=0, reward=1.0, next_state=[ 0.07591657  0.37145207 -0.13417571 -0.66592042]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 11 ] state=[ 0.07591657  0.37145207 -0.13417571 -0.66592042], action=0, reward=1.0, next_state=[ 0.08334561  0.17842626 -0.14749411 -0.41831349]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 12 ] state=[ 0.08334561  0.17842626 -0.14749411 -0.41831349], action=0, reward=1.0, next_state=[ 0.08691413 -0.01433143 -0.15586038 -0.17552249]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 13 ] state=[ 0.08691413 -0.01433143 -0.15586038 -0.17552249], action=0, reward=1.0, next_state=[ 0.08662751 -0.20691925 -0.15937083  0.06421986]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 14 ] state=[ 0.08662751 -0.20691925 -0.15937083  0.06421986], action=0, reward=1.0, next_state=[ 0.08248912 -0.39943996 -0.15808644  0.30268436]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 15 ] state=[ 0.08248912 -0.39943996 -0.15808644  0.30268436], action=0, reward=1.0, next_state=[ 0.07450032 -0.59199718 -0.15203275  0.54163445]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 16 ] state=[ 0.07450032 -0.59199718 -0.15203275  0.54163445], action=1, reward=1.0, next_state=[ 0.06266038 -0.39510209 -0.14120006  0.20517287]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 17 ] state=[ 0.06266038 -0.39510209 -0.14120006  0.20517287], action=0, reward=1.0, next_state=[ 0.05475834 -0.58795214 -0.1370966   0.45019402]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 18 ] state=[ 0.05475834 -0.58795214 -0.1370966   0.45019402], action=1, reward=1.0, next_state=[ 0.04299929 -0.39118451 -0.12809272  0.11763174]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 19 ] state=[ 0.04299929 -0.39118451 -0.12809272  0.11763174], action=0, reward=1.0, next_state=[ 0.0351756  -0.58426064 -0.12574009  0.36731686]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 20 ] state=[ 0.0351756  -0.58426064 -0.12574009  0.36731686], action=1, reward=1.0, next_state=[ 0.02349039 -0.38759717 -0.11839375  0.03778035]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 21 ] state=[ 0.02349039 -0.38759717 -0.11839375  0.03778035], action=1, reward=1.0, next_state=[ 0.01573845 -0.19099397 -0.11763814 -0.28978652]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 22 ] state=[ 0.01573845 -0.19099397 -0.11763814 -0.28978652], action=1, reward=1.0, next_state=[ 0.01191857  0.00559181 -0.12343387 -0.61713305]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 23 ] state=[ 0.01191857  0.00559181 -0.12343387 -0.61713305], action=0, reward=1.0, next_state=[ 0.0120304  -0.1876093  -0.13577654 -0.36573376]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 24 ] state=[ 0.0120304  -0.1876093  -0.13577654 -0.36573376], action=0, reward=1.0, next_state=[ 0.00827822 -0.38056684 -0.14309121 -0.11875705]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 25 ] state=[ 0.00827822 -0.38056684 -0.14309121 -0.11875705], action=0, reward=1.0, next_state=[ 0.00066688 -0.57337961 -0.14546635  0.12558086]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 26 ] state=[ 0.00066688 -0.57337961 -0.14546635  0.12558086], action=0, reward=1.0, next_state=[-0.01080071 -0.76615052 -0.14295473  0.36906684]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 292 ][ timestamp 27 ] state=[-0.01080071 -0.76615052 -0.14295473  0.36906684], action=1, reward=1.0, next_state=[-0.02612372 -0.56931747 -0.1355734   0.03494331]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 28 ] state=[-0.02612372 -0.56931747 -0.1355734   0.03494331], action=0, reward=1.0, next_state=[-0.03751007 -0.7622611  -0.13487453  0.28196649]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 29 ] state=[-0.03751007 -0.7622611  -0.13487453  0.28196649], action=0, reward=1.0, next_state=[-0.05275529 -0.95522726 -0.1292352   0.52925401]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 30 ] state=[-0.05275529 -0.95522726 -0.1292352   0.52925401], action=0, reward=1.0, next_state=[-0.07185984 -1.14831692 -0.11865012  0.77858369]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 31 ] state=[-0.07185984 -1.14831692 -0.11865012  0.77858369], action=0, reward=1.0, next_state=[-0.09482618 -1.34162499 -0.10307845  1.03170582]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 32 ] state=[-0.09482618 -1.34162499 -0.10307845  1.03170582], action=0, reward=1.0, next_state=[-0.12165868 -1.53523596 -0.08244433  1.29032935]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 33 ] state=[-0.12165868 -1.53523596 -0.08244433  1.29032935], action=0, reward=1.0, next_state=[-0.1523634  -1.72921839 -0.05663774  1.5561035 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 34 ] state=[-0.1523634  -1.72921839 -0.05663774  1.5561035 ], action=0, reward=1.0, next_state=[-0.18694776 -1.92361796 -0.02551567  1.83059268]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 35 ] state=[-0.18694776 -1.92361796 -0.02551567  1.83059268], action=1, reward=1.0, next_state=[-0.22542012 -1.72822295  0.01109618  1.53009478]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 36 ] state=[-0.22542012 -1.72822295  0.01109618  1.53009478], action=1, reward=1.0, next_state=[-0.25998458 -1.53323654  0.04169807  1.24089537]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 37 ] state=[-0.25998458 -1.53323654  0.04169807  1.24089537], action=0, reward=1.0, next_state=[-0.29064931 -1.72886831  0.06651598  1.54634363]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 38 ] state=[-0.29064931 -1.72886831  0.06651598  1.54634363], action=1, reward=1.0, next_state=[-0.32522668 -1.53460522  0.09744285  1.27513466]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 39 ] state=[-0.32522668 -1.53460522  0.09744285  1.27513466], action=1, reward=1.0, next_state=[-0.35591878 -1.34085177  0.12294555  1.01448606]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 40 ] state=[-0.35591878 -1.34085177  0.12294555  1.01448606], action=1, reward=1.0, next_state=[-0.38273582 -1.14756489  0.14323527  0.76279922]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 41 ] state=[-0.38273582 -1.14756489  0.14323527  0.76279922], action=1, reward=1.0, next_state=[-0.40568712 -0.95467572  0.15849125  0.51839575]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 42 ] state=[-0.40568712 -0.95467572  0.15849125  0.51839575], action=1, reward=1.0, next_state=[-0.42478063 -0.76209852  0.16885917  0.27955203]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 43 ] state=[-0.42478063 -0.76209852  0.16885917  0.27955203], action=1, reward=1.0, next_state=[-0.4400226  -0.5697372   0.17445021  0.04452296]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 44 ] state=[-0.4400226  -0.5697372   0.17445021  0.04452296], action=0, reward=1.0, next_state=[-0.45141735 -0.76687569  0.17534067  0.38677108]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 45 ] state=[-0.45141735 -0.76687569  0.17534067  0.38677108], action=0, reward=1.0, next_state=[-0.46675486 -0.96399668  0.18307609  0.72920535]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 46 ] state=[-0.46675486 -0.96399668  0.18307609  0.72920535], action=1, reward=1.0, next_state=[-0.48603479 -0.77181366  0.1976602   0.49927258]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 47 ] state=[-0.48603479 -0.77181366  0.1976602   0.49927258], action=1, reward=1.0, next_state=[-0.50147107 -0.57994683  0.20764565  0.27481059]\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ][ timestamp 48 ] state=[-0.50147107 -0.57994683  0.20764565  0.27481059], action=1, reward=-1.0, next_state=[-0.51307    -0.38829875  0.21314186  0.05412374]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 292: Exploration_rate=0.01. Score=48.\n",
      "[ episode 293 ] state=[-0.02741579  0.03369335  0.04789875 -0.03204784]\n",
      "[ episode 293 ][ timestamp 1 ] state=[-0.02741579  0.03369335  0.04789875 -0.03204784], action=1, reward=1.0, next_state=[-0.02674192  0.22809685  0.0472578  -0.30924179]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 2 ] state=[-0.02674192  0.22809685  0.0472578  -0.30924179], action=1, reward=1.0, next_state=[-0.02217998  0.42251473  0.04107296 -0.5866544 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 3 ] state=[-0.02217998  0.42251473  0.04107296 -0.5866544 ], action=1, reward=1.0, next_state=[-0.01372969  0.61703808  0.02933987 -0.86612129]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 4 ] state=[-0.01372969  0.61703808  0.02933987 -0.86612129], action=0, reward=1.0, next_state=[-0.00138893  0.42152938  0.01201745 -0.56435977]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 5 ] state=[-0.00138893  0.42152938  0.01201745 -0.56435977], action=1, reward=1.0, next_state=[ 7.04166242e-03  6.16480671e-01  7.30251231e-04 -8.53232548e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 6 ] state=[ 7.04166242e-03  6.16480671e-01  7.30251231e-04 -8.53232548e-01], action=0, reward=1.0, next_state=[ 0.01937128  0.42134877 -0.0163344  -0.56032009]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 7 ] state=[ 0.01937128  0.42134877 -0.0163344  -0.56032009], action=0, reward=1.0, next_state=[ 0.02779825  0.22645984 -0.0275408  -0.27282778]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 8 ] state=[ 0.02779825  0.22645984 -0.0275408  -0.27282778], action=1, reward=1.0, next_state=[ 0.03232745  0.42196372 -0.03299736 -0.57406837]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 9 ] state=[ 0.03232745  0.42196372 -0.03299736 -0.57406837], action=0, reward=1.0, next_state=[ 0.04076672  0.22731955 -0.04447872 -0.29196051]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 10 ] state=[ 0.04076672  0.22731955 -0.04447872 -0.29196051], action=1, reward=1.0, next_state=[ 0.04531311  0.42304654 -0.05031793 -0.59833306]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 11 ] state=[ 0.04531311  0.42304654 -0.05031793 -0.59833306], action=1, reward=1.0, next_state=[ 0.05377404  0.6188351  -0.0622846  -0.90643142]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 12 ] state=[ 0.05377404  0.6188351  -0.0622846  -0.90643142], action=1, reward=1.0, next_state=[ 0.06615075  0.81474256 -0.08041322 -1.21802264]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 13 ] state=[ 0.06615075  0.81474256 -0.08041322 -1.21802264], action=0, reward=1.0, next_state=[ 0.0824456   0.62074421 -0.10477368 -0.95158146]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 14 ] state=[ 0.0824456   0.62074421 -0.10477368 -0.95158146], action=0, reward=1.0, next_state=[ 0.09486048  0.42717642 -0.12380531 -0.69356912]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 15 ] state=[ 0.09486048  0.42717642 -0.12380531 -0.69356912], action=0, reward=1.0, next_state=[ 0.10340401  0.23396956 -0.13767669 -0.44228291]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 16 ] state=[ 0.10340401  0.23396956 -0.13767669 -0.44228291], action=0, reward=1.0, next_state=[ 0.1080834   0.04103695 -0.14652235 -0.19597162]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 17 ] state=[ 0.1080834   0.04103695 -0.14652235 -0.19597162], action=0, reward=1.0, next_state=[ 0.10890414 -0.15171844 -0.15044178  0.04713974]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 18 ] state=[ 0.10890414 -0.15171844 -0.15044178  0.04713974], action=0, reward=1.0, next_state=[ 0.10586977 -0.34439888 -0.14949898  0.28883267]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 19 ] state=[ 0.10586977 -0.34439888 -0.14949898  0.28883267], action=0, reward=1.0, next_state=[ 0.09898179 -0.53710791 -0.14372233  0.5308828 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 20 ] state=[ 0.09898179 -0.53710791 -0.14372233  0.5308828 ], action=0, reward=1.0, next_state=[ 0.08823964 -0.72994687 -0.13310468  0.77504984]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 21 ] state=[ 0.08823964 -0.72994687 -0.13310468  0.77504984], action=1, reward=1.0, next_state=[ 0.0736407  -0.5332697  -0.11760368  0.44362629]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 22 ] state=[ 0.0736407  -0.5332697  -0.11760368  0.44362629], action=0, reward=1.0, next_state=[ 0.0629753  -0.72654838 -0.10873115  0.69704591]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 293 ][ timestamp 23 ] state=[ 0.0629753  -0.72654838 -0.10873115  0.69704591], action=0, reward=1.0, next_state=[ 0.04844434 -0.92000798 -0.09479023  0.95361761]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 24 ] state=[ 0.04844434 -0.92000798 -0.09479023  0.95361761], action=1, reward=1.0, next_state=[ 0.03004418 -0.72374734 -0.07571788  0.63272163]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 25 ] state=[ 0.03004418 -0.72374734 -0.07571788  0.63272163], action=0, reward=1.0, next_state=[ 0.01556923 -0.91773586 -0.06306345  0.90063088]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 26 ] state=[ 0.01556923 -0.91773586 -0.06306345  0.90063088], action=0, reward=1.0, next_state=[-0.00278549 -1.11194921 -0.04505083  1.17284345]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 27 ] state=[-0.00278549 -1.11194921 -0.04505083  1.17284345], action=0, reward=1.0, next_state=[-0.02502447 -1.30645751 -0.02159396  1.45106941]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 28 ] state=[-0.02502447 -1.30645751 -0.02159396  1.45106941], action=0, reward=1.0, next_state=[-0.05115362 -1.50130758  0.00742743  1.73692824]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 29 ] state=[-0.05115362 -1.50130758  0.00742743  1.73692824], action=0, reward=1.0, next_state=[-0.08117977 -1.6965134   0.04216599  2.03191253]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 30 ] state=[-0.08117977 -1.6965134   0.04216599  2.03191253], action=1, reward=1.0, next_state=[-0.11511004 -1.50185097  0.08280424  1.75257155]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 31 ] state=[-0.11511004 -1.50185097  0.08280424  1.75257155], action=1, reward=1.0, next_state=[-0.14514706 -1.30776044  0.11785567  1.48674991]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 32 ] state=[-0.14514706 -1.30776044  0.11785567  1.48674991], action=1, reward=1.0, next_state=[-0.17130227 -1.11425529  0.14759067  1.2330751 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 33 ] state=[-0.17130227 -1.11425529  0.14759067  1.2330751 ], action=1, reward=1.0, next_state=[-0.19358738 -0.92130657  0.17225217  0.99003285]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 34 ] state=[-0.19358738 -0.92130657  0.17225217  0.99003285], action=0, reward=1.0, next_state=[-0.21201351 -1.118263    0.19205283  1.33148751]\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ][ timestamp 35 ] state=[-0.21201351 -1.118263    0.19205283  1.33148751], action=0, reward=-1.0, next_state=[-0.23437877 -1.31521686  0.21868258  1.67760371]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 293: Exploration_rate=0.01. Score=35.\n",
      "[ episode 294 ] state=[-0.02007218 -0.04114938 -0.04451245  0.03196513]\n",
      "[ episode 294 ][ timestamp 1 ] state=[-0.02007218 -0.04114938 -0.04451245  0.03196513], action=0, reward=1.0, next_state=[-0.02089517 -0.23560567 -0.04387315  0.31027831]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 2 ] state=[-0.02089517 -0.23560567 -0.04387315  0.31027831], action=0, reward=1.0, next_state=[-0.02560728 -0.43007597 -0.03766758  0.58880849]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 3 ] state=[-0.02560728 -0.43007597 -0.03766758  0.58880849], action=0, reward=1.0, next_state=[-0.0342088  -0.62465077 -0.02589141  0.86939201]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 4 ] state=[-0.0342088  -0.62465077 -0.02589141  0.86939201], action=1, reward=1.0, next_state=[-0.04670182 -0.42918634 -0.00850357  0.56868241]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 5 ] state=[-0.04670182 -0.42918634 -0.00850357  0.56868241], action=0, reward=1.0, next_state=[-0.05528554 -0.62418799  0.00287008  0.8586743 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 6 ] state=[-0.05528554 -0.62418799  0.00287008  0.8586743 ], action=0, reward=1.0, next_state=[-0.0677693  -0.81934892  0.02004356  1.15225829]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 7 ] state=[-0.0677693  -0.81934892  0.02004356  1.15225829], action=1, reward=1.0, next_state=[-0.08415628 -0.62449412  0.04308873  0.8659272 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 8 ] state=[-0.08415628 -0.62449412  0.04308873  0.8659272 ], action=0, reward=1.0, next_state=[-0.09664616 -0.82017518  0.06040727  1.17184052]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 9 ] state=[-0.09664616 -0.82017518  0.06040727  1.17184052], action=1, reward=1.0, next_state=[-0.11304967 -0.62588838  0.08384408  0.89869083]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 10 ] state=[-0.11304967 -0.62588838  0.08384408  0.89869083], action=0, reward=1.0, next_state=[-0.12556743 -0.82204051  0.1018179   1.21650672]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 11 ] state=[-0.12556743 -0.82204051  0.1018179   1.21650672], action=1, reward=1.0, next_state=[-0.14200824 -0.62836836  0.12614803  0.95738579]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 12 ] state=[-0.14200824 -0.62836836  0.12614803  0.95738579], action=1, reward=1.0, next_state=[-0.15457561 -0.43514771  0.14529575  0.70684709]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 13 ] state=[-0.15457561 -0.43514771  0.14529575  0.70684709], action=1, reward=1.0, next_state=[-0.16327857 -0.24230552  0.15943269  0.46319855]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 14 ] state=[-0.16327857 -0.24230552  0.15943269  0.46319855], action=1, reward=1.0, next_state=[-0.16812468 -0.04975339  0.16869666  0.22470831]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 15 ] state=[-0.16812468 -0.04975339  0.16869666  0.22470831], action=1, reward=1.0, next_state=[-0.16911974  0.14260634  0.17319083 -0.0103734 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 16 ] state=[-0.16911974  0.14260634  0.17319083 -0.0103734 ], action=1, reward=1.0, next_state=[-0.16626762  0.33487609  0.17298336 -0.24379952]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 17 ] state=[-0.16626762  0.33487609  0.17298336 -0.24379952], action=0, reward=1.0, next_state=[-0.1595701   0.13775978  0.16810737  0.09806605]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 18 ] state=[-0.1595701   0.13775978  0.16810737  0.09806605], action=1, reward=1.0, next_state=[-0.1568149   0.33012352  0.17006869 -0.13722089]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 19 ] state=[-0.1568149   0.33012352  0.17006869 -0.13722089], action=1, reward=1.0, next_state=[-0.15021243  0.52245318  0.16732427 -0.37179379]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 20 ] state=[-0.15021243  0.52245318  0.16732427 -0.37179379], action=1, reward=1.0, next_state=[-0.13976337  0.71485197  0.1598884  -0.60739729]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 21 ] state=[-0.13976337  0.71485197  0.1598884  -0.60739729], action=1, reward=1.0, next_state=[-0.12546633  0.90741992  0.14774045 -0.84575777]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 22 ] state=[-0.12546633  0.90741992  0.14774045 -0.84575777], action=0, reward=1.0, next_state=[-0.10731793  0.71062461  0.1308253  -0.51050271]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 23 ] state=[-0.10731793  0.71062461  0.1308253  -0.51050271], action=1, reward=1.0, next_state=[-0.09310544  0.90368444  0.12061524 -0.75926477]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 24 ] state=[-0.09310544  0.90368444  0.12061524 -0.75926477], action=1, reward=1.0, next_state=[-0.07503175  1.09695625  0.10542995 -1.01169128]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 25 ] state=[-0.07503175  1.09695625  0.10542995 -1.01169128], action=1, reward=1.0, next_state=[-0.05309262  1.29052559  0.08519612 -1.26949407]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 26 ] state=[-0.05309262  1.29052559  0.08519612 -1.26949407], action=1, reward=1.0, next_state=[-0.02728211  1.48446265  0.05980624 -1.53432717]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 27 ] state=[-0.02728211  1.48446265  0.05980624 -1.53432717], action=1, reward=1.0, next_state=[ 0.00240714  1.67881541  0.0291197  -1.80776254]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 28 ] state=[ 0.00240714  1.67881541  0.0291197  -1.80776254], action=1, reward=1.0, next_state=[ 0.03598345  1.87360072 -0.00703555 -2.09125666]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 29 ] state=[ 0.03598345  1.87360072 -0.00703555 -2.09125666], action=1, reward=1.0, next_state=[ 0.07345546  2.06879284 -0.04886069 -2.38610604]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 30 ] state=[ 0.07345546  2.06879284 -0.04886069 -2.38610604], action=1, reward=1.0, next_state=[ 0.11483132  2.26430899 -0.09658281 -2.69338958]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 31 ] state=[ 0.11483132  2.26430899 -0.09658281 -2.69338958], action=1, reward=1.0, next_state=[ 0.1601175   2.45999184 -0.1504506  -3.0138971 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 294 ][ timestamp 32 ] state=[ 0.1601175   2.45999184 -0.1504506  -3.0138971 ], action=1, reward=-1.0, next_state=[ 0.20931734  2.65558927 -0.21072854 -3.34804473]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 294: Exploration_rate=0.01. Score=32.\n",
      "[ episode 295 ] state=[-0.01394372  0.019515   -0.02194852 -0.01020139]\n",
      "[ episode 295 ][ timestamp 1 ] state=[-0.01394372  0.019515   -0.02194852 -0.01020139], action=1, reward=1.0, next_state=[-0.01355342  0.21494474 -0.02215255 -0.30972774]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 295 ][ timestamp 2 ] state=[-0.01355342  0.21494474 -0.02215255 -0.30972774], action=1, reward=1.0, next_state=[-0.00925452  0.41037519 -0.0283471  -0.60931381]\n",
      "[ Experience replay ] starts\n",
      "[ episode 295 ][ timestamp 3 ] state=[-0.00925452  0.41037519 -0.0283471  -0.60931381], action=1, reward=1.0, next_state=[-0.00104702  0.60588171 -0.04053338 -0.9107887 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 295 ][ timestamp 4 ] state=[-0.00104702  0.60588171 -0.04053338 -0.9107887 ], action=1, reward=1.0, next_state=[ 0.01107061  0.80152803 -0.05874915 -1.21593068]\n",
      "[ Experience replay ] starts\n",
      "[ episode 295 ][ timestamp 5 ] state=[ 0.01107061  0.80152803 -0.05874915 -1.21593068], action=1, reward=1.0, next_state=[ 0.02710117  0.9973565  -0.08306776 -1.52642893]\n",
      "[ Experience replay ] starts\n",
      "[ episode 295 ][ timestamp 6 ] state=[ 0.02710117  0.9973565  -0.08306776 -1.52642893], action=1, reward=1.0, next_state=[ 0.0470483   1.19337694 -0.11359634 -1.84383958]\n",
      "[ Experience replay ] starts\n",
      "[ episode 295 ][ timestamp 7 ] state=[ 0.0470483   1.19337694 -0.11359634 -1.84383958], action=1, reward=1.0, next_state=[ 0.07091584  1.38955365 -0.15047313 -2.1695336 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 295 ][ timestamp 8 ] state=[ 0.07091584  1.38955365 -0.15047313 -2.1695336 ], action=1, reward=1.0, next_state=[ 0.09870692  1.58579013 -0.19386381 -2.50463454]\n",
      "[ Experience replay ] starts\n",
      "[ episode 295 ][ timestamp 9 ] state=[ 0.09870692  1.58579013 -0.19386381 -2.50463454], action=1, reward=-1.0, next_state=[ 0.13042272  1.78191156 -0.2439565  -2.84994544]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 295: Exploration_rate=0.01. Score=9.\n",
      "[ episode 296 ] state=[-0.04743947 -0.02583275 -0.035825   -0.00274793]\n",
      "[ episode 296 ][ timestamp 1 ] state=[-0.04743947 -0.02583275 -0.035825   -0.00274793], action=1, reward=1.0, next_state=[-0.04795613  0.16978418 -0.03587995 -0.30651535]\n",
      "[ Experience replay ] starts\n",
      "[ episode 296 ][ timestamp 2 ] state=[-0.04795613  0.16978418 -0.03587995 -0.30651535], action=1, reward=1.0, next_state=[-0.04456044  0.36539855 -0.04201026 -0.6102945 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 296 ][ timestamp 3 ] state=[-0.04456044  0.36539855 -0.04201026 -0.6102945 ], action=1, reward=1.0, next_state=[-0.03725247  0.56108177 -0.05421615 -0.91590773]\n",
      "[ Experience replay ] starts\n",
      "[ episode 296 ][ timestamp 4 ] state=[-0.03725247  0.56108177 -0.05421615 -0.91590773], action=1, reward=1.0, next_state=[-0.02603084  0.75689331 -0.07253431 -1.22512522]\n",
      "[ Experience replay ] starts\n",
      "[ episode 296 ][ timestamp 5 ] state=[-0.02603084  0.75689331 -0.07253431 -1.22512522], action=1, reward=1.0, next_state=[-0.01089297  0.95287043 -0.09703681 -1.53962432]\n",
      "[ Experience replay ] starts\n",
      "[ episode 296 ][ timestamp 6 ] state=[-0.01089297  0.95287043 -0.09703681 -1.53962432], action=1, reward=1.0, next_state=[ 0.00816444  1.14901654 -0.1278293  -1.86094343]\n",
      "[ Experience replay ] starts\n",
      "[ episode 296 ][ timestamp 7 ] state=[ 0.00816444  1.14901654 -0.1278293  -1.86094343], action=1, reward=1.0, next_state=[ 0.03114477  1.34528774 -0.16504817 -2.1904277 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 296 ][ timestamp 8 ] state=[ 0.03114477  1.34528774 -0.16504817 -2.1904277 ], action=1, reward=1.0, next_state=[ 0.05805052  1.54157718 -0.20885672 -2.52916478]\n",
      "[ Experience replay ] starts\n",
      "[ episode 296 ][ timestamp 9 ] state=[ 0.05805052  1.54157718 -0.20885672 -2.52916478], action=1, reward=-1.0, next_state=[ 0.08888207  1.73769722 -0.25944002 -2.87791033]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 296: Exploration_rate=0.01. Score=9.\n",
      "[ episode 297 ] state=[ 0.03936613 -0.00907758  0.0471985  -0.02660083]\n",
      "[ episode 297 ][ timestamp 1 ] state=[ 0.03936613 -0.00907758  0.0471985  -0.02660083], action=1, reward=1.0, next_state=[ 0.03918458  0.18533685  0.04666649 -0.3040265 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 297 ][ timestamp 2 ] state=[ 0.03918458  0.18533685  0.04666649 -0.3040265 ], action=1, reward=1.0, next_state=[ 0.04289131  0.37976375  0.04058596 -0.58163439]\n",
      "[ Experience replay ] starts\n",
      "[ episode 297 ][ timestamp 3 ] state=[ 0.04289131  0.37976375  0.04058596 -0.58163439], action=1, reward=1.0, next_state=[ 0.05048659  0.57429424  0.02895327 -0.86126083]\n",
      "[ Experience replay ] starts\n",
      "[ episode 297 ][ timestamp 4 ] state=[ 0.05048659  0.57429424  0.02895327 -0.86126083], action=1, reward=1.0, next_state=[ 0.06197247  0.7690102   0.01172805 -1.14470128]\n",
      "[ Experience replay ] starts\n",
      "[ episode 297 ][ timestamp 5 ] state=[ 0.06197247  0.7690102   0.01172805 -1.14470128], action=1, reward=1.0, next_state=[ 0.07735268  0.963977   -0.01116597 -1.4336834 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 297 ][ timestamp 6 ] state=[ 0.07735268  0.963977   -0.01116597 -1.4336834 ], action=1, reward=1.0, next_state=[ 0.09663222  1.1592349  -0.03983964 -1.72983472]\n",
      "[ Experience replay ] starts\n",
      "[ episode 297 ][ timestamp 7 ] state=[ 0.09663222  1.1592349  -0.03983964 -1.72983472], action=1, reward=1.0, next_state=[ 0.11981692  1.35478864 -0.07443634 -2.03464232]\n",
      "[ Experience replay ] starts\n",
      "[ episode 297 ][ timestamp 8 ] state=[ 0.11981692  1.35478864 -0.07443634 -2.03464232], action=1, reward=1.0, next_state=[ 0.14691269  1.55059458 -0.11512918 -2.34940201]\n",
      "[ Experience replay ] starts\n",
      "[ episode 297 ][ timestamp 9 ] state=[ 0.14691269  1.55059458 -0.11512918 -2.34940201], action=1, reward=1.0, next_state=[ 0.17792458  1.74654529 -0.16211722 -2.67515552]\n",
      "[ Experience replay ] starts\n",
      "[ episode 297 ][ timestamp 10 ] state=[ 0.17792458  1.74654529 -0.16211722 -2.67515552], action=1, reward=-1.0, next_state=[ 0.21285549  1.94245132 -0.21562033 -3.01261538]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 297: Exploration_rate=0.01. Score=10.\n",
      "[ episode 298 ] state=[ 0.02947255 -0.00350513  0.02216218 -0.04434541]\n",
      "[ episode 298 ][ timestamp 1 ] state=[ 0.02947255 -0.00350513  0.02216218 -0.04434541], action=1, reward=1.0, next_state=[ 0.02940244  0.19129213  0.02127527 -0.3299544 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 298 ][ timestamp 2 ] state=[ 0.02940244  0.19129213  0.02127527 -0.3299544 ], action=1, reward=1.0, next_state=[ 0.03322829  0.38610487  0.01467618 -0.61585291]\n",
      "[ Experience replay ] starts\n",
      "[ episode 298 ][ timestamp 3 ] state=[ 0.03322829  0.38610487  0.01467618 -0.61585291], action=1, reward=1.0, next_state=[ 0.04095038  0.58101873  0.00235912 -0.90387758]\n",
      "[ Experience replay ] starts\n",
      "[ episode 298 ][ timestamp 4 ] state=[ 0.04095038  0.58101873  0.00235912 -0.90387758], action=1, reward=1.0, next_state=[ 0.05257076  0.77610865 -0.01571843 -1.19581806]\n",
      "[ Experience replay ] starts\n",
      "[ episode 298 ][ timestamp 5 ] state=[ 0.05257076  0.77610865 -0.01571843 -1.19581806], action=1, reward=1.0, next_state=[ 0.06809293  0.97143053 -0.03963479 -1.49338571]\n",
      "[ Experience replay ] starts\n",
      "[ episode 298 ][ timestamp 6 ] state=[ 0.06809293  0.97143053 -0.03963479 -1.49338571], action=1, reward=1.0, next_state=[ 0.08752154  1.16701162 -0.0695025  -1.79817653]\n",
      "[ Experience replay ] starts\n",
      "[ episode 298 ][ timestamp 7 ] state=[ 0.08752154  1.16701162 -0.0695025  -1.79817653], action=1, reward=1.0, next_state=[ 0.11086178  1.36283895 -0.10546603 -2.11162563]\n",
      "[ Experience replay ] starts\n",
      "[ episode 298 ][ timestamp 8 ] state=[ 0.11086178  1.36283895 -0.10546603 -2.11162563], action=1, reward=1.0, next_state=[ 0.13811855  1.55884537 -0.14769855 -2.43495118]\n",
      "[ Experience replay ] starts\n",
      "[ episode 298 ][ timestamp 9 ] state=[ 0.13811855  1.55884537 -0.14769855 -2.43495118], action=1, reward=1.0, next_state=[ 0.16929546  1.75489295 -0.19639757 -2.76908649]\n",
      "[ Experience replay ] starts\n",
      "[ episode 298 ][ timestamp 10 ] state=[ 0.16929546  1.75489295 -0.19639757 -2.76908649], action=1, reward=-1.0, next_state=[ 0.20439332  1.95075416 -0.2517793  -3.11460082]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 298: Exploration_rate=0.01. Score=10.\n",
      "[ episode 299 ] state=[-0.03608013 -0.02063794  0.04196844 -0.00438427]\n",
      "[ episode 299 ][ timestamp 1 ] state=[-0.03608013 -0.02063794  0.04196844 -0.00438427], action=1, reward=1.0, next_state=[-0.03649289  0.17385778  0.04188076 -0.28353585]\n",
      "[ Experience replay ] starts\n",
      "[ episode 299 ][ timestamp 2 ] state=[-0.03649289  0.17385778  0.04188076 -0.28353585], action=1, reward=1.0, next_state=[-0.03301574  0.36835814  0.03621004 -0.56272122]\n",
      "[ Experience replay ] starts\n",
      "[ episode 299 ][ timestamp 3 ] state=[-0.03301574  0.36835814  0.03621004 -0.56272122], action=1, reward=1.0, next_state=[-0.02564857  0.56295376  0.02495562 -0.84377989]\n",
      "[ Experience replay ] starts\n",
      "[ episode 299 ][ timestamp 4 ] state=[-0.02564857  0.56295376  0.02495562 -0.84377989], action=1, reward=1.0, next_state=[-0.0143895   0.75772642  0.00808002 -1.12851172]\n",
      "[ Experience replay ] starts\n",
      "[ episode 299 ][ timestamp 5 ] state=[-0.0143895   0.75772642  0.00808002 -1.12851172], action=1, reward=1.0, next_state=[ 7.65029650e-04  9.52741603e-01 -1.44902153e-02 -1.41864944e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 299 ][ timestamp 6 ] state=[ 7.65029650e-04  9.52741603e-01 -1.44902153e-02 -1.41864944e+00], action=1, reward=1.0, next_state=[ 0.01981986  1.14803989 -0.0428632  -1.71582609]\n",
      "[ Experience replay ] starts\n",
      "[ episode 299 ][ timestamp 7 ] state=[ 0.01981986  1.14803989 -0.0428632  -1.71582609], action=1, reward=1.0, next_state=[ 0.04278066  1.34362645 -0.07717973 -2.0215344 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 299 ][ timestamp 8 ] state=[ 0.04278066  1.34362645 -0.07717973 -2.0215344 ], action=1, reward=1.0, next_state=[ 0.06965319  1.53945829 -0.11761041 -2.33707603]\n",
      "[ Experience replay ] starts\n",
      "[ episode 299 ][ timestamp 9 ] state=[ 0.06965319  1.53945829 -0.11761041 -2.33707603], action=1, reward=1.0, next_state=[ 0.10044235  1.73542874 -0.16435193 -2.66349881]\n",
      "[ Experience replay ] starts\n",
      "[ episode 299 ][ timestamp 10 ] state=[ 0.10044235  1.73542874 -0.16435193 -2.66349881], action=1, reward=-1.0, next_state=[ 0.13515093  1.93134937 -0.21762191 -3.00152182]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Ended! ] Episode 299: Exploration_rate=0.01. Score=10.\n",
      "[ episode 300 ] state=[ 0.01905837  0.0176583  -0.01354051  0.01228794]\n",
      "[ episode 300 ][ timestamp 1 ] state=[ 0.01905837  0.0176583  -0.01354051  0.01228794], action=1, reward=1.0, next_state=[ 0.01941154  0.21297179 -0.01329475 -0.28463624]\n",
      "[ Experience replay ] starts\n",
      "[ episode 300 ][ timestamp 2 ] state=[ 0.01941154  0.21297179 -0.01329475 -0.28463624], action=1, reward=1.0, next_state=[ 0.02367097  0.40828081 -0.01898748 -0.58148242]\n",
      "[ Experience replay ] starts\n",
      "[ episode 300 ][ timestamp 3 ] state=[ 0.02367097  0.40828081 -0.01898748 -0.58148242], action=1, reward=1.0, next_state=[ 0.03183659  0.60366359 -0.03061713 -0.88008574]\n",
      "[ Experience replay ] starts\n",
      "[ episode 300 ][ timestamp 4 ] state=[ 0.03183659  0.60366359 -0.03061713 -0.88008574], action=1, reward=1.0, next_state=[ 0.04390986  0.79918782 -0.04821884 -1.18223466]\n",
      "[ Experience replay ] starts\n",
      "[ episode 300 ][ timestamp 5 ] state=[ 0.04390986  0.79918782 -0.04821884 -1.18223466], action=1, reward=1.0, next_state=[ 0.05989362  0.99490123 -0.07186354 -1.48963441]\n",
      "[ Experience replay ] starts\n",
      "[ episode 300 ][ timestamp 6 ] state=[ 0.05989362  0.99490123 -0.07186354 -1.48963441], action=1, reward=1.0, next_state=[ 0.07979164  1.19082092 -0.10165622 -1.80386511]\n",
      "[ Experience replay ] starts\n",
      "[ episode 300 ][ timestamp 7 ] state=[ 0.07979164  1.19082092 -0.10165622 -1.80386511], action=1, reward=1.0, next_state=[ 0.10360806  1.38692082 -0.13773353 -2.12633188]\n",
      "[ Experience replay ] starts\n",
      "[ episode 300 ][ timestamp 8 ] state=[ 0.10360806  1.38692082 -0.13773353 -2.12633188], action=1, reward=1.0, next_state=[ 0.13134648  1.58311694 -0.18026016 -2.45820476]\n",
      "[ Experience replay ] starts\n",
      "[ episode 300 ][ timestamp 9 ] state=[ 0.13134648  1.58311694 -0.18026016 -2.45820476], action=1, reward=-1.0, next_state=[ 0.16300882  1.77925019 -0.22942426 -2.80034769]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 300: Exploration_rate=0.01. Score=9.\n",
      "[ episode 301 ] state=[-0.00580059 -0.03888155  0.01423531 -0.02519646]\n",
      "[ episode 301 ][ timestamp 1 ] state=[-0.00580059 -0.03888155  0.01423531 -0.02519646], action=1, reward=1.0, next_state=[-0.00657822  0.15603339  0.01373139 -0.31335421]\n",
      "[ Experience replay ] starts\n",
      "[ episode 301 ][ timestamp 2 ] state=[-0.00657822  0.15603339  0.01373139 -0.31335421], action=1, reward=1.0, next_state=[-0.00345755  0.35095707  0.0074643  -0.60167525]\n",
      "[ Experience replay ] starts\n",
      "[ episode 301 ][ timestamp 3 ] state=[-0.00345755  0.35095707  0.0074643  -0.60167525], action=1, reward=1.0, next_state=[ 0.00356159  0.54597381 -0.0045692  -0.89199774]\n",
      "[ Experience replay ] starts\n",
      "[ episode 301 ][ timestamp 4 ] state=[ 0.00356159  0.54597381 -0.0045692  -0.89199774], action=1, reward=1.0, next_state=[ 0.01448107  0.74115745 -0.02240916 -1.18611348]\n",
      "[ Experience replay ] starts\n",
      "[ episode 301 ][ timestamp 5 ] state=[ 0.01448107  0.74115745 -0.02240916 -1.18611348], action=1, reward=1.0, next_state=[ 0.02930422  0.93656274 -0.04613143 -1.48573556]\n",
      "[ Experience replay ] starts\n",
      "[ episode 301 ][ timestamp 6 ] state=[ 0.02930422  0.93656274 -0.04613143 -1.48573556], action=1, reward=1.0, next_state=[ 0.04803547  1.13221559 -0.07584614 -1.79246044]\n",
      "[ Experience replay ] starts\n",
      "[ episode 301 ][ timestamp 7 ] state=[ 0.04803547  1.13221559 -0.07584614 -1.79246044], action=1, reward=1.0, next_state=[ 0.07067978  1.3281013  -0.11169535 -2.10772167]\n",
      "[ Experience replay ] starts\n",
      "[ episode 301 ][ timestamp 8 ] state=[ 0.07067978  1.3281013  -0.11169535 -2.10772167], action=1, reward=1.0, next_state=[ 0.09724181  1.52415052 -0.15384978 -2.43273319]\n",
      "[ Experience replay ] starts\n",
      "[ episode 301 ][ timestamp 9 ] state=[ 0.09724181  1.52415052 -0.15384978 -2.43273319], action=1, reward=1.0, next_state=[ 0.12772482  1.72022253 -0.20250445 -2.76842094]\n",
      "[ Experience replay ] starts\n",
      "[ episode 301 ][ timestamp 10 ] state=[ 0.12772482  1.72022253 -0.20250445 -2.76842094], action=1, reward=-1.0, next_state=[ 0.16212927  1.91608638 -0.25787286 -3.11534351]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 301: Exploration_rate=0.01. Score=10.\n",
      "[ episode 302 ] state=[-0.02556469 -0.03206176  0.03584896  0.03421253]\n",
      "[ episode 302 ][ timestamp 1 ] state=[-0.02556469 -0.03206176  0.03584896  0.03421253], action=1, reward=1.0, next_state=[-0.02620593  0.16252825  0.03653322 -0.24694762]\n",
      "[ Experience replay ] starts\n",
      "[ episode 302 ][ timestamp 2 ] state=[-0.02620593  0.16252825  0.03653322 -0.24694762], action=1, reward=1.0, next_state=[-0.02295536  0.35710991  0.03159426 -0.52788697]\n",
      "[ Experience replay ] starts\n",
      "[ episode 302 ][ timestamp 3 ] state=[-0.02295536  0.35710991  0.03159426 -0.52788697], action=1, reward=1.0, next_state=[-0.01581317  0.55177343  0.02103652 -0.81044936]\n",
      "[ Experience replay ] starts\n",
      "[ episode 302 ][ timestamp 4 ] state=[-0.01581317  0.55177343  0.02103652 -0.81044936], action=1, reward=1.0, next_state=[-0.0047777   0.74660095  0.00482754 -1.09644169]\n",
      "[ Experience replay ] starts\n",
      "[ episode 302 ][ timestamp 5 ] state=[-0.0047777   0.74660095  0.00482754 -1.09644169], action=1, reward=1.0, next_state=[ 0.01015432  0.941659   -0.0171013  -1.38760606]\n",
      "[ Experience replay ] starts\n",
      "[ episode 302 ][ timestamp 6 ] state=[ 0.01015432  0.941659   -0.0171013  -1.38760606], action=1, reward=1.0, next_state=[ 0.0289875   1.13698985 -0.04485342 -1.68558704]\n",
      "[ Experience replay ] starts\n",
      "[ episode 302 ][ timestamp 7 ] state=[ 0.0289875   1.13698985 -0.04485342 -1.68558704], action=1, reward=1.0, next_state=[ 0.0517273   1.33260114 -0.07856516 -1.99189136]\n",
      "[ Experience replay ] starts\n",
      "[ episode 302 ][ timestamp 8 ] state=[ 0.0517273   1.33260114 -0.07856516 -1.99189136], action=1, reward=1.0, next_state=[ 0.07837932  1.52845313 -0.11840299 -2.30783753]\n",
      "[ Experience replay ] starts\n",
      "[ episode 302 ][ timestamp 9 ] state=[ 0.07837932  1.52845313 -0.11840299 -2.30783753], action=1, reward=1.0, next_state=[ 0.10894838  1.7244434  -0.16455974 -2.63449382]\n",
      "[ Experience replay ] starts\n",
      "[ episode 302 ][ timestamp 10 ] state=[ 0.10894838  1.7244434  -0.16455974 -2.63449382], action=1, reward=-1.0, next_state=[ 0.14343725  1.92038898 -0.21724961 -2.97260403]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 302: Exploration_rate=0.01. Score=10.\n",
      "[ episode 303 ] state=[0.00334792 0.02398538 0.0325281  0.00448266]\n",
      "[ episode 303 ][ timestamp 1 ] state=[0.00334792 0.02398538 0.0325281  0.00448266], action=1, reward=1.0, next_state=[ 0.00382762  0.21862609  0.03261775 -0.2777624 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 303 ][ timestamp 2 ] state=[ 0.00382762  0.21862609  0.03261775 -0.2777624 ], action=1, reward=1.0, next_state=[ 0.00820015  0.4132679   0.0270625  -0.55998189]\n",
      "[ Experience replay ] starts\n",
      "[ episode 303 ][ timestamp 3 ] state=[ 0.00820015  0.4132679   0.0270625  -0.55998189], action=1, reward=1.0, next_state=[ 0.0164655   0.60799977  0.01586287 -0.84401733]\n",
      "[ Experience replay ] starts\n",
      "[ episode 303 ][ timestamp 4 ] state=[ 0.0164655   0.60799977  0.01586287 -0.84401733], action=1, reward=1.0, next_state=[ 2.86255001e-02  8.02901697e-01 -1.01748157e-03 -1.13166996e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 303 ][ timestamp 5 ] state=[ 2.86255001e-02  8.02901697e-01 -1.01748157e-03 -1.13166996e+00], action=1, reward=1.0, next_state=[ 0.04468353  0.99803695 -0.02365088 -1.42467183]\n",
      "[ Experience replay ] starts\n",
      "[ episode 303 ][ timestamp 6 ] state=[ 0.04468353  0.99803695 -0.02365088 -1.42467183], action=1, reward=1.0, next_state=[ 0.06464427  1.19344314 -0.05214432 -1.72465185]\n",
      "[ Experience replay ] starts\n",
      "[ episode 303 ][ timestamp 7 ] state=[ 0.06464427  1.19344314 -0.05214432 -1.72465185], action=1, reward=1.0, next_state=[ 0.08851314  1.38912142 -0.08663735 -2.0330938 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 303 ][ timestamp 8 ] state=[ 0.08851314  1.38912142 -0.08663735 -2.0330938 ], action=1, reward=1.0, next_state=[ 0.11629556  1.5850234  -0.12729923 -2.35128416]\n",
      "[ Experience replay ] starts\n",
      "[ episode 303 ][ timestamp 9 ] state=[ 0.11629556  1.5850234  -0.12729923 -2.35128416], action=1, reward=1.0, next_state=[ 0.14799603  1.78103543 -0.17432491 -2.68024809]\n",
      "[ Experience replay ] starts\n",
      "[ episode 303 ][ timestamp 10 ] state=[ 0.14799603  1.78103543 -0.17432491 -2.68024809], action=1, reward=-1.0, next_state=[ 0.18361674  1.97696027 -0.22792988 -3.02067348]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 303: Exploration_rate=0.01. Score=10.\n",
      "[ episode 304 ] state=[-0.00343493  0.03122857  0.01465782  0.04991634]\n",
      "[ episode 304 ][ timestamp 1 ] state=[-0.00343493  0.03122857  0.01465782  0.04991634], action=1, reward=1.0, next_state=[-0.00281036  0.2261373   0.01565614 -0.23810612]\n",
      "[ Experience replay ] starts\n",
      "[ episode 304 ][ timestamp 2 ] state=[-0.00281036  0.2261373   0.01565614 -0.23810612], action=1, reward=1.0, next_state=[ 0.00171238  0.42103213  0.01089402 -0.52580981]\n",
      "[ Experience replay ] starts\n",
      "[ episode 304 ][ timestamp 3 ] state=[ 0.00171238  0.42103213  0.01089402 -0.52580981], action=1, reward=1.0, next_state=[ 1.01330263e-02  6.15999104e-01  3.77826138e-04 -8.15040135e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 304 ][ timestamp 4 ] state=[ 1.01330263e-02  6.15999104e-01  3.77826138e-04 -8.15040135e-01], action=1, reward=1.0, next_state=[ 0.02245301  0.81111588 -0.01592298 -1.1076042 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 304 ][ timestamp 5 ] state=[ 0.02245301  0.81111588 -0.01592298 -1.1076042 ], action=1, reward=1.0, next_state=[ 0.03867533  1.00644347 -0.03807506 -1.4052396 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 304 ][ timestamp 6 ] state=[ 0.03867533  1.00644347 -0.03807506 -1.4052396 ], action=1, reward=1.0, next_state=[ 0.0588042   1.20201688 -0.06617985 -1.70957845]\n",
      "[ Experience replay ] starts\n",
      "[ episode 304 ][ timestamp 7 ] state=[ 0.0588042   1.20201688 -0.06617985 -1.70957845], action=1, reward=1.0, next_state=[ 0.08284453  1.39783395 -0.10037142 -2.02210374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 304 ][ timestamp 8 ] state=[ 0.08284453  1.39783395 -0.10037142 -2.02210374], action=1, reward=1.0, next_state=[ 0.11080121  1.59384199 -0.1408135  -2.34409573]\n",
      "[ Experience replay ] starts\n",
      "[ episode 304 ][ timestamp 9 ] state=[ 0.11080121  1.59384199 -0.1408135  -2.34409573], action=1, reward=1.0, next_state=[ 0.14267805  1.78992184 -0.18769541 -2.67656684]\n",
      "[ Experience replay ] starts\n",
      "[ episode 304 ][ timestamp 10 ] state=[ 0.14267805  1.78992184 -0.18769541 -2.67656684], action=1, reward=-1.0, next_state=[ 0.17847649  1.98586947 -0.24122675 -3.02018511]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 304: Exploration_rate=0.01. Score=10.\n",
      "[ episode 305 ] state=[-0.02092663 -0.04115671  0.03433351  0.03770665]\n",
      "[ episode 305 ][ timestamp 1 ] state=[-0.02092663 -0.04115671  0.03433351  0.03770665], action=1, reward=1.0, next_state=[-0.02174977  0.1534565   0.03508765 -0.24394907]\n",
      "[ Experience replay ] starts\n",
      "[ episode 305 ][ timestamp 2 ] state=[-0.02174977  0.1534565   0.03508765 -0.24394907], action=1, reward=1.0, next_state=[-0.01868064  0.34806017  0.03020866 -0.52536125]\n",
      "[ Experience replay ] starts\n",
      "[ episode 305 ][ timestamp 3 ] state=[-0.01868064  0.34806017  0.03020866 -0.52536125], action=1, reward=1.0, next_state=[-0.01171943  0.54274428  0.01970144 -0.80837419]\n",
      "[ Experience replay ] starts\n",
      "[ episode 305 ][ timestamp 4 ] state=[-0.01171943  0.54274428  0.01970144 -0.80837419], action=1, reward=1.0, next_state=[-8.64546707e-04  7.37590788e-01  3.53395587e-03 -1.09479537e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 305 ][ timestamp 5 ] state=[-8.64546707e-04  7.37590788e-01  3.53395587e-03 -1.09479537e+00], action=1, reward=1.0, next_state=[ 0.01388727  0.93266601 -0.01836195 -1.3863674 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 305 ][ timestamp 6 ] state=[ 0.01388727  0.93266601 -0.01836195 -1.3863674 ], action=1, reward=1.0, next_state=[ 0.03254059  1.12801199 -0.0460893  -1.68473509]\n",
      "[ Experience replay ] starts\n",
      "[ episode 305 ][ timestamp 7 ] state=[ 0.03254059  1.12801199 -0.0460893  -1.68473509], action=1, reward=1.0, next_state=[ 0.05510083  1.32363603 -0.079784   -1.991405  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 305 ][ timestamp 8 ] state=[ 0.05510083  1.32363603 -0.079784   -1.991405  ], action=1, reward=1.0, next_state=[ 0.08157355  1.51949796 -0.1196121  -2.30769494]\n",
      "[ Experience replay ] starts\n",
      "[ episode 305 ][ timestamp 9 ] state=[ 0.08157355  1.51949796 -0.1196121  -2.30769494], action=1, reward=1.0, next_state=[ 0.11196351  1.71549484 -0.165766   -2.63467182]\n",
      "[ Experience replay ] starts\n",
      "[ episode 305 ][ timestamp 10 ] state=[ 0.11196351  1.71549484 -0.165766   -2.63467182], action=1, reward=-1.0, next_state=[ 0.14627341  1.91144296 -0.21845944 -2.97307731]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 305: Exploration_rate=0.01. Score=10.\n",
      "[ episode 306 ] state=[ 0.04060167 -0.04990466 -0.04858994  0.04503864]\n",
      "[ episode 306 ][ timestamp 1 ] state=[ 0.04060167 -0.04990466 -0.04858994  0.04503864], action=1, reward=1.0, next_state=[ 0.03960358  0.14587915 -0.04768917 -0.26257029]\n",
      "[ Experience replay ] starts\n",
      "[ episode 306 ][ timestamp 2 ] state=[ 0.03960358  0.14587915 -0.04768917 -0.26257029], action=1, reward=1.0, next_state=[ 0.04252116  0.34164824 -0.05294057 -0.56990536]\n",
      "[ Experience replay ] starts\n",
      "[ episode 306 ][ timestamp 3 ] state=[ 0.04252116  0.34164824 -0.05294057 -0.56990536], action=1, reward=1.0, next_state=[ 0.04935413  0.53747113 -0.06433868 -0.87878543]\n",
      "[ Experience replay ] starts\n",
      "[ episode 306 ][ timestamp 4 ] state=[ 0.04935413  0.53747113 -0.06433868 -0.87878543], action=1, reward=1.0, next_state=[ 0.06010355  0.73340554 -0.08191439 -1.19098148]\n",
      "[ Experience replay ] starts\n",
      "[ episode 306 ][ timestamp 5 ] state=[ 0.06010355  0.73340554 -0.08191439 -1.19098148], action=1, reward=1.0, next_state=[ 0.07477166  0.92948773 -0.10573402 -1.50817445]\n",
      "[ Experience replay ] starts\n",
      "[ episode 306 ][ timestamp 6 ] state=[ 0.07477166  0.92948773 -0.10573402 -1.50817445], action=1, reward=1.0, next_state=[ 0.09336142  1.12572066 -0.13589751 -1.83190792]\n",
      "[ Experience replay ] starts\n",
      "[ episode 306 ][ timestamp 7 ] state=[ 0.09336142  1.12572066 -0.13589751 -1.83190792], action=1, reward=1.0, next_state=[ 0.11587583  1.32206042 -0.17253566 -2.16353322]\n",
      "[ Experience replay ] starts\n",
      "[ episode 306 ][ timestamp 8 ] state=[ 0.11587583  1.32206042 -0.17253566 -2.16353322], action=1, reward=-1.0, next_state=[ 0.14231704  1.51840052 -0.21580633 -2.50414484]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 306: Exploration_rate=0.01. Score=8.\n",
      "[ episode 307 ] state=[-0.04436083  0.02073563  0.00548693 -0.03211743]\n",
      "[ episode 307 ][ timestamp 1 ] state=[-0.04436083  0.02073563  0.00548693 -0.03211743], action=1, reward=1.0, next_state=[-0.04394612  0.21577847  0.00484458 -0.32306413]\n",
      "[ Experience replay ] starts\n",
      "[ episode 307 ][ timestamp 2 ] state=[-0.04394612  0.21577847  0.00484458 -0.32306413], action=1, reward=1.0, next_state=[-0.03963055  0.4108311  -0.0016167  -0.61421535]\n",
      "[ Experience replay ] starts\n",
      "[ episode 307 ][ timestamp 3 ] state=[-0.03963055  0.4108311  -0.0016167  -0.61421535], action=1, reward=1.0, next_state=[-0.03141393  0.6059756  -0.01390101 -0.90740703]\n",
      "[ Experience replay ] starts\n",
      "[ episode 307 ][ timestamp 4 ] state=[-0.03141393  0.6059756  -0.01390101 -0.90740703], action=1, reward=1.0, next_state=[-0.01929442  0.80128296 -0.03204915 -1.20442653]\n",
      "[ Experience replay ] starts\n",
      "[ episode 307 ][ timestamp 5 ] state=[-0.01929442  0.80128296 -0.03204915 -1.20442653], action=1, reward=1.0, next_state=[-0.00326876  0.99680419 -0.05613768 -1.5069786 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 307 ][ timestamp 6 ] state=[-0.00326876  0.99680419 -0.05613768 -1.5069786 ], action=1, reward=1.0, next_state=[ 0.01666733  1.19256014 -0.08627725 -1.81664577]\n",
      "[ Experience replay ] starts\n",
      "[ episode 307 ][ timestamp 7 ] state=[ 0.01666733  1.19256014 -0.08627725 -1.81664577], action=1, reward=1.0, next_state=[ 0.04051853  1.38852939 -0.12261017 -2.13484031]\n",
      "[ Experience replay ] starts\n",
      "[ episode 307 ][ timestamp 8 ] state=[ 0.04051853  1.38852939 -0.12261017 -2.13484031], action=1, reward=1.0, next_state=[ 0.06828912  1.58463374 -0.16530697 -2.46274568]\n",
      "[ Experience replay ] starts\n",
      "[ episode 307 ][ timestamp 9 ] state=[ 0.06828912  1.58463374 -0.16530697 -2.46274568], action=1, reward=-1.0, next_state=[ 0.09998179  1.78072126 -0.21456189 -2.80124654]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 307: Exploration_rate=0.01. Score=9.\n",
      "[ episode 308 ] state=[ 0.00198691  0.02381976 -0.0159085   0.03802183]\n",
      "[ episode 308 ][ timestamp 1 ] state=[ 0.00198691  0.02381976 -0.0159085   0.03802183], action=1, reward=1.0, next_state=[ 0.00246331  0.21916618 -0.01514806 -0.25963763]\n",
      "[ Experience replay ] starts\n",
      "[ episode 308 ][ timestamp 2 ] state=[ 0.00246331  0.21916618 -0.01514806 -0.25963763], action=1, reward=1.0, next_state=[ 0.00684663  0.41450107 -0.02034082 -0.55705971]\n",
      "[ Experience replay ] starts\n",
      "[ episode 308 ][ timestamp 3 ] state=[ 0.00684663  0.41450107 -0.02034082 -0.55705971], action=1, reward=1.0, next_state=[ 0.01513665  0.60990259 -0.03148201 -0.85608114]\n",
      "[ Experience replay ] starts\n",
      "[ episode 308 ][ timestamp 4 ] state=[ 0.01513665  0.60990259 -0.03148201 -0.85608114], action=1, reward=1.0, next_state=[ 0.0273347   0.80543905 -0.04860363 -1.15849469]\n",
      "[ Experience replay ] starts\n",
      "[ episode 308 ][ timestamp 5 ] state=[ 0.0273347   0.80543905 -0.04860363 -1.15849469], action=1, reward=1.0, next_state=[ 0.04344349  1.00115954 -0.07177353 -1.46601255]\n",
      "[ Experience replay ] starts\n",
      "[ episode 308 ][ timestamp 6 ] state=[ 0.04344349  1.00115954 -0.07177353 -1.46601255], action=1, reward=1.0, next_state=[ 0.06346668  1.19708321 -0.10109378 -1.78022472]\n",
      "[ Experience replay ] starts\n",
      "[ episode 308 ][ timestamp 7 ] state=[ 0.06346668  1.19708321 -0.10109378 -1.78022472], action=1, reward=1.0, next_state=[ 0.08740834  1.39318694 -0.13669827 -2.10254945]\n",
      "[ Experience replay ] starts\n",
      "[ episode 308 ][ timestamp 8 ] state=[ 0.08740834  1.39318694 -0.13669827 -2.10254945], action=1, reward=1.0, next_state=[ 0.11527208  1.58939068 -0.17874926 -2.43417383]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 308 ][ timestamp 9 ] state=[ 0.11527208  1.58939068 -0.17874926 -2.43417383], action=1, reward=-1.0, next_state=[ 0.14705989  1.78554039 -0.22743274 -2.77598334]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 308: Exploration_rate=0.01. Score=9.\n",
      "[ episode 309 ] state=[-0.04423704  0.01798281  0.00018785 -0.00372284]\n",
      "[ episode 309 ][ timestamp 1 ] state=[-0.04423704  0.01798281  0.00018785 -0.00372284], action=1, reward=1.0, next_state=[-4.38773794e-02  2.13102071e-01  1.13393862e-04 -2.96346495e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 309 ][ timestamp 2 ] state=[-4.38773794e-02  2.13102071e-01  1.13393862e-04 -2.96346495e-01], action=1, reward=1.0, next_state=[-0.03961534  0.40822241 -0.00581354 -0.58899366]\n",
      "[ Experience replay ] starts\n",
      "[ episode 309 ][ timestamp 3 ] state=[-0.03961534  0.40822241 -0.00581354 -0.58899366], action=1, reward=1.0, next_state=[-0.03145089  0.60342528 -0.01759341 -0.88350219]\n",
      "[ Experience replay ] starts\n",
      "[ episode 309 ][ timestamp 4 ] state=[-0.03145089  0.60342528 -0.01759341 -0.88350219], action=1, reward=1.0, next_state=[-0.01938238  0.79878167 -0.03526345 -1.18166362]\n",
      "[ Experience replay ] starts\n",
      "[ episode 309 ][ timestamp 5 ] state=[-0.01938238  0.79878167 -0.03526345 -1.18166362], action=1, reward=1.0, next_state=[-0.00340675  0.99434312 -0.05889673 -1.48518873]\n",
      "[ Experience replay ] starts\n",
      "[ episode 309 ][ timestamp 6 ] state=[-0.00340675  0.99434312 -0.05889673 -1.48518873], action=1, reward=1.0, next_state=[ 0.01648011  1.19013147 -0.0886005  -1.79566768]\n",
      "[ Experience replay ] starts\n",
      "[ episode 309 ][ timestamp 7 ] state=[ 0.01648011  1.19013147 -0.0886005  -1.79566768], action=1, reward=1.0, next_state=[ 0.04028274  1.38612681 -0.12451385 -2.11452198]\n",
      "[ Experience replay ] starts\n",
      "[ episode 309 ][ timestamp 8 ] state=[ 0.04028274  1.38612681 -0.12451385 -2.11452198], action=1, reward=1.0, next_state=[ 0.06800528  1.58225299 -0.16680429 -2.44294624]\n",
      "[ Experience replay ] starts\n",
      "[ episode 309 ][ timestamp 9 ] state=[ 0.06800528  1.58225299 -0.16680429 -2.44294624], action=1, reward=-1.0, next_state=[ 0.09965034  1.77836076 -0.21566322 -2.78183841]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 309: Exploration_rate=0.01. Score=9.\n",
      "[ episode 310 ] state=[ 0.03724037 -0.02649702  0.02218003  0.03375365]\n",
      "[ episode 310 ][ timestamp 1 ] state=[ 0.03724037 -0.02649702  0.02218003  0.03375365], action=1, reward=1.0, next_state=[ 0.03671043  0.16829996  0.0228551  -0.25184955]\n",
      "[ Experience replay ] starts\n",
      "[ episode 310 ][ timestamp 2 ] state=[ 0.03671043  0.16829996  0.0228551  -0.25184955], action=1, reward=1.0, next_state=[ 0.04007643  0.36308822  0.01781811 -0.53723682]\n",
      "[ Experience replay ] starts\n",
      "[ episode 310 ][ timestamp 3 ] state=[ 0.04007643  0.36308822  0.01781811 -0.53723682], action=1, reward=1.0, next_state=[ 0.04733819  0.55795518  0.00707337 -0.82425261]\n",
      "[ Experience replay ] starts\n",
      "[ episode 310 ][ timestamp 4 ] state=[ 0.04733819  0.55795518  0.00707337 -0.82425261], action=1, reward=1.0, next_state=[ 0.0584973   0.75297966 -0.00941168 -1.11470247]\n",
      "[ Experience replay ] starts\n",
      "[ episode 310 ][ timestamp 5 ] state=[ 0.0584973   0.75297966 -0.00941168 -1.11470247], action=1, reward=1.0, next_state=[ 0.07355689  0.94822391 -0.03170573 -1.41032286]\n",
      "[ Experience replay ] starts\n",
      "[ episode 310 ][ timestamp 6 ] state=[ 0.07355689  0.94822391 -0.03170573 -1.41032286], action=1, reward=1.0, next_state=[ 0.09252137  1.14372437 -0.05991219 -1.71274609]\n",
      "[ Experience replay ] starts\n",
      "[ episode 310 ][ timestamp 7 ] state=[ 0.09252137  1.14372437 -0.05991219 -1.71274609], action=1, reward=1.0, next_state=[ 0.11539585  1.33948078 -0.09416711 -2.02345751]\n",
      "[ Experience replay ] starts\n",
      "[ episode 310 ][ timestamp 8 ] state=[ 0.11539585  1.33948078 -0.09416711 -2.02345751], action=1, reward=1.0, next_state=[ 0.14218547  1.53544289 -0.13463626 -2.34374261]\n",
      "[ Experience replay ] starts\n",
      "[ episode 310 ][ timestamp 9 ] state=[ 0.14218547  1.53544289 -0.13463626 -2.34374261], action=1, reward=1.0, next_state=[ 0.17289433  1.73149464 -0.18151111 -2.67462249]\n",
      "[ Experience replay ] starts\n",
      "[ episode 310 ][ timestamp 10 ] state=[ 0.17289433  1.73149464 -0.18151111 -2.67462249], action=1, reward=-1.0, next_state=[ 0.20752422  1.92743589 -0.23500356 -3.01677771]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 310: Exploration_rate=0.01. Score=10.\n",
      "[ episode 311 ] state=[ 0.02062566 -0.02638916  0.01126309 -0.00673929]\n",
      "[ episode 311 ][ timestamp 1 ] state=[ 0.02062566 -0.02638916  0.01126309 -0.00673929], action=1, reward=1.0, next_state=[ 0.02009788  0.16856946  0.0111283  -0.2958474 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 311 ][ timestamp 2 ] state=[ 0.02009788  0.16856946  0.0111283  -0.2958474 ], action=1, reward=1.0, next_state=[ 0.02346927  0.36353102  0.00521135 -0.58499997]\n",
      "[ Experience replay ] starts\n",
      "[ episode 311 ][ timestamp 3 ] state=[ 0.02346927  0.36353102  0.00521135 -0.58499997], action=1, reward=1.0, next_state=[ 0.03073989  0.55857958 -0.00648865 -0.87603672]\n",
      "[ Experience replay ] starts\n",
      "[ episode 311 ][ timestamp 4 ] state=[ 0.03073989  0.55857958 -0.00648865 -0.87603672], action=1, reward=1.0, next_state=[ 0.04191148  0.75378913 -0.02400938 -1.17075252]\n",
      "[ Experience replay ] starts\n",
      "[ episode 311 ][ timestamp 5 ] state=[ 0.04191148  0.75378913 -0.02400938 -1.17075252], action=1, reward=1.0, next_state=[ 0.05698726  0.94921493 -0.04742443 -1.47086482]\n",
      "[ Experience replay ] starts\n",
      "[ episode 311 ][ timestamp 6 ] state=[ 0.05698726  0.94921493 -0.04742443 -1.47086482], action=1, reward=1.0, next_state=[ 0.07597156  1.14488376 -0.07684173 -1.77797563]\n",
      "[ Experience replay ] starts\n",
      "[ episode 311 ][ timestamp 7 ] state=[ 0.07597156  1.14488376 -0.07684173 -1.77797563], action=1, reward=1.0, next_state=[ 0.09886924  1.34078218 -0.11240124 -2.09352539]\n",
      "[ Experience replay ] starts\n",
      "[ episode 311 ][ timestamp 8 ] state=[ 0.09886924  1.34078218 -0.11240124 -2.09352539], action=1, reward=1.0, next_state=[ 0.12568488  1.53684249 -0.15427175 -2.41873646]\n",
      "[ Experience replay ] starts\n",
      "[ episode 311 ][ timestamp 9 ] state=[ 0.12568488  1.53684249 -0.15427175 -2.41873646], action=1, reward=1.0, next_state=[ 0.15642173  1.73292619 -0.20264648 -2.75454508]\n",
      "[ Experience replay ] starts\n",
      "[ episode 311 ][ timestamp 10 ] state=[ 0.15642173  1.73292619 -0.20264648 -2.75454508], action=1, reward=-1.0, next_state=[ 0.19108025  1.92880507 -0.25773738 -3.10152224]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 311: Exploration_rate=0.01. Score=10.\n",
      "[ episode 312 ] state=[-0.01924703  0.00376477  0.00718515 -0.02135947]\n",
      "[ episode 312 ][ timestamp 1 ] state=[-0.01924703  0.00376477  0.00718515 -0.02135947], action=1, reward=1.0, next_state=[-0.01917174  0.19878295  0.00675796 -0.31176677]\n",
      "[ Experience replay ] starts\n",
      "[ episode 312 ][ timestamp 2 ] state=[-0.01917174  0.19878295  0.00675796 -0.31176677], action=1, reward=1.0, next_state=[-1.51960787e-02  3.93807972e-01  5.22623636e-04 -6.02310803e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 312 ][ timestamp 3 ] state=[-1.51960787e-02  3.93807972e-01  5.22623636e-04 -6.02310803e-01], action=1, reward=1.0, next_state=[-0.00731992  0.58892261 -0.01152359 -0.89482907]\n",
      "[ Experience replay ] starts\n",
      "[ episode 312 ][ timestamp 4 ] state=[-0.00731992  0.58892261 -0.01152359 -0.89482907], action=1, reward=1.0, next_state=[ 0.00445853  0.78419891 -0.02942017 -1.19111193]\n",
      "[ Experience replay ] starts\n",
      "[ episode 312 ][ timestamp 5 ] state=[ 0.00445853  0.78419891 -0.02942017 -1.19111193], action=1, reward=1.0, next_state=[ 0.02014251  0.97968945 -0.05324241 -1.49286914]\n",
      "[ Experience replay ] starts\n",
      "[ episode 312 ][ timestamp 6 ] state=[ 0.02014251  0.97968945 -0.05324241 -1.49286914], action=1, reward=1.0, next_state=[ 0.0397363   1.17541727 -0.0830998  -1.8016907 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 312 ][ timestamp 7 ] state=[ 0.0397363   1.17541727 -0.0830998  -1.8016907 ], action=1, reward=1.0, next_state=[ 0.06324465  1.37136386 -0.11913361 -2.11899956]\n",
      "[ Experience replay ] starts\n",
      "[ episode 312 ][ timestamp 8 ] state=[ 0.06324465  1.37136386 -0.11913361 -2.11899956], action=1, reward=1.0, next_state=[ 0.09067192  1.56745492 -0.1615136  -2.44599379]\n",
      "[ Experience replay ] starts\n",
      "[ episode 312 ][ timestamp 9 ] state=[ 0.09067192  1.56745492 -0.1615136  -2.44599379], action=1, reward=-1.0, next_state=[ 0.12202102  1.76354344 -0.21043348 -2.78357724]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 312: Exploration_rate=0.01. Score=9.\n",
      "[ episode 313 ] state=[-0.02286923 -0.0171292  -0.00720667  0.03386126]\n",
      "[ episode 313 ][ timestamp 1 ] state=[-0.02286923 -0.0171292  -0.00720667  0.03386126], action=1, reward=1.0, next_state=[-0.02321182  0.17809535 -0.00652945 -0.2610867 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 313 ][ timestamp 2 ] state=[-0.02321182  0.17809535 -0.00652945 -0.2610867 ], action=1, reward=1.0, next_state=[-0.01964991  0.3733099  -0.01175118 -0.55582193]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 313 ][ timestamp 3 ] state=[-0.01964991  0.3733099  -0.01175118 -0.55582193], action=1, reward=1.0, next_state=[-0.01218371  0.56859485 -0.02286762 -0.85218389]\n",
      "[ Experience replay ] starts\n",
      "[ episode 313 ][ timestamp 4 ] state=[-0.01218371  0.56859485 -0.02286762 -0.85218389], action=1, reward=1.0, next_state=[-8.11813303e-04  7.64020963e-01 -3.99112964e-02 -1.15196892e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 313 ][ timestamp 5 ] state=[-8.11813303e-04  7.64020963e-01 -3.99112964e-02 -1.15196892e+00], action=1, reward=1.0, next_state=[ 0.01446861  0.95964025 -0.06295067 -1.45689498]\n",
      "[ Experience replay ] starts\n",
      "[ episode 313 ][ timestamp 6 ] state=[ 0.01446861  0.95964025 -0.06295067 -1.45689498], action=1, reward=1.0, next_state=[ 0.03366141  1.15547564 -0.09208857 -1.7685615 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 313 ][ timestamp 7 ] state=[ 0.03366141  1.15547564 -0.09208857 -1.7685615 ], action=1, reward=1.0, next_state=[ 0.05677092  1.35150891 -0.1274598  -2.08840126]\n",
      "[ Experience replay ] starts\n",
      "[ episode 313 ][ timestamp 8 ] state=[ 0.05677092  1.35150891 -0.1274598  -2.08840126], action=1, reward=1.0, next_state=[ 0.0838011   1.54766629 -0.16922783 -2.41762228]\n",
      "[ Experience replay ] starts\n",
      "[ episode 313 ][ timestamp 9 ] state=[ 0.0838011   1.54766629 -0.16922783 -2.41762228], action=1, reward=-1.0, next_state=[ 0.11475443  1.74380166 -0.21758028 -2.75713853]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 313: Exploration_rate=0.01. Score=9.\n",
      "[ episode 314 ] state=[ 0.00424191 -0.00838576 -0.03558675 -0.00115524]\n",
      "[ episode 314 ][ timestamp 1 ] state=[ 0.00424191 -0.00838576 -0.03558675 -0.00115524], action=1, reward=1.0, next_state=[ 0.0040742   0.18722801 -0.03560985 -0.30485041]\n",
      "[ Experience replay ] starts\n",
      "[ episode 314 ][ timestamp 2 ] state=[ 0.0040742   0.18722801 -0.03560985 -0.30485041], action=1, reward=1.0, next_state=[ 0.00781876  0.38283886 -0.04170686 -0.60854775]\n",
      "[ Experience replay ] starts\n",
      "[ episode 314 ][ timestamp 3 ] state=[ 0.00781876  0.38283886 -0.04170686 -0.60854775], action=1, reward=1.0, next_state=[ 0.01547554  0.5785183  -0.05387782 -0.91406993]\n",
      "[ Experience replay ] starts\n",
      "[ episode 314 ][ timestamp 4 ] state=[ 0.01547554  0.5785183  -0.05387782 -0.91406993], action=1, reward=1.0, next_state=[ 0.0270459   0.77432599 -0.07215921 -1.2231877 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 314 ][ timestamp 5 ] state=[ 0.0270459   0.77432599 -0.07215921 -1.2231877 ], action=1, reward=1.0, next_state=[ 0.04253242  0.97029945 -0.09662297 -1.5375793 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 314 ][ timestamp 6 ] state=[ 0.04253242  0.97029945 -0.09662297 -1.5375793 ], action=1, reward=1.0, next_state=[ 0.06193841  1.16644242 -0.12737455 -1.8587844 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 314 ][ timestamp 7 ] state=[ 0.06193841  1.16644242 -0.12737455 -1.8587844 ], action=1, reward=1.0, next_state=[ 0.08526726  1.36271148 -0.16455024 -2.18814991]\n",
      "[ Experience replay ] starts\n",
      "[ episode 314 ][ timestamp 8 ] state=[ 0.08526726  1.36271148 -0.16455024 -2.18814991], action=1, reward=1.0, next_state=[ 0.11252149  1.55900037 -0.20831324 -2.52676583]\n",
      "[ Experience replay ] starts\n",
      "[ episode 314 ][ timestamp 9 ] state=[ 0.11252149  1.55900037 -0.20831324 -2.52676583], action=1, reward=-1.0, next_state=[ 0.1437015   1.7551222  -0.25884856 -2.8753908 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 314: Exploration_rate=0.01. Score=9.\n",
      "[ episode 315 ] state=[-0.02220633 -0.01104927 -0.02139373 -0.0118338 ]\n",
      "[ episode 315 ][ timestamp 1 ] state=[-0.02220633 -0.01104927 -0.02139373 -0.0118338 ], action=1, reward=1.0, next_state=[-0.02242732  0.18437286 -0.02163041 -0.31118919]\n",
      "[ Experience replay ] starts\n",
      "[ episode 315 ][ timestamp 2 ] state=[-0.02242732  0.18437286 -0.02163041 -0.31118919], action=1, reward=1.0, next_state=[-0.01873986  0.37979619 -0.02785419 -0.61061446]\n",
      "[ Experience replay ] starts\n",
      "[ episode 315 ][ timestamp 3 ] state=[-0.01873986  0.37979619 -0.02785419 -0.61061446], action=1, reward=1.0, next_state=[-0.01114394  0.57529618 -0.04006648 -0.91193877]\n",
      "[ Experience replay ] starts\n",
      "[ episode 315 ][ timestamp 4 ] state=[-0.01114394  0.57529618 -0.04006648 -0.91193877], action=1, reward=1.0, next_state=[ 3.61987310e-04  7.70936661e-01 -5.83052570e-02 -1.21694037e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 315 ][ timestamp 5 ] state=[ 3.61987310e-04  7.70936661e-01 -5.83052570e-02 -1.21694037e+00], action=1, reward=1.0, next_state=[ 0.01578072  0.96676005 -0.08264406 -1.52730835]\n",
      "[ Experience replay ] starts\n",
      "[ episode 315 ][ timestamp 6 ] state=[ 0.01578072  0.96676005 -0.08264406 -1.52730835], action=1, reward=1.0, next_state=[ 0.03511592  1.16277625 -0.11319023 -1.84459882]\n",
      "[ Experience replay ] starts\n",
      "[ episode 315 ][ timestamp 7 ] state=[ 0.03511592  1.16277625 -0.11319023 -1.84459882], action=1, reward=1.0, next_state=[ 0.05837145  1.35894962 -0.15008221 -2.17018278]\n",
      "[ Experience replay ] starts\n",
      "[ episode 315 ][ timestamp 8 ] state=[ 0.05837145  1.35894962 -0.15008221 -2.17018278], action=1, reward=1.0, next_state=[ 0.08555044  1.55518378 -0.19348586 -2.50518386]\n",
      "[ Experience replay ] starts\n",
      "[ episode 315 ][ timestamp 9 ] state=[ 0.08555044  1.55518378 -0.19348586 -2.50518386], action=1, reward=-1.0, next_state=[ 0.11665411  1.75130399 -0.24358954 -2.85040533]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 315: Exploration_rate=0.01. Score=9.\n",
      "[ episode 316 ] state=[ 0.00389902 -0.00023103  0.00446962 -0.0037901 ]\n",
      "[ episode 316 ][ timestamp 1 ] state=[ 0.00389902 -0.00023103  0.00446962 -0.0037901 ], action=1, reward=1.0, next_state=[ 0.0038944   0.19482654  0.00439382 -0.29505946]\n",
      "[ Experience replay ] starts\n",
      "[ episode 316 ][ timestamp 2 ] state=[ 0.0038944   0.19482654  0.00439382 -0.29505946], action=1, reward=1.0, next_state=[ 0.00779093  0.38988558 -0.00150737 -0.58635341]\n",
      "[ Experience replay ] starts\n",
      "[ episode 316 ][ timestamp 3 ] state=[ 0.00779093  0.38988558 -0.00150737 -0.58635341], action=1, reward=1.0, next_state=[ 0.01558864  0.58502861 -0.01323444 -0.87951079]\n",
      "[ Experience replay ] starts\n",
      "[ episode 316 ][ timestamp 4 ] state=[ 0.01558864  0.58502861 -0.01323444 -0.87951079], action=1, reward=1.0, next_state=[ 0.02728921  0.78032785 -0.03082465 -1.17632481]\n",
      "[ Experience replay ] starts\n",
      "[ episode 316 ][ timestamp 5 ] state=[ 0.02728921  0.78032785 -0.03082465 -1.17632481], action=1, reward=1.0, next_state=[ 0.04289577  0.9758364  -0.05435115 -1.47850933]\n",
      "[ Experience replay ] starts\n",
      "[ episode 316 ][ timestamp 6 ] state=[ 0.04289577  0.9758364  -0.05435115 -1.47850933], action=1, reward=1.0, next_state=[ 0.0624125   1.17157816 -0.08392134 -1.78765979]\n",
      "[ Experience replay ] starts\n",
      "[ episode 316 ][ timestamp 7 ] state=[ 0.0624125   1.17157816 -0.08392134 -1.78765979], action=1, reward=1.0, next_state=[ 0.08584406  1.36753594 -0.11967453 -2.10520591]\n",
      "[ Experience replay ] starts\n",
      "[ episode 316 ][ timestamp 8 ] state=[ 0.08584406  1.36753594 -0.11967453 -2.10520591], action=1, reward=1.0, next_state=[ 0.11319478  1.56363712 -0.16177865 -2.43235416]\n",
      "[ Experience replay ] starts\n",
      "[ episode 316 ][ timestamp 9 ] state=[ 0.11319478  1.56363712 -0.16177865 -2.43235416], action=1, reward=-1.0, next_state=[ 0.14446752  1.75973694 -0.21042573 -2.77001871]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 316: Exploration_rate=0.01. Score=9.\n",
      "[ episode 317 ] state=[-0.04418393 -0.00964337  0.0233631   0.01293565]\n",
      "[ episode 317 ][ timestamp 1 ] state=[-0.04418393 -0.00964337  0.0233631   0.01293565], action=1, reward=1.0, next_state=[-0.0443768   0.18513587  0.02362181 -0.27228534]\n",
      "[ Experience replay ] starts\n",
      "[ episode 317 ][ timestamp 2 ] state=[-0.0443768   0.18513587  0.02362181 -0.27228534], action=1, reward=1.0, next_state=[-0.04067408  0.37991293  0.01817611 -0.55742526]\n",
      "[ Experience replay ] starts\n",
      "[ episode 317 ][ timestamp 3 ] state=[-0.04067408  0.37991293  0.01817611 -0.55742526], action=1, reward=1.0, next_state=[-0.03307583  0.57477507  0.0070276  -0.8443267 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 317 ][ timestamp 4 ] state=[-0.03307583  0.57477507  0.0070276  -0.8443267 ], action=1, reward=1.0, next_state=[-0.02158032  0.76980042 -0.00985893 -1.13479141]\n",
      "[ Experience replay ] starts\n",
      "[ episode 317 ][ timestamp 5 ] state=[-0.02158032  0.76980042 -0.00985893 -1.13479141], action=1, reward=1.0, next_state=[-0.00618432  0.96504998 -0.03255476 -1.43054999]\n",
      "[ Experience replay ] starts\n",
      "[ episode 317 ][ timestamp 6 ] state=[-0.00618432  0.96504998 -0.03255476 -1.43054999], action=1, reward=1.0, next_state=[ 0.01311668  1.16055834 -0.06116576 -1.73322656]\n",
      "[ Experience replay ] starts\n",
      "[ episode 317 ][ timestamp 7 ] state=[ 0.01311668  1.16055834 -0.06116576 -1.73322656], action=1, reward=1.0, next_state=[ 0.03632785  1.35632264 -0.09583029 -2.04429539]\n",
      "[ Experience replay ] starts\n",
      "[ episode 317 ][ timestamp 8 ] state=[ 0.03632785  1.35632264 -0.09583029 -2.04429539], action=1, reward=1.0, next_state=[ 0.0634543   1.55228913 -0.1367162  -2.36502744]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 317 ][ timestamp 9 ] state=[ 0.0634543   1.55228913 -0.1367162  -2.36502744], action=1, reward=1.0, next_state=[ 0.09450009  1.74833726 -0.18401675 -2.69642508]\n",
      "[ Experience replay ] starts\n",
      "[ episode 317 ][ timestamp 10 ] state=[ 0.09450009  1.74833726 -0.18401675 -2.69642508], action=1, reward=-1.0, next_state=[ 0.12946683  1.94426118 -0.23794525 -3.03914527]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 317: Exploration_rate=0.01. Score=10.\n",
      "[ episode 318 ] state=[ 0.03686101 -0.04620754 -0.04448619 -0.04701657]\n",
      "[ episode 318 ][ timestamp 1 ] state=[ 0.03686101 -0.04620754 -0.04448619 -0.04701657], action=1, reward=1.0, next_state=[ 0.03593686  0.14952314 -0.04542652 -0.35339675]\n",
      "[ Experience replay ] starts\n",
      "[ episode 318 ][ timestamp 2 ] state=[ 0.03593686  0.14952314 -0.04542652 -0.35339675], action=1, reward=1.0, next_state=[ 0.03892732  0.34526061 -0.05249446 -0.66005088]\n",
      "[ Experience replay ] starts\n",
      "[ episode 318 ][ timestamp 3 ] state=[ 0.03892732  0.34526061 -0.05249446 -0.66005088], action=1, reward=1.0, next_state=[ 0.04583253  0.54107228 -0.06569547 -0.96879006]\n",
      "[ Experience replay ] starts\n",
      "[ episode 318 ][ timestamp 4 ] state=[ 0.04583253  0.54107228 -0.06569547 -0.96879006], action=1, reward=1.0, next_state=[ 0.05665398  0.73701179 -0.08507128 -1.2813659 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 318 ][ timestamp 5 ] state=[ 0.05665398  0.73701179 -0.08507128 -1.2813659 ], action=1, reward=1.0, next_state=[ 0.07139422  0.93310822 -0.11069859 -1.59942759]\n",
      "[ Experience replay ] starts\n",
      "[ episode 318 ][ timestamp 6 ] state=[ 0.07139422  0.93310822 -0.11069859 -1.59942759], action=1, reward=1.0, next_state=[ 0.09005638  1.12935385 -0.14268715 -1.92447321]\n",
      "[ Experience replay ] starts\n",
      "[ episode 318 ][ timestamp 7 ] state=[ 0.09005638  1.12935385 -0.14268715 -1.92447321], action=1, reward=1.0, next_state=[ 0.11264346  1.32569015 -0.18117661 -2.25779257]\n",
      "[ Experience replay ] starts\n",
      "[ episode 318 ][ timestamp 8 ] state=[ 0.11264346  1.32569015 -0.18117661 -2.25779257], action=1, reward=-1.0, next_state=[ 0.13915726  1.52199148 -0.22633246 -2.60040008]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 318: Exploration_rate=0.01. Score=8.\n",
      "[ episode 319 ] state=[ 0.01752308 -0.00386657 -0.02201242  0.0198224 ]\n",
      "[ episode 319 ][ timestamp 1 ] state=[ 0.01752308 -0.00386657 -0.02201242  0.0198224 ], action=1, reward=1.0, next_state=[ 0.01744575  0.19156403 -0.02161598 -0.27972362]\n",
      "[ Experience replay ] starts\n",
      "[ episode 319 ][ timestamp 2 ] state=[ 0.01744575  0.19156403 -0.02161598 -0.27972362], action=1, reward=1.0, next_state=[ 0.02127703  0.38698756 -0.02721045 -0.57914503]\n",
      "[ Experience replay ] starts\n",
      "[ episode 319 ][ timestamp 3 ] state=[ 0.02127703  0.38698756 -0.02721045 -0.57914503], action=1, reward=1.0, next_state=[ 0.02901678  0.58248007 -0.03879335 -0.88027412]\n",
      "[ Experience replay ] starts\n",
      "[ episode 319 ][ timestamp 4 ] state=[ 0.02901678  0.58248007 -0.03879335 -0.88027412], action=1, reward=1.0, next_state=[ 0.04066638  0.77810696 -0.05639883 -1.18489607]\n",
      "[ Experience replay ] starts\n",
      "[ episode 319 ][ timestamp 5 ] state=[ 0.04066638  0.77810696 -0.05639883 -1.18489607], action=1, reward=1.0, next_state=[ 0.05622852  0.9739133  -0.08009675 -1.49471106]\n",
      "[ Experience replay ] starts\n",
      "[ episode 319 ][ timestamp 6 ] state=[ 0.05622852  0.9739133  -0.08009675 -1.49471106], action=1, reward=1.0, next_state=[ 0.07570678  1.16991285 -0.10999097 -1.81129108]\n",
      "[ Experience replay ] starts\n",
      "[ episode 319 ][ timestamp 7 ] state=[ 0.07570678  1.16991285 -0.10999097 -1.81129108], action=1, reward=1.0, next_state=[ 0.09910504  1.36607524 -0.1462168  -2.13602877]\n",
      "[ Experience replay ] starts\n",
      "[ episode 319 ][ timestamp 8 ] state=[ 0.09910504  1.36607524 -0.1462168  -2.13602877], action=1, reward=1.0, next_state=[ 0.12642655  1.56231106 -0.18893737 -2.47007627]\n",
      "[ Experience replay ] starts\n",
      "[ episode 319 ][ timestamp 9 ] state=[ 0.12642655  1.56231106 -0.18893737 -2.47007627], action=1, reward=-1.0, next_state=[ 0.15767277  1.7584544  -0.2383389  -2.81427323]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 319: Exploration_rate=0.01. Score=9.\n",
      "[ episode 320 ] state=[-0.01602912 -0.04912929  0.03934842  0.00280794]\n",
      "[ episode 320 ][ timestamp 1 ] state=[-0.01602912 -0.04912929  0.03934842  0.00280794], action=1, reward=1.0, next_state=[-0.0170117   0.1454069   0.03940458 -0.27720503]\n",
      "[ Experience replay ] starts\n",
      "[ episode 320 ][ timestamp 2 ] state=[-0.0170117   0.1454069   0.03940458 -0.27720503], action=1, reward=1.0, next_state=[-0.01410356  0.33994518  0.03386047 -0.55720398]\n",
      "[ Experience replay ] starts\n",
      "[ episode 320 ][ timestamp 3 ] state=[-0.01410356  0.33994518  0.03386047 -0.55720398], action=1, reward=1.0, next_state=[-0.00730466  0.53457583  0.0227164  -0.83902953]\n",
      "[ Experience replay ] starts\n",
      "[ episode 320 ][ timestamp 4 ] state=[-0.00730466  0.53457583  0.0227164  -0.83902953], action=1, reward=1.0, next_state=[ 0.00338686  0.72938035  0.0059358  -1.12448287]\n",
      "[ Experience replay ] starts\n",
      "[ episode 320 ][ timestamp 5 ] state=[ 0.00338686  0.72938035  0.0059358  -1.12448287], action=1, reward=1.0, next_state=[ 0.01797446  0.924424   -0.01655385 -1.41529807]\n",
      "[ Experience replay ] starts\n",
      "[ episode 320 ][ timestamp 6 ] state=[ 0.01797446  0.924424   -0.01655385 -1.41529807], action=1, reward=1.0, next_state=[ 0.03646294  1.11974705 -0.04485981 -1.71310911]\n",
      "[ Experience replay ] starts\n",
      "[ episode 320 ][ timestamp 7 ] state=[ 0.03646294  1.11974705 -0.04485981 -1.71310911], action=1, reward=1.0, next_state=[ 0.05885788  1.31535431 -0.079122   -2.01940919]\n",
      "[ Experience replay ] starts\n",
      "[ episode 320 ][ timestamp 8 ] state=[ 0.05885788  1.31535431 -0.079122   -2.01940919], action=1, reward=1.0, next_state=[ 0.08516497  1.51120227 -0.11951018 -2.33549966]\n",
      "[ Experience replay ] starts\n",
      "[ episode 320 ][ timestamp 9 ] state=[ 0.08516497  1.51120227 -0.11951018 -2.33549966], action=1, reward=1.0, next_state=[ 0.11538902  1.70718359 -0.16622017 -2.6624272 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 320 ][ timestamp 10 ] state=[ 0.11538902  1.70718359 -0.16622017 -2.6624272 ], action=1, reward=-1.0, next_state=[ 0.14953269  1.90310903 -0.21946872 -3.00090877]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 320: Exploration_rate=0.01. Score=10.\n",
      "[ episode 321 ] state=[-0.03937171  0.02073997  0.0167814   0.00615652]\n",
      "[ episode 321 ][ timestamp 1 ] state=[-0.03937171  0.02073997  0.0167814   0.00615652], action=1, reward=1.0, next_state=[-0.03895691  0.21561728  0.01690453 -0.28118479]\n",
      "[ Experience replay ] starts\n",
      "[ episode 321 ][ timestamp 2 ] state=[-0.03895691  0.21561728  0.01690453 -0.28118479], action=1, reward=1.0, next_state=[-0.03464456  0.41049407  0.01128083 -0.56848851]\n",
      "[ Experience replay ] starts\n",
      "[ episode 321 ][ timestamp 3 ] state=[-0.03464456  0.41049407  0.01128083 -0.56848851], action=1, reward=1.0, next_state=[-2.64346797e-02  6.05455992e-01 -8.89378875e-05 -8.57596295e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 321 ][ timestamp 4 ] state=[-2.64346797e-02  6.05455992e-01 -8.89378875e-05 -8.57596295e-01], action=0, reward=1.0, next_state=[-0.01432556  0.41033525 -0.01724086 -0.56494133]\n",
      "[ Experience replay ] starts\n",
      "[ episode 321 ][ timestamp 5 ] state=[-0.01432556  0.41033525 -0.01724086 -0.56494133], action=0, reward=1.0, next_state=[-0.00611885  0.21545938 -0.02853969 -0.27773953]\n",
      "[ Experience replay ] starts\n",
      "[ episode 321 ][ timestamp 6 ] state=[-0.00611885  0.21545938 -0.02853969 -0.27773953], action=0, reward=1.0, next_state=[-0.00180967  0.02075596 -0.03409448  0.00580713]\n",
      "[ Experience replay ] starts\n",
      "[ episode 321 ][ timestamp 7 ] state=[-0.00180967  0.02075596 -0.03409448  0.00580713], action=0, reward=1.0, next_state=[-0.00139455 -0.17386086 -0.03397834  0.28754087]\n",
      "[ Experience replay ] starts\n",
      "[ episode 321 ][ timestamp 8 ] state=[-0.00139455 -0.17386086 -0.03397834  0.28754087], action=0, reward=1.0, next_state=[-0.00487177 -0.36848219 -0.02822752  0.56931665]\n",
      "[ Experience replay ] starts\n",
      "[ episode 321 ][ timestamp 9 ] state=[-0.00487177 -0.36848219 -0.02822752  0.56931665], action=0, reward=1.0, next_state=[-0.01224141 -0.56319711 -0.01684119  0.85297489]\n",
      "[ Experience replay ] starts\n",
      "[ episode 321 ][ timestamp 10 ] state=[-0.01224141 -0.56319711 -0.01684119  0.85297489], action=0, reward=1.0, next_state=[-2.35053510e-02 -7.58085486e-01  2.18309643e-04  1.14031493e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 321 ][ timestamp 11 ] state=[-2.35053510e-02 -7.58085486e-01  2.18309643e-04  1.14031493e+00], action=0, reward=1.0, next_state=[-0.03866706 -0.95321029  0.02302461  1.43306631]\n",
      "[ Experience replay ] starts\n",
      "[ episode 321 ][ timestamp 12 ] state=[-0.03866706 -0.95321029  0.02302461  1.43306631], action=0, reward=1.0, next_state=[-0.05773127 -1.14860863  0.05168593  1.73285476]\n",
      "[ Experience replay ] starts\n",
      "[ episode 321 ][ timestamp 13 ] state=[-0.05773127 -1.14860863  0.05168593  1.73285476], action=1, reward=1.0, next_state=[-0.08070344 -0.95411324  0.08634303  1.45669019]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 321 ][ timestamp 14 ] state=[-0.08070344 -0.95411324  0.08634303  1.45669019], action=1, reward=1.0, next_state=[-0.0997857  -0.76015045  0.11547683  1.19218316]\n",
      "[ Experience replay ] starts\n",
      "[ episode 321 ][ timestamp 15 ] state=[-0.0997857  -0.76015045  0.11547683  1.19218316], action=1, reward=1.0, next_state=[-0.11498871 -0.56669807  0.1393205   0.93781198]\n",
      "[ Experience replay ] starts\n",
      "[ episode 321 ][ timestamp 16 ] state=[-0.11498871 -0.56669807  0.1393205   0.93781198], action=1, reward=1.0, next_state=[-0.12632267 -0.37370161  0.15807674  0.69195016]\n",
      "[ Experience replay ] starts\n",
      "[ episode 321 ][ timestamp 17 ] state=[-0.12632267 -0.37370161  0.15807674  0.69195016], action=0, reward=1.0, next_state=[-0.13379671 -0.57062243  0.17191574  1.0299298 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 321 ][ timestamp 18 ] state=[-0.13379671 -0.57062243  0.17191574  1.0299298 ], action=1, reward=1.0, next_state=[-0.14520916 -0.37815282  0.19251434  0.79577584]\n",
      "[ Experience replay ] starts\n",
      "[ episode 321 ][ timestamp 19 ] state=[-0.14520916 -0.37815282  0.19251434  0.79577584], action=0, reward=1.0, next_state=[-0.15277221 -0.57532167  0.20842985  1.1423157 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 321 ][ timestamp 20 ] state=[-0.15277221 -0.57532167  0.20842985  1.1423157 ], action=0, reward=-1.0, next_state=[-0.16427865 -0.77246591  0.23127617  1.49246753]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 321: Exploration_rate=0.01. Score=20.\n",
      "[ episode 322 ] state=[ 0.03135268 -0.00976926 -0.03672819  0.02619545]\n",
      "[ episode 322 ][ timestamp 1 ] state=[ 0.03135268 -0.00976926 -0.03672819  0.02619545], action=0, reward=1.0, next_state=[ 0.03115729 -0.20434578 -0.03620429  0.30706773]\n",
      "[ Experience replay ] starts\n",
      "[ episode 322 ][ timestamp 2 ] state=[ 0.03115729 -0.20434578 -0.03620429  0.30706773], action=0, reward=1.0, next_state=[ 0.02707038 -0.39893363 -0.03006293  0.58811651]\n",
      "[ Experience replay ] starts\n",
      "[ episode 322 ][ timestamp 3 ] state=[ 0.02707038 -0.39893363 -0.03006293  0.58811651], action=0, reward=1.0, next_state=[ 0.01909171 -0.59362197 -0.0183006   0.87117989]\n",
      "[ Experience replay ] starts\n",
      "[ episode 322 ][ timestamp 4 ] state=[ 0.01909171 -0.59362197 -0.0183006   0.87117989], action=0, reward=1.0, next_state=[ 7.21926630e-03 -7.88490298e-01 -8.77003116e-04  1.15805336e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 322 ][ timestamp 5 ] state=[ 7.21926630e-03 -7.88490298e-01 -8.77003116e-04  1.15805336e+00], action=0, reward=1.0, next_state=[-0.00855054 -0.98360081  0.02228406  1.45046117]\n",
      "[ Experience replay ] starts\n",
      "[ episode 322 ][ timestamp 6 ] state=[-0.00855054 -0.98360081  0.02228406  1.45046117], action=0, reward=1.0, next_state=[-0.02822256 -1.17898941  0.05129329  1.75002228]\n",
      "[ Experience replay ] starts\n",
      "[ episode 322 ][ timestamp 7 ] state=[-0.02822256 -1.17898941  0.05129329  1.75002228], action=1, reward=1.0, next_state=[-0.05180234 -0.98448601  0.08629373  1.47372451]\n",
      "[ Experience replay ] starts\n",
      "[ episode 322 ][ timestamp 8 ] state=[-0.05180234 -0.98448601  0.08629373  1.47372451], action=1, reward=1.0, next_state=[-0.07149206 -0.79051831  0.11576822  1.20919447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 322 ][ timestamp 9 ] state=[-0.07149206 -0.79051831  0.11576822  1.20919447], action=1, reward=1.0, next_state=[-0.08730243 -0.59706594  0.13995211  0.95491815]\n",
      "[ Experience replay ] starts\n",
      "[ episode 322 ][ timestamp 10 ] state=[-0.08730243 -0.59706594  0.13995211  0.95491815], action=1, reward=1.0, next_state=[-0.09924375 -0.40407568  0.15905048  0.7092749 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 322 ][ timestamp 11 ] state=[-0.09924375 -0.40407568  0.15905048  0.7092749 ], action=1, reward=1.0, next_state=[-0.10732526 -0.21147225  0.17323597  0.47058022]\n",
      "[ Experience replay ] starts\n",
      "[ episode 322 ][ timestamp 12 ] state=[-0.10732526 -0.21147225  0.17323597  0.47058022], action=1, reward=1.0, next_state=[-0.11155471 -0.01916619  0.18264758  0.23711575]\n",
      "[ Experience replay ] starts\n",
      "[ episode 322 ][ timestamp 13 ] state=[-0.11155471 -0.01916619  0.18264758  0.23711575], action=0, reward=1.0, next_state=[-0.11193803 -0.21636372  0.18738989  0.58139216]\n",
      "[ Experience replay ] starts\n",
      "[ episode 322 ][ timestamp 14 ] state=[-0.11193803 -0.21636372  0.18738989  0.58139216], action=1, reward=1.0, next_state=[-0.11626531 -0.02429306  0.19901774  0.35310057]\n",
      "[ Experience replay ] starts\n",
      "[ episode 322 ][ timestamp 15 ] state=[-0.11626531 -0.02429306  0.19901774  0.35310057], action=0, reward=1.0, next_state=[-0.11675117 -0.22160606  0.20607975  0.70135373]\n",
      "[ Experience replay ] starts\n",
      "[ episode 322 ][ timestamp 16 ] state=[-0.11675117 -0.22160606  0.20607975  0.70135373], action=0, reward=-1.0, next_state=[-0.12118329 -0.41889788  0.22010682  1.05118911]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 322: Exploration_rate=0.01. Score=16.\n",
      "[ episode 323 ] state=[-0.01506724 -0.02456203 -0.00174991 -0.01151574]\n",
      "[ episode 323 ][ timestamp 1 ] state=[-0.01506724 -0.02456203 -0.00174991 -0.01151574], action=0, reward=1.0, next_state=[-0.01555848 -0.21965884 -0.00198022  0.28061456]\n",
      "[ Experience replay ] starts\n",
      "[ episode 323 ][ timestamp 2 ] state=[-0.01555848 -0.21965884 -0.00198022  0.28061456], action=0, reward=1.0, next_state=[-0.01995166 -0.41475249  0.00363207  0.57267227]\n",
      "[ Experience replay ] starts\n",
      "[ episode 323 ][ timestamp 3 ] state=[-0.01995166 -0.41475249  0.00363207  0.57267227], action=0, reward=1.0, next_state=[-0.02824671 -0.60992518  0.01508551  0.8664972 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 323 ][ timestamp 4 ] state=[-0.02824671 -0.60992518  0.01508551  0.8664972 ], action=0, reward=1.0, next_state=[-0.04044521 -0.80524914  0.03241546  1.16388478]\n",
      "[ Experience replay ] starts\n",
      "[ episode 323 ][ timestamp 5 ] state=[-0.04044521 -0.80524914  0.03241546  1.16388478], action=1, reward=1.0, next_state=[-0.0565502  -0.61056388  0.05569315  0.88153878]\n",
      "[ Experience replay ] starts\n",
      "[ episode 323 ][ timestamp 6 ] state=[-0.0565502  -0.61056388  0.05569315  0.88153878], action=0, reward=1.0, next_state=[-0.06876147 -0.8063963   0.07332393  1.19119729]\n",
      "[ Experience replay ] starts\n",
      "[ episode 323 ][ timestamp 7 ] state=[-0.06876147 -0.8063963   0.07332393  1.19119729], action=1, reward=1.0, next_state=[-0.0848894  -0.61229697  0.09714787  0.92236852]\n",
      "[ Experience replay ] starts\n",
      "[ episode 323 ][ timestamp 8 ] state=[-0.0848894  -0.61229697  0.09714787  0.92236852], action=0, reward=1.0, next_state=[-0.09713534 -0.80858778  0.11559525  1.243933  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 323 ][ timestamp 9 ] state=[-0.09713534 -0.80858778  0.11559525  1.243933  ], action=0, reward=1.0, next_state=[-0.1133071  -1.00498752  0.14047391  1.57047591]\n",
      "[ Experience replay ] starts\n",
      "[ episode 323 ][ timestamp 10 ] state=[-0.1133071  -1.00498752  0.14047391  1.57047591], action=1, reward=1.0, next_state=[-0.13340685 -0.81179398  0.17188342  1.32470375]\n",
      "[ Experience replay ] starts\n",
      "[ episode 323 ][ timestamp 11 ] state=[-0.13340685 -0.81179398  0.17188342  1.32470375], action=1, reward=1.0, next_state=[-0.14964273 -0.61920824  0.1983775   1.09036721]\n",
      "[ Experience replay ] starts\n",
      "[ episode 323 ][ timestamp 12 ] state=[-0.14964273 -0.61920824  0.1983775   1.09036721], action=1, reward=-1.0, next_state=[-0.16202689 -0.42717441  0.22018484  0.86590703]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 323: Exploration_rate=0.01. Score=12.\n",
      "[ episode 324 ] state=[-0.0292733   0.02375754  0.0196457   0.00230873]\n",
      "[ episode 324 ][ timestamp 1 ] state=[-0.0292733   0.02375754  0.0196457   0.00230873], action=1, reward=1.0, next_state=[-0.02879815  0.21859231  0.01969187 -0.28411157]\n",
      "[ Experience replay ] starts\n",
      "[ episode 324 ][ timestamp 2 ] state=[-0.02879815  0.21859231  0.01969187 -0.28411157], action=0, reward=1.0, next_state=[-0.0244263   0.02319512  0.01400964  0.01471643]\n",
      "[ Experience replay ] starts\n",
      "[ episode 324 ][ timestamp 3 ] state=[-0.0244263   0.02319512  0.01400964  0.01471643], action=0, reward=1.0, next_state=[-0.0239624  -0.17212492  0.01430397  0.31178644]\n",
      "[ Experience replay ] starts\n",
      "[ episode 324 ][ timestamp 4 ] state=[-0.0239624  -0.17212492  0.01430397  0.31178644], action=0, reward=1.0, next_state=[-0.0274049  -0.3674477   0.0205397   0.60894586]\n",
      "[ Experience replay ] starts\n",
      "[ episode 324 ][ timestamp 5 ] state=[-0.0274049  -0.3674477   0.0205397   0.60894586], action=0, reward=1.0, next_state=[-0.03475385 -0.56285068  0.03271861  0.90802675]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 324 ][ timestamp 6 ] state=[-0.03475385 -0.56285068  0.03271861  0.90802675], action=0, reward=1.0, next_state=[-0.04601087 -0.7583999   0.05087915  1.21081115]\n",
      "[ Experience replay ] starts\n",
      "[ episode 324 ][ timestamp 7 ] state=[-0.04601087 -0.7583999   0.05087915  1.21081115], action=0, reward=1.0, next_state=[-0.06117887 -0.95414049  0.07509537  1.5189941 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 324 ][ timestamp 8 ] state=[-0.06117887 -0.95414049  0.07509537  1.5189941 ], action=1, reward=1.0, next_state=[-0.08026167 -0.76000254  0.10547525  1.25066519]\n",
      "[ Experience replay ] starts\n",
      "[ episode 324 ][ timestamp 9 ] state=[-0.08026167 -0.76000254  0.10547525  1.25066519], action=1, reward=1.0, next_state=[-0.09546173 -0.56637844  0.13048856  0.99279535]\n",
      "[ Experience replay ] starts\n",
      "[ episode 324 ][ timestamp 10 ] state=[-0.09546173 -0.56637844  0.13048856  0.99279535], action=1, reward=1.0, next_state=[-0.10678929 -0.37322089  0.15034446  0.74377709]\n",
      "[ Experience replay ] starts\n",
      "[ episode 324 ][ timestamp 11 ] state=[-0.10678929 -0.37322089  0.15034446  0.74377709], action=0, reward=1.0, next_state=[-0.11425371 -0.5700627   0.16522001  1.07974406]\n",
      "[ Experience replay ] starts\n",
      "[ episode 324 ][ timestamp 12 ] state=[-0.11425371 -0.5700627   0.16522001  1.07974406], action=1, reward=1.0, next_state=[-0.12565497 -0.3774615   0.18681489  0.84313045]\n",
      "[ Experience replay ] starts\n",
      "[ episode 324 ][ timestamp 13 ] state=[-0.12565497 -0.3774615   0.18681489  0.84313045], action=1, reward=1.0, next_state=[-0.1332042  -0.18531304  0.2036775   0.61452726]\n",
      "[ Experience replay ] starts\n",
      "[ episode 324 ][ timestamp 14 ] state=[-0.1332042  -0.18531304  0.2036775   0.61452726], action=1, reward=-1.0, next_state=[-0.13691046  0.00646839  0.21596804  0.39226952]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 324: Exploration_rate=0.01. Score=14.\n",
      "[ episode 325 ] state=[-0.00673737  0.01092694  0.01341354 -0.02547977]\n",
      "[ episode 325 ][ timestamp 1 ] state=[-0.00673737  0.01092694  0.01341354 -0.02547977], action=0, reward=1.0, next_state=[-0.00651883 -0.18438478  0.01290394  0.27140492]\n",
      "[ Experience replay ] starts\n",
      "[ episode 325 ][ timestamp 2 ] state=[-0.00651883 -0.18438478  0.01290394  0.27140492], action=0, reward=1.0, next_state=[-0.01020653 -0.37968846  0.01833204  0.56812971]\n",
      "[ Experience replay ] starts\n",
      "[ episode 325 ][ timestamp 3 ] state=[-0.01020653 -0.37968846  0.01833204  0.56812971], action=0, reward=1.0, next_state=[-0.0178003  -0.57506269  0.02969464  0.86653112]\n",
      "[ Experience replay ] starts\n",
      "[ episode 325 ][ timestamp 4 ] state=[-0.0178003  -0.57506269  0.02969464  0.86653112], action=0, reward=1.0, next_state=[-0.02930155 -0.77057589  0.04702526  1.16840058]\n",
      "[ Experience replay ] starts\n",
      "[ episode 325 ][ timestamp 5 ] state=[-0.02930155 -0.77057589  0.04702526  1.16840058], action=0, reward=1.0, next_state=[-0.04471307 -0.96627701  0.07039327  1.47544807]\n",
      "[ Experience replay ] starts\n",
      "[ episode 325 ][ timestamp 6 ] state=[-0.04471307 -0.96627701  0.07039327  1.47544807], action=1, reward=1.0, next_state=[-0.06403861 -0.77208218  0.09990223  1.20555577]\n",
      "[ Experience replay ] starts\n",
      "[ episode 325 ][ timestamp 7 ] state=[-0.06403861 -0.77208218  0.09990223  1.20555577], action=0, reward=1.0, next_state=[-0.07948025 -0.9683431   0.12401335  1.52780171]\n",
      "[ Experience replay ] starts\n",
      "[ episode 325 ][ timestamp 8 ] state=[-0.07948025 -0.9683431   0.12401335  1.52780171], action=1, reward=1.0, next_state=[-0.09884711 -0.77491637  0.15456938  1.27625638]\n",
      "[ Experience replay ] starts\n",
      "[ episode 325 ][ timestamp 9 ] state=[-0.09884711 -0.77491637  0.15456938  1.27625638], action=1, reward=1.0, next_state=[-0.11434544 -0.58206584  0.18009451  1.03569201]\n",
      "[ Experience replay ] starts\n",
      "[ episode 325 ][ timestamp 10 ] state=[-0.11434544 -0.58206584  0.18009451  1.03569201], action=1, reward=1.0, next_state=[-0.12598676 -0.3897353   0.20080835  0.80452415]\n",
      "[ Experience replay ] starts\n",
      "[ episode 325 ][ timestamp 11 ] state=[-0.12598676 -0.3897353   0.20080835  0.80452415], action=1, reward=-1.0, next_state=[-0.13378146 -0.19784908  0.21689883  0.58112024]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 325: Exploration_rate=0.01. Score=11.\n",
      "[ episode 326 ] state=[-0.04753123 -0.04799182  0.02721661  0.00690436]\n",
      "[ episode 326 ][ timestamp 1 ] state=[-0.04753123 -0.04799182  0.02721661  0.00690436], action=1, reward=1.0, next_state=[-0.04849107  0.14672945  0.0273547  -0.27706867]\n",
      "[ Experience replay ] starts\n",
      "[ episode 326 ][ timestamp 2 ] state=[-0.04849107  0.14672945  0.0273547  -0.27706867], action=1, reward=1.0, next_state=[-0.04555648  0.34145068  0.02181332 -0.56099997]\n",
      "[ Experience replay ] starts\n",
      "[ episode 326 ][ timestamp 3 ] state=[-0.04555648  0.34145068  0.02181332 -0.56099997], action=1, reward=1.0, next_state=[-0.03872747  0.53625981  0.01059333 -0.84673154]\n",
      "[ Experience replay ] starts\n",
      "[ episode 326 ][ timestamp 4 ] state=[-0.03872747  0.53625981  0.01059333 -0.84673154], action=1, reward=1.0, next_state=[-0.02800227  0.73123565 -0.00634131 -1.13606452]\n",
      "[ Experience replay ] starts\n",
      "[ episode 326 ][ timestamp 5 ] state=[-0.02800227  0.73123565 -0.00634131 -1.13606452], action=1, reward=1.0, next_state=[-0.01337756  0.92643999 -0.0290626  -1.43072946]\n",
      "[ Experience replay ] starts\n",
      "[ episode 326 ][ timestamp 6 ] state=[-0.01337756  0.92643999 -0.0290626  -1.43072946], action=1, reward=1.0, next_state=[ 0.00515124  1.1219084  -0.05767719 -1.73235146]\n",
      "[ Experience replay ] starts\n",
      "[ episode 326 ][ timestamp 7 ] state=[ 0.00515124  1.1219084  -0.05767719 -1.73235146], action=1, reward=1.0, next_state=[ 0.02758941  1.31763932 -0.09232421 -2.04240733]\n",
      "[ Experience replay ] starts\n",
      "[ episode 326 ][ timestamp 8 ] state=[ 0.02758941  1.31763932 -0.09232421 -2.04240733], action=0, reward=1.0, next_state=[ 0.0539422   1.12357942 -0.13317236 -1.77966195]\n",
      "[ Experience replay ] starts\n",
      "[ episode 326 ][ timestamp 9 ] state=[ 0.0539422   1.12357942 -0.13317236 -1.77966195], action=0, reward=1.0, next_state=[ 0.07641378  0.93018405 -0.1687656  -1.53117454]\n",
      "[ Experience replay ] starts\n",
      "[ episode 326 ][ timestamp 10 ] state=[ 0.07641378  0.93018405 -0.1687656  -1.53117454], action=0, reward=1.0, next_state=[ 0.09501747  0.73745043 -0.19938909 -1.29556329]\n",
      "[ Experience replay ] starts\n",
      "[ episode 326 ][ timestamp 11 ] state=[ 0.09501747  0.73745043 -0.19938909 -1.29556329], action=1, reward=-1.0, next_state=[ 0.10976647  0.93446681 -0.22530036 -1.6434656 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 326: Exploration_rate=0.01. Score=11.\n",
      "[ episode 327 ] state=[ 0.03010453 -0.02380792  0.0339224  -0.03144814]\n",
      "[ episode 327 ][ timestamp 1 ] state=[ 0.03010453 -0.02380792  0.0339224  -0.03144814], action=1, reward=1.0, next_state=[ 0.02962837  0.17081156  0.03329344 -0.31323813]\n",
      "[ Experience replay ] starts\n",
      "[ episode 327 ][ timestamp 2 ] state=[ 0.02962837  0.17081156  0.03329344 -0.31323813], action=1, reward=1.0, next_state=[ 0.03304461  0.36544379  0.02702868 -0.59523823]\n",
      "[ Experience replay ] starts\n",
      "[ episode 327 ][ timestamp 3 ] state=[ 0.03304461  0.36544379  0.02702868 -0.59523823], action=1, reward=1.0, next_state=[ 0.04035348  0.56017724  0.01512391 -0.87928624]\n",
      "[ Experience replay ] starts\n",
      "[ episode 327 ][ timestamp 4 ] state=[ 0.04035348  0.56017724  0.01512391 -0.87928624], action=1, reward=1.0, next_state=[ 0.05155703  0.75509047 -0.00246181 -1.16717639]\n",
      "[ Experience replay ] starts\n",
      "[ episode 327 ][ timestamp 5 ] state=[ 0.05155703  0.75509047 -0.00246181 -1.16717639], action=1, reward=1.0, next_state=[ 0.06665884  0.95024437 -0.02580534 -1.46063012]\n",
      "[ Experience replay ] starts\n",
      "[ episode 327 ][ timestamp 6 ] state=[ 0.06665884  0.95024437 -0.02580534 -1.46063012], action=1, reward=1.0, next_state=[ 0.08566372  1.14567301 -0.05501794 -1.76126142]\n",
      "[ Experience replay ] starts\n",
      "[ episode 327 ][ timestamp 7 ] state=[ 0.08566372  1.14567301 -0.05501794 -1.76126142], action=1, reward=1.0, next_state=[ 0.10857718  1.34137269 -0.09024317 -2.07053388]\n",
      "[ Experience replay ] starts\n",
      "[ episode 327 ][ timestamp 8 ] state=[ 0.10857718  1.34137269 -0.09024317 -2.07053388], action=1, reward=1.0, next_state=[ 0.13540464  1.53728848 -0.13165385 -2.38970725]\n",
      "[ Experience replay ] starts\n",
      "[ episode 327 ][ timestamp 9 ] state=[ 0.13540464  1.53728848 -0.13165385 -2.38970725], action=1, reward=1.0, next_state=[ 0.16615041  1.73329826 -0.17944799 -2.71977207]\n",
      "[ Experience replay ] starts\n",
      "[ episode 327 ][ timestamp 10 ] state=[ 0.16615041  1.73329826 -0.17944799 -2.71977207], action=1, reward=-1.0, next_state=[ 0.20081637  1.92919414 -0.23384343 -3.06137247]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 327: Exploration_rate=0.01. Score=10.\n",
      "[ episode 328 ] state=[-0.03172456  0.02514922  0.01974657 -0.03318515]\n",
      "[ episode 328 ][ timestamp 1 ] state=[-0.03172456  0.02514922  0.01974657 -0.03318515], action=1, reward=1.0, next_state=[-0.03122158  0.21998252  0.01908287 -0.31957299]\n",
      "[ Experience replay ] starts\n",
      "[ episode 328 ][ timestamp 2 ] state=[-0.03122158  0.21998252  0.01908287 -0.31957299], action=1, reward=1.0, next_state=[-0.02682193  0.41482757  0.01269141 -0.60617734]\n",
      "[ Experience replay ] starts\n",
      "[ episode 328 ][ timestamp 3 ] state=[-0.02682193  0.41482757  0.01269141 -0.60617734], action=0, reward=1.0, next_state=[-0.01852538  0.21953047  0.00056786 -0.30952411]\n",
      "[ Experience replay ] starts\n",
      "[ episode 328 ][ timestamp 4 ] state=[-0.01852538  0.21953047  0.00056786 -0.30952411], action=0, reward=1.0, next_state=[-0.01413477  0.02440044 -0.00562262 -0.01666215]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 328 ][ timestamp 5 ] state=[-0.01413477  0.02440044 -0.00562262 -0.01666215], action=0, reward=1.0, next_state=[-0.01364676 -0.17064043 -0.00595586  0.27424148]\n",
      "[ Experience replay ] starts\n",
      "[ episode 328 ][ timestamp 6 ] state=[-0.01364676 -0.17064043 -0.00595586  0.27424148], action=0, reward=1.0, next_state=[-1.70595672e-02 -3.65676900e-01 -4.71033042e-04  5.65039982e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 328 ][ timestamp 7 ] state=[-1.70595672e-02 -3.65676900e-01 -4.71033042e-04  5.65039982e-01], action=0, reward=1.0, next_state=[-0.02437311 -0.56079224  0.01082977  0.85757448]\n",
      "[ Experience replay ] starts\n",
      "[ episode 328 ][ timestamp 8 ] state=[-0.02437311 -0.56079224  0.01082977  0.85757448], action=0, reward=1.0, next_state=[-0.03558895 -0.75606005  0.02798126  1.1536429 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 328 ][ timestamp 9 ] state=[-0.03558895 -0.75606005  0.02798126  1.1536429 ], action=0, reward=1.0, next_state=[-0.05071015 -0.95153556  0.05105411  1.4549668 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 328 ][ timestamp 10 ] state=[-0.05071015 -0.95153556  0.05105411  1.4549668 ], action=0, reward=1.0, next_state=[-0.06974086 -1.14724574  0.08015345  1.76315296]\n",
      "[ Experience replay ] starts\n",
      "[ episode 328 ][ timestamp 11 ] state=[-0.06974086 -1.14724574  0.08015345  1.76315296], action=0, reward=1.0, next_state=[-0.09268578 -1.34317755  0.11541651  2.07964698]\n",
      "[ Experience replay ] starts\n",
      "[ episode 328 ][ timestamp 12 ] state=[-0.09268578 -1.34317755  0.11541651  2.07964698], action=0, reward=1.0, next_state=[-0.11954933 -1.53926389  0.15700945  2.40567678]\n",
      "[ Experience replay ] starts\n",
      "[ episode 328 ][ timestamp 13 ] state=[-0.11954933 -1.53926389  0.15700945  2.40567678], action=1, reward=1.0, next_state=[-0.15033461 -1.34582007  0.20512298  2.16505166]\n",
      "[ Experience replay ] starts\n",
      "[ episode 328 ][ timestamp 14 ] state=[-0.15033461 -1.34582007  0.20512298  2.16505166], action=1, reward=-1.0, next_state=[-0.17725101 -1.15321129  0.24842402  1.94207941]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 328: Exploration_rate=0.01. Score=14.\n",
      "[ episode 329 ] state=[ 0.04532742 -0.0054446  -0.01980548  0.01699338]\n",
      "[ episode 329 ][ timestamp 1 ] state=[ 0.04532742 -0.0054446  -0.01980548  0.01699338], action=0, reward=1.0, next_state=[ 0.04521853 -0.200277   -0.01946561  0.30336223]\n",
      "[ Experience replay ] starts\n",
      "[ episode 329 ][ timestamp 2 ] state=[ 0.04521853 -0.200277   -0.01946561  0.30336223], action=0, reward=1.0, next_state=[ 0.04121299 -0.3951162  -0.01339837  0.58984314]\n",
      "[ Experience replay ] starts\n",
      "[ episode 329 ][ timestamp 3 ] state=[ 0.04121299 -0.3951162  -0.01339837  0.58984314], action=1, reward=1.0, next_state=[ 0.03331067 -0.19980923 -0.00160151  0.29296998]\n",
      "[ Experience replay ] starts\n",
      "[ episode 329 ][ timestamp 4 ] state=[ 0.03331067 -0.19980923 -0.00160151  0.29296998], action=1, reward=1.0, next_state=[ 0.02931448 -0.00466448  0.00425789 -0.00021761]\n",
      "[ Experience replay ] starts\n",
      "[ episode 329 ][ timestamp 5 ] state=[ 0.02931448 -0.00466448  0.00425789 -0.00021761], action=1, reward=1.0, next_state=[ 0.02922119  0.19039615  0.00425354 -0.29155408]\n",
      "[ Experience replay ] starts\n",
      "[ episode 329 ][ timestamp 6 ] state=[ 0.02922119  0.19039615  0.00425354 -0.29155408], action=0, reward=1.0, next_state=[ 0.03302912 -0.0047862  -0.00157754  0.00246732]\n",
      "[ Experience replay ] starts\n",
      "[ episode 329 ][ timestamp 7 ] state=[ 0.03302912 -0.0047862  -0.00157754  0.00246732], action=0, reward=1.0, next_state=[ 0.03293339 -0.19988549 -0.0015282   0.29465209]\n",
      "[ Experience replay ] starts\n",
      "[ episode 329 ][ timestamp 8 ] state=[ 0.03293339 -0.19988549 -0.0015282   0.29465209], action=1, reward=1.0, next_state=[ 0.02893568 -0.00474178  0.00436485  0.00148759]\n",
      "[ Experience replay ] starts\n",
      "[ episode 329 ][ timestamp 9 ] state=[ 0.02893568 -0.00474178  0.00436485  0.00148759], action=1, reward=1.0, next_state=[ 0.02884085  0.1903173   0.0043946  -0.28981499]\n",
      "[ Experience replay ] starts\n",
      "[ episode 329 ][ timestamp 10 ] state=[ 0.02884085  0.1903173   0.0043946  -0.28981499], action=1, reward=1.0, next_state=[ 0.03264719  0.38537631 -0.0014017  -0.58110867]\n",
      "[ Experience replay ] starts\n",
      "[ episode 329 ][ timestamp 11 ] state=[ 0.03264719  0.38537631 -0.0014017  -0.58110867], action=1, reward=1.0, next_state=[ 0.04035472  0.58051788 -0.01302387 -0.87423283]\n",
      "[ Experience replay ] starts\n",
      "[ episode 329 ][ timestamp 12 ] state=[ 0.04035472  0.58051788 -0.01302387 -0.87423283], action=1, reward=1.0, next_state=[ 0.05196508  0.77581445 -0.03050853 -1.17098176]\n",
      "[ Experience replay ] starts\n",
      "[ episode 329 ][ timestamp 13 ] state=[ 0.05196508  0.77581445 -0.03050853 -1.17098176], action=1, reward=1.0, next_state=[ 0.06748137  0.97131955 -0.05392817 -1.47307106]\n",
      "[ Experience replay ] starts\n",
      "[ episode 329 ][ timestamp 14 ] state=[ 0.06748137  0.97131955 -0.05392817 -1.47307106], action=1, reward=1.0, next_state=[ 0.08690776  1.16705769 -0.08338959 -1.78209863]\n",
      "[ Experience replay ] starts\n",
      "[ episode 329 ][ timestamp 15 ] state=[ 0.08690776  1.16705769 -0.08338959 -1.78209863], action=1, reward=1.0, next_state=[ 0.11024891  1.36301248 -0.11903156 -2.09949757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 329 ][ timestamp 16 ] state=[ 0.11024891  1.36301248 -0.11903156 -2.09949757], action=1, reward=1.0, next_state=[ 0.13750916  1.55911243 -0.16102151 -2.42647881]\n",
      "[ Experience replay ] starts\n",
      "[ episode 329 ][ timestamp 17 ] state=[ 0.13750916  1.55911243 -0.16102151 -2.42647881], action=1, reward=-1.0, next_state=[ 0.16869141  1.75521417 -0.20955109 -2.76396229]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 329: Exploration_rate=0.01. Score=17.\n",
      "[ episode 330 ] state=[-0.01677044  0.03027792 -0.04351233  0.02625351]\n",
      "[ episode 330 ][ timestamp 1 ] state=[-0.01677044  0.03027792 -0.04351233  0.02625351], action=1, reward=1.0, next_state=[-0.01616489  0.22599599 -0.04298726 -0.27983431]\n",
      "[ Experience replay ] starts\n",
      "[ episode 330 ][ timestamp 2 ] state=[-0.01616489  0.22599599 -0.04298726 -0.27983431], action=1, reward=1.0, next_state=[-0.01164497  0.42170395 -0.04858395 -0.58575942]\n",
      "[ Experience replay ] starts\n",
      "[ episode 330 ][ timestamp 3 ] state=[-0.01164497  0.42170395 -0.04858395 -0.58575942], action=1, reward=1.0, next_state=[-0.00321089  0.61747153 -0.06029914 -0.89334235]\n",
      "[ Experience replay ] starts\n",
      "[ episode 330 ][ timestamp 4 ] state=[-0.00321089  0.61747153 -0.06029914 -0.89334235], action=0, reward=1.0, next_state=[ 0.00913854  0.42321696 -0.07816599 -0.62020727]\n",
      "[ Experience replay ] starts\n",
      "[ episode 330 ][ timestamp 5 ] state=[ 0.00913854  0.42321696 -0.07816599 -0.62020727], action=1, reward=1.0, next_state=[ 0.01760288  0.61933856 -0.09057013 -0.93644882]\n",
      "[ Experience replay ] starts\n",
      "[ episode 330 ][ timestamp 6 ] state=[ 0.01760288  0.61933856 -0.09057013 -0.93644882], action=1, reward=1.0, next_state=[ 0.02998965  0.81555749 -0.10929911 -1.2561621 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 330 ][ timestamp 7 ] state=[ 0.02998965  0.81555749 -0.10929911 -1.2561621 ], action=1, reward=1.0, next_state=[ 0.0463008   1.01189563 -0.13442235 -1.58098192]\n",
      "[ Experience replay ] starts\n",
      "[ episode 330 ][ timestamp 8 ] state=[ 0.0463008   1.01189563 -0.13442235 -1.58098192], action=1, reward=1.0, next_state=[ 0.06653872  1.20833729 -0.16604199 -1.91238748]\n",
      "[ Experience replay ] starts\n",
      "[ episode 330 ][ timestamp 9 ] state=[ 0.06653872  1.20833729 -0.16604199 -1.91238748], action=1, reward=1.0, next_state=[ 0.09070546  1.40481454 -0.20428974 -2.25164239]\n",
      "[ Experience replay ] starts\n",
      "[ episode 330 ][ timestamp 10 ] state=[ 0.09070546  1.40481454 -0.20428974 -2.25164239], action=0, reward=-1.0, next_state=[ 0.11880175  1.21211842 -0.24932259 -2.02825307]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 330: Exploration_rate=0.01. Score=10.\n",
      "[ episode 331 ] state=[-0.00241476 -0.03619608  0.01384377  0.04563215]\n",
      "[ episode 331 ][ timestamp 1 ] state=[-0.00241476 -0.03619608  0.01384377  0.04563215], action=0, reward=1.0, next_state=[-0.00313868 -0.23151378  0.01475641  0.34265056]\n",
      "[ Experience replay ] starts\n",
      "[ episode 331 ][ timestamp 2 ] state=[-0.00313868 -0.23151378  0.01475641  0.34265056], action=0, reward=1.0, next_state=[-0.00776895 -0.42684253  0.02160942  0.63995001]\n",
      "[ Experience replay ] starts\n",
      "[ episode 331 ][ timestamp 3 ] state=[-0.00776895 -0.42684253  0.02160942  0.63995001], action=0, reward=1.0, next_state=[-0.01630581 -0.62225899  0.03440842  0.93935893]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 331 ][ timestamp 4 ] state=[-0.01630581 -0.62225899  0.03440842  0.93935893], action=0, reward=1.0, next_state=[-0.02875098 -0.81782746  0.0531956   1.24265209]\n",
      "[ Experience replay ] starts\n",
      "[ episode 331 ][ timestamp 5 ] state=[-0.02875098 -0.81782746  0.0531956   1.24265209], action=0, reward=1.0, next_state=[-0.04510753 -1.01359028  0.07804864  1.55151308]\n",
      "[ Experience replay ] starts\n",
      "[ episode 331 ][ timestamp 6 ] state=[-0.04510753 -1.01359028  0.07804864  1.55151308], action=0, reward=1.0, next_state=[-0.06537934 -1.20955674  0.1090789   1.86749093]\n",
      "[ Experience replay ] starts\n",
      "[ episode 331 ][ timestamp 7 ] state=[-0.06537934 -1.20955674  0.1090789   1.86749093], action=0, reward=1.0, next_state=[-0.08957047 -1.40569019  0.14642872  2.19194824]\n",
      "[ Experience replay ] starts\n",
      "[ episode 331 ][ timestamp 8 ] state=[-0.08957047 -1.40569019  0.14642872  2.19194824], action=1, reward=1.0, next_state=[-0.11768428 -1.21225573  0.19026769  1.947798  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 331 ][ timestamp 9 ] state=[-0.11768428 -1.21225573  0.19026769  1.947798  ], action=1, reward=-1.0, next_state=[-0.14192939 -1.0196013   0.22922365  1.71963321]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 331: Exploration_rate=0.01. Score=9.\n",
      "[ episode 332 ] state=[-0.01650812 -0.03061145 -0.03207702  0.01840194]\n",
      "[ episode 332 ][ timestamp 1 ] state=[-0.01650812 -0.03061145 -0.03207702  0.01840194], action=0, reward=1.0, next_state=[-0.01712035 -0.22525905 -0.03170898  0.30079411]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 2 ] state=[-0.01712035 -0.22525905 -0.03170898  0.30079411], action=0, reward=1.0, next_state=[-0.02162553 -0.41991503 -0.0256931   0.58331044]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 3 ] state=[-0.02162553 -0.41991503 -0.0256931   0.58331044], action=1, reward=1.0, next_state=[-0.03002383 -0.22444273 -0.01402689  0.28264582]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 4 ] state=[-0.03002383 -0.22444273 -0.01402689  0.28264582], action=1, reward=1.0, next_state=[-0.03451268 -0.02912355 -0.00837398 -0.01442791]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 5 ] state=[-0.03451268 -0.02912355 -0.00837398 -0.01442791], action=0, reward=1.0, next_state=[-0.03509515 -0.22412441 -0.00866254  0.27560121]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 6 ] state=[-0.03509515 -0.22412441 -0.00866254  0.27560121], action=0, reward=1.0, next_state=[-0.03957764 -0.41912171 -0.00315051  0.56553943]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 7 ] state=[-0.03957764 -0.41912171 -0.00315051  0.56553943], action=0, reward=1.0, next_state=[-0.04796008 -0.61419932  0.00816028  0.85722814]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 8 ] state=[-0.04796008 -0.61419932  0.00816028  0.85722814], action=1, reward=1.0, next_state=[-0.06024406 -0.41918949  0.02530484  0.56712223]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 9 ] state=[-0.06024406 -0.41918949  0.02530484  0.56712223], action=1, reward=1.0, next_state=[-0.06862785 -0.22443148  0.03664728  0.28251757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 10 ] state=[-0.06862785 -0.22443148  0.03664728  0.28251757], action=1, reward=1.0, next_state=[-0.07311648 -0.02985089  0.04229764  0.00161455]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 11 ] state=[-0.07311648 -0.02985089  0.04229764  0.00161455], action=1, reward=1.0, next_state=[-0.0737135   0.16463973  0.04232993 -0.27742865]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 12 ] state=[-0.0737135   0.16463973  0.04232993 -0.27742865], action=1, reward=1.0, next_state=[-0.0704207   0.35913303  0.03678135 -0.55646597]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 13 ] state=[-0.0704207   0.35913303  0.03678135 -0.55646597], action=1, reward=1.0, next_state=[-0.06323804  0.55371981  0.02565204 -0.83733745]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 14 ] state=[-0.06323804  0.55371981  0.02565204 -0.83733745], action=0, reward=1.0, next_state=[-0.05216365  0.35825709  0.00890529 -0.53669896]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 15 ] state=[-0.05216365  0.35825709  0.00890529 -0.53669896], action=0, reward=1.0, next_state=[-0.04499851  0.16301106 -0.00182869 -0.24122341]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 16 ] state=[-0.04499851  0.16301106 -0.00182869 -0.24122341], action=1, reward=1.0, next_state=[-0.04173828  0.35815909 -0.00665316 -0.5344826 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 17 ] state=[-0.04173828  0.35815909 -0.00665316 -0.5344826 ], action=0, reward=1.0, next_state=[-0.0345751   0.16313133 -0.01734281 -0.24390345]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 18 ] state=[-0.0345751   0.16313133 -0.01734281 -0.24390345], action=0, reward=1.0, next_state=[-0.03131248 -0.03173867 -0.02222088  0.04325906]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 19 ] state=[-0.03131248 -0.03173867 -0.02222088  0.04325906], action=1, reward=1.0, next_state=[-0.03194725  0.16369476 -0.0213557  -0.25635111]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 20 ] state=[-0.03194725  0.16369476 -0.0213557  -0.25635111], action=1, reward=1.0, next_state=[-0.02867335  0.359115   -0.02648272 -0.55569273]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 21 ] state=[-0.02867335  0.359115   -0.02648272 -0.55569273], action=0, reward=1.0, next_state=[-0.02149105  0.16437468 -0.03759658 -0.2714697 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 22 ] state=[-0.02149105  0.16437468 -0.03759658 -0.2714697 ], action=0, reward=1.0, next_state=[-0.01820356 -0.03019118 -0.04302597  0.00912206]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 23 ] state=[-0.01820356 -0.03019118 -0.04302597  0.00912206], action=0, reward=1.0, next_state=[-0.01880738 -0.22467051 -0.04284353  0.28792535]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 24 ] state=[-0.01880738 -0.22467051 -0.04284353  0.28792535], action=0, reward=1.0, next_state=[-0.02330079 -0.41915613 -0.03708502  0.56679394]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 25 ] state=[-0.02330079 -0.41915613 -0.03708502  0.56679394], action=0, reward=1.0, next_state=[-0.03168392 -0.61373877 -0.02574914  0.8475667 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 26 ] state=[-0.03168392 -0.61373877 -0.02574914  0.8475667 ], action=0, reward=1.0, next_state=[-0.04395869 -0.8085002  -0.00879781  1.1320426 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 27 ] state=[-0.04395869 -0.8085002  -0.00879781  1.1320426 ], action=1, reward=1.0, next_state=[-0.0601287  -0.61326418  0.01384304  0.83661339]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 28 ] state=[-0.0601287  -0.61326418  0.01384304  0.83661339], action=1, reward=1.0, next_state=[-0.07239398 -0.41833402  0.03057531  0.54831588]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 29 ] state=[-0.07239398 -0.41833402  0.03057531  0.54831588], action=1, reward=1.0, next_state=[-0.08076066 -0.22365463  0.04154163  0.26542103]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 30 ] state=[-0.08076066 -0.22365463  0.04154163  0.26542103], action=1, reward=1.0, next_state=[-0.08523375 -0.02914946  0.04685005 -0.0138753 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 31 ] state=[-0.08523375 -0.02914946  0.04685005 -0.0138753 ], action=0, reward=1.0, next_state=[-0.08581674 -0.2249109   0.04657254  0.29321353]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 32 ] state=[-0.08581674 -0.2249109   0.04657254  0.29321353], action=0, reward=1.0, next_state=[-0.09031496 -0.42066485  0.05243681  0.60021345]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 33 ] state=[-0.09031496 -0.42066485  0.05243681  0.60021345], action=1, reward=1.0, next_state=[-0.09872826 -0.22631419  0.06444108  0.32449752]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 34 ] state=[-0.09872826 -0.22631419  0.06444108  0.32449752], action=0, reward=1.0, next_state=[-0.10325454 -0.42229168  0.07093103  0.63678617]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 35 ] state=[-0.10325454 -0.42229168  0.07093103  0.63678617], action=0, reward=1.0, next_state=[-0.11170037 -0.61832739  0.08366675  0.95093657]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 36 ] state=[-0.11170037 -0.61832739  0.08366675  0.95093657], action=0, reward=1.0, next_state=[-0.12406692 -0.8144697   0.10268549  1.2686902 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 37 ] state=[-0.12406692 -0.8144697   0.10268549  1.2686902 ], action=1, reward=1.0, next_state=[-0.14035632 -0.62079801  0.12805929  1.00984943]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 38 ] state=[-0.14035632 -0.62079801  0.12805929  1.00984943], action=0, reward=1.0, next_state=[-0.15277228 -0.81737485  0.14825628  1.33984683]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 39 ] state=[-0.15277228 -0.81737485  0.14825628  1.33984683], action=1, reward=1.0, next_state=[-0.16911977 -0.62439748  0.17505321  1.09698402]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 332 ][ timestamp 40 ] state=[-0.16911977 -0.62439748  0.17505321  1.09698402], action=1, reward=1.0, next_state=[-0.18160772 -0.43195771  0.1969929   0.86393907]\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ][ timestamp 41 ] state=[-0.18160772 -0.43195771  0.1969929   0.86393907], action=0, reward=-1.0, next_state=[-0.19024688 -0.6291371   0.21427168  1.2115299 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 332: Exploration_rate=0.01. Score=41.\n",
      "[ episode 333 ] state=[-0.02653375 -0.01525245 -0.01943731  0.00811976]\n",
      "[ episode 333 ][ timestamp 1 ] state=[-0.02653375 -0.01525245 -0.01943731  0.00811976], action=0, reward=1.0, next_state=[-0.0268388  -0.21009033 -0.01927491  0.29460716]\n",
      "[ Experience replay ] starts\n",
      "[ episode 333 ][ timestamp 2 ] state=[-0.0268388  -0.21009033 -0.01927491  0.29460716], action=1, reward=1.0, next_state=[-0.03104061 -0.01469896 -0.01338277 -0.00409192]\n",
      "[ Experience replay ] starts\n",
      "[ episode 333 ][ timestamp 3 ] state=[-0.03104061 -0.01469896 -0.01338277 -0.00409192], action=1, reward=1.0, next_state=[-0.03133459  0.18061234 -0.01346461 -0.30096705]\n",
      "[ Experience replay ] starts\n",
      "[ episode 333 ][ timestamp 4 ] state=[-0.03133459  0.18061234 -0.01346461 -0.30096705], action=1, reward=1.0, next_state=[-0.02772234  0.37592359 -0.01948395 -0.59786584]\n",
      "[ Experience replay ] starts\n",
      "[ episode 333 ][ timestamp 5 ] state=[-0.02772234  0.37592359 -0.01948395 -0.59786584], action=0, reward=1.0, next_state=[-0.02020387  0.18107961 -0.03144127 -0.31138327]\n",
      "[ Experience replay ] starts\n",
      "[ episode 333 ][ timestamp 6 ] state=[-0.02020387  0.18107961 -0.03144127 -0.31138327], action=0, reward=1.0, next_state=[-0.01658228 -0.01358062 -0.03766893 -0.02877945]\n",
      "[ Experience replay ] starts\n",
      "[ episode 333 ][ timestamp 7 ] state=[-0.01658228 -0.01358062 -0.03766893 -0.02877945], action=0, reward=1.0, next_state=[-0.01685389 -0.20814269 -0.03824452  0.25178459]\n",
      "[ Experience replay ] starts\n",
      "[ episode 333 ][ timestamp 8 ] state=[-0.01685389 -0.20814269 -0.03824452  0.25178459], action=1, reward=1.0, next_state=[-0.02101674 -0.01249609 -0.03320883 -0.05271187]\n",
      "[ Experience replay ] starts\n",
      "[ episode 333 ][ timestamp 9 ] state=[-0.02101674 -0.01249609 -0.03320883 -0.05271187], action=1, reward=1.0, next_state=[-0.02126667  0.18308591 -0.03426307 -0.35568472]\n",
      "[ Experience replay ] starts\n",
      "[ episode 333 ][ timestamp 10 ] state=[-0.02126667  0.18308591 -0.03426307 -0.35568472], action=1, reward=1.0, next_state=[-0.01760495  0.37867784 -0.04137676 -0.65897178]\n",
      "[ Experience replay ] starts\n",
      "[ episode 333 ][ timestamp 11 ] state=[-0.01760495  0.37867784 -0.04137676 -0.65897178], action=1, reward=1.0, next_state=[-0.01003139  0.57435049 -0.0545562  -0.96439085]\n",
      "[ Experience replay ] starts\n",
      "[ episode 333 ][ timestamp 12 ] state=[-0.01003139  0.57435049 -0.0545562  -0.96439085], action=1, reward=1.0, next_state=[ 0.00145562  0.77016123 -0.07384401 -1.27370152]\n",
      "[ Experience replay ] starts\n",
      "[ episode 333 ][ timestamp 13 ] state=[ 0.00145562  0.77016123 -0.07384401 -1.27370152], action=1, reward=1.0, next_state=[ 0.01685884  0.96614354 -0.09931804 -1.58856426]\n",
      "[ Experience replay ] starts\n",
      "[ episode 333 ][ timestamp 14 ] state=[ 0.01685884  0.96614354 -0.09931804 -1.58856426], action=0, reward=1.0, next_state=[ 0.03618171  0.77233192 -0.13108933 -1.328431  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 333 ][ timestamp 15 ] state=[ 0.03618171  0.77233192 -0.13108933 -1.328431  ], action=0, reward=1.0, next_state=[ 0.05162835  0.57908506 -0.15765795 -1.07947775]\n",
      "[ Experience replay ] starts\n",
      "[ episode 333 ][ timestamp 16 ] state=[ 0.05162835  0.57908506 -0.15765795 -1.07947775], action=1, reward=1.0, next_state=[ 0.06321005  0.77589729 -0.1792475  -1.41719437]\n",
      "[ Experience replay ] starts\n",
      "[ episode 333 ][ timestamp 17 ] state=[ 0.06321005  0.77589729 -0.1792475  -1.41719437], action=1, reward=1.0, next_state=[ 0.078728    0.97272805 -0.20759139 -1.76012713]\n",
      "[ Experience replay ] starts\n",
      "[ episode 333 ][ timestamp 18 ] state=[ 0.078728    0.97272805 -0.20759139 -1.76012713], action=0, reward=-1.0, next_state=[ 0.09818256  0.78047309 -0.24279393 -1.53853067]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 333: Exploration_rate=0.01. Score=18.\n",
      "[ episode 334 ] state=[-0.01737706 -0.03443965 -0.03603813 -0.006228  ]\n",
      "[ episode 334 ][ timestamp 1 ] state=[-0.01737706 -0.03443965 -0.03603813 -0.006228  ], action=0, reward=1.0, next_state=[-0.01806586 -0.22902673 -0.03616269  0.27487018]\n",
      "[ Experience replay ] starts\n",
      "[ episode 334 ][ timestamp 2 ] state=[-0.01806586 -0.22902673 -0.03616269  0.27487018], action=0, reward=1.0, next_state=[-0.02264639 -0.42361456 -0.03066528  0.55593158]\n",
      "[ Experience replay ] starts\n",
      "[ episode 334 ][ timestamp 3 ] state=[-0.02264639 -0.42361456 -0.03066528  0.55593158], action=0, reward=1.0, next_state=[-0.03111868 -0.61829286 -0.01954665  0.83879755]\n",
      "[ Experience replay ] starts\n",
      "[ episode 334 ][ timestamp 4 ] state=[-0.03111868 -0.61829286 -0.01954665  0.83879755], action=0, reward=1.0, next_state=[-0.04348454 -0.81314252 -0.0027707   1.12526986]\n",
      "[ Experience replay ] starts\n",
      "[ episode 334 ][ timestamp 5 ] state=[-0.04348454 -0.81314252 -0.0027707   1.12526986], action=1, reward=1.0, next_state=[-0.05974739 -0.61798437  0.0197347   0.83171917]\n",
      "[ Experience replay ] starts\n",
      "[ episode 334 ][ timestamp 6 ] state=[-0.05974739 -0.61798437  0.0197347   0.83171917], action=1, reward=1.0, next_state=[-0.07210708 -0.4231376   0.03636908  0.54530756]\n",
      "[ Experience replay ] starts\n",
      "[ episode 334 ][ timestamp 7 ] state=[-0.07210708 -0.4231376   0.03636908  0.54530756], action=1, reward=1.0, next_state=[-0.08056983 -0.22854505  0.04727523  0.26430191]\n",
      "[ Experience replay ] starts\n",
      "[ episode 334 ][ timestamp 8 ] state=[-0.08056983 -0.22854505  0.04727523  0.26430191], action=0, reward=1.0, next_state=[-0.08514073 -0.42430878  0.05256127  0.57151316]\n",
      "[ Experience replay ] starts\n",
      "[ episode 334 ][ timestamp 9 ] state=[-0.08514073 -0.42430878  0.05256127  0.57151316], action=1, reward=1.0, next_state=[-0.09362691 -0.22996175  0.06399153  0.29584112]\n",
      "[ Experience replay ] starts\n",
      "[ episode 334 ][ timestamp 10 ] state=[-0.09362691 -0.22996175  0.06399153  0.29584112], action=1, reward=1.0, next_state=[-0.09822614 -0.03580767  0.06990835  0.02400674]\n",
      "[ Experience replay ] starts\n",
      "[ episode 334 ][ timestamp 11 ] state=[-0.09822614 -0.03580767  0.06990835  0.02400674], action=1, reward=1.0, next_state=[-0.09894229  0.15824572  0.07038849 -0.24582603]\n",
      "[ Experience replay ] starts\n",
      "[ episode 334 ][ timestamp 12 ] state=[-0.09894229  0.15824572  0.07038849 -0.24582603], action=0, reward=1.0, next_state=[-0.09577738 -0.03780727  0.06547197  0.06820237]\n",
      "[ Experience replay ] starts\n",
      "[ episode 334 ][ timestamp 13 ] state=[-0.09577738 -0.03780727  0.06547197  0.06820237], action=0, reward=1.0, next_state=[-0.09653353 -0.23380382  0.06683602  0.38080232]\n",
      "[ Experience replay ] starts\n",
      "[ episode 334 ][ timestamp 14 ] state=[-0.09653353 -0.23380382  0.06683602  0.38080232], action=1, reward=1.0, next_state=[-0.1012096  -0.03969145  0.07445206  0.10991901]\n",
      "[ Experience replay ] starts\n",
      "[ episode 334 ][ timestamp 15 ] state=[-0.1012096  -0.03969145  0.07445206  0.10991901], action=1, reward=1.0, next_state=[-0.10200343  0.15428904  0.07665044 -0.15837696]\n",
      "[ Experience replay ] starts\n",
      "[ episode 334 ][ timestamp 16 ] state=[-0.10200343  0.15428904  0.07665044 -0.15837696], action=0, reward=1.0, next_state=[-0.09891765 -0.04184187  0.0734829   0.15746875]\n",
      "[ Experience replay ] starts\n",
      "[ episode 334 ][ timestamp 17 ] state=[-0.09891765 -0.04184187  0.0734829   0.15746875], action=1, reward=1.0, next_state=[-0.09975449  0.15215529  0.07663228 -0.11115715]\n",
      "[ Experience replay ] starts\n",
      "[ episode 334 ][ timestamp 18 ] state=[-0.09975449  0.15215529  0.07663228 -0.11115715], action=0, reward=1.0, next_state=[-0.09671138 -0.04397635  0.07440914  0.20468474]\n",
      "[ Experience replay ] starts\n",
      "[ episode 334 ][ timestamp 19 ] state=[-0.09671138 -0.04397635  0.07440914  0.20468474], action=0, reward=1.0, next_state=[-0.09759091 -0.24007917  0.07850283  0.51988111]\n",
      "[ Experience replay ] starts\n",
      "[ episode 334 ][ timestamp 20 ] state=[-0.09759091 -0.24007917  0.07850283  0.51988111], action=0, reward=1.0, next_state=[-0.10239249 -0.4362134   0.08890045  0.83623253]\n",
      "[ Experience replay ] starts\n",
      "[ episode 334 ][ timestamp 21 ] state=[-0.10239249 -0.4362134   0.08890045  0.83623253], action=0, reward=1.0, next_state=[-0.11111676 -0.63242987  0.1056251   1.15549725]\n",
      "[ Experience replay ] starts\n",
      "[ episode 334 ][ timestamp 22 ] state=[-0.11111676 -0.63242987  0.1056251   1.15549725], action=0, reward=1.0, next_state=[-0.12376536 -0.82875839  0.12873505  1.47934484]\n",
      "[ Experience replay ] starts\n",
      "[ episode 334 ][ timestamp 23 ] state=[-0.12376536 -0.82875839  0.12873505  1.47934484], action=1, reward=1.0, next_state=[-0.14034053 -0.63542141  0.15832194  1.2294828 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 334 ][ timestamp 24 ] state=[-0.14034053 -0.63542141  0.15832194  1.2294828 ], action=0, reward=1.0, next_state=[-0.15304895 -0.83218579  0.1829116   1.56729047]\n",
      "[ Experience replay ] starts\n",
      "[ episode 334 ][ timestamp 25 ] state=[-0.15304895 -0.83218579  0.1829116   1.56729047], action=0, reward=-1.0, next_state=[-0.16969267 -1.02896057  0.21425741  1.91100548]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Ended! ] Episode 334: Exploration_rate=0.01. Score=25.\n",
      "[ episode 335 ] state=[-0.00209227  0.00103428  0.01185679 -0.01661282]\n",
      "[ episode 335 ][ timestamp 1 ] state=[-0.00209227  0.00103428  0.01185679 -0.01661282], action=0, reward=1.0, next_state=[-0.00207158 -0.19425569  0.01152454  0.27978735]\n",
      "[ Experience replay ] starts\n",
      "[ episode 335 ][ timestamp 2 ] state=[-0.00207158 -0.19425569  0.01152454  0.27978735], action=1, reward=1.0, next_state=[-0.00595669  0.00069998  0.01712028 -0.0092386 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 335 ][ timestamp 3 ] state=[-0.00595669  0.00069998  0.01712028 -0.0092386 ], action=1, reward=1.0, next_state=[-0.00594269  0.19557228  0.01693551 -0.29647108]\n",
      "[ Experience replay ] starts\n",
      "[ episode 335 ][ timestamp 4 ] state=[-0.00594269  0.19557228  0.01693551 -0.29647108], action=0, reward=1.0, next_state=[-0.00203125  0.00021304  0.01100609  0.00150455]\n",
      "[ Experience replay ] starts\n",
      "[ episode 335 ][ timestamp 5 ] state=[-0.00203125  0.00021304  0.01100609  0.00150455], action=1, reward=1.0, next_state=[-0.00202699  0.19517544  0.01103618 -0.28768561]\n",
      "[ Experience replay ] starts\n",
      "[ episode 335 ][ timestamp 6 ] state=[-0.00202699  0.19517544  0.01103618 -0.28768561], action=0, reward=1.0, next_state=[ 0.00187652 -0.00010215  0.00528247  0.0084575 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 335 ][ timestamp 7 ] state=[ 0.00187652 -0.00010215  0.00528247  0.0084575 ], action=1, reward=1.0, next_state=[ 0.00187448  0.19494365  0.00545162 -0.28255407]\n",
      "[ Experience replay ] starts\n",
      "[ episode 335 ][ timestamp 8 ] state=[ 0.00187448  0.19494365  0.00545162 -0.28255407], action=1, reward=1.0, next_state=[ 5.77335143e-03  3.89987420e-01 -1.99463845e-04 -5.73512613e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 335 ][ timestamp 9 ] state=[ 5.77335143e-03  3.89987420e-01 -1.99463845e-04 -5.73512613e-01], action=1, reward=1.0, next_state=[ 0.0135731   0.58511217 -0.01166972 -0.86625837]\n",
      "[ Experience replay ] starts\n",
      "[ episode 335 ][ timestamp 10 ] state=[ 0.0135731   0.58511217 -0.01166972 -0.86625837], action=1, reward=1.0, next_state=[ 0.02527534  0.78039097 -0.02899488 -1.16258746]\n",
      "[ Experience replay ] starts\n",
      "[ episode 335 ][ timestamp 11 ] state=[ 0.02527534  0.78039097 -0.02899488 -1.16258746], action=1, reward=1.0, next_state=[ 0.04088316  0.97587827 -0.05224663 -1.46421845]\n",
      "[ Experience replay ] starts\n",
      "[ episode 335 ][ timestamp 12 ] state=[ 0.04088316  0.97587827 -0.05224663 -1.46421845], action=1, reward=1.0, next_state=[ 0.06040073  1.17159986 -0.081531   -1.77275376]\n",
      "[ Experience replay ] starts\n",
      "[ episode 335 ][ timestamp 13 ] state=[ 0.06040073  1.17159986 -0.081531   -1.77275376], action=1, reward=1.0, next_state=[ 0.08383273  1.36754112 -0.11698608 -2.08963289]\n",
      "[ Experience replay ] starts\n",
      "[ episode 335 ][ timestamp 14 ] state=[ 0.08383273  1.36754112 -0.11698608 -2.08963289], action=0, reward=1.0, next_state=[ 0.11118355  1.17377758 -0.15877873 -1.83528966]\n",
      "[ Experience replay ] starts\n",
      "[ episode 335 ][ timestamp 15 ] state=[ 0.11118355  1.17377758 -0.15877873 -1.83528966], action=1, reward=1.0, next_state=[ 0.1346591   1.37025958 -0.19548453 -2.17279043]\n",
      "[ Experience replay ] starts\n",
      "[ episode 335 ][ timestamp 16 ] state=[ 0.1346591   1.37025958 -0.19548453 -2.17279043], action=0, reward=-1.0, next_state=[ 0.16206429  1.17750776 -0.23894034 -1.94627664]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 335: Exploration_rate=0.01. Score=16.\n",
      "[ episode 336 ] state=[-0.01439283 -0.04421451 -0.01050571  0.00963085]\n",
      "[ episode 336 ][ timestamp 1 ] state=[-0.01439283 -0.04421451 -0.01050571  0.00963085], action=1, reward=1.0, next_state=[-0.01527712  0.15105652 -0.0103131  -0.28634815]\n",
      "[ Experience replay ] starts\n",
      "[ episode 336 ][ timestamp 2 ] state=[-0.01527712  0.15105652 -0.0103131  -0.28634815], action=0, reward=1.0, next_state=[-0.01225599 -0.04391684 -0.01604006  0.00306434]\n",
      "[ Experience replay ] starts\n",
      "[ episode 336 ][ timestamp 3 ] state=[-0.01225599 -0.04391684 -0.01604006  0.00306434], action=1, reward=1.0, next_state=[-0.01313432  0.15143143 -0.01597877 -0.29463595]\n",
      "[ Experience replay ] starts\n",
      "[ episode 336 ][ timestamp 4 ] state=[-0.01313432  0.15143143 -0.01597877 -0.29463595], action=1, reward=1.0, next_state=[-0.01010569  0.3467775  -0.02187149 -0.5923152 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 336 ][ timestamp 5 ] state=[-0.01010569  0.3467775  -0.02187149 -0.5923152 ], action=0, reward=1.0, next_state=[-0.00317014  0.15196845 -0.0337178  -0.30660122]\n",
      "[ Experience replay ] starts\n",
      "[ episode 336 ][ timestamp 6 ] state=[-0.00317014  0.15196845 -0.0337178  -0.30660122], action=0, reward=1.0, next_state=[-0.00013077 -0.04265721 -0.03984982 -0.02473982]\n",
      "[ Experience replay ] starts\n",
      "[ episode 336 ][ timestamp 7 ] state=[-0.00013077 -0.04265721 -0.03984982 -0.02473982], action=0, reward=1.0, next_state=[-0.00098392 -0.2371857  -0.04034462  0.2551085 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 336 ][ timestamp 8 ] state=[-0.00098392 -0.2371857  -0.04034462  0.2551085 ], action=0, reward=1.0, next_state=[-0.00572763 -0.43170908 -0.03524245  0.53479804]\n",
      "[ Experience replay ] starts\n",
      "[ episode 336 ][ timestamp 9 ] state=[-0.00572763 -0.43170908 -0.03524245  0.53479804], action=0, reward=1.0, next_state=[-0.01436181 -0.62631818 -0.02454649  0.81617129]\n",
      "[ Experience replay ] starts\n",
      "[ episode 336 ][ timestamp 10 ] state=[-0.01436181 -0.62631818 -0.02454649  0.81617129], action=0, reward=1.0, next_state=[-0.02688818 -0.8210956  -0.00822306  1.10103347]\n",
      "[ Experience replay ] starts\n",
      "[ episode 336 ][ timestamp 11 ] state=[-0.02688818 -0.8210956  -0.00822306  1.10103347], action=1, reward=1.0, next_state=[-0.04331009 -0.62586642  0.01379761  0.80578204]\n",
      "[ Experience replay ] starts\n",
      "[ episode 336 ][ timestamp 12 ] state=[-0.04331009 -0.62586642  0.01379761  0.80578204], action=0, reward=1.0, next_state=[-0.05582742 -0.82117476  0.02991325  1.10277304]\n",
      "[ Experience replay ] starts\n",
      "[ episode 336 ][ timestamp 13 ] state=[-0.05582742 -0.82117476  0.02991325  1.10277304], action=0, reward=1.0, next_state=[-0.07225091 -1.01667717  0.05196871  1.40468865]\n",
      "[ Experience replay ] starts\n",
      "[ episode 336 ][ timestamp 14 ] state=[-0.07225091 -1.01667717  0.05196871  1.40468865], action=1, reward=1.0, next_state=[-0.09258446 -0.82223758  0.08006248  1.12869495]\n",
      "[ Experience replay ] starts\n",
      "[ episode 336 ][ timestamp 15 ] state=[-0.09258446 -0.82223758  0.08006248  1.12869495], action=1, reward=1.0, next_state=[-0.10902921 -0.62825033  0.10263638  0.8621594 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 336 ][ timestamp 16 ] state=[-0.10902921 -0.62825033  0.10263638  0.8621594 ], action=0, reward=1.0, next_state=[-0.12159422 -0.8246088   0.11987957  1.18526925]\n",
      "[ Experience replay ] starts\n",
      "[ episode 336 ][ timestamp 17 ] state=[-0.12159422 -0.8246088   0.11987957  1.18526925], action=0, reward=1.0, next_state=[-0.13808639 -1.02106407  0.14358495  1.51299746]\n",
      "[ Experience replay ] starts\n",
      "[ episode 336 ][ timestamp 18 ] state=[-0.13808639 -1.02106407  0.14358495  1.51299746], action=1, reward=1.0, next_state=[-0.15850767 -0.82794291  0.1738449   1.2683658 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 336 ][ timestamp 19 ] state=[-0.15850767 -0.82794291  0.1738449   1.2683658 ], action=1, reward=1.0, next_state=[-0.17506653 -0.63541422  0.19921222  1.03477909]\n",
      "[ Experience replay ] starts\n",
      "[ episode 336 ][ timestamp 20 ] state=[-0.17506653 -0.63541422  0.19921222  1.03477909], action=1, reward=-1.0, next_state=[-0.18777482 -0.44341773  0.2199078   0.81066186]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 336: Exploration_rate=0.01. Score=20.\n",
      "[ episode 337 ] state=[ 0.03758918  0.03633719 -0.02843884  0.0092214 ]\n",
      "[ episode 337 ][ timestamp 1 ] state=[ 0.03758918  0.03633719 -0.02843884  0.0092214 ], action=1, reward=1.0, next_state=[ 0.03831593  0.23185521 -0.02825441 -0.29229693]\n",
      "[ Experience replay ] starts\n",
      "[ episode 337 ][ timestamp 2 ] state=[ 0.03831593  0.23185521 -0.02825441 -0.29229693], action=0, reward=1.0, next_state=[ 0.04295303  0.03714727 -0.03410035 -0.00865728]\n",
      "[ Experience replay ] starts\n",
      "[ episode 337 ][ timestamp 3 ] state=[ 0.04295303  0.03714727 -0.03410035 -0.00865728], action=1, reward=1.0, next_state=[ 0.04369598  0.23274125 -0.0342735  -0.31190125]\n",
      "[ Experience replay ] starts\n",
      "[ episode 337 ][ timestamp 4 ] state=[ 0.04369598  0.23274125 -0.0342735  -0.31190125], action=1, reward=1.0, next_state=[ 0.0483508   0.42833429 -0.04051152 -0.61519294]\n",
      "[ Experience replay ] starts\n",
      "[ episode 337 ][ timestamp 5 ] state=[ 0.0483508   0.42833429 -0.04051152 -0.61519294], action=1, reward=1.0, next_state=[ 0.05691749  0.62399816 -0.05281538 -0.92035508]\n",
      "[ Experience replay ] starts\n",
      "[ episode 337 ][ timestamp 6 ] state=[ 0.05691749  0.62399816 -0.05281538 -0.92035508], action=0, reward=1.0, next_state=[ 0.06939745  0.42962826 -0.07122248 -0.64472728]\n",
      "[ Experience replay ] starts\n",
      "[ episode 337 ][ timestamp 7 ] state=[ 0.06939745  0.42962826 -0.07122248 -0.64472728], action=1, reward=1.0, next_state=[ 0.07799002  0.6256667  -0.08411703 -0.95896113]\n",
      "[ Experience replay ] starts\n",
      "[ episode 337 ][ timestamp 8 ] state=[ 0.07799002  0.6256667  -0.08411703 -0.95896113], action=1, reward=1.0, next_state=[ 0.09050335  0.82181264 -0.10329625 -1.27684101]\n",
      "[ Experience replay ] starts\n",
      "[ episode 337 ][ timestamp 9 ] state=[ 0.09050335  0.82181264 -0.10329625 -1.27684101], action=0, reward=1.0, next_state=[ 0.1069396   0.62814825 -0.12883307 -1.01820798]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 337 ][ timestamp 10 ] state=[ 0.1069396   0.62814825 -0.12883307 -1.01820798], action=0, reward=1.0, next_state=[ 0.11950257  0.43495696 -0.14919723 -0.76859489]\n",
      "[ Experience replay ] starts\n",
      "[ episode 337 ][ timestamp 11 ] state=[ 0.11950257  0.43495696 -0.14919723 -0.76859489], action=0, reward=1.0, next_state=[ 0.12820171  0.24216912 -0.16456913 -0.52632716]\n",
      "[ Experience replay ] starts\n",
      "[ episode 337 ][ timestamp 12 ] state=[ 0.12820171  0.24216912 -0.16456913 -0.52632716], action=1, reward=1.0, next_state=[ 0.13304509  0.43917768 -0.17509567 -0.86601256]\n",
      "[ Experience replay ] starts\n",
      "[ episode 337 ][ timestamp 13 ] state=[ 0.13304509  0.43917768 -0.17509567 -0.86601256], action=0, reward=1.0, next_state=[ 0.14182864  0.24681551 -0.19241592 -0.63309667]\n",
      "[ Experience replay ] starts\n",
      "[ episode 337 ][ timestamp 14 ] state=[ 0.14182864  0.24681551 -0.19241592 -0.63309667], action=0, reward=1.0, next_state=[ 0.14676495  0.05482443 -0.20507786 -0.40664666]\n",
      "[ Experience replay ] starts\n",
      "[ episode 337 ][ timestamp 15 ] state=[ 0.14676495  0.05482443 -0.20507786 -0.40664666], action=0, reward=-1.0, next_state=[ 0.14786144 -0.13688929 -0.21321079 -0.18497324]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 337: Exploration_rate=0.01. Score=15.\n",
      "[ episode 338 ] state=[ 0.04251037  0.01220402  0.02641322 -0.03234645]\n",
      "[ episode 338 ][ timestamp 1 ] state=[ 0.04251037  0.01220402  0.02641322 -0.03234645], action=1, reward=1.0, next_state=[ 0.04275445  0.20693743  0.02576629 -0.3165801 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 338 ][ timestamp 2 ] state=[ 0.04275445  0.20693743  0.02576629 -0.3165801 ], action=0, reward=1.0, next_state=[ 0.0468932   0.01145813  0.01943469 -0.01588403]\n",
      "[ Experience replay ] starts\n",
      "[ episode 338 ][ timestamp 3 ] state=[ 0.0468932   0.01145813  0.01943469 -0.01588403], action=0, reward=1.0, next_state=[ 0.04712237 -0.18393707  0.019117    0.28286686]\n",
      "[ Experience replay ] starts\n",
      "[ episode 338 ][ timestamp 4 ] state=[ 0.04712237 -0.18393707  0.019117    0.28286686], action=0, reward=1.0, next_state=[ 0.04344362 -0.37932641  0.02477434  0.58151736]\n",
      "[ Experience replay ] starts\n",
      "[ episode 338 ][ timestamp 5 ] state=[ 0.04344362 -0.37932641  0.02477434  0.58151736], action=0, reward=1.0, next_state=[ 0.0358571  -0.57478656  0.03640469  0.88190054]\n",
      "[ Experience replay ] starts\n",
      "[ episode 338 ][ timestamp 6 ] state=[ 0.0358571  -0.57478656  0.03640469  0.88190054], action=0, reward=1.0, next_state=[ 0.02436136 -0.77038357  0.0540427   1.18580227]\n",
      "[ Experience replay ] starts\n",
      "[ episode 338 ][ timestamp 7 ] state=[ 0.02436136 -0.77038357  0.0540427   1.18580227], action=0, reward=1.0, next_state=[ 0.00895369 -0.96616317  0.07775875  1.49492374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 338 ][ timestamp 8 ] state=[ 0.00895369 -0.96616317  0.07775875  1.49492374], action=0, reward=1.0, next_state=[-0.01036957 -1.16213991  0.10765722  1.81083862]\n",
      "[ Experience replay ] starts\n",
      "[ episode 338 ][ timestamp 9 ] state=[-0.01036957 -1.16213991  0.10765722  1.81083862], action=1, reward=1.0, next_state=[-0.03361237 -0.96836996  0.14387399  1.55345655]\n",
      "[ Experience replay ] starts\n",
      "[ episode 338 ][ timestamp 10 ] state=[-0.03361237 -0.96836996  0.14387399  1.55345655], action=1, reward=1.0, next_state=[-0.05297977 -0.77523596  0.17494312  1.30890193]\n",
      "[ Experience replay ] starts\n",
      "[ episode 338 ][ timestamp 11 ] state=[-0.05297977 -0.77523596  0.17494312  1.30890193], action=0, reward=1.0, next_state=[-0.06848449 -0.97208871  0.20112116  1.65084536]\n",
      "[ Experience replay ] starts\n",
      "[ episode 338 ][ timestamp 12 ] state=[-0.06848449 -0.97208871  0.20112116  1.65084536], action=1, reward=-1.0, next_state=[-0.08792626 -0.77980431  0.23413807  1.42696431]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 338: Exploration_rate=0.01. Score=12.\n",
      "[ episode 339 ] state=[ 0.02302375  0.04946729 -0.02713549 -0.03035186]\n",
      "[ episode 339 ][ timestamp 1 ] state=[ 0.02302375  0.04946729 -0.02713549 -0.03035186], action=0, reward=1.0, next_state=[ 0.02401309 -0.14525523 -0.02774253  0.25364753]\n",
      "[ Experience replay ] starts\n",
      "[ episode 339 ][ timestamp 2 ] state=[ 0.02401309 -0.14525523 -0.02774253  0.25364753], action=0, reward=1.0, next_state=[ 0.02110799 -0.33997029 -0.02266958  0.53745248]\n",
      "[ Experience replay ] starts\n",
      "[ episode 339 ][ timestamp 3 ] state=[ 0.02110799 -0.33997029 -0.02266958  0.53745248], action=0, reward=1.0, next_state=[ 0.01430858 -0.53476631 -0.01192053  0.82290713]\n",
      "[ Experience replay ] starts\n",
      "[ episode 339 ][ timestamp 4 ] state=[ 0.01430858 -0.53476631 -0.01192053  0.82290713], action=1, reward=1.0, next_state=[ 0.00361326 -0.33948332  0.00453762  0.52649891]\n",
      "[ Experience replay ] starts\n",
      "[ episode 339 ][ timestamp 5 ] state=[ 0.00361326 -0.33948332  0.00453762  0.52649891], action=0, reward=1.0, next_state=[-0.00317641 -0.53466882  0.0150676   0.82060821]\n",
      "[ Experience replay ] starts\n",
      "[ episode 339 ][ timestamp 6 ] state=[-0.00317641 -0.53466882  0.0150676   0.82060821], action=1, reward=1.0, next_state=[-0.01386979 -0.33975627  0.03147976  0.53270228]\n",
      "[ Experience replay ] starts\n",
      "[ episode 339 ][ timestamp 7 ] state=[-0.01386979 -0.33975627  0.03147976  0.53270228], action=1, reward=1.0, next_state=[-0.02066491 -0.14509088  0.04213381  0.25010239]\n",
      "[ Experience replay ] starts\n",
      "[ episode 339 ][ timestamp 8 ] state=[-0.02066491 -0.14509088  0.04213381  0.25010239], action=0, reward=1.0, next_state=[-0.02356673 -0.3407884   0.04713585  0.55577182]\n",
      "[ Experience replay ] starts\n",
      "[ episode 339 ][ timestamp 9 ] state=[-0.02356673 -0.3407884   0.04713585  0.55577182], action=1, reward=1.0, next_state=[-0.0303825  -0.14635884  0.05825129  0.2783042 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 339 ][ timestamp 10 ] state=[-0.0303825  -0.14635884  0.05825129  0.2783042 ], action=0, reward=1.0, next_state=[-0.03330967 -0.34226132  0.06381737  0.58877572]\n",
      "[ Experience replay ] starts\n",
      "[ episode 339 ][ timestamp 11 ] state=[-0.03330967 -0.34226132  0.06381737  0.58877572], action=1, reward=1.0, next_state=[-0.0401549  -0.14808834  0.07559289  0.31685871]\n",
      "[ Experience replay ] starts\n",
      "[ episode 339 ][ timestamp 12 ] state=[-0.0401549  -0.14808834  0.07559289  0.31685871], action=0, reward=1.0, next_state=[-0.04311667 -0.34420104  0.08193006  0.63239083]\n",
      "[ Experience replay ] starts\n",
      "[ episode 339 ][ timestamp 13 ] state=[-0.04311667 -0.34420104  0.08193006  0.63239083], action=1, reward=1.0, next_state=[-0.05000069 -0.15031192  0.09457788  0.36659322]\n",
      "[ Experience replay ] starts\n",
      "[ episode 339 ][ timestamp 14 ] state=[-0.05000069 -0.15031192  0.09457788  0.36659322], action=1, reward=1.0, next_state=[-0.05300693  0.04334773  0.10190974  0.10516645]\n",
      "[ Experience replay ] starts\n",
      "[ episode 339 ][ timestamp 15 ] state=[-0.05300693  0.04334773  0.10190974  0.10516645], action=1, reward=1.0, next_state=[-0.05213997  0.23687278  0.10401307 -0.1537054 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 339 ][ timestamp 16 ] state=[-0.05213997  0.23687278  0.10401307 -0.1537054 ], action=1, reward=1.0, next_state=[-0.04740252  0.43036354  0.10093896 -0.41184823]\n",
      "[ Experience replay ] starts\n",
      "[ episode 339 ][ timestamp 17 ] state=[-0.04740252  0.43036354  0.10093896 -0.41184823], action=0, reward=1.0, next_state=[-0.03879525  0.23396639  0.092702   -0.08912631]\n",
      "[ Experience replay ] starts\n",
      "[ episode 339 ][ timestamp 18 ] state=[-0.03879525  0.23396639  0.092702   -0.08912631], action=0, reward=1.0, next_state=[-0.03411592  0.03764637  0.09091947  0.23130466]\n",
      "[ Experience replay ] starts\n",
      "[ episode 339 ][ timestamp 19 ] state=[-0.03411592  0.03764637  0.09091947  0.23130466], action=0, reward=1.0, next_state=[-0.03336299 -0.15864918  0.09554557  0.55122536]\n",
      "[ Experience replay ] starts\n",
      "[ episode 339 ][ timestamp 20 ] state=[-0.03336299 -0.15864918  0.09554557  0.55122536], action=0, reward=1.0, next_state=[-0.03653597 -0.35497405  0.10657007  0.87241718]\n",
      "[ Experience replay ] starts\n",
      "[ episode 339 ][ timestamp 21 ] state=[-0.03653597 -0.35497405  0.10657007  0.87241718], action=0, reward=1.0, next_state=[-0.04363546 -0.55137125  0.12401842  1.196614  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 339 ][ timestamp 22 ] state=[-0.04363546 -0.55137125  0.12401842  1.196614  ], action=0, reward=1.0, next_state=[-0.05466288 -0.74786082  0.1479507   1.52545269]\n",
      "[ Experience replay ] starts\n",
      "[ episode 339 ][ timestamp 23 ] state=[-0.05466288 -0.74786082  0.1479507   1.52545269], action=1, reward=1.0, next_state=[-0.0696201  -0.55480218  0.17845975  1.28236739]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 339 ][ timestamp 24 ] state=[-0.0696201  -0.55480218  0.17845975  1.28236739], action=0, reward=1.0, next_state=[-0.08071614 -0.75169075  0.2041071   1.62519897]\n",
      "[ Experience replay ] starts\n",
      "[ episode 339 ][ timestamp 25 ] state=[-0.08071614 -0.75169075  0.2041071   1.62519897], action=0, reward=-1.0, next_state=[-0.09574996 -0.94854518  0.23661108  1.97394297]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 339: Exploration_rate=0.01. Score=25.\n",
      "[ episode 340 ] state=[-0.03185152  0.01907287  0.00333619  0.02256361]\n",
      "[ episode 340 ][ timestamp 1 ] state=[-0.03185152  0.01907287  0.00333619  0.02256361], action=1, reward=1.0, next_state=[-0.03147007  0.21414682  0.00378746 -0.26906485]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 2 ] state=[-0.03147007  0.21414682  0.00378746 -0.26906485], action=0, reward=1.0, next_state=[-0.02718713  0.01897102 -0.00159384  0.02481025]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 3 ] state=[-0.02718713  0.01897102 -0.00159384  0.02481025], action=0, reward=1.0, next_state=[-0.02680771 -0.17612804 -0.00109763  0.31698988]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 4 ] state=[-0.02680771 -0.17612804 -0.00109763  0.31698988], action=0, reward=1.0, next_state=[-0.03033027 -0.37123434  0.00524216  0.60932645]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 5 ] state=[-0.03033027 -0.37123434  0.00524216  0.60932645], action=1, reward=1.0, next_state=[-0.03775496 -0.17618606  0.01742869  0.31829924]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 6 ] state=[-0.03775496 -0.17618606  0.01742869  0.31829924], action=1, reward=1.0, next_state=[-0.04127868  0.01868338  0.02379468  0.03116325]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 7 ] state=[-0.04127868  0.01868338  0.02379468  0.03116325], action=0, reward=1.0, next_state=[-0.04090501 -0.17677157  0.02441794  0.33125766]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 8 ] state=[-0.04090501 -0.17677157  0.02441794  0.33125766], action=0, reward=1.0, next_state=[-0.04444044 -0.37223244  0.03104309  0.63153971]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 9 ] state=[-0.04444044 -0.37223244  0.03104309  0.63153971], action=0, reward=1.0, next_state=[-0.05188509 -0.56777344  0.04367389  0.93383511]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 10 ] state=[-0.05188509 -0.56777344  0.04367389  0.93383511], action=0, reward=1.0, next_state=[-0.06324056 -0.76345651  0.06235059  1.23991586]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 11 ] state=[-0.06324056 -0.76345651  0.06235059  1.23991586], action=1, reward=1.0, next_state=[-0.07850969 -0.56918817  0.08714891  0.96739879]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 12 ] state=[-0.07850969 -0.56918817  0.08714891  0.96739879], action=1, reward=1.0, next_state=[-0.08989345 -0.37533773  0.10649688  0.703316  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 13 ] state=[-0.08989345 -0.37533773  0.10649688  0.703316  ], action=1, reward=1.0, next_state=[-0.09740021 -0.18184025  0.1205632   0.44596509]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 14 ] state=[-0.09740021 -0.18184025  0.1205632   0.44596509], action=1, reward=1.0, next_state=[-0.10103701  0.01138817  0.12948251  0.19358618]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 15 ] state=[-0.10103701  0.01138817  0.12948251  0.19358618], action=1, reward=1.0, next_state=[-0.10080925  0.20444305  0.13335423 -0.05561042]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 16 ] state=[-0.10080925  0.20444305  0.13335423 -0.05561042], action=1, reward=1.0, next_state=[-0.09672039  0.39742587  0.13224202 -0.30342451]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 17 ] state=[-0.09672039  0.39742587  0.13224202 -0.30342451], action=1, reward=1.0, next_state=[-0.08877187  0.59043961  0.12617353 -0.55165131]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 18 ] state=[-0.08877187  0.59043961  0.12617353 -0.55165131], action=1, reward=1.0, next_state=[-0.07696308  0.78358476  0.11514051 -0.8020693 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 19 ] state=[-0.07696308  0.78358476  0.11514051 -0.8020693 ], action=1, reward=1.0, next_state=[-0.06129138  0.97695534  0.09909912 -1.05642804]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 20 ] state=[-0.06129138  0.97695534  0.09909912 -1.05642804], action=1, reward=1.0, next_state=[-0.04175228  1.17063435  0.07797056 -1.31643372]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 21 ] state=[-0.04175228  1.17063435  0.07797056 -1.31643372], action=1, reward=1.0, next_state=[-0.01833959  1.36468817  0.05164188 -1.58372996]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 22 ] state=[-0.01833959  1.36468817  0.05164188 -1.58372996], action=0, reward=1.0, next_state=[ 0.00895417  1.16899138  0.01996728 -1.27540015]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 23 ] state=[ 0.00895417  1.16899138  0.01996728 -1.27540015], action=0, reward=1.0, next_state=[ 0.032334    0.97362053 -0.00554072 -0.9765323 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 24 ] state=[ 0.032334    0.97362053 -0.00554072 -0.9765323 ], action=0, reward=1.0, next_state=[ 0.05180641  0.77857332 -0.02507136 -0.68559495]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 25 ] state=[ 0.05180641  0.77857332 -0.02507136 -0.68559495], action=0, reward=1.0, next_state=[ 0.06737788  0.58380824 -0.03878326 -0.40090935]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 26 ] state=[ 0.06737788  0.58380824 -0.03878326 -0.40090935], action=0, reward=1.0, next_state=[ 0.07905404  0.38925726 -0.04680145 -0.12070175]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 27 ] state=[ 0.07905404  0.38925726 -0.04680145 -0.12070175], action=1, reward=1.0, next_state=[ 0.08683919  0.58501741 -0.04921549 -0.42777505]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 28 ] state=[ 0.08683919  0.58501741 -0.04921549 -0.42777505], action=1, reward=1.0, next_state=[ 0.09853954  0.78080059 -0.05777099 -0.73555774]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 29 ] state=[ 0.09853954  0.78080059 -0.05777099 -0.73555774], action=1, reward=1.0, next_state=[ 0.11415555  0.97667097 -0.07248214 -1.04584837]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 30 ] state=[ 0.11415555  0.97667097 -0.07248214 -1.04584837], action=0, reward=1.0, next_state=[ 0.13368897  0.78258208 -0.09339911 -0.77677056]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 31 ] state=[ 0.13368897  0.78258208 -0.09339911 -0.77677056], action=0, reward=1.0, next_state=[ 0.14934061  0.58886022 -0.10893452 -0.51487372]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 32 ] state=[ 0.14934061  0.58886022 -0.10893452 -0.51487372], action=0, reward=1.0, next_state=[ 0.16111781  0.3954274  -0.11923199 -0.25840779]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 33 ] state=[ 0.16111781  0.3954274  -0.11923199 -0.25840779], action=0, reward=1.0, next_state=[ 0.16902636  0.20219156 -0.12440015 -0.00558312]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 34 ] state=[ 0.16902636  0.20219156 -0.12440015 -0.00558312], action=0, reward=1.0, next_state=[ 0.17307019  0.00905291 -0.12451181  0.24540669]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 35 ] state=[ 0.17307019  0.00905291 -0.12451181  0.24540669], action=0, reward=1.0, next_state=[ 0.17325125 -0.18409109 -0.11960368  0.49636786]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 36 ] state=[ 0.17325125 -0.18409109 -0.11960368  0.49636786], action=0, reward=1.0, next_state=[ 0.16956943 -0.37734143 -0.10967632  0.74909279]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 37 ] state=[ 0.16956943 -0.37734143 -0.10967632  0.74909279], action=0, reward=1.0, next_state=[ 0.1620226  -0.57079338 -0.09469447  1.00534698]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 38 ] state=[ 0.1620226  -0.57079338 -0.09469447  1.00534698], action=0, reward=1.0, next_state=[ 0.15060673 -0.76453188 -0.07458753  1.26685417]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 39 ] state=[ 0.15060673 -0.76453188 -0.07458753  1.26685417], action=1, reward=1.0, next_state=[ 0.13531609 -0.56854051 -0.04925044  0.9517761 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 40 ] state=[ 0.13531609 -0.56854051 -0.04925044  0.9517761 ], action=1, reward=1.0, next_state=[ 0.12394528 -0.3727916  -0.03021492  0.64403499]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 41 ] state=[ 0.12394528 -0.3727916  -0.03021492  0.64403499], action=1, reward=1.0, next_state=[ 0.11648945 -0.17726187 -0.01733422  0.34199243]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 340 ][ timestamp 42 ] state=[ 0.11648945 -0.17726187 -0.01733422  0.34199243], action=1, reward=1.0, next_state=[ 0.11294422  0.01810236 -0.01049437  0.04389411]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 43 ] state=[ 0.11294422  0.01810236 -0.01049437  0.04389411], action=1, reward=1.0, next_state=[ 0.11330626  0.21337321 -0.00961649 -0.25208133]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 44 ] state=[ 0.11330626  0.21337321 -0.00961649 -0.25208133], action=1, reward=1.0, next_state=[ 0.11757373  0.40863115 -0.01465812 -0.5477819 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 45 ] state=[ 0.11757373  0.40863115 -0.01465812 -0.5477819 ], action=1, reward=1.0, next_state=[ 0.12574635  0.60395593 -0.02561376 -0.84504693]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 46 ] state=[ 0.12574635  0.60395593 -0.02561376 -0.84504693], action=0, reward=1.0, next_state=[ 0.13782547  0.40919266 -0.04251469 -0.56052747]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 47 ] state=[ 0.13782547  0.40919266 -0.04251469 -0.56052747], action=1, reward=1.0, next_state=[ 0.14600932  0.60488471 -0.05372524 -0.86629585]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 48 ] state=[ 0.14600932  0.60488471 -0.05372524 -0.86629585], action=0, reward=1.0, next_state=[ 0.15810702  0.41053347 -0.07105116 -0.59097724]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 49 ] state=[ 0.15810702  0.41053347 -0.07105116 -0.59097724], action=0, reward=1.0, next_state=[ 0.16631769  0.21647444 -0.08287071 -0.32149461]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 50 ] state=[ 0.16631769  0.21647444 -0.08287071 -0.32149461], action=0, reward=1.0, next_state=[ 0.17064717  0.02262438 -0.0893006  -0.05605352]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 51 ] state=[ 0.17064717  0.02262438 -0.0893006  -0.05605352], action=0, reward=1.0, next_state=[ 0.17109966 -0.17111119 -0.09042167  0.2071724 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 52 ] state=[ 0.17109966 -0.17111119 -0.09042167  0.2071724 ], action=0, reward=1.0, next_state=[ 0.16767744 -0.36483163 -0.08627822  0.47001819]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 53 ] state=[ 0.16767744 -0.36483163 -0.08627822  0.47001819], action=0, reward=1.0, next_state=[ 0.16038081 -0.55863563 -0.07687786  0.73430853]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 54 ] state=[ 0.16038081 -0.55863563 -0.07687786  0.73430853], action=1, reward=1.0, next_state=[ 0.14920809 -0.36254051 -0.06219169  0.41845481]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 55 ] state=[ 0.14920809 -0.36254051 -0.06219169  0.41845481], action=1, reward=1.0, next_state=[ 0.14195728 -0.16659494 -0.05382259  0.10683211]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 56 ] state=[ 0.14195728 -0.16659494 -0.05382259  0.10683211], action=1, reward=1.0, next_state=[ 0.13862538  0.02925534 -0.05168595 -0.2023341 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 57 ] state=[ 0.13862538  0.02925534 -0.05168595 -0.2023341 ], action=0, reward=1.0, next_state=[ 0.13921049 -0.16509078 -0.05573263  0.07360688]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 58 ] state=[ 0.13921049 -0.16509078 -0.05573263  0.07360688], action=1, reward=1.0, next_state=[ 0.13590867  0.03078404 -0.05426049 -0.23612607]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 59 ] state=[ 0.13590867  0.03078404 -0.05426049 -0.23612607], action=0, reward=1.0, next_state=[ 0.13652436 -0.1635224  -0.05898301  0.03895988]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 60 ] state=[ 0.13652436 -0.1635224  -0.05898301  0.03895988], action=0, reward=1.0, next_state=[ 0.13325391 -0.35775111 -0.05820381  0.31246535]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 61 ] state=[ 0.13325391 -0.35775111 -0.05820381  0.31246535], action=1, reward=1.0, next_state=[ 0.12609889 -0.16185036 -0.05195451  0.00200956]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 62 ] state=[ 0.12609889 -0.16185036 -0.05195451  0.00200956], action=1, reward=1.0, next_state=[ 0.12286188  0.03397671 -0.05191432 -0.30660245]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 63 ] state=[ 0.12286188  0.03397671 -0.05191432 -0.30660245], action=0, reward=1.0, next_state=[ 0.12354141 -0.16036852 -0.05804637 -0.03073331]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 64 ] state=[ 0.12354141 -0.16036852 -0.05804637 -0.03073331], action=0, reward=1.0, next_state=[ 0.12033404 -0.35461209 -0.05866103  0.24308527]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 65 ] state=[ 0.12033404 -0.35461209 -0.05866103  0.24308527], action=0, reward=1.0, next_state=[ 0.1132418  -0.54884921 -0.05379933  0.51670335]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 66 ] state=[ 0.1132418  -0.54884921 -0.05379933  0.51670335], action=0, reward=1.0, next_state=[ 0.10226482 -0.74317398 -0.04346526  0.7919594 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 67 ] state=[ 0.10226482 -0.74317398 -0.04346526  0.7919594 ], action=1, reward=1.0, next_state=[ 0.08740134 -0.54748308 -0.02762607  0.48592552]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 68 ] state=[ 0.08740134 -0.54748308 -0.02762607  0.48592552], action=1, reward=1.0, next_state=[ 0.07645167 -0.35198241 -0.01790756  0.18466538]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 69 ] state=[ 0.07645167 -0.35198241 -0.01790756  0.18466538], action=0, reward=1.0, next_state=[ 0.06941203 -0.54684362 -0.01421425  0.47164579]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 70 ] state=[ 0.06941203 -0.54684362 -0.01421425  0.47164579], action=1, reward=1.0, next_state=[ 0.05847515 -0.35152381 -0.00478134  0.17451683]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 71 ] state=[ 0.05847515 -0.35152381 -0.00478134  0.17451683], action=1, reward=1.0, next_state=[ 0.05144468 -0.15633376 -0.001291   -0.11967061]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 72 ] state=[ 0.05144468 -0.15633376 -0.001291   -0.11967061], action=1, reward=1.0, next_state=[ 0.048318    0.03880666 -0.00368441 -0.41276056]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 73 ] state=[ 0.048318    0.03880666 -0.00368441 -0.41276056], action=0, reward=1.0, next_state=[ 0.04909414 -0.15626287 -0.01193962 -0.12124146]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 74 ] state=[ 0.04909414 -0.15626287 -0.01193962 -0.12124146], action=0, reward=1.0, next_state=[ 0.04596888 -0.35121174 -0.01436445  0.16765084]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 75 ] state=[ 0.04596888 -0.35121174 -0.01436445  0.16765084], action=0, reward=1.0, next_state=[ 0.03894464 -0.54612516 -0.01101144  0.45576781]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 76 ] state=[ 0.03894464 -0.54612516 -0.01101144  0.45576781], action=1, reward=1.0, next_state=[ 0.02802214 -0.35084927 -0.00189608  0.15963443]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 77 ] state=[ 0.02802214 -0.35084927 -0.00189608  0.15963443], action=0, reward=1.0, next_state=[ 0.02100516 -0.54594402  0.00129661  0.45171859]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 78 ] state=[ 0.02100516 -0.54594402  0.00129661  0.45171859], action=0, reward=1.0, next_state=[ 0.01008627 -0.74108429  0.01033098  0.74480994]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 79 ] state=[ 0.01008627 -0.74108429  0.01033098  0.74480994], action=1, reward=1.0, next_state=[-0.00473541 -0.54610642  0.02522718  0.455396  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 80 ] state=[-0.00473541 -0.54610642  0.02522718  0.455396  ], action=1, reward=1.0, next_state=[-0.01565754 -0.35135007  0.0343351   0.17077043]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 81 ] state=[-0.01565754 -0.35135007  0.0343351   0.17077043], action=0, reward=1.0, next_state=[-0.02268454 -0.54694621  0.03775051  0.47408425]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 82 ] state=[-0.02268454 -0.54694621  0.03775051  0.47408425], action=0, reward=1.0, next_state=[-0.03362347 -0.74258038  0.04723219  0.77842244]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 83 ] state=[-0.03362347 -0.74258038  0.04723219  0.77842244], action=0, reward=1.0, next_state=[-0.04847507 -0.93831886  0.06280064  1.08558383]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 84 ] state=[-0.04847507 -0.93831886  0.06280064  1.08558383], action=0, reward=1.0, next_state=[-0.06724145 -1.13421049  0.08451232  1.39729328]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 85 ] state=[-0.06724145 -1.13421049  0.08451232  1.39729328], action=0, reward=1.0, next_state=[-0.08992566 -1.3302757   0.11245818  1.7151585 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 340 ][ timestamp 86 ] state=[-0.08992566 -1.3302757   0.11245818  1.7151585 ], action=0, reward=1.0, next_state=[-0.11653117 -1.52649401  0.14676135  2.04061983]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 87 ] state=[-0.11653117 -1.52649401  0.14676135  2.04061983], action=0, reward=1.0, next_state=[-0.14706105 -1.7227894   0.18757375  2.37489073]\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ][ timestamp 88 ] state=[-0.14706105 -1.7227894   0.18757375  2.37489073], action=0, reward=-1.0, next_state=[-0.18151684 -1.91901342  0.23507156  2.71888784]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 340: Exploration_rate=0.01. Score=88.\n",
      "[ episode 341 ] state=[ 0.02170645 -0.02133633 -0.03047621 -0.03430287]\n",
      "[ episode 341 ][ timestamp 1 ] state=[ 0.02170645 -0.02133633 -0.03047621 -0.03430287], action=0, reward=1.0, next_state=[ 0.02127972 -0.21600828 -0.03116227  0.24861085]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 2 ] state=[ 0.02127972 -0.21600828 -0.03116227  0.24861085], action=1, reward=1.0, next_state=[ 0.01695955 -0.02045548 -0.02619005 -0.05373617]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 3 ] state=[ 0.01695955 -0.02045548 -0.02619005 -0.05373617], action=1, reward=1.0, next_state=[ 0.01655044  0.17503202 -0.02726478 -0.35456586]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 4 ] state=[ 0.01655044  0.17503202 -0.02726478 -0.35456586], action=1, reward=1.0, next_state=[ 0.02005108  0.37053082 -0.03435609 -0.65571992]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 5 ] state=[ 0.02005108  0.37053082 -0.03435609 -0.65571992], action=1, reward=1.0, next_state=[ 0.0274617   0.56611381 -0.04747049 -0.95901998]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 6 ] state=[ 0.0274617   0.56611381 -0.04747049 -0.95901998], action=1, reward=1.0, next_state=[ 0.03878398  0.76184071 -0.06665089 -1.26623068]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 7 ] state=[ 0.03878398  0.76184071 -0.06665089 -1.26623068], action=0, reward=1.0, next_state=[ 0.05402079  0.56763065 -0.09197551 -0.99514327]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 8 ] state=[ 0.05402079  0.56763065 -0.09197551 -0.99514327], action=0, reward=1.0, next_state=[ 0.0653734   0.37385121 -0.11187837 -0.73270539]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 9 ] state=[ 0.0653734   0.37385121 -0.11187837 -0.73270539], action=0, reward=1.0, next_state=[ 0.07285043  0.18043833 -0.12653248 -0.47722352]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 10 ] state=[ 0.07285043  0.18043833 -0.12653248 -0.47722352], action=1, reward=1.0, next_state=[ 0.07645919  0.37709843 -0.13607695 -0.80695673]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 11 ] state=[ 0.07645919  0.37709843 -0.13607695 -0.80695673], action=0, reward=1.0, next_state=[ 0.08400116  0.18407775 -0.15221608 -0.55998546]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 12 ] state=[ 0.08400116  0.18407775 -0.15221608 -0.55998546], action=0, reward=1.0, next_state=[ 0.08768272 -0.00861698 -0.16341579 -0.31886433]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 13 ] state=[ 0.08768272 -0.00861698 -0.16341579 -0.31886433], action=0, reward=1.0, next_state=[ 0.08751038 -0.20108021 -0.16979308 -0.08184636]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 14 ] state=[ 0.08751038 -0.20108021 -0.16979308 -0.08184636], action=0, reward=1.0, next_state=[ 0.08348877 -0.39341288 -0.17143001  0.1528243 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 15 ] state=[ 0.08348877 -0.39341288 -0.17143001  0.1528243 ], action=0, reward=1.0, next_state=[ 0.07562052 -0.58571855 -0.16837352  0.38690062]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 16 ] state=[ 0.07562052 -0.58571855 -0.16837352  0.38690062], action=0, reward=1.0, next_state=[ 0.06390615 -0.77810032 -0.16063551  0.62212421]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 17 ] state=[ 0.06390615 -0.77810032 -0.16063551  0.62212421], action=1, reward=1.0, next_state=[ 0.04834414 -0.58114313 -0.14819302  0.28346791]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 18 ] state=[ 0.04834414 -0.58114313 -0.14819302  0.28346791], action=0, reward=1.0, next_state=[ 0.03672128 -0.77387489 -0.14252367  0.52598743]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 19 ] state=[ 0.03672128 -0.77387489 -0.14252367  0.52598743], action=0, reward=1.0, next_state=[ 0.02124378 -0.96673403 -0.13200392  0.77058272]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 20 ] state=[ 0.02124378 -0.96673403 -0.13200392  0.77058272], action=1, reward=1.0, next_state=[ 0.0019091  -0.77006641 -0.11659226  0.43945123]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 21 ] state=[ 0.0019091  -0.77006641 -0.11659226  0.43945123], action=0, reward=1.0, next_state=[-0.01349223 -0.9633619  -0.10780324  0.69322546]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 22 ] state=[-0.01349223 -0.9633619  -0.10780324  0.69322546], action=0, reward=1.0, next_state=[-0.03275947 -1.1568363  -0.09393873  0.95011954]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 23 ] state=[-0.03275947 -1.1568363  -0.09393873  0.95011954], action=1, reward=1.0, next_state=[-0.05589619 -0.960584   -0.07493634  0.62946162]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 24 ] state=[-0.05589619 -0.960584   -0.07493634  0.62946162], action=0, reward=1.0, next_state=[-0.07510787 -1.15458465 -0.06234711  0.89763526]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 25 ] state=[-0.07510787 -1.15458465 -0.06234711  0.89763526], action=1, reward=1.0, next_state=[-0.09819957 -0.9586755  -0.0443944   0.58602431]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 26 ] state=[-0.09819957 -0.9586755  -0.0443944   0.58602431], action=1, reward=1.0, next_state=[-0.11737308 -0.76296077 -0.03267392  0.27969381]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 27 ] state=[-0.11737308 -0.76296077 -0.03267392  0.27969381], action=1, reward=1.0, next_state=[-0.13263229 -0.56738833 -0.02708004 -0.0231127 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 28 ] state=[-0.13263229 -0.56738833 -0.02708004 -0.0231127 ], action=1, reward=1.0, next_state=[-0.14398006 -0.3718887  -0.02754229 -0.32421518]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 29 ] state=[-0.14398006 -0.3718887  -0.02754229 -0.32421518], action=1, reward=1.0, next_state=[-0.15141783 -0.17638563 -0.0340266  -0.62545498]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 30 ] state=[-0.15141783 -0.17638563 -0.0340266  -0.62545498], action=0, reward=1.0, next_state=[-0.15494555 -0.37101647 -0.0465357  -0.34367961]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 31 ] state=[-0.15494555 -0.37101647 -0.0465357  -0.34367961], action=1, reward=1.0, next_state=[-0.16236587 -0.17526444 -0.05340929 -0.65066632]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 32 ] state=[-0.16236587 -0.17526444 -0.05340929 -0.65066632], action=0, reward=1.0, next_state=[-0.16587116 -0.36960341 -0.06642261 -0.37526841]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 33 ] state=[-0.16587116 -0.36960341 -0.06642261 -0.37526841], action=0, reward=1.0, next_state=[-0.17326323 -0.56372211 -0.07392798 -0.10424635]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 34 ] state=[-0.17326323 -0.56372211 -0.07392798 -0.10424635], action=0, reward=1.0, next_state=[-0.18453767 -0.75771103 -0.07601291  0.16422719]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 35 ] state=[-0.18453767 -0.75771103 -0.07601291  0.16422719], action=1, reward=1.0, next_state=[-0.19969189 -0.56158789 -0.07272837 -0.15143431]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 36 ] state=[-0.19969189 -0.56158789 -0.07272837 -0.15143431], action=0, reward=1.0, next_state=[-0.21092365 -0.75559715 -0.07575705  0.11744699]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 37 ] state=[-0.21092365 -0.75559715 -0.07575705  0.11744699], action=0, reward=1.0, next_state=[-0.2260356  -0.94955651 -0.07340811  0.38530028]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 38 ] state=[-0.2260356  -0.94955651 -0.07340811  0.38530028], action=0, reward=1.0, next_state=[-0.24502673 -1.14356373 -0.06570211  0.65396475]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 39 ] state=[-0.24502673 -1.14356373 -0.06570211  0.65396475], action=1, reward=1.0, next_state=[-0.267898   -0.94759143 -0.05262281  0.34133802]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 40 ] state=[-0.267898   -0.94759143 -0.05262281  0.34133802], action=1, reward=1.0, next_state=[-0.28684983 -0.7517618  -0.04579605  0.03253623]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 41 ] state=[-0.28684983 -0.7517618  -0.04579605  0.03253623], action=0, reward=1.0, next_state=[-0.30188506 -0.94619812 -0.04514533  0.31042559]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 42 ] state=[-0.30188506 -0.94619812 -0.04514533  0.31042559], action=1, reward=1.0, next_state=[-0.32080903 -0.75046301 -0.03893682  0.00385385]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 43 ] state=[-0.32080903 -0.75046301 -0.03893682  0.00385385], action=1, reward=1.0, next_state=[-0.33581829 -0.5548049  -0.03885974 -0.30085539]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 44 ] state=[-0.33581829 -0.5548049  -0.03885974 -0.30085539], action=1, reward=1.0, next_state=[-0.34691439 -0.35915125 -0.04487685 -0.6055362 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 45 ] state=[-0.34691439 -0.35915125 -0.04487685 -0.6055362 ], action=0, reward=1.0, next_state=[-0.35409741 -0.55361787 -0.05698757 -0.32731931]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 341 ][ timestamp 46 ] state=[-0.35409741 -0.55361787 -0.05698757 -0.32731931], action=0, reward=1.0, next_state=[-0.36516977 -0.74788415 -0.06353396 -0.05313822]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 47 ] state=[-0.36516977 -0.74788415 -0.06353396 -0.05313822], action=0, reward=1.0, next_state=[-0.38012745 -0.94204029 -0.06459672  0.21884197]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 48 ] state=[-0.38012745 -0.94204029 -0.06459672  0.21884197], action=0, reward=1.0, next_state=[-0.39896826 -1.13618222 -0.06021988  0.49046928]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 49 ] state=[-0.39896826 -1.13618222 -0.06021988  0.49046928], action=0, reward=1.0, next_state=[-0.4216919  -1.33040527 -0.0504105   0.76358181]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 50 ] state=[-0.4216919  -1.33040527 -0.0504105   0.76358181], action=1, reward=1.0, next_state=[-0.44830001 -1.13462662 -0.03513886  0.45547249]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 51 ] state=[-0.44830001 -1.13462662 -0.03513886  0.45547249], action=0, reward=1.0, next_state=[-0.47099254 -1.32923458 -0.02602941  0.73687553]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 52 ] state=[-0.47099254 -1.32923458 -0.02602941  0.73687553], action=0, reward=1.0, next_state=[-0.49757723 -1.52398753 -0.0112919   1.02125422]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 53 ] state=[-0.49757723 -1.52398753 -0.0112919   1.02125422], action=1, reward=1.0, next_state=[-0.52805698 -1.32871696  0.00913319  0.72504729]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 54 ] state=[-0.52805698 -1.32871696  0.00913319  0.72504729], action=1, reward=1.0, next_state=[-0.55463132 -1.13372249  0.02363413  0.43525291]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 55 ] state=[-0.55463132 -1.13372249  0.02363413  0.43525291], action=1, reward=1.0, next_state=[-0.57730577 -0.93894296  0.03233919  0.15011298]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 56 ] state=[-0.57730577 -0.93894296  0.03233919  0.15011298], action=1, reward=1.0, next_state=[-0.59608463 -0.74429865  0.03534145 -0.13219475]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 57 ] state=[-0.59608463 -0.74429865  0.03534145 -0.13219475], action=1, reward=1.0, next_state=[-0.6109706  -0.5497003   0.03269755 -0.41352178]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 58 ] state=[-0.6109706  -0.5497003   0.03269755 -0.41352178], action=0, reward=1.0, next_state=[-0.62196461 -0.7452701   0.02442712 -0.11071251]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 59 ] state=[-0.62196461 -0.7452701   0.02442712 -0.11071251], action=1, reward=1.0, next_state=[-0.63687001 -0.55050654  0.02221287 -0.39558984]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 60 ] state=[-0.63687001 -0.55050654  0.02221287 -0.39558984], action=0, reward=1.0, next_state=[-0.64788014 -0.74593651  0.01430107 -0.09598716]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 61 ] state=[-0.64788014 -0.74593651  0.01430107 -0.09598716], action=1, reward=1.0, next_state=[-0.66279887 -0.55102241  0.01238133 -0.38412403]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 62 ] state=[-0.66279887 -0.55102241  0.01238133 -0.38412403], action=0, reward=1.0, next_state=[-0.67381932 -0.74631794  0.00469885 -0.08756318]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 63 ] state=[-0.67381932 -0.74631794  0.00469885 -0.08756318], action=0, reward=1.0, next_state=[-0.68874568 -0.94150693  0.00294758  0.20659853]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 64 ] state=[-0.68874568 -0.94150693  0.00294758  0.20659853], action=0, reward=1.0, next_state=[-0.70757582 -1.13667091  0.00707955  0.50020981]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 65 ] state=[-0.70757582 -1.13667091  0.00707955  0.50020981], action=1, reward=1.0, next_state=[-0.73030923 -0.94164947  0.01708375  0.20976636]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 66 ] state=[-0.73030923 -0.94164947  0.01708375  0.20976636], action=1, reward=1.0, next_state=[-0.74914222 -0.7467759   0.02127908 -0.07747896]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 67 ] state=[-0.74914222 -0.7467759   0.02127908 -0.07747896], action=0, reward=1.0, next_state=[-0.76407774 -0.94219634  0.0197295   0.22184091]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 68 ] state=[-0.76407774 -0.94219634  0.0197295   0.22184091], action=1, reward=1.0, next_state=[-0.78292167 -0.74736187  0.02416632 -0.06455383]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 69 ] state=[-0.78292167 -0.74736187  0.02416632 -0.06455383], action=0, reward=1.0, next_state=[-0.79786891 -0.94282181  0.02287524  0.23565469]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 70 ] state=[-0.79786891 -0.94282181  0.02287524  0.23565469], action=1, reward=1.0, next_state=[-0.81672534 -0.74803403  0.02758833 -0.04972581]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 71 ] state=[-0.81672534 -0.74803403  0.02758833 -0.04972581], action=0, reward=1.0, next_state=[-0.83168602 -0.94354048  0.02659382  0.25153222]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 72 ] state=[-0.83168602 -0.94354048  0.02659382  0.25153222], action=1, reward=1.0, next_state=[-0.85055683 -0.74880818  0.03162446 -0.03264529]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 73 ] state=[-0.85055683 -0.74880818  0.03162446 -0.03264529], action=0, reward=1.0, next_state=[-0.865533   -0.94436903  0.03097156  0.26984536]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 74 ] state=[-0.865533   -0.94436903  0.03097156  0.26984536], action=0, reward=1.0, next_state=[-0.88442038 -1.13991896  0.03636846  0.57213375]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 75 ] state=[-0.88442038 -1.13991896  0.03636846  0.57213375], action=1, reward=1.0, next_state=[-0.90721876 -0.94532534  0.04781114  0.29112631]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 76 ] state=[-0.90721876 -0.94532534  0.04781114  0.29112631], action=0, reward=1.0, next_state=[-0.92612526 -1.14109525  0.05363366  0.59849673]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 77 ] state=[-0.92612526 -1.14109525  0.05363366  0.59849673], action=0, reward=1.0, next_state=[-0.94894717 -1.336925    0.0656036   0.90757971]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 78 ] state=[-0.94894717 -1.336925    0.0656036   0.90757971], action=1, reward=1.0, next_state=[-0.97568567 -1.14274958  0.08375519  0.63621675]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 79 ] state=[-0.97568567 -1.14274958  0.08375519  0.63621675], action=0, reward=1.0, next_state=[-0.99854066 -1.3389336   0.09647953  0.95405646]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 80 ] state=[-0.99854066 -1.3389336   0.09647953  0.95405646], action=0, reward=1.0, next_state=[-1.02531933 -1.53521182  0.11556066  1.2754256 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 81 ] state=[-1.02531933 -1.53521182  0.11556066  1.2754256 ], action=0, reward=1.0, next_state=[-1.05602357 -1.73160233  0.14106917  1.60194582]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 82 ] state=[-1.05602357 -1.73160233  0.14106917  1.60194582], action=0, reward=1.0, next_state=[-1.09065561 -1.92808446  0.17310809  1.93507821]\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ][ timestamp 83 ] state=[-1.09065561 -1.92808446  0.17310809  1.93507821], action=0, reward=-1.0, next_state=[-1.1292173  -2.12458391  0.21180965  2.27606211]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 341: Exploration_rate=0.01. Score=83.\n",
      "[ episode 342 ] state=[-0.01045868 -0.01736578 -0.01823658 -0.00295807]\n",
      "[ episode 342 ][ timestamp 1 ] state=[-0.01045868 -0.01736578 -0.01823658 -0.00295807], action=0, reward=1.0, next_state=[-0.010806   -0.2122215  -0.01829574  0.28391566]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 2 ] state=[-0.010806   -0.2122215  -0.01829574  0.28391566], action=1, reward=1.0, next_state=[-0.01505043 -0.01684345 -0.01261743 -0.01448102]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 3 ] state=[-0.01505043 -0.01684345 -0.01261743 -0.01448102], action=0, reward=1.0, next_state=[-0.0153873  -0.2117822  -0.01290705  0.2741944 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 4 ] state=[-0.0153873  -0.2117822  -0.01290705  0.2741944 ], action=1, reward=1.0, next_state=[-0.01962294 -0.01647849 -0.00742316 -0.02253133]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 5 ] state=[-0.01962294 -0.01647849 -0.00742316 -0.02253133], action=0, reward=1.0, next_state=[-0.01995251 -0.2114932  -0.00787379  0.26780029]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 6 ] state=[-0.01995251 -0.2114932  -0.00787379  0.26780029], action=1, reward=1.0, next_state=[-0.02418238 -0.01625977 -0.00251778 -0.02735565]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 7 ] state=[-0.02418238 -0.01625977 -0.00251778 -0.02735565], action=0, reward=1.0, next_state=[-0.02450757 -0.21134552 -0.0030649   0.26453183]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 8 ] state=[-0.02450757 -0.21134552 -0.0030649   0.26453183], action=1, reward=1.0, next_state=[-0.02873448 -0.01617996  0.00222574 -0.02911622]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 9 ] state=[-0.02873448 -0.01617996  0.00222574 -0.02911622], action=1, reward=1.0, next_state=[-0.02905808  0.17891     0.00164341 -0.32109607]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 10 ] state=[-0.02905808  0.17891     0.00164341 -0.32109607], action=1, reward=1.0, next_state=[-0.02547988  0.37400851 -0.00477851 -0.61326028]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 11 ] state=[-0.02547988  0.37400851 -0.00477851 -0.61326028], action=0, reward=1.0, next_state=[-0.01799971  0.17895366 -0.01704371 -0.32208622]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 342 ][ timestamp 12 ] state=[-0.01799971  0.17895366 -0.01704371 -0.32208622], action=0, reward=1.0, next_state=[-0.01442064 -0.01592149 -0.02348544 -0.03482656]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 13 ] state=[-0.01442064 -0.01592149 -0.02348544 -0.03482656], action=1, reward=1.0, next_state=[-0.01473907  0.17952924 -0.02418197 -0.33482589]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 14 ] state=[-0.01473907  0.17952924 -0.02418197 -0.33482589], action=1, reward=1.0, next_state=[-0.01114848  0.37498685 -0.03087849 -0.6350354 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 15 ] state=[-0.01114848  0.37498685 -0.03087849 -0.6350354 ], action=0, reward=1.0, next_state=[-0.00364875  0.1803089  -0.04357919 -0.35223449]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 16 ] state=[-0.00364875  0.1803089  -0.04357919 -0.35223449], action=1, reward=1.0, next_state=[-4.25686164e-05  3.76022593e-01 -5.06238831e-02 -6.58334544e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 17 ] state=[-4.25686164e-05  3.76022593e-01 -5.06238831e-02 -6.58334544e-01], action=1, reward=1.0, next_state=[ 0.00747788  0.57181125 -0.06379057 -0.96651835]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 18 ] state=[ 0.00747788  0.57181125 -0.06379057 -0.96651835], action=0, reward=1.0, next_state=[ 0.01891411  0.37760131 -0.08312094 -0.69453768]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 19 ] state=[ 0.01891411  0.37760131 -0.08312094 -0.69453768], action=0, reward=1.0, next_state=[ 0.02646613  0.18372467 -0.09701169 -0.42913619]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 20 ] state=[ 0.02646613  0.18372467 -0.09701169 -0.42913619], action=0, reward=1.0, next_state=[ 0.03014063 -0.00989919 -0.10559442 -0.16854274]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 21 ] state=[ 0.03014063 -0.00989919 -0.10559442 -0.16854274], action=0, reward=1.0, next_state=[ 0.02994264 -0.20336366 -0.10896527  0.0890505 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 22 ] state=[ 0.02994264 -0.20336366 -0.10896527  0.0890505 ], action=0, reward=1.0, next_state=[ 0.02587537 -0.39676871 -0.10718426  0.34546506]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 23 ] state=[ 0.02587537 -0.39676871 -0.10718426  0.34546506], action=0, reward=1.0, next_state=[ 0.01794    -0.59021569 -0.10027496  0.60251845]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 24 ] state=[ 0.01794    -0.59021569 -0.10027496  0.60251845], action=0, reward=1.0, next_state=[ 0.00613568 -0.78380267 -0.08822459  0.86200879]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 25 ] state=[ 0.00613568 -0.78380267 -0.08822459  0.86200879], action=0, reward=1.0, next_state=[-0.00954037 -0.97761969 -0.07098442  1.1256992 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 26 ] state=[-0.00954037 -0.97761969 -0.07098442  1.1256992 ], action=1, reward=1.0, next_state=[-0.02909276 -0.78164295 -0.04847043  0.8116225 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 27 ] state=[-0.02909276 -0.78164295 -0.04847043  0.8116225 ], action=0, reward=1.0, next_state=[-0.04472562 -0.97606859 -0.03223798  1.08867372]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 28 ] state=[-0.04472562 -0.97606859 -0.03223798  1.08867372], action=0, reward=1.0, next_state=[-0.064247   -1.17075099 -0.01046451  1.37106927]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 29 ] state=[-0.064247   -1.17075099 -0.01046451  1.37106927], action=1, reward=1.0, next_state=[-0.08766201 -0.97549973  0.01695688  1.0751319 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 30 ] state=[-0.08766201 -0.97549973  0.01695688  1.0751319 ], action=0, reward=1.0, next_state=[-0.10717201 -1.17084159  0.03845951  1.37308765]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 31 ] state=[-0.10717201 -1.17084159  0.03845951  1.37308765], action=1, reward=1.0, next_state=[-0.13058884 -0.97622099  0.06592127  1.09267693]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 32 ] state=[-0.13058884 -0.97622099  0.06592127  1.09267693], action=1, reward=1.0, next_state=[-0.15011326 -0.78202664  0.08777481  0.82138492]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 33 ] state=[-0.15011326 -0.78202664  0.08777481  0.82138492], action=1, reward=1.0, next_state=[-0.16575379 -0.58820833  0.1042025   0.55754935]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 34 ] state=[-0.16575379 -0.58820833  0.1042025   0.55754935], action=1, reward=1.0, next_state=[-0.17751796 -0.39469165  0.11535349  0.29942896]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 35 ] state=[-0.17751796 -0.39469165  0.11535349  0.29942896], action=1, reward=1.0, next_state=[-0.18541179 -0.20138671  0.12134207  0.04523732]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 36 ] state=[-0.18541179 -0.20138671  0.12134207  0.04523732], action=1, reward=1.0, next_state=[-0.18943953 -0.00819483  0.12224682 -0.20683263]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 37 ] state=[-0.18943953 -0.00819483  0.12224682 -0.20683263], action=1, reward=1.0, next_state=[-0.18960342  0.18498624  0.11811016 -0.4585906 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 38 ] state=[-0.18960342  0.18498624  0.11811016 -0.4585906 ], action=0, reward=1.0, next_state=[-0.1859037  -0.01158998  0.10893835 -0.13113685]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 39 ] state=[-0.1859037  -0.01158998  0.10893835 -0.13113685], action=1, reward=1.0, next_state=[-0.1861355   0.18181651  0.10631562 -0.38756228]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 40 ] state=[-0.1861355   0.18181651  0.10631562 -0.38756228], action=0, reward=1.0, next_state=[-0.18249917 -0.01464128  0.09856437 -0.06334151]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 41 ] state=[-0.18249917 -0.01464128  0.09856437 -0.06334151], action=1, reward=1.0, next_state=[-0.18279199  0.17893948  0.09729754 -0.32337228]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 42 ] state=[-0.18279199  0.17893948  0.09729754 -0.32337228], action=1, reward=1.0, next_state=[-0.17921321  0.37255105  0.09083009 -0.5838557 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 43 ] state=[-0.17921321  0.37255105  0.09083009 -0.5838557 ], action=0, reward=1.0, next_state=[-0.17176218  0.17628193  0.07915298 -0.26399828]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 44 ] state=[-0.17176218  0.17628193  0.07915298 -0.26399828], action=1, reward=1.0, next_state=[-0.16823655  0.37019012  0.07387301 -0.53070319]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 45 ] state=[-0.16823655  0.37019012  0.07387301 -0.53070319], action=1, reward=1.0, next_state=[-0.16083274  0.56419943  0.06325895 -0.79922453]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 46 ] state=[-0.16083274  0.56419943  0.06325895 -0.79922453], action=0, reward=1.0, next_state=[-0.14954875  0.36826936  0.04727446 -0.48733154]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 47 ] state=[-0.14954875  0.36826936  0.04727446 -0.48733154], action=0, reward=1.0, next_state=[-0.14218337  0.17251337  0.03752783 -0.1801321 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 48 ] state=[-0.14218337  0.17251337  0.03752783 -0.1801321 ], action=1, reward=1.0, next_state=[-0.1387331   0.36707877  0.03392519 -0.46074412]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 49 ] state=[-0.1387331   0.36707877  0.03392519 -0.46074412], action=1, reward=1.0, next_state=[-0.13139153  0.56170519  0.0247103  -0.74254369]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 50 ] state=[-0.13139153  0.56170519  0.0247103  -0.74254369], action=1, reward=1.0, next_state=[-0.12015742  0.7564775   0.00985943 -1.02734886]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 51 ] state=[-0.12015742  0.7564775   0.00985943 -1.02734886], action=0, reward=1.0, next_state=[-0.10502787  0.5612257  -0.01068755 -0.73158677]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 52 ] state=[-0.10502787  0.5612257  -0.01068755 -0.73158677], action=0, reward=1.0, next_state=[-0.09380336  0.36625306 -0.02531928 -0.44228659]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 53 ] state=[-0.09380336  0.36625306 -0.02531928 -0.44228659], action=1, reward=1.0, next_state=[-0.0864783   0.56172397 -0.03416501 -0.74284206]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 342 ][ timestamp 54 ] state=[-0.0864783   0.56172397 -0.03416501 -0.74284206], action=0, reward=1.0, next_state=[-0.07524382  0.36708984 -0.04902185 -0.4611038 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 55 ] state=[-0.07524382  0.36708984 -0.04902185 -0.4611038 ], action=0, reward=1.0, next_state=[-0.06790202  0.1726938  -0.05824393 -0.18426669]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 56 ] state=[-0.06790202  0.1726938  -0.05824393 -0.18426669], action=0, reward=1.0, next_state=[-0.06444814 -0.0215485  -0.06192926  0.08948866]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 57 ] state=[-0.06444814 -0.0215485  -0.06192926  0.08948866], action=1, reward=1.0, next_state=[-0.06487911  0.17440393 -0.06013949 -0.2220721 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 58 ] state=[-0.06487911  0.17440393 -0.06013949 -0.2220721 ], action=0, reward=1.0, next_state=[-0.06139104 -0.01980916 -0.06458093  0.05105054]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 59 ] state=[-0.06139104 -0.01980916 -0.06458093  0.05105054], action=1, reward=1.0, next_state=[-0.06178722  0.17617651 -0.06355992 -0.26128873]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 60 ] state=[-0.06178722  0.17617651 -0.06355992 -0.26128873], action=1, reward=1.0, next_state=[-0.05826369  0.37214547 -0.0687857  -0.57332264]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 61 ] state=[-0.05826369  0.37214547 -0.0687857  -0.57332264], action=0, reward=1.0, next_state=[-0.05082078  0.17805195 -0.08025215 -0.3030779 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 62 ] state=[-0.05082078  0.17805195 -0.08025215 -0.3030779 ], action=0, reward=1.0, next_state=[-0.04725974 -0.01584002 -0.08631371 -0.03674483]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 63 ] state=[-0.04725974 -0.01584002 -0.08631371 -0.03674483], action=0, reward=1.0, next_state=[-0.04757654 -0.20962498 -0.0870486   0.22750578]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 64 ] state=[-0.04757654 -0.20962498 -0.0870486   0.22750578], action=1, reward=1.0, next_state=[-0.05176904 -0.01337385 -0.08249849 -0.09131629]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 65 ] state=[-0.05176904 -0.01337385 -0.08249849 -0.09131629], action=1, reward=1.0, next_state=[-0.05203652  0.18282773 -0.08432481 -0.40884478]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 66 ] state=[-0.05203652  0.18282773 -0.08432481 -0.40884478], action=0, reward=1.0, next_state=[-0.04837996 -0.01100373 -0.09250171 -0.1438928 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 67 ] state=[-0.04837996 -0.01100373 -0.09250171 -0.1438928 ], action=0, reward=1.0, next_state=[-0.04860004 -0.20468757 -0.09537957  0.11823416]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 68 ] state=[-0.04860004 -0.20468757 -0.09537957  0.11823416], action=1, reward=1.0, next_state=[-0.05269379 -0.00833762 -0.09301488 -0.20295119]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 69 ] state=[-0.05269379 -0.00833762 -0.09301488 -0.20295119], action=0, reward=1.0, next_state=[-0.05286054 -0.20201476 -0.09707391  0.05900173]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 70 ] state=[-0.05286054 -0.20201476 -0.09707391  0.05900173], action=0, reward=1.0, next_state=[-0.05690084 -0.39562051 -0.09589387  0.3195482 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 71 ] state=[-0.05690084 -0.39562051 -0.09589387  0.3195482 ], action=1, reward=1.0, next_state=[-0.06481325 -0.19927297 -0.08950291 -0.00176962]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 72 ] state=[-0.06481325 -0.19927297 -0.08950291 -0.00176962], action=1, reward=1.0, next_state=[-0.06879871 -0.002989   -0.0895383  -0.3212958 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 73 ] state=[-0.06879871 -0.002989   -0.0895383  -0.3212958 ], action=1, reward=1.0, next_state=[-0.06885849  0.19328637 -0.09596422 -0.64081857]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 74 ] state=[-0.06885849  0.19328637 -0.09596422 -0.64081857], action=1, reward=1.0, next_state=[-0.06499276  0.38960589 -0.10878059 -0.96211313]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 75 ] state=[-0.06499276  0.38960589 -0.10878059 -0.96211313], action=0, reward=1.0, next_state=[-0.05720064  0.19610057 -0.12802285 -0.70548927]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 76 ] state=[-0.05720064  0.19610057 -0.12802285 -0.70548927], action=0, reward=1.0, next_state=[-0.05327863  0.00296302 -0.14213264 -0.45568981]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 77 ] state=[-0.05327863  0.00296302 -0.14213264 -0.45568981], action=0, reward=1.0, next_state=[-0.05321937 -0.18989342 -0.15124643 -0.2109687 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 78 ] state=[-0.05321937 -0.18989342 -0.15124643 -0.2109687 ], action=0, reward=1.0, next_state=[-0.05701724 -0.3825657  -0.15546581  0.0304433 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 79 ] state=[-0.05701724 -0.3825657  -0.15546581  0.0304433 ], action=0, reward=1.0, next_state=[-0.06466855 -0.57515607 -0.15485694  0.27032172]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 80 ] state=[-0.06466855 -0.57515607 -0.15485694  0.27032172], action=0, reward=1.0, next_state=[-0.07617167 -0.7677682  -0.14945051  0.5104364 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 81 ] state=[-0.07617167 -0.7677682  -0.14945051  0.5104364 ], action=0, reward=1.0, next_state=[-0.09152704 -0.96050378 -0.13924178  0.75254209]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 82 ] state=[-0.09152704 -0.96050378 -0.13924178  0.75254209], action=1, reward=1.0, next_state=[-0.11073711 -0.7637646  -0.12419094  0.4194846 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 83 ] state=[-0.11073711 -0.7637646  -0.12419094  0.4194846 ], action=1, reward=1.0, next_state=[-0.1260124  -0.56712187 -0.11580124  0.0903739 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 84 ] state=[-0.1260124  -0.56712187 -0.11580124  0.0903739 ], action=0, reward=1.0, next_state=[-0.13735484 -0.76040998 -0.11399377  0.34439473]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 85 ] state=[-0.13735484 -0.76040998 -0.11399377  0.34439473], action=0, reward=1.0, next_state=[-0.15256304 -0.95374136 -0.10710587  0.599068  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 86 ] state=[-0.15256304 -0.95374136 -0.10710587  0.599068  ], action=0, reward=1.0, next_state=[-0.17163787 -1.14721462 -0.09512451  0.85618594]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 87 ] state=[-0.17163787 -1.14721462 -0.09512451  0.85618594], action=0, reward=1.0, next_state=[-0.19458216 -1.34092063 -0.07800079  1.1175069 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 88 ] state=[-0.19458216 -1.34092063 -0.07800079  1.1175069 ], action=0, reward=1.0, next_state=[-0.22140057 -1.5349372  -0.05565065  1.38473792]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 89 ] state=[-0.22140057 -1.5349372  -0.05565065  1.38473792], action=1, reward=1.0, next_state=[-0.25209932 -1.33916716 -0.0279559   1.07518461]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 90 ] state=[-0.25209932 -1.33916716 -0.0279559   1.07518461], action=1, reward=1.0, next_state=[-0.27888266 -1.14368719 -0.0064522   0.77386126]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 91 ] state=[-0.27888266 -1.14368719 -0.0064522   0.77386126], action=1, reward=1.0, next_state=[-0.3017564  -0.94847707  0.00902502  0.47915524]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 92 ] state=[-0.3017564  -0.94847707  0.00902502  0.47915524], action=1, reward=1.0, next_state=[-0.32072595 -0.75348368  0.01860813  0.1893304 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 93 ] state=[-0.32072595 -0.75348368  0.01860813  0.1893304 ], action=1, reward=1.0, next_state=[-0.33579562 -0.55863282  0.02239473 -0.09742482]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 94 ] state=[-0.33579562 -0.55863282  0.02239473 -0.09742482], action=1, reward=1.0, next_state=[-0.34696828 -0.36383888  0.02044624 -0.38295897]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 95 ] state=[-0.34696828 -0.36383888  0.02044624 -0.38295897], action=0, reward=1.0, next_state=[-0.35424505 -0.55924508  0.01278706 -0.08390016]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 96 ] state=[-0.35424505 -0.55924508  0.01278706 -0.08390016], action=0, reward=1.0, next_state=[-0.36542995 -0.75454797  0.01110906  0.21278952]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 97 ] state=[-0.36542995 -0.75454797  0.01110906  0.21278952], action=1, reward=1.0, next_state=[-0.38052091 -0.55958659  0.01536485 -0.0763685 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 98 ] state=[-0.38052091 -0.55958659  0.01536485 -0.0763685 ], action=1, reward=1.0, next_state=[-0.39171265 -0.36468824  0.01383748 -0.36416444]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 99 ] state=[-0.39171265 -0.36468824  0.01383748 -0.36416444], action=1, reward=1.0, next_state=[-0.39900641 -0.16976565  0.00655419 -0.65245223]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 100 ] state=[-0.39900641 -0.16976565  0.00655419 -0.65245223], action=1, reward=1.0, next_state=[-0.40240172  0.02526441 -0.00649486 -0.94306413]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 101 ] state=[-0.40240172  0.02526441 -0.00649486 -0.94306413], action=0, reward=1.0, next_state=[-0.40189644 -0.16976943 -0.02535614 -0.65242902]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 102 ] state=[-0.40189644 -0.16976943 -0.02535614 -0.65242902], action=0, reward=1.0, next_state=[-0.40529182 -0.36452926 -0.03840472 -0.36783708]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 103 ] state=[-0.40529182 -0.36452926 -0.03840472 -0.36783708], action=0, reward=1.0, next_state=[-0.41258241 -0.55908506 -0.04576146 -0.08750679]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 342 ][ timestamp 104 ] state=[-0.41258241 -0.55908506 -0.04576146 -0.08750679], action=1, reward=1.0, next_state=[-0.42376411 -0.36333805 -0.0475116  -0.3942691 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 105 ] state=[-0.42376411 -0.36333805 -0.0475116  -0.3942691 ], action=0, reward=1.0, next_state=[-0.43103087 -0.55775475 -0.05539698 -0.11693628]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 106 ] state=[-0.43103087 -0.55775475 -0.05539698 -0.11693628], action=0, reward=1.0, next_state=[-0.44218597 -0.75204101 -0.05773571  0.15776766]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 107 ] state=[-0.44218597 -0.75204101 -0.05773571  0.15776766], action=1, reward=1.0, next_state=[-0.45722679 -0.55614201 -0.05458035 -0.15255608]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 108 ] state=[-0.45722679 -0.55614201 -0.05458035 -0.15255608], action=1, reward=1.0, next_state=[-0.46834963 -0.36028274 -0.05763147 -0.46194615]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 109 ] state=[-0.46834963 -0.36028274 -0.05763147 -0.46194615], action=0, reward=1.0, next_state=[-0.47555528 -0.55454484 -0.0668704  -0.18797107]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 110 ] state=[-0.47555528 -0.55454484 -0.0668704  -0.18797107], action=0, reward=1.0, next_state=[-0.48664618 -0.74864951 -0.07062982  0.08288997]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 111 ] state=[-0.48664618 -0.74864951 -0.07062982  0.08288997], action=1, reward=1.0, next_state=[-0.50161917 -0.55258991 -0.06897202 -0.2312141 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 112 ] state=[-0.50161917 -0.55258991 -0.06897202 -0.2312141 ], action=1, reward=1.0, next_state=[-0.51267097 -0.35655366 -0.0735963  -0.54483103]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 113 ] state=[-0.51267097 -0.35655366 -0.0735963  -0.54483103], action=1, reward=1.0, next_state=[-0.51980204 -0.1604789  -0.08449292 -0.8597648 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 114 ] state=[-0.51980204 -0.1604789  -0.08449292 -0.8597648 ], action=0, reward=1.0, next_state=[-0.52301162 -0.35435468 -0.10168822 -0.59479994]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 115 ] state=[-0.52301162 -0.35435468 -0.10168822 -0.59479994], action=0, reward=1.0, next_state=[-0.53009871 -0.5479174  -0.11358422 -0.33580057]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 116 ] state=[-0.53009871 -0.5479174  -0.11358422 -0.33580057], action=0, reward=1.0, next_state=[-0.54105706 -0.7412551  -0.12030023 -0.08098475]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 117 ] state=[-0.54105706 -0.7412551  -0.12030023 -0.08098475], action=0, reward=1.0, next_state=[-0.55588216 -0.93446552 -0.12191992  0.17145327]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 118 ] state=[-0.55588216 -0.93446552 -0.12191992  0.17145327], action=0, reward=1.0, next_state=[-0.57457147 -1.12765065 -0.11849086  0.42332421]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 119 ] state=[-0.57457147 -1.12765065 -0.11849086  0.42332421], action=0, reward=1.0, next_state=[-0.59712449 -1.32091218 -0.11002437  0.67642898]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 120 ] state=[-0.59712449 -1.32091218 -0.11002437  0.67642898], action=0, reward=1.0, next_state=[-0.62354273 -1.5143473  -0.09649579  0.93254528]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 121 ] state=[-0.62354273 -1.5143473  -0.09649579  0.93254528], action=0, reward=1.0, next_state=[-0.65382967 -1.70804411 -0.07784489  1.1934131 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 122 ] state=[-0.65382967 -1.70804411 -0.07784489  1.1934131 ], action=0, reward=1.0, next_state=[-0.68799056 -1.90207636 -0.05397663  1.46071676]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 123 ] state=[-0.68799056 -1.90207636 -0.05397663  1.46071676], action=0, reward=1.0, next_state=[-0.72603208 -2.0964966  -0.02476229  1.73606099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 124 ] state=[-0.72603208 -2.0964966  -0.02476229  1.73606099], action=0, reward=1.0, next_state=[-0.76796202 -2.29132764  0.00995893  2.02093858]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 125 ] state=[-0.76796202 -2.29132764  0.00995893  2.02093858], action=0, reward=1.0, next_state=[-0.81378857 -2.48655131  0.0503777   2.31668743]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 126 ] state=[-0.81378857 -2.48655131  0.0503777   2.31668743], action=0, reward=1.0, next_state=[-0.8635196  -2.68209457  0.09671145  2.62443499]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 127 ] state=[-0.8635196  -2.68209457  0.09671145  2.62443499], action=0, reward=1.0, next_state=[-0.91716149 -2.87781249  0.14920015  2.94502887]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 128 ] state=[-0.91716149 -2.87781249  0.14920015  2.94502887], action=0, reward=1.0, next_state=[-0.97471774 -3.07346844  0.20810073  3.27895456]\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ][ timestamp 129 ] state=[-0.97471774 -3.07346844  0.20810073  3.27895456], action=0, reward=-1.0, next_state=[-1.03618711 -3.26871254  0.27367982  3.62624311]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 342: Exploration_rate=0.01. Score=129.\n",
      "[ episode 343 ] state=[-0.03448542 -0.02425851  0.03300848 -0.01344822]\n",
      "[ episode 343 ][ timestamp 1 ] state=[-0.03448542 -0.02425851  0.03300848 -0.01344822], action=0, reward=1.0, next_state=[-0.03497059 -0.21983792  0.03273952  0.28946381]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 2 ] state=[-0.03497059 -0.21983792  0.03273952  0.28946381], action=1, reward=1.0, next_state=[-0.03936735 -0.02519775  0.03852879  0.00728371]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 3 ] state=[-0.03936735 -0.02519775  0.03852879  0.00728371], action=1, reward=1.0, next_state=[-0.03987131  0.16935107  0.03867447 -0.27299828]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 4 ] state=[-0.03987131  0.16935107  0.03867447 -0.27299828], action=1, reward=1.0, next_state=[-0.03648428  0.36390046  0.0332145  -0.55323668]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 5 ] state=[-0.03648428  0.36390046  0.0332145  -0.55323668], action=0, reward=1.0, next_state=[-0.02920627  0.16832821  0.02214977 -0.25027685]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 6 ] state=[-0.02920627  0.16832821  0.02214977 -0.25027685], action=0, reward=1.0, next_state=[-0.02583971 -0.02710293  0.01714423  0.04930945]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 7 ] state=[-0.02583971 -0.02710293  0.01714423  0.04930945], action=1, reward=1.0, next_state=[-0.02638177  0.16776904  0.01813042 -0.2379154 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 8 ] state=[-0.02638177  0.16776904  0.01813042 -0.2379154 ], action=1, reward=1.0, next_state=[-0.02302639  0.36262735  0.01337211 -0.52482477]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 9 ] state=[-0.02302639  0.36262735  0.01337211 -0.52482477], action=1, reward=1.0, next_state=[-0.01577384  0.55755859  0.00287562 -0.8132642 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 10 ] state=[-0.01577384  0.55755859  0.00287562 -0.8132642 ], action=1, reward=1.0, next_state=[-0.00462267  0.75264104 -0.01338967 -1.10504124]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 11 ] state=[-0.00462267  0.75264104 -0.01338967 -1.10504124], action=0, reward=1.0, next_state=[ 0.01043015  0.5576977  -0.03549049 -0.81658888]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 12 ] state=[ 0.01043015  0.5576977  -0.03549049 -0.81658888], action=0, reward=1.0, next_state=[ 0.02158411  0.36307915 -0.05182227 -0.53527691]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 13 ] state=[ 0.02158411  0.36307915 -0.05182227 -0.53527691], action=0, reward=1.0, next_state=[ 0.02884569  0.16872276 -0.06252781 -0.25936262]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 14 ] state=[ 0.02884569  0.16872276 -0.06252781 -0.25936262], action=0, reward=1.0, next_state=[ 0.03222014 -0.02545341 -0.06771506  0.01296124]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 15 ] state=[ 0.03222014 -0.02545341 -0.06771506  0.01296124], action=1, reward=1.0, next_state=[ 0.03171108  0.17057103 -0.06745584 -0.30029457]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 16 ] state=[ 0.03171108  0.17057103 -0.06745584 -0.30029457], action=0, reward=1.0, next_state=[ 0.0351225  -0.02352785 -0.07346173 -0.0296254 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 343 ][ timestamp 17 ] state=[ 0.0351225  -0.02352785 -0.07346173 -0.0296254 ], action=1, reward=1.0, next_state=[ 0.03465194  0.17256651 -0.07405424 -0.34455193]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 18 ] state=[ 0.03465194  0.17256651 -0.07405424 -0.34455193], action=0, reward=1.0, next_state=[ 0.03810327 -0.02142814 -0.08094527 -0.07610954]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 19 ] state=[ 0.03810327 -0.02142814 -0.08094527 -0.07610954], action=0, reward=1.0, next_state=[ 0.03767471 -0.21530199 -0.08246747  0.18997711]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 20 ] state=[ 0.03767471 -0.21530199 -0.08246747  0.18997711], action=1, reward=1.0, next_state=[ 0.03336867 -0.01910301 -0.07866792 -0.12753916]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 21 ] state=[ 0.03336867 -0.01910301 -0.07866792 -0.12753916], action=1, reward=1.0, next_state=[ 0.03298661  0.17705262 -0.08121871 -0.44396714]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 22 ] state=[ 0.03298661  0.17705262 -0.08121871 -0.44396714], action=1, reward=1.0, next_state=[ 0.03652766  0.37322417 -0.09009805 -0.76110652]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 23 ] state=[ 0.03652766  0.37322417 -0.09009805 -0.76110652], action=0, reward=1.0, next_state=[ 0.04399214  0.17945131 -0.10532018 -0.49807917]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 24 ] state=[ 0.04399214  0.17945131 -0.10532018 -0.49807917], action=0, reward=1.0, next_state=[ 0.04758117 -0.01404032 -0.11528176 -0.24035687]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 25 ] state=[ 0.04758117 -0.01404032 -0.11528176 -0.24035687], action=0, reward=1.0, next_state=[ 0.04730036 -0.20734291 -0.1200889   0.01385461]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 26 ] state=[ 0.04730036 -0.20734291 -0.1200889   0.01385461], action=0, reward=1.0, next_state=[ 0.0431535  -0.40055624 -0.11981181  0.26636598]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 27 ] state=[ 0.0431535  -0.40055624 -0.11981181  0.26636598], action=0, reward=1.0, next_state=[ 0.03514238 -0.5937826  -0.11448449  0.51898724]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 28 ] state=[ 0.03514238 -0.5937826  -0.11448449  0.51898724], action=0, reward=1.0, next_state=[ 0.02326673 -0.78712239 -0.10410474  0.77351351]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 29 ] state=[ 0.02326673 -0.78712239 -0.10410474  0.77351351], action=1, reward=1.0, next_state=[ 0.00752428 -0.59073402 -0.08863447  0.44997429]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 30 ] state=[ 0.00752428 -0.59073402 -0.08863447  0.44997429], action=0, reward=1.0, next_state=[-0.0042904  -0.78449787 -0.07963499  0.71345471]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 31 ] state=[-0.0042904  -0.78449787 -0.07963499  0.71345471], action=1, reward=1.0, next_state=[-0.01998036 -0.58836898 -0.06536589  0.39680579]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 32 ] state=[-0.01998036 -0.58836898 -0.06536589  0.39680579], action=1, reward=1.0, next_state=[-0.03174774 -0.39238347 -0.05742978  0.08425145]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 33 ] state=[-0.03174774 -0.39238347 -0.05742978  0.08425145], action=1, reward=1.0, next_state=[-0.03959541 -0.19648733 -0.05574475 -0.2259834 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 34 ] state=[-0.03959541 -0.19648733 -0.05574475 -0.2259834 ], action=0, reward=1.0, next_state=[-0.04352515 -0.39077012 -0.06026442  0.04860764]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 35 ] state=[-0.04352515 -0.39077012 -0.06026442  0.04860764], action=0, reward=1.0, next_state=[-0.05134056 -0.58497847 -0.05929226  0.32168432]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 36 ] state=[-0.05134056 -0.58497847 -0.05929226  0.32168432], action=0, reward=1.0, next_state=[-0.06304013 -0.77920815 -0.05285858  0.59509516]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 37 ] state=[-0.06304013 -0.77920815 -0.05285858  0.59509516], action=0, reward=1.0, next_state=[-0.07862429 -0.97355201 -0.04095667  0.87067059]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 38 ] state=[-0.07862429 -0.97355201 -0.04095667  0.87067059], action=1, reward=1.0, next_state=[-0.09809533 -0.77789762 -0.02354326  0.56539723]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 39 ] state=[-0.09809533 -0.77789762 -0.02354326  0.56539723], action=0, reward=1.0, next_state=[-0.11365328 -0.97268149 -0.01223532  0.85057099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 40 ] state=[-0.11365328 -0.97268149 -0.01223532  0.85057099], action=0, reward=1.0, next_state=[-0.13310691 -1.16763449  0.0047761   1.1393815 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 41 ] state=[-0.13310691 -1.16763449  0.0047761   1.1393815 ], action=1, reward=1.0, next_state=[-0.1564596  -0.97257531  0.02756373  0.84820024]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 42 ] state=[-0.1564596  -0.97257531  0.02756373  0.84820024], action=1, reward=1.0, next_state=[-0.17591111 -0.77783994  0.04452774  0.56431085]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 43 ] state=[-0.17591111 -0.77783994  0.04452774  0.56431085], action=1, reward=1.0, next_state=[-0.19146791 -0.5833701   0.05581395  0.28598206]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 44 ] state=[-0.19146791 -0.5833701   0.05581395  0.28598206], action=1, reward=1.0, next_state=[-0.20313531 -0.38908673  0.0615336   0.0114116 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 45 ] state=[-0.20313531 -0.38908673  0.0615336   0.0114116 ], action=1, reward=1.0, next_state=[-0.21091704 -0.19489876  0.06176183 -0.26123962]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 46 ] state=[-0.21091704 -0.19489876  0.06176183 -0.26123962], action=0, reward=1.0, next_state=[-0.21481502 -0.39084549  0.05653703  0.0502665 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 47 ] state=[-0.21481502 -0.39084549  0.05653703  0.0502665 ], action=1, reward=1.0, next_state=[-0.22263193 -0.19657788  0.05754236 -0.22405627]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 48 ] state=[-0.22263193 -0.19657788  0.05754236 -0.22405627], action=0, reward=1.0, next_state=[-0.22656348 -0.39247303  0.05306124  0.08620823]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 49 ] state=[-0.22656348 -0.39247303  0.05306124  0.08620823], action=1, reward=1.0, next_state=[-0.23441295 -0.19815024  0.0547854  -0.18927304]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 50 ] state=[-0.23441295 -0.19815024  0.0547854  -0.18927304], action=0, reward=1.0, next_state=[-0.23837595 -0.39401143  0.05099994  0.12017682]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 51 ] state=[-0.23837595 -0.39401143  0.05099994  0.12017682], action=0, reward=1.0, next_state=[-0.24625618 -0.58982558  0.05340348  0.42850362]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 52 ] state=[-0.24625618 -0.58982558  0.05340348  0.42850362], action=1, reward=1.0, next_state=[-0.25805269 -0.39549901  0.06197355  0.15312248]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 53 ] state=[-0.25805269 -0.39549901  0.06197355  0.15312248], action=1, reward=1.0, next_state=[-0.26596267 -0.20131666  0.065036   -0.11938331]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 54 ] state=[-0.26596267 -0.20131666  0.065036   -0.11938331], action=1, reward=1.0, next_state=[-0.269989   -0.00718388  0.06264834 -0.39085974]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 55 ] state=[-0.269989   -0.00718388  0.06264834 -0.39085974], action=1, reward=1.0, next_state=[-0.27013268  0.18699559  0.05483114 -0.66315098]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 56 ] state=[-0.27013268  0.18699559  0.05483114 -0.66315098], action=0, reward=1.0, next_state=[-0.26639277 -0.0088446   0.04156812 -0.3537199 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 57 ] state=[-0.26639277 -0.0088446   0.04156812 -0.3537199 ], action=1, reward=1.0, next_state=[-0.26656966  0.18566239  0.03449372 -0.63301084]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 58 ] state=[-0.26656966  0.18566239  0.03449372 -0.63301084], action=1, reward=1.0, next_state=[-0.26285641  0.38028659  0.02183351 -0.91463433]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 59 ] state=[-0.26285641  0.38028659  0.02183351 -0.91463433], action=0, reward=1.0, next_state=[-0.25525068  0.18487624  0.00354082 -0.61517014]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 343 ][ timestamp 60 ] state=[-0.25525068  0.18487624  0.00354082 -0.61517014], action=0, reward=1.0, next_state=[-0.25155316 -0.010295   -0.00876258 -0.32137411]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 61 ] state=[-0.25155316 -0.010295   -0.00876258 -0.32137411], action=0, reward=1.0, next_state=[-0.25175906 -0.20529108 -0.01519006 -0.03146739]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 62 ] state=[-0.25175906 -0.20529108 -0.01519006 -0.03146739], action=0, reward=1.0, next_state=[-0.25586488 -0.40019194 -0.01581941  0.25638447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 63 ] state=[-0.25586488 -0.40019194 -0.01581941  0.25638447], action=0, reward=1.0, next_state=[-0.26386872 -0.5950845  -0.01069172  0.54403602]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 64 ] state=[-0.26386872 -0.5950845  -0.01069172  0.54403602], action=1, reward=1.0, next_state=[-2.75770407e-01 -3.99813947e-01  1.88997244e-04  2.48003625e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 65 ] state=[-2.75770407e-01 -3.99813947e-01  1.88997244e-04  2.48003625e-01], action=1, reward=1.0, next_state=[-0.28376669 -0.2046947   0.00514907 -0.04461968]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 66 ] state=[-0.28376669 -0.2046947   0.00514907 -0.04461968], action=1, reward=1.0, next_state=[-0.28786058 -0.00964696  0.00425668 -0.33567359]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 67 ] state=[-0.28786058 -0.00964696  0.00425668 -0.33567359], action=1, reward=1.0, next_state=[-0.28805352  0.18541416 -0.0024568  -0.62701115]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 68 ] state=[-0.28805352  0.18541416 -0.0024568  -0.62701115], action=1, reward=1.0, next_state=[-0.28434524  0.38057031 -0.01499702 -0.9204668 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 69 ] state=[-0.28434524  0.38057031 -0.01499702 -0.9204668 ], action=1, reward=1.0, next_state=[-0.27673383  0.5758917  -0.03340635 -1.2178249 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 70 ] state=[-0.27673383  0.5758917  -0.03340635 -1.2178249 ], action=0, reward=1.0, next_state=[-0.265216    0.38121605 -0.05776285 -0.93579399]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 71 ] state=[-0.265216    0.38121605 -0.05776285 -0.93579399], action=0, reward=1.0, next_state=[-0.25759167  0.18691873 -0.07647873 -0.66180691]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 72 ] state=[-0.25759167  0.18691873 -0.07647873 -0.66180691], action=0, reward=1.0, next_state=[-0.2538533  -0.00706047 -0.08971487 -0.39415147]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 73 ] state=[-0.2538533  -0.00706047 -0.08971487 -0.39415147], action=0, reward=1.0, next_state=[-0.25399451 -0.20080246 -0.0975979  -0.13104805]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 74 ] state=[-0.25399451 -0.20080246 -0.0975979  -0.13104805], action=0, reward=1.0, next_state=[-0.25801056 -0.39440071 -0.10021886  0.12931911]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 75 ] state=[-0.25801056 -0.39440071 -0.10021886  0.12931911], action=0, reward=1.0, next_state=[-0.26589857 -0.58795484 -0.09763248  0.38877846]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 76 ] state=[-0.26589857 -0.58795484 -0.09763248  0.38877846], action=0, reward=1.0, next_state=[-0.27765767 -0.78156525 -0.08985691  0.64915266]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 77 ] state=[-0.27765767 -0.78156525 -0.08985691  0.64915266], action=1, reward=1.0, next_state=[-0.29328897 -0.58531407 -0.07687386  0.32958114]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 78 ] state=[-0.29328897 -0.58531407 -0.07687386  0.32958114], action=0, reward=1.0, next_state=[-0.30499526 -0.77926231 -0.07028223  0.59706565]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 79 ] state=[-0.30499526 -0.77926231 -0.07028223  0.59706565], action=1, reward=1.0, next_state=[-0.3205805  -0.58323089 -0.05834092  0.28309848]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 80 ] state=[-0.3205805  -0.58323089 -0.05834092  0.28309848], action=0, reward=1.0, next_state=[-0.33224512 -0.77747428 -0.05267895  0.55682535]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 81 ] state=[-0.33224512 -0.77747428 -0.05267895  0.55682535], action=1, reward=1.0, next_state=[-0.34779461 -0.58165388 -0.04154244  0.24802178]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 82 ] state=[-0.34779461 -0.58165388 -0.04154244  0.24802178], action=1, reward=1.0, next_state=[-0.35942768 -0.38596403 -0.03658201 -0.05746972]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 83 ] state=[-0.35942768 -0.38596403 -0.03658201 -0.05746972], action=0, reward=1.0, next_state=[-0.36714696 -0.58054288 -0.0377314   0.22345057]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 84 ] state=[-0.36714696 -0.58054288 -0.0377314   0.22345057], action=1, reward=1.0, next_state=[-0.37875782 -0.38490253 -0.03326239 -0.08089149]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 85 ] state=[-0.37875782 -0.38490253 -0.03326239 -0.08089149], action=0, reward=1.0, next_state=[-0.38645587 -0.57953226 -0.03488022  0.20111428]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 86 ] state=[-0.38645587 -0.57953226 -0.03488022  0.20111428], action=1, reward=1.0, next_state=[-0.39804652 -0.38392926 -0.03085794 -0.10236445]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 87 ] state=[-0.39804652 -0.38392926 -0.03085794 -0.10236445], action=1, reward=1.0, next_state=[-0.4057251  -0.18837898 -0.03290523 -0.40462103]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 88 ] state=[-0.4057251  -0.18837898 -0.03290523 -0.40462103], action=1, reward=1.0, next_state=[-0.40949268  0.00719379 -0.04099765 -0.70749378]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 89 ] state=[-0.40949268  0.00719379 -0.04099765 -0.70749378], action=0, reward=1.0, next_state=[-0.40934881 -0.18733695 -0.05514752 -0.42799278]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 90 ] state=[-0.40934881 -0.18733695 -0.05514752 -0.42799278], action=1, reward=1.0, next_state=[-0.41309555  0.0085209  -0.06370738 -0.73753809]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 91 ] state=[-0.41309555  0.0085209  -0.06370738 -0.73753809], action=1, reward=1.0, next_state=[-0.41292513  0.20446214 -0.07845814 -1.04957102]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 92 ] state=[-0.41292513  0.20446214 -0.07845814 -1.04957102], action=1, reward=1.0, next_state=[-0.40883588  0.40053232 -0.09944956 -1.36581457]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 93 ] state=[-0.40883588  0.40053232 -0.09944956 -1.36581457], action=0, reward=1.0, next_state=[-0.40082524  0.20678629 -0.12676585 -1.10582148]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 94 ] state=[-0.40082524  0.20678629 -0.12676585 -1.10582148], action=0, reward=1.0, next_state=[-0.39668951  0.01353811 -0.14888228 -0.85544459]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 95 ] state=[-0.39668951  0.01353811 -0.14888228 -0.85544459], action=0, reward=1.0, next_state=[-0.39641875 -0.17927554 -0.16599117 -0.61303348]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 96 ] state=[-0.39641875 -0.17927554 -0.16599117 -0.61303348], action=0, reward=1.0, next_state=[-0.40000426 -0.37173654 -0.17825184 -0.37688763]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 97 ] state=[-0.40000426 -0.37173654 -0.17825184 -0.37688763], action=0, reward=1.0, next_state=[-0.40743899 -0.56393834 -0.18578959 -0.145282  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 98 ] state=[-0.40743899 -0.56393834 -0.18578959 -0.145282  ], action=0, reward=1.0, next_state=[-0.41871776 -0.75598101 -0.18869523  0.08351618]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 99 ] state=[-0.41871776 -0.75598101 -0.18869523  0.08351618], action=0, reward=1.0, next_state=[-0.43383738 -0.94796757 -0.18702491  0.31123656]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 100 ] state=[-0.43383738 -0.94796757 -0.18702491  0.31123656], action=0, reward=1.0, next_state=[-0.45279673 -1.14000121 -0.18080018  0.53959859]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 101 ] state=[-0.45279673 -1.14000121 -0.18080018  0.53959859], action=0, reward=1.0, next_state=[-0.47559675 -1.33218276 -0.17000821  0.77030598]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 102 ] state=[-0.47559675 -1.33218276 -0.17000821  0.77030598], action=0, reward=1.0, next_state=[-0.50224041 -1.52460815 -0.15460209  1.00504089]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 103 ] state=[-0.50224041 -1.52460815 -0.15460209  1.00504089], action=0, reward=1.0, next_state=[-0.53273257 -1.71736536 -0.13450127  1.24545598]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 104 ] state=[-0.53273257 -1.71736536 -0.13450127  1.24545598], action=0, reward=1.0, next_state=[-0.56707988 -1.91053032 -0.10959215  1.49316227]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 105 ] state=[-0.56707988 -1.91053032 -0.10959215  1.49316227], action=1, reward=1.0, next_state=[-0.60529049 -1.71425891 -0.07972891  1.16836573]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 106 ] state=[-0.60529049 -1.71425891 -0.07972891  1.16836573], action=1, reward=1.0, next_state=[-0.63957566 -1.51819544 -0.05636159  0.85178929]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 343 ][ timestamp 107 ] state=[-0.63957566 -1.51819544 -0.05636159  0.85178929], action=1, reward=1.0, next_state=[-0.66993957 -1.32235224 -0.03932581  0.54192942]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 108 ] state=[-0.66993957 -1.32235224 -0.03932581  0.54192942], action=1, reward=1.0, next_state=[-0.69638662 -1.12670027 -0.02848722  0.23711957]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 109 ] state=[-0.69638662 -1.12670027 -0.02848722  0.23711957], action=1, reward=1.0, next_state=[-0.71892062 -0.93118316 -0.02374483 -0.06441121]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 110 ] state=[-0.71892062 -0.93118316 -0.02374483 -0.06441121], action=0, reward=1.0, next_state=[-0.73754429 -1.12595677 -0.02503305  0.22068652]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 111 ] state=[-0.73754429 -1.12595677 -0.02503305  0.22068652], action=0, reward=1.0, next_state=[-0.76006342 -1.32071212 -0.02061932  0.50536907]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 112 ] state=[-0.76006342 -1.32071212 -0.02061932  0.50536907], action=1, reward=1.0, next_state=[-0.78647766 -1.12530576 -0.01051194  0.20626018]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 113 ] state=[-0.78647766 -1.12530576 -0.01051194  0.20626018], action=0, reward=1.0, next_state=[-0.80898378 -1.32027582 -0.00638673  0.49560867]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 114 ] state=[-0.80898378 -1.32027582 -0.00638673  0.49560867], action=0, reward=1.0, next_state=[-0.8353893  -1.51530713  0.00352544  0.78627198]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 115 ] state=[-0.8353893  -1.51530713  0.00352544  0.78627198], action=0, reward=1.0, next_state=[-0.86569544 -1.71047734  0.01925088  1.08006194]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 116 ] state=[-0.86569544 -1.71047734  0.01925088  1.08006194], action=0, reward=1.0, next_state=[-0.89990499 -1.9058481   0.04085212  1.3787232 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 117 ] state=[-0.89990499 -1.9058481   0.04085212  1.3787232 ], action=0, reward=1.0, next_state=[-0.93802195 -2.10145567  0.06842658  1.68389693]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 118 ] state=[-0.93802195 -2.10145567  0.06842658  1.68389693], action=0, reward=1.0, next_state=[-0.98005106 -2.29729976  0.10210452  1.99707732]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 119 ] state=[-0.98005106 -2.29729976  0.10210452  1.99707732], action=0, reward=1.0, next_state=[-1.02599706 -2.49333026  0.14204607  2.31955823]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 120 ] state=[-1.02599706 -2.49333026  0.14204607  2.31955823], action=0, reward=1.0, next_state=[-1.07586366 -2.68943138  0.18843723  2.65236859]\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ][ timestamp 121 ] state=[-1.07586366 -2.68943138  0.18843723  2.65236859], action=1, reward=-1.0, next_state=[-1.12965229 -2.49615892  0.2414846   2.42266507]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 343: Exploration_rate=0.01. Score=121.\n",
      "[ episode 344 ] state=[ 0.03886343 -0.02747804 -0.00310958  0.04692053]\n",
      "[ episode 344 ][ timestamp 1 ] state=[ 0.03886343 -0.02747804 -0.00310958  0.04692053], action=0, reward=1.0, next_state=[ 0.03831387 -0.22255527 -0.00217117  0.33862073]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 2 ] state=[ 0.03831387 -0.22255527 -0.00217117  0.33862073], action=1, reward=1.0, next_state=[ 0.03386276 -0.02740249  0.00460124  0.04525393]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 3 ] state=[ 0.03386276 -0.02740249  0.00460124  0.04525393], action=1, reward=1.0, next_state=[ 0.03331471  0.16765318  0.00550632 -0.24597372]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 4 ] state=[ 0.03331471  0.16765318  0.00550632 -0.24597372], action=1, reward=1.0, next_state=[ 0.03666777  0.36269606  0.00058685 -0.53691475]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 5 ] state=[ 0.03666777  0.36269606  0.00058685 -0.53691475], action=0, reward=1.0, next_state=[ 0.0439217   0.16756586 -0.01015145 -0.24404697]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 6 ] state=[ 0.0439217   0.16756586 -0.01015145 -0.24404697], action=0, reward=1.0, next_state=[ 0.04727301 -0.02740963 -0.01503239  0.04541673]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 7 ] state=[ 0.04727301 -0.02740963 -0.01503239  0.04541673], action=1, reward=1.0, next_state=[ 0.04672482  0.16792461 -0.01412405 -0.25197089]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 8 ] state=[ 0.04672482  0.16792461 -0.01412405 -0.25197089], action=0, reward=1.0, next_state=[ 0.05008331 -0.02699284 -0.01916347  0.03622379]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 9 ] state=[ 0.05008331 -0.02699284 -0.01916347  0.03622379], action=1, reward=1.0, next_state=[ 0.04954346  0.16839861 -0.018439   -0.26244328]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 10 ] state=[ 0.04954346  0.16839861 -0.018439   -0.26244328], action=1, reward=1.0, next_state=[ 0.05291143  0.36377884 -0.02368786 -0.56088457]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 11 ] state=[ 0.05291143  0.36377884 -0.02368786 -0.56088457], action=0, reward=1.0, next_state=[ 0.060187    0.16899721 -0.03490555 -0.27575766]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 12 ] state=[ 0.060187    0.16899721 -0.03490555 -0.27575766], action=0, reward=1.0, next_state=[ 0.06356695 -0.0256098  -0.04042071  0.00571489]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 13 ] state=[ 0.06356695 -0.0256098  -0.04042071  0.00571489], action=0, reward=1.0, next_state=[ 0.06305475 -0.22012945 -0.04030641  0.28537558]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 14 ] state=[ 0.06305475 -0.22012945 -0.04030641  0.28537558], action=0, reward=1.0, next_state=[ 0.05865216 -0.41465407 -0.0345989   0.56507864]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 15 ] state=[ 0.05865216 -0.41465407 -0.0345989   0.56507864], action=1, reward=1.0, next_state=[ 0.05035908 -0.21906421 -0.02329732  0.2616994 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 16 ] state=[ 0.05035908 -0.21906421 -0.02329732  0.2616994 ], action=1, reward=1.0, next_state=[ 0.0459778  -0.02361758 -0.01806333 -0.03823978]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 17 ] state=[ 0.0459778  -0.02361758 -0.01806333 -0.03823978], action=0, reward=1.0, next_state=[ 0.04550545 -0.21847591 -0.01882813  0.2486897 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 18 ] state=[ 0.04550545 -0.21847591 -0.01882813  0.2486897 ], action=1, reward=1.0, next_state=[ 0.04113593 -0.0230902  -0.01385434 -0.04987206]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 19 ] state=[ 0.04113593 -0.0230902  -0.01385434 -0.04987206], action=1, reward=1.0, next_state=[ 0.04067412  0.17222764 -0.01485178 -0.34689375]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 20 ] state=[ 0.04067412  0.17222764 -0.01485178 -0.34689375], action=0, reward=1.0, next_state=[ 0.04411868 -0.02267995 -0.02178965 -0.05893088]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 21 ] state=[ 0.04411868 -0.02267995 -0.02178965 -0.05893088], action=1, reward=1.0, next_state=[ 0.04366508  0.17274754 -0.02296827 -0.35840817]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 22 ] state=[ 0.04366508  0.17274754 -0.02296827 -0.35840817], action=0, reward=1.0, next_state=[ 0.04712003 -0.02204049 -0.03013643 -0.07305527]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 23 ] state=[ 0.04712003 -0.02204049 -0.03013643 -0.07305527], action=0, reward=1.0, next_state=[ 0.04667922 -0.21671772 -0.03159754  0.20996922]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 24 ] state=[ 0.04667922 -0.21671772 -0.03159754  0.20996922], action=0, reward=1.0, next_state=[ 0.04234486 -0.41137397 -0.02739815  0.4925197 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 25 ] state=[ 0.04234486 -0.41137397 -0.02739815  0.4925197 ], action=0, reward=1.0, next_state=[ 0.03411739 -0.60609897 -0.01754776  0.77644354]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 26 ] state=[ 0.03411739 -0.60609897 -0.01754776  0.77644354], action=1, reward=1.0, next_state=[ 0.02199541 -0.41074014 -0.00201889  0.47829162]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 27 ] state=[ 0.02199541 -0.41074014 -0.00201889  0.47829162], action=1, reward=1.0, next_state=[ 0.0137806  -0.21558974  0.00754694  0.18497307]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 344 ][ timestamp 28 ] state=[ 0.0137806  -0.21558974  0.00754694  0.18497307], action=0, reward=1.0, next_state=[ 0.00946881 -0.41081886  0.0112464   0.48002719]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 29 ] state=[ 0.00946881 -0.41081886  0.0112464   0.48002719], action=1, reward=1.0, next_state=[ 0.00125243 -0.21585746  0.02084695  0.19090995]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 30 ] state=[ 0.00125243 -0.21585746  0.02084695  0.19090995], action=1, reward=1.0, next_state=[-0.00306472 -0.02103985  0.02466515 -0.0951244 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 31 ] state=[-0.00306472 -0.02103985  0.02466515 -0.0951244 ], action=1, reward=1.0, next_state=[-0.00348551  0.17372006  0.02276266 -0.37992459]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 32 ] state=[-0.00348551  0.17372006  0.02276266 -0.37992459], action=1, reward=1.0, next_state=[-1.11132680e-05  3.68511498e-01  1.51641669e-02 -6.65344406e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 33 ] state=[-1.11132680e-05  3.68511498e-01  1.51641669e-02 -6.65344406e-01], action=0, reward=1.0, next_state=[ 0.00735912  0.17318194  0.00185728 -0.36792566]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 34 ] state=[ 0.00735912  0.17318194  0.00185728 -0.36792566], action=0, reward=1.0, next_state=[ 0.01082276 -0.02196635 -0.00550123 -0.07465769]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 35 ] state=[ 0.01082276 -0.02196635 -0.00550123 -0.07465769], action=1, reward=1.0, next_state=[ 0.01038343  0.17323403 -0.00699439 -0.36907119]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 36 ] state=[ 0.01038343  0.17323403 -0.00699439 -0.36907119], action=0, reward=1.0, next_state=[ 0.01384811 -0.02178785 -0.01437581 -0.07860186]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 37 ] state=[ 0.01384811 -0.02178785 -0.01437581 -0.07860186], action=0, reward=1.0, next_state=[ 0.01341235 -0.2167008  -0.01594785  0.20951101]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 38 ] state=[ 0.01341235 -0.2167008  -0.01594785  0.20951101], action=0, reward=1.0, next_state=[ 0.00907834 -0.41159113 -0.01175763  0.49712086]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 39 ] state=[ 0.00907834 -0.41159113 -0.01175763  0.49712086], action=0, reward=1.0, next_state=[ 0.00084651 -0.60654533 -0.00181521  0.7860753 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 40 ] state=[ 0.00084651 -0.60654533 -0.00181521  0.7860753 ], action=0, reward=1.0, next_state=[-0.01128439 -0.8016423   0.01390629  1.07818659]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 41 ] state=[-0.01128439 -0.8016423   0.01390629  1.07818659], action=0, reward=1.0, next_state=[-0.02731724 -0.99694513  0.03547003  1.37520083]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 42 ] state=[-0.02731724 -0.99694513  0.03547003  1.37520083], action=1, reward=1.0, next_state=[-0.04725614 -0.80228392  0.06297404  1.09381868]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 43 ] state=[-0.04725614 -0.80228392  0.06297404  1.09381868], action=0, reward=1.0, next_state=[-0.06330182 -0.9981764   0.08485042  1.40557709]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 44 ] state=[-0.06330182 -0.9981764   0.08485042  1.40557709], action=0, reward=1.0, next_state=[-0.08326535 -1.19424301  0.11296196  1.72353503]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 45 ] state=[-0.08326535 -1.19424301  0.11296196  1.72353503], action=1, reward=1.0, next_state=[-0.10715021 -1.00058072  0.14743266  1.46803327]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 46 ] state=[-0.10715021 -1.00058072  0.14743266  1.46803327], action=0, reward=1.0, next_state=[-0.12716182 -1.1971673   0.17679332  1.80290247]\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ][ timestamp 47 ] state=[-0.12716182 -1.1971673   0.17679332  1.80290247], action=1, reward=-1.0, next_state=[-0.15110517 -1.00440687  0.21285137  1.56997563]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 344: Exploration_rate=0.01. Score=47.\n",
      "[ episode 345 ] state=[ 0.01001406  0.03834866 -0.01681694 -0.03926449]\n",
      "[ episode 345 ][ timestamp 1 ] state=[ 0.01001406  0.03834866 -0.01681694 -0.03926449], action=0, reward=1.0, next_state=[ 0.01078104 -0.15652815 -0.01760223  0.24806544]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 2 ] state=[ 0.01078104 -0.15652815 -0.01760223  0.24806544], action=0, reward=1.0, next_state=[ 0.00765047 -0.35139435 -0.01264093  0.53514467]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 3 ] state=[ 0.00765047 -0.35139435 -0.01264093  0.53514467], action=1, reward=1.0, next_state=[ 0.00062259 -0.15609694 -0.00193803  0.23850563]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 4 ] state=[ 0.00062259 -0.15609694 -0.00193803  0.23850563], action=1, reward=1.0, next_state=[-0.00249935  0.03905264  0.00283208 -0.05478797]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 5 ] state=[-0.00249935  0.03905264  0.00283208 -0.05478797], action=1, reward=1.0, next_state=[-0.0017183   0.23413387  0.00173632 -0.34657601]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 6 ] state=[-0.0017183   0.23413387  0.00173632 -0.34657601], action=0, reward=1.0, next_state=[ 0.00296438  0.03898726 -0.0051952  -0.05334607]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 7 ] state=[ 0.00296438  0.03898726 -0.0051952  -0.05334607], action=0, reward=1.0, next_state=[ 0.00374412 -0.15605981 -0.00626212  0.23769321]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 8 ] state=[ 0.00374412 -0.15605981 -0.00626212  0.23769321], action=0, reward=1.0, next_state=[ 0.00062293 -0.35109174 -0.00150826  0.52839432]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 9 ] state=[ 0.00062293 -0.35109174 -0.00150826  0.52839432], action=1, reward=1.0, next_state=[-0.00639891 -0.1559486   0.00905963  0.23523652]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 10 ] state=[-0.00639891 -0.1559486   0.00905963  0.23523652], action=1, reward=1.0, next_state=[-0.00951788  0.03904274  0.01376436 -0.054575  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 11 ] state=[-0.00951788  0.03904274  0.01376436 -0.054575  ], action=1, reward=1.0, next_state=[-0.00873703  0.23396466  0.01267286 -0.34288358]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 12 ] state=[-0.00873703  0.23396466  0.01267286 -0.34288358], action=0, reward=1.0, next_state=[-0.00405773  0.03866473  0.00581519 -0.04623149]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 13 ] state=[-0.00405773  0.03866473  0.00581519 -0.04623149], action=0, reward=1.0, next_state=[-0.00328444 -0.15654013  0.00489056  0.2482805 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 14 ] state=[-0.00328444 -0.15654013  0.00489056  0.2482805 ], action=1, reward=1.0, next_state=[-0.00641524  0.03851164  0.00985617 -0.04285584]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 15 ] state=[-0.00641524  0.03851164  0.00985617 -0.04285584], action=1, reward=1.0, next_state=[-0.00564501  0.23349088  0.00899905 -0.33241282]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 16 ] state=[-0.00564501  0.23349088  0.00899905 -0.33241282], action=0, reward=1.0, next_state=[-0.00097519  0.038242    0.0023508  -0.03690568]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 17 ] state=[-0.00097519  0.038242    0.0023508  -0.03690568], action=1, reward=1.0, next_state=[-2.10350444e-04  2.33330167e-01  1.61268271e-03 -3.28845982e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 18 ] state=[-2.10350444e-04  2.33330167e-01  1.61268271e-03 -3.28845982e-01], action=0, reward=1.0, next_state=[ 0.00445625  0.03818529 -0.00496424 -0.03565493]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 19 ] state=[ 0.00445625  0.03818529 -0.00496424 -0.03565493], action=1, reward=1.0, next_state=[ 0.00521996  0.23337808 -0.00567734 -0.32989998]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 20 ] state=[ 0.00521996  0.23337808 -0.00567734 -0.32989998], action=0, reward=1.0, next_state=[ 0.00988752  0.03833741 -0.01227533 -0.03901281]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 21 ] state=[ 0.00988752  0.03833741 -0.01227533 -0.03901281], action=1, reward=1.0, next_state=[ 0.01065427  0.23363321 -0.01305559 -0.33554331]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 22 ] state=[ 0.01065427  0.23363321 -0.01305559 -0.33554331], action=1, reward=1.0, next_state=[ 0.01532693  0.42893851 -0.01976646 -0.63231452]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 23 ] state=[ 0.01532693  0.42893851 -0.01976646 -0.63231452], action=0, reward=1.0, next_state=[ 0.0239057   0.23409783 -0.03241275 -0.34592155]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 24 ] state=[ 0.0239057   0.23409783 -0.03241275 -0.34592155], action=0, reward=1.0, next_state=[ 0.02858766  0.03945157 -0.03933118 -0.0636332 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 25 ] state=[ 0.02858766  0.03945157 -0.03933118 -0.0636332 ], action=0, reward=1.0, next_state=[ 0.02937669 -0.15508504 -0.04060384  0.21638566]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 345 ][ timestamp 26 ] state=[ 0.02937669 -0.15508504 -0.04060384  0.21638566], action=1, reward=1.0, next_state=[ 0.02627499  0.04059314 -0.03627613 -0.08882394]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 27 ] state=[ 0.02627499  0.04059314 -0.03627613 -0.08882394], action=0, reward=1.0, next_state=[ 0.02708685 -0.15399057 -0.03805261  0.19219675]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 28 ] state=[ 0.02708685 -0.15399057 -0.03805261  0.19219675], action=1, reward=1.0, next_state=[ 0.02400704  0.0416545  -0.03420867 -0.11224317]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 29 ] state=[ 0.02400704  0.0416545  -0.03420867 -0.11224317], action=1, reward=1.0, next_state=[ 0.02484013  0.23724951 -0.03645354 -0.41551942]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 30 ] state=[ 0.02484013  0.23724951 -0.03645354 -0.41551942], action=0, reward=1.0, next_state=[ 0.02958512  0.04266266 -0.04476392 -0.13454803]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 31 ] state=[ 0.02958512  0.04266266 -0.04476392 -0.13454803], action=0, reward=1.0, next_state=[ 0.03043837 -0.15179046 -0.04745489  0.14368327]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 32 ] state=[ 0.03043837 -0.15179046 -0.04745489  0.14368327], action=1, reward=1.0, next_state=[ 0.02740257  0.04397785 -0.04458122 -0.16358511]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 33 ] state=[ 0.02740257  0.04397785 -0.04458122 -0.16358511], action=1, reward=1.0, next_state=[ 0.02828212  0.2397087  -0.04785292 -0.46999222]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 34 ] state=[ 0.02828212  0.2397087  -0.04785292 -0.46999222], action=1, reward=1.0, next_state=[ 0.0330763   0.43547281 -0.05725277 -0.77736561]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 35 ] state=[ 0.0330763   0.43547281 -0.05725277 -0.77736561], action=0, reward=1.0, next_state=[ 0.04178575  0.24118296 -0.07280008 -0.50323148]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 36 ] state=[ 0.04178575  0.24118296 -0.07280008 -0.50323148], action=1, reward=1.0, next_state=[ 0.04660941  0.43725142 -0.08286471 -0.81793948]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 37 ] state=[ 0.04660941  0.43725142 -0.08286471 -0.81793948], action=0, reward=1.0, next_state=[ 0.05535444  0.24335561 -0.0992235  -0.5524281 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 38 ] state=[ 0.05535444  0.24335561 -0.0992235  -0.5524281 ], action=0, reward=1.0, next_state=[ 0.06022155  0.04975686 -0.11027206 -0.29258219]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 39 ] state=[ 0.06022155  0.04975686 -0.11027206 -0.29258219], action=0, reward=1.0, next_state=[ 0.06121669 -0.14363424 -0.1161237  -0.0366118 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 40 ] state=[ 0.06121669 -0.14363424 -0.1161237  -0.0366118 ], action=0, reward=1.0, next_state=[ 0.05834401 -0.33691603 -0.11685594  0.21729463]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 41 ] state=[ 0.05834401 -0.33691603 -0.11685594  0.21729463], action=0, reward=1.0, next_state=[ 0.05160568 -0.53019045 -0.11251005  0.47095158]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 42 ] state=[ 0.05160568 -0.53019045 -0.11251005  0.47095158], action=0, reward=1.0, next_state=[ 0.04100188 -0.72355837 -0.10309102  0.72616137]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 43 ] state=[ 0.04100188 -0.72355837 -0.10309102  0.72616137], action=1, reward=1.0, next_state=[ 0.02653071 -0.52717351 -0.08856779  0.40289295]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 44 ] state=[ 0.02653071 -0.52717351 -0.08856779  0.40289295], action=0, reward=1.0, next_state=[ 0.01598724 -0.72093499 -0.08050993  0.66639107]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 45 ] state=[ 0.01598724 -0.72093499 -0.08050993  0.66639107], action=0, reward=1.0, next_state=[ 0.00156854 -0.91485036 -0.06718211  0.93267759]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 46 ] state=[ 0.00156854 -0.91485036 -0.06718211  0.93267759], action=1, reward=1.0, next_state=[-0.01672847 -0.71888941 -0.04852856  0.61966257]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 47 ] state=[-0.01672847 -0.71888941 -0.04852856  0.61966257], action=1, reward=1.0, next_state=[-0.03110626 -0.52312446 -0.0361353   0.31209905]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 48 ] state=[-0.03110626 -0.52312446 -0.0361353   0.31209905], action=1, reward=1.0, next_state=[-0.04156875 -0.32750684 -0.02989332  0.00824271]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 49 ] state=[-0.04156875 -0.32750684 -0.02989332  0.00824271], action=0, reward=1.0, next_state=[-0.04811888 -0.52218761 -0.02972847  0.29134607]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 50 ] state=[-0.04811888 -0.52218761 -0.02972847  0.29134607], action=0, reward=1.0, next_state=[-0.05856264 -0.71687334 -0.02390155  0.57450674]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 51 ] state=[-0.05856264 -0.71687334 -0.02390155  0.57450674], action=0, reward=1.0, next_state=[-0.0729001  -0.91165219 -0.01241141  0.85956519]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 52 ] state=[-0.0729001  -0.91165219 -0.01241141  0.85956519], action=0, reward=1.0, next_state=[-0.09113315 -1.10660291  0.00477989  1.14831988]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 53 ] state=[-0.09113315 -1.10660291  0.00477989  1.14831988], action=1, reward=1.0, next_state=[-0.1132652  -0.91154369  0.02774629  0.85713967]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 54 ] state=[-0.1132652  -0.91154369  0.02774629  0.85713967], action=0, reward=1.0, next_state=[-0.13149608 -1.10703246  0.04488908  1.15841633]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 55 ] state=[-0.13149608 -1.10703246  0.04488908  1.15841633], action=1, reward=1.0, next_state=[-0.15363673 -0.91252333  0.06805741  0.8801395 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 56 ] state=[-0.15363673 -0.91252333  0.06805741  0.8801395 ], action=1, reward=1.0, next_state=[-0.17188719 -0.71838871  0.0856602   0.60960514]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 57 ] state=[-0.17188719 -0.71838871  0.0856602   0.60960514], action=1, reward=1.0, next_state=[-0.18625497 -0.52456205  0.0978523   0.34508447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 58 ] state=[-0.18625497 -0.52456205  0.0978523   0.34508447], action=1, reward=1.0, next_state=[-0.19674621 -0.33095836  0.10475399  0.08479085]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 59 ] state=[-0.19674621 -0.33095836  0.10475399  0.08479085], action=1, reward=1.0, next_state=[-0.20336538 -0.13748181  0.10644981 -0.17309172]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 60 ] state=[-0.20336538 -0.13748181  0.10644981 -0.17309172], action=1, reward=1.0, next_state=[-0.20611501  0.05596831  0.10298797 -0.43038722]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 61 ] state=[-0.20611501  0.05596831  0.10298797 -0.43038722], action=0, reward=1.0, next_state=[-0.20499565 -0.14044974  0.09438023 -0.10709629]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 62 ] state=[-0.20499565 -0.14044974  0.09438023 -0.10709629], action=1, reward=1.0, next_state=[-0.20780464  0.0532019   0.0922383  -0.36857436]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 63 ] state=[-0.20780464  0.0532019   0.0922383  -0.36857436], action=0, reward=1.0, next_state=[-0.2067406  -0.14310133  0.08486682 -0.0482916 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 64 ] state=[-0.2067406  -0.14310133  0.08486682 -0.0482916 ], action=1, reward=1.0, next_state=[-0.20960263  0.05070764  0.08390098 -0.31303787]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 65 ] state=[-0.20960263  0.05070764  0.08390098 -0.31303787], action=0, reward=1.0, next_state=[-0.20858848 -0.14550309  0.07764023  0.0048809 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 66 ] state=[-0.20858848 -0.14550309  0.07764023  0.0048809 ], action=1, reward=1.0, next_state=[-0.21149854  0.04842449  0.07773784 -0.26233086]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 67 ] state=[-0.21149854  0.04842449  0.07773784 -0.26233086], action=1, reward=1.0, next_state=[-0.21053005  0.24235568  0.07249123 -0.52951722]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 68 ] state=[-0.21053005  0.24235568  0.07249123 -0.52951722], action=1, reward=1.0, next_state=[-0.20568294  0.43638698  0.06190088 -0.79850601]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 69 ] state=[-0.20568294  0.43638698  0.06190088 -0.79850601], action=0, reward=1.0, next_state=[-0.1969552   0.24047289  0.04593076 -0.48701047]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 345 ][ timestamp 70 ] state=[-0.1969552   0.24047289  0.04593076 -0.48701047], action=1, reward=1.0, next_state=[-0.19214574  0.43491768  0.03619055 -0.76487116]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 71 ] state=[-0.19214574  0.43491768  0.03619055 -0.76487116], action=0, reward=1.0, next_state=[-0.18344738  0.23931654  0.02089313 -0.46102388]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 72 ] state=[-0.18344738  0.23931654  0.02089313 -0.46102388], action=0, reward=1.0, next_state=[-0.17866105  0.04390561  0.01167265 -0.16182932]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 73 ] state=[-0.17866105  0.04390561  0.01167265 -0.16182932], action=1, reward=1.0, next_state=[-0.17778294  0.23885853  0.00843607 -0.4508071 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 74 ] state=[-0.17778294  0.23885853  0.00843607 -0.4508071 ], action=1, reward=1.0, next_state=[-1.73005771e-01  4.33860162e-01 -5.80075964e-04 -7.40818959e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 75 ] state=[-1.73005771e-01  4.33860162e-01 -5.80075964e-04 -7.40818959e-01], action=0, reward=1.0, next_state=[-0.16432857  0.23874622 -0.01539646 -0.44831864]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 76 ] state=[-0.16432857  0.23874622 -0.01539646 -0.44831864], action=0, reward=1.0, next_state=[-0.15955364  0.04384541 -0.02436283 -0.16052845]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 77 ] state=[-0.15955364  0.04384541 -0.02436283 -0.16052845], action=1, reward=1.0, next_state=[-0.15867673  0.23930752 -0.0275734  -0.46079657]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 78 ] state=[-0.15867673  0.23930752 -0.0275734  -0.46079657], action=1, reward=1.0, next_state=[-0.15389058  0.43480813 -0.03678933 -0.76204156]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 79 ] state=[-0.15389058  0.43480813 -0.03678933 -0.76204156], action=1, reward=1.0, next_state=[-0.14519442  0.63041701 -0.05203016 -1.06606998]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 80 ] state=[-0.14519442  0.63041701 -0.05203016 -1.06606998], action=1, reward=1.0, next_state=[-0.13258608  0.8261874  -0.07335156 -1.37461813]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 81 ] state=[-0.13258608  0.8261874  -0.07335156 -1.37461813], action=1, reward=1.0, next_state=[-0.11606233  1.02214544 -0.10084392 -1.68931081]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 82 ] state=[-0.11606233  1.02214544 -0.10084392 -1.68931081], action=1, reward=1.0, next_state=[-0.09561942  1.21827812 -0.13463014 -2.01161305]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 83 ] state=[-0.09561942  1.21827812 -0.13463014 -2.01161305], action=1, reward=1.0, next_state=[-0.07125386  1.41451902 -0.1748624  -2.34277254]\n",
      "[ Experience replay ] starts\n",
      "[ episode 345 ][ timestamp 84 ] state=[-0.07125386  1.41451902 -0.1748624  -2.34277254], action=1, reward=-1.0, next_state=[-0.04296348  1.61073183 -0.22171785 -2.6837515 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 345: Exploration_rate=0.01. Score=84.\n",
      "[ episode 346 ] state=[-0.03365501  0.04724422  0.03509834  0.02636796]\n",
      "[ episode 346 ][ timestamp 1 ] state=[-0.03365501  0.04724422  0.03509834  0.02636796], action=0, reward=1.0, next_state=[-0.03271012 -0.14836303  0.0356257   0.32991492]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 2 ] state=[-0.03271012 -0.14836303  0.0356257   0.32991492], action=0, reward=1.0, next_state=[-0.03567738 -0.34397354  0.042224    0.63361623]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 3 ] state=[-0.03567738 -0.34397354  0.042224    0.63361623], action=0, reward=1.0, next_state=[-0.04255685 -0.53965828  0.05489632  0.93929189]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 4 ] state=[-0.04255685 -0.53965828  0.05489632  0.93929189], action=1, reward=1.0, next_state=[-0.05335002 -0.34531762  0.07368216  0.66435146]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 5 ] state=[-0.05335002 -0.34531762  0.07368216  0.66435146], action=1, reward=1.0, next_state=[-0.06025637 -0.1512938   0.08696919  0.39574835]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 6 ] state=[-0.06025637 -0.1512938   0.08696919  0.39574835], action=1, reward=1.0, next_state=[-0.06328225  0.04249346  0.09488416  0.13170279]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 7 ] state=[-0.06328225  0.04249346  0.09488416  0.13170279], action=1, reward=1.0, next_state=[-0.06243238  0.23613721  0.09751821 -0.12960217]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 8 ] state=[-0.06243238  0.23613721  0.09751821 -0.12960217], action=0, reward=1.0, next_state=[-0.05770964  0.03976336  0.09492617  0.19218403]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 9 ] state=[-0.05770964  0.03976336  0.09492617  0.19218403], action=0, reward=1.0, next_state=[-0.05691437 -0.15657932  0.09876985  0.51323853]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 10 ] state=[-0.05691437 -0.15657932  0.09876985  0.51323853], action=1, reward=1.0, next_state=[-0.06004595  0.03702291  0.10903462  0.25324168]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 11 ] state=[-0.06004595  0.03702291  0.10903462  0.25324168], action=1, reward=1.0, next_state=[-0.0593055   0.23043274  0.11409945 -0.00315755]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 12 ] state=[-0.0593055   0.23043274  0.11409945 -0.00315755], action=1, reward=1.0, next_state=[-0.05469684  0.42374915  0.1140363  -0.25777417]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 13 ] state=[-0.05469684  0.42374915  0.1140363  -0.25777417], action=1, reward=1.0, next_state=[-0.04622186  0.61707401  0.10888082 -0.51242391]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 14 ] state=[-0.04622186  0.61707401  0.10888082 -0.51242391], action=1, reward=1.0, next_state=[-0.03388038  0.81050747  0.09863234 -0.76890818]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 15 ] state=[-0.03388038  0.81050747  0.09863234 -0.76890818], action=1, reward=1.0, next_state=[-0.01767023  1.00414346  0.08325418 -1.02899957]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 16 ] state=[-0.01767023  1.00414346  0.08325418 -1.02899957], action=1, reward=1.0, next_state=[ 0.00241264  1.19806471  0.06267419 -1.29442548]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 17 ] state=[ 0.00241264  1.19806471  0.06267419 -1.29442548], action=0, reward=1.0, next_state=[ 0.02637393  1.00220488  0.03678568 -0.9827984 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 18 ] state=[ 0.02637393  1.00220488  0.03678568 -0.9827984 ], action=0, reward=1.0, next_state=[ 0.04641803  0.80660986  0.01712971 -0.67879181]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 19 ] state=[ 0.04641803  0.80660986  0.01712971 -0.67879181], action=0, reward=1.0, next_state=[ 0.06255023  0.61125418  0.00355387 -0.3807654 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 20 ] state=[ 0.06255023  0.61125418  0.00355387 -0.3807654 ], action=0, reward=1.0, next_state=[ 0.07477531  0.41608195 -0.00406144 -0.08696406]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 21 ] state=[ 0.07477531  0.41608195 -0.00406144 -0.08696406], action=1, reward=1.0, next_state=[ 0.08309695  0.61126188 -0.00580072 -0.3809256 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 22 ] state=[ 0.08309695  0.61126188 -0.00580072 -0.3809256 ], action=1, reward=1.0, next_state=[ 0.09532219  0.80646572 -0.01341923 -0.67543183]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 23 ] state=[ 0.09532219  0.80646572 -0.01341923 -0.67543183], action=1, reward=1.0, next_state=[ 0.1114515   1.00177155 -0.02692786 -0.97230934]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 24 ] state=[ 0.1114515   1.00177155 -0.02692786 -0.97230934], action=0, reward=1.0, next_state=[ 0.13148694  0.8070211  -0.04637405 -0.6882054 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 25 ] state=[ 0.13148694  0.8070211  -0.04637405 -0.6882054 ], action=0, reward=1.0, next_state=[ 0.14762736  0.61257242 -0.06013816 -0.41047504]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 26 ] state=[ 0.14762736  0.61257242 -0.06013816 -0.41047504], action=0, reward=1.0, next_state=[ 0.15987881  0.41835231 -0.06834766 -0.1373415 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 27 ] state=[ 0.15987881  0.41835231 -0.06834766 -0.1373415 ], action=1, reward=1.0, next_state=[ 0.16824585  0.61438326 -0.07109449 -0.45077995]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 28 ] state=[ 0.16824585  0.61438326 -0.07109449 -0.45077995], action=0, reward=1.0, next_state=[ 0.18053352  0.42033504 -0.08011009 -0.18132709]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 346 ][ timestamp 29 ] state=[ 0.18053352  0.42033504 -0.08011009 -0.18132709], action=1, reward=1.0, next_state=[ 0.18894022  0.61650649 -0.08373663 -0.49816774]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 30 ] state=[ 0.18894022  0.61650649 -0.08373663 -0.49816774], action=0, reward=1.0, next_state=[ 0.20127035  0.42265881 -0.09369999 -0.23300485]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 31 ] state=[ 0.20127035  0.42265881 -0.09369999 -0.23300485], action=0, reward=1.0, next_state=[ 0.20972352  0.2289919  -0.09836008  0.0287137 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 32 ] state=[ 0.20972352  0.2289919  -0.09836008  0.0287137 ], action=1, reward=1.0, next_state=[ 0.21430336  0.42537674 -0.09778581 -0.293311  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 33 ] state=[ 0.21430336  0.42537674 -0.09778581 -0.293311  ], action=0, reward=1.0, next_state=[ 0.2228109   0.23177509 -0.10365203 -0.03299908]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 34 ] state=[ 0.2228109   0.23177509 -0.10365203 -0.03299908], action=0, reward=1.0, next_state=[ 0.2274464   0.0382805  -0.10431201  0.22526591]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 35 ] state=[ 0.2274464   0.0382805  -0.10431201  0.22526591], action=0, reward=1.0, next_state=[ 0.22821201 -0.15520796 -0.09980669  0.48330887]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 36 ] state=[ 0.22821201 -0.15520796 -0.09980669  0.48330887], action=1, reward=1.0, next_state=[ 0.22510785  0.04117051 -0.09014051  0.16091262]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 37 ] state=[ 0.22510785  0.04117051 -0.09014051  0.16091262], action=0, reward=1.0, next_state=[ 0.22593126 -0.1525531  -0.08692226  0.42385285]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 38 ] state=[ 0.22593126 -0.1525531  -0.08692226  0.42385285], action=0, reward=1.0, next_state=[ 0.2228802  -0.34634307 -0.07844521  0.6879174 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 39 ] state=[ 0.2228802  -0.34634307 -0.07844521  0.6879174 ], action=1, reward=1.0, next_state=[ 0.21595334 -0.15022502 -0.06468686  0.37160574]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 40 ] state=[ 0.21595334 -0.15022502 -0.06468686  0.37160574], action=1, reward=1.0, next_state=[ 0.21294884  0.04575342 -0.05725474  0.05924822]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 41 ] state=[ 0.21294884  0.04575342 -0.05725474  0.05924822], action=0, reward=1.0, next_state=[ 0.2138639  -0.14850286 -0.05606978  0.33333148]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 42 ] state=[ 0.2138639  -0.14850286 -0.05606978  0.33333148], action=1, reward=1.0, next_state=[ 0.21089385  0.04737045 -0.04940315  0.02350737]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 43 ] state=[ 0.21089385  0.04737045 -0.04940315  0.02350737], action=0, reward=1.0, next_state=[ 0.21184126 -0.14700948 -0.048933    0.3002029 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 44 ] state=[ 0.21184126 -0.14700948 -0.048933    0.3002029 ], action=1, reward=1.0, next_state=[ 0.20890107  0.04877454 -0.04292894 -0.00750217]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 45 ] state=[ 0.20890107  0.04877454 -0.04292894 -0.00750217], action=1, reward=1.0, next_state=[ 0.20987656  0.24448502 -0.04307899 -0.31341467]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 46 ] state=[ 0.20987656  0.24448502 -0.04307899 -0.31341467], action=0, reward=1.0, next_state=[ 0.21476626  0.05000239 -0.04934728 -0.03462267]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 47 ] state=[ 0.21476626  0.05000239 -0.04934728 -0.03462267], action=0, reward=1.0, next_state=[ 0.21576631 -0.14437845 -0.05003973  0.24209144]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 48 ] state=[ 0.21576631 -0.14437845 -0.05003973  0.24209144], action=1, reward=1.0, next_state=[ 0.21287874  0.05142125 -0.0451979  -0.06594601]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 49 ] state=[ 0.21287874  0.05142125 -0.0451979  -0.06594601], action=0, reward=1.0, next_state=[ 0.21390716 -0.14302453 -0.04651683  0.21214112]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 50 ] state=[ 0.21390716 -0.14302453 -0.04651683  0.21214112], action=1, reward=1.0, next_state=[ 0.21104667  0.05273057 -0.042274   -0.09484491]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 51 ] state=[ 0.21104667  0.05273057 -0.042274   -0.09484491], action=1, reward=1.0, next_state=[ 0.21210128  0.24843212 -0.0441709  -0.40055983]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 52 ] state=[ 0.21210128  0.24843212 -0.0441709  -0.40055983], action=0, reward=1.0, next_state=[ 0.21706992  0.05396365 -0.0521821  -0.12212368]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 53 ] state=[ 0.21706992  0.05396365 -0.0521821  -0.12212368], action=0, reward=1.0, next_state=[ 0.2181492  -0.14037336 -0.05462457  0.15365048]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 54 ] state=[ 0.2181492  -0.14037336 -0.05462457  0.15365048], action=0, reward=1.0, next_state=[ 0.21534173 -0.33467235 -0.05155156  0.42861262]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 55 ] state=[ 0.21534173 -0.33467235 -0.05155156  0.42861262], action=1, reward=1.0, next_state=[ 0.20864828 -0.13885967 -0.04297931  0.12013434]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 56 ] state=[ 0.20864828 -0.13885967 -0.04297931  0.12013434], action=1, reward=1.0, next_state=[ 0.20587109  0.05685087 -0.04057662 -0.18579239]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 57 ] state=[ 0.20587109  0.05685087 -0.04057662 -0.18579239], action=0, reward=1.0, next_state=[ 0.20700811 -0.13766773 -0.04429247  0.0938191 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 58 ] state=[ 0.20700811 -0.13766773 -0.04429247  0.0938191 ], action=1, reward=1.0, next_state=[ 0.20425475  0.05806015 -0.04241609 -0.21250251]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 59 ] state=[ 0.20425475  0.05806015 -0.04241609 -0.21250251], action=0, reward=1.0, next_state=[ 0.20541596 -0.13643051 -0.04666614  0.06650448]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 60 ] state=[ 0.20541596 -0.13643051 -0.04666614  0.06650448], action=1, reward=1.0, next_state=[ 0.20268735  0.05932836 -0.04533605 -0.24052902]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 61 ] state=[ 0.20268735  0.05932836 -0.04533605 -0.24052902], action=0, reward=1.0, next_state=[ 0.20387391 -0.13511763 -0.05014663  0.03751605]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 62 ] state=[ 0.20387391 -0.13511763 -0.05014663  0.03751605], action=0, reward=1.0, next_state=[ 0.20117156 -0.32948594 -0.04939631  0.31396508]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 63 ] state=[ 0.20117156 -0.32948594 -0.04939631  0.31396508], action=1, reward=1.0, next_state=[ 0.19458184 -0.1336964  -0.04311701  0.00612239]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 64 ] state=[ 0.19458184 -0.1336964  -0.04311701  0.00612239], action=0, reward=1.0, next_state=[ 0.19190791 -0.32817432 -0.04299456  0.28489567]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 65 ] state=[ 0.19190791 -0.32817432 -0.04299456  0.28489567], action=1, reward=1.0, next_state=[ 0.18534443 -0.13246639 -0.03729665 -0.02103145]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 66 ] state=[ 0.18534443 -0.13246639 -0.03729665 -0.02103145], action=0, reward=1.0, next_state=[ 0.1826951  -0.32703417 -0.03771727  0.25965458]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 67 ] state=[ 0.1826951  -0.32703417 -0.03771727  0.25965458], action=1, reward=1.0, next_state=[ 0.17615442 -0.13139464 -0.03252418 -0.04468224]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 68 ] state=[ 0.17615442 -0.13139464 -0.03252418 -0.04468224], action=1, reward=1.0, next_state=[ 0.17352652  0.06417823 -0.03341783 -0.34744683]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 69 ] state=[ 0.17352652  0.06417823 -0.03341783 -0.34744683], action=1, reward=1.0, next_state=[ 0.17481009  0.25975917 -0.04036676 -0.65047746]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 70 ] state=[ 0.17481009  0.25975917 -0.04036676 -0.65047746], action=0, reward=1.0, next_state=[ 0.18000527  0.06522204 -0.05337631 -0.37077407]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 346 ][ timestamp 71 ] state=[ 0.18000527  0.06522204 -0.05337631 -0.37077407], action=0, reward=1.0, next_state=[ 0.18130971 -0.12910256 -0.06079179 -0.09538749]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 72 ] state=[ 0.18130971 -0.12910256 -0.06079179 -0.09538749], action=0, reward=1.0, next_state=[ 0.17872766 -0.3233029  -0.06269954  0.17751314]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 73 ] state=[ 0.17872766 -0.3233029  -0.06269954  0.17751314], action=1, reward=1.0, next_state=[ 0.1722616  -0.12734232 -0.05914928 -0.13427174]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 74 ] state=[ 0.1722616  -0.12734232 -0.05914928 -0.13427174], action=0, reward=1.0, next_state=[ 0.16971476 -0.32156934 -0.06183472  0.13917954]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 75 ] state=[ 0.16971476 -0.32156934 -0.06183472  0.13917954], action=1, reward=1.0, next_state=[ 0.16328337 -0.12561877 -0.05905113 -0.17235241]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 76 ] state=[ 0.16328337 -0.12561877 -0.05905113 -0.17235241], action=0, reward=1.0, next_state=[ 0.16077099 -0.31984802 -0.06249817  0.10113271]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 77 ] state=[ 0.16077099 -0.31984802 -0.06249817  0.10113271], action=1, reward=1.0, next_state=[ 0.15437403 -0.12388864 -0.06047552 -0.21059498]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 78 ] state=[ 0.15437403 -0.12388864 -0.06047552 -0.21059498], action=0, reward=1.0, next_state=[ 0.15189626 -0.3180961  -0.06468742  0.06241471]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 79 ] state=[ 0.15189626 -0.3180961  -0.06468742  0.06241471], action=0, reward=1.0, next_state=[ 0.14553434 -0.51223382 -0.06343913  0.33400737]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 80 ] state=[ 0.14553434 -0.51223382 -0.06343913  0.33400737], action=0, reward=1.0, next_state=[ 0.13528966 -0.7063982  -0.05675898  0.60602948]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 81 ] state=[ 0.13528966 -0.7063982  -0.05675898  0.60602948], action=1, reward=1.0, next_state=[ 0.1211617  -0.51053043 -0.04463839  0.29602278]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 82 ] state=[ 0.1211617  -0.51053043 -0.04463839  0.29602278], action=1, reward=1.0, next_state=[ 0.11095109 -0.31480148 -0.03871793 -0.01039751]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 83 ] state=[ 0.11095109 -0.31480148 -0.03871793 -0.01039751], action=0, reward=1.0, next_state=[ 0.10465506 -0.50934739 -0.03892588  0.26982242]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 84 ] state=[ 0.10465506 -0.50934739 -0.03892588  0.26982242], action=1, reward=1.0, next_state=[ 0.09446811 -0.3136922  -0.03352943 -0.03487937]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 85 ] state=[ 0.09446811 -0.3136922  -0.03352943 -0.03487937], action=0, reward=1.0, next_state=[ 0.08819427 -0.50831768 -0.03422702  0.24703896]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 86 ] state=[ 0.08819427 -0.50831768 -0.03422702  0.24703896], action=1, reward=1.0, next_state=[ 0.07802791 -0.31272405 -0.02928624 -0.05624044]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 87 ] state=[ 0.07802791 -0.31272405 -0.02928624 -0.05624044], action=0, reward=1.0, next_state=[ 0.07177343 -0.50741411 -0.03041105  0.2270605 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 88 ] state=[ 0.07177343 -0.50741411 -0.03041105  0.2270605 ], action=1, reward=1.0, next_state=[ 0.06162515 -0.31187104 -0.02586984 -0.07505794]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 89 ] state=[ 0.06162515 -0.31187104 -0.02586984 -0.07505794], action=0, reward=1.0, next_state=[ 0.05538773 -0.50661276 -0.027371    0.209352  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 90 ] state=[ 0.05538773 -0.50661276 -0.027371    0.209352  ], action=0, reward=1.0, next_state=[ 0.04525548 -0.70133286 -0.02318396  0.49327669]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 91 ] state=[ 0.04525548 -0.70133286 -0.02318396  0.49327669], action=0, reward=1.0, next_state=[ 0.03122882 -0.89612028 -0.01331843  0.77856383]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 92 ] state=[ 0.03122882 -0.89612028 -0.01331843  0.77856383], action=1, reward=1.0, next_state=[ 0.01330641 -0.70081776  0.00225285  0.48172052]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 93 ] state=[ 0.01330641 -0.70081776  0.00225285  0.48172052], action=1, reward=1.0, next_state=[-0.00070994 -0.50572768  0.01188726  0.18974848]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 94 ] state=[-0.00070994 -0.50572768  0.01188726  0.18974848], action=1, reward=1.0, next_state=[-0.0108245  -0.31077779  0.01568223 -0.09916092]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 95 ] state=[-0.0108245  -0.31077779  0.01568223 -0.09916092], action=1, reward=1.0, next_state=[-0.01704005 -0.11588407  0.01369901 -0.38685517]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 96 ] state=[-0.01704005 -0.11588407  0.01369901 -0.38685517], action=0, reward=1.0, next_state=[-0.01935773 -0.31119778  0.00596191 -0.08988471]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 97 ] state=[-0.01935773 -0.31119778  0.00596191 -0.08988471], action=0, reward=1.0, next_state=[-0.02558169 -0.50640467  0.00416421  0.20467322]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 98 ] state=[-0.02558169 -0.50640467  0.00416421  0.20467322], action=1, reward=1.0, next_state=[-0.03570978 -0.31134252  0.00825768 -0.0866932 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 99 ] state=[-0.03570978 -0.31134252  0.00825768 -0.0866932 ], action=0, reward=1.0, next_state=[-0.04193663 -0.50658186  0.00652381  0.20858355]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 100 ] state=[-0.04193663 -0.50658186  0.00652381  0.20858355], action=1, reward=1.0, next_state=[-0.05206827 -0.3115538   0.01069549 -0.08203433]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 101 ] state=[-0.05206827 -0.3115538   0.01069549 -0.08203433], action=1, reward=1.0, next_state=[-0.05829935 -0.11658678  0.0090548  -0.37132371]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 102 ] state=[-0.05829935 -0.11658678  0.0090548  -0.37132371], action=0, reward=1.0, next_state=[-0.06063108 -0.3118362   0.00162833 -0.07579952]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 103 ] state=[-0.06063108 -0.3118362   0.00162833 -0.07579952], action=0, reward=1.0, next_state=[-6.68678056e-02 -5.06981454e-01  1.12335021e-04  2.17396708e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 104 ] state=[-6.68678056e-02 -5.06981454e-01  1.12335021e-04  2.17396708e-01], action=1, reward=1.0, next_state=[-0.07700743 -0.31186111  0.00446027 -0.07525078]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 105 ] state=[-0.07700743 -0.31186111  0.00446027 -0.07525078], action=1, reward=1.0, next_state=[-0.08324466 -0.11680338  0.00295525 -0.36652315]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 106 ] state=[-0.08324466 -0.11680338  0.00295525 -0.36652315], action=0, reward=1.0, next_state=[-0.08558072 -0.3119672  -0.00437521 -0.07290985]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 107 ] state=[-0.08558072 -0.3119672  -0.00437521 -0.07290985], action=0, reward=1.0, next_state=[-0.09182007 -0.50702616 -0.00583341  0.21838948]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 108 ] state=[-0.09182007 -0.50702616 -0.00583341  0.21838948], action=0, reward=1.0, next_state=[-0.10196059 -0.70206424 -0.00146562  0.50922661]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 109 ] state=[-0.10196059 -0.70206424 -0.00146562  0.50922661], action=1, reward=1.0, next_state=[-0.11600188 -0.50692167  0.00871892  0.21608218]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 110 ] state=[-0.11600188 -0.50692167  0.00871892  0.21608218], action=1, reward=1.0, next_state=[-0.12614031 -0.31192544  0.01304056 -0.07383772]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 111 ] state=[-0.12614031 -0.31192544  0.01304056 -0.07383772], action=1, reward=1.0, next_state=[-0.13237882 -0.11699285  0.0115638  -0.36237794]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 112 ] state=[-0.13237882 -0.11699285  0.0115638  -0.36237794], action=0, reward=1.0, next_state=[-0.13471868 -0.31227723  0.00431625 -0.06607126]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 113 ] state=[-0.13471868 -0.31227723  0.00431625 -0.06607126], action=0, reward=1.0, next_state=[-0.14096422 -0.5074608   0.00299482  0.22797034]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 114 ] state=[-0.14096422 -0.5074608   0.00299482  0.22797034], action=1, reward=1.0, next_state=[-0.15111344 -0.31238177  0.00755423 -0.06376641]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 115 ] state=[-0.15111344 -0.31238177  0.00755423 -0.06376641], action=0, reward=1.0, next_state=[-0.15736107 -0.50761121  0.0062789   0.23129031]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 346 ][ timestamp 116 ] state=[-0.15736107 -0.50761121  0.0062789   0.23129031], action=0, reward=1.0, next_state=[-0.1675133  -0.70282232  0.01090471  0.52594719]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 117 ] state=[-0.1675133  -0.70282232  0.01090471  0.52594719], action=1, reward=1.0, next_state=[-0.18156974 -0.5078555   0.02142365  0.23672026]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 118 ] state=[-0.18156974 -0.5078555   0.02142365  0.23672026], action=0, reward=1.0, next_state=[-0.19172685 -0.70327687  0.02615805  0.53608312]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 119 ] state=[-0.19172685 -0.70327687  0.02615805  0.53608312], action=1, reward=1.0, next_state=[-0.20579239 -0.50853231  0.03687972  0.2517558 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 120 ] state=[-0.20579239 -0.50853231  0.03687972  0.2517558 ], action=1, reward=1.0, next_state=[-0.21596304 -0.31395586  0.04191483 -0.02907023]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 121 ] state=[-0.21596304 -0.31395586  0.04191483 -0.02907023], action=1, reward=1.0, next_state=[-0.22224215 -0.11945928  0.04133343 -0.30823951]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 122 ] state=[-0.22224215 -0.11945928  0.04133343 -0.30823951], action=0, reward=1.0, next_state=[-0.22463134 -0.31514506  0.03516864 -0.00281297]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 123 ] state=[-0.22463134 -0.31514506  0.03516864 -0.00281297], action=0, reward=1.0, next_state=[-0.23093424 -0.51075327  0.03511238  0.30075536]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 124 ] state=[-0.23093424 -0.51075327  0.03511238  0.30075536], action=1, reward=1.0, next_state=[-0.24114931 -0.31614892  0.04112749  0.01934967]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 125 ] state=[-0.24114931 -0.31614892  0.04112749  0.01934967], action=1, reward=1.0, next_state=[-0.24747228 -0.12164017  0.04151448 -0.26007865]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 126 ] state=[-0.24747228 -0.12164017  0.04151448 -0.26007865], action=0, reward=1.0, next_state=[-0.24990509 -0.31732942  0.03631291  0.04540406]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 127 ] state=[-0.24990509 -0.31732942  0.03631291  0.04540406], action=1, reward=1.0, next_state=[-0.25625168 -0.12274648  0.03722099 -0.23560428]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 128 ] state=[-0.25625168 -0.12274648  0.03722099 -0.23560428], action=0, reward=1.0, next_state=[-0.25870661 -0.31837991  0.0325089   0.06858305]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 129 ] state=[-0.25870661 -0.31837991  0.0325089   0.06858305], action=0, reward=1.0, next_state=[-0.2650742  -0.51395249  0.03388056  0.37134285]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 130 ] state=[-0.2650742  -0.51395249  0.03388056  0.37134285], action=0, reward=1.0, next_state=[-0.27535325 -0.70953899  0.04130742  0.67451321]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 131 ] state=[-0.27535325 -0.70953899  0.04130742  0.67451321], action=1, reward=1.0, next_state=[-0.28954403 -0.51501471  0.05479768  0.39511663]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 132 ] state=[-0.28954403 -0.51501471  0.05479768  0.39511663], action=1, reward=1.0, next_state=[-0.29984433 -0.32071137  0.06270002  0.12020156]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 133 ] state=[-0.29984433 -0.32071137  0.06270002  0.12020156], action=1, reward=1.0, next_state=[-0.30625855 -0.12654118  0.06510405 -0.15205969]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 134 ] state=[-0.30625855 -0.12654118  0.06510405 -0.15205969], action=1, reward=1.0, next_state=[-0.30878938  0.06759107  0.06206285 -0.42351408]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 135 ] state=[-0.30878938  0.06759107  0.06206285 -0.42351408], action=1, reward=1.0, next_state=[-0.30743756  0.26178142  0.05359257 -0.69600404]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 136 ] state=[-0.30743756  0.26178142  0.05359257 -0.69600404], action=0, reward=1.0, next_state=[-0.30220193  0.06595878  0.03967249 -0.38694312]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 137 ] state=[-0.30220193  0.06595878  0.03967249 -0.38694312], action=1, reward=1.0, next_state=[-0.30088275  0.26049577  0.03193363 -0.66685834]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 138 ] state=[-0.30088275  0.26049577  0.03193363 -0.66685834], action=1, reward=1.0, next_state=[-0.29567284  0.45515939  0.01859646 -0.94931801]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 139 ] state=[-0.29567284  0.45515939  0.01859646 -0.94931801], action=0, reward=1.0, next_state=[-2.86569650e-01  2.59792089e-01 -3.89897746e-04 -6.50850689e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 140 ] state=[-2.86569650e-01  2.59792089e-01 -3.89897746e-04 -6.50850689e-01], action=0, reward=1.0, next_state=[-0.28137381  0.06467557 -0.01340691 -0.35829056]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 141 ] state=[-0.28137381  0.06467557 -0.01340691 -0.35829056], action=0, reward=1.0, next_state=[-0.2800803  -0.13025324 -0.02057272 -0.06986513]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 142 ] state=[-0.2800803  -0.13025324 -0.02057272 -0.06986513], action=0, reward=1.0, next_state=[-0.28268536 -0.3250743  -0.02197003  0.21625666]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 143 ] state=[-0.28268536 -0.3250743  -0.02197003  0.21625666], action=1, reward=1.0, next_state=[-0.28918685 -0.12964527 -0.01764489 -0.08327481]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 144 ] state=[-0.28918685 -0.12964527 -0.01764489 -0.08327481], action=0, reward=1.0, next_state=[-0.29177975 -0.3245099  -0.01931039  0.20378931]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 145 ] state=[-0.29177975 -0.3245099  -0.01931039  0.20378931], action=0, reward=1.0, next_state=[-0.29826995 -0.51935045 -0.0152346   0.49031874]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 146 ] state=[-0.29826995 -0.51935045 -0.0152346   0.49031874], action=1, reward=1.0, next_state=[-0.30865696 -0.32401694 -0.00542823  0.19287367]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 147 ] state=[-0.30865696 -0.32401694 -0.00542823  0.19287367], action=1, reward=1.0, next_state=[-0.3151373  -0.12881776 -0.00157075 -0.10151668]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 148 ] state=[-0.3151373  -0.12881776 -0.00157075 -0.10151668], action=1, reward=1.0, next_state=[-0.31771365  0.06632667 -0.00360109 -0.39469476]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 149 ] state=[-0.31771365  0.06632667 -0.00360109 -0.39469476], action=0, reward=1.0, next_state=[-0.31638712 -0.128744   -0.01149498 -0.10314937]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 150 ] state=[-0.31638712 -0.128744   -0.01149498 -0.10314937], action=0, reward=1.0, next_state=[-0.318962   -0.32369934 -0.01355797  0.18588488]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 151 ] state=[-0.318962   -0.32369934 -0.01355797  0.18588488], action=1, reward=1.0, next_state=[-0.32543599 -0.12838606 -0.00984027 -0.11104404]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 152 ] state=[-0.32543599 -0.12838606 -0.00984027 -0.11104404], action=0, reward=1.0, next_state=[-0.32800371 -0.32336563 -0.01206115  0.17851816]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 153 ] state=[-0.32800371 -0.32336563 -0.01206115  0.17851816], action=1, reward=1.0, next_state=[-0.33447102 -0.12807318 -0.00849079 -0.11794511]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 154 ] state=[-0.33447102 -0.12807318 -0.00849079 -0.11794511], action=0, reward=1.0, next_state=[-0.33703248 -0.32307245 -0.01084969  0.172047  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 155 ] state=[-0.33703248 -0.32307245 -0.01084969  0.172047  ], action=1, reward=1.0, next_state=[-0.34349393 -0.12779691 -0.00740875 -0.12403882]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 156 ] state=[-0.34349393 -0.12779691 -0.00740875 -0.12403882], action=0, reward=1.0, next_state=[-0.34604987 -0.32281194 -0.00988953  0.16629754]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 157 ] state=[-0.34604987 -0.32281194 -0.00988953  0.16629754], action=0, reward=1.0, next_state=[-0.35250611 -0.51779094 -0.00656358  0.45584427]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 158 ] state=[-0.35250611 -0.51779094 -0.00656358  0.45584427], action=1, reward=1.0, next_state=[-0.36286193 -0.3225768   0.00255331  0.1610997 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 159 ] state=[-0.36286193 -0.3225768   0.00255331  0.1610997 ], action=1, reward=1.0, next_state=[-0.36931347 -0.1274915   0.0057753  -0.13077664]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 160 ] state=[-0.36931347 -0.1274915   0.0057753  -0.13077664], action=0, reward=1.0, next_state=[-0.3718633  -0.3226957   0.00315977  0.16372271]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 161 ] state=[-0.3718633  -0.3226957   0.00315977  0.16372271], action=1, reward=1.0, next_state=[-0.37831721 -0.12761913  0.00643422 -0.12796172]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 162 ] state=[-0.37831721 -0.12761913  0.00643422 -0.12796172], action=0, reward=1.0, next_state=[-0.38086959 -0.32283266  0.00387499  0.16674416]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 346 ][ timestamp 163 ] state=[-0.38086959 -0.32283266  0.00387499  0.16674416], action=1, reward=1.0, next_state=[-0.38732625 -0.12776639  0.00720987 -0.1247138 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 164 ] state=[-0.38732625 -0.12776639  0.00720987 -0.1247138 ], action=1, reward=1.0, next_state=[-0.38988157  0.06725153  0.0047156  -0.4151134 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 165 ] state=[-0.38988157  0.06725153  0.0047156  -0.4151134 ], action=1, reward=1.0, next_state=[-0.38853654  0.26230633 -0.00358667 -0.70630596]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 166 ] state=[-0.38853654  0.26230633 -0.00358667 -0.70630596], action=0, reward=1.0, next_state=[-0.38329042  0.06723426 -0.01771279 -0.41475421]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 167 ] state=[-0.38329042  0.06723426 -0.01771279 -0.41475421], action=0, reward=1.0, next_state=[-0.38194573 -0.12763222 -0.02600788 -0.12770764]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 168 ] state=[-0.38194573 -0.12763222 -0.02600788 -0.12770764], action=1, reward=1.0, next_state=[-0.38449837  0.06785247 -0.02856203 -0.42848095]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 169 ] state=[-0.38449837  0.06785247 -0.02856203 -0.42848095], action=0, reward=1.0, next_state=[-0.38314133 -0.12685358 -0.03713165 -0.1449371 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 170 ] state=[-0.38314133 -0.12685358 -0.03713165 -0.1449371 ], action=0, reward=1.0, next_state=[-0.3856784  -0.32142464 -0.04003039  0.13580412]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 171 ] state=[-0.3856784  -0.32142464 -0.04003039  0.13580412], action=0, reward=1.0, next_state=[-0.39210689 -0.51595103 -0.03731431  0.41559416]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 172 ] state=[-0.39210689 -0.51595103 -0.03731431  0.41559416], action=1, reward=1.0, next_state=[-0.40242591 -0.32032064 -0.02900242  0.11138499]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 173 ] state=[-0.40242591 -0.32032064 -0.02900242  0.11138499], action=1, reward=1.0, next_state=[-0.40883232 -0.12479537 -0.02677472 -0.1903051 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 174 ] state=[-0.40883232 -0.12479537 -0.02677472 -0.1903051 ], action=0, reward=1.0, next_state=[-0.41132823 -0.31952425 -0.03058083  0.0938127 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 175 ] state=[-0.41132823 -0.31952425 -0.03058083  0.0938127 ], action=0, reward=1.0, next_state=[-0.41771872 -0.51419485 -0.02870457  0.3766927 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 176 ] state=[-0.41771872 -0.51419485 -0.02870457  0.3766927 ], action=1, reward=1.0, next_state=[-0.42800261 -0.31867721 -0.02117072  0.07509908]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 177 ] state=[-0.42800261 -0.31867721 -0.02117072  0.07509908], action=0, reward=1.0, next_state=[-0.43437616 -0.51348937 -0.01966874  0.3610281 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 178 ] state=[-0.43437616 -0.51348937 -0.01966874  0.3610281 ], action=1, reward=1.0, next_state=[-0.44464594 -0.31809344 -0.01244817  0.06220866]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 179 ] state=[-0.44464594 -0.31809344 -0.01244817  0.06220866], action=1, reward=1.0, next_state=[-0.45100781 -0.12279524 -0.011204   -0.2343756 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 180 ] state=[-0.45100781 -0.12279524 -0.011204   -0.2343756 ], action=0, reward=1.0, next_state=[-0.45346372 -0.31775534 -0.01589151  0.05475227]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 181 ] state=[-0.45346372 -0.31775534 -0.01589151  0.05475227], action=1, reward=1.0, next_state=[-0.45981882 -0.12240917 -0.01479647 -0.24290188]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 182 ] state=[-0.45981882 -0.12240917 -0.01479647 -0.24290188], action=1, reward=1.0, next_state=[-0.46226701  0.07292097 -0.01965451 -0.54021503]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 183 ] state=[-0.46226701  0.07292097 -0.01965451 -0.54021503], action=0, reward=1.0, next_state=[-0.46080859 -0.12191927 -0.03045881 -0.25378917]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 184 ] state=[-0.46080859 -0.12191927 -0.03045881 -0.25378917], action=1, reward=1.0, next_state=[-0.46324697  0.07362405 -0.03553459 -0.55592161]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 185 ] state=[-0.46324697  0.07362405 -0.03553459 -0.55592161], action=1, reward=1.0, next_state=[-0.46177449  0.26922642 -0.04665302 -0.8595849 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 186 ] state=[-0.46177449  0.26922642 -0.04665302 -0.8595849 ], action=0, reward=1.0, next_state=[-0.45638996  0.0747699  -0.06384472 -0.5819285 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 187 ] state=[-0.45638996  0.0747699  -0.06384472 -0.5819285 ], action=0, reward=1.0, next_state=[-0.45489457 -0.11940216 -0.07548329 -0.31002142]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 188 ] state=[-0.45489457 -0.11940216 -0.07548329 -0.31002142], action=0, reward=1.0, next_state=[-0.45728261 -0.31337203 -0.08168372 -0.04206612]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 189 ] state=[-0.45728261 -0.31337203 -0.08168372 -0.04206612], action=0, reward=1.0, next_state=[-0.46355005 -0.50723343 -0.08252504  0.22376808]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 190 ] state=[-0.46355005 -0.50723343 -0.08252504  0.22376808], action=0, reward=1.0, next_state=[-0.47369472 -0.70108486 -0.07804968  0.48932081]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 191 ] state=[-0.47369472 -0.70108486 -0.07804968  0.48932081], action=1, reward=1.0, next_state=[-0.48771642 -0.50495357 -0.06826326  0.17309618]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 192 ] state=[-0.48771642 -0.50495357 -0.06826326  0.17309618], action=0, reward=1.0, next_state=[-0.49781549 -0.69903548 -0.06480134  0.4434872 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 193 ] state=[-0.49781549 -0.69903548 -0.06480134  0.4434872 ], action=0, reward=1.0, next_state=[-0.5117962  -0.89318354 -0.0559316   0.71505979]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 194 ] state=[-0.5117962  -0.89318354 -0.0559316   0.71505979], action=0, reward=1.0, next_state=[-0.52965987 -1.08748848 -0.0416304   0.98962611]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 195 ] state=[-0.52965987 -1.08748848 -0.0416304   0.98962611], action=0, reward=1.0, next_state=[-0.55140964 -1.28202919 -0.02183788  1.26894854]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 196 ] state=[-0.55140964 -1.28202919 -0.02183788  1.26894854], action=1, reward=1.0, next_state=[-0.57705022 -1.08663527  0.00354109  0.96950772]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 197 ] state=[-0.57705022 -1.08663527  0.00354109  0.96950772], action=0, reward=1.0, next_state=[-0.59878293 -1.28180458  0.02293125  1.26330093]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 198 ] state=[-0.59878293 -1.28180458  0.02293125  1.26330093], action=0, reward=1.0, next_state=[-0.62441902 -1.47721207  0.04819727  1.56307629]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 199 ] state=[-0.62441902 -1.47721207  0.04819727  1.56307629], action=0, reward=1.0, next_state=[-0.65396326 -1.67287609  0.07945879  1.87039602]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 200 ] state=[-0.65396326 -1.67287609  0.07945879  1.87039602], action=0, reward=1.0, next_state=[-0.68742078 -1.86877159  0.11686671  2.18664844]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 201 ] state=[-0.68742078 -1.86877159  0.11686671  2.18664844], action=0, reward=1.0, next_state=[-0.72479621 -2.06481541  0.16059968  2.51298897]\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ][ timestamp 202 ] state=[-0.72479621 -2.06481541  0.16059968  2.51298897], action=1, reward=-1.0, next_state=[-0.76609252 -1.87133409  0.21085946  2.27351529]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 346: Exploration_rate=0.01. Score=202.\n",
      "[ episode 347 ] state=[-0.04010403  0.00733398 -0.01989423 -0.03092663]\n",
      "[ episode 347 ][ timestamp 1 ] state=[-0.04010403  0.00733398 -0.01989423 -0.03092663], action=0, reward=1.0, next_state=[-0.03995735 -0.18749711 -0.02051276  0.25541366]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 2 ] state=[-0.03995735 -0.18749711 -0.02051276  0.25541366], action=1, reward=1.0, next_state=[-0.04370729  0.00791162 -0.01540448 -0.0436681 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 3 ] state=[-0.04370729  0.00791162 -0.01540448 -0.0436681 ], action=1, reward=1.0, next_state=[-0.04354906  0.20325104 -0.01627785 -0.3411712 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 4 ] state=[-0.04354906  0.20325104 -0.01627785 -0.3411712 ], action=0, reward=1.0, next_state=[-0.03948404  0.00836443 -0.02310127 -0.05366549]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 5 ] state=[-0.03948404  0.00836443 -0.02310127 -0.05366549], action=0, reward=1.0, next_state=[-0.03931675 -0.1864188  -0.02417458  0.23164022]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 6 ] state=[-0.03931675 -0.1864188  -0.02417458  0.23164022], action=0, reward=1.0, next_state=[-0.04304512 -0.38118712 -0.01954178  0.51660071]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 7 ] state=[-0.04304512 -0.38118712 -0.01954178  0.51660071], action=0, reward=1.0, next_state=[-0.05066887 -0.57602853 -0.00920976  0.80306211]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 8 ] state=[-0.05066887 -0.57602853 -0.00920976  0.80306211], action=1, reward=1.0, next_state=[-0.06218944 -0.38078151  0.00685148  0.50749637]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 9 ] state=[-0.06218944 -0.38078151  0.00685148  0.50749637], action=1, reward=1.0, next_state=[-0.06980507 -0.18575677  0.01700141  0.21698043]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 347 ][ timestamp 10 ] state=[-0.06980507 -0.18575677  0.01700141  0.21698043], action=1, reward=1.0, next_state=[-0.0735202   0.00911807  0.02134102 -0.0702914 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 11 ] state=[-0.0735202   0.00911807  0.02134102 -0.0702914 ], action=0, reward=1.0, next_state=[-0.07333784 -0.18630324  0.01993519  0.22904759]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 12 ] state=[-0.07333784 -0.18630324  0.01993519  0.22904759], action=1, reward=1.0, next_state=[-0.07706391  0.00852824  0.02451614 -0.057281  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 13 ] state=[-0.07706391  0.00852824  0.02451614 -0.057281  ], action=0, reward=1.0, next_state=[-0.07689334 -0.18693649  0.02337052  0.24303501]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 14 ] state=[-0.07689334 -0.18693649  0.02337052  0.24303501], action=1, reward=1.0, next_state=[-0.08063207  0.00784398  0.02823122 -0.0421856 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 15 ] state=[-0.08063207  0.00784398  0.02823122 -0.0421856 ], action=0, reward=1.0, next_state=[-0.08047519 -0.18767119  0.02738751  0.25926916]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 16 ] state=[-0.08047519 -0.18767119  0.02738751  0.25926916], action=1, reward=1.0, next_state=[-0.08422862  0.0070493   0.03257289 -0.0246511 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 17 ] state=[-0.08422862  0.0070493   0.03257289 -0.0246511 ], action=1, reward=1.0, next_state=[-0.08408763  0.20168935  0.03207987 -0.30688158]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 18 ] state=[-0.08408763  0.20168935  0.03207987 -0.30688158], action=0, reward=1.0, next_state=[-0.08005384  0.00612531  0.02594224 -0.00425658]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 19 ] state=[-0.08005384  0.00612531  0.02594224 -0.00425658], action=1, reward=1.0, next_state=[-0.07993134  0.20086579  0.02585711 -0.28864285]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 20 ] state=[-0.07993134  0.20086579  0.02585711 -0.28864285], action=1, reward=1.0, next_state=[-0.07591402  0.39560965  0.02008425 -0.57305986]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 21 ] state=[-0.07591402  0.39560965  0.02008425 -0.57305986], action=1, reward=1.0, next_state=[-0.06800183  0.59044433  0.00862305 -0.85934856]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 22 ] state=[-0.06800183  0.59044433  0.00862305 -0.85934856], action=0, reward=1.0, next_state=[-0.05619294  0.39520599 -0.00856392 -0.56396679]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 23 ] state=[-0.05619294  0.39520599 -0.00856392 -0.56396679], action=0, reward=1.0, next_state=[-0.04828882  0.20020524 -0.01984325 -0.27399416]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 24 ] state=[-0.04828882  0.20020524 -0.01984325 -0.27399416], action=0, reward=1.0, next_state=[-0.04428472  0.00537196 -0.02532314  0.0123647 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 25 ] state=[-0.04428472  0.00537196 -0.02532314  0.0123647 ], action=0, reward=1.0, next_state=[-0.04417728 -0.18937785 -0.02507584  0.29695154]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 26 ] state=[-0.04417728 -0.18937785 -0.02507584  0.29695154], action=0, reward=1.0, next_state=[-0.04796483 -0.38413352 -0.01913681  0.58162169]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 27 ] state=[-0.04796483 -0.38413352 -0.01913681  0.58162169], action=1, reward=1.0, next_state=[-0.0556475  -0.18874874 -0.00750438  0.28297229]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 28 ] state=[-0.0556475  -0.18874874 -0.00750438  0.28297229], action=1, reward=1.0, next_state=[-0.05942248  0.00647944 -0.00184493 -0.012068  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 29 ] state=[-0.05942248  0.00647944 -0.00184493 -0.012068  ], action=1, reward=1.0, next_state=[-0.05929289  0.2016278  -0.00208629 -0.30533245]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 30 ] state=[-0.05929289  0.2016278  -0.00208629 -0.30533245], action=0, reward=1.0, next_state=[-0.05526033  0.00653565 -0.00819294 -0.01330822]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 31 ] state=[-0.05526033  0.00653565 -0.00819294 -0.01330822], action=1, reward=1.0, next_state=[-0.05512962  0.20177413 -0.00845911 -0.30856482]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 32 ] state=[-0.05512962  0.20177413 -0.00845911 -0.30856482], action=0, reward=1.0, next_state=[-0.05109414  0.00677372 -0.0146304  -0.01856162]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 33 ] state=[-0.05109414  0.00677372 -0.0146304  -0.01856162], action=1, reward=1.0, next_state=[-0.05095866  0.2021024  -0.01500164 -0.31582447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 34 ] state=[-0.05095866  0.2021024  -0.01500164 -0.31582447], action=0, reward=1.0, next_state=[-0.04691662  0.00719731 -0.02131813 -0.02791005]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 35 ] state=[-0.04691662  0.00719731 -0.02131813 -0.02791005], action=0, reward=1.0, next_state=[-0.04677267 -0.18761254 -0.02187633  0.25797128]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 36 ] state=[-0.04677267 -0.18761254 -0.02187633  0.25797128], action=0, reward=1.0, next_state=[-0.05052492 -0.38241545 -0.0167169   0.5436746 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 37 ] state=[-0.05052492 -0.38241545 -0.0167169   0.5436746 ], action=1, reward=1.0, next_state=[-0.05817323 -0.18706261 -0.00584341  0.24577175]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 38 ] state=[-0.05817323 -0.18706261 -0.00584341  0.24577175], action=1, reward=1.0, next_state=[-0.06191448  0.00814231 -0.00092797 -0.04874859]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 39 ] state=[-0.06191448  0.00814231 -0.00092797 -0.04874859], action=1, reward=1.0, next_state=[-0.06175164  0.20327755 -0.00190295 -0.34172415]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 40 ] state=[-0.06175164  0.20327755 -0.00190295 -0.34172415], action=0, reward=1.0, next_state=[-0.05768608  0.00818273 -0.00873743 -0.04964191]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 41 ] state=[-0.05768608  0.00818273 -0.00873743 -0.04964191], action=0, reward=1.0, next_state=[-0.05752243 -0.18681285 -0.00973027  0.24027153]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 42 ] state=[-0.05752243 -0.18681285 -0.00973027  0.24027153], action=1, reward=1.0, next_state=[-0.06125869  0.00844674 -0.00492484 -0.05546464]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 43 ] state=[-0.06125869  0.00844674 -0.00492484 -0.05546464], action=0, reward=1.0, next_state=[-0.06108975 -0.18660426 -0.00603413  0.2356604 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 44 ] state=[-0.06108975 -0.18660426 -0.00603413  0.2356604 ], action=0, reward=1.0, next_state=[-0.06482184 -0.38163948 -0.00132092  0.52643389]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 45 ] state=[-0.06482184 -0.38163948 -0.00132092  0.52643389], action=1, reward=1.0, next_state=[-0.07245463 -0.18649897  0.00920776  0.23333502]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 46 ] state=[-0.07245463 -0.18649897  0.00920776  0.23333502], action=1, reward=1.0, next_state=[-0.07618461  0.00849022  0.01387446 -0.05642932]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 47 ] state=[-0.07618461  0.00849022  0.01387446 -0.05642932], action=0, reward=1.0, next_state=[-0.0760148  -0.18682789  0.01274587  0.24059861]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 48 ] state=[-0.0760148  -0.18682789  0.01274587  0.24059861], action=1, reward=1.0, next_state=[-0.07975136  0.00810969  0.01755784 -0.04803682]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 49 ] state=[-0.07975136  0.00810969  0.01755784 -0.04803682], action=1, reward=1.0, next_state=[-0.07958917  0.20297553  0.01659711 -0.33512879]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 50 ] state=[-0.07958917  0.20297553  0.01659711 -0.33512879], action=0, reward=1.0, next_state=[-0.07552966  0.00762135  0.00989453 -0.03725856]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 51 ] state=[-0.07552966  0.00762135  0.00989453 -0.03725856], action=0, reward=1.0, next_state=[-0.07537723 -0.18764108  0.00914936  0.2585297 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 52 ] state=[-0.07537723 -0.18764108  0.00914936  0.2585297 ], action=0, reward=1.0, next_state=[-0.07913005 -0.38289245  0.01431995  0.55408436]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 347 ][ timestamp 53 ] state=[-0.07913005 -0.38289245  0.01431995  0.55408436], action=0, reward=1.0, next_state=[-0.0867879  -0.57821252  0.02540164  0.85124436]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 54 ] state=[-0.0867879  -0.57821252  0.02540164  0.85124436], action=1, reward=1.0, next_state=[-0.09835215 -0.38344595  0.04242653  0.56665603]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 55 ] state=[-0.09835215 -0.38344595  0.04242653  0.56665603], action=1, reward=1.0, next_state=[-0.10602107 -0.18894404  0.05375965  0.28763536]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 56 ] state=[-0.10602107 -0.18894404  0.05375965  0.28763536], action=1, reward=1.0, next_state=[-0.10979995  0.00537169  0.05951236  0.01238059]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 57 ] state=[-0.10979995  0.00537169  0.05951236  0.01238059], action=1, reward=1.0, next_state=[-0.10969252  0.1995919   0.05975997 -0.26094766]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 58 ] state=[-0.10969252  0.1995919   0.05975997 -0.26094766], action=1, reward=1.0, next_state=[-0.10570068  0.39381212  0.05454101 -0.53419897]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 59 ] state=[-0.10570068  0.39381212  0.05454101 -0.53419897], action=0, reward=1.0, next_state=[-0.09782444  0.19796728  0.04385703 -0.22484144]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 60 ] state=[-0.09782444  0.19796728  0.04385703 -0.22484144], action=0, reward=1.0, next_state=[-0.09386509  0.00224685  0.03936021  0.08134675]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 61 ] state=[-0.09386509  0.00224685  0.03936021  0.08134675], action=1, reward=1.0, next_state=[-0.09382015  0.19678312  0.04098714 -0.19866273]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 62 ] state=[-0.09382015  0.19678312  0.04098714 -0.19866273], action=1, reward=1.0, next_state=[-0.08988449  0.39129559  0.03701389 -0.47813955]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 63 ] state=[-0.08988449  0.39129559  0.03701389 -0.47813955], action=0, reward=1.0, next_state=[-0.08205858  0.19567115  0.0274511  -0.17402427]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 64 ] state=[-0.08205858  0.19567115  0.0274511  -0.17402427], action=0, reward=1.0, next_state=[-0.07814516  0.00016729  0.02397061  0.12719063]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 65 ] state=[-0.07814516  0.00016729  0.02397061  0.12719063], action=1, reward=1.0, next_state=[-0.07814181  0.19493779  0.02651442 -0.15783451]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 66 ] state=[-0.07814181  0.19493779  0.02651442 -0.15783451], action=1, reward=1.0, next_state=[-0.07424305  0.3896703   0.02335773 -0.44203627]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 67 ] state=[-0.07424305  0.3896703   0.02335773 -0.44203627], action=0, reward=1.0, next_state=[-0.06644965  0.19422574  0.01451701 -0.14208285]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 68 ] state=[-0.06644965  0.19422574  0.01451701 -0.14208285], action=1, reward=1.0, next_state=[-0.06256513  0.3891368   0.01167535 -0.43015079]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 69 ] state=[-0.06256513  0.3891368   0.01167535 -0.43015079], action=0, reward=1.0, next_state=[-0.0547824   0.19385148  0.00307233 -0.13381029]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 70 ] state=[-0.0547824   0.19385148  0.00307233 -0.13381029], action=1, reward=1.0, next_state=[-5.09053675e-02  3.88929288e-01  3.96128395e-04 -4.25522362e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 71 ] state=[-5.09053675e-02  3.88929288e-01  3.96128395e-04 -4.25522362e-01], action=0, reward=1.0, next_state=[-0.04312678  0.19380173 -0.00811432 -0.13271458]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 72 ] state=[-0.04312678  0.19380173 -0.00811432 -0.13271458], action=1, reward=1.0, next_state=[-0.03925075  0.38903897 -0.01076861 -0.42794638]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 73 ] state=[-0.03925075  0.38903897 -0.01076861 -0.42794638], action=1, reward=1.0, next_state=[-0.03146997  0.58431176 -0.01932754 -0.7240045 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 74 ] state=[-0.03146997  0.58431176 -0.01932754 -0.7240045 ], action=0, reward=1.0, next_state=[-0.01978373  0.38946237 -0.03380763 -0.43746694]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 75 ] state=[-0.01978373  0.38946237 -0.03380763 -0.43746694], action=1, reward=1.0, next_state=[-0.01199449  0.58504614 -0.04255697 -0.74061251]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 76 ] state=[-0.01199449  0.58504614 -0.04255697 -0.74061251], action=0, reward=1.0, next_state=[-2.93562515e-04  3.90536774e-01 -5.73692172e-02 -4.61620600e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 77 ] state=[-2.93562515e-04  3.90536774e-01 -5.73692172e-02 -4.61620600e-01], action=0, reward=1.0, next_state=[ 0.00751717  0.19627059 -0.06660163 -0.18755802]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 78 ] state=[ 0.00751717  0.19627059 -0.06660163 -0.18755802], action=0, reward=1.0, next_state=[ 0.01144258  0.00216161 -0.07035279  0.08339352]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 79 ] state=[ 0.01144258  0.00216161 -0.07035279  0.08339352], action=0, reward=1.0, next_state=[ 0.01148582 -0.19188503 -0.06868492  0.35307678]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 80 ] state=[ 0.01148582 -0.19188503 -0.06868492  0.35307678], action=1, reward=1.0, next_state=[ 0.00764812  0.00414296 -0.06162338  0.03955061]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 81 ] state=[ 0.00764812  0.00414296 -0.06162338  0.03955061], action=1, reward=1.0, next_state=[ 0.00773098  0.20009198 -0.06083237 -0.27192081]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 82 ] state=[ 0.00773098  0.20009198 -0.06083237 -0.27192081], action=1, reward=1.0, next_state=[ 0.01173282  0.39602683 -0.06627079 -0.58315314]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 83 ] state=[ 0.01173282  0.39602683 -0.06627079 -0.58315314], action=0, reward=1.0, next_state=[ 0.01965335  0.20189284 -0.07793385 -0.31206073]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 84 ] state=[ 0.01965335  0.20189284 -0.07793385 -0.31206073], action=1, reward=1.0, next_state=[ 0.02369121  0.39803357 -0.08417507 -0.62826817]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 85 ] state=[ 0.02369121  0.39803357 -0.08417507 -0.62826817], action=0, reward=1.0, next_state=[ 0.03165188  0.204181   -0.09674043 -0.36323712]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 86 ] state=[ 0.03165188  0.204181   -0.09674043 -0.36323712], action=0, reward=1.0, next_state=[ 0.0357355   0.01055756 -0.10400517 -0.10255728]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 87 ] state=[ 0.0357355   0.01055756 -0.10400517 -0.10255728], action=0, reward=1.0, next_state=[ 0.03594665 -0.18293201 -0.10605632  0.15558632]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 88 ] state=[ 0.03594665 -0.18293201 -0.10605632  0.15558632], action=0, reward=1.0, next_state=[ 0.03228801 -0.37638822 -0.10294459  0.41301803]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 89 ] state=[ 0.03228801 -0.37638822 -0.10294459  0.41301803], action=1, reward=1.0, next_state=[ 0.02476025 -0.17996917 -0.09468423  0.08973699]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 90 ] state=[ 0.02476025 -0.17996917 -0.09468423  0.08973699], action=0, reward=1.0, next_state=[ 0.02116086 -0.37361541 -0.09288949  0.35110968]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 91 ] state=[ 0.02116086 -0.37361541 -0.09288949  0.35110968], action=0, reward=1.0, next_state=[ 0.01368855 -0.56730206 -0.0858673   0.61311689]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 92 ] state=[ 0.01368855 -0.56730206 -0.0858673   0.61311689], action=1, reward=1.0, next_state=[ 0.00234251 -0.37109174 -0.07360496  0.29467179]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 93 ] state=[ 0.00234251 -0.37109174 -0.07360496  0.29467179], action=1, reward=1.0, next_state=[-0.00507932 -0.17500182 -0.06771152 -0.02028701]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 94 ] state=[-0.00507932 -0.17500182 -0.06771152 -0.02028701], action=0, reward=1.0, next_state=[-0.00857936 -0.36909067 -0.06811726  0.25028714]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 95 ] state=[-0.00857936 -0.36909067 -0.06811726  0.25028714], action=1, reward=1.0, next_state=[-0.01596117 -0.17306544 -0.06311152 -0.06307979]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 96 ] state=[-0.01596117 -0.17306544 -0.06311152 -0.06307979], action=0, reward=1.0, next_state=[-0.01942248 -0.36722842 -0.06437312  0.20904237]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 97 ] state=[-0.01942248 -0.36722842 -0.06437312  0.20904237], action=0, reward=1.0, next_state=[-0.02676705 -0.56137367 -0.06019227  0.48074444]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 98 ] state=[-0.02676705 -0.56137367 -0.06019227  0.48074444], action=1, reward=1.0, next_state=[-0.03799452 -0.365456   -0.05057738  0.1697143 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 99 ] state=[-0.03799452 -0.365456   -0.05057738  0.1697143 ], action=0, reward=1.0, next_state=[-0.04530364 -0.5598189  -0.04718309  0.44602243]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 100 ] state=[-0.04530364 -0.5598189  -0.04718309  0.44602243], action=1, reward=1.0, next_state=[-0.05650002 -0.3640623  -0.03826265  0.13884763]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 347 ][ timestamp 101 ] state=[-0.05650002 -0.3640623  -0.03826265  0.13884763], action=1, reward=1.0, next_state=[-0.06378127 -0.16841381 -0.03548569 -0.16565678]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 102 ] state=[-0.06378127 -0.16841381 -0.03548569 -0.16565678], action=0, reward=1.0, next_state=[-0.06714954 -0.3630103  -0.03879883  0.11562359]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 103 ] state=[-0.06714954 -0.3630103  -0.03879883  0.11562359], action=0, reward=1.0, next_state=[-0.07440975 -0.55755546 -0.03648636  0.39581773]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 104 ] state=[-0.07440975 -0.55755546 -0.03648636  0.39581773], action=1, reward=1.0, next_state=[-0.08556086 -0.36193533 -0.02857     0.09185821]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 105 ] state=[-0.08556086 -0.36193533 -0.02857     0.09185821], action=1, reward=1.0, next_state=[-0.09279956 -0.16641577 -0.02673284 -0.20969987]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 106 ] state=[-0.09279956 -0.16641577 -0.02673284 -0.20969987], action=1, reward=1.0, next_state=[-0.09612788  0.02907801 -0.03092684 -0.5106943 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 107 ] state=[-0.09612788  0.02907801 -0.03092684 -0.5106943 ], action=0, reward=1.0, next_state=[-0.09554632 -0.16559493 -0.04114072 -0.22791556]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 108 ] state=[-0.09554632 -0.16559493 -0.04114072 -0.22791556], action=0, reward=1.0, next_state=[-0.09885822 -0.36010554 -0.04569903  0.05151151]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 109 ] state=[-0.09885822 -0.36010554 -0.04569903  0.05151151], action=0, reward=1.0, next_state=[-0.10606033 -0.55454344 -0.0446688   0.32943303]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 110 ] state=[-0.10606033 -0.55454344 -0.0446688   0.32943303], action=0, reward=1.0, next_state=[-0.1171512  -0.74900198 -0.03808014  0.60770163]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 111 ] state=[-0.1171512  -0.74900198 -0.03808014  0.60770163], action=1, reward=1.0, next_state=[-0.13213124 -0.5533689  -0.02592611  0.30327188]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 112 ] state=[-0.13213124 -0.5533689  -0.02592611  0.30327188], action=0, reward=1.0, next_state=[-0.14319861 -0.74811195 -0.01986067  0.58766687]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 113 ] state=[-0.14319861 -0.74811195 -0.01986067  0.58766687], action=0, reward=1.0, next_state=[-0.15816085 -0.94295021 -0.00810733  0.87402797]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 114 ] state=[-0.15816085 -0.94295021 -0.00810733  0.87402797], action=0, reward=1.0, next_state=[-0.17701986 -1.137961    0.00937323  1.16415101]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 115 ] state=[-0.17701986 -1.137961    0.00937323  1.16415101], action=1, reward=1.0, next_state=[-0.19977908 -0.94296233  0.03265625  0.87442154]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 116 ] state=[-0.19977908 -0.94296233  0.03265625  0.87442154], action=1, reward=1.0, next_state=[-0.21863832 -0.74829921  0.05014468  0.59218178]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 117 ] state=[-0.21863832 -0.74829921  0.05014468  0.59218178], action=1, reward=1.0, next_state=[-0.23360431 -0.55391379  0.06198831  0.31570651]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 118 ] state=[-0.23360431 -0.55391379  0.06198831  0.31570651], action=1, reward=1.0, next_state=[-0.24468258 -0.35972707  0.06830244  0.04319877]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 119 ] state=[-0.24468258 -0.35972707  0.06830244  0.04319877], action=1, reward=1.0, next_state=[-0.25187713 -0.16564765  0.06916642 -0.22717624]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 120 ] state=[-0.25187713 -0.16564765  0.06916642 -0.22717624], action=1, reward=1.0, next_state=[-0.25519008  0.02842117  0.06462289 -0.49726471]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 121 ] state=[-0.25519008  0.02842117  0.06462289 -0.49726471], action=1, reward=1.0, next_state=[-0.25462165  0.22257524  0.0546776  -0.76890202]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 122 ] state=[-0.25462165  0.22257524  0.0546776  -0.76890202], action=1, reward=1.0, next_state=[-0.25017015  0.41690365  0.03929956 -1.0438918 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 123 ] state=[-0.25017015  0.41690365  0.03929956 -1.0438918 ], action=0, reward=1.0, next_state=[-0.24183208  0.22128253  0.01842172 -0.7391356 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 124 ] state=[-0.24183208  0.22128253  0.01842172 -0.7391356 ], action=0, reward=1.0, next_state=[-0.23740643  0.02591112  0.00363901 -0.44071252]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 125 ] state=[-0.23740643  0.02591112  0.00363901 -0.44071252], action=0, reward=1.0, next_state=[-0.2368882  -0.16926214 -0.00517524 -0.1468847 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 126 ] state=[-0.2368882  -0.16926214 -0.00517524 -0.1468847 ], action=1, reward=1.0, next_state=[-0.24027345  0.02593353 -0.00811293 -0.44119581]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 127 ] state=[-0.24027345  0.02593353 -0.00811293 -0.44119581], action=0, reward=1.0, next_state=[-0.23975478 -0.16907267 -0.01693685 -0.1510813 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 128 ] state=[-0.23975478 -0.16907267 -0.01693685 -0.1510813 ], action=0, reward=1.0, next_state=[-0.24313623 -0.36394806 -0.01995848  0.13621066]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 129 ] state=[-0.24313623 -0.36394806 -0.01995848  0.13621066], action=1, reward=1.0, next_state=[-0.25041519 -0.168546   -0.01723426 -0.16270145]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 130 ] state=[-0.25041519 -0.168546   -0.01723426 -0.16270145], action=1, reward=1.0, next_state=[-0.25378611  0.02681837 -0.02048829 -0.46077112]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 131 ] state=[-0.25378611  0.02681837 -0.02048829 -0.46077112], action=0, reward=1.0, next_state=[-0.25324974 -0.16800809 -0.02970371 -0.17461589]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 132 ] state=[-0.25324974 -0.16800809 -0.02970371 -0.17461589], action=0, reward=1.0, next_state=[-0.25660991 -0.36269262 -0.03319603  0.10855047]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 133 ] state=[-0.25660991 -0.36269262 -0.03319603  0.10855047], action=0, reward=1.0, next_state=[-0.26386376 -0.55732353 -0.03102502  0.39057815]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 134 ] state=[-0.26386376 -0.55732353 -0.03102502  0.39057815], action=0, reward=1.0, next_state=[-0.27501023 -0.75199173 -0.02321346  0.67332004]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 135 ] state=[-0.27501023 -0.75199173 -0.02321346  0.67332004], action=1, reward=1.0, next_state=[-0.29005006 -0.55655496 -0.00974706  0.37341972]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 136 ] state=[-0.29005006 -0.55655496 -0.00974706  0.37341972], action=1, reward=1.0, next_state=[-0.30118116 -0.36129591 -0.00227866  0.07767947]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 137 ] state=[-0.30118116 -0.36129591 -0.00227866  0.07767947], action=1, reward=1.0, next_state=[-0.30840708 -0.16614137 -0.00072507 -0.21572151]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 138 ] state=[-0.30840708 -0.16614137 -0.00072507 -0.21572151], action=0, reward=1.0, next_state=[-0.31172991 -0.36125295 -0.0050395   0.07673261]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 139 ] state=[-0.31172991 -0.36125295 -0.0050395   0.07673261], action=0, reward=1.0, next_state=[-0.31895497 -0.55630229 -0.00350485  0.3678213 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 140 ] state=[-0.31895497 -0.55630229 -0.00350485  0.3678213 ], action=1, reward=1.0, next_state=[-0.33008101 -0.36113071  0.00385157  0.07403531]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 141 ] state=[-0.33008101 -0.36113071  0.00385157  0.07403531], action=0, reward=1.0, next_state=[-0.33730363 -0.55630767  0.00533228  0.36793093]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 142 ] state=[-0.33730363 -0.55630767  0.00533228  0.36793093], action=1, reward=1.0, next_state=[-0.34842978 -0.36126189  0.0126909   0.0769341 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 143 ] state=[-0.34842978 -0.36126189  0.0126909   0.0769341 ], action=0, reward=1.0, next_state=[-0.35565502 -0.55656345  0.01422958  0.37359388]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 347 ][ timestamp 144 ] state=[-0.35565502 -0.55656345  0.01422958  0.37359388], action=1, reward=1.0, next_state=[-0.36678629 -0.3616465   0.02170146  0.0854314 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 145 ] state=[-0.36678629 -0.3616465   0.02170146  0.0854314 ], action=0, reward=1.0, next_state=[-0.37401922 -0.55707269  0.02341009  0.3848814 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 146 ] state=[-0.37401922 -0.55707269  0.02341009  0.3848814 ], action=1, reward=1.0, next_state=[-0.38516067 -0.36229078  0.03110771  0.09967052]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 147 ] state=[-0.38516067 -0.36229078  0.03110771  0.09967052], action=1, reward=1.0, next_state=[-0.39240649 -0.16762815  0.03310112 -0.18303796]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 148 ] state=[-0.39240649 -0.16762815  0.03310112 -0.18303796], action=0, reward=1.0, next_state=[-0.39575905 -0.36320772  0.02944036  0.11990064]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 149 ] state=[-0.39575905 -0.36320772  0.02944036  0.11990064], action=1, reward=1.0, next_state=[-0.4030232  -0.16851967  0.03183838 -0.16335067]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 150 ] state=[-0.4030232  -0.16851967  0.03183838 -0.16335067], action=0, reward=1.0, next_state=[-0.4063936  -0.36408259  0.02857136  0.13920395]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 151 ] state=[-0.4063936  -0.36408259  0.02857136  0.13920395], action=1, reward=1.0, next_state=[-0.41367525 -0.16938126  0.03135544 -0.14433001]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 152 ] state=[-0.41367525 -0.16938126  0.03135544 -0.14433001], action=0, reward=1.0, next_state=[-0.41706287 -0.3649379   0.02846884  0.15807775]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 153 ] state=[-0.41706287 -0.3649379   0.02846884  0.15807775], action=1, reward=1.0, next_state=[-0.42436163 -0.17023486  0.0316304  -0.12548975]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 154 ] state=[-0.42436163 -0.17023486  0.0316304  -0.12548975], action=1, reward=1.0, next_state=[-0.42776633  0.02442001  0.0291206  -0.40802822]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 155 ] state=[-0.42776633  0.02442001  0.0291206  -0.40802822], action=0, reward=1.0, next_state=[-0.42727793 -0.17110248  0.02096004 -0.10630858]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 156 ] state=[-0.42727793 -0.17110248  0.02096004 -0.10630858], action=0, reward=1.0, next_state=[-0.43069998 -0.36651843  0.01883387  0.19291275]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 157 ] state=[-0.43069998 -0.36651843  0.01883387  0.19291275], action=1, reward=1.0, next_state=[-0.43803035 -0.1716709   0.02269212 -0.09376989]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 158 ] state=[-0.43803035 -0.1716709   0.02269212 -0.09376989], action=1, reward=1.0, next_state=[-0.44146377  0.02311859  0.02081672 -0.37920797]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 159 ] state=[-0.44146377  0.02311859  0.02081672 -0.37920797], action=0, reward=1.0, next_state=[-0.44100139 -0.17229271  0.01323256 -0.08003487]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 160 ] state=[-0.44100139 -0.17229271  0.01323256 -0.08003487], action=0, reward=1.0, next_state=[-0.44444725 -0.36760182  0.01163187  0.21679342]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 161 ] state=[-0.44444725 -0.36760182  0.01163187  0.21679342], action=0, reward=1.0, next_state=[-0.45179928 -0.56288811  0.01596774  0.51312273]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 162 ] state=[-0.45179928 -0.56288811  0.01596774  0.51312273], action=1, reward=1.0, next_state=[-0.46305705 -0.36799466  0.02623019  0.22551413]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 163 ] state=[-0.46305705 -0.36799466  0.02623019  0.22551413], action=1, reward=1.0, next_state=[-0.47041694 -0.17325721  0.03074047 -0.05878076]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 164 ] state=[-0.47041694 -0.17325721  0.03074047 -0.05878076], action=0, reward=1.0, next_state=[-0.47388208 -0.36880613  0.02956486  0.24344031]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 165 ] state=[-0.47388208 -0.36880613  0.02956486  0.24344031], action=1, reward=1.0, next_state=[-0.48125821 -0.17411867  0.03443366 -0.03977246]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 166 ] state=[-0.48125821 -0.17411867  0.03443366 -0.03977246], action=0, reward=1.0, next_state=[-0.48474058 -0.36971705  0.03363821  0.26357268]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 167 ] state=[-0.48474058 -0.36971705  0.03363821  0.26357268], action=1, reward=1.0, next_state=[-0.49213492 -0.17509098  0.03890967 -0.01831349]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 168 ] state=[-0.49213492 -0.17509098  0.03890967 -0.01831349], action=1, reward=1.0, next_state=[-0.49563674  0.01945198  0.0385434  -0.29847051]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 169 ] state=[-0.49563674  0.01945198  0.0385434  -0.29847051], action=0, reward=1.0, next_state=[-0.4952477  -0.17619758  0.03257399  0.00611482]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 170 ] state=[-0.4952477  -0.17619758  0.03257399  0.00611482], action=0, reward=1.0, next_state=[-0.49877165 -0.37177118  0.03269628  0.30889466]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 171 ] state=[-0.49877165 -0.37177118  0.03269628  0.30889466], action=1, reward=1.0, next_state=[-0.50620708 -0.17712999  0.03887418  0.02669991]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 172 ] state=[-0.50620708 -0.17712999  0.03887418  0.02669991], action=1, reward=1.0, next_state=[-0.50974968  0.01741354  0.03940818 -0.25346878]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 173 ] state=[-0.50974968  0.01741354  0.03940818 -0.25346878], action=0, reward=1.0, next_state=[-0.50940141 -0.17824831  0.0343388   0.05137912]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 174 ] state=[-0.50940141 -0.17824831  0.0343388   0.05137912], action=1, reward=1.0, next_state=[-0.51296637  0.01636487  0.03536638 -0.23027492]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 175 ] state=[-0.51296637  0.01636487  0.03536638 -0.23027492], action=1, reward=1.0, next_state=[-0.51263907  0.21096406  0.03076088 -0.51159564]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 176 ] state=[-0.51263907  0.21096406  0.03076088 -0.51159564], action=1, reward=1.0, next_state=[-0.50841979  0.40563952  0.02052897 -0.7944284 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 177 ] state=[-0.50841979  0.40563952  0.02052897 -0.7944284 ], action=1, reward=1.0, next_state=[-0.500307    0.60047377  0.0046404  -1.0805831 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 178 ] state=[-0.500307    0.60047377  0.0046404  -1.0805831 ], action=0, reward=1.0, next_state=[-0.48829753  0.40529086 -0.01697126 -0.78644762]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 179 ] state=[-0.48829753  0.40529086 -0.01697126 -0.78644762], action=0, reward=1.0, next_state=[-0.48019171  0.21040612 -0.03270021 -0.49915192]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 180 ] state=[-0.48019171  0.21040612 -0.03270021 -0.49915192], action=0, reward=1.0, next_state=[-0.47598359  0.01576008 -0.04268325 -0.21695109]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 181 ] state=[-0.47598359  0.01576008 -0.04268325 -0.21695109], action=0, reward=1.0, next_state=[-0.47566839 -0.17872652 -0.04702227  0.06196804]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 182 ] state=[-0.47566839 -0.17872652 -0.04702227  0.06196804], action=1, reward=1.0, next_state=[-0.47924292  0.01703698 -0.04578291 -0.24517209]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 183 ] state=[-0.47924292  0.01703698 -0.04578291 -0.24517209], action=1, reward=1.0, next_state=[-0.47890218  0.21278192 -0.05068635 -0.55193731]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 184 ] state=[-0.47890218  0.21278192 -0.05068635 -0.55193731], action=1, reward=1.0, next_state=[-0.47464654  0.40857771 -0.0617251  -0.86014921]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 185 ] state=[-0.47464654  0.40857771 -0.0617251  -0.86014921], action=0, reward=1.0, next_state=[-0.46647498  0.2143483  -0.07892808 -0.58749558]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 186 ] state=[-0.46647498  0.2143483  -0.07892808 -0.58749558], action=0, reward=1.0, next_state=[-0.46218802  0.02041526 -0.09067799 -0.32068243]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 347 ][ timestamp 187 ] state=[-0.46218802  0.02041526 -0.09067799 -0.32068243], action=0, reward=1.0, next_state=[-0.46177971 -0.17330621 -0.09709164 -0.05791688]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 188 ] state=[-0.46177971 -0.17330621 -0.09709164 -0.05791688], action=0, reward=1.0, next_state=[-0.46524584 -0.36691165 -0.09824998  0.20262344]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 189 ] state=[-0.46524584 -0.36691165 -0.09824998  0.20262344], action=0, reward=1.0, next_state=[-0.47258407 -0.56050123 -0.09419751  0.46276835]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 190 ] state=[-0.47258407 -0.56050123 -0.09419751  0.46276835], action=0, reward=1.0, next_state=[-0.48379409 -0.75417451 -0.08494214  0.72433722]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 191 ] state=[-0.48379409 -0.75417451 -0.08494214  0.72433722], action=1, reward=1.0, next_state=[-0.49887759 -0.55798697 -0.0704554   0.40617394]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 192 ] state=[-0.49887759 -0.55798697 -0.0704554   0.40617394], action=1, reward=1.0, next_state=[-0.51003732 -0.36194035 -0.06233192  0.09213683]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 193 ] state=[-0.51003732 -0.36194035 -0.06233192  0.09213683], action=0, reward=1.0, next_state=[-0.51727613 -0.55611607 -0.06048918  0.36452105]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 194 ] state=[-0.51727613 -0.55611607 -0.06048918  0.36452105], action=1, reward=1.0, next_state=[-0.52839845 -0.36018895 -0.05319876  0.0533949 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 195 ] state=[-0.52839845 -0.36018895 -0.05319876  0.0533949 ], action=1, reward=1.0, next_state=[-0.53560223 -0.16434616 -0.05213087 -0.25558676]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 196 ] state=[-0.53560223 -0.16434616 -0.05213087 -0.25558676], action=1, reward=1.0, next_state=[-0.53888916  0.03147985 -0.0572426  -0.56424625]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 197 ] state=[-0.53888916  0.03147985 -0.0572426  -0.56424625], action=1, reward=1.0, next_state=[-0.53825956  0.22735627 -0.06852753 -0.87439977]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 198 ] state=[-0.53825956  0.22735627 -0.06852753 -0.87439977], action=0, reward=1.0, next_state=[-0.53371243  0.03322955 -0.08601552 -0.60402447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 199 ] state=[-0.53371243  0.03322955 -0.08601552 -0.60402447], action=0, reward=1.0, next_state=[-0.53304784 -0.1605908  -0.09809601 -0.33962619]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 200 ] state=[-0.53304784 -0.1605908  -0.09809601 -0.33962619], action=0, reward=1.0, next_state=[-0.53625966 -0.35419006 -0.10488853 -0.0794174 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 201 ] state=[-0.53625966 -0.35419006 -0.10488853 -0.0794174 ], action=0, reward=1.0, next_state=[-0.54334346 -0.54766423 -0.10647688  0.17841821]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 202 ] state=[-0.54334346 -0.54766423 -0.10647688  0.17841821], action=0, reward=1.0, next_state=[-0.55429674 -0.74111409 -0.10290852  0.43570456]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 203 ] state=[-0.55429674 -0.74111409 -0.10290852  0.43570456], action=1, reward=1.0, next_state=[-0.56911903 -0.54469737 -0.09419443  0.11243644]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 204 ] state=[-0.56911903 -0.54469737 -0.09419443  0.11243644], action=0, reward=1.0, next_state=[-0.58001297 -0.73835222 -0.0919457   0.37397877]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 205 ] state=[-0.58001297 -0.73835222 -0.0919457   0.37397877], action=1, reward=1.0, next_state=[-0.59478002 -0.54205267 -0.08446612  0.05377924]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 206 ] state=[-0.59478002 -0.54205267 -0.08446612  0.05377924], action=0, reward=1.0, next_state=[-0.60562107 -0.73586831 -0.08339054  0.3186627 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 207 ] state=[-0.60562107 -0.73586831 -0.08339054  0.3186627 ], action=0, reward=1.0, next_state=[-0.62033844 -0.9297097  -0.07701728  0.58392599]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 208 ] state=[-0.62033844 -0.9297097  -0.07701728  0.58392599], action=0, reward=1.0, next_state=[-0.63893263 -1.12367305 -0.06533876  0.85138784]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 209 ] state=[-0.63893263 -1.12367305 -0.06533876  0.85138784], action=0, reward=1.0, next_state=[-0.66140609 -1.31784622 -0.04831101  1.12283017]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 210 ] state=[-0.66140609 -1.31784622 -0.04831101  1.12283017], action=0, reward=1.0, next_state=[-0.68776302 -1.51230261 -0.0258544   1.39997652]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 211 ] state=[-0.68776302 -1.51230261 -0.0258544   1.39997652], action=0, reward=1.0, next_state=[-0.71800907 -1.70709384  0.00214513  1.68446538]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 212 ] state=[-0.71800907 -1.70709384  0.00214513  1.68446538], action=0, reward=1.0, next_state=[-0.75215095 -1.90224055  0.03583443  1.97781544]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 213 ] state=[-0.75215095 -1.90224055  0.03583443  1.97781544], action=0, reward=1.0, next_state=[-0.79019576 -2.0977209   0.07539074  2.28138078]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 214 ] state=[-0.79019576 -2.0977209   0.07539074  2.28138078], action=0, reward=1.0, next_state=[-0.83215017 -2.29345627  0.12101836  2.59629373]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 215 ] state=[-0.83215017 -2.29345627  0.12101836  2.59629373], action=0, reward=1.0, next_state=[-0.8780193  -2.48929415  0.17294423  2.92339469]\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ][ timestamp 216 ] state=[-0.8780193  -2.48929415  0.17294423  2.92339469], action=0, reward=-1.0, next_state=[-0.92780518 -2.68498839  0.23141213  3.26314963]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 347: Exploration_rate=0.01. Score=216.\n",
      "[ episode 348 ] state=[ 0.02803236 -0.04122525 -0.00141153 -0.04072042]\n",
      "[ episode 348 ][ timestamp 1 ] state=[ 0.02803236 -0.04122525 -0.00141153 -0.04072042], action=0, reward=1.0, next_state=[ 0.02720785 -0.23632693 -0.00222594  0.25151682]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 2 ] state=[ 0.02720785 -0.23632693 -0.00222594  0.25151682], action=1, reward=1.0, next_state=[ 0.02248131 -0.04117327  0.0028044  -0.04186738]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 3 ] state=[ 0.02248131 -0.04117327  0.0028044  -0.04186738], action=1, reward=1.0, next_state=[ 0.02165785  0.15390836  0.00196705 -0.33366417]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 4 ] state=[ 0.02165785  0.15390836  0.00196705 -0.33366417], action=0, reward=1.0, next_state=[ 0.02473602 -0.04124153 -0.00470623 -0.04036159]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 5 ] state=[ 0.02473602 -0.04124153 -0.00470623 -0.04036159], action=0, reward=1.0, next_state=[ 0.02391118 -0.23629568 -0.00551346  0.25083277]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 6 ] state=[ 0.02391118 -0.23629568 -0.00551346  0.25083277], action=1, reward=1.0, next_state=[ 0.01918527 -0.04109543 -0.00049681 -0.0435841 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 7 ] state=[ 0.01918527 -0.04109543 -0.00049681 -0.0435841 ], action=0, reward=1.0, next_state=[ 0.01836336 -0.23621026 -0.00136849  0.24894203]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 8 ] state=[ 0.01836336 -0.23621026 -0.00136849  0.24894203], action=1, reward=1.0, next_state=[ 0.01363916 -0.04106879  0.00361035 -0.04417223]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 9 ] state=[ 0.01363916 -0.04106879  0.00361035 -0.04417223], action=0, reward=1.0, next_state=[ 0.01281778 -0.23624233  0.0027269   0.2496476 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 10 ] state=[ 0.01281778 -0.23624233  0.0027269   0.2496476 ], action=1, reward=1.0, next_state=[ 0.00809294 -0.04115942  0.00771986 -0.04217395]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 11 ] state=[ 0.00809294 -0.04115942  0.00771986 -0.04217395], action=1, reward=1.0, next_state=[ 0.00726975  0.15385098  0.00687638 -0.33241123]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 348 ][ timestamp 12 ] state=[ 0.00726975  0.15385098  0.00687638 -0.33241123], action=0, reward=1.0, next_state=[ 0.01034677 -0.04136817  0.00022815 -0.03756779]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 13 ] state=[ 0.01034677 -0.04136817  0.00022815 -0.03756779], action=0, reward=1.0, next_state=[ 0.0095194  -0.23649339 -0.0005232   0.25518711]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 14 ] state=[ 0.0095194  -0.23649339 -0.0005232   0.25518711], action=1, reward=1.0, next_state=[ 0.00478954 -0.04136397  0.00458054 -0.03766079]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 15 ] state=[ 0.00478954 -0.04136397  0.00458054 -0.03766079], action=1, reward=1.0, next_state=[ 0.00396226  0.153692    0.00382732 -0.328895  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 16 ] state=[ 0.00396226  0.153692    0.00382732 -0.328895  ], action=1, reward=1.0, next_state=[ 0.0070361   0.34875925 -0.00275058 -0.62036851]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 17 ] state=[ 0.0070361   0.34875925 -0.00275058 -0.62036851], action=0, reward=1.0, next_state=[ 0.01401128  0.15367582 -0.01515795 -0.32855315]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 18 ] state=[ 0.01401128  0.15367582 -0.01515795 -0.32855315], action=0, reward=1.0, next_state=[ 0.0170848  -0.04122709 -0.02172901 -0.04068862]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 19 ] state=[ 0.0170848  -0.04122709 -0.02172901 -0.04068862], action=0, reward=1.0, next_state=[ 0.01626026 -0.23603082 -0.02254278  0.24506017]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 20 ] state=[ 0.01626026 -0.23603082 -0.02254278  0.24506017], action=0, reward=1.0, next_state=[ 0.01153964 -0.43082367 -0.01764158  0.53054818]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 21 ] state=[ 0.01153964 -0.43082367 -0.01764158  0.53054818], action=1, reward=1.0, next_state=[ 0.00292317 -0.23545805 -0.00703062  0.232359  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 22 ] state=[ 0.00292317 -0.23545805 -0.00703062  0.232359  ], action=0, reward=1.0, next_state=[-0.001786   -0.43047884 -0.00238344  0.52281598]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 23 ] state=[-0.001786   -0.43047884 -0.00238344  0.52281598], action=1, reward=1.0, next_state=[-0.01039557 -0.23532343  0.00807288  0.22938295]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 24 ] state=[-0.01039557 -0.23532343  0.00807288  0.22938295], action=0, reward=1.0, next_state=[-0.01510204 -0.43055981  0.01266054  0.52460138]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 25 ] state=[-0.01510204 -0.43055981  0.01266054  0.52460138], action=1, reward=1.0, next_state=[-0.02371324 -0.23561829  0.02315257  0.23593465]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 26 ] state=[-0.02371324 -0.23561829  0.02315257  0.23593465], action=1, reward=1.0, next_state=[-0.0284256  -0.04083465  0.02787126 -0.04935627]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 27 ] state=[-0.0284256  -0.04083465  0.02787126 -0.04935627], action=1, reward=1.0, next_state=[-0.0292423   0.1538768   0.02688414 -0.33311692]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 28 ] state=[-0.0292423   0.1538768   0.02688414 -0.33311692], action=0, reward=1.0, next_state=[-0.02616476 -0.04161728  0.0202218  -0.03207878]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 29 ] state=[-0.02616476 -0.04161728  0.0202218  -0.03207878], action=1, reward=1.0, next_state=[-0.02699711  0.15320893  0.01958022 -0.31831354]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 30 ] state=[-0.02699711  0.15320893  0.01958022 -0.31831354], action=1, reward=1.0, next_state=[-0.02393293  0.34804662  0.01321395 -0.60475783]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 31 ] state=[-0.02393293  0.34804662  0.01321395 -0.60475783], action=1, reward=1.0, next_state=[-0.01697199  0.54298131  0.0011188  -0.89324955]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 32 ] state=[-0.01697199  0.54298131  0.0011188  -0.89324955], action=1, reward=1.0, next_state=[-0.00611237  0.73808807 -0.01674619 -1.18558058]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 33 ] state=[-0.00611237  0.73808807 -0.01674619 -1.18558058], action=0, reward=1.0, next_state=[ 0.00864939  0.54318727 -0.04045781 -0.89819353]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 34 ] state=[ 0.00864939  0.54318727 -0.04045781 -0.89819353], action=1, reward=1.0, next_state=[ 0.01951314  0.73883356 -0.05842168 -1.20331416]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 35 ] state=[ 0.01951314  0.73883356 -0.05842168 -1.20331416], action=0, reward=1.0, next_state=[ 0.03428981  0.54451355 -0.08248796 -0.92949763]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 36 ] state=[ 0.03428981  0.54451355 -0.08248796 -0.92949763], action=0, reward=1.0, next_state=[ 0.04518008  0.35059611 -0.10107791 -0.66383448]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 37 ] state=[ 0.04518008  0.35059611 -0.10107791 -0.66383448], action=0, reward=1.0, next_state=[ 0.052192    0.15701475 -0.1143546  -0.40461084]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 38 ] state=[ 0.052192    0.15701475 -0.1143546  -0.40461084], action=0, reward=1.0, next_state=[ 0.0553323  -0.03631549 -0.12244682 -0.15005657]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 39 ] state=[ 0.0553323  -0.03631549 -0.12244682 -0.15005657], action=0, reward=1.0, next_state=[ 0.05460599 -0.22949068 -0.12544795  0.10162723]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 40 ] state=[ 0.05460599 -0.22949068 -0.12544795  0.10162723], action=0, reward=1.0, next_state=[ 0.05001617 -0.42261239 -0.12341541  0.35224834]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 41 ] state=[ 0.05001617 -0.42261239 -0.12341541  0.35224834], action=0, reward=1.0, next_state=[ 0.04156393 -0.61578301 -0.11637044  0.60360829]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 42 ] state=[ 0.04156393 -0.61578301 -0.11637044  0.60360829], action=0, reward=1.0, next_state=[ 0.02924827 -0.80910164 -0.10429827  0.85748926]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 43 ] state=[ 0.02924827 -0.80910164 -0.10429827  0.85748926], action=0, reward=1.0, next_state=[ 0.01306623 -1.0026598  -0.08714849  1.11564064]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 44 ] state=[ 0.01306623 -1.0026598  -0.08714849  1.11564064], action=1, reward=1.0, next_state=[-0.00698696 -0.80650875 -0.06483567  0.79694143]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 45 ] state=[-0.00698696 -0.80650875 -0.06483567  0.79694143], action=1, reward=1.0, next_state=[-0.02311714 -0.6105599  -0.04889685  0.48458737]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 46 ] state=[-0.02311714 -0.6105599  -0.04889685  0.48458737], action=1, reward=1.0, next_state=[-0.03532833 -0.41478323 -0.0392051   0.17690342]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 47 ] state=[-0.03532833 -0.41478323 -0.0392051   0.17690342], action=0, reward=1.0, next_state=[-0.043624   -0.60932283 -0.03566703  0.45696524]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 48 ] state=[-0.043624   -0.60932283 -0.03566703  0.45696524], action=0, reward=1.0, next_state=[-0.05581046 -0.80392286 -0.02652773  0.73819574]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 49 ] state=[-0.05581046 -0.80392286 -0.02652773  0.73819574], action=0, reward=1.0, next_state=[-0.07188891 -0.99866862 -0.01176381  1.02241337]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 50 ] state=[-0.07188891 -0.99866862 -0.01176381  1.02241337], action=1, reward=1.0, next_state=[-0.09186229 -0.80339194  0.00868446  0.72606014]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 51 ] state=[-0.09186229 -0.80339194  0.00868446  0.72606014], action=1, reward=1.0, next_state=[-0.10793012 -0.60839114  0.02320566  0.43612317]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 52 ] state=[-0.10793012 -0.60839114  0.02320566  0.43612317], action=1, reward=1.0, next_state=[-0.12009795 -0.41360524  0.03192812  0.15084484]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 53 ] state=[-0.12009795 -0.41360524  0.03192812  0.15084484], action=0, reward=1.0, next_state=[-0.12837005 -0.60916949  0.03494502  0.45342697]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 348 ][ timestamp 54 ] state=[-0.12837005 -0.60916949  0.03494502  0.45342697], action=1, reward=1.0, next_state=[-0.14055344 -0.41455867  0.04401356  0.17196071]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 55 ] state=[-0.14055344 -0.41455867  0.04401356  0.17196071], action=0, reward=1.0, next_state=[-0.14884462 -0.61028203  0.04745277  0.47819724]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 56 ] state=[-0.14884462 -0.61028203  0.04745277  0.47819724], action=1, reward=1.0, next_state=[-0.16105026 -0.41586102  0.05701672  0.20083989]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 57 ] state=[-0.16105026 -0.41586102  0.05701672  0.20083989], action=1, reward=1.0, next_state=[-0.16936748 -0.22159892  0.06103352 -0.07332591]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 58 ] state=[-0.16936748 -0.22159892  0.06103352 -0.07332591], action=0, reward=1.0, next_state=[-0.17379945 -0.41754035  0.059567    0.23797169]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 59 ] state=[-0.17379945 -0.41754035  0.059567    0.23797169], action=1, reward=1.0, next_state=[-0.18215026 -0.22331773  0.06432643 -0.03534318]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 60 ] state=[-0.18215026 -0.22331773  0.06432643 -0.03534318], action=1, reward=1.0, next_state=[-0.18661662 -0.0291744   0.06361957 -0.30705696]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 61 ] state=[-0.18661662 -0.0291744   0.06361957 -0.30705696], action=1, reward=1.0, next_state=[-0.1872001   0.16498604  0.05747843 -0.57901688]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 62 ] state=[-0.1872001   0.16498604  0.05747843 -0.57901688], action=0, reward=1.0, next_state=[-0.18390038 -0.03089233  0.04589809 -0.26879519]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 63 ] state=[-0.18390038 -0.03089233  0.04589809 -0.26879519], action=1, reward=1.0, next_state=[-0.18451823  0.16354558  0.04052219 -0.5466556 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 64 ] state=[-0.18451823  0.16354558  0.04052219 -0.5466556 ], action=1, reward=1.0, next_state=[-0.18124732  0.35807547  0.02958908 -0.82630064]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 65 ] state=[-0.18124732  0.35807547  0.02958908 -0.82630064], action=1, reward=1.0, next_state=[-0.17408581  0.55278056  0.01306306 -1.10953251]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 66 ] state=[-0.17408581  0.55278056  0.01306306 -1.10953251], action=0, reward=1.0, next_state=[-0.1630302   0.35748941 -0.00912759 -0.81278035]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 67 ] state=[-0.1630302   0.35748941 -0.00912759 -0.81278035], action=0, reward=1.0, next_state=[-0.15588041  0.16249366 -0.02538319 -0.52298238]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 68 ] state=[-0.15588041  0.16249366 -0.02538319 -0.52298238], action=1, reward=1.0, next_state=[-0.15263054  0.3579635  -0.03584284 -0.82355455]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 69 ] state=[-0.15263054  0.3579635  -0.03584284 -0.82355455], action=0, reward=1.0, next_state=[-0.14547127  0.16334972 -0.05231393 -0.54235692]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 70 ] state=[-0.14547127  0.16334972 -0.05231393 -0.54235692], action=0, reward=1.0, next_state=[-0.14220427 -0.03099946 -0.06316107 -0.26660525]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 71 ] state=[-0.14220427 -0.03099946 -0.06316107 -0.26660525], action=0, reward=1.0, next_state=[-0.14282426 -0.22516577 -0.06849318  0.00550646]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 72 ] state=[-0.14282426 -0.22516577 -0.06849318  0.00550646], action=0, reward=1.0, next_state=[-0.14732758 -0.41924198 -0.06838305  0.27581694]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 73 ] state=[-0.14732758 -0.41924198 -0.06838305  0.27581694], action=0, reward=1.0, next_state=[-0.15571242 -0.61332504 -0.06286671  0.54617216]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 74 ] state=[-0.15571242 -0.61332504 -0.06286671  0.54617216], action=1, reward=1.0, next_state=[-0.16797892 -0.41737874 -0.05194327  0.23436269]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 75 ] state=[-0.16797892 -0.41737874 -0.05194327  0.23436269], action=1, reward=1.0, next_state=[-0.17632649 -0.22155459 -0.04725601 -0.07424181]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 76 ] state=[-0.17632649 -0.22155459 -0.04725601 -0.07424181], action=1, reward=1.0, next_state=[-0.18075758 -0.02578815 -0.04874085 -0.38145176]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 77 ] state=[-0.18075758 -0.02578815 -0.04874085 -0.38145176], action=1, reward=1.0, next_state=[-0.18127335  0.16999079 -0.05636988 -0.68909554]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 78 ] state=[-0.18127335  0.16999079 -0.05636988 -0.68909554], action=1, reward=1.0, next_state=[-0.17787353  0.36584787 -0.07015179 -0.99897849]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 79 ] state=[-0.17787353  0.36584787 -0.07015179 -0.99897849], action=1, reward=1.0, next_state=[-0.17055657  0.5618339  -0.09013136 -1.31284217]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 80 ] state=[-0.17055657  0.5618339  -0.09013136 -1.31284217], action=1, reward=1.0, next_state=[-0.1593199   0.75797385 -0.11638821 -1.63232063]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 81 ] state=[-0.1593199   0.75797385 -0.11638821 -1.63232063], action=1, reward=1.0, next_state=[-0.14416042  0.9542544  -0.14903462 -1.9588905 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 82 ] state=[-0.14416042  0.9542544  -0.14903462 -1.9588905 ], action=1, reward=1.0, next_state=[-0.12507533  1.15060958 -0.18821243 -2.2938125 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ][ timestamp 83 ] state=[-0.12507533  1.15060958 -0.18821243 -2.2938125 ], action=0, reward=-1.0, next_state=[-0.10206314  0.95765739 -0.23408868 -2.06450377]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 348: Exploration_rate=0.01. Score=83.\n",
      "[ episode 349 ] state=[-0.01600016  0.0045856  -0.01332611  0.01794386]\n",
      "[ episode 349 ][ timestamp 1 ] state=[-0.01600016  0.0045856  -0.01332611  0.01794386], action=1, reward=1.0, next_state=[-0.01590845  0.1998961  -0.01296723 -0.27891364]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 2 ] state=[-0.01590845  0.1998961  -0.01296723 -0.27891364], action=0, reward=1.0, next_state=[-0.01191052  0.00496151 -0.0185455   0.0096514 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 3 ] state=[-0.01191052  0.00496151 -0.0185455   0.0096514 ], action=1, reward=1.0, next_state=[-0.01181129  0.20034445 -0.01835248 -0.28882468]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 4 ] state=[-0.01181129  0.20034445 -0.01835248 -0.28882468], action=0, reward=1.0, next_state=[-0.0078044   0.00548895 -0.02412897 -0.00198597]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 5 ] state=[-0.0078044   0.00548895 -0.02412897 -0.00198597], action=0, reward=1.0, next_state=[-0.00769462 -0.18927879 -0.02416869  0.28298737]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 6 ] state=[-0.00769462 -0.18927879 -0.02416869  0.28298737], action=0, reward=1.0, next_state=[-0.0114802  -0.38404783 -0.01850894  0.5679507 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 7 ] state=[-0.0114802  -0.38404783 -0.01850894  0.5679507 ], action=1, reward=1.0, next_state=[-0.01916116 -0.18867121 -0.00714993  0.26949466]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 8 ] state=[-0.01916116 -0.18867121 -0.00714993  0.26949466], action=1, reward=1.0, next_state=[-0.02293458  0.00655204 -0.00176003 -0.02543479]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 9 ] state=[-0.02293458  0.00655204 -0.00176003 -0.02543479], action=0, reward=1.0, next_state=[-0.02280354 -0.18854463 -0.00226873  0.2666923 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 10 ] state=[-0.02280354 -0.18854463 -0.00226873  0.2666923 ], action=1, reward=1.0, next_state=[-0.02657443  0.00660963  0.00306512 -0.02670533]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 11 ] state=[-0.02657443  0.00660963  0.00306512 -0.02670533], action=0, reward=1.0, next_state=[-0.02644224 -0.18855615  0.00253101  0.26694309]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 12 ] state=[-0.02644224 -0.18855615  0.00253101  0.26694309], action=1, reward=1.0, next_state=[-0.03021336  0.00652959  0.00786987 -0.02494046]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 13 ] state=[-0.03021336  0.00652959  0.00786987 -0.02494046], action=1, reward=1.0, next_state=[-0.03008277  0.2015378   0.00737106 -0.31513   ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 14 ] state=[-0.03008277  0.2015378   0.00737106 -0.31513   ], action=0, reward=1.0, next_state=[-0.02605202  0.00631163  0.00106846 -0.02013163]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 15 ] state=[-0.02605202  0.00631163  0.00106846 -0.02013163], action=0, reward=1.0, next_state=[-0.02592578 -0.18882562  0.00066583  0.27288822]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 16 ] state=[-0.02592578 -0.18882562  0.00066583  0.27288822], action=1, reward=1.0, next_state=[-0.0297023   0.00628682  0.00612359 -0.01958463]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 349 ][ timestamp 17 ] state=[-0.0297023   0.00628682  0.00612359 -0.01958463], action=0, reward=1.0, next_state=[-0.02957656 -0.18892241  0.0057319   0.27502405]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 18 ] state=[-0.02957656 -0.18892241  0.0057319   0.27502405], action=1, reward=1.0, next_state=[-0.03335501  0.00611729  0.01123238 -0.01584553]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 19 ] state=[-0.03335501  0.00611729  0.01123238 -0.01584553], action=0, reward=1.0, next_state=[-0.03323266 -0.18916393  0.01091547  0.28036008]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 20 ] state=[-0.03323266 -0.18916393  0.01091547  0.28036008], action=0, reward=1.0, next_state=[-0.03701594 -0.38443987  0.01652267  0.57646563]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 21 ] state=[-0.03701594 -0.38443987  0.01652267  0.57646563], action=1, reward=1.0, next_state=[-0.04470474 -0.18955337  0.02805199  0.28903323]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 22 ] state=[-0.04470474 -0.18955337  0.02805199  0.28903323], action=0, reward=1.0, next_state=[-0.04849581 -0.38506388  0.03383265  0.59042981]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 23 ] state=[-0.04849581 -0.38506388  0.03383265  0.59042981], action=1, reward=1.0, next_state=[-0.05619708 -0.19043156  0.04564125  0.3085933 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 24 ] state=[-0.05619708 -0.19043156  0.04564125  0.3085933 ], action=1, reward=1.0, next_state=[-0.06000571  0.00401136  0.05181311  0.03064652]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 25 ] state=[-0.06000571  0.00401136  0.05181311  0.03064652], action=1, reward=1.0, next_state=[-0.05992549  0.19835347  0.05242604 -0.24524919]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 26 ] state=[-0.05992549  0.19835347  0.05242604 -0.24524919], action=1, reward=1.0, next_state=[-0.05595842  0.39268896  0.04752106 -0.52094573]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 27 ] state=[-0.05595842  0.39268896  0.04752106 -0.52094573], action=0, reward=1.0, next_state=[-0.04810464  0.19693141  0.03710214 -0.21367496]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 28 ] state=[-0.04810464  0.19693141  0.03710214 -0.21367496], action=0, reward=1.0, next_state=[-0.04416601  0.0012992   0.03282865  0.09047694]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 29 ] state=[-0.04416601  0.0012992   0.03282865  0.09047694], action=1, reward=1.0, next_state=[-0.04414003  0.1959356   0.03463818 -0.19167046]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 30 ] state=[-0.04414003  0.1959356   0.03463818 -0.19167046], action=0, reward=1.0, next_state=[-0.04022131  0.00033569  0.03080477  0.111735  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 31 ] state=[-0.04022131  0.00033569  0.03080477  0.111735  ], action=1, reward=1.0, next_state=[-0.0402146   0.195003    0.03303947 -0.17107226]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 32 ] state=[-0.0402146   0.195003    0.03303947 -0.17107226], action=0, reward=1.0, next_state=[-0.03631454 -0.00057589  0.02961803  0.1318478 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 33 ] state=[-0.03631454 -0.00057589  0.02961803  0.1318478 ], action=1, reward=1.0, next_state=[-0.03632606  0.19410956  0.03225499 -0.15134586]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 34 ] state=[-0.03632606  0.19410956  0.03225499 -0.15134586], action=0, reward=1.0, next_state=[-0.03244387 -0.00145905  0.02922807  0.15133579]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 35 ] state=[-0.03244387 -0.00145905  0.02922807  0.15133579], action=1, reward=1.0, next_state=[-0.03247305  0.19323245  0.03225478 -0.13198491]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 36 ] state=[-0.03247305  0.19323245  0.03225478 -0.13198491], action=1, reward=1.0, next_state=[-0.0286084   0.38787788  0.02961509 -0.41431992]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 37 ] state=[-0.0286084   0.38787788  0.02961509 -0.41431992], action=0, reward=1.0, next_state=[-0.02085084  0.19234895  0.02132869 -0.11244958]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 38 ] state=[-0.02085084  0.19234895  0.02132869 -0.11244958], action=0, reward=1.0, next_state=[-0.01700386 -0.00307202  0.0190797   0.18688537]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 39 ] state=[-0.01700386 -0.00307202  0.0190797   0.18688537], action=1, reward=1.0, next_state=[-0.0170653   0.19177182  0.0228174  -0.09971812]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 40 ] state=[-0.0170653   0.19177182  0.0228174  -0.09971812], action=0, reward=1.0, next_state=[-0.01322987 -0.00366958  0.02082304  0.20007542]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 41 ] state=[-0.01322987 -0.00366958  0.02082304  0.20007542], action=1, reward=1.0, next_state=[-0.01330326  0.19114846  0.02482455 -0.08596676]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 42 ] state=[-0.01330326  0.19114846  0.02482455 -0.08596676], action=0, reward=1.0, next_state=[-0.00948029 -0.00432038  0.02310521  0.21444382]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 43 ] state=[-0.00948029 -0.00432038  0.02310521  0.21444382], action=1, reward=1.0, next_state=[-0.0095667   0.19046376  0.02739409 -0.07086207]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 44 ] state=[-0.0095667   0.19046376  0.02739409 -0.07086207], action=0, reward=1.0, next_state=[-0.00575742 -0.00504     0.02597685  0.2303364 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 45 ] state=[-0.00575742 -0.00504     0.02597685  0.2303364 ], action=1, reward=1.0, next_state=[-0.00585822  0.1897013   0.03058358 -0.05404067]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 46 ] state=[-0.00585822  0.1897013   0.03058358 -0.05404067], action=1, reward=1.0, next_state=[-0.0020642   0.38437168  0.02950276 -0.33691951]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 47 ] state=[-0.0020642   0.38437168  0.02950276 -0.33691951], action=0, reward=1.0, next_state=[ 0.00562324  0.18884258  0.02276437 -0.03508094]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 48 ] state=[ 0.00562324  0.18884258  0.02276437 -0.03508094], action=0, reward=1.0, next_state=[ 0.00940009 -0.0065983   0.02206275  0.26469656]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 49 ] state=[ 0.00940009 -0.0065983   0.02206275  0.26469656], action=0, reward=1.0, next_state=[ 0.00926812 -0.20202809  0.02735669  0.56425583]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 50 ] state=[ 0.00926812 -0.20202809  0.02735669  0.56425583], action=1, reward=1.0, next_state=[ 0.00522756 -0.00730044  0.0386418   0.28031552]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 51 ] state=[ 0.00522756 -0.00730044  0.0386418   0.28031552], action=1, reward=1.0, next_state=[5.08155332e-03 1.87249596e-01 4.42481129e-02 6.61703180e-05]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 52 ] state=[5.08155332e-03 1.87249596e-01 4.42481129e-02 6.61703180e-05], action=1, reward=1.0, next_state=[ 0.00882655  0.38170995  0.04424944 -0.27833416]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 53 ] state=[ 0.00882655  0.38170995  0.04424944 -0.27833416], action=1, reward=1.0, next_state=[ 0.01646074  0.57617363  0.03868275 -0.55673906]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 54 ] state=[ 0.01646074  0.57617363  0.03868275 -0.55673906], action=0, reward=1.0, next_state=[ 0.02798422  0.38053057  0.02754797 -0.25212412]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 55 ] state=[ 0.02798422  0.38053057  0.02754797 -0.25212412], action=0, reward=1.0, next_state=[0.03559483 0.1850263  0.02250549 0.0491191 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 56 ] state=[0.03559483 0.1850263  0.02250549 0.0491191 ], action=1, reward=1.0, next_state=[ 0.03929535  0.37981844  0.02348787 -0.23637905]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 57 ] state=[ 0.03929535  0.37981844  0.02348787 -0.23637905], action=1, reward=1.0, next_state=[ 0.04689172  0.57459708  0.01876029 -0.52156163]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 58 ] state=[ 0.04689172  0.57459708  0.01876029 -0.52156163], action=0, reward=1.0, next_state=[ 0.05838366  0.37921615  0.00832906 -0.22302661]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 59 ] state=[ 0.05838366  0.37921615  0.00832906 -0.22302661], action=0, reward=1.0, next_state=[0.06596799 0.18397615 0.00386853 0.07227195]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 60 ] state=[0.06596799 0.18397615 0.00386853 0.07227195], action=1, reward=1.0, next_state=[ 0.06964751  0.37904243  0.00531396 -0.21918793]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 61 ] state=[ 0.06964751  0.37904243  0.00531396 -0.21918793], action=1, reward=1.0, next_state=[ 0.07722836  0.57408802  0.00093021 -0.51018989]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 349 ][ timestamp 62 ] state=[ 0.07722836  0.57408802  0.00093021 -0.51018989], action=0, reward=1.0, next_state=[ 0.08871012  0.37895297 -0.00927359 -0.21721397]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 63 ] state=[ 0.08871012  0.37895297 -0.00927359 -0.21721397], action=0, reward=1.0, next_state=[ 0.09628918  0.18396481 -0.01361787  0.0725293 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 64 ] state=[ 0.09628918  0.18396481 -0.01361787  0.0725293 ], action=0, reward=1.0, next_state=[ 0.09996848 -0.01095929 -0.01216729  0.36088481]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 65 ] state=[ 0.09996848 -0.01095929 -0.01216729  0.36088481], action=0, reward=1.0, next_state=[ 0.09974929 -0.2059062  -0.00494959  0.64970643]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 66 ] state=[ 0.09974929 -0.2059062  -0.00494959  0.64970643], action=1, reward=1.0, next_state=[ 0.09563117 -0.01071565  0.00804454  0.35546902]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 67 ] state=[ 0.09563117 -0.01071565  0.00804454  0.35546902], action=1, reward=1.0, next_state=[0.09541685 0.184291   0.01515392 0.06533358]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 68 ] state=[0.09541685 0.184291   0.01515392 0.06533358], action=1, reward=1.0, next_state=[ 0.09910267  0.37919245  0.01646059 -0.22252994]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 69 ] state=[ 0.09910267  0.37919245  0.01646059 -0.22252994], action=0, reward=1.0, next_state=[0.10668652 0.18383914 0.01200999 0.07529952]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 70 ] state=[0.10668652 0.18383914 0.01200999 0.07529952], action=1, reward=1.0, next_state=[ 0.1103633   0.37878687  0.01351598 -0.21357014]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 71 ] state=[ 0.1103633   0.37878687  0.01351598 -0.21357014], action=0, reward=1.0, next_state=[0.11793904 0.18347432 0.00924458 0.08334551]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 72 ] state=[0.11793904 0.18347432 0.00924458 0.08334551], action=0, reward=1.0, next_state=[ 0.12160853 -0.01177892  0.01091149  0.37893073]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 73 ] state=[ 0.12160853 -0.01177892  0.01091149  0.37893073], action=1, reward=1.0, next_state=[0.12137295 0.18318639 0.01849011 0.08970809]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 74 ] state=[0.12137295 0.18318639 0.01849011 0.08970809], action=0, reward=1.0, next_state=[ 0.12503668 -0.01219565  0.02028427  0.38816682]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 75 ] state=[ 0.12503668 -0.01219565  0.02028427  0.38816682], action=0, reward=1.0, next_state=[ 0.12479276 -0.20759956  0.0280476   0.68717556]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 76 ] state=[ 0.12479276 -0.20759956  0.0280476   0.68717556], action=1, reward=1.0, next_state=[ 0.12064077 -0.01287793  0.04179111  0.40345291]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 77 ] state=[ 0.12064077 -0.01287793  0.04179111  0.40345291], action=1, reward=1.0, next_state=[0.12038321 0.18162717 0.04986017 0.12423301]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 78 ] state=[0.12038321 0.18162717 0.04986017 0.12423301], action=0, reward=1.0, next_state=[ 0.12401576 -0.01417233  0.05234483  0.43222008]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 79 ] state=[ 0.12401576 -0.01417233  0.05234483  0.43222008], action=1, reward=1.0, next_state=[0.12373231 0.1801709  0.06098923 0.15648687]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 80 ] state=[0.12373231 0.1801709  0.06098923 0.15648687], action=1, reward=1.0, next_state=[ 0.12733573  0.37436901  0.06411897 -0.11634897]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 81 ] state=[ 0.12733573  0.37436901  0.06411897 -0.11634897], action=1, reward=1.0, next_state=[ 0.13482311  0.56851644  0.06179199 -0.38813362]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 82 ] state=[ 0.13482311  0.56851644  0.06179199 -0.38813362], action=0, reward=1.0, next_state=[ 0.14619344  0.37257431  0.05402932 -0.07662607]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 83 ] state=[ 0.14619344  0.37257431  0.05402932 -0.07662607], action=0, reward=1.0, next_state=[0.15364492 0.1767211  0.0524968  0.23260193]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 84 ] state=[0.15364492 0.1767211  0.0524968  0.23260193], action=0, reward=1.0, next_state=[ 0.15717935 -0.01911012  0.05714884  0.54137107]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 85 ] state=[ 0.15717935 -0.01911012  0.05714884  0.54137107], action=1, reward=1.0, next_state=[0.15679714 0.17516397 0.06797626 0.26722828]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 86 ] state=[0.15679714 0.17516397 0.06797626 0.26722828], action=0, reward=1.0, next_state=[ 0.16030042 -0.02085895  0.07332082  0.58055323]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 87 ] state=[ 0.16030042 -0.02085895  0.07332082  0.58055323], action=1, reward=1.0, next_state=[0.15988324 0.17316314 0.08493189 0.31183904]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 88 ] state=[0.15988324 0.17316314 0.08493189 0.31183904], action=1, reward=1.0, next_state=[0.16334651 0.36697889 0.09116867 0.04710332]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 89 ] state=[0.16334651 0.36697889 0.09116867 0.04710332], action=1, reward=1.0, next_state=[ 0.17068608  0.5606833   0.09211074 -0.21548015]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 90 ] state=[ 0.17068608  0.5606833   0.09211074 -0.21548015], action=1, reward=1.0, next_state=[ 0.18189975  0.75437596  0.08780113 -0.47774522]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 91 ] state=[ 0.18189975  0.75437596  0.08780113 -0.47774522], action=0, reward=1.0, next_state=[ 0.19698727  0.55813122  0.07824623 -0.15873163]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 92 ] state=[ 0.19698727  0.55813122  0.07824623 -0.15873163], action=1, reward=1.0, next_state=[ 0.20814989  0.7520508   0.0750716  -0.42574009]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 93 ] state=[ 0.20814989  0.7520508   0.0750716  -0.42574009], action=1, reward=1.0, next_state=[ 0.22319091  0.94603358  0.06655679 -0.69384439]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 94 ] state=[ 0.22319091  0.94603358  0.06655679 -0.69384439], action=1, reward=1.0, next_state=[ 0.24211158  1.14017223  0.05267991 -0.96485435]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 95 ] state=[ 0.24211158  1.14017223  0.05267991 -0.96485435], action=0, reward=1.0, next_state=[ 0.26491503  0.94438371  0.03338282 -0.65609826]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 96 ] state=[ 0.26491503  0.94438371  0.03338282 -0.65609826], action=1, reward=1.0, next_state=[ 0.2838027   1.13902541  0.02026085 -0.93808541]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 97 ] state=[ 0.2838027   1.13902541  0.02026085 -0.93808541], action=0, reward=1.0, next_state=[ 0.30658321  0.94363623  0.00149915 -0.63910551]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 98 ] state=[ 0.30658321  0.94363623  0.00149915 -0.63910551], action=0, reward=1.0, next_state=[ 0.32545593  0.74849341 -0.01128296 -0.34595086]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 99 ] state=[ 0.32545593  0.74849341 -0.01128296 -0.34595086], action=0, reward=1.0, next_state=[ 0.3404258   0.55353375 -0.01820198 -0.05684712]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 100 ] state=[ 0.3404258   0.55353375 -0.01820198 -0.05684712], action=0, reward=1.0, next_state=[ 0.35149648  0.35867745 -0.01933892  0.23003783]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 101 ] state=[ 0.35149648  0.35867745 -0.01933892  0.23003783], action=1, reward=1.0, next_state=[ 0.35867003  0.55407034 -0.01473817 -0.06868198]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 102 ] state=[ 0.35867003  0.55407034 -0.01473817 -0.06868198], action=0, reward=1.0, next_state=[ 0.36975143  0.35916275 -0.01611181  0.21931478]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 103 ] state=[ 0.36975143  0.35916275 -0.01611181  0.21931478], action=1, reward=1.0, next_state=[ 0.37693469  0.55451127 -0.01172551 -0.07840662]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 349 ][ timestamp 104 ] state=[ 0.37693469  0.55451127 -0.01172551 -0.07840662], action=1, reward=1.0, next_state=[ 0.38802491  0.74979933 -0.01329364 -0.3747658 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 105 ] state=[ 0.38802491  0.74979933 -0.01329364 -0.3747658 ], action=0, reward=1.0, next_state=[ 0.4030209   0.5548687  -0.02078896 -0.08630391]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 106 ] state=[ 0.4030209   0.5548687  -0.02078896 -0.08630391], action=0, reward=1.0, next_state=[ 0.41411827  0.36005082 -0.02251504  0.19974825]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 107 ] state=[ 0.41411827  0.36005082 -0.02251504  0.19974825], action=0, reward=1.0, next_state=[ 0.42131929  0.16525801 -0.01852007  0.48524456]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 108 ] state=[ 0.42131929  0.16525801 -0.01852007  0.48524456], action=1, reward=1.0, next_state=[ 0.42462445  0.36063634 -0.00881518  0.18678272]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 109 ] state=[ 0.42462445  0.36063634 -0.00881518  0.18678272], action=0, reward=1.0, next_state=[ 0.43183718  0.16564162 -0.00507953  0.47667181]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 110 ] state=[ 0.43183718  0.16564162 -0.00507953  0.47667181], action=0, reward=1.0, next_state=[ 0.43515001 -0.02940824  0.00445391  0.76774946]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 111 ] state=[ 0.43515001 -0.02940824  0.00445391  0.76774946], action=1, reward=1.0, next_state=[0.43456184 0.16565211 0.0198089  0.47647127]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 112 ] state=[0.43456184 0.16565211 0.0198089  0.47647127], action=1, reward=1.0, next_state=[0.43787489 0.36048884 0.02933832 0.19009695]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 113 ] state=[0.43787489 0.36048884 0.02933832 0.19009695], action=1, reward=1.0, next_state=[ 0.44508466  0.55517905  0.03314026 -0.09318847]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 114 ] state=[ 0.44508466  0.55517905  0.03314026 -0.09318847], action=0, reward=1.0, next_state=[0.45618824 0.35959816 0.03127649 0.20976324]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 115 ] state=[0.45618824 0.35959816 0.03127649 0.20976324], action=0, reward=1.0, next_state=[0.46338021 0.16404329 0.03547176 0.51214588]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 116 ] state=[0.46338021 0.16404329 0.03547176 0.51214588], action=1, reward=1.0, next_state=[0.46666107 0.35864812 0.04571468 0.23084877]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 117 ] state=[0.46666107 0.35864812 0.04571468 0.23084877], action=1, reward=1.0, next_state=[ 0.47383404  0.55308803  0.05033165 -0.04707096]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 118 ] state=[ 0.47383404  0.55308803  0.05033165 -0.04707096], action=0, reward=1.0, next_state=[0.4848958  0.35728184 0.04939023 0.26105764]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 119 ] state=[0.4848958  0.35728184 0.04939023 0.26105764], action=1, reward=1.0, next_state=[ 0.49204143  0.55166523  0.05461138 -0.01564706]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 120 ] state=[ 0.49204143  0.55166523  0.05461138 -0.01564706], action=1, reward=1.0, next_state=[ 0.50307474  0.74596318  0.05429844 -0.29061172]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 121 ] state=[ 0.50307474  0.74596318  0.05429844 -0.29061172], action=1, reward=1.0, next_state=[ 0.517994    0.94027054  0.04848621 -0.5656873 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 122 ] state=[ 0.517994    0.94027054  0.04848621 -0.5656873 ], action=1, reward=1.0, next_state=[ 0.53679941  1.13467993  0.03717246 -0.84270932]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 123 ] state=[ 0.53679941  1.13467993  0.03717246 -0.84270932], action=1, reward=1.0, next_state=[ 0.55949301  1.32927535  0.02031828 -1.12347461]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 124 ] state=[ 0.55949301  1.32927535  0.02031828 -1.12347461], action=0, reward=1.0, next_state=[ 0.58607852  1.13389301 -0.00215122 -0.82448843]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 125 ] state=[ 0.58607852  1.13389301 -0.00215122 -0.82448843], action=0, reward=1.0, next_state=[ 0.60875638  0.93880055 -0.01864098 -0.53248287]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 126 ] state=[ 0.60875638  0.93880055 -0.01864098 -0.53248287], action=0, reward=1.0, next_state=[ 0.62753239  0.74394567 -0.02929064 -0.24573147]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 127 ] state=[ 0.62753239  0.74394567 -0.02929064 -0.24573147], action=0, reward=1.0, next_state=[ 0.6424113   0.54925404 -0.03420527  0.03757049]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 128 ] state=[ 0.6424113   0.54925404 -0.03420527  0.03757049], action=1, reward=1.0, next_state=[ 0.65339638  0.74484938 -0.03345386 -0.26570528]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 129 ] state=[ 0.65339638  0.74484938 -0.03345386 -0.26570528], action=1, reward=1.0, next_state=[ 0.66829337  0.94043243 -0.03876797 -0.56874932]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 130 ] state=[ 0.66829337  0.94043243 -0.03876797 -0.56874932], action=0, reward=1.0, next_state=[ 0.68710202  0.74587507 -0.05014295 -0.28852748]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 131 ] state=[ 0.68710202  0.74587507 -0.05014295 -0.28852748], action=0, reward=1.0, next_state=[ 0.70201952  0.5515027  -0.0559135  -0.01207123]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 132 ] state=[ 0.70201952  0.5515027  -0.0559135  -0.01207123], action=0, reward=1.0, next_state=[ 0.71304957  0.35722534 -0.05615493  0.26245938]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 133 ] state=[ 0.71304957  0.35722534 -0.05615493  0.26245938], action=1, reward=1.0, next_state=[ 0.72019408  0.55310203 -0.05090574 -0.04739338]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 134 ] state=[ 0.72019408  0.55310203 -0.05090574 -0.04739338], action=1, reward=1.0, next_state=[ 0.73125612  0.74891557 -0.05185361 -0.35569303]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 135 ] state=[ 0.73125612  0.74891557 -0.05185361 -0.35569303], action=0, reward=1.0, next_state=[ 0.74623443  0.55456775 -0.05896747 -0.07980126]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 136 ] state=[ 0.74623443  0.55456775 -0.05896747 -0.07980126], action=1, reward=1.0, next_state=[ 0.75732579  0.75048327 -0.06056349 -0.39049015]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 137 ] state=[ 0.75732579  0.75048327 -0.06056349 -0.39049015], action=0, reward=1.0, next_state=[ 0.77233545  0.55627082 -0.0683733  -0.11750037]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 138 ] state=[ 0.77233545  0.55627082 -0.0683733  -0.11750037], action=0, reward=1.0, next_state=[ 0.78346087  0.36219175 -0.0707233   0.15285193]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 139 ] state=[ 0.78346087  0.36219175 -0.0707233   0.15285193], action=0, reward=1.0, next_state=[ 0.79070471  0.16814998 -0.06766626  0.42241164]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 140 ] state=[ 0.79070471  0.16814998 -0.06766626  0.42241164], action=1, reward=1.0, next_state=[ 0.7940677   0.36416207 -0.05921803  0.10918767]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 141 ] state=[ 0.7940677   0.36416207 -0.05921803  0.10918767], action=0, reward=1.0, next_state=[ 0.80135095  0.1699365  -0.05703428  0.38261542]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 142 ] state=[ 0.80135095  0.1699365  -0.05703428  0.38261542], action=0, reward=1.0, next_state=[ 0.80474968 -0.02433122 -0.04938197  0.65678419]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 143 ] state=[ 0.80474968 -0.02433122 -0.04938197  0.65678419], action=1, reward=1.0, next_state=[ 0.80426305  0.17144211 -0.03624629  0.34896977]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 144 ] state=[ 0.80426305  0.17144211 -0.03624629  0.34896977], action=1, reward=1.0, next_state=[ 0.80769189  0.36706033 -0.02926689  0.0450811 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 349 ][ timestamp 145 ] state=[ 0.80769189  0.36706033 -0.02926689  0.0450811 ], action=1, reward=1.0, next_state=[ 0.8150331   0.56258946 -0.02836527 -0.25669023]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 146 ] state=[ 0.8150331   0.56258946 -0.02836527 -0.25669023], action=1, reward=1.0, next_state=[ 0.82628489  0.75810466 -0.03349907 -0.55818333]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 147 ] state=[ 0.82628489  0.75810466 -0.03349907 -0.55818333], action=0, reward=1.0, next_state=[ 0.84144698  0.56346857 -0.04466274 -0.27623988]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 148 ] state=[ 0.84144698  0.56346857 -0.04466274 -0.27623988], action=0, reward=1.0, next_state=[ 0.85271635  0.36901134 -0.05018754  0.00202861]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 149 ] state=[ 0.85271635  0.36901134 -0.05018754  0.00202861], action=1, reward=1.0, next_state=[ 0.86009658  0.56481579 -0.05014697 -0.30605719]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 150 ] state=[ 0.86009658  0.56481579 -0.05014697 -0.30605719], action=1, reward=1.0, next_state=[ 0.8713929   0.76061514 -0.05626811 -0.61412403]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 151 ] state=[ 0.8713929   0.76061514 -0.05626811 -0.61412403], action=0, reward=1.0, next_state=[ 0.8866052   0.56632272 -0.06855059 -0.33968074]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 152 ] state=[ 0.8866052   0.56632272 -0.06855059 -0.33968074], action=0, reward=1.0, next_state=[ 0.89793165  0.37223973 -0.0753442  -0.0693781 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 153 ] state=[ 0.89793165  0.37223973 -0.0753442  -0.0693781 ], action=0, reward=1.0, next_state=[ 0.90537645  0.1782743  -0.07673177  0.19861437]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 154 ] state=[ 0.90537645  0.1782743  -0.07673177  0.19861437], action=0, reward=1.0, next_state=[ 0.90894193 -0.01567108 -0.07275948  0.46613942]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 155 ] state=[ 0.90894193 -0.01567108 -0.07275948  0.46613942], action=1, reward=1.0, next_state=[ 0.90862851  0.18039944 -0.06343669  0.15143936]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 156 ] state=[ 0.90862851  0.18039944 -0.06343669  0.15143936], action=1, reward=1.0, next_state=[ 0.9122365   0.37636968 -0.0604079  -0.16056261]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 157 ] state=[ 0.9122365   0.37636968 -0.0604079  -0.16056261], action=1, reward=1.0, next_state=[ 0.9197639   0.5723021  -0.06361916 -0.47167428]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 158 ] state=[ 0.9197639   0.5723021  -0.06361916 -0.47167428], action=0, reward=1.0, next_state=[ 0.93120994  0.3781337  -0.07305264 -0.19970232]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 159 ] state=[ 0.93120994  0.3781337  -0.07305264 -0.19970232], action=0, reward=1.0, next_state=[ 0.93877261  0.18412849 -0.07704669  0.06907095]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 160 ] state=[ 0.93877261  0.18412849 -0.07704669  0.06907095], action=1, reward=1.0, next_state=[ 0.94245518  0.38026566 -0.07566527 -0.24689131]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 161 ] state=[ 0.94245518  0.38026566 -0.07566527 -0.24689131], action=0, reward=1.0, next_state=[ 0.95006049  0.18630132 -0.0806031   0.02099835]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 162 ] state=[ 0.95006049  0.18630132 -0.0806031   0.02099835], action=0, reward=1.0, next_state=[ 0.95378652 -0.00757773 -0.08018313  0.28720108]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 163 ] state=[ 0.95378652 -0.00757773 -0.08018313  0.28720108], action=1, reward=1.0, next_state=[ 0.95363497  0.18859071 -0.07443911 -0.02965475]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 164 ] state=[ 0.95363497  0.18859071 -0.07443911 -0.02965475], action=0, reward=1.0, next_state=[ 0.95740678 -0.00538918 -0.0750322   0.2386444 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 165 ] state=[ 0.95740678 -0.00538918 -0.0750322   0.2386444 ], action=1, reward=1.0, next_state=[ 0.957299    0.19072001 -0.07025931 -0.0767305 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 166 ] state=[ 0.957299    0.19072001 -0.07025931 -0.0767305 ], action=0, reward=1.0, next_state=[ 0.9611134  -0.00332806 -0.07179392  0.19298423]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 167 ] state=[ 0.9611134  -0.00332806 -0.07179392  0.19298423], action=1, reward=1.0, next_state=[ 0.96104684  0.19274356 -0.06793424 -0.12145484]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 168 ] state=[ 0.96104684  0.19274356 -0.06793424 -0.12145484], action=0, reward=1.0, next_state=[ 0.96490171 -0.00134264 -0.07036334  0.14904563]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 169 ] state=[ 0.96490171 -0.00134264 -0.07036334  0.14904563], action=0, reward=1.0, next_state=[ 0.96487485 -0.19539015 -0.06738242  0.41872689]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 170 ] state=[ 0.96487485 -0.19539015 -0.06738242  0.41872689], action=0, reward=1.0, next_state=[ 0.96096705 -0.38949579 -0.05900789  0.68942917]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 171 ] state=[ 0.96096705 -0.38949579 -0.05900789  0.68942917], action=1, reward=1.0, next_state=[ 0.95317714 -0.19360674 -0.0452193   0.37876874]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 172 ] state=[ 0.95317714 -0.19360674 -0.0452193   0.37876874], action=1, reward=1.0, next_state=[ 0.949305    0.00212724 -0.03764393  0.07217794]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 173 ] state=[ 0.949305    0.00212724 -0.03764393  0.07217794], action=1, reward=1.0, next_state=[ 0.94934755  0.19776809 -0.03620037 -0.23214012]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 174 ] state=[ 0.94934755  0.19776809 -0.03620037 -0.23214012], action=1, reward=1.0, next_state=[ 0.95330291  0.3933881  -0.04084317 -0.53601848]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 175 ] state=[ 0.95330291  0.3933881  -0.04084317 -0.53601848], action=0, reward=1.0, next_state=[ 0.96117067  0.19886353 -0.05156354 -0.25647952]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 176 ] state=[ 0.96117067  0.19886353 -0.05156354 -0.25647952], action=0, reward=1.0, next_state=[ 0.96514794  0.00451424 -0.05669313  0.01950399]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 177 ] state=[ 0.96514794  0.00451424 -0.05669313  0.01950399], action=1, reward=1.0, next_state=[ 0.96523822  0.20040147 -0.05630305 -0.29051363]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 178 ] state=[ 0.96523822  0.20040147 -0.05630305 -0.29051363], action=0, reward=1.0, next_state=[ 0.96924625  0.00612566 -0.06211332 -0.01610605]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 179 ] state=[ 0.96924625  0.00612566 -0.06211332 -0.01610605], action=0, reward=1.0, next_state=[ 0.96936877 -0.18805305 -0.06243545  0.25635076]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 180 ] state=[ 0.96936877 -0.18805305 -0.06243545  0.25635076], action=1, reward=1.0, next_state=[ 0.96560771  0.00790217 -0.05730843 -0.05535345]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 181 ] state=[ 0.96560771  0.00790217 -0.05730843 -0.05535345], action=1, reward=1.0, next_state=[ 0.96576575  0.20379701 -0.0584155  -0.36555278]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 182 ] state=[ 0.96576575  0.20379701 -0.0584155  -0.36555278], action=0, reward=1.0, next_state=[ 0.96984169  0.00955176 -0.06572655 -0.09184627]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 183 ] state=[ 0.96984169  0.00955176 -0.06572655 -0.09184627], action=0, reward=1.0, next_state=[ 0.97003273 -0.18456956 -0.06756348  0.17939728]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 184 ] state=[ 0.97003273 -0.18456956 -0.06756348  0.17939728], action=0, reward=1.0, next_state=[ 0.96634133 -0.37866289 -0.06397553  0.45002447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 185 ] state=[ 0.96634133 -0.37866289 -0.06397553  0.45002447], action=0, reward=1.0, next_state=[ 0.95876808 -0.5728244  -0.05497505  0.72187495]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 186 ] state=[ 0.95876808 -0.5728244  -0.05497505  0.72187495], action=0, reward=1.0, next_state=[ 0.94731159 -0.76714452 -0.04053755  0.99676026]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 187 ] state=[ 0.94731159 -0.76714452 -0.04053755  0.99676026], action=1, reward=1.0, next_state=[ 0.9319687  -0.57150463 -0.02060234  0.69162674]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 188 ] state=[ 0.9319687  -0.57150463 -0.02060234  0.69162674], action=0, reward=1.0, next_state=[ 0.92053861 -0.76633477 -0.00676981  0.97775326]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 349 ][ timestamp 189 ] state=[ 0.92053861 -0.76633477 -0.00676981  0.97775326], action=0, reward=1.0, next_state=[ 0.90521191 -0.96136529  0.01278526  1.26830203]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 190 ] state=[ 0.90521191 -0.96136529  0.01278526  1.26830203], action=1, reward=1.0, next_state=[ 0.8859846  -0.76640895  0.0381513   0.97965018]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 191 ] state=[ 0.8859846  -0.76640895  0.0381513   0.97965018], action=1, reward=1.0, next_state=[ 0.87065643 -0.57181861  0.0577443   0.69919084]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 192 ] state=[ 0.87065643 -0.57181861  0.0577443   0.69919084], action=1, reward=1.0, next_state=[ 0.85922005 -0.37754278  0.07172812  0.42523019]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 193 ] state=[ 0.85922005 -0.37754278  0.07172812  0.42523019], action=1, reward=1.0, next_state=[ 0.8516692  -0.18350627  0.08023272  0.15599382]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 194 ] state=[ 0.8516692  -0.18350627  0.08023272  0.15599382], action=1, reward=1.0, next_state=[ 0.84799907  0.01038073  0.0833526  -0.11033798]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 195 ] state=[ 0.84799907  0.01038073  0.0833526  -0.11033798], action=0, reward=1.0, next_state=[ 0.84820669 -0.18583058  0.08114584  0.20743448]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 196 ] state=[ 0.84820669 -0.18583058  0.08114584  0.20743448], action=1, reward=1.0, next_state=[ 0.84449007  0.00804292  0.08529453 -0.05858816]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 197 ] state=[ 0.84449007  0.00804292  0.08529453 -0.05858816], action=0, reward=1.0, next_state=[ 0.84465093 -0.18819186  0.08412277  0.25974013]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 198 ] state=[ 0.84465093 -0.18819186  0.08412277  0.25974013], action=1, reward=1.0, next_state=[ 0.8408871   0.00563473  0.08931757 -0.0052687 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 199 ] state=[ 0.8408871   0.00563473  0.08931757 -0.0052687 ], action=1, reward=1.0, next_state=[ 0.84099979  0.19936975  0.0892122  -0.26848838]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 200 ] state=[ 0.84099979  0.19936975  0.0892122  -0.26848838], action=1, reward=1.0, next_state=[ 0.84498719  0.39311279  0.08384243 -0.53175361]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 201 ] state=[ 0.84498719  0.39311279  0.08384243 -0.53175361], action=0, reward=1.0, next_state=[ 0.85284944  0.19691782  0.07320736 -0.21387413]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 202 ] state=[ 0.85284944  0.19691782  0.07320736 -0.21387413], action=1, reward=1.0, next_state=[ 0.8567878   0.39092094  0.06892987 -0.48259562]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 203 ] state=[ 0.8567878   0.39092094  0.06892987 -0.48259562], action=0, reward=1.0, next_state=[ 0.86460622  0.19489725  0.05927796 -0.169009  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 204 ] state=[ 0.86460622  0.19489725  0.05927796 -0.169009  ], action=0, reward=1.0, next_state=[ 0.86850416 -0.00102088  0.05589778  0.14176953]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 205 ] state=[ 0.86850416 -0.00102088  0.05589778  0.14176953], action=1, reward=1.0, next_state=[ 0.86848374  0.19325781  0.05873317 -0.13276795]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 206 ] state=[ 0.86848374  0.19325781  0.05873317 -0.13276795], action=1, reward=1.0, next_state=[ 0.8723489   0.38749142  0.05607781 -0.40635836]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 207 ] state=[ 0.8723489   0.38749142  0.05607781 -0.40635836], action=1, reward=1.0, next_state=[ 0.88009873  0.58177519  0.04795065 -0.68084767]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 208 ] state=[ 0.88009873  0.58177519  0.04795065 -0.68084767], action=0, reward=1.0, next_state=[ 0.89173423  0.3860212   0.03433369 -0.37346209]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 209 ] state=[ 0.89173423  0.3860212   0.03433369 -0.37346209], action=1, reward=1.0, next_state=[ 0.89945466  0.58063903  0.02686445 -0.65512468]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 210 ] state=[ 0.89945466  0.58063903  0.02686445 -0.65512468], action=1, reward=1.0, next_state=[ 0.91106744  0.77537686  0.01376196 -0.93922882]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 211 ] state=[ 0.91106744  0.77537686  0.01376196 -0.93922882], action=1, reward=1.0, next_state=[ 0.92657497  0.97031061 -0.00502262 -1.22755587]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 212 ] state=[ 0.92657497  0.97031061 -0.00502262 -1.22755587], action=0, reward=1.0, next_state=[ 0.94598119  0.77525366 -0.02957374 -0.93645079]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 213 ] state=[ 0.94598119  0.77525366 -0.02957374 -0.93645079], action=1, reward=1.0, next_state=[ 0.96148626  0.97076169 -0.04830275 -1.23827801]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 214 ] state=[ 0.96148626  0.97076169 -0.04830275 -1.23827801], action=0, reward=1.0, next_state=[ 0.98090149  0.77629234 -0.07306831 -0.9611097 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 215 ] state=[ 0.98090149  0.77629234 -0.07306831 -0.9611097 ], action=0, reward=1.0, next_state=[ 0.99642734  0.58222446 -0.09229051 -0.69224759]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 216 ] state=[ 0.99642734  0.58222446 -0.09229051 -0.69224759], action=1, reward=1.0, next_state=[ 1.00807183  0.77849742 -0.10613546 -1.01249902]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 217 ] state=[ 1.00807183  0.77849742 -0.10613546 -1.01249902], action=1, reward=1.0, next_state=[ 1.02364178  0.97486291 -0.12638544 -1.33653507]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 218 ] state=[ 1.02364178  0.97486291 -0.12638544 -1.33653507], action=0, reward=1.0, next_state=[ 1.04313904  0.78153935 -0.15311614 -1.08592113]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 219 ] state=[ 1.04313904  0.78153935 -0.15311614 -1.08592113], action=1, reward=1.0, next_state=[ 1.05876982  0.97831265 -0.17483456 -1.42246836]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 220 ] state=[ 1.05876982  0.97831265 -0.17483456 -1.42246836], action=1, reward=1.0, next_state=[ 1.07833608  1.17511213 -0.20328393 -1.76430725]\n",
      "[ Experience replay ] starts\n",
      "[ episode 349 ][ timestamp 221 ] state=[ 1.07833608  1.17511213 -0.20328393 -1.76430725], action=1, reward=-1.0, next_state=[ 1.10183832  1.37186992 -0.23857008 -2.11272143]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 349: Exploration_rate=0.01. Score=221.\n",
      "[ episode 350 ] state=[-0.01924429 -0.0270123  -0.04969916 -0.03368877]\n",
      "[ episode 350 ][ timestamp 1 ] state=[-0.01924429 -0.0270123  -0.04969916 -0.03368877], action=0, reward=1.0, next_state=[-0.01978453 -0.22138762 -0.05037294  0.24290867]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 2 ] state=[-0.01978453 -0.22138762 -0.05037294  0.24290867], action=0, reward=1.0, next_state=[-0.02421229 -0.41575521 -0.04551477  0.51928685]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 3 ] state=[-0.02421229 -0.41575521 -0.04551477  0.51928685], action=0, reward=1.0, next_state=[-0.03252739 -0.61020783 -0.03512903  0.79728699]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 4 ] state=[-0.03252739 -0.61020783 -0.03512903  0.79728699], action=0, reward=1.0, next_state=[-0.04473155 -0.80483061 -0.01918329  1.07871524]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 5 ] state=[-0.04473155 -0.80483061 -0.01918329  1.07871524], action=1, reward=1.0, next_state=[-0.06082816 -0.60946065  0.00239101  0.78007467]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 6 ] state=[-0.06082816 -0.60946065  0.00239101  0.78007467], action=1, reward=1.0, next_state=[-0.07301737 -0.41437165  0.01799251  0.48814497]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 7 ] state=[-0.07301737 -0.41437165  0.01799251  0.48814497], action=1, reward=1.0, next_state=[-0.08130481 -0.21950811  0.02775541  0.20118649]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 8 ] state=[-0.08130481 -0.21950811  0.02775541  0.20118649], action=1, reward=1.0, next_state=[-0.08569497 -0.02479389  0.03177914 -0.08261331]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 9 ] state=[-0.08569497 -0.02479389  0.03177914 -0.08261331], action=1, reward=1.0, next_state=[-0.08619085  0.16985845  0.03012687 -0.36510289]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 350 ][ timestamp 10 ] state=[-0.08619085  0.16985845  0.03012687 -0.36510289], action=0, reward=1.0, next_state=[-0.08279368 -0.02567841  0.02282481 -0.06307475]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 11 ] state=[-0.08279368 -0.02567841  0.02282481 -0.06307475], action=1, reward=1.0, next_state=[-0.08330725  0.16910898  0.02156332 -0.34846981]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 12 ] state=[-0.08330725  0.16910898  0.02156332 -0.34846981], action=0, reward=1.0, next_state=[-0.07992507 -0.02631292  0.01459392 -0.04906598]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 13 ] state=[-0.07992507 -0.02631292  0.01459392 -0.04906598], action=0, reward=1.0, next_state=[-0.08045132 -0.22164106  0.0136126   0.24818549]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 14 ] state=[-0.08045132 -0.22164106  0.0136126   0.24818549], action=1, reward=1.0, next_state=[-0.08488415 -0.02671614  0.01857631 -0.04017283]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 15 ] state=[-0.08488415 -0.02671614  0.01857631 -0.04017283], action=1, reward=1.0, next_state=[-0.08541847  0.16813457  0.01777286 -0.32693734]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 16 ] state=[-0.08541847  0.16813457  0.01777286 -0.32693734], action=0, reward=1.0, next_state=[-0.08205578 -0.02723585  0.01123411 -0.02870305]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 17 ] state=[-0.08205578 -0.02723585  0.01123411 -0.02870305], action=1, reward=1.0, next_state=[-0.08260049  0.16772321  0.01066005 -0.31782043]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 18 ] state=[-0.08260049  0.16772321  0.01066005 -0.31782043], action=0, reward=1.0, next_state=[-0.07924603 -0.02754893  0.00430364 -0.02179486]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 19 ] state=[-0.07924603 -0.02754893  0.00430364 -0.02179486], action=0, reward=1.0, next_state=[-0.07979701 -0.22273234  0.00386774  0.2722428 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 20 ] state=[-0.07979701 -0.22273234  0.00386774  0.2722428 ], action=0, reward=1.0, next_state=[-0.08425165 -0.41790926  0.0093126   0.56614311]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 21 ] state=[-0.08425165 -0.41790926  0.0093126   0.56614311], action=1, reward=1.0, next_state=[-0.09260984 -0.22291919  0.02063546  0.27640854]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 22 ] state=[-0.09260984 -0.22291919  0.02063546  0.27640854], action=0, reward=1.0, next_state=[-0.09706822 -0.41832937  0.02616363  0.57552781]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 23 ] state=[-0.09706822 -0.41832937  0.02616363  0.57552781], action=1, reward=1.0, next_state=[-0.10543481 -0.22358377  0.03767419  0.29120062]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 24 ] state=[-0.10543481 -0.22358377  0.03767419  0.29120062], action=1, reward=1.0, next_state=[-0.10990649 -0.02901869  0.0434982   0.01063368]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 25 ] state=[-0.10990649 -0.02901869  0.0434982   0.01063368], action=1, reward=1.0, next_state=[-0.11048686  0.16545332  0.04371087 -0.26801397]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 26 ] state=[-0.11048686  0.16545332  0.04371087 -0.26801397], action=1, reward=1.0, next_state=[-0.10717779  0.35992508  0.03835059 -0.54659608]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 27 ] state=[-0.10717779  0.35992508  0.03835059 -0.54659608], action=0, reward=1.0, next_state=[-0.09997929  0.16428588  0.02741867 -0.24208075]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 28 ] state=[-0.09997929  0.16428588  0.02741867 -0.24208075], action=1, reward=1.0, next_state=[-0.09669357  0.35900567  0.02257706 -0.52599056]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 29 ] state=[-0.09669357  0.35900567  0.02257706 -0.52599056], action=1, reward=1.0, next_state=[-0.08951346  0.55380277  0.01205725 -0.81147466]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 30 ] state=[-0.08951346  0.55380277  0.01205725 -0.81147466], action=0, reward=1.0, next_state=[-0.07843741  0.35851774 -0.00417225 -0.51502366]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 31 ] state=[-0.07843741  0.35851774 -0.00417225 -0.51502366], action=0, reward=1.0, next_state=[-0.07126705  0.16345479 -0.01447272 -0.22365842]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 32 ] state=[-0.07126705  0.16345479 -0.01447272 -0.22365842], action=0, reward=1.0, next_state=[-0.06799796 -0.03145735 -0.01894589  0.06442434]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 33 ] state=[-0.06799796 -0.03145735 -0.01894589  0.06442434], action=0, reward=1.0, next_state=[-0.0686271  -0.22630261 -0.0176574   0.35107002]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 34 ] state=[-0.0686271  -0.22630261 -0.0176574   0.35107002], action=0, reward=1.0, next_state=[-0.07315315 -0.42116906 -0.010636    0.63813312]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 35 ] state=[-0.07315315 -0.42116906 -0.010636    0.63813312], action=1, reward=1.0, next_state=[-0.08157654 -0.22590043  0.00212666  0.34211981]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 36 ] state=[-0.08157654 -0.22590043  0.00212666  0.34211981], action=0, reward=1.0, next_state=[-0.08609454 -0.42105257  0.00896906  0.6354726 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 37 ] state=[-0.08609454 -0.42105257  0.00896906  0.6354726 ], action=1, reward=1.0, next_state=[-0.0945156  -0.22605686  0.02167851  0.34562766]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 38 ] state=[-0.0945156  -0.22605686  0.02167851  0.34562766], action=1, reward=1.0, next_state=[-0.09903673 -0.03124988  0.02859106  0.05985884]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 39 ] state=[-0.09903673 -0.03124988  0.02859106  0.05985884], action=1, reward=1.0, next_state=[-0.09966173  0.16345071  0.02978824 -0.22366806]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 40 ] state=[-0.09966173  0.16345071  0.02978824 -0.22366806], action=1, reward=1.0, next_state=[-0.09639272  0.35813453  0.02531488 -0.50680778]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 41 ] state=[-0.09639272  0.35813453  0.02531488 -0.50680778], action=0, reward=1.0, next_state=[-0.08923003  0.16266519  0.01517872 -0.20625593]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 42 ] state=[-0.08923003  0.16266519  0.01517872 -0.20625593], action=0, reward=1.0, next_state=[-0.08597672 -0.03267049  0.0110536   0.09117621]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 43 ] state=[-0.08597672 -0.03267049  0.0110536   0.09117621], action=0, reward=1.0, next_state=[-0.08663013 -0.22794912  0.01287713  0.38732595]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 44 ] state=[-0.08663013 -0.22794912  0.01287713  0.38732595], action=1, reward=1.0, next_state=[-0.09118911 -0.03301231  0.02062365  0.09873074]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 45 ] state=[-0.09118911 -0.03301231  0.02062365  0.09873074], action=0, reward=1.0, next_state=[-0.09184936 -0.22842367  0.02259826  0.39784838]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 46 ] state=[-0.09184936 -0.22842367  0.02259826  0.39784838], action=1, reward=1.0, next_state=[-0.09641783 -0.03362949  0.03055523  0.11237503]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 47 ] state=[-0.09641783 -0.03362949  0.03055523  0.11237503], action=1, reward=1.0, next_state=[-0.09709042  0.16104161  0.03280273 -0.17051347]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 48 ] state=[-0.09709042  0.16104161  0.03280273 -0.17051347], action=1, reward=1.0, next_state=[-0.09386959  0.35567907  0.02939246 -0.45267033]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 49 ] state=[-0.09386959  0.35567907  0.02939246 -0.45267033], action=1, reward=1.0, next_state=[-0.08675601  0.5503733   0.02033905 -0.7359454 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 50 ] state=[-0.08675601  0.5503733   0.02033905 -0.7359454 ], action=0, reward=1.0, next_state=[-0.07574854  0.3549764   0.00562015 -0.4369314 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 51 ] state=[-0.07574854  0.3549764   0.00562015 -0.4369314 ], action=0, reward=1.0, next_state=[-0.06864902  0.15977535 -0.00311848 -0.14248213]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 52 ] state=[-0.06864902  0.15977535 -0.00311848 -0.14248213], action=0, reward=1.0, next_state=[-0.06545351 -0.0353018  -0.00596812  0.14921534]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 350 ][ timestamp 53 ] state=[-0.06545351 -0.0353018  -0.00596812  0.14921534], action=0, reward=1.0, next_state=[-0.06615954 -0.23033778 -0.00298382  0.44000948]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 54 ] state=[-0.06615954 -0.23033778 -0.00298382  0.44000948], action=1, reward=1.0, next_state=[-0.0707663  -0.03517373  0.00581637  0.14638747]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 55 ] state=[-0.0707663  -0.03517373  0.00581637  0.14638747], action=1, reward=1.0, next_state=[-0.07146978  0.15986444  0.00874412 -0.14445484]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 56 ] state=[-0.07146978  0.15986444  0.00874412 -0.14445484], action=1, reward=1.0, next_state=[-0.06827249  0.35486009  0.00585502 -0.43436639]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 57 ] state=[-0.06827249  0.35486009  0.00585502 -0.43436639], action=0, reward=1.0, next_state=[-0.06117528  0.15965573 -0.0028323  -0.13984351]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 58 ] state=[-0.06117528  0.15965573 -0.0028323  -0.13984351], action=0, reward=1.0, next_state=[-0.05798217 -0.03542554 -0.00562917  0.15194453]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 59 ] state=[-0.05798217 -0.03542554 -0.00562917  0.15194453], action=1, reward=1.0, next_state=[-0.05869068  0.15977656 -0.00259028 -0.14250895]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 60 ] state=[-0.05869068  0.15977656 -0.00259028 -0.14250895], action=1, reward=1.0, next_state=[-0.05549515  0.35493552 -0.00544046 -0.43600794]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 61 ] state=[-0.05549515  0.35493552 -0.00544046 -0.43600794], action=1, reward=1.0, next_state=[-0.04839644  0.55013406 -0.01416062 -0.73040091]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 62 ] state=[-0.04839644  0.55013406 -0.01416062 -0.73040091], action=0, reward=1.0, next_state=[-0.03739376  0.35521065 -0.02876864 -0.4422082 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 63 ] state=[-0.03739376  0.35521065 -0.02876864 -0.4422082 ], action=0, reward=1.0, next_state=[-0.03028954  0.16050736 -0.0376128  -0.15873092]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 64 ] state=[-0.03028954  0.16050736 -0.0376128  -0.15873092], action=0, reward=1.0, next_state=[-0.0270794  -0.03405647 -0.04078742  0.12185285]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 65 ] state=[-0.0270794  -0.03405647 -0.04078742  0.12185285], action=0, reward=1.0, next_state=[-0.02776053 -0.22857105 -0.03835036  0.40139387]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 66 ] state=[-0.02776053 -0.22857105 -0.03835036  0.40139387], action=1, reward=1.0, next_state=[-0.03233195 -0.0329267  -0.03032249  0.0968709 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 67 ] state=[-0.03233195 -0.0329267  -0.03032249  0.0968709 ], action=1, reward=1.0, next_state=[-0.03299048  0.16261642 -0.02838507 -0.2052224 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 68 ] state=[-0.03299048  0.16261642 -0.02838507 -0.2052224 ], action=0, reward=1.0, next_state=[-0.02973815 -0.03208836 -0.03248952  0.07837303]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 69 ] state=[-0.02973815 -0.03208836 -0.03248952  0.07837303], action=0, reward=1.0, next_state=[-0.03037992 -0.22672985 -0.03092206  0.36063096]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 70 ] state=[-0.03037992 -0.22672985 -0.03092206  0.36063096], action=0, reward=1.0, next_state=[-0.03491452 -0.42139893 -0.02370944  0.64340534]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 71 ] state=[-0.03491452 -0.42139893 -0.02370944  0.64340534], action=0, reward=1.0, next_state=[-0.0433425  -0.61618254 -0.01084133  0.92852872]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 72 ] state=[-0.0433425  -0.61618254 -0.01084133  0.92852872], action=1, reward=1.0, next_state=[-0.05566615 -0.42091592  0.00772924  0.63245871]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 73 ] state=[-0.05566615 -0.42091592  0.00772924  0.63245871], action=1, reward=1.0, next_state=[-0.06408447 -0.22590265  0.02037842  0.34221992]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 74 ] state=[-0.06408447 -0.22590265  0.02037842  0.34221992], action=1, reward=1.0, next_state=[-0.06860252 -0.03107647  0.02722282  0.05603216]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 75 ] state=[-0.06860252 -0.03107647  0.02722282  0.05603216], action=1, reward=1.0, next_state=[-0.06922405  0.16364479  0.02834346 -0.22793898]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 76 ] state=[-0.06922405  0.16364479  0.02834346 -0.22793898], action=1, reward=1.0, next_state=[-0.06595115  0.35835047  0.02378468 -0.51154833]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 77 ] state=[-0.06595115  0.35835047  0.02378468 -0.51154833], action=0, reward=1.0, next_state=[-0.05878414  0.1629017   0.01355371 -0.21146606]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 78 ] state=[-0.05878414  0.1629017   0.01355371 -0.21146606], action=0, reward=1.0, next_state=[-0.05552611 -0.03241139  0.00932439  0.08546133]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 79 ] state=[-0.05552611 -0.03241139  0.00932439  0.08546133], action=0, reward=1.0, next_state=[-0.05617434 -0.22766575  0.01103362  0.38107147]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 80 ] state=[-0.05617434 -0.22766575  0.01103362  0.38107147], action=1, reward=1.0, next_state=[-0.06072765 -0.0327022   0.01865505  0.09188777]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 81 ] state=[-0.06072765 -0.0327022   0.01865505  0.09188777], action=1, reward=1.0, next_state=[-0.0613817   0.16214746  0.0204928  -0.19485161]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 82 ] state=[-0.0613817   0.16214746  0.0204928  -0.19485161], action=0, reward=1.0, next_state=[-0.05813875 -0.03326154  0.01659577  0.10422481]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 83 ] state=[-0.05813875 -0.03326154  0.01659577  0.10422481], action=1, reward=1.0, next_state=[-0.05880398  0.1616187   0.01868027 -0.18317636]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 84 ] state=[-0.05880398  0.1616187   0.01868027 -0.18317636], action=1, reward=1.0, next_state=[-0.0555716   0.35646844  0.01501674 -0.46990831]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 85 ] state=[-0.0555716   0.35646844  0.01501674 -0.46990831], action=1, reward=1.0, next_state=[-0.04844223  0.55137509  0.00561857 -0.75782055]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 86 ] state=[-0.04844223  0.55137509  0.00561857 -0.75782055], action=0, reward=1.0, next_state=[-0.03741473  0.35617616 -0.00953784 -0.46337492]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 87 ] state=[-0.03741473  0.35617616 -0.00953784 -0.46337492], action=0, reward=1.0, next_state=[-0.03029121  0.16119028 -0.01880534 -0.1737135 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 88 ] state=[-0.03029121  0.16119028 -0.01880534 -0.1737135 ], action=1, reward=1.0, next_state=[-0.0270674   0.35657626 -0.02227961 -0.47226908]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 89 ] state=[-0.0270674   0.35657626 -0.02227961 -0.47226908], action=0, reward=1.0, next_state=[-0.01993588  0.16177595 -0.03172499 -0.1866908 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 90 ] state=[-0.01993588  0.16177595 -0.03172499 -0.1866908 ], action=0, reward=1.0, next_state=[-0.01670036 -0.03287807 -0.0354588   0.09581773]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 91 ] state=[-0.01670036 -0.03287807 -0.0354588   0.09581773], action=0, reward=1.0, next_state=[-0.01735792 -0.22747434 -0.03354245  0.37710594]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 92 ] state=[-0.01735792 -0.22747434 -0.03354245  0.37710594], action=1, reward=1.0, next_state=[-0.02190741 -0.03189245 -0.02600033  0.0740385 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 93 ] state=[-0.02190741 -0.03189245 -0.02600033  0.0740385 ], action=0, reward=1.0, next_state=[-0.02254526 -0.2266322  -0.02451956  0.35840615]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 94 ] state=[-0.02254526 -0.2266322  -0.02451956  0.35840615], action=1, reward=1.0, next_state=[-0.0270779  -0.03117041 -0.01735144  0.05809357]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 350 ][ timestamp 95 ] state=[-0.0270779  -0.03117041 -0.01735144  0.05809357], action=1, reward=1.0, next_state=[-0.02770131  0.16419598 -0.01618957 -0.24001296]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 96 ] state=[-0.02770131  0.16419598 -0.01618957 -0.24001296], action=1, reward=1.0, next_state=[-0.02441739  0.35954541 -0.02098982 -0.53775824]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 97 ] state=[-0.02441739  0.35954541 -0.02098982 -0.53775824], action=0, reward=1.0, next_state=[-0.01722648  0.16472476 -0.03174499 -0.25176218]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 98 ] state=[-0.01722648  0.16472476 -0.03174499 -0.25176218], action=1, reward=1.0, next_state=[-0.01393199  0.36028529 -0.03678023 -0.55428665]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 99 ] state=[-0.01393199  0.36028529 -0.03678023 -0.55428665], action=1, reward=1.0, next_state=[-0.00672628  0.55590388 -0.04786597 -0.85832703]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 100 ] state=[-0.00672628  0.55590388 -0.04786597 -0.85832703], action=1, reward=1.0, next_state=[ 0.0043918   0.75164407 -0.06503251 -1.16566824]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 101 ] state=[ 0.0043918   0.75164407 -0.06503251 -1.16566824], action=1, reward=1.0, next_state=[ 0.01942468  0.94754936 -0.08834587 -1.4780111 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 102 ] state=[ 0.01942468  0.94754936 -0.08834587 -1.4780111 ], action=0, reward=1.0, next_state=[ 0.03837567  0.75361027 -0.11790609 -1.2141769 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 103 ] state=[ 0.03837567  0.75361027 -0.11790609 -1.2141769 ], action=0, reward=1.0, next_state=[ 0.05344787  0.56019029 -0.14218963 -0.96064539]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 104 ] state=[ 0.05344787  0.56019029 -0.14218963 -0.96064539], action=0, reward=1.0, next_state=[ 0.06465168  0.36723611 -0.16140254 -0.71579807]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 105 ] state=[ 0.06465168  0.36723611 -0.16140254 -0.71579807], action=0, reward=1.0, next_state=[ 0.0719964   0.17467247 -0.1757185  -0.47795336]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 106 ] state=[ 0.0719964   0.17467247 -0.1757185  -0.47795336], action=0, reward=1.0, next_state=[ 0.07548985 -0.01759002 -0.18527757 -0.24539635]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 107 ] state=[ 0.07548985 -0.01759002 -0.18527757 -0.24539635], action=0, reward=1.0, next_state=[ 0.07513805 -0.20964912 -0.19018549 -0.01639878]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 108 ] state=[ 0.07513805 -0.20964912 -0.19018549 -0.01639878], action=0, reward=1.0, next_state=[ 0.07094507 -0.40160693 -0.19051347  0.21076814]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 109 ] state=[ 0.07094507 -0.40160693 -0.19051347  0.21076814], action=0, reward=1.0, next_state=[ 0.06291293 -0.59356682 -0.18629811  0.43782556]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 110 ] state=[ 0.06291293 -0.59356682 -0.18629811  0.43782556], action=0, reward=1.0, next_state=[ 0.05104159 -0.78563088 -0.1775416   0.66648125]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 111 ] state=[ 0.05104159 -0.78563088 -0.1775416   0.66648125], action=0, reward=1.0, next_state=[ 0.03532897 -0.97789752 -0.16421197  0.89842437]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 112 ] state=[ 0.03532897 -0.97789752 -0.16421197  0.89842437], action=0, reward=1.0, next_state=[ 0.01577102 -1.17045874 -0.14624348  1.13531891]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 113 ] state=[ 0.01577102 -1.17045874 -0.14624348  1.13531891], action=0, reward=1.0, next_state=[-0.00763815 -1.36339658 -0.12353711  1.37879391]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 114 ] state=[-0.00763815 -1.36339658 -0.12353711  1.37879391], action=1, reward=1.0, next_state=[-0.03490608 -1.16696768 -0.09596123  1.05016843]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 115 ] state=[-0.03490608 -1.16696768 -0.09596123  1.05016843], action=1, reward=1.0, next_state=[-0.05824544 -0.97071286 -0.07495786  0.72897126]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 116 ] state=[-0.05824544 -0.97071286 -0.07495786  0.72897126], action=1, reward=1.0, next_state=[-0.07765969 -0.77463921 -0.06037843  0.41366968]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 117 ] state=[-0.07765969 -0.77463921 -0.06037843  0.41366968], action=0, reward=1.0, next_state=[-0.09315248 -0.96885568 -0.05210504  0.68672304]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 118 ] state=[-0.09315248 -0.96885568 -0.05210504  0.68672304], action=1, reward=1.0, next_state=[-0.11252959 -0.77305064 -0.03837058  0.37810215]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 119 ] state=[-0.11252959 -0.77305064 -0.03837058  0.37810215], action=0, reward=1.0, next_state=[-0.12799061 -0.96760724 -0.03080854  0.65844406]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 120 ] state=[-0.12799061 -0.96760724 -0.03080854  0.65844406], action=0, reward=1.0, next_state=[-0.14734275 -1.16228715 -0.01763965  0.94126906]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 121 ] state=[-0.14734275 -1.16228715 -0.01763965  0.94126906], action=0, reward=1.0, next_state=[-1.70588493e-01 -1.35716698e+00  1.18572651e-03  1.22835755e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 122 ] state=[-1.70588493e-01 -1.35716698e+00  1.18572651e-03  1.22835755e+00], action=1, reward=1.0, next_state=[-0.19773183 -1.16206031  0.02575288  0.93604635]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 123 ] state=[-0.19773183 -1.16206031  0.02575288  0.93604635], action=1, reward=1.0, next_state=[-0.22097304 -0.96729497  0.0444738   0.65156572]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 124 ] state=[-0.22097304 -0.96729497  0.0444738   0.65156572], action=1, reward=1.0, next_state=[-0.24031894 -0.77281971  0.05750512  0.37321226]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 125 ] state=[-0.24031894 -0.77281971  0.05750512  0.37321226], action=1, reward=1.0, next_state=[-0.25577533 -0.57855979  0.06496936  0.09920123]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 126 ] state=[-0.25577533 -0.57855979  0.06496936  0.09920123], action=1, reward=1.0, next_state=[-0.26734653 -0.38442622  0.06695339 -0.17229719]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 127 ] state=[-0.26734653 -0.38442622  0.06695339 -0.17229719], action=1, reward=1.0, next_state=[-0.27503505 -0.19032325  0.06350744 -0.44312972]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 128 ] state=[-0.27503505 -0.19032325  0.06350744 -0.44312972], action=0, reward=1.0, next_state=[-0.27884152 -0.38628363  0.05464485 -0.13112307]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 129 ] state=[-0.27884152 -0.38628363  0.05464485 -0.13112307], action=1, reward=1.0, next_state=[-0.28656719 -0.1919853   0.05202239 -0.40607794]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 130 ] state=[-0.28656719 -0.1919853   0.05202239 -0.40607794], action=0, reward=1.0, next_state=[-0.2904069  -0.38780488  0.04390083 -0.09745826]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 131 ] state=[-0.2904069  -0.38780488  0.04390083 -0.09745826], action=1, reward=1.0, next_state=[-0.29816299 -0.19333872  0.04195166 -0.37597375]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 132 ] state=[-0.29816299 -0.19333872  0.04195166 -0.37597375], action=0, reward=1.0, next_state=[-0.30202977 -0.38903065  0.03443219 -0.07036396]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 133 ] state=[-0.30202977 -0.38903065  0.03443219 -0.07036396], action=0, reward=1.0, next_state=[-0.30981038 -0.58462889  0.03302491  0.23298056]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 134 ] state=[-0.30981038 -0.58462889  0.03302491  0.23298056], action=1, reward=1.0, next_state=[-0.32150296 -0.389994    0.03768452 -0.04910503]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 135 ] state=[-0.32150296 -0.389994    0.03768452 -0.04910503], action=1, reward=1.0, next_state=[-0.32930284 -0.1954321   0.03670242 -0.32966403]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 350 ][ timestamp 136 ] state=[-0.32930284 -0.1954321   0.03670242 -0.32966403], action=0, reward=1.0, next_state=[-0.33321148 -0.39105678  0.03010914 -0.02563654]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 137 ] state=[-0.33321148 -0.39105678  0.03010914 -0.02563654], action=1, reward=1.0, next_state=[-0.34103262 -0.19637927  0.02959641 -0.30866971]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 138 ] state=[-0.34103262 -0.19637927  0.02959641 -0.30866971], action=1, reward=1.0, next_state=[-0.3449602  -0.00169125  0.02342302 -0.59187377]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 139 ] state=[-0.3449602  -0.00169125  0.02342302 -0.59187377], action=1, reward=1.0, next_state=[-0.34499403  0.19309509  0.01158554 -0.87708739]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 140 ] state=[-0.34499403  0.19309509  0.01158554 -0.87708739], action=0, reward=1.0, next_state=[-0.34113213 -0.00218239 -0.00595621 -0.58078476]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 141 ] state=[-0.34113213 -0.00218239 -0.00595621 -0.58078476], action=0, reward=1.0, next_state=[-0.34117577 -0.19722038 -0.0175719  -0.28998409]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 142 ] state=[-0.34117577 -0.19722038 -0.0175719  -0.28998409], action=0, reward=1.0, next_state=[-0.34512018 -0.39208741 -0.02337158 -0.00289453]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 143 ] state=[-0.34512018 -0.39208741 -0.02337158 -0.00289453], action=1, reward=1.0, next_state=[-0.35296193 -0.19663821 -0.02342948 -0.30285889]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 144 ] state=[-0.35296193 -0.19663821 -0.02342948 -0.30285889], action=0, reward=1.0, next_state=[-0.35689469 -0.39141855 -0.02948665 -0.01765621]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 145 ] state=[-0.35689469 -0.39141855 -0.02948665 -0.01765621], action=0, reward=1.0, next_state=[-0.36472306 -0.58610549 -0.02983978  0.26557944]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 146 ] state=[-0.36472306 -0.58610549 -0.02983978  0.26557944], action=1, reward=1.0, next_state=[-0.37644517 -0.39057063 -0.02452819 -0.03636387]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 147 ] state=[-0.37644517 -0.39057063 -0.02452819 -0.03636387], action=0, reward=1.0, next_state=[-0.38425659 -0.58533241 -0.02525547  0.24848036]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 148 ] state=[-0.38425659 -0.58533241 -0.02525547  0.24848036], action=0, reward=1.0, next_state=[-0.39596323 -0.78008475 -0.02028586  0.53309139]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 149 ] state=[-0.39596323 -0.78008475 -0.02028586  0.53309139], action=1, reward=1.0, next_state=[-0.41156493 -0.58468346 -0.00962403  0.23408613]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 150 ] state=[-0.41156493 -0.58468346 -0.00962403  0.23408613], action=1, reward=1.0, next_state=[-0.4232586  -0.38942533 -0.00494231 -0.06161692]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 151 ] state=[-0.4232586  -0.38942533 -0.00494231 -0.06161692], action=0, reward=1.0, next_state=[-0.43104711 -0.58447607 -0.00617465  0.22950258]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 152 ] state=[-0.43104711 -0.58447607 -0.00617465  0.22950258], action=1, reward=1.0, next_state=[-0.44273663 -0.38926643 -0.00158459 -0.06512163]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 153 ] state=[-0.44273663 -0.38926643 -0.00158459 -0.06512163], action=0, reward=1.0, next_state=[-0.45052196 -0.58436563 -0.00288703  0.22706093]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 154 ] state=[-0.45052196 -0.58436563 -0.00288703  0.22706093], action=1, reward=1.0, next_state=[-0.46220927 -0.38920254  0.00165419 -0.06653127]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 155 ] state=[-0.46220927 -0.38920254  0.00165419 -0.06653127], action=1, reward=1.0, next_state=[-4.69993319e-01 -1.94104343e-01  3.23565690e-04 -3.58691835e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 156 ] state=[-4.69993319e-01 -1.94104343e-01  3.23565690e-04 -3.58691835e-01], action=0, reward=1.0, next_state=[-0.47387541 -0.38923089 -0.00685027 -0.0659069 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 157 ] state=[-0.47387541 -0.38923089 -0.00685027 -0.0659069 ], action=1, reward=1.0, next_state=[-0.48166002 -0.1940114  -0.00816841 -0.36074323]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 158 ] state=[-0.48166002 -0.1940114  -0.00816841 -0.36074323], action=0, reward=1.0, next_state=[-0.48554025 -0.3890163  -0.01538327 -0.07064713]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 159 ] state=[-0.48554025 -0.3890163  -0.01538327 -0.07064713], action=0, reward=1.0, next_state=[-0.49332058 -0.58391436 -0.01679622  0.21714288]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 160 ] state=[-0.49332058 -0.58391436 -0.01679622  0.21714288], action=1, reward=1.0, next_state=[-0.50499887 -0.38855638 -0.01245336 -0.08079062]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 161 ] state=[-0.50499887 -0.38855638 -0.01245336 -0.08079062], action=1, reward=1.0, next_state=[-0.51276999 -0.19325814 -0.01406917 -0.37737645]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 162 ] state=[-0.51276999 -0.19325814 -0.01406917 -0.37737645], action=1, reward=1.0, next_state=[-0.51663516  0.00206077 -0.0216167  -0.67446203]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 163 ] state=[-0.51663516  0.00206077 -0.0216167  -0.67446203], action=1, reward=1.0, next_state=[-0.51659394  0.19747637 -0.03510594 -0.97387175]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 164 ] state=[-0.51659394  0.19747637 -0.03510594 -0.97387175], action=1, reward=1.0, next_state=[-0.51264441  0.39305127 -0.05458338 -1.27737238]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 165 ] state=[-0.51264441  0.39305127 -0.05458338 -1.27737238], action=0, reward=1.0, next_state=[-0.50478339  0.19866606 -0.08013082 -1.00226835]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 166 ] state=[-0.50478339  0.19866606 -0.08013082 -1.00226835], action=1, reward=1.0, next_state=[-0.50081007  0.39476191 -0.10017619 -1.31900155]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 167 ] state=[-0.50081007  0.39476191 -0.10017619 -1.31900155], action=0, reward=1.0, next_state=[-0.49291483  0.20103906 -0.12655622 -1.05927667]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 168 ] state=[-0.49291483  0.20103906 -0.12655622 -1.05927667], action=0, reward=1.0, next_state=[-0.48889405  0.00779988 -0.14774175 -0.80884435]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 169 ] state=[-0.48889405  0.00779988 -0.14774175 -0.80884435], action=0, reward=1.0, next_state=[-0.48873805 -0.18502216 -0.16391864 -0.56604041]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 170 ] state=[-0.48873805 -0.18502216 -0.16391864 -0.56604041], action=0, reward=1.0, next_state=[-0.49243849 -0.37751111 -0.17523945 -0.32915391]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 171 ] state=[-0.49243849 -0.37751111 -0.17523945 -0.32915391], action=0, reward=1.0, next_state=[-0.49998871 -0.56976185 -0.18182253 -0.09645144]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 172 ] state=[-0.49998871 -0.56976185 -0.18182253 -0.09645144], action=0, reward=1.0, next_state=[-0.51138395 -0.76187532 -0.18375156  0.13380672]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 173 ] state=[-0.51138395 -0.76187532 -0.18375156  0.13380672], action=0, reward=1.0, next_state=[-0.52662146 -0.95395488 -0.18107542  0.36335616]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 174 ] state=[-0.52662146 -0.95395488 -0.18107542  0.36335616], action=0, reward=1.0, next_state=[-0.54570056 -1.14610356 -0.1738083   0.59392117]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 175 ] state=[-0.54570056 -1.14610356 -0.1738083   0.59392117], action=0, reward=1.0, next_state=[-0.56862263 -1.33842142 -0.16192988  0.82720882]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 176 ] state=[-0.56862263 -1.33842142 -0.16192988  0.82720882], action=0, reward=1.0, next_state=[-0.59539106 -1.53100279 -0.1453857   1.06490226]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 177 ] state=[-0.59539106 -1.53100279 -0.1453857   1.06490226], action=1, reward=1.0, next_state=[-0.62601111 -1.33428744 -0.12408765  0.73034924]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 350 ][ timestamp 178 ] state=[-0.62601111 -1.33428744 -0.12408765  0.73034924], action=0, reward=1.0, next_state=[-0.65269686 -1.52749585 -0.10948067  0.98154527]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 179 ] state=[-0.65269686 -1.52749585 -0.10948067  0.98154527], action=1, reward=1.0, next_state=[-0.68324678 -1.33109057 -0.08984976  0.65657811]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 180 ] state=[-0.68324678 -1.33109057 -0.08984976  0.65657811], action=1, reward=1.0, next_state=[-0.70986859 -1.13484032 -0.0767182   0.33700987]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 181 ] state=[-0.70986859 -1.13484032 -0.0767182   0.33700987], action=1, reward=1.0, next_state=[-0.73256539 -0.93871522 -0.069978    0.02115451]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 182 ] state=[-0.73256539 -0.93871522 -0.069978    0.02115451], action=0, reward=1.0, next_state=[-0.7513397  -1.13276747 -0.06955491  0.29096374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 183 ] state=[-0.7513397  -1.13276747 -0.06955491  0.29096374], action=1, reward=1.0, next_state=[-0.77399505 -0.93672624 -0.06373564 -0.02281973]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 184 ] state=[-0.77399505 -0.93672624 -0.06373564 -0.02281973], action=0, reward=1.0, next_state=[-0.79272957 -1.130879   -0.06419203  0.2490925 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 185 ] state=[-0.79272957 -1.130879   -0.06419203  0.2490925 ], action=1, reward=1.0, next_state=[-0.81534715 -0.93490186 -0.05921018 -0.06312726]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 186 ] state=[-0.81534715 -0.93490186 -0.05921018 -0.06312726], action=0, reward=1.0, next_state=[-0.83404519 -1.1291271  -0.06047273  0.21030243]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 187 ] state=[-0.83404519 -1.1291271  -0.06047273  0.21030243], action=1, reward=1.0, next_state=[-0.85662773 -0.93319496 -0.05626668 -0.1008267 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 188 ] state=[-0.85662773 -0.93319496 -0.05626668 -0.1008267 ], action=0, reward=1.0, next_state=[-0.87529163 -1.12746727 -0.05828321  0.17358692]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 189 ] state=[-0.87529163 -1.12746727 -0.05828321  0.17358692], action=1, reward=1.0, next_state=[-0.89784098 -0.93156169 -0.05481148 -0.13689804]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 190 ] state=[-0.89784098 -0.93156169 -0.05481148 -0.13689804], action=0, reward=1.0, next_state=[-0.91647221 -1.12585748 -0.05754944  0.13800144]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 191 ] state=[-0.91647221 -1.12585748 -0.05754944  0.13800144], action=1, reward=1.0, next_state=[-0.93898936 -0.92996049 -0.05478941 -0.17226777]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 192 ] state=[-0.93898936 -0.92996049 -0.05478941 -0.17226777], action=1, reward=1.0, next_state=[-0.95758857 -0.73409892 -0.05823476 -0.48171931]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 193 ] state=[-0.95758857 -0.73409892 -0.05823476 -0.48171931], action=0, reward=1.0, next_state=[-0.97227055 -0.92835261 -0.06786915 -0.20794405]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 194 ] state=[-0.97227055 -0.92835261 -0.06786915 -0.20794405], action=0, reward=1.0, next_state=[-0.9908376  -1.12244175 -0.07202803  0.06258119]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 195 ] state=[-0.9908376  -1.12244175 -0.07202803  0.06258119], action=0, reward=1.0, next_state=[-1.01328644 -1.31646103 -0.07077641  0.33169757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 196 ] state=[-1.01328644 -1.31646103 -0.07077641  0.33169757], action=1, reward=1.0, next_state=[-1.03961566 -1.12040676 -0.06414246  0.01756153]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 197 ] state=[-1.03961566 -1.12040676 -0.06414246  0.01756153], action=1, reward=1.0, next_state=[-1.06202379 -0.92442637 -0.06379123 -0.29464948]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 198 ] state=[-1.06202379 -0.92442637 -0.06379123 -0.29464948], action=1, reward=1.0, next_state=[-1.08051232 -0.72845571 -0.06968421 -0.60674947]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 199 ] state=[-1.08051232 -0.72845571 -0.06968421 -0.60674947], action=0, reward=1.0, next_state=[-1.09508143 -0.92253768 -0.0818192  -0.33680365]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 200 ] state=[-1.09508143 -0.92253768 -0.0818192  -0.33680365], action=0, reward=1.0, next_state=[-1.11353219 -1.11640575 -0.08855528 -0.07100239]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 201 ] state=[-1.11353219 -1.11640575 -0.08855528 -0.07100239], action=0, reward=1.0, next_state=[-1.1358603  -1.31015387 -0.08997533  0.19247976]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 202 ] state=[-1.1358603  -1.31015387 -0.08997533  0.19247976], action=1, reward=1.0, next_state=[-1.16206338 -1.1138677  -0.08612573 -0.12717558]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 203 ] state=[-1.16206338 -1.1138677  -0.08612573 -0.12717558], action=1, reward=1.0, next_state=[-1.18434073 -0.91762428 -0.08866924 -0.4457393 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 204 ] state=[-1.18434073 -0.91762428 -0.08866924 -0.4457393 ], action=1, reward=1.0, next_state=[-1.20269322 -0.72136712 -0.09758403 -0.76500316]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 205 ] state=[-1.20269322 -0.72136712 -0.09758403 -0.76500316], action=1, reward=1.0, next_state=[-1.21712056 -0.52504652 -0.11288409 -1.08672724]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 206 ] state=[-1.21712056 -0.52504652 -0.11288409 -1.08672724], action=1, reward=1.0, next_state=[-1.22762149 -0.32863151 -0.13461864 -1.41259207]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 207 ] state=[-1.22762149 -0.32863151 -0.13461864 -1.41259207], action=0, reward=1.0, next_state=[-1.23419412 -0.52185268 -0.16287048 -1.16484099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 208 ] state=[-1.23419412 -0.52185268 -0.16287048 -1.16484099], action=0, reward=1.0, next_state=[-1.24463118 -0.71452389 -0.1861673  -0.92733141]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 209 ] state=[-1.24463118 -0.71452389 -0.1861673  -0.92733141], action=0, reward=1.0, next_state=[-1.25892165 -0.90671071 -0.20471392 -0.69845   ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 350 ][ timestamp 210 ] state=[-1.25892165 -0.90671071 -0.20471392 -0.69845   ], action=0, reward=-1.0, next_state=[-1.27705587 -1.09849493 -0.21868292 -0.47654698]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 350: Exploration_rate=0.01. Score=210.\n",
      "[ episode 351 ] state=[ 0.01814884  0.04693025  0.03649043 -0.03501287]\n",
      "[ episode 351 ][ timestamp 1 ] state=[ 0.01814884  0.04693025  0.03649043 -0.03501287], action=1, reward=1.0, next_state=[ 0.01908744  0.24151044  0.03579018 -0.31596304]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 2 ] state=[ 0.01908744  0.24151044  0.03579018 -0.31596304], action=1, reward=1.0, next_state=[ 0.02391765  0.4361048   0.02947092 -0.59714759]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 3 ] state=[ 0.02391765  0.4361048   0.02947092 -0.59714759], action=0, reward=1.0, next_state=[ 0.03263975  0.24058311  0.01752796 -0.29532921]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 4 ] state=[ 0.03263975  0.24058311  0.01752796 -0.29532921], action=0, reward=1.0, next_state=[0.03745141 0.04521571 0.01162138 0.00282982]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 5 ] state=[0.03745141 0.04521571 0.01162138 0.00282982], action=0, reward=1.0, next_state=[ 0.03835572 -0.15007096  0.01167798  0.29915666]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 6 ] state=[ 0.03835572 -0.15007096  0.01167798  0.29915666], action=1, reward=1.0, next_state=[0.0353543  0.0448826  0.01766111 0.0101795 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 7 ] state=[0.0353543  0.0448826  0.01766111 0.0101795 ], action=1, reward=1.0, next_state=[ 0.03625195  0.23974687  0.0178647  -0.27687922]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 8 ] state=[ 0.03625195  0.23974687  0.0178647  -0.27687922], action=0, reward=1.0, next_state=[0.04104689 0.04437467 0.01232712 0.02138426]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 9 ] state=[0.04104689 0.04437467 0.01232712 0.02138426], action=0, reward=1.0, next_state=[ 0.04193439 -0.15092188  0.0127548   0.3179309 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 10 ] state=[ 0.04193439 -0.15092188  0.0127548   0.3179309 ], action=0, reward=1.0, next_state=[ 0.03891595 -0.34622315  0.01911342  0.61460879]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 351 ][ timestamp 11 ] state=[ 0.03891595 -0.34622315  0.01911342  0.61460879], action=0, reward=1.0, next_state=[ 0.03199149 -0.54160688  0.03140559  0.91324986]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 12 ] state=[ 0.03199149 -0.54160688  0.03140559  0.91324986], action=0, reward=1.0, next_state=[ 0.02115935 -0.73713928  0.04967059  1.21563556]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 13 ] state=[ 0.02115935 -0.73713928  0.04967059  1.21563556], action=1, reward=1.0, next_state=[ 0.00641656 -0.542692    0.0739833   0.93892151]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 14 ] state=[ 0.00641656 -0.542692    0.0739833   0.93892151], action=0, reward=1.0, next_state=[-0.00443728 -0.73872917  0.09276173  1.25390413]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 15 ] state=[-0.00443728 -0.73872917  0.09276173  1.25390413], action=1, reward=1.0, next_state=[-0.01921186 -0.54490954  0.11783982  0.99165746]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 16 ] state=[-0.01921186 -0.54490954  0.11783982  0.99165746], action=1, reward=1.0, next_state=[-0.03011005 -0.35154475  0.13767297  0.73818656]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 17 ] state=[-0.03011005 -0.35154475  0.13767297  0.73818656], action=1, reward=1.0, next_state=[-0.03714095 -0.15856536  0.1524367   0.49180454]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 18 ] state=[-0.03714095 -0.15856536  0.1524367   0.49180454], action=1, reward=1.0, next_state=[-0.04031225  0.03411489  0.16227279  0.25077867]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 19 ] state=[-0.04031225  0.03411489  0.16227279  0.25077867], action=1, reward=1.0, next_state=[-0.03962996  0.22659254  0.16728836  0.01335425]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 20 ] state=[-0.03962996  0.22659254  0.16728836  0.01335425], action=1, reward=1.0, next_state=[-0.03509811  0.4189696   0.16755545 -0.22222924]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 21 ] state=[-0.03509811  0.4189696   0.16755545 -0.22222924], action=0, reward=1.0, next_state=[-0.02671871  0.22189848  0.16311086  0.11826871]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 22 ] state=[-0.02671871  0.22189848  0.16311086  0.11826871], action=1, reward=1.0, next_state=[-0.02228074  0.41435333  0.16547623 -0.11883961]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 23 ] state=[-0.02228074  0.41435333  0.16547623 -0.11883961], action=1, reward=1.0, next_state=[-0.01399368  0.60676545  0.16309944 -0.35508698]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 24 ] state=[-0.01399368  0.60676545  0.16309944 -0.35508698], action=1, reward=1.0, next_state=[-0.00185837  0.79923823  0.1559977  -0.59222569]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 25 ] state=[-0.00185837  0.79923823  0.1559977  -0.59222569], action=0, reward=1.0, next_state=[ 0.0141264   0.60231613  0.14415319 -0.25475185]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 26 ] state=[ 0.0141264   0.60231613  0.14415319 -0.25475185], action=0, reward=1.0, next_state=[0.02617272 0.40546168 0.13905815 0.07970155]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 27 ] state=[0.02617272 0.40546168 0.13905815 0.07970155], action=1, reward=1.0, next_state=[ 0.03428195  0.59834465  0.14065218 -0.16607858]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 28 ] state=[ 0.03428195  0.59834465  0.14065218 -0.16607858], action=1, reward=1.0, next_state=[ 0.04624885  0.79120238  0.13733061 -0.41129287]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 29 ] state=[ 0.04624885  0.79120238  0.13733061 -0.41129287], action=1, reward=1.0, next_state=[ 0.06207289  0.98413749  0.12910475 -0.6577224 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 30 ] state=[ 0.06207289  0.98413749  0.12910475 -0.6577224 ], action=0, reward=1.0, next_state=[ 0.08175564  0.78747735  0.11595031 -0.32733579]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 31 ] state=[ 0.08175564  0.78747735  0.11595031 -0.32733579], action=0, reward=1.0, next_state=[ 9.75051896e-02  5.90911967e-01  1.09403591e-01 -4.54475900e-04]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 32 ] state=[ 9.75051896e-02  5.90911967e-01  1.09403591e-01 -4.54475900e-04], action=1, reward=1.0, next_state=[ 0.10932343  0.7843087   0.1093945  -0.25671468]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 33 ] state=[ 0.10932343  0.7843087   0.1093945  -0.25671468], action=1, reward=1.0, next_state=[ 0.1250096   0.9777126   0.10426021 -0.51298852]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 34 ] state=[ 0.1250096   0.9777126   0.10426021 -0.51298852], action=0, reward=1.0, next_state=[ 0.14456385  0.7812886   0.09400044 -0.18935545]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 35 ] state=[ 0.14456385  0.7812886   0.09400044 -0.18935545], action=1, reward=1.0, next_state=[ 0.16018963  0.97494882  0.09021333 -0.45096787]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 36 ] state=[ 0.16018963  0.97494882  0.09021333 -0.45096787], action=1, reward=1.0, next_state=[ 0.1796886   1.16868681  0.08119397 -0.71390635]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 37 ] state=[ 0.1796886   1.16868681  0.08119397 -0.71390635], action=1, reward=1.0, next_state=[ 0.20306234  1.36259642  0.06691584 -0.97996774]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 38 ] state=[ 0.20306234  1.36259642  0.06691584 -0.97996774], action=0, reward=1.0, next_state=[ 0.23031427  1.16664442  0.04731649 -0.66703897]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 39 ] state=[ 0.23031427  1.16664442  0.04731649 -0.66703897], action=0, reward=1.0, next_state=[ 0.25364716  0.97089747  0.03397571 -0.35984131]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 40 ] state=[ 0.25364716  0.97089747  0.03397571 -0.35984131], action=0, reward=1.0, next_state=[ 0.27306511  0.77530943  0.02677888 -0.05664164]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 41 ] state=[ 0.27306511  0.77530943  0.02677888 -0.05664164], action=1, reward=1.0, next_state=[ 0.28857129  0.97003739  0.02564605 -0.3407568 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 42 ] state=[ 0.28857129  0.97003739  0.02564605 -0.3407568 ], action=1, reward=1.0, next_state=[ 0.30797204  1.16478523  0.01883091 -0.62524339]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 43 ] state=[ 0.30797204  1.16478523  0.01883091 -0.62524339], action=0, reward=1.0, next_state=[ 0.33126775  0.96940553  0.00632605 -0.32668984]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 44 ] state=[ 0.33126775  0.96940553  0.00632605 -0.32668984], action=0, reward=1.0, next_state=[ 3.50655858e-01  7.74194091e-01 -2.07750558e-04 -3.20186912e-02]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 45 ] state=[ 3.50655858e-01  7.74194091e-01 -2.07750558e-04 -3.20186912e-02], action=1, reward=1.0, next_state=[ 3.66139739e-01  9.69319020e-01 -8.48124382e-04 -3.24767158e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 46 ] state=[ 3.66139739e-01  9.69319020e-01 -8.48124382e-04 -3.24767158e-01], action=0, reward=1.0, next_state=[ 0.38552612  0.77420916 -0.00734347 -0.03235181]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 47 ] state=[ 0.38552612  0.77420916 -0.00734347 -0.03235181], action=0, reward=1.0, next_state=[ 0.4010103   0.57919328 -0.0079905   0.25800515]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 48 ] state=[ 0.4010103   0.57919328 -0.0079905   0.25800515], action=1, reward=1.0, next_state=[ 0.41259417  0.77442839 -0.0028304  -0.03718735]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 49 ] state=[ 0.41259417  0.77442839 -0.0028304  -0.03718735], action=1, reward=1.0, next_state=[ 0.42808274  0.96959081 -0.00357415 -0.33076195]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 50 ] state=[ 0.42808274  0.96959081 -0.00357415 -0.33076195], action=1, reward=1.0, next_state=[ 0.44747455  1.16476346 -0.01018939 -0.62456984]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 51 ] state=[ 0.44747455  1.16476346 -0.01018939 -0.62456984], action=0, reward=1.0, next_state=[ 0.47076982  0.96978523 -0.02268078 -0.33511331]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 351 ][ timestamp 52 ] state=[ 0.47076982  0.96978523 -0.02268078 -0.33511331], action=1, reward=1.0, next_state=[ 0.49016553  1.16522251 -0.02938305 -0.63486141]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 53 ] state=[ 0.49016553  1.16522251 -0.02938305 -0.63486141], action=1, reward=1.0, next_state=[ 0.51346998  1.36074171 -0.04208028 -0.93665099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 54 ] state=[ 0.51346998  1.36074171 -0.04208028 -0.93665099], action=1, reward=1.0, next_state=[ 0.54068481  1.5564051  -0.0608133  -1.24225421]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 55 ] state=[ 0.54068481  1.5564051  -0.0608133  -1.24225421], action=0, reward=1.0, next_state=[ 0.57181291  1.36211416 -0.08565838 -0.96922463]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 56 ] state=[ 0.57181291  1.36211416 -0.08565838 -0.96922463], action=0, reward=1.0, next_state=[ 0.5990552   1.1682401  -0.10504287 -0.70463256]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 57 ] state=[ 0.5990552   1.1682401  -0.10504287 -0.70463256], action=1, reward=1.0, next_state=[ 0.62242     1.36464868 -0.11913553 -1.02844739]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 58 ] state=[ 0.62242     1.36464868 -0.11913553 -1.02844739], action=1, reward=1.0, next_state=[ 0.64971297  1.56113735 -0.13970447 -1.3560343 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 59 ] state=[ 0.64971297  1.56113735 -0.13970447 -1.3560343 ], action=1, reward=1.0, next_state=[ 0.68093572  1.75770819 -0.16682516 -1.68895746]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 60 ] state=[ 0.68093572  1.75770819 -0.16682516 -1.68895746], action=1, reward=1.0, next_state=[ 0.71608988  1.95431971 -0.20060431 -2.0285998 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 351 ][ timestamp 61 ] state=[ 0.71608988  1.95431971 -0.20060431 -2.0285998 ], action=0, reward=-1.0, next_state=[ 0.75517628  1.76175759 -0.24117631 -1.80413189]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 351: Exploration_rate=0.01. Score=61.\n",
      "[ episode 352 ] state=[-0.02012448 -0.00445563 -0.00859701 -0.04066189]\n",
      "[ episode 352 ][ timestamp 1 ] state=[-0.02012448 -0.00445563 -0.00859701 -0.04066189], action=1, reward=1.0, next_state=[-0.02021359  0.19078854 -0.00941025 -0.33604481]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 2 ] state=[-0.02021359  0.19078854 -0.00941025 -0.33604481], action=0, reward=1.0, next_state=[-0.01639782 -0.00419824 -0.01613115 -0.04634417]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 3 ] state=[-0.01639782 -0.00419824 -0.01613115 -0.04634417], action=1, reward=1.0, next_state=[-0.01648178  0.19115127 -0.01705803 -0.34407265]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 4 ] state=[-0.01648178  0.19115127 -0.01705803 -0.34407265], action=0, reward=1.0, next_state=[-0.01265876 -0.00372392 -0.02393948 -0.05681722]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 5 ] state=[-0.01265876 -0.00372392 -0.02393948 -0.05681722], action=0, reward=1.0, next_state=[-0.01273324 -0.19849458 -0.02507583  0.22821752]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 6 ] state=[-0.01273324 -0.19849458 -0.02507583  0.22821752], action=1, reward=1.0, next_state=[-0.01670313 -0.00302342 -0.02051148 -0.07226856]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 7 ] state=[-0.01670313 -0.00302342 -0.02051148 -0.07226856], action=0, reward=1.0, next_state=[-0.0167636  -0.1978454  -0.02195685  0.21387299]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 8 ] state=[-0.0167636  -0.1978454  -0.02195685  0.21387299], action=0, reward=1.0, next_state=[-0.02072051 -0.39264667 -0.01767939  0.49954966]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 9 ] state=[-0.02072051 -0.39264667 -0.01767939  0.49954966], action=1, reward=1.0, next_state=[-0.02857344 -0.19727999 -0.00768839  0.20134798]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 10 ] state=[-0.02857344 -0.19727999 -0.00768839  0.20134798], action=1, reward=1.0, next_state=[-0.03251904 -0.00204893 -0.00366144 -0.09375033]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 11 ] state=[-0.03251904 -0.00204893 -0.00366144 -0.09375033], action=1, reward=1.0, next_state=[-0.03256002  0.19312531 -0.00553644 -0.38758618]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 12 ] state=[-0.03256002  0.19312531 -0.00553644 -0.38758618], action=0, reward=1.0, next_state=[-0.02869751 -0.00191762 -0.01328817 -0.09665398]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 13 ] state=[-0.02869751 -0.00191762 -0.01328817 -0.09665398], action=0, reward=1.0, next_state=[-0.02873586 -0.19684663 -0.01522124  0.19180711]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 14 ] state=[-0.02873586 -0.19684663 -0.01522124  0.19180711], action=1, reward=1.0, next_state=[-0.0326728  -0.00151027 -0.0113851  -0.10563836]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 15 ] state=[-0.0326728  -0.00151027 -0.0113851  -0.10563836], action=0, reward=1.0, next_state=[-0.032703   -0.19646723 -0.01349787  0.18343098]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 16 ] state=[-0.032703   -0.19646723 -0.01349787  0.18343098], action=0, reward=1.0, next_state=[-0.03663235 -0.39139347 -0.00982925  0.47182546]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 17 ] state=[-0.03663235 -0.39139347 -0.00982925  0.47182546], action=1, reward=1.0, next_state=[-0.04446022 -0.19613408 -0.00039274  0.17606076]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 18 ] state=[-0.04446022 -0.19613408 -0.00039274  0.17606076], action=1, reward=1.0, next_state=[-0.0483829  -0.00100651  0.00312847 -0.11674604]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 19 ] state=[-0.0483829  -0.00100651  0.00312847 -0.11674604], action=1, reward=1.0, next_state=[-0.04840303  0.19407048  0.00079355 -0.40844031]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 20 ] state=[-0.04840303  0.19407048  0.00079355 -0.40844031], action=0, reward=1.0, next_state=[-0.04452162 -0.00106272 -0.00737525 -0.11550731]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 21 ] state=[-0.04452162 -0.00106272 -0.00737525 -0.11550731], action=0, reward=1.0, next_state=[-0.04454287 -0.19607822 -0.0096854   0.17483968]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 22 ] state=[-0.04454287 -0.19607822 -0.0096854   0.17483968], action=1, reward=1.0, next_state=[-0.04846444 -0.000819   -0.00618861 -0.12088287]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 23 ] state=[-0.04846444 -0.000819   -0.00618861 -0.12088287], action=0, reward=1.0, next_state=[-0.04848082 -0.19585174 -0.00860626  0.16984121]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 24 ] state=[-0.04848082 -0.19585174 -0.00860626  0.16984121], action=1, reward=1.0, next_state=[-0.05239785 -0.00060767 -0.00520944 -0.12554427]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 25 ] state=[-0.05239785 -0.00060767 -0.00520944 -0.12554427], action=0, reward=1.0, next_state=[-0.05241    -0.19565461 -0.00772032  0.1654906 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 26 ] state=[-0.05241    -0.19565461 -0.00772032  0.1654906 ], action=0, reward=1.0, next_state=[-0.0563231  -0.3906652  -0.00441051  0.45572801]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 27 ] state=[-0.0563231  -0.3906652  -0.00441051  0.45572801], action=0, reward=1.0, next_state=[-0.0641364  -0.58572451  0.00470405  0.74701745]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 28 ] state=[-0.0641364  -0.58572451  0.00470405  0.74701745], action=1, reward=1.0, next_state=[-0.07585089 -0.39066778  0.0196444   0.45581858]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 29 ] state=[-0.07585089 -0.39066778  0.0196444   0.45581858], action=1, reward=1.0, next_state=[-0.08366425 -0.195829    0.02876077  0.16939188]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 30 ] state=[-0.08366425 -0.195829    0.02876077  0.16939188], action=1, reward=1.0, next_state=[-0.08758083 -0.00113027  0.03214861 -0.11408093]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 31 ] state=[-0.08758083 -0.00113027  0.03214861 -0.11408093], action=1, reward=1.0, next_state=[-0.08760343  0.19351663  0.02986699 -0.39645036]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 32 ] state=[-0.08760343  0.19351663  0.02986699 -0.39645036], action=0, reward=1.0, next_state=[-0.0837331  -0.00201606  0.02193798 -0.09450253]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 352 ][ timestamp 33 ] state=[-0.0837331  -0.00201606  0.02193798 -0.09450253], action=0, reward=1.0, next_state=[-0.08377342 -0.19744546  0.02004793  0.20502028]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 34 ] state=[-0.08377342 -0.19744546  0.02004793  0.20502028], action=1, reward=1.0, next_state=[-0.08772233 -0.00261586  0.02414834 -0.08127171]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 35 ] state=[-0.08772233 -0.00261586  0.02414834 -0.08127171], action=0, reward=1.0, next_state=[-0.08777465 -0.1980755   0.0225229   0.2189312 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 36 ] state=[-0.08777465 -0.1980755   0.0225229   0.2189312 ], action=1, reward=1.0, next_state=[-0.09173616 -0.00328263  0.02690153 -0.06656283]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 37 ] state=[-0.09173616 -0.00328263  0.02690153 -0.06656283], action=0, reward=1.0, next_state=[-0.09180181 -0.19877974  0.02557027  0.23448482]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 38 ] state=[-0.09180181 -0.19877974  0.02557027  0.23448482], action=1, reward=1.0, next_state=[-0.0957774  -0.00403229  0.03025997 -0.05002402]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 39 ] state=[-0.0957774  -0.00403229  0.03025997 -0.05002402], action=0, reward=1.0, next_state=[-0.09585805 -0.19957477  0.02925949  0.25205051]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 40 ] state=[-0.09585805 -0.19957477  0.02925949  0.25205051], action=1, reward=1.0, next_state=[-0.09984955 -0.00488259  0.0343005  -0.03126171]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 41 ] state=[-0.09984955 -0.00488259  0.0343005  -0.03126171], action=1, reward=1.0, next_state=[-0.0999472   0.18973112  0.03367526 -0.31292819]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 42 ] state=[-0.0999472   0.18973112  0.03367526 -0.31292819], action=0, reward=1.0, next_state=[-0.09615258 -0.00585398  0.0274167  -0.00981822]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 43 ] state=[-0.09615258 -0.00585398  0.0274167  -0.00981822], action=1, reward=1.0, next_state=[-0.09626965  0.18886427  0.02722033 -0.29372633]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 44 ] state=[-0.09626965  0.18886427  0.02722033 -0.29372633], action=0, reward=1.0, next_state=[-0.09249237 -0.00663498  0.02134581  0.0074157 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 45 ] state=[-0.09249237 -0.00663498  0.02134581  0.0074157 ], action=0, reward=1.0, next_state=[-0.09262507 -0.20205646  0.02149412  0.30675633]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 46 ] state=[-0.09262507 -0.20205646  0.02149412  0.30675633], action=1, reward=1.0, next_state=[-0.0966662  -0.00724728  0.02762925  0.02092884]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 47 ] state=[-0.0966662  -0.00724728  0.02762925  0.02092884], action=1, reward=1.0, next_state=[-0.09681114  0.18746777  0.02804782 -0.26291029]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 48 ] state=[-0.09681114  0.18746777  0.02804782 -0.26291029], action=1, reward=1.0, next_state=[-0.09306179  0.38217837  0.02278962 -0.54661634]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 49 ] state=[-0.09306179  0.38217837  0.02278962 -0.54661634], action=0, reward=1.0, next_state=[-0.08541822  0.18674376  0.01185729 -0.24684099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 50 ] state=[-0.08541822  0.18674376  0.01185729 -0.24684099], action=0, reward=1.0, next_state=[-0.08168335 -0.00854551  0.00692047  0.04955829]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 51 ] state=[-0.08168335 -0.00854551  0.00692047  0.04955829], action=0, reward=1.0, next_state=[-0.08185426 -0.20376601  0.00791164  0.34441663]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 52 ] state=[-0.08185426 -0.20376601  0.00791164  0.34441663], action=1, reward=1.0, next_state=[-0.08592958 -0.00875749  0.01479997  0.05423901]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 53 ] state=[-0.08592958 -0.00875749  0.01479997  0.05423901], action=0, reward=1.0, next_state=[-0.08610473 -0.20408849  0.01588475  0.35155445]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 54 ] state=[-0.08610473 -0.20408849  0.01588475  0.35155445], action=1, reward=1.0, next_state=[-0.0901865  -0.009196    0.02291584  0.06392251]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 55 ] state=[-0.0901865  -0.009196    0.02291584  0.06392251], action=1, reward=1.0, next_state=[-0.09037042  0.18559003  0.02419429 -0.22144315]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 56 ] state=[-0.09037042  0.18559003  0.02419429 -0.22144315], action=0, reward=1.0, next_state=[-0.08665862 -0.00986924  0.01976543  0.07877237]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 57 ] state=[-0.08665862 -0.00986924  0.01976543  0.07877237], action=1, reward=1.0, next_state=[-0.086856    0.18496387  0.02134087 -0.20760955]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 58 ] state=[-0.086856    0.18496387  0.02134087 -0.20760955], action=0, reward=1.0, next_state=[-0.08315672 -0.01045664  0.01718868  0.09172821]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 59 ] state=[-0.08315672 -0.01045664  0.01718868  0.09172821], action=1, reward=1.0, next_state=[-0.08336586  0.18441478  0.01902325 -0.19548252]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 60 ] state=[-0.08336586  0.18441478  0.01902325 -0.19548252], action=0, reward=1.0, next_state=[-0.07967756 -0.01097404  0.0151136   0.10314019]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 61 ] state=[-0.07967756 -0.01097404  0.0151136   0.10314019], action=0, reward=1.0, next_state=[-0.07989704 -0.20630929  0.0171764   0.40055283]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 62 ] state=[-0.07989704 -0.20630929  0.0171764   0.40055283], action=1, reward=1.0, next_state=[-0.08402323 -0.01143514  0.02518746  0.11333434]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 63 ] state=[-0.08402323 -0.01143514  0.02518746  0.11333434], action=0, reward=1.0, next_state=[-0.08425193 -0.20690878  0.02745414  0.41385612]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 64 ] state=[-0.08425193 -0.20690878  0.02745414  0.41385612], action=1, reward=1.0, next_state=[-0.0883901  -0.01218651  0.03573127  0.12995329]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 65 ] state=[-0.0883901  -0.01218651  0.03573127  0.12995329], action=1, reward=1.0, next_state=[-0.08863383  0.18240585  0.03833033 -0.15124619]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 66 ] state=[-0.08863383  0.18240585  0.03833033 -0.15124619], action=1, reward=1.0, next_state=[-0.08498572  0.37695858  0.03530541 -0.43159457]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 67 ] state=[-0.08498572  0.37695858  0.03530541 -0.43159457], action=1, reward=1.0, next_state=[-0.07744655  0.57156329  0.02667352 -0.7129421 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 68 ] state=[-0.07744655  0.57156329  0.02667352 -0.7129421 ], action=0, reward=1.0, next_state=[-0.06601528  0.37608239  0.01241467 -0.41198396]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 69 ] state=[-0.06601528  0.37608239  0.01241467 -0.41198396], action=0, reward=1.0, next_state=[-0.05849363  0.18078667  0.004175   -0.11541313]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 70 ] state=[-0.05849363  0.18078667  0.004175   -0.11541313], action=0, reward=1.0, next_state=[-0.0548779  -0.01439486  0.00186673  0.17858405]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 71 ] state=[-0.0548779  -0.01439486  0.00186673  0.17858405], action=1, reward=1.0, next_state=[-0.0551658   0.18070033  0.00543841 -0.11350941]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 72 ] state=[-0.0551658   0.18070033  0.00543841 -0.11350941], action=1, reward=1.0, next_state=[-0.05155179  0.37574394  0.00316823 -0.4044716 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 73 ] state=[-0.05155179  0.37574394  0.00316823 -0.4044716 ], action=0, reward=1.0, next_state=[-0.04403691  0.1805772  -0.00492121 -0.1107915 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 74 ] state=[-0.04403691  0.1805772  -0.00492121 -0.1107915 ], action=0, reward=1.0, next_state=[-0.04042537 -0.01447389 -0.00713704  0.18033476]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 75 ] state=[-0.04042537 -0.01447389 -0.00713704  0.18033476], action=1, reward=1.0, next_state=[-0.04071484  0.18074946 -0.00353034 -0.11459108]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 76 ] state=[-0.04071484  0.18074946 -0.00353034 -0.11459108], action=0, reward=1.0, next_state=[-0.03709986 -0.01432173 -0.00582216  0.17697596]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 77 ] state=[-0.03709986 -0.01432173 -0.00582216  0.17697596], action=1, reward=1.0, next_state=[-0.03738629  0.18088306 -0.00228264 -0.11753796]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 352 ][ timestamp 78 ] state=[-0.03738629  0.18088306 -0.00228264 -0.11753796], action=0, reward=1.0, next_state=[-0.03376863 -0.01420612 -0.0046334   0.17442394]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 79 ] state=[-0.03376863 -0.01420612 -0.0046334   0.17442394], action=0, reward=1.0, next_state=[-0.03405275 -0.20926145 -0.00114492  0.46564158]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 80 ] state=[-0.03405275 -0.20926145 -0.00114492  0.46564158], action=0, reward=1.0, next_state=[-0.03823798 -0.4043672   0.00816791  0.75796341]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 81 ] state=[-0.03823798 -0.4043672   0.00816791  0.75796341], action=1, reward=1.0, next_state=[-0.04632532 -0.20935876  0.02332718  0.46786185]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 82 ] state=[-0.04632532 -0.20935876  0.02332718  0.46786185], action=1, reward=1.0, next_state=[-0.0505125  -0.01457401  0.03268441  0.18262177]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 83 ] state=[-0.0505125  -0.01457401  0.03268441  0.18262177], action=0, reward=1.0, next_state=[-0.05080398 -0.21014802  0.03633685  0.48543362]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 84 ] state=[-0.05080398 -0.21014802  0.03633685  0.48543362], action=1, reward=1.0, next_state=[-0.05500694 -0.01555718  0.04604552  0.20442071]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 85 ] state=[-0.05500694 -0.01555718  0.04604552  0.20442071], action=1, reward=1.0, next_state=[-0.05531808  0.17887708  0.05013394 -0.07338895]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 86 ] state=[-0.05531808  0.17887708  0.05013394 -0.07338895], action=1, reward=1.0, next_state=[-0.05174054  0.37324579  0.04866616 -0.34984249]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 87 ] state=[-0.05174054  0.37324579  0.04866616 -0.34984249], action=0, reward=1.0, next_state=[-0.04427563  0.17746671  0.04166931 -0.04221936]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 88 ] state=[-0.04427563  0.17746671  0.04166931 -0.04221936], action=1, reward=1.0, next_state=[-0.04072629  0.37196713  0.04082492 -0.3214695 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 89 ] state=[-0.04072629  0.37196713  0.04082492 -0.3214695 ], action=1, reward=1.0, next_state=[-0.03328695  0.56648465  0.03439553 -0.60100347]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 90 ] state=[-0.03328695  0.56648465  0.03439553 -0.60100347], action=0, reward=1.0, next_state=[-0.02195726  0.37089884  0.02237546 -0.297688  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 91 ] state=[-0.02195726  0.37089884  0.02237546 -0.297688  ], action=0, reward=1.0, next_state=[-0.01453928  0.1754652   0.0164217   0.00196692]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 92 ] state=[-0.01453928  0.1754652   0.0164217   0.00196692], action=0, reward=1.0, next_state=[-0.01102998 -0.01988837  0.01646104  0.29978552]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 93 ] state=[-0.01102998 -0.01988837  0.01646104  0.29978552], action=1, reward=1.0, next_state=[-0.01142774  0.17499513  0.02245675  0.01233921]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 94 ] state=[-0.01142774  0.17499513  0.02245675  0.01233921], action=0, reward=1.0, next_state=[-0.00792784 -0.02044156  0.02270353  0.31202206]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 95 ] state=[-0.00792784 -0.02044156  0.02270353  0.31202206], action=1, reward=1.0, next_state=[-0.00833667  0.17434971  0.02894397  0.02658472]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 96 ] state=[-0.00833667  0.17434971  0.02894397  0.02658472], action=1, reward=1.0, next_state=[-0.00484968  0.36904488  0.02947567 -0.25682738]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 97 ] state=[-0.00484968  0.36904488  0.02947567 -0.25682738], action=1, reward=1.0, next_state=[ 0.00253122  0.56373388  0.02433912 -0.54006943]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 98 ] state=[ 0.00253122  0.56373388  0.02433912 -0.54006943], action=1, reward=1.0, next_state=[ 0.0138059   0.75850539  0.01353773 -0.82498518]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 99 ] state=[ 0.0138059   0.75850539  0.01353773 -0.82498518], action=0, reward=1.0, next_state=[ 0.028976    0.56320092 -0.00296197 -0.52807534]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 100 ] state=[ 0.028976    0.56320092 -0.00296197 -0.52807534], action=0, reward=1.0, next_state=[ 0.04024002  0.36812077 -0.01352348 -0.23632722]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 101 ] state=[ 0.04024002  0.36812077 -0.01352348 -0.23632722], action=0, reward=1.0, next_state=[ 0.04760244  0.17319461 -0.01825002  0.0520595 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 102 ] state=[ 0.04760244  0.17319461 -0.01825002  0.0520595 ], action=0, reward=1.0, next_state=[ 0.05106633 -0.02166097 -0.01720883  0.33892899]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 103 ] state=[ 0.05106633 -0.02166097 -0.01720883  0.33892899], action=0, reward=1.0, next_state=[ 0.05063311 -0.21653387 -0.01043025  0.62613592]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 104 ] state=[ 0.05063311 -0.21653387 -0.01043025  0.62613592], action=0, reward=1.0, next_state=[ 0.04630243 -0.41150869  0.00209247  0.91551579]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 105 ] state=[ 0.04630243 -0.41150869  0.00209247  0.91551579], action=1, reward=1.0, next_state=[ 0.03807226 -0.2164151   0.02040278  0.62349123]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 106 ] state=[ 0.03807226 -0.2164151   0.02040278  0.62349123], action=1, reward=1.0, next_state=[ 0.03374396 -0.02158387  0.03287261  0.33730321]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 107 ] state=[ 0.03374396 -0.02158387  0.03287261  0.33730321], action=1, reward=1.0, next_state=[0.03331228 0.17305525 0.03961867 0.05516508]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 108 ] state=[0.03331228 0.17305525 0.03961867 0.05516508], action=1, reward=1.0, next_state=[ 0.03677339  0.36758739  0.04072197 -0.22475931]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 109 ] state=[ 0.03677339  0.36758739  0.04072197 -0.22475931], action=0, reward=1.0, next_state=[0.04412513 0.17190781 0.03622679 0.08048567]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 110 ] state=[0.04412513 0.17190781 0.03622679 0.08048567], action=0, reward=1.0, next_state=[ 0.04756329 -0.02371423  0.0378365   0.38437455]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 111 ] state=[ 0.04756329 -0.02371423  0.0378365   0.38437455], action=1, reward=1.0, next_state=[0.04708901 0.17085069 0.04552399 0.10385732]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 112 ] state=[0.04708901 0.17085069 0.04552399 0.10385732], action=1, reward=1.0, next_state=[ 0.05050602  0.36529168  0.04760114 -0.17412255]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 113 ] state=[ 0.05050602  0.36529168  0.04760114 -0.17412255], action=0, reward=1.0, next_state=[0.05781185 0.16952193 0.04411869 0.13318889]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 114 ] state=[0.05781185 0.16952193 0.04411869 0.13318889], action=1, reward=1.0, next_state=[ 0.06120229  0.36398506  0.04678246 -0.14525528]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 115 ] state=[ 0.06120229  0.36398506  0.04678246 -0.14525528], action=0, reward=1.0, next_state=[0.06848199 0.16822545 0.04387736 0.1618119 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 116 ] state=[0.06848199 0.16822545 0.04387736 0.1618119 ], action=0, reward=1.0, next_state=[ 0.0718465  -0.02749629  0.0471136   0.46800775]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 117 ] state=[ 0.0718465  -0.02749629  0.0471136   0.46800775], action=1, reward=1.0, next_state=[0.07129658 0.16692949 0.05647375 0.19053896]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 118 ] state=[0.07129658 0.16692949 0.05647375 0.19053896], action=1, reward=1.0, next_state=[ 0.07463517  0.36119996  0.06028453 -0.08380772]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 119 ] state=[ 0.07463517  0.36119996  0.06028453 -0.08380772], action=0, reward=1.0, next_state=[0.08185916 0.16526799 0.05860838 0.22726928]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 120 ] state=[0.08185916 0.16526799 0.05860838 0.22726928], action=1, reward=1.0, next_state=[ 0.08516452  0.35950552  0.06315376 -0.04636577]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 121 ] state=[ 0.08516452  0.35950552  0.06315376 -0.04636577], action=1, reward=1.0, next_state=[ 0.09235463  0.5536677   0.06222645 -0.31847358]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 352 ][ timestamp 122 ] state=[ 0.09235463  0.5536677   0.06222645 -0.31847358], action=0, reward=1.0, next_state=[ 0.10342799  0.35771724  0.05585697 -0.00683398]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 123 ] state=[ 0.10342799  0.35771724  0.05585697 -0.00683398], action=1, reward=1.0, next_state=[ 0.11058233  0.55199548  0.05572029 -0.28138344]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 124 ] state=[ 0.11058233  0.55199548  0.05572029 -0.28138344], action=1, reward=1.0, next_state=[ 0.12162224  0.74628018  0.05009263 -0.55598492]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 125 ] state=[ 0.12162224  0.74628018  0.05009263 -0.55598492], action=0, reward=1.0, next_state=[ 0.13654785  0.55049205  0.03897293 -0.24795003]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 126 ] state=[ 0.13654785  0.55049205  0.03897293 -0.24795003], action=0, reward=1.0, next_state=[0.14755769 0.35483581 0.03401393 0.05676662]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 127 ] state=[0.14755769 0.35483581 0.03401393 0.05676662], action=1, reward=1.0, next_state=[ 0.1546544   0.54945396  0.03514926 -0.22499359]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 128 ] state=[ 0.1546544   0.54945396  0.03514926 -0.22499359], action=1, reward=1.0, next_state=[ 0.16564348  0.74405639  0.03064939 -0.50638518]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 129 ] state=[ 0.16564348  0.74405639  0.03064939 -0.50638518], action=1, reward=1.0, next_state=[ 0.18052461  0.93873335  0.02052168 -0.78925396]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 130 ] state=[ 0.18052461  0.93873335  0.02052168 -0.78925396], action=0, reward=1.0, next_state=[ 0.19929928  0.74333566  0.0047366  -0.49018619]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 131 ] state=[ 0.19929928  0.74333566  0.0047366  -0.49018619], action=0, reward=1.0, next_state=[ 0.21416599  0.54814721 -0.00506712 -0.19601424]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 132 ] state=[ 0.21416599  0.54814721 -0.00506712 -0.19601424], action=0, reward=1.0, next_state=[ 0.22512894  0.3530981  -0.0089874   0.09506593]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 133 ] state=[ 0.22512894  0.3530981  -0.0089874   0.09506593], action=0, reward=1.0, next_state=[ 0.2321909   0.15810611 -0.00708609  0.38489985]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 134 ] state=[ 0.2321909   0.15810611 -0.00708609  0.38489985], action=1, reward=1.0, next_state=[0.23535302 0.35332794 0.00061191 0.08999116]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 135 ] state=[0.23535302 0.35332794 0.00061191 0.08999116], action=1, reward=1.0, next_state=[ 0.24241958  0.54844112  0.00241173 -0.20249864]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 136 ] state=[ 0.24241958  0.54844112  0.00241173 -0.20249864], action=0, reward=1.0, next_state=[ 0.2533884   0.35328476 -0.00163824  0.09094409]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 137 ] state=[ 0.2533884   0.35328476 -0.00163824  0.09094409], action=0, reward=1.0, next_state=[2.60454095e-01 1.58186324e-01 1.80643944e-04 3.83109707e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 138 ] state=[2.60454095e-01 1.58186324e-01 1.80643944e-04 3.83109707e-01], action=1, reward=1.0, next_state=[0.26361782 0.35330571 0.00784284 0.09048374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 139 ] state=[0.26361782 0.35330571 0.00784284 0.09048374], action=1, reward=1.0, next_state=[ 0.27068394  0.54831437  0.00965251 -0.19971449]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 140 ] state=[ 0.27068394  0.54831437  0.00965251 -0.19971449], action=1, reward=1.0, next_state=[ 0.28165022  0.74329695  0.00565822 -0.48933693]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 141 ] state=[ 0.28165022  0.74329695  0.00565822 -0.48933693], action=0, reward=1.0, next_state=[ 0.29651616  0.54809563 -0.00412852 -0.19487613]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 142 ] state=[ 0.29651616  0.54809563 -0.00412852 -0.19487613], action=1, reward=1.0, next_state=[ 0.30747808  0.74327639 -0.00802604 -0.48885856]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 143 ] state=[ 0.30747808  0.74327639 -0.00802604 -0.48885856], action=1, reward=1.0, next_state=[ 0.3223436   0.93851065 -0.01780321 -0.78406015]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 144 ] state=[ 0.3223436   0.93851065 -0.01780321 -0.78406015], action=0, reward=1.0, next_state=[ 0.34111382  0.74363781 -0.03348441 -0.49703108]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 145 ] state=[ 0.34111382  0.74363781 -0.03348441 -0.49703108], action=0, reward=1.0, next_state=[ 0.35598657  0.54900361 -0.04342503 -0.21508601]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 146 ] state=[ 0.35598657  0.54900361 -0.04342503 -0.21508601], action=0, reward=1.0, next_state=[ 0.36696664  0.35452852 -0.04772675  0.06358869]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 147 ] state=[ 0.36696664  0.35452852 -0.04772675  0.06358869], action=1, reward=1.0, next_state=[ 0.37405721  0.55030111 -0.04645498 -0.24376215]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 148 ] state=[ 0.37405721  0.55030111 -0.04645498 -0.24376215], action=1, reward=1.0, next_state=[ 0.38506324  0.74605475 -0.05133022 -0.55072869]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 149 ] state=[ 0.38506324  0.74605475 -0.05133022 -0.55072869], action=1, reward=1.0, next_state=[ 0.39998433  0.94185866 -0.0623448  -0.85913217]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 150 ] state=[ 0.39998433  0.94185866 -0.0623448  -0.85913217], action=0, reward=1.0, next_state=[ 0.41882151  0.74763881 -0.07952744 -0.58668589]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 151 ] state=[ 0.41882151  0.74763881 -0.07952744 -0.58668589], action=0, reward=1.0, next_state=[ 0.43377428  0.55371547 -0.09126116 -0.3200767 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 152 ] state=[ 0.43377428  0.55371547 -0.09126116 -0.3200767 ], action=0, reward=1.0, next_state=[ 0.44484859  0.3600037  -0.09766269 -0.05751176]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 153 ] state=[ 0.44484859  0.3600037  -0.09766269 -0.05751176], action=1, reward=1.0, next_state=[ 0.45204867  0.55638046 -0.09881293 -0.37934044]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 154 ] state=[ 0.45204867  0.55638046 -0.09881293 -0.37934044], action=0, reward=1.0, next_state=[ 0.46317627  0.36279043 -0.10639974 -0.11937565]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 155 ] state=[ 0.46317627  0.36279043 -0.10639974 -0.11937565], action=0, reward=1.0, next_state=[ 0.47043208  0.16934108 -0.10878725  0.13793487]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 156 ] state=[ 0.47043208  0.16934108 -0.10878725  0.13793487], action=1, reward=1.0, next_state=[ 0.4738189   0.36583942 -0.10602855 -0.18699064]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 157 ] state=[ 0.4738189   0.36583942 -0.10602855 -0.18699064], action=0, reward=1.0, next_state=[ 0.48113569  0.17238163 -0.10976836  0.07045241]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 158 ] state=[ 0.48113569  0.17238163 -0.10976836  0.07045241], action=0, reward=1.0, next_state=[ 0.48458333 -0.02100941 -0.10835932  0.32658595]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 159 ] state=[ 0.48458333 -0.02100941 -0.10835932  0.32658595], action=1, reward=1.0, next_state=[ 0.48416314  0.17547503 -0.1018276   0.00179256]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 160 ] state=[ 0.48416314  0.17547503 -0.1018276   0.00179256], action=0, reward=1.0, next_state=[ 0.48767264 -0.01805032 -0.10179175  0.2606913 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 161 ] state=[ 0.48767264 -0.01805032 -0.10179175  0.2606913 ], action=1, reward=1.0, next_state=[ 0.48731163  0.17836628 -0.09657792 -0.06228363]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 162 ] state=[ 0.48731163  0.17836628 -0.09657792 -0.06228363], action=0, reward=1.0, next_state=[ 0.49087896 -0.01524784 -0.09782359  0.19843438]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 163 ] state=[ 0.49087896 -0.01524784 -0.09782359  0.19843438], action=0, reward=1.0, next_state=[ 0.490574   -0.20884442 -0.0938549   0.45872661]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 164 ] state=[ 0.490574   -0.20884442 -0.0938549   0.45872661], action=0, reward=1.0, next_state=[ 0.48639711 -0.40252302 -0.08468037  0.72041304]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 352 ][ timestamp 165 ] state=[ 0.48639711 -0.40252302 -0.08468037  0.72041304], action=0, reward=1.0, next_state=[ 0.47834665 -0.59637768 -0.07027211  0.98528681]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 166 ] state=[ 0.47834665 -0.59637768 -0.07027211  0.98528681], action=0, reward=1.0, next_state=[ 0.4664191  -0.79049162 -0.05056638  1.25509609]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 167 ] state=[ 0.4664191  -0.79049162 -0.05056638  1.25509609], action=1, reward=1.0, next_state=[ 0.45060927 -0.59475997 -0.02546445  0.94701372]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 168 ] state=[ 0.45060927 -0.59475997 -0.02546445  0.94701372], action=1, reward=1.0, next_state=[ 0.43871407 -0.39930453 -0.00652418  0.64643987]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 169 ] state=[ 0.43871407 -0.39930453 -0.00652418  0.64643987], action=1, reward=1.0, next_state=[ 0.43072798 -0.20409229  0.00640462  0.35170964]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 170 ] state=[ 0.43072798 -0.20409229  0.00640462  0.35170964], action=1, reward=1.0, next_state=[ 0.42664613 -0.009062    0.01343881  0.06105315]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 171 ] state=[ 0.42664613 -0.009062    0.01343881  0.06105315], action=1, reward=1.0, next_state=[ 0.42646489  0.18586472  0.01465987 -0.22735963]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 172 ] state=[ 0.42646489  0.18586472  0.01465987 -0.22735963], action=0, reward=1.0, next_state=[ 0.43018218 -0.00946363  0.01011268  0.06991126]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 173 ] state=[ 0.43018218 -0.00946363  0.01011268  0.06991126], action=1, reward=1.0, next_state=[ 0.42999291  0.18551189  0.01151091 -0.21956398]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 174 ] state=[ 0.42999291  0.18551189  0.01151091 -0.21956398], action=0, reward=1.0, next_state=[ 0.43370315 -0.0097727   0.00711963  0.07672762]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 175 ] state=[ 0.43370315 -0.0097727   0.00711963  0.07672762], action=0, reward=1.0, next_state=[ 0.4335077  -0.20499599  0.00865418  0.37164828]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 176 ] state=[ 0.4335077  -0.20499599  0.00865418  0.37164828], action=0, reward=1.0, next_state=[ 0.42940778 -0.40023981  0.01608714  0.66704735]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 177 ] state=[ 0.42940778 -0.40023981  0.01608714  0.66704735], action=1, reward=1.0, next_state=[ 0.42140298 -0.20534524  0.02942809  0.37947274]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 178 ] state=[ 0.42140298 -0.20534524  0.02942809  0.37947274], action=1, reward=1.0, next_state=[ 0.41729607 -0.01065329  0.03701755  0.09621186]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 179 ] state=[ 0.41729607 -0.01065329  0.03701755  0.09621186], action=0, reward=1.0, next_state=[ 0.41708301 -0.2062857   0.03894178  0.40034012]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 180 ] state=[ 0.41708301 -0.2062857   0.03894178  0.40034012], action=1, reward=1.0, next_state=[ 0.41295729 -0.01173715  0.04694859  0.12018453]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 181 ] state=[ 0.41295729 -0.01173715  0.04694859  0.12018453], action=0, reward=1.0, next_state=[ 0.41272255 -0.20749922  0.04935228  0.42730188]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 182 ] state=[ 0.41272255 -0.20749922  0.04935228  0.42730188], action=1, reward=1.0, next_state=[ 0.40857257 -0.01310973  0.05789831  0.15057636]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 183 ] state=[ 0.40857257 -0.01310973  0.05789831  0.15057636], action=1, reward=1.0, next_state=[ 0.40831037  0.18113742  0.06090984 -0.12329353]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 184 ] state=[ 0.40831037  0.18113742  0.06090984 -0.12329353], action=0, reward=1.0, next_state=[ 0.41193312 -0.01480188  0.05844397  0.18796681]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 185 ] state=[ 0.41193312 -0.01480188  0.05844397  0.18796681], action=1, reward=1.0, next_state=[ 0.41163708  0.17943733  0.06220331 -0.08572181]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 186 ] state=[ 0.41163708  0.17943733  0.06220331 -0.08572181], action=0, reward=1.0, next_state=[ 0.41522583 -0.01651856  0.06048887  0.22591954]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 187 ] state=[ 0.41522583 -0.01651856  0.06048887  0.22591954], action=0, reward=1.0, next_state=[ 0.41489546 -0.21245049  0.06500726  0.53705281]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 188 ] state=[ 0.41489546 -0.21245049  0.06500726  0.53705281], action=1, reward=1.0, next_state=[ 0.41064645 -0.01829989  0.07574832  0.26554072]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 189 ] state=[ 0.41064645 -0.01829989  0.07574832  0.26554072], action=1, reward=1.0, next_state=[ 0.41028045  0.1756638   0.08105913 -0.0023218 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 190 ] state=[ 0.41028045  0.1756638   0.08105913 -0.0023218 ], action=1, reward=1.0, next_state=[ 0.41379373  0.36953533  0.0810127  -0.26836893]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 191 ] state=[ 0.41379373  0.36953533  0.0810127  -0.26836893], action=0, reward=1.0, next_state=[0.42118443 0.1733563  0.07564532 0.04872617]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 192 ] state=[0.42118443 0.1733563  0.07564532 0.04872617], action=1, reward=1.0, next_state=[ 0.42465156  0.36731664  0.07661984 -0.21916381]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 193 ] state=[ 0.42465156  0.36731664  0.07661984 -0.21916381], action=1, reward=1.0, next_state=[ 0.43199789  0.56126449  0.07223656 -0.48672785]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 194 ] state=[ 0.43199789  0.56126449  0.07223656 -0.48672785], action=0, reward=1.0, next_state=[ 0.44322318  0.36520157  0.06250201 -0.17218136]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 195 ] state=[ 0.44322318  0.36520157  0.06250201 -0.17218136], action=1, reward=1.0, next_state=[ 0.45052721  0.55937586  0.05905838 -0.44451045]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 196 ] state=[ 0.45052721  0.55937586  0.05905838 -0.44451045], action=0, reward=1.0, next_state=[ 0.46171473  0.36347021  0.05016817 -0.13381122]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 197 ] state=[ 0.46171473  0.36347021  0.05016817 -0.13381122], action=0, reward=1.0, next_state=[0.46898413 0.16766688 0.04749195 0.17426749]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 198 ] state=[0.46898413 0.16766688 0.04749195 0.17426749], action=1, reward=1.0, next_state=[ 0.47233747  0.3620781   0.0509773  -0.10306314]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 199 ] state=[ 0.47233747  0.3620781   0.0509773  -0.10306314], action=0, reward=1.0, next_state=[0.47957903 0.16626405 0.04891603 0.2052572 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 200 ] state=[0.47957903 0.16626405 0.04891603 0.2052572 ], action=1, reward=1.0, next_state=[ 0.48290432  0.3606536   0.05302118 -0.07160276]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 201 ] state=[ 0.48290432  0.3606536   0.05302118 -0.07160276], action=0, reward=1.0, next_state=[0.49011739 0.16481318 0.05158912 0.23732597]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 202 ] state=[0.49011739 0.16481318 0.05158912 0.23732597], action=1, reward=1.0, next_state=[ 0.49341365  0.3591616   0.05633564 -0.03864834]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 203 ] state=[ 0.49341365  0.3591616   0.05633564 -0.03864834], action=1, reward=1.0, next_state=[ 0.50059688  0.55343234  0.05556268 -0.31303823]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 204 ] state=[ 0.50059688  0.55343234  0.05556268 -0.31303823], action=0, reward=1.0, next_state=[ 0.51166553  0.35756469  0.04930191 -0.00336313]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 205 ] state=[ 0.51166553  0.35756469  0.04930191 -0.00336313], action=1, reward=1.0, next_state=[ 0.51881682  0.55194618  0.04923465 -0.28009219]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 206 ] state=[ 0.51881682  0.55194618  0.04923465 -0.28009219], action=1, reward=1.0, next_state=[ 0.52985575  0.74633249  0.0436328  -0.55684918]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 207 ] state=[ 0.52985575  0.74633249  0.0436328  -0.55684918], action=0, reward=1.0, next_state=[ 0.5447824   0.55062602  0.03249582 -0.25074489]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 352 ][ timestamp 208 ] state=[ 0.5447824   0.55062602  0.03249582 -0.25074489], action=1, reward=1.0, next_state=[ 0.55579492  0.74526922  0.02748092 -0.53300346]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 209 ] state=[ 0.55579492  0.74526922  0.02748092 -0.53300346], action=1, reward=1.0, next_state=[ 0.5707003   0.93999411  0.01682085 -0.81690214]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 210 ] state=[ 0.5707003   0.93999411  0.01682085 -0.81690214], action=0, reward=1.0, next_state=[ 5.89500184e-01  7.44645962e-01  4.82811145e-04 -5.18976276e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 211 ] state=[ 5.89500184e-01  7.44645962e-01  4.82811145e-04 -5.18976276e-01], action=0, reward=1.0, next_state=[ 0.6043931   0.54951722 -0.00989671 -0.22614125]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 212 ] state=[ 0.6043931   0.54951722 -0.00989671 -0.22614125], action=0, reward=1.0, next_state=[ 0.61538345  0.35453809 -0.01441954  0.06340353]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 213 ] state=[ 0.61538345  0.35453809 -0.01441954  0.06340353], action=1, reward=1.0, next_state=[ 0.62247421  0.54986378 -0.01315147 -0.23379375]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 214 ] state=[ 0.62247421  0.54986378 -0.01315147 -0.23379375], action=1, reward=1.0, next_state=[ 0.63347149  0.74517115 -0.01782734 -0.53059588]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 215 ] state=[ 0.63347149  0.74517115 -0.01782734 -0.53059588], action=1, reward=1.0, next_state=[ 0.64837491  0.94053928 -0.02843926 -0.82884247]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 216 ] state=[ 0.64837491  0.94053928 -0.02843926 -0.82884247], action=0, reward=1.0, next_state=[ 0.66718569  0.74581743 -0.04501611 -0.54523782]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 217 ] state=[ 0.66718569  0.74581743 -0.04501611 -0.54523782], action=1, reward=1.0, next_state=[ 0.68210204  0.94154206 -0.05592087 -0.85175761]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 218 ] state=[ 0.68210204  0.94154206 -0.05592087 -0.85175761], action=0, reward=1.0, next_state=[ 0.70093288  0.74722528 -0.07295602 -0.57717023]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 219 ] state=[ 0.70093288  0.74722528 -0.07295602 -0.57717023], action=0, reward=1.0, next_state=[ 0.71587739  0.55319765 -0.08449942 -0.30833304]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 220 ] state=[ 0.71587739  0.55319765 -0.08449942 -0.30833304], action=0, reward=1.0, next_state=[ 0.72694134  0.35937498 -0.09066608 -0.04344964]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 221 ] state=[ 0.72694134  0.35937498 -0.09066608 -0.04344964], action=1, reward=1.0, next_state=[ 0.73412884  0.55567219 -0.09153508 -0.36330538]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 222 ] state=[ 0.73412884  0.55567219 -0.09153508 -0.36330538], action=0, reward=1.0, next_state=[ 0.74524229  0.36196232 -0.09880119 -0.10083075]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 223 ] state=[ 0.74524229  0.36196232 -0.09880119 -0.10083075], action=1, reward=1.0, next_state=[ 0.75248153  0.55835123 -0.1008178  -0.42297778]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 224 ] state=[ 0.75248153  0.55835123 -0.1008178  -0.42297778], action=1, reward=1.0, next_state=[ 0.76364856  0.75474614 -0.10927736 -0.7456645 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 225 ] state=[ 0.76364856  0.75474614 -0.10927736 -0.7456645 ], action=0, reward=1.0, next_state=[ 0.77874348  0.56128814 -0.12419065 -0.48927207]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 226 ] state=[ 0.77874348  0.56128814 -0.12419065 -0.48927207], action=0, reward=1.0, next_state=[ 0.78996924  0.36811696 -0.13397609 -0.2381652 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 227 ] state=[ 0.78996924  0.36811696 -0.13397609 -0.2381652 ], action=0, reward=1.0, next_state=[ 0.79733158  0.17513807 -0.13873939  0.00943787]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 228 ] state=[ 0.79733158  0.17513807 -0.13873939  0.00943787], action=1, reward=1.0, next_state=[ 0.80083434  0.37194887 -0.13855063 -0.32360029]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 229 ] state=[ 0.80083434  0.37194887 -0.13855063 -0.32360029], action=1, reward=1.0, next_state=[ 0.80827332  0.56874375 -0.14502264 -0.65656753]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 230 ] state=[ 0.80827332  0.56874375 -0.14502264 -0.65656753], action=1, reward=1.0, next_state=[ 0.8196482   0.76555496 -0.15815399 -0.99117271]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 231 ] state=[ 0.8196482   0.76555496 -0.15815399 -0.99117271], action=1, reward=1.0, next_state=[ 0.83495929  0.96239922 -0.17797744 -1.32905776]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 232 ] state=[ 0.83495929  0.96239922 -0.17797744 -1.32905776], action=0, reward=1.0, next_state=[ 0.85420728  0.76991252 -0.2045586  -1.09693811]\n",
      "[ Experience replay ] starts\n",
      "[ episode 352 ][ timestamp 233 ] state=[ 0.85420728  0.76991252 -0.2045586  -1.09693811], action=0, reward=-1.0, next_state=[ 0.86960553  0.5779841  -0.22649736 -0.87476953]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 352: Exploration_rate=0.01. Score=233.\n",
      "[ episode 353 ] state=[-0.03551988  0.02725787  0.01127134  0.04140675]\n",
      "[ episode 353 ][ timestamp 1 ] state=[-0.03551988  0.02725787  0.01127134  0.04140675], action=0, reward=1.0, next_state=[-0.03497472 -0.16802388  0.01209947  0.33762447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 2 ] state=[-0.03497472 -0.16802388  0.01209947  0.33762447], action=1, reward=1.0, next_state=[-0.0383352   0.02692382  0.01885196  0.04878148]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 3 ] state=[-0.0383352   0.02692382  0.01885196  0.04878148], action=0, reward=1.0, next_state=[-0.03779673 -0.16846331  0.01982759  0.34735224]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 4 ] state=[-0.03779673 -0.16846331  0.01982759  0.34735224], action=1, reward=1.0, next_state=[-0.04116599  0.02637109  0.02677464  0.06098702]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 5 ] state=[-0.04116599  0.02637109  0.02677464  0.06098702], action=0, reward=1.0, next_state=[-0.04063857 -0.16912432  0.02799438  0.36199583]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 6 ] state=[-0.04063857 -0.16912432  0.02799438  0.36199583], action=0, reward=1.0, next_state=[-0.04402106 -0.36463275  0.0352343   0.66337285]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 7 ] state=[-0.04402106 -0.36463275  0.0352343   0.66337285], action=1, reward=1.0, next_state=[-0.05131371 -0.17001824  0.04850175  0.38198901]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 8 ] state=[-0.05131371 -0.17001824  0.04850175  0.38198901], action=1, reward=1.0, next_state=[-0.05471408  0.02438268  0.05614153  0.10498447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 9 ] state=[-0.05471408  0.02438268  0.05614153  0.10498447], action=1, reward=1.0, next_state=[-0.05422642  0.21865702  0.05824122 -0.16947097]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 10 ] state=[-0.05422642  0.21865702  0.05824122 -0.16947097], action=1, reward=1.0, next_state=[-0.04985328  0.41289907  0.0548518  -0.44322678]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 11 ] state=[-0.04985328  0.41289907  0.0548518  -0.44322678], action=1, reward=1.0, next_state=[-0.0415953   0.60720371  0.04598727 -0.71812706]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 12 ] state=[-0.0415953   0.60720371  0.04598727 -0.71812706], action=0, reward=1.0, next_state=[-0.02945123  0.41147656  0.03162473 -0.41133124]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 13 ] state=[-0.02945123  0.41147656  0.03162473 -0.41133124], action=0, reward=1.0, next_state=[-0.02122169  0.2159209   0.0233981  -0.10884829]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 14 ] state=[-0.02122169  0.2159209   0.0233981  -0.10884829], action=1, reward=1.0, next_state=[-0.01690328  0.41069988  0.02122114 -0.39405837]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 15 ] state=[-0.01690328  0.41069988  0.02122114 -0.39405837], action=1, reward=1.0, next_state=[-0.00868928  0.60551437  0.01333997 -0.67997577]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 16 ] state=[-0.00868928  0.60551437  0.01333997 -0.67997577], action=0, reward=1.0, next_state=[ 3.42100794e-03  4.10209689e-01 -2.59547389e-04 -3.83122982e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 17 ] state=[ 3.42100794e-03  4.10209689e-01 -2.59547389e-04 -3.83122982e-01], action=0, reward=1.0, next_state=[ 0.0116252   0.21509142 -0.00792201 -0.0905219 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 353 ][ timestamp 18 ] state=[ 0.0116252   0.21509142 -0.00792201 -0.0905219 ], action=1, reward=1.0, next_state=[ 0.01592703  0.41032602 -0.00973245 -0.38569366]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 19 ] state=[ 0.01592703  0.41032602 -0.00973245 -0.38569366], action=1, reward=1.0, next_state=[ 0.02413355  0.60558478 -0.01744632 -0.68142921]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 20 ] state=[ 0.02413355  0.60558478 -0.01744632 -0.68142921], action=0, reward=1.0, next_state=[ 0.03624525  0.41070942 -0.0310749  -0.39428961]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 21 ] state=[ 0.03624525  0.41070942 -0.0310749  -0.39428961], action=0, reward=1.0, next_state=[ 0.04445943  0.21604188 -0.03896069 -0.11156383]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 22 ] state=[ 0.04445943  0.21604188 -0.03896069 -0.11156383], action=1, reward=1.0, next_state=[ 0.04878027  0.41169982 -0.04119197 -0.41627957]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 23 ] state=[ 0.04878027  0.41169982 -0.04119197 -0.41627957], action=0, reward=1.0, next_state=[ 0.05701427  0.21718513 -0.04951756 -0.13686205]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 24 ] state=[ 0.05701427  0.21718513 -0.04951756 -0.13686205], action=0, reward=1.0, next_state=[ 0.06135797  0.02280612 -0.0522548   0.13979687]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 25 ] state=[ 0.06135797  0.02280612 -0.0522548   0.13979687], action=0, reward=1.0, next_state=[ 0.06181409 -0.17152999 -0.04945887  0.41554721]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 26 ] state=[ 0.06181409 -0.17152999 -0.04945887  0.41554721], action=1, reward=1.0, next_state=[ 0.05838349  0.02425677 -0.04114792  0.10769121]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 27 ] state=[ 0.05838349  0.02425677 -0.04114792  0.10769121], action=1, reward=1.0, next_state=[ 0.05886863  0.21994348 -0.0389941  -0.19768447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 28 ] state=[ 0.05886863  0.21994348 -0.0389941  -0.19768447], action=1, reward=1.0, next_state=[ 0.0632675   0.41560086 -0.04294779 -0.50240879]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 29 ] state=[ 0.0632675   0.41560086 -0.04294779 -0.50240879], action=1, reward=1.0, next_state=[ 0.07157952  0.61130102 -0.05299596 -0.80831111]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 30 ] state=[ 0.07157952  0.61130102 -0.05299596 -0.80831111], action=1, reward=1.0, next_state=[ 0.08380554  0.80710762 -0.06916219 -1.11718218]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 31 ] state=[ 0.08380554  0.80710762 -0.06916219 -1.11718218], action=0, reward=1.0, next_state=[ 0.09994769  0.61295809 -0.09150583 -0.8469716 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 32 ] state=[ 0.09994769  0.61295809 -0.09150583 -0.8469716 ], action=0, reward=1.0, next_state=[ 0.11220685  0.41919558 -0.10844526 -0.58440901]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 33 ] state=[ 0.11220685  0.41919558 -0.10844526 -0.58440901], action=0, reward=1.0, next_state=[ 0.12059076  0.22574648 -0.12013344 -0.32776041]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 34 ] state=[ 0.12059076  0.22574648 -0.12013344 -0.32776041], action=0, reward=1.0, next_state=[ 0.12510569  0.0325214  -0.12668865 -0.07524608]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 35 ] state=[ 0.12510569  0.0325214  -0.12668865 -0.07524608], action=0, reward=1.0, next_state=[ 0.12575612 -0.16057817 -0.12819357  0.17493502]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 36 ] state=[ 0.12575612 -0.16057817 -0.12819357  0.17493502], action=0, reward=1.0, next_state=[ 0.12254456 -0.35365463 -0.12469487  0.4245875 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 37 ] state=[ 0.12254456 -0.35365463 -0.12469487  0.4245875 ], action=0, reward=1.0, next_state=[ 0.11547146 -0.54681001 -0.11620312  0.67550562]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 38 ] state=[ 0.11547146 -0.54681001 -0.11620312  0.67550562], action=0, reward=1.0, next_state=[ 0.10453526 -0.74014185 -0.10269301  0.92946077]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 39 ] state=[ 0.10453526 -0.74014185 -0.10269301  0.92946077], action=0, reward=1.0, next_state=[ 0.08973243 -0.9337389  -0.08410379  1.18818775]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 40 ] state=[ 0.08973243 -0.9337389  -0.08410379  1.18818775], action=0, reward=1.0, next_state=[ 0.07105765 -1.12767593 -0.06034004  1.45336766]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 41 ] state=[ 0.07105765 -1.12767593 -0.06034004  1.45336766], action=1, reward=1.0, next_state=[ 0.04850413 -0.93186708 -0.03127268  1.14245972]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 42 ] state=[ 0.04850413 -0.93186708 -0.03127268  1.14245972], action=1, reward=1.0, next_state=[ 0.02986679 -0.73635073 -0.00842349  0.84013592]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 43 ] state=[ 0.02986679 -0.73635073 -0.00842349  0.84013592], action=0, reward=1.0, next_state=[ 0.01513977 -0.93135667  0.00837923  1.13015797]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 44 ] state=[ 0.01513977 -0.93135667  0.00837923  1.13015797], action=0, reward=1.0, next_state=[-0.00348736 -1.12658734  0.03098239  1.42545717]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 45 ] state=[-0.00348736 -1.12658734  0.03098239  1.42545717], action=0, reward=1.0, next_state=[-0.02601911 -1.32207821  0.05949153  1.7276601 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 46 ] state=[-0.02601911 -1.32207821  0.05949153  1.7276601 ], action=1, reward=1.0, next_state=[-0.05246067 -1.12768459  0.09404473  1.45406571]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 47 ] state=[-0.05246067 -1.12768459  0.09404473  1.45406571], action=1, reward=1.0, next_state=[-0.07501436 -0.93383481  0.12312605  1.19218438]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 48 ] state=[-0.07501436 -0.93383481  0.12312605  1.19218438], action=1, reward=1.0, next_state=[-0.09369106 -0.74050395  0.14696974  0.94049115]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 49 ] state=[-0.09369106 -0.74050395  0.14696974  0.94049115], action=0, reward=1.0, next_state=[-0.10850114 -0.93726832  0.16577956  1.27550956]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 50 ] state=[-0.10850114 -0.93726832  0.16577956  1.27550956], action=0, reward=1.0, next_state=[-0.1272465  -1.13407039  0.19128975  1.61518169]\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ][ timestamp 51 ] state=[-0.1272465  -1.13407039  0.19128975  1.61518169], action=1, reward=-1.0, next_state=[-0.14992791 -0.94165042  0.22359338  1.38771322]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 353: Exploration_rate=0.01. Score=51.\n",
      "[ episode 354 ] state=[ 0.0185754   0.01235829 -0.0303771   0.00765918]\n",
      "[ episode 354 ][ timestamp 1 ] state=[ 0.0185754   0.01235829 -0.0303771   0.00765918], action=1, reward=1.0, next_state=[ 0.01882256  0.20790242 -0.03022391 -0.29445119]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 2 ] state=[ 0.01882256  0.20790242 -0.03022391 -0.29445119], action=1, reward=1.0, next_state=[ 0.02298061  0.40344194 -0.03611294 -0.59651099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 3 ] state=[ 0.02298061  0.40344194 -0.03611294 -0.59651099], action=0, reward=1.0, next_state=[ 0.03104945  0.20884348 -0.04804316 -0.31541851]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 4 ] state=[ 0.03104945  0.20884348 -0.04804316 -0.31541851], action=0, reward=1.0, next_state=[ 0.03522632  0.01443763 -0.05435153 -0.03826546]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 5 ] state=[ 0.03522632  0.01443763 -0.05435153 -0.03826546], action=0, reward=1.0, next_state=[ 0.03551507 -0.1798645  -0.05511684  0.23678586]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 6 ] state=[ 0.03551507 -0.1798645  -0.05511684  0.23678586], action=0, reward=1.0, next_state=[ 0.03191778 -0.37415746 -0.05038112  0.5115866 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 7 ] state=[ 0.03191778 -0.37415746 -0.05038112  0.5115866 ], action=1, reward=1.0, next_state=[ 0.02443463 -0.17836339 -0.04014939  0.20346235]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 354 ][ timestamp 8 ] state=[ 0.02443463 -0.17836339 -0.04014939  0.20346235], action=0, reward=1.0, next_state=[ 0.02086737 -0.37288884 -0.03608014  0.48321464]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 9 ] state=[ 0.02086737 -0.37288884 -0.03608014  0.48321464], action=0, reward=1.0, next_state=[ 0.01340959 -0.56748349 -0.02641585  0.76431139]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 10 ] state=[ 0.01340959 -0.56748349 -0.02641585  0.76431139], action=1, reward=1.0, next_state=[ 0.00205992 -0.37200791 -0.01112962  0.46343495]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 11 ] state=[ 0.00205992 -0.37200791 -0.01112962  0.46343495], action=1, reward=1.0, next_state=[-0.00538024 -0.17673046 -0.00186092  0.16726487]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 12 ] state=[-0.00538024 -0.17673046 -0.00186092  0.16726487], action=1, reward=1.0, next_state=[-0.00891485  0.01841808  0.00148438 -0.12600454]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 13 ] state=[-0.00891485  0.01841808  0.00148438 -0.12600454], action=1, reward=1.0, next_state=[-0.00854649  0.21351874 -0.00103571 -0.41821879]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 14 ] state=[-0.00854649  0.21351874 -0.00103571 -0.41821879], action=1, reward=1.0, next_state=[-0.00427611  0.40865535 -0.00940009 -0.71122805]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 15 ] state=[-0.00427611  0.40865535 -0.00940009 -0.71122805], action=0, reward=1.0, next_state=[ 0.003897    0.21366482 -0.02362465 -0.42151877]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 16 ] state=[ 0.003897    0.21366482 -0.02362465 -0.42151877], action=0, reward=1.0, next_state=[ 0.00817029  0.01888542 -0.03205503 -0.13637619]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 17 ] state=[ 0.00817029  0.01888542 -0.03205503 -0.13637619], action=0, reward=1.0, next_state=[ 0.008548   -0.17576308 -0.03478255  0.14602401]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 18 ] state=[ 0.008548   -0.17576308 -0.03478255  0.14602401], action=0, reward=1.0, next_state=[ 0.00503274 -0.37037011 -0.03186207  0.42753397]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 19 ] state=[ 0.00503274 -0.37037011 -0.03186207  0.42753397], action=0, reward=1.0, next_state=[-0.00237466 -0.56502665 -0.02331139  0.71000472]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 20 ] state=[-0.00237466 -0.56502665 -0.02331139  0.71000472], action=1, reward=1.0, next_state=[-0.0136752  -0.36958973 -0.0091113   0.41007606]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 21 ] state=[-0.0136752  -0.36958973 -0.0091113   0.41007606], action=0, reward=1.0, next_state=[-0.02106699 -0.56458133 -0.00090977  0.69987264]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 22 ] state=[-0.02106699 -0.56458133 -0.00090977  0.69987264], action=1, reward=1.0, next_state=[-0.03235862 -0.36944678  0.01308768  0.40690346]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 23 ] state=[-0.03235862 -0.36944678  0.01308768  0.40690346], action=1, reward=1.0, next_state=[-0.03974755 -0.17451283  0.02122575  0.11837525]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 24 ] state=[-0.03974755 -0.17451283  0.02122575  0.11837525], action=1, reward=1.0, next_state=[-0.04323781  0.02029867  0.02359325 -0.16753628]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 25 ] state=[-0.04323781  0.02029867  0.02359325 -0.16753628], action=0, reward=1.0, next_state=[-0.04283184 -0.17515291  0.02024253  0.13249527]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 26 ] state=[-0.04283184 -0.17515291  0.02024253  0.13249527], action=1, reward=1.0, next_state=[-0.04633489  0.01967332  0.02289243 -0.15373331]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 27 ] state=[-0.04633489  0.01967332  0.02289243 -0.15373331], action=1, reward=1.0, next_state=[-0.04594143  0.21446013  0.01981777 -0.43910718]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 28 ] state=[-0.04594143  0.21446013  0.01981777 -0.43910718], action=0, reward=1.0, next_state=[-0.04165223  0.01906338  0.01103562 -0.14024357]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 29 ] state=[-0.04165223  0.01906338  0.01103562 -0.14024357], action=1, reward=1.0, next_state=[-0.04127096  0.21402556  0.00823075 -0.42942461]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 30 ] state=[-0.04127096  0.21402556  0.00823075 -0.42942461], action=0, reward=1.0, next_state=[-0.03699045  0.01878802 -0.00035774 -0.13415841]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 31 ] state=[-0.03699045  0.01878802 -0.00035774 -0.13415841], action=0, reward=1.0, next_state=[-0.03661469 -0.17632881 -0.00304091  0.15841163]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 32 ] state=[-0.03661469 -0.17632881 -0.00304091  0.15841163], action=1, reward=1.0, next_state=[-4.01412625e-02  1.88365480e-02  1.27322668e-04 -1.35229075e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 33 ] state=[-4.01412625e-02  1.88365480e-02  1.27322668e-04 -1.35229075e-01], action=1, reward=1.0, next_state=[-0.03976453  0.21395668 -0.00257726 -0.42787183]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 34 ] state=[-0.03976453  0.21395668 -0.00257726 -0.42787183], action=0, reward=1.0, next_state=[-0.0354854   0.01887132 -0.0111347  -0.13600248]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 35 ] state=[-0.0354854   0.01887132 -0.0111347  -0.13600248], action=0, reward=1.0, next_state=[-0.03510797 -0.17608939 -0.01385475  0.15314692]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 36 ] state=[-0.03510797 -0.17608939 -0.01385475  0.15314692], action=1, reward=1.0, next_state=[-0.03862976  0.01922817 -0.01079181 -0.14387447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 37 ] state=[-0.03862976  0.01922817 -0.01079181 -0.14387447], action=0, reward=1.0, next_state=[-0.0382452  -0.17573758 -0.0136693   0.14538439]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 38 ] state=[-0.0382452  -0.17573758 -0.0136693   0.14538439], action=0, reward=1.0, next_state=[-0.04175995 -0.37066113 -0.01076161  0.43372376]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 39 ] state=[-0.04175995 -0.37066113 -0.01076161  0.43372376], action=1, reward=1.0, next_state=[-0.04917317 -0.17538848 -0.00208713  0.1376679 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 40 ] state=[-0.04917317 -0.17538848 -0.00208713  0.1376679 ], action=1, reward=1.0, next_state=[-0.05268094  0.0197633   0.00066622 -0.15567275]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 41 ] state=[-0.05268094  0.0197633   0.00066622 -0.15567275], action=0, reward=1.0, next_state=[-0.05228567 -0.17536818 -0.00244723  0.13722028]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 42 ] state=[-0.05228567 -0.17536818 -0.00244723  0.13722028], action=1, reward=1.0, next_state=[-0.05579304  0.01978873  0.00029718 -0.15623371]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 43 ] state=[-0.05579304  0.01978873  0.00029718 -0.15623371], action=1, reward=1.0, next_state=[-0.05539726  0.21490643 -0.0028275  -0.44882287]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 44 ] state=[-0.05539726  0.21490643 -0.0028275  -0.44882287], action=1, reward=1.0, next_state=[-0.05109913  0.41006826 -0.01180396 -0.74239573]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 45 ] state=[-0.05109913  0.41006826 -0.01180396 -0.74239573], action=0, reward=1.0, next_state=[-0.04289777  0.21511122 -0.02665187 -0.45345082]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 46 ] state=[-0.04289777  0.21511122 -0.02665187 -0.45345082], action=0, reward=1.0, next_state=[-0.03859554  0.02037609 -0.03572089 -0.16928658]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 47 ] state=[-0.03859554  0.02037609 -0.03572089 -0.16928658], action=0, reward=1.0, next_state=[-0.03818802 -0.17421685 -0.03910662  0.11191691]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 48 ] state=[-0.03818802 -0.17421685 -0.03910662  0.11191691], action=0, reward=1.0, next_state=[-0.04167236 -0.36875724 -0.03686828  0.39200998]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 49 ] state=[-0.04167236 -0.36875724 -0.03686828  0.39200998], action=1, reward=1.0, next_state=[-0.0490475  -0.173132   -0.02902808  0.0879347 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 50 ] state=[-0.0490475  -0.173132   -0.02902808  0.0879347 ], action=1, reward=1.0, next_state=[-0.05251014  0.02239375 -0.02726939 -0.21376342]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 51 ] state=[-0.05251014  0.02239375 -0.02726939 -0.21376342], action=0, reward=1.0, next_state=[-0.05206227 -0.17232793 -0.03154466  0.07019431]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 354 ][ timestamp 52 ] state=[-0.05206227 -0.17232793 -0.03154466  0.07019431], action=1, reward=1.0, next_state=[-0.05550883  0.02323173 -0.03014077 -0.23227184]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 53 ] state=[-0.05550883  0.02323173 -0.03014077 -0.23227184], action=0, reward=1.0, next_state=[-0.05504419 -0.17144687 -0.03478621  0.05075338]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 54 ] state=[-0.05504419 -0.17144687 -0.03478621  0.05075338], action=1, reward=1.0, next_state=[-0.05847313  0.02415616 -0.03377114 -0.25269875]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 55 ] state=[-0.05847313  0.02415616 -0.03377114 -0.25269875], action=1, reward=1.0, next_state=[-0.05799001  0.21974365 -0.03882511 -0.55583953]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 56 ] state=[-0.05799001  0.21974365 -0.03882511 -0.55583953], action=1, reward=1.0, next_state=[-0.05359513  0.41538859 -0.0499419  -0.86049749]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 57 ] state=[-0.05359513  0.41538859 -0.0499419  -0.86049749], action=1, reward=1.0, next_state=[-0.04528736  0.61115383 -0.06715185 -1.16845604]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 58 ] state=[-0.04528736  0.61115383 -0.06715185 -1.16845604], action=0, reward=1.0, next_state=[-0.03306429  0.41696664 -0.09052097 -0.89755956]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 59 ] state=[-0.03306429  0.41696664 -0.09052097 -0.89755956], action=0, reward=1.0, next_state=[-0.02472495  0.22318063 -0.10847217 -0.6346475 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 60 ] state=[-0.02472495  0.22318063 -0.10847217 -0.6346475 ], action=0, reward=1.0, next_state=[-0.02026134  0.02972551 -0.12116512 -0.37799864]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 61 ] state=[-0.02026134  0.02972551 -0.12116512 -0.37799864], action=0, reward=1.0, next_state=[-0.01966683 -0.16348602 -0.12872509 -0.12584158]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 62 ] state=[-0.01966683 -0.16348602 -0.12872509 -0.12584158], action=0, reward=1.0, next_state=[-0.02293655 -0.35655138 -0.13124192  0.12361968]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 63 ] state=[-0.02293655 -0.35655138 -0.13124192  0.12361968], action=0, reward=1.0, next_state=[-0.03006758 -0.54957275 -0.12876953  0.37218734]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 64 ] state=[-0.03006758 -0.54957275 -0.12876953  0.37218734], action=0, reward=1.0, next_state=[-0.04105903 -0.74265269 -0.12132578  0.62165568]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 65 ] state=[-0.04105903 -0.74265269 -0.12132578  0.62165568], action=0, reward=1.0, next_state=[-0.05591209 -0.93589019 -0.10889267  0.87379889]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 66 ] state=[-0.05591209 -0.93589019 -0.10889267  0.87379889], action=0, reward=1.0, next_state=[-0.07462989 -1.12937652 -0.09141669  1.13035816]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 67 ] state=[-0.07462989 -1.12937652 -0.09141669  1.13035816], action=1, reward=1.0, next_state=[-0.09721742 -0.93318425 -0.06880953  0.81045949]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 68 ] state=[-0.09721742 -0.93318425 -0.06880953  0.81045949], action=1, reward=1.0, next_state=[-0.11588111 -0.73719043 -0.05260034  0.49695044]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 69 ] state=[-0.11588111 -0.73719043 -0.05260034  0.49695044], action=0, reward=1.0, next_state=[-0.13062492 -0.93153277 -0.04266133  0.77260339]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 70 ] state=[-0.13062492 -0.93153277 -0.04266133  0.77260339], action=1, reward=1.0, next_state=[-0.14925557 -0.73585061 -0.02720926  0.46680859]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 71 ] state=[-0.14925557 -0.73585061 -0.02720926  0.46680859], action=1, reward=1.0, next_state=[-0.16397258 -0.540355   -0.01787309  0.16567519]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 72 ] state=[-0.16397258 -0.540355   -0.01787309  0.16567519], action=1, reward=1.0, next_state=[-0.17477968 -0.34498183 -0.01455958 -0.13259218]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 73 ] state=[-0.17477968 -0.34498183 -0.01455958 -0.13259218], action=0, reward=1.0, next_state=[-0.18167932 -0.53989223 -0.01721143  0.15546207]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 74 ] state=[-0.18167932 -0.53989223 -0.01721143  0.15546207], action=1, reward=1.0, next_state=[-0.19247716 -0.34452813 -0.01410219 -0.14260059]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 75 ] state=[-0.19247716 -0.34452813 -0.01410219 -0.14260059], action=1, reward=1.0, next_state=[-0.19936773 -0.14920708 -0.0169542  -0.43969893]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 76 ] state=[-0.19936773 -0.14920708 -0.0169542  -0.43969893], action=0, reward=1.0, next_state=[-0.20235187 -0.34408503 -0.02574818 -0.15240831]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 77 ] state=[-0.20235187 -0.34408503 -0.02574818 -0.15240831], action=0, reward=1.0, next_state=[-0.20923357 -0.53882902 -0.02879634  0.13204171]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 78 ] state=[-0.20923357 -0.53882902 -0.02879634  0.13204171], action=0, reward=1.0, next_state=[-0.22001015 -0.73352689 -0.02615551  0.41550249]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 79 ] state=[-0.22001015 -0.73352689 -0.02615551  0.41550249], action=1, reward=1.0, next_state=[-0.23468069 -0.53804419 -0.01784546  0.11468989]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 80 ] state=[-0.23468069 -0.53804419 -0.01784546  0.11468989], action=1, reward=1.0, next_state=[-0.24544157 -0.34267115 -0.01555166 -0.1835693 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 81 ] state=[-0.24544157 -0.34267115 -0.01555166 -0.1835693 ], action=0, reward=1.0, next_state=[-0.25229499 -0.53756716 -0.01922305  0.10416737]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 82 ] state=[-0.25229499 -0.53756716 -0.01922305  0.10416737], action=0, reward=1.0, next_state=[-0.26304634 -0.73240843 -0.0171397   0.39072405]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 83 ] state=[-0.26304634 -0.73240843 -0.0171397   0.39072405], action=0, reward=1.0, next_state=[-0.27769451 -0.92728299 -0.00932522  0.67795412]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 84 ] state=[-0.27769451 -0.92728299 -0.00932522  0.67795412], action=1, reward=1.0, next_state=[-0.29624017 -0.73203273  0.00423386  0.3823499 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 85 ] state=[-0.29624017 -0.73203273  0.00423386  0.3823499 ], action=1, reward=1.0, next_state=[-0.31088082 -0.53697115  0.01188086  0.0910049 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 86 ] state=[-0.31088082 -0.53697115  0.01188086  0.0910049 ], action=1, reward=1.0, next_state=[-0.32162024 -0.34202149  0.01370096 -0.19790606]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 87 ] state=[-0.32162024 -0.34202149  0.01370096 -0.19790606], action=0, reward=1.0, next_state=[-0.32846067 -0.5373367   0.00974284  0.09906722]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 88 ] state=[-0.32846067 -0.5373367   0.00974284  0.09906722], action=1, reward=1.0, next_state=[-0.33920741 -0.34235573  0.01172418 -0.19052601]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 89 ] state=[-0.33920741 -0.34235573  0.01172418 -0.19052601], action=1, reward=1.0, next_state=[-0.34605452 -0.14740345  0.00791366 -0.4794875 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 90 ] state=[-0.34605452 -0.14740345  0.00791366 -0.4794875 ], action=0, reward=1.0, next_state=[-0.34900259 -0.34263622 -0.00167609 -0.18432092]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 91 ] state=[-0.34900259 -0.34263622 -0.00167609 -0.18432092], action=0, reward=1.0, next_state=[-0.35585531 -0.53773415 -0.00536251  0.10783279]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 92 ] state=[-0.35585531 -0.53773415 -0.00536251  0.10783279], action=1, reward=1.0, next_state=[-0.36661    -0.34253576 -0.00320585 -0.18653715]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 93 ] state=[-0.36661    -0.34253576 -0.00320585 -0.18653715], action=0, reward=1.0, next_state=[-0.37346071 -0.5376117  -0.00693659  0.10513274]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 94 ] state=[-0.37346071 -0.5376117  -0.00693659  0.10513274], action=1, reward=1.0, next_state=[-0.38421295 -0.34239103 -0.00483394 -0.18973056]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 95 ] state=[-0.38421295 -0.34239103 -0.00483394 -0.18973056], action=0, reward=1.0, next_state=[-0.39106077 -0.5374435  -0.00862855  0.10142354]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 96 ] state=[-0.39106077 -0.5374435  -0.00862855  0.10142354], action=1, reward=1.0, next_state=[-0.40180964 -0.34219896 -0.00660008 -0.19396913]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 97 ] state=[-0.40180964 -0.34219896 -0.00660008 -0.19396913], action=0, reward=1.0, next_state=[-0.40865362 -0.53722588 -0.01047946  0.09662447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 98 ] state=[-0.40865362 -0.53722588 -0.01047946  0.09662447], action=1, reward=1.0, next_state=[-0.41939813 -0.34195531 -0.00854697 -0.1993462 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 354 ][ timestamp 99 ] state=[-0.41939813 -0.34195531 -0.00854697 -0.1993462 ], action=0, reward=1.0, next_state=[-0.42623724 -0.53695398 -0.0125339   0.09062834]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 100 ] state=[-0.42623724 -0.53695398 -0.0125339   0.09062834], action=1, reward=1.0, next_state=[-0.43697632 -0.34165464 -0.01072133 -0.20598253]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 101 ] state=[-0.43697632 -0.34165464 -0.01072133 -0.20598253], action=0, reward=1.0, next_state=[-0.44380941 -0.53662165 -0.01484098  0.08329917]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 102 ] state=[-0.44380941 -0.53662165 -0.01484098  0.08329917], action=1, reward=1.0, next_state=[-0.45454185 -0.34129013 -0.013175   -0.21402892]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 103 ] state=[-0.45454185 -0.34129013 -0.013175   -0.21402892], action=0, reward=1.0, next_state=[-0.46136765 -0.53622127 -0.01745557  0.07446907]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 104 ] state=[-0.46136765 -0.53622127 -0.01745557  0.07446907], action=1, reward=1.0, next_state=[-0.47209207 -0.34085348 -0.01596619 -0.22366965]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 105 ] state=[-0.47209207 -0.34085348 -0.01596619 -0.22366965], action=1, reward=1.0, next_state=[-0.47890914 -0.14550701 -0.02043959 -0.52134586]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 106 ] state=[-0.47890914 -0.14550701 -0.02043959 -0.52134586], action=1, reward=1.0, next_state=[-0.48181928  0.0498966  -0.0308665  -0.82039888]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 107 ] state=[-0.48181928  0.0498966  -0.0308665  -0.82039888], action=0, reward=1.0, next_state=[-0.48082135 -0.14478966 -0.04727448 -0.5375819 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 108 ] state=[-0.48082135 -0.14478966 -0.04727448 -0.5375819 ], action=0, reward=1.0, next_state=[-0.48371714 -0.33921619 -0.05802612 -0.26016146]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 109 ] state=[-0.48371714 -0.33921619 -0.05802612 -0.26016146], action=1, reward=1.0, next_state=[-0.49050147 -0.14331596 -0.06322935 -0.57056734]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 110 ] state=[-0.49050147 -0.14331596 -0.06322935 -0.57056734], action=0, reward=1.0, next_state=[-0.49336779 -0.33749687 -0.07464069 -0.29845508]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 111 ] state=[-0.49336779 -0.33749687 -0.07464069 -0.29845508], action=0, reward=1.0, next_state=[-0.50011773 -0.53147988 -0.0806098  -0.03021471]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 112 ] state=[-0.50011773 -0.53147988 -0.0806098  -0.03021471], action=0, reward=1.0, next_state=[-0.51074732 -0.72535886 -0.08121409  0.23598578]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 113 ] state=[-0.51074732 -0.72535886 -0.08121409  0.23598578], action=1, reward=1.0, next_state=[-0.5252545  -0.52917618 -0.07649437 -0.081169  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 114 ] state=[-0.5252545  -0.52917618 -0.07649437 -0.081169  ], action=1, reward=1.0, next_state=[-0.53583802 -0.33304576 -0.07811775 -0.39697174]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 115 ] state=[-0.53583802 -0.33304576 -0.07811775 -0.39697174], action=0, reward=1.0, next_state=[-0.54249894 -0.52697752 -0.08605719 -0.1299045 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 116 ] state=[-0.54249894 -0.52697752 -0.08605719 -0.1299045 ], action=0, reward=1.0, next_state=[-0.55303849 -0.72076803 -0.08865528  0.13443594]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 117 ] state=[-0.55303849 -0.72076803 -0.08865528  0.13443594], action=1, reward=1.0, next_state=[-0.56745385 -0.52449543 -0.08596656 -0.18484725]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 118 ] state=[-0.56745385 -0.52449543 -0.08596656 -0.18484725], action=0, reward=1.0, next_state=[-0.57794376 -0.71828889 -0.08966351  0.07952641]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 119 ] state=[-0.57794376 -0.71828889 -0.08966351  0.07952641], action=0, reward=1.0, next_state=[-0.59230954 -0.91201871 -0.08807298  0.34262804]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 120 ] state=[-0.59230954 -0.91201871 -0.08807298  0.34262804], action=0, reward=1.0, next_state=[-0.61054991 -1.10578447 -0.08122042  0.60629015]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 121 ] state=[-0.61054991 -1.10578447 -0.08122042  0.60629015], action=1, reward=1.0, next_state=[-0.6326656  -0.90962638 -0.06909461  0.28917044]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 122 ] state=[-0.6326656  -0.90962638 -0.06909461  0.28917044], action=1, reward=1.0, next_state=[-0.65085813 -0.71359067 -0.06331121 -0.02447915]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 123 ] state=[-0.65085813 -0.71359067 -0.06331121 -0.02447915], action=0, reward=1.0, next_state=[-0.66512994 -0.90775024 -0.06380079  0.24757564]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 124 ] state=[-0.66512994 -0.90775024 -0.06380079  0.24757564], action=0, reward=1.0, next_state=[-0.68328495 -1.10190573 -0.05884928  0.51947164]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 125 ] state=[-0.68328495 -1.10190573 -0.05884928  0.51947164], action=1, reward=1.0, next_state=[-0.70532306 -0.90600681 -0.04845984  0.20884025]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 126 ] state=[-0.70532306 -0.90600681 -0.04845984  0.20884025], action=1, reward=1.0, next_state=[-0.7234432  -0.71022664 -0.04428304 -0.09872688]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 127 ] state=[-0.7234432  -0.71022664 -0.04428304 -0.09872688], action=1, reward=1.0, next_state=[-0.73764773 -0.51449892 -0.04625757 -0.40504559]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 128 ] state=[-0.73764773 -0.51449892 -0.04625757 -0.40504559], action=0, reward=1.0, next_state=[-0.74793771 -0.7089354  -0.05435849 -0.12729774]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 129 ] state=[-0.74793771 -0.7089354  -0.05435849 -0.12729774], action=0, reward=1.0, next_state=[-0.76211642 -0.9032382  -0.05690444  0.14775244]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 130 ] state=[-0.76211642 -0.9032382  -0.05690444  0.14775244], action=1, reward=1.0, next_state=[-0.78018118 -0.70734949 -0.05394939 -0.1623259 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 131 ] state=[-0.78018118 -0.70734949 -0.05394939 -0.1623259 ], action=0, reward=1.0, next_state=[-0.79432817 -0.90165927 -0.05719591  0.11286129]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 132 ] state=[-0.79432817 -0.90165927 -0.05719591  0.11286129], action=1, reward=1.0, next_state=[-0.81236136 -0.70576638 -0.05493869 -0.19730397]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 133 ] state=[-0.81236136 -0.70576638 -0.05493869 -0.19730397], action=0, reward=1.0, next_state=[-0.82647668 -0.90006124 -0.05888476  0.07755474]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 134 ] state=[-0.82647668 -0.90006124 -0.05888476  0.07755474], action=1, reward=1.0, next_state=[-0.84447791 -0.70414674 -0.05733367 -0.23310978]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 135 ] state=[-0.84447791 -0.70414674 -0.05733367 -0.23310978], action=0, reward=1.0, next_state=[-0.85856084 -0.89840461 -0.06199587  0.04095137]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 136 ] state=[-0.85856084 -0.89840461 -0.06199587  0.04095137], action=1, reward=1.0, next_state=[-0.87652893 -0.70245096 -0.06117684 -0.27062953]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 137 ] state=[-0.87652893 -0.70245096 -0.06117684 -0.27062953], action=1, reward=1.0, next_state=[-0.89057795 -0.50651179 -0.06658943 -0.58196324]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 138 ] state=[-0.89057795 -0.50651179 -0.06658943 -0.58196324], action=0, reward=1.0, next_state=[-0.90070819 -0.70064066 -0.07822869 -0.31097811]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 139 ] state=[-0.90070819 -0.70064066 -0.07822869 -0.31097811], action=0, reward=1.0, next_state=[-0.914721   -0.89456599 -0.08444826 -0.04395554]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 140 ] state=[-0.914721   -0.89456599 -0.08444826 -0.04395554], action=1, reward=1.0, next_state=[-0.93261232 -0.69834098 -0.08532737 -0.36204242]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 141 ] state=[-0.93261232 -0.69834098 -0.08532737 -0.36204242], action=0, reward=1.0, next_state=[-0.94657914 -0.89215304 -0.09256822 -0.09743783]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 354 ][ timestamp 142 ] state=[-0.94657914 -0.89215304 -0.09256822 -0.09743783], action=0, reward=1.0, next_state=[-0.9644222  -1.08583477 -0.09451697  0.16466472]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 143 ] state=[-0.9644222  -1.08583477 -0.09451697  0.16466472], action=1, reward=1.0, next_state=[-0.9861389  -0.88949582 -0.09122368 -0.15627583]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 144 ] state=[-0.9861389  -0.88949582 -0.09122368 -0.15627583], action=0, reward=1.0, next_state=[-1.00392881 -1.08320128 -0.09434919  0.10629165]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 145 ] state=[-1.00392881 -1.08320128 -0.09434919  0.10629165], action=1, reward=1.0, next_state=[-1.02559284 -0.88686279 -0.09222336 -0.21460378]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 146 ] state=[-1.02559284 -0.88686279 -0.09222336 -0.21460378], action=0, reward=1.0, next_state=[-1.0433301  -1.08055355 -0.09651544  0.04762246]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 147 ] state=[-1.0433301  -1.08055355 -0.09651544  0.04762246], action=1, reward=1.0, next_state=[-1.06494117 -0.88418964 -0.09556299 -0.27388408]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 148 ] state=[-1.06494117 -0.88418964 -0.09556299 -0.27388408], action=0, reward=1.0, next_state=[-1.08262496 -1.07782739 -0.10104067 -0.01280549]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 149 ] state=[-1.08262496 -1.07782739 -0.10104067 -0.01280549], action=1, reward=1.0, next_state=[-1.10418151 -0.88141246 -0.10129678 -0.33558068]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 150 ] state=[-1.10418151 -0.88141246 -0.10129678 -0.33558068], action=0, reward=1.0, next_state=[-1.12180976 -1.07495787 -0.10800839 -0.07648111]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 151 ] state=[-1.12180976 -1.07495787 -0.10800839 -0.07648111], action=0, reward=1.0, next_state=[-1.14330891 -1.268379   -0.10953801  0.18026715]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 152 ] state=[-1.14330891 -1.268379   -0.10953801  0.18026715], action=1, reward=1.0, next_state=[-1.16867649 -1.07187394 -0.10593267 -0.14486368]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 153 ] state=[-1.16867649 -1.07187394 -0.10593267 -0.14486368], action=1, reward=1.0, next_state=[-1.19011397 -0.87540699 -0.10882995 -0.46899813]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 154 ] state=[-1.19011397 -0.87540699 -0.10882995 -0.46899813], action=0, reward=1.0, next_state=[-1.20762211 -1.06883679 -0.11820991 -0.21250284]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 155 ] state=[-1.20762211 -1.06883679 -0.11820991 -0.21250284], action=0, reward=1.0, next_state=[-1.22899885 -1.26208771 -0.12245996  0.04067776]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 156 ] state=[-1.22899885 -1.26208771 -0.12245996  0.04067776], action=0, reward=1.0, next_state=[-1.2542406  -1.4552602  -0.12164641  0.29235322]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 157 ] state=[-1.2542406  -1.4552602  -0.12164641  0.29235322], action=1, reward=1.0, next_state=[-1.28334581 -1.25863279 -0.11579934 -0.03608424]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 158 ] state=[-1.28334581 -1.25863279 -0.11579934 -0.03608424], action=0, reward=1.0, next_state=[-1.30851846 -1.45192016 -0.11652103  0.2179361 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 159 ] state=[-1.30851846 -1.45192016 -0.11652103  0.2179361 ], action=1, reward=1.0, next_state=[-1.33755687 -1.25534201 -0.11216231 -0.10911138]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 160 ] state=[-1.33755687 -1.25534201 -0.11216231 -0.10911138], action=0, reward=1.0, next_state=[-1.36266371 -1.4486929  -0.11434454  0.14618594]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 161 ] state=[-1.36266371 -1.4486929  -0.11434454  0.14618594], action=1, reward=1.0, next_state=[-1.39163756 -1.25213495 -0.11142082 -0.18026973]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 162 ] state=[-1.39163756 -1.25213495 -0.11142082 -0.18026973], action=0, reward=1.0, next_state=[-1.41668026 -1.44550075 -0.11502621  0.07529044]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 163 ] state=[-1.41668026 -1.44550075 -0.11502621  0.07529044], action=0, reward=1.0, next_state=[-1.44559028 -1.63880189 -0.1135204   0.3295829 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 164 ] state=[-1.44559028 -1.63880189 -0.1135204   0.3295829 ], action=0, reward=1.0, next_state=[-1.47836632 -1.83214022 -0.10692874  0.58442039]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 165 ] state=[-1.47836632 -1.83214022 -0.10692874  0.58442039], action=1, reward=1.0, next_state=[-1.51500912 -1.63569571 -0.09524034  0.26005941]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 166 ] state=[-1.51500912 -1.63569571 -0.09524034  0.26005941], action=1, reward=1.0, next_state=[-1.54772303 -1.43935231 -0.09003915 -0.06107931]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 167 ] state=[-1.54772303 -1.43935231 -0.09003915 -0.06107931], action=1, reward=1.0, next_state=[-1.57651008 -1.24306249 -0.09126073 -0.38075711]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 168 ] state=[-1.57651008 -1.24306249 -0.09126073 -0.38075711], action=1, reward=1.0, next_state=[-1.60137133 -1.04677116 -0.09887588 -0.70076227]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 169 ] state=[-1.60137133 -1.04677116 -0.09887588 -0.70076227], action=0, reward=1.0, next_state=[-1.62230675 -1.24039354 -0.11289112 -0.4407694 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 170 ] state=[-1.62230675 -1.24039354 -0.11289112 -0.4407694 ], action=0, reward=1.0, next_state=[-1.64711462 -1.433752   -0.12170651 -0.18569747]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 171 ] state=[-1.64711462 -1.433752   -0.12170651 -0.18569747], action=0, reward=1.0, next_state=[-1.67578966 -1.62694141 -0.12542046  0.06624965]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 172 ] state=[-1.67578966 -1.62694141 -0.12542046  0.06624965], action=1, reward=1.0, next_state=[-1.70832849 -1.43026528 -0.12409547 -0.26322429]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 173 ] state=[-1.70832849 -1.43026528 -0.12409547 -0.26322429], action=1, reward=1.0, next_state=[-1.7369338  -1.23361066 -0.12935995 -0.59232832]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 174 ] state=[-1.7369338  -1.23361066 -0.12935995 -0.59232832], action=0, reward=1.0, next_state=[-1.76160601 -1.42670706 -0.14120652 -0.34302963]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 175 ] state=[-1.76160601 -1.42670706 -0.14120652 -0.34302963], action=0, reward=1.0, next_state=[-1.79014015 -1.61956736 -0.14806711 -0.09799541]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 176 ] state=[-1.79014015 -1.61956736 -0.14806711 -0.09799541], action=0, reward=1.0, next_state=[-1.8225315  -1.81229118 -0.15002702  0.14455433]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 177 ] state=[-1.8225315  -1.81229118 -0.15002702  0.14455433], action=0, reward=1.0, next_state=[-1.85877732 -2.00498174 -0.14713593  0.38640077]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 178 ] state=[-1.85877732 -2.00498174 -0.14713593  0.38640077], action=1, reward=1.0, next_state=[-1.89887696 -1.80811094 -0.13940792  0.05118329]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 179 ] state=[-1.89887696 -1.80811094 -0.13940792  0.05118329], action=0, reward=1.0, next_state=[-1.93503918 -2.00098723 -0.13838425  0.29683763]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 180 ] state=[-1.93503918 -2.00098723 -0.13838425  0.29683763], action=0, reward=1.0, next_state=[-1.97505892 -2.19389306 -0.1324475   0.54287492]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 181 ] state=[-1.97505892 -2.19389306 -0.1324475   0.54287492], action=0, reward=1.0, next_state=[-2.01893678 -2.38692929 -0.12159     0.79106743]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 182 ] state=[-2.01893678 -2.38692929 -0.12159     0.79106743], action=0, reward=1.0, next_state=[-2.06667537 -2.58019062 -0.10576865  1.04315971]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 183 ] state=[-2.06667537 -2.58019062 -0.10576865  1.04315971], action=0, reward=1.0, next_state=[-2.11827918 -2.7737612  -0.08490546  1.30085495]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 354 ][ timestamp 184 ] state=[-2.11827918 -2.7737612  -0.08490546  1.30085495], action=0, reward=1.0, next_state=[-2.1737544  -2.96770927 -0.05888836  1.56579685]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 185 ] state=[-2.1737544  -2.96770927 -0.05888836  1.56579685], action=0, reward=1.0, next_state=[-2.23310859 -3.16208014 -0.02757242  1.83954459]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 186 ] state=[-2.23310859 -3.16208014 -0.02757242  1.83954459], action=0, reward=1.0, next_state=[-2.29635019 -3.35688705  0.00921847  2.12353862]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 187 ] state=[-2.29635019 -3.35688705  0.00921847  2.12353862], action=0, reward=1.0, next_state=[-2.36348793 -3.55209943  0.05168924  2.41905494]\n",
      "[ Experience replay ] starts\n",
      "[ episode 354 ][ timestamp 188 ] state=[-2.36348793 -3.55209943  0.05168924  2.41905494], action=0, reward=-1.0, next_state=[-2.43452992 -3.7476282   0.10007034  2.72714625]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 354: Exploration_rate=0.01. Score=188.\n",
      "[ episode 355 ] state=[ 0.04264007 -0.01913567  0.02898181 -0.0347113 ]\n",
      "[ episode 355 ][ timestamp 1 ] state=[ 0.04264007 -0.01913567  0.02898181 -0.0347113 ], action=1, reward=1.0, next_state=[ 0.04225735  0.17555894  0.02828758 -0.31811112]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 2 ] state=[ 0.04225735  0.17555894  0.02828758 -0.31811112], action=0, reward=1.0, next_state=[ 0.04576853 -0.01995424  0.02192536 -0.01664323]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 3 ] state=[ 0.04576853 -0.01995424  0.02192536 -0.01664323], action=0, reward=1.0, next_state=[ 0.04536945 -0.21538365  0.0215925   0.28287597]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 4 ] state=[ 0.04536945 -0.21538365  0.0215925   0.28287597], action=0, reward=1.0, next_state=[ 0.04106177 -0.41080683  0.02725001  0.5822901 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 5 ] state=[ 0.04106177 -0.41080683  0.02725001  0.5822901 ], action=1, reward=1.0, next_state=[ 0.03284564 -0.21607705  0.03889582  0.29831439]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 6 ] state=[ 0.03284564 -0.21607705  0.03889582  0.29831439], action=1, reward=1.0, next_state=[ 0.0285241  -0.02153051  0.0448621   0.01814778]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 7 ] state=[ 0.0285241  -0.02153051  0.0448621   0.01814778], action=1, reward=1.0, next_state=[ 0.02809349  0.17292032  0.04522506 -0.26004995]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 8 ] state=[ 0.02809349  0.17292032  0.04522506 -0.26004995], action=0, reward=1.0, next_state=[ 0.03155189 -0.02281709  0.04002406  0.04654758]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 9 ] state=[ 0.03155189 -0.02281709  0.04002406  0.04654758], action=0, reward=1.0, next_state=[ 0.03109555 -0.21848942  0.04095501  0.35158496]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 10 ] state=[ 0.03109555 -0.21848942  0.04095501  0.35158496], action=1, reward=1.0, next_state=[ 0.02672576 -0.02397309  0.04798671  0.07209253]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 11 ] state=[ 0.02672576 -0.02397309  0.04798671  0.07209253], action=1, reward=1.0, next_state=[ 0.0262463   0.17042923  0.04942856 -0.20507259]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 12 ] state=[ 0.0262463   0.17042923  0.04942856 -0.20507259], action=1, reward=1.0, next_state=[ 0.02965489  0.36481076  0.04532711 -0.4817627 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 13 ] state=[ 0.02965489  0.36481076  0.04532711 -0.4817627 ], action=0, reward=1.0, next_state=[ 0.0369511   0.16907931  0.03569186 -0.17514546]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 14 ] state=[ 0.0369511   0.16907931  0.03569186 -0.17514546], action=0, reward=1.0, next_state=[ 0.04033269 -0.02653479  0.03218895  0.12857998]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 15 ] state=[ 0.04033269 -0.02653479  0.03218895  0.12857998], action=1, reward=1.0, next_state=[ 0.03980199  0.16811161  0.03476055 -0.15377646]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 16 ] state=[ 0.03980199  0.16811161  0.03476055 -0.15377646], action=1, reward=1.0, next_state=[ 0.04316422  0.36271905  0.03168502 -0.43529373]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 17 ] state=[ 0.04316422  0.36271905  0.03168502 -0.43529373], action=1, reward=1.0, next_state=[ 0.0504186   0.55737846  0.02297914 -0.71782245]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 18 ] state=[ 0.0504186   0.55737846  0.02297914 -0.71782245], action=1, reward=1.0, next_state=[ 0.06156617  0.752175    0.00862269 -1.00318484]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 19 ] state=[ 0.06156617  0.752175    0.00862269 -1.00318484], action=1, reward=1.0, next_state=[ 0.07660967  0.9471807  -0.011441   -1.29314748]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 20 ] state=[ 0.07660967  0.9471807  -0.011441   -1.29314748], action=0, reward=1.0, next_state=[ 0.09555329  0.75220601 -0.03730395 -1.00406818]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 21 ] state=[ 0.09555329  0.75220601 -0.03730395 -1.00406818], action=0, reward=1.0, next_state=[ 0.11059741  0.55760168 -0.05738532 -0.72332958]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 22 ] state=[ 0.11059741  0.55760168 -0.05738532 -0.72332958], action=0, reward=1.0, next_state=[ 0.12174944  0.3633184  -0.07185191 -0.44924639]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 23 ] state=[ 0.12174944  0.3633184  -0.07185191 -0.44924639], action=0, reward=1.0, next_state=[ 0.12901581  0.16928241 -0.08083684 -0.18004968]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 24 ] state=[ 0.12901581  0.16928241 -0.08083684 -0.18004968], action=0, reward=1.0, next_state=[ 0.13240146 -0.02459532 -0.08443783  0.0860771 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 25 ] state=[ 0.13240146 -0.02459532 -0.08443783  0.0860771 ], action=0, reward=1.0, next_state=[ 0.13190955 -0.2184118  -0.08271629  0.35097079]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 26 ] state=[ 0.13190955 -0.2184118  -0.08271629  0.35097079], action=1, reward=1.0, next_state=[ 0.12754131 -0.0222169  -0.07569687  0.03339379]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 27 ] state=[ 0.12754131 -0.0222169  -0.07569687  0.03339379], action=0, reward=1.0, next_state=[ 0.12709698 -0.21617631 -0.075029    0.30126612]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 28 ] state=[ 0.12709698 -0.21617631 -0.075029    0.30126612], action=0, reward=1.0, next_state=[ 0.12277345 -0.41015316 -0.06900367  0.56937496]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 29 ] state=[ 0.12277345 -0.41015316 -0.06900367  0.56937496], action=0, reward=1.0, next_state=[ 0.11457039 -0.60424293 -0.05761617  0.83954579]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 30 ] state=[ 0.11457039 -0.60424293 -0.05761617  0.83954579], action=1, reward=1.0, next_state=[ 0.10248553 -0.40838362 -0.04082526  0.52931455]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 31 ] state=[ 0.10248553 -0.40838362 -0.04082526  0.52931455], action=0, reward=1.0, next_state=[ 0.09431786 -0.60290818 -0.03023897  0.80885896]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 32 ] state=[ 0.09431786 -0.60290818 -0.03023897  0.80885896], action=1, reward=1.0, next_state=[ 0.08225969 -0.40738519 -0.01406179  0.50681966]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 33 ] state=[ 0.08225969 -0.40738519 -0.01406179  0.50681966], action=1, reward=1.0, next_state=[ 0.07411199 -0.21206795 -0.00392539  0.20973874]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 34 ] state=[ 0.07411199 -0.21206795 -0.00392539  0.20973874], action=0, reward=1.0, next_state=[ 6.98706302e-02 -4.07133556e-01  2.69380337e-04  5.01180825e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 35 ] state=[ 6.98706302e-02 -4.07133556e-01  2.69380337e-04  5.01180825e-01], action=1, reward=1.0, next_state=[ 0.06172796 -0.2120154   0.010293    0.2085828 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 36 ] state=[ 0.06172796 -0.2120154   0.010293    0.2085828 ], action=0, reward=1.0, next_state=[ 0.05748765 -0.40728301  0.01446465  0.50449479]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 37 ] state=[ 0.05748765 -0.40728301  0.01446465  0.50449479], action=1, reward=1.0, next_state=[ 0.04934199 -0.21236787  0.02455455  0.21640512]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 38 ] state=[ 0.04934199 -0.21236787  0.02455455  0.21640512], action=1, reward=1.0, next_state=[ 0.04509463 -0.01760539  0.02888265 -0.06843222]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 39 ] state=[ 0.04509463 -0.01760539  0.02888265 -0.06843222], action=0, reward=1.0, next_state=[ 0.04474253 -0.21312927  0.02751401  0.23322159]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 355 ][ timestamp 40 ] state=[ 0.04474253 -0.21312927  0.02751401  0.23322159], action=1, reward=1.0, next_state=[ 0.04047994 -0.01841103  0.03217844 -0.05065712]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 41 ] state=[ 0.04047994 -0.01841103  0.03217844 -0.05065712], action=1, reward=1.0, next_state=[ 0.04011172  0.17623509  0.0311653  -0.33301632]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 42 ] state=[ 0.04011172  0.17623509  0.0311653  -0.33301632], action=0, reward=1.0, next_state=[ 0.04363642 -0.01931626  0.02450497 -0.03067062]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 43 ] state=[ 0.04363642 -0.01931626  0.02450497 -0.03067062], action=0, reward=1.0, next_state=[ 0.0432501  -0.2147809   0.02389156  0.26964205]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 44 ] state=[ 0.0432501  -0.2147809   0.02389156  0.26964205], action=1, reward=1.0, next_state=[ 0.03895448 -0.0200079   0.0292844  -0.01541062]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 45 ] state=[ 0.03895448 -0.0200079   0.0292844  -0.01541062], action=0, reward=1.0, next_state=[ 0.03855432 -0.21553732  0.02897619  0.28636614]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 46 ] state=[ 0.03855432 -0.21553732  0.02897619  0.28636614], action=0, reward=1.0, next_state=[ 0.03424357 -0.41106027  0.03470351  0.58804526]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 47 ] state=[ 0.03424357 -0.41106027  0.03470351  0.58804526], action=1, reward=1.0, next_state=[ 0.02602237 -0.21644106  0.04646441  0.30649299]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 48 ] state=[ 0.02602237 -0.21644106  0.04646441  0.30649299], action=0, reward=1.0, next_state=[ 0.02169355 -0.41219326  0.05259427  0.61346001]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 49 ] state=[ 0.02169355 -0.41219326  0.05259427  0.61346001], action=1, reward=1.0, next_state=[ 0.01344968 -0.2178442   0.06486347  0.33779511]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 50 ] state=[ 0.01344968 -0.2178442   0.06486347  0.33779511], action=0, reward=1.0, next_state=[ 0.0090928  -0.41382631  0.07161938  0.65020659]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 51 ] state=[ 0.0090928  -0.41382631  0.07161938  0.65020659], action=1, reward=1.0, next_state=[ 0.00081627 -0.21977117  0.08462351  0.38090819]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 52 ] state=[ 0.00081627 -0.21977117  0.08462351  0.38090819], action=1, reward=1.0, next_state=[-0.00357915 -0.02594642  0.09224167  0.11606106]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 53 ] state=[-0.00357915 -0.02594642  0.09224167  0.11606106], action=1, reward=1.0, next_state=[-0.00409808  0.16774111  0.09456289 -0.1461545 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 54 ] state=[-0.00409808  0.16774111  0.09456289 -0.1461545 ], action=1, reward=1.0, next_state=[-0.00074326  0.3613906   0.0916398  -0.4075709 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 55 ] state=[-0.00074326  0.3613906   0.0916398  -0.4075709 ], action=1, reward=1.0, next_state=[ 0.00648455  0.55510178  0.08348838 -0.67001404]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 56 ] state=[ 0.00648455  0.55510178  0.08348838 -0.67001404], action=0, reward=1.0, next_state=[ 0.01758659  0.35892439  0.0700881  -0.35225584]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 57 ] state=[ 0.01758659  0.35892439  0.0700881  -0.35225584], action=0, reward=1.0, next_state=[ 0.02476508  0.16287939  0.06304299 -0.03832129]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 58 ] state=[ 0.02476508  0.16287939  0.06304299 -0.03832129], action=0, reward=1.0, next_state=[ 0.02802267 -0.03308729  0.06227656  0.27356715]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 59 ] state=[ 0.02802267 -0.03308729  0.06227656  0.27356715], action=1, reward=1.0, next_state=[0.02736092 0.16109334 0.0677479  0.00115833]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 60 ] state=[0.02736092 0.16109334 0.0677479  0.00115833], action=0, reward=1.0, next_state=[ 0.03058279 -0.03493152  0.06777107  0.31442373]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 61 ] state=[ 0.03058279 -0.03493152  0.06777107  0.31442373], action=1, reward=1.0, next_state=[0.02988416 0.15916288 0.07405955 0.04385993]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 62 ] state=[0.02988416 0.15916288 0.07405955 0.04385993], action=0, reward=1.0, next_state=[ 0.03306741 -0.03693862  0.07493674  0.35895947]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 63 ] state=[ 0.03306741 -0.03693862  0.07493674  0.35895947], action=1, reward=1.0, next_state=[0.03232864 0.1570425  0.08211593 0.09081517]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 64 ] state=[0.03232864 0.1570425  0.08211593 0.09081517], action=1, reward=1.0, next_state=[ 0.03546949  0.3508973   0.08393224 -0.17487225]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 65 ] state=[ 0.03546949  0.3508973   0.08393224 -0.17487225], action=0, reward=1.0, next_state=[0.04248744 0.15468069 0.08043479 0.14306369]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 66 ] state=[0.04248744 0.15468069 0.08043479 0.14306369], action=1, reward=1.0, next_state=[ 0.04558105  0.34856408  0.08329607 -0.12319878]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 67 ] state=[ 0.04558105  0.34856408  0.08329607 -0.12319878], action=0, reward=1.0, next_state=[0.05255233 0.15235367 0.08083209 0.19455715]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 68 ] state=[0.05255233 0.15235367 0.08083209 0.19455715], action=1, reward=1.0, next_state=[ 0.05559941  0.34623191  0.08472323 -0.07157189]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 69 ] state=[ 0.05559941  0.34623191  0.08472323 -0.07157189], action=0, reward=1.0, next_state=[0.06252404 0.15000395 0.0832918  0.24659313]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 70 ] state=[0.06252404 0.15000395 0.0832918  0.24659313], action=1, reward=1.0, next_state=[ 0.06552412  0.34384365  0.08822366 -0.01869895]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 71 ] state=[ 0.06552412  0.34384365  0.08822366 -0.01869895], action=0, reward=1.0, next_state=[0.072401   0.14757452 0.08784968 0.30046388]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 72 ] state=[0.072401   0.14757452 0.08784968 0.30046388], action=1, reward=1.0, next_state=[0.07535249 0.34134164 0.09385896 0.03672863]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 73 ] state=[0.07535249 0.34134164 0.09385896 0.03672863], action=0, reward=1.0, next_state=[0.08217932 0.14500781 0.09459353 0.35748716]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 74 ] state=[0.08217932 0.14500781 0.09459353 0.35748716], action=1, reward=1.0, next_state=[0.08507948 0.33866659 0.10174327 0.09606669]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 75 ] state=[0.08507948 0.33866659 0.10174327 0.09606669], action=1, reward=1.0, next_state=[ 0.09185281  0.53219428  0.10366461 -0.16286269]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 76 ] state=[ 0.09185281  0.53219428  0.10366461 -0.16286269], action=1, reward=1.0, next_state=[ 0.10249669  0.72569123  0.10040735 -0.42112713]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 77 ] state=[ 0.10249669  0.72569123  0.10040735 -0.42112713], action=0, reward=1.0, next_state=[ 0.11701052  0.52930068  0.09198481 -0.09855483]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 78 ] state=[ 0.11701052  0.52930068  0.09198481 -0.09855483], action=1, reward=1.0, next_state=[ 0.12759653  0.72299215  0.09001371 -0.36085834]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 79 ] state=[ 0.12759653  0.72299215  0.09001371 -0.36085834], action=1, reward=1.0, next_state=[ 0.14205637  0.91672701  0.08279655 -0.62385582]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 80 ] state=[ 0.14205637  0.91672701  0.08279655 -0.62385582], action=1, reward=1.0, next_state=[ 0.16039091  1.11060133  0.07031943 -0.88935669]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 81 ] state=[ 0.16039091  1.11060133  0.07031943 -0.88935669], action=1, reward=1.0, next_state=[ 0.18260294  1.30470223  0.0525323  -1.15913161]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 355 ][ timestamp 82 ] state=[ 0.18260294  1.30470223  0.0525323  -1.15913161], action=1, reward=1.0, next_state=[ 0.20869699  1.49910179  0.02934966 -1.43489129]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 83 ] state=[ 0.20869699  1.49910179  0.02934966 -1.43489129], action=1, reward=1.0, next_state=[ 2.38679022e-01  1.69384974e+00  6.51838387e-04 -1.71825984e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 84 ] state=[ 2.38679022e-01  1.69384974e+00  6.51838387e-04 -1.71825984e+00], action=1, reward=1.0, next_state=[ 0.27255602  1.88896421 -0.03371336 -2.01073985]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 85 ] state=[ 0.27255602  1.88896421 -0.03371336 -2.01073985], action=0, reward=1.0, next_state=[ 0.3103353   1.69420863 -0.07392816 -1.72868233]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 86 ] state=[ 0.3103353   1.69420863 -0.07392816 -1.72868233], action=0, reward=1.0, next_state=[ 0.34421947  1.50000524 -0.1085018  -1.45988801]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 87 ] state=[ 0.34421947  1.50000524 -0.1085018  -1.45988801], action=0, reward=1.0, next_state=[ 0.37421958  1.30636816 -0.13769956 -1.20297741]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 88 ] state=[ 0.37421958  1.30636816 -0.13769956 -1.20297741], action=1, reward=1.0, next_state=[ 0.40034694  1.5029752  -0.16175911 -1.53545232]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 89 ] state=[ 0.40034694  1.5029752  -0.16175911 -1.53545232], action=1, reward=1.0, next_state=[ 0.43040645  1.69963307 -0.19246816 -1.87393827]\n",
      "[ Experience replay ] starts\n",
      "[ episode 355 ][ timestamp 90 ] state=[ 0.43040645  1.69963307 -0.19246816 -1.87393827], action=1, reward=-1.0, next_state=[ 0.46439911  1.89626587 -0.22994692 -2.21967819]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 355: Exploration_rate=0.01. Score=90.\n",
      "[ episode 356 ] state=[-0.01177014 -0.02883492 -0.01596233  0.01439056]\n",
      "[ episode 356 ][ timestamp 1 ] state=[-0.01177014 -0.02883492 -0.01596233  0.01439056], action=1, reward=1.0, next_state=[-0.01234684  0.16651227 -0.01567451 -0.28328562]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 2 ] state=[-0.01234684  0.16651227 -0.01567451 -0.28328562], action=0, reward=1.0, next_state=[-0.00901659 -0.02838265 -0.02134023  0.00441272]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 3 ] state=[-0.00901659 -0.02838265 -0.02134023  0.00441272], action=0, reward=1.0, next_state=[-0.00958425 -0.22319215 -0.02125197  0.29028689]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 4 ] state=[-0.00958425 -0.22319215 -0.02125197  0.29028689], action=0, reward=1.0, next_state=[-0.01404809 -0.41800472 -0.01544623  0.57619215]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 5 ] state=[-0.01404809 -0.41800472 -0.01544623  0.57619215], action=1, reward=1.0, next_state=[-0.02240818 -0.22266969 -0.00392239  0.27868355]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 6 ] state=[-0.02240818 -0.22266969 -0.00392239  0.27868355], action=1, reward=1.0, next_state=[-0.02686158 -0.02749201  0.00165128 -0.01523391]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 7 ] state=[-0.02686158 -0.02749201  0.00165128 -0.01523391], action=0, reward=1.0, next_state=[-0.02741142 -0.2226376   0.0013466   0.27796956]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 8 ] state=[-0.02741142 -0.2226376   0.0013466   0.27796956], action=0, reward=1.0, next_state=[-0.03186417 -0.41777874  0.00690599  0.5710769 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 9 ] state=[-0.03186417 -0.41777874  0.00690599  0.5710769 ], action=1, reward=1.0, next_state=[-0.04021974 -0.22275431  0.01832753  0.28057757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 10 ] state=[-0.04021974 -0.22275431  0.01832753  0.28057757], action=1, reward=1.0, next_state=[-0.04467483 -0.02789852  0.02393908 -0.00626902]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 11 ] state=[-0.04467483 -0.02789852  0.02393908 -0.00626902], action=1, reward=1.0, next_state=[-0.0452328   0.16687207  0.0238137  -0.29130379]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 12 ] state=[-0.0452328   0.16687207  0.0238137  -0.29130379], action=0, reward=1.0, next_state=[-0.04189536 -0.02858119  0.01798763  0.00879355]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 13 ] state=[-0.04189536 -0.02858119  0.01798763  0.00879355], action=0, reward=1.0, next_state=[-0.04246698 -0.22395643  0.0181635   0.30709707]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 14 ] state=[-0.04246698 -0.22395643  0.0181635   0.30709707], action=1, reward=1.0, next_state=[-0.04694611 -0.02909795  0.02430544  0.02019733]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 15 ] state=[-0.04694611 -0.02909795  0.02430544  0.02019733], action=0, reward=1.0, next_state=[-0.04752807 -0.22455988  0.02470938  0.32044873]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 16 ] state=[-0.04752807 -0.22455988  0.02470938  0.32044873], action=1, reward=1.0, next_state=[-0.05201927 -0.02979838  0.03111836  0.03565947]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 17 ] state=[-0.05201927 -0.02979838  0.03111836  0.03565947], action=1, reward=1.0, next_state=[-0.05261524  0.16486383  0.03183155 -0.24704515]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 18 ] state=[-0.05261524  0.16486383  0.03183155 -0.24704515], action=0, reward=1.0, next_state=[-0.04931796 -0.03069793  0.02689065  0.05550579]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 19 ] state=[-0.04931796 -0.03069793  0.02689065  0.05550579], action=1, reward=1.0, next_state=[-0.04993192  0.16402833  0.02800076 -0.22857311]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 20 ] state=[-0.04993192  0.16402833  0.02800076 -0.22857311], action=0, reward=1.0, next_state=[-0.04665135 -0.03148234  0.0234293   0.07280908]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 21 ] state=[-0.04665135 -0.03148234  0.0234293   0.07280908], action=0, reward=1.0, next_state=[-0.047281   -0.22693221  0.02488548  0.37279101]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 22 ] state=[-0.047281   -0.22693221  0.02488548  0.37279101], action=1, reward=1.0, next_state=[-0.05181964 -0.03217245  0.0323413   0.08805741]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 23 ] state=[-0.05181964 -0.03217245  0.0323413   0.08805741], action=0, reward=1.0, next_state=[-0.05246309 -0.2277427   0.03410245  0.39076605]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 24 ] state=[-0.05246309 -0.2277427   0.03410245  0.39076605], action=1, reward=1.0, next_state=[-0.05701794 -0.03312092  0.04191777  0.1090273 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 25 ] state=[-0.05701794 -0.03312092  0.04191777  0.1090273 ], action=0, reward=1.0, next_state=[-0.05768036 -0.2288177   0.04409832  0.41463483]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 26 ] state=[-0.05768036 -0.2288177   0.04409832  0.41463483], action=1, reward=1.0, next_state=[-0.06225672 -0.03434762  0.05239101  0.136174  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 27 ] state=[-0.06225672 -0.03434762  0.05239101  0.136174  ], action=1, reward=1.0, next_state=[-0.06294367  0.15998629  0.05511449 -0.13953099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 28 ] state=[-0.06294367  0.15998629  0.05511449 -0.13953099], action=1, reward=1.0, next_state=[-0.05974394  0.35427733  0.05232387 -0.41432956]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 29 ] state=[-0.05974394  0.35427733  0.05232387 -0.41432956], action=0, reward=1.0, next_state=[-0.0526584   0.1584543   0.04403728 -0.10562081]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 30 ] state=[-0.0526584   0.1584543   0.04403728 -0.10562081], action=1, reward=1.0, next_state=[-0.04948931  0.35291841  0.04192487 -0.38409141]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 31 ] state=[-0.04948931  0.35291841  0.04192487 -0.38409141], action=1, reward=1.0, next_state=[-0.04243094  0.54742083  0.03424304 -0.66326637]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 356 ][ timestamp 32 ] state=[-0.04243094  0.54742083  0.03424304 -0.66326637], action=1, reward=1.0, next_state=[-0.03148253  0.74205007  0.02097771 -0.9449736 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 33 ] state=[-0.03148253  0.74205007  0.02097771 -0.9449736 ], action=1, reward=1.0, next_state=[-0.01664152  0.93688326  0.00207824 -1.23099209]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 34 ] state=[-0.01664152  0.93688326  0.00207824 -1.23099209], action=1, reward=1.0, next_state=[ 0.00209614  1.13197842 -0.0225416  -1.52302319]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 35 ] state=[ 0.00209614  1.13197842 -0.0225416  -1.52302319], action=0, reward=1.0, next_state=[ 0.02473571  0.93713587 -0.05300207 -1.23746029]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 36 ] state=[ 0.02473571  0.93713587 -0.05300207 -1.23746029], action=1, reward=1.0, next_state=[ 0.04347843  1.13289718 -0.07775127 -1.54626521]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 37 ] state=[ 0.04347843  1.13289718 -0.07775127 -1.54626521], action=0, reward=1.0, next_state=[ 0.06613637  0.93879031 -0.10867658 -1.27882039]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 38 ] state=[ 0.06613637  0.93879031 -0.10867658 -1.27882039], action=0, reward=1.0, next_state=[ 0.08491218  0.74520828 -0.13425299 -1.02204845]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 39 ] state=[ 0.08491218  0.74520828 -0.13425299 -1.02204845], action=1, reward=1.0, next_state=[ 0.09981634  0.94183839 -0.15469396 -1.3536915 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 40 ] state=[ 0.09981634  0.94183839 -0.15469396 -1.3536915 ], action=0, reward=1.0, next_state=[ 0.11865311  0.74895934 -0.18176779 -1.1131266 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 41 ] state=[ 0.11865311  0.74895934 -0.18176779 -1.1131266 ], action=0, reward=1.0, next_state=[ 0.1336323   0.55662852 -0.20403032 -0.88252909]\n",
      "[ Experience replay ] starts\n",
      "[ episode 356 ][ timestamp 42 ] state=[ 0.1336323   0.55662852 -0.20403032 -0.88252909], action=0, reward=-1.0, next_state=[ 0.14476487  0.36477448 -0.2216809  -0.66028682]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 356: Exploration_rate=0.01. Score=42.\n",
      "[ episode 357 ] state=[ 0.00407489 -0.03439702  0.02528369  0.03818897]\n",
      "[ episode 357 ][ timestamp 1 ] state=[ 0.00407489 -0.03439702  0.02528369  0.03818897], action=0, reward=1.0, next_state=[ 0.00338695 -0.22987224  0.02604747  0.3387407 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 2 ] state=[ 0.00338695 -0.22987224  0.02604747  0.3387407 ], action=0, reward=1.0, next_state=[-0.0012105  -0.42535497  0.03282229  0.63952241]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 3 ] state=[-0.0012105  -0.42535497  0.03282229  0.63952241], action=1, reward=1.0, next_state=[-0.0097176  -0.23070564  0.04561274  0.3573537 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 4 ] state=[-0.0097176  -0.23070564  0.04561274  0.3573537 ], action=1, reward=1.0, next_state=[-0.01433171 -0.03626084  0.05275981  0.07939535]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 5 ] state=[-0.01433171 -0.03626084  0.05275981  0.07939535], action=0, reward=1.0, next_state=[-0.01505693 -0.23209786  0.05434772  0.38824632]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 6 ] state=[-0.01505693 -0.23209786  0.05434772  0.38824632], action=1, reward=1.0, next_state=[-0.01969888 -0.03778777  0.06211264  0.11318188]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 7 ] state=[-0.01969888 -0.03778777  0.06211264  0.11318188], action=1, reward=1.0, next_state=[-0.02045464  0.15639172  0.06437628 -0.15927629]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 8 ] state=[-0.02045464  0.15639172  0.06437628 -0.15927629], action=1, reward=1.0, next_state=[-0.01732681  0.35053577  0.06119075 -0.43097558]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 9 ] state=[-0.01732681  0.35053577  0.06119075 -0.43097558], action=1, reward=1.0, next_state=[-0.01031609  0.54474028  0.05257124 -0.70375829]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 10 ] state=[-0.01031609  0.54474028  0.05257124 -0.70375829], action=1, reward=1.0, next_state=[ 5.78715794e-04  7.39095799e-01  3.84960767e-02 -9.79439971e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 11 ] state=[ 5.78715794e-04  7.39095799e-01  3.84960767e-02 -9.79439971e-01], action=1, reward=1.0, next_state=[ 0.01536063  0.93368114  0.01890728 -1.25978668]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 12 ] state=[ 0.01536063  0.93368114  0.01890728 -1.25978668], action=0, reward=1.0, next_state=[ 0.03403425  0.73832247 -0.00628846 -0.96124265]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 13 ] state=[ 0.03403425  0.73832247 -0.00628846 -0.96124265], action=0, reward=1.0, next_state=[ 0.0488007   0.5432856  -0.02551331 -0.67054192]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 14 ] state=[ 0.0488007   0.5432856  -0.02551331 -0.67054192], action=0, reward=1.0, next_state=[ 0.05966642  0.34852747 -0.03892415 -0.38599991]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 15 ] state=[ 0.05966642  0.34852747 -0.03892415 -0.38599991], action=0, reward=1.0, next_state=[ 0.06663697  0.15397909 -0.04664415 -0.10583918]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 16 ] state=[ 0.06663697  0.15397909 -0.04664415 -0.10583918], action=0, reward=1.0, next_state=[ 0.06971655 -0.04044447 -0.04876093  0.17177056]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 17 ] state=[ 0.06971655 -0.04044447 -0.04876093  0.17177056], action=0, reward=1.0, next_state=[ 0.06890766 -0.23483584 -0.04532552  0.448681  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 18 ] state=[ 0.06890766 -0.23483584 -0.04532552  0.448681  ], action=1, reward=1.0, next_state=[ 0.06421094 -0.03910304 -0.0363519   0.1420622 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 19 ] state=[ 0.06421094 -0.03910304 -0.0363519   0.1420622 ], action=0, reward=1.0, next_state=[ 0.06342888 -0.23368602 -0.03351065  0.42305874]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 20 ] state=[ 0.06342888 -0.23368602 -0.03351065  0.42305874], action=0, reward=1.0, next_state=[ 0.05875516 -0.42831761 -0.02504948  0.70499192]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 21 ] state=[ 0.05875516 -0.42831761 -0.02504948  0.70499192], action=1, reward=1.0, next_state=[ 0.05018881 -0.23285768 -0.01094964  0.40453023]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 22 ] state=[ 0.05018881 -0.23285768 -0.01094964  0.40453023], action=0, reward=1.0, next_state=[ 0.04553165 -0.42782264 -0.00285904  0.69374102]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 23 ] state=[ 0.04553165 -0.42782264 -0.00285904  0.69374102], action=0, reward=1.0, next_state=[ 0.0369752  -0.62290482  0.01101578  0.98552253]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 24 ] state=[ 0.0369752  -0.62290482  0.01101578  0.98552253], action=1, reward=1.0, next_state=[ 0.02451711 -0.42793213  0.03072623  0.69631981]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 25 ] state=[ 0.02451711 -0.42793213  0.03072623  0.69631981], action=1, reward=1.0, next_state=[ 0.01595846 -0.23324947  0.04465263  0.41346576]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 26 ] state=[ 0.01595846 -0.23324947  0.04465263  0.41346576], action=0, reward=1.0, next_state=[ 0.01129347 -0.42897498  0.05292195  0.71988489]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 27 ] state=[ 0.01129347 -0.42897498  0.05292195  0.71988489], action=1, reward=1.0, next_state=[ 0.00271397 -0.23462363  0.06731964  0.44431781]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 28 ] state=[ 0.00271397 -0.23462363  0.06731964  0.44431781], action=1, reward=1.0, next_state=[-0.0019785  -0.04051554  0.076206    0.17359222]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 29 ] state=[-0.0019785  -0.04051554  0.076206    0.17359222], action=1, reward=1.0, next_state=[-0.00278881  0.15343771  0.07967784 -0.09411042]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 30 ] state=[-0.00278881  0.15343771  0.07967784 -0.09411042], action=0, reward=1.0, next_state=[ 0.00027994 -0.04273049  0.07779564  0.22260884]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 31 ] state=[ 0.00027994 -0.04273049  0.07779564  0.22260884], action=1, reward=1.0, next_state=[-0.00057467  0.15119831  0.08224781 -0.04455567]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 357 ][ timestamp 32 ] state=[-0.00057467  0.15119831  0.08224781 -0.04455567], action=0, reward=1.0, next_state=[ 0.0024493  -0.04500083  0.0813567   0.27290178]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 33 ] state=[ 0.0024493  -0.04500083  0.0813567   0.27290178], action=0, reward=1.0, next_state=[ 0.00154928 -0.24118371  0.08681473  0.59009523]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 34 ] state=[ 0.00154928 -0.24118371  0.08681473  0.59009523], action=0, reward=1.0, next_state=[-0.00327439 -0.43740706  0.09861664  0.90881328]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 35 ] state=[-0.00327439 -0.43740706  0.09861664  0.90881328], action=0, reward=1.0, next_state=[-0.01202253 -0.63371559  0.1167929   1.23079169]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 36 ] state=[-0.01202253 -0.63371559  0.1167929   1.23079169], action=0, reward=1.0, next_state=[-0.02469684 -0.83012997  0.14140874  1.55766523]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 37 ] state=[-0.02469684 -0.83012997  0.14140874  1.55766523], action=0, reward=1.0, next_state=[-0.04129944 -1.02663378  0.17256204  1.89091457]\n",
      "[ Experience replay ] starts\n",
      "[ episode 357 ][ timestamp 38 ] state=[-0.04129944 -1.02663378  0.17256204  1.89091457], action=0, reward=-1.0, next_state=[-0.06183212 -1.22315874  0.21038033  2.23180569]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 357: Exploration_rate=0.01. Score=38.\n",
      "[ episode 358 ] state=[-0.0328819  -0.0189421  -0.0313894   0.02549705]\n",
      "[ episode 358 ][ timestamp 1 ] state=[-0.0328819  -0.0189421  -0.0313894   0.02549705], action=0, reward=1.0, next_state=[-0.03326074 -0.21360017 -0.03087946  0.30811335]\n",
      "[ Experience replay ] starts\n",
      "[ episode 358 ][ timestamp 2 ] state=[-0.03326074 -0.21360017 -0.03087946  0.30811335], action=0, reward=1.0, next_state=[-0.03753274 -0.40826883 -0.02471719  0.59090001]\n",
      "[ Experience replay ] starts\n",
      "[ episode 358 ][ timestamp 3 ] state=[-0.03753274 -0.40826883 -0.02471719  0.59090001], action=0, reward=1.0, next_state=[-0.04569812 -0.60303615 -0.01289919  0.87569565]\n",
      "[ Experience replay ] starts\n",
      "[ episode 358 ][ timestamp 4 ] state=[-0.04569812 -0.60303615 -0.01289919  0.87569565], action=0, reward=1.0, next_state=[-0.05775884 -0.79798041  0.00461472  1.16429545]\n",
      "[ Experience replay ] starts\n",
      "[ episode 358 ][ timestamp 5 ] state=[-0.05775884 -0.79798041  0.00461472  1.16429545], action=0, reward=1.0, next_state=[-0.07371845 -0.99316214  0.02790063  1.45842164]\n",
      "[ Experience replay ] starts\n",
      "[ episode 358 ][ timestamp 6 ] state=[-0.07371845 -0.99316214  0.02790063  1.45842164], action=0, reward=1.0, next_state=[-0.09358169 -1.188615    0.05706907  1.75968855]\n",
      "[ Experience replay ] starts\n",
      "[ episode 358 ][ timestamp 7 ] state=[-0.09358169 -1.188615    0.05706907  1.75968855], action=0, reward=1.0, next_state=[-0.11735399 -1.38433472  0.09226284  2.0695594 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 358 ][ timestamp 8 ] state=[-0.11735399 -1.38433472  0.09226284  2.0695594 ], action=0, reward=1.0, next_state=[-0.14504069 -1.58026569  0.13365403  2.38929265]\n",
      "[ Experience replay ] starts\n",
      "[ episode 358 ][ timestamp 9 ] state=[-0.14504069 -1.58026569  0.13365403  2.38929265], action=0, reward=1.0, next_state=[-0.176646   -1.77628483  0.18143988  2.71987649]\n",
      "[ Experience replay ] starts\n",
      "[ episode 358 ][ timestamp 10 ] state=[-0.176646   -1.77628483  0.18143988  2.71987649], action=0, reward=-1.0, next_state=[-0.2121717  -1.9721831   0.23583741  3.06195149]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 358: Exploration_rate=0.01. Score=10.\n",
      "[ episode 359 ] state=[-0.04981744  0.04326082  0.00905213 -0.01958366]\n",
      "[ episode 359 ][ timestamp 1 ] state=[-0.04981744  0.04326082  0.00905213 -0.01958366], action=0, reward=1.0, next_state=[-0.04895223 -0.15198977  0.00866046  0.27594152]\n",
      "[ Experience replay ] starts\n",
      "[ episode 359 ][ timestamp 2 ] state=[-0.04895223 -0.15198977  0.00866046  0.27594152], action=0, reward=1.0, next_state=[-0.05199202 -0.3472342   0.01417929  0.57134333]\n",
      "[ Experience replay ] starts\n",
      "[ episode 359 ][ timestamp 3 ] state=[-0.05199202 -0.3472342   0.01417929  0.57134333], action=0, reward=1.0, next_state=[-0.05893671 -0.54255209  0.02560615  0.86845927]\n",
      "[ Experience replay ] starts\n",
      "[ episode 359 ][ timestamp 4 ] state=[-0.05893671 -0.54255209  0.02560615  0.86845927], action=0, reward=1.0, next_state=[-0.06978775 -0.73801289  0.04297534  1.16908176]\n",
      "[ Experience replay ] starts\n",
      "[ episode 359 ][ timestamp 5 ] state=[-0.06978775 -0.73801289  0.04297534  1.16908176], action=0, reward=1.0, next_state=[-0.08454801 -0.93366671  0.06635697  1.47492236]\n",
      "[ Experience replay ] starts\n",
      "[ episode 359 ][ timestamp 6 ] state=[-0.08454801 -0.93366671  0.06635697  1.47492236], action=0, reward=1.0, next_state=[-0.10322134 -1.12953378  0.09585542  1.78757101]\n",
      "[ Experience replay ] starts\n",
      "[ episode 359 ][ timestamp 7 ] state=[-0.10322134 -1.12953378  0.09585542  1.78757101], action=0, reward=1.0, next_state=[-0.12581202 -1.32559225  0.13160684  2.10844703]\n",
      "[ Experience replay ] starts\n",
      "[ episode 359 ][ timestamp 8 ] state=[-0.12581202 -1.32559225  0.13160684  2.10844703], action=0, reward=1.0, next_state=[-0.15232386 -1.52176359  0.17377578  2.43874021]\n",
      "[ Experience replay ] starts\n",
      "[ episode 359 ][ timestamp 9 ] state=[-0.15232386 -1.52176359  0.17377578  2.43874021], action=0, reward=-1.0, next_state=[-0.18275913 -1.71789559  0.22255059  2.77934061]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 359: Exploration_rate=0.01. Score=9.\n",
      "[ episode 360 ] state=[-0.0387313  -0.00043089  0.0194782   0.03602811]\n",
      "[ episode 360 ][ timestamp 1 ] state=[-0.0387313  -0.00043089  0.0194782   0.03602811], action=0, reward=1.0, next_state=[-0.03873992 -0.19582667  0.02019876  0.33479241]\n",
      "[ Experience replay ] starts\n",
      "[ episode 360 ][ timestamp 2 ] state=[-0.03873992 -0.19582667  0.02019876  0.33479241], action=0, reward=1.0, next_state=[-0.04265645 -0.39123018  0.02689461  0.63377592]\n",
      "[ Experience replay ] starts\n",
      "[ episode 360 ][ timestamp 3 ] state=[-0.04265645 -0.39123018  0.02689461  0.63377592], action=0, reward=1.0, next_state=[-0.05048105 -0.58671677  0.03957013  0.93480583]\n",
      "[ Experience replay ] starts\n",
      "[ episode 360 ][ timestamp 4 ] state=[-0.05048105 -0.58671677  0.03957013  0.93480583], action=0, reward=1.0, next_state=[-0.06221539 -0.7823495   0.05826625  1.23965578]\n",
      "[ Experience replay ] starts\n",
      "[ episode 360 ][ timestamp 5 ] state=[-0.06221539 -0.7823495   0.05826625  1.23965578], action=0, reward=1.0, next_state=[-0.07786238 -0.97816929  0.08305936  1.55000759]\n",
      "[ Experience replay ] starts\n",
      "[ episode 360 ][ timestamp 6 ] state=[-0.07786238 -0.97816929  0.08305936  1.55000759], action=0, reward=1.0, next_state=[-0.09742576 -1.17418378  0.11405951  1.86740709]\n",
      "[ Experience replay ] starts\n",
      "[ episode 360 ][ timestamp 7 ] state=[-0.09742576 -1.17418378  0.11405951  1.86740709], action=0, reward=1.0, next_state=[-0.12090944 -1.37035422  0.15140766  2.1932116 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 360 ][ timestamp 8 ] state=[-0.12090944 -1.37035422  0.15140766  2.1932116 ], action=0, reward=1.0, next_state=[-0.14831652 -1.56658008  0.19527189  2.52852705]\n",
      "[ Experience replay ] starts\n",
      "[ episode 360 ][ timestamp 9 ] state=[-0.14831652 -1.56658008  0.19527189  2.52852705], action=0, reward=-1.0, next_state=[-0.17964812 -1.76268137  0.24584243  2.87413438]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 360: Exploration_rate=0.01. Score=9.\n",
      "[ episode 361 ] state=[ 0.03223971  0.0282471  -0.0030383  -0.0246041 ]\n",
      "[ episode 361 ][ timestamp 1 ] state=[ 0.03223971  0.0282471  -0.0030383  -0.0246041 ], action=0, reward=1.0, next_state=[ 0.03280465 -0.16683115 -0.00353038  0.26711867]\n",
      "[ Experience replay ] starts\n",
      "[ episode 361 ][ timestamp 2 ] state=[ 0.03280465 -0.16683115 -0.00353038  0.26711867], action=0, reward=1.0, next_state=[ 0.02946803 -0.36190254  0.00181199  0.558686  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 361 ][ timestamp 3 ] state=[ 0.02946803 -0.36190254  0.00181199  0.558686  ], action=0, reward=1.0, next_state=[ 0.02222998 -0.55704988  0.01298571  0.85193925]\n",
      "[ Experience replay ] starts\n",
      "[ episode 361 ][ timestamp 4 ] state=[ 0.02222998 -0.55704988  0.01298571  0.85193925], action=0, reward=1.0, next_state=[ 0.01108898 -0.75234644  0.0300245   1.14867709]\n",
      "[ Experience replay ] starts\n",
      "[ episode 361 ][ timestamp 5 ] state=[ 0.01108898 -0.75234644  0.0300245   1.14867709], action=0, reward=1.0, next_state=[-0.00395795 -0.94784719  0.05299804  1.45062192]\n",
      "[ Experience replay ] starts\n",
      "[ episode 361 ][ timestamp 6 ] state=[-0.00395795 -0.94784719  0.05299804  1.45062192], action=0, reward=1.0, next_state=[-0.02291489 -1.14357885  0.08201048  1.75938131]\n",
      "[ Experience replay ] starts\n",
      "[ episode 361 ][ timestamp 7 ] state=[-0.02291489 -1.14357885  0.08201048  1.75938131], action=0, reward=1.0, next_state=[-0.04578647 -1.33952808  0.11719811  2.07640135]\n",
      "[ Experience replay ] starts\n",
      "[ episode 361 ][ timestamp 8 ] state=[-0.04578647 -1.33952808  0.11719811  2.07640135], action=1, reward=1.0, next_state=[-0.07257703 -1.14577357  0.15872613  1.82214069]\n",
      "[ Experience replay ] starts\n",
      "[ episode 361 ][ timestamp 9 ] state=[-0.07257703 -1.14577357  0.15872613  1.82214069], action=0, reward=1.0, next_state=[-0.0954925  -1.34226266  0.19516895  2.15963914]\n",
      "[ Experience replay ] starts\n",
      "[ episode 361 ][ timestamp 10 ] state=[-0.0954925  -1.34226266  0.19516895  2.15963914], action=0, reward=-1.0, next_state=[-0.12233776 -1.53869008  0.23836173  2.50570257]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 361: Exploration_rate=0.01. Score=10.\n",
      "[ episode 362 ] state=[-0.04286739 -0.00231946 -0.00056744 -0.01917353]\n",
      "[ episode 362 ][ timestamp 1 ] state=[-0.04286739 -0.00231946 -0.00056744 -0.01917353], action=0, reward=1.0, next_state=[-0.04291378 -0.19743327 -0.00095091  0.27333031]\n",
      "[ Experience replay ] starts\n",
      "[ episode 362 ][ timestamp 2 ] state=[-0.04291378 -0.19743327 -0.00095091  0.27333031], action=0, reward=1.0, next_state=[-0.04686245 -0.39254164  0.0045157   0.56571317]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 362 ][ timestamp 3 ] state=[-0.04686245 -0.39254164  0.0045157   0.56571317], action=0, reward=1.0, next_state=[-0.05471328 -0.58772665  0.01582996  0.85981531]\n",
      "[ Experience replay ] starts\n",
      "[ episode 362 ][ timestamp 4 ] state=[-0.05471328 -0.58772665  0.01582996  0.85981531], action=0, reward=1.0, next_state=[-0.06646781 -0.78306059  0.03302627  1.15743332]\n",
      "[ Experience replay ] starts\n",
      "[ episode 362 ][ timestamp 5 ] state=[-0.06646781 -0.78306059  0.03302627  1.15743332], action=0, reward=1.0, next_state=[-0.08212903 -0.97859709  0.05617493  1.46028608]\n",
      "[ Experience replay ] starts\n",
      "[ episode 362 ][ timestamp 6 ] state=[-0.08212903 -0.97859709  0.05617493  1.46028608], action=0, reward=1.0, next_state=[-0.10170097 -1.17436102  0.08538065  1.76997552]\n",
      "[ Experience replay ] starts\n",
      "[ episode 362 ][ timestamp 7 ] state=[-0.10170097 -1.17436102  0.08538065  1.76997552], action=0, reward=1.0, next_state=[-0.12518819 -1.37033659  0.12078016  2.08793948]\n",
      "[ Experience replay ] starts\n",
      "[ episode 362 ][ timestamp 8 ] state=[-0.12518819 -1.37033659  0.12078016  2.08793948], action=0, reward=1.0, next_state=[-0.15259492 -1.56645314  0.16253895  2.41539432]\n",
      "[ Experience replay ] starts\n",
      "[ episode 362 ][ timestamp 9 ] state=[-0.15259492 -1.56645314  0.16253895  2.41539432], action=0, reward=-1.0, next_state=[-0.18392398 -1.76256844  0.21084684  2.75326627]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 362: Exploration_rate=0.01. Score=9.\n",
      "[ episode 363 ] state=[ 0.03661177 -0.03297199  0.02993612  0.04069142]\n",
      "[ episode 363 ][ timestamp 1 ] state=[ 0.03661177 -0.03297199  0.02993612  0.04069142], action=0, reward=1.0, next_state=[ 0.03595233 -0.22851015  0.03074995  0.34266715]\n",
      "[ Experience replay ] starts\n",
      "[ episode 363 ][ timestamp 2 ] state=[ 0.03595233 -0.22851015  0.03074995  0.34266715], action=0, reward=1.0, next_state=[ 0.03138213 -0.42405578  0.03760329  0.64488598]\n",
      "[ Experience replay ] starts\n",
      "[ episode 363 ][ timestamp 3 ] state=[ 0.03138213 -0.42405578  0.03760329  0.64488598], action=0, reward=1.0, next_state=[ 0.02290101 -0.61968102  0.05050101  0.94916917]\n",
      "[ Experience replay ] starts\n",
      "[ episode 363 ][ timestamp 4 ] state=[ 0.02290101 -0.61968102  0.05050101  0.94916917], action=0, reward=1.0, next_state=[ 0.01050739 -0.81544513  0.0694844   1.25728195]\n",
      "[ Experience replay ] starts\n",
      "[ episode 363 ][ timestamp 5 ] state=[ 0.01050739 -0.81544513  0.0694844   1.25728195], action=0, reward=1.0, next_state=[-0.00580151 -1.0113842   0.09463003  1.57089332]\n",
      "[ Experience replay ] starts\n",
      "[ episode 363 ][ timestamp 6 ] state=[-0.00580151 -1.0113842   0.09463003  1.57089332], action=0, reward=1.0, next_state=[-0.0260292  -1.2074996   0.1260479   1.89152998]\n",
      "[ Experience replay ] starts\n",
      "[ episode 363 ][ timestamp 7 ] state=[-0.0260292  -1.2074996   0.1260479   1.89152998], action=0, reward=1.0, next_state=[-0.05017919 -1.40374442  0.1638785   2.22052187]\n",
      "[ Experience replay ] starts\n",
      "[ episode 363 ][ timestamp 8 ] state=[-0.05017919 -1.40374442  0.1638785   2.22052187], action=0, reward=1.0, next_state=[-0.07825408 -1.60000773  0.20828894  2.55893744]\n",
      "[ Experience replay ] starts\n",
      "[ episode 363 ][ timestamp 9 ] state=[-0.07825408 -1.60000773  0.20828894  2.55893744], action=0, reward=-1.0, next_state=[-0.11025423 -1.79609663  0.25946769  2.90750856]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 363: Exploration_rate=0.01. Score=9.\n",
      "[ episode 364 ] state=[-0.04883336  0.0469295  -0.00681473  0.04110225]\n",
      "[ episode 364 ][ timestamp 1 ] state=[-0.04883336  0.0469295  -0.00681473  0.04110225], action=0, reward=1.0, next_state=[-0.04789477 -0.14809407 -0.00599268  0.33162729]\n",
      "[ Experience replay ] starts\n",
      "[ episode 364 ][ timestamp 2 ] state=[-0.04789477 -0.14809407 -0.00599268  0.33162729], action=0, reward=1.0, next_state=[-0.05085665 -0.34313021  0.00063986  0.62241441]\n",
      "[ Experience replay ] starts\n",
      "[ episode 364 ][ timestamp 3 ] state=[-0.05085665 -0.34313021  0.00063986  0.62241441], action=0, reward=1.0, next_state=[-0.05771925 -0.53826109  0.01308815  0.91529879]\n",
      "[ Experience replay ] starts\n",
      "[ episode 364 ][ timestamp 4 ] state=[-0.05771925 -0.53826109  0.01308815  0.91529879], action=0, reward=1.0, next_state=[-0.06848447 -0.73355758  0.03139413  1.21206624]\n",
      "[ Experience replay ] starts\n",
      "[ episode 364 ][ timestamp 5 ] state=[-0.06848447 -0.73355758  0.03139413  1.21206624], action=0, reward=1.0, next_state=[-0.08315562 -0.92907038  0.05563545  1.5144193 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 364 ][ timestamp 6 ] state=[-0.08315562 -0.92907038  0.05563545  1.5144193 ], action=0, reward=1.0, next_state=[-0.10173703 -1.12481987  0.08592384  1.82393761]\n",
      "[ Experience replay ] starts\n",
      "[ episode 364 ][ timestamp 7 ] state=[-0.10173703 -1.12481987  0.08592384  1.82393761], action=0, reward=1.0, next_state=[-0.12423343 -1.32078391  0.12240259  2.14202978]\n",
      "[ Experience replay ] starts\n",
      "[ episode 364 ][ timestamp 8 ] state=[-0.12423343 -1.32078391  0.12240259  2.14202978], action=0, reward=1.0, next_state=[-0.15064911 -1.51688335  0.16524318  2.46987473]\n",
      "[ Experience replay ] starts\n",
      "[ episode 364 ][ timestamp 9 ] state=[-0.15064911 -1.51688335  0.16524318  2.46987473], action=0, reward=-1.0, next_state=[-0.18098677 -1.71296504  0.21464068  2.80835155]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 364: Exploration_rate=0.01. Score=9.\n",
      "[ episode 365 ] state=[-0.04768254  0.02699804  0.02762565  0.01399023]\n",
      "[ episode 365 ][ timestamp 1 ] state=[-0.04768254  0.02699804  0.02762565  0.01399023], action=0, reward=1.0, next_state=[-0.04714258 -0.16850898  0.02790546  0.31525977]\n",
      "[ Experience replay ] starts\n",
      "[ episode 365 ][ timestamp 2 ] state=[-0.04714258 -0.16850898  0.02790546  0.31525977], action=0, reward=1.0, next_state=[-0.05051276 -0.36401708  0.03421065  0.61661089]\n",
      "[ Experience replay ] starts\n",
      "[ episode 365 ][ timestamp 3 ] state=[-0.05051276 -0.36401708  0.03421065  0.61661089], action=0, reward=1.0, next_state=[-0.0577931  -0.55959985  0.04654287  0.91986936]\n",
      "[ Experience replay ] starts\n",
      "[ episode 365 ][ timestamp 4 ] state=[-0.0577931  -0.55959985  0.04654287  0.91986936], action=0, reward=1.0, next_state=[-0.0689851  -0.75531893  0.06494026  1.22680871]\n",
      "[ Experience replay ] starts\n",
      "[ episode 365 ][ timestamp 5 ] state=[-0.0689851  -0.75531893  0.06494026  1.22680871], action=0, reward=1.0, next_state=[-0.08409148 -0.95121395  0.08947643  1.53911087]\n",
      "[ Experience replay ] starts\n",
      "[ episode 365 ][ timestamp 6 ] state=[-0.08409148 -0.95121395  0.08947643  1.53911087], action=0, reward=1.0, next_state=[-0.10311576 -1.14729121  0.12025865  1.85832119]\n",
      "[ Experience replay ] starts\n",
      "[ episode 365 ][ timestamp 7 ] state=[-0.10311576 -1.14729121  0.12025865  1.85832119], action=0, reward=1.0, next_state=[-0.12606158 -1.34351044  0.15742507  2.18579516]\n",
      "[ Experience replay ] starts\n",
      "[ episode 365 ][ timestamp 8 ] state=[-0.12606158 -1.34351044  0.15742507  2.18579516], action=0, reward=1.0, next_state=[-0.15293179 -1.53976926  0.20114098  2.5226351 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 365 ][ timestamp 9 ] state=[-0.15293179 -1.53976926  0.20114098  2.5226351 ], action=0, reward=-1.0, next_state=[-0.18372717 -1.73588554  0.25159368  2.86961625]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 365: Exploration_rate=0.01. Score=9.\n",
      "[ episode 366 ] state=[-0.04372949  0.0477095   0.00048419 -0.0470768 ]\n",
      "[ episode 366 ][ timestamp 1 ] state=[-0.04372949  0.0477095   0.00048419 -0.0470768 ], action=0, reward=1.0, next_state=[-0.0427753  -0.1474194  -0.00045735  0.24575885]\n",
      "[ Experience replay ] starts\n",
      "[ episode 366 ][ timestamp 2 ] state=[-0.0427753  -0.1474194  -0.00045735  0.24575885], action=0, reward=1.0, next_state=[-0.04572369 -0.34253481  0.00445783  0.53829749]\n",
      "[ Experience replay ] starts\n",
      "[ episode 366 ][ timestamp 3 ] state=[-0.04572369 -0.34253481  0.00445783  0.53829749], action=0, reward=1.0, next_state=[-0.05257439 -0.53771915  0.01522378  0.83238168]\n",
      "[ Experience replay ] starts\n",
      "[ episode 366 ][ timestamp 4 ] state=[-0.05257439 -0.53771915  0.01522378  0.83238168], action=0, reward=1.0, next_state=[-0.06332877 -0.7330458   0.03187141  1.12981332]\n",
      "[ Experience replay ] starts\n",
      "[ episode 366 ][ timestamp 5 ] state=[-0.06332877 -0.7330458   0.03187141  1.12981332], action=0, reward=1.0, next_state=[-0.07798969 -0.92857031  0.05446768  1.43231975]\n",
      "[ Experience replay ] starts\n",
      "[ episode 366 ][ timestamp 6 ] state=[-0.07798969 -0.92857031  0.05446768  1.43231975], action=0, reward=1.0, next_state=[-0.09656109 -1.12432045  0.08311407  1.7415151 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 366 ][ timestamp 7 ] state=[-0.09656109 -1.12432045  0.08311407  1.7415151 ], action=0, reward=1.0, next_state=[-0.1190475  -1.32028443  0.11794438  2.05885378]\n",
      "[ Experience replay ] starts\n",
      "[ episode 366 ][ timestamp 8 ] state=[-0.1190475  -1.32028443  0.11794438  2.05885378], action=0, reward=1.0, next_state=[-0.14545319 -1.51639691  0.15912145  2.38557411]\n",
      "[ Experience replay ] starts\n",
      "[ episode 366 ][ timestamp 9 ] state=[-0.14545319 -1.51639691  0.15912145  2.38557411], action=0, reward=1.0, next_state=[-0.17578113 -1.71252253  0.20683293  2.72263056]\n",
      "[ Experience replay ] starts\n",
      "[ episode 366 ][ timestamp 10 ] state=[-0.17578113 -1.71252253  0.20683293  2.72263056], action=0, reward=-1.0, next_state=[-0.21003158 -1.90843717  0.26128554  3.07061524]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 366: Exploration_rate=0.01. Score=10.\n",
      "[ episode 367 ] state=[0.01991573 0.02081575 0.01363675 0.00308637]\n",
      "[ episode 367 ][ timestamp 1 ] state=[0.01991573 0.02081575 0.01363675 0.00308637], action=0, reward=1.0, next_state=[ 0.02033205 -0.17449909  0.01369848  0.30004047]\n",
      "[ Experience replay ] starts\n",
      "[ episode 367 ][ timestamp 2 ] state=[ 0.02033205 -0.17449909  0.01369848  0.30004047], action=0, reward=1.0, next_state=[ 0.01684207 -0.36981359  0.01969929  0.59701196]\n",
      "[ Experience replay ] starts\n",
      "[ episode 367 ][ timestamp 3 ] state=[ 0.01684207 -0.36981359  0.01969929  0.59701196], action=0, reward=1.0, next_state=[ 0.00944579 -0.56520559  0.03163953  0.8958343 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 367 ][ timestamp 4 ] state=[ 0.00944579 -0.56520559  0.03163953  0.8958343 ], action=0, reward=1.0, next_state=[-0.00185832 -0.76074191  0.04955621  1.19829246]\n",
      "[ Experience replay ] starts\n",
      "[ episode 367 ][ timestamp 5 ] state=[-0.00185832 -0.76074191  0.04955621  1.19829246], action=0, reward=1.0, next_state=[-0.01707316 -0.95646887  0.07352206  1.50608603]\n",
      "[ Experience replay ] starts\n",
      "[ episode 367 ][ timestamp 6 ] state=[-0.01707316 -0.95646887  0.07352206  1.50608603], action=0, reward=1.0, next_state=[-0.03620253 -1.15240153  0.10364378  1.82078706]\n",
      "[ Experience replay ] starts\n",
      "[ episode 367 ][ timestamp 7 ] state=[-0.03620253 -1.15240153  0.10364378  1.82078706], action=0, reward=1.0, next_state=[-0.05925056 -1.34851104  0.14005952  2.14378953]\n",
      "[ Experience replay ] starts\n",
      "[ episode 367 ][ timestamp 8 ] state=[-0.05925056 -1.34851104  0.14005952  2.14378953], action=0, reward=1.0, next_state=[-0.08622078 -1.54470977  0.18293532  2.47624877]\n",
      "[ Experience replay ] starts\n",
      "[ episode 367 ][ timestamp 9 ] state=[-0.08622078 -1.54470977  0.18293532  2.47624877], action=0, reward=-1.0, next_state=[-0.11711498 -1.74083395  0.23246029  2.81900974]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 367: Exploration_rate=0.01. Score=9.\n",
      "[ episode 368 ] state=[-0.00928911 -0.00588941  0.00151029 -0.02038853]\n",
      "[ episode 368 ][ timestamp 1 ] state=[-0.00928911 -0.00588941  0.00151029 -0.02038853], action=0, reward=1.0, next_state=[-0.00940689 -0.20103299  0.00110252  0.27277053]\n",
      "[ Experience replay ] starts\n",
      "[ episode 368 ][ timestamp 2 ] state=[-0.00940689 -0.20103299  0.00110252  0.27277053], action=0, reward=1.0, next_state=[-0.01342755 -0.39617066  0.00655793  0.56580099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 368 ][ timestamp 3 ] state=[-0.01342755 -0.39617066  0.00655793  0.56580099], action=0, reward=1.0, next_state=[-0.02135097 -0.59138399  0.01787395  0.86054272]\n",
      "[ Experience replay ] starts\n",
      "[ episode 368 ][ timestamp 4 ] state=[-0.02135097 -0.59138399  0.01787395  0.86054272], action=0, reward=1.0, next_state=[-0.03317865 -0.78674475  0.03508481  1.15879171]\n",
      "[ Experience replay ] starts\n",
      "[ episode 368 ][ timestamp 5 ] state=[-0.03317865 -0.78674475  0.03508481  1.15879171], action=0, reward=1.0, next_state=[-0.04891354 -0.9823059   0.05826064  1.46226572]\n",
      "[ Experience replay ] starts\n",
      "[ episode 368 ][ timestamp 6 ] state=[-0.04891354 -0.9823059   0.05826064  1.46226572], action=0, reward=1.0, next_state=[-0.06855966 -1.17809146  0.08750596  1.77256473]\n",
      "[ Experience replay ] starts\n",
      "[ episode 368 ][ timestamp 7 ] state=[-0.06855966 -1.17809146  0.08750596  1.77256473], action=0, reward=1.0, next_state=[-0.09212149 -1.37408455  0.12295725  2.09112344]\n",
      "[ Experience replay ] starts\n",
      "[ episode 368 ][ timestamp 8 ] state=[-0.09212149 -1.37408455  0.12295725  2.09112344], action=0, reward=1.0, next_state=[-0.11960318 -1.57021307  0.16477972  2.41915355]\n",
      "[ Experience replay ] starts\n",
      "[ episode 368 ][ timestamp 9 ] state=[-0.11960318 -1.57021307  0.16477972  2.41915355], action=0, reward=-1.0, next_state=[-0.15100744 -1.76633296  0.21316279  2.75757487]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 368: Exploration_rate=0.01. Score=9.\n",
      "[ episode 369 ] state=[ 0.03287488  0.02927805  0.03847561 -0.03553073]\n",
      "[ episode 369 ][ timestamp 1 ] state=[ 0.03287488  0.02927805  0.03847561 -0.03553073], action=0, reward=1.0, next_state=[ 0.03346044 -0.16637392  0.03776499  0.26903907]\n",
      "[ Experience replay ] starts\n",
      "[ episode 369 ][ timestamp 2 ] state=[ 0.03346044 -0.16637392  0.03776499  0.26903907], action=0, reward=1.0, next_state=[ 0.03013296 -0.36201389  0.04314577  0.57339005]\n",
      "[ Experience replay ] starts\n",
      "[ episode 369 ][ timestamp 3 ] state=[ 0.03013296 -0.36201389  0.04314577  0.57339005], action=0, reward=1.0, next_state=[ 0.02289268 -0.55771337  0.05461358  0.87934701]\n",
      "[ Experience replay ] starts\n",
      "[ episode 369 ][ timestamp 4 ] state=[ 0.02289268 -0.55771337  0.05461358  0.87934701], action=0, reward=1.0, next_state=[ 0.01173842 -0.75353313  0.07220052  1.18868712]\n",
      "[ Experience replay ] starts\n",
      "[ episode 369 ][ timestamp 5 ] state=[ 0.01173842 -0.75353313  0.07220052  1.18868712], action=0, reward=1.0, next_state=[-0.00333225 -0.94951288  0.09597426  1.50309938]\n",
      "[ Experience replay ] starts\n",
      "[ episode 369 ][ timestamp 6 ] state=[-0.00333225 -0.94951288  0.09597426  1.50309938], action=0, reward=1.0, next_state=[-0.0223225  -1.14565979  0.12603625  1.82413888]\n",
      "[ Experience replay ] starts\n",
      "[ episode 369 ][ timestamp 7 ] state=[-0.0223225  -1.14565979  0.12603625  1.82413888], action=0, reward=1.0, next_state=[-0.0452357  -1.34193521  0.16251902  2.15317332]\n",
      "[ Experience replay ] starts\n",
      "[ episode 369 ][ timestamp 8 ] state=[-0.0452357  -1.34193521  0.16251902  2.15317332], action=0, reward=1.0, next_state=[-0.0720744  -1.53823919  0.20558249  2.49131973]\n",
      "[ Experience replay ] starts\n",
      "[ episode 369 ][ timestamp 9 ] state=[-0.0720744  -1.53823919  0.20558249  2.49131973], action=0, reward=-1.0, next_state=[-0.10283919 -1.7343929   0.25540888  2.83937087]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 369: Exploration_rate=0.01. Score=9.\n",
      "[ episode 370 ] state=[-0.02296701  0.00202557  0.01072442 -0.03846321]\n",
      "[ episode 370 ][ timestamp 1 ] state=[-0.02296701  0.00202557  0.01072442 -0.03846321], action=0, reward=1.0, next_state=[-0.0229265  -0.19324851  0.00995515  0.25758399]\n",
      "[ Experience replay ] starts\n",
      "[ episode 370 ][ timestamp 2 ] state=[-0.0229265  -0.19324851  0.00995515  0.25758399], action=0, reward=1.0, next_state=[-0.02679147 -0.38851117  0.01510683  0.55339022]\n",
      "[ Experience replay ] starts\n",
      "[ episode 370 ][ timestamp 3 ] state=[-0.02679147 -0.38851117  0.01510683  0.55339022], action=0, reward=1.0, next_state=[-0.03456169 -0.58384196  0.02617464  0.85079423]\n",
      "[ Experience replay ] starts\n",
      "[ episode 370 ][ timestamp 4 ] state=[-0.03456169 -0.58384196  0.02617464  0.85079423], action=0, reward=1.0, next_state=[-0.04623853 -0.77931085  0.04319052  1.15159159]\n",
      "[ Experience replay ] starts\n",
      "[ episode 370 ][ timestamp 5 ] state=[-0.04623853 -0.77931085  0.04319052  1.15159159], action=0, reward=1.0, next_state=[-0.06182475 -0.97496889  0.06622236  1.45749902]\n",
      "[ Experience replay ] starts\n",
      "[ episode 370 ][ timestamp 6 ] state=[-0.06182475 -0.97496889  0.06622236  1.45749902], action=0, reward=1.0, next_state=[-0.08132412 -1.17083789  0.09537234  1.77011368]\n",
      "[ Experience replay ] starts\n",
      "[ episode 370 ][ timestamp 7 ] state=[-0.08132412 -1.17083789  0.09537234  1.77011368], action=0, reward=1.0, next_state=[-0.10474088 -1.36689815  0.13077461  2.09086455]\n",
      "[ Experience replay ] starts\n",
      "[ episode 370 ][ timestamp 8 ] state=[-0.10474088 -1.36689815  0.13077461  2.09086455], action=0, reward=1.0, next_state=[-0.13207884 -1.56307402  0.1725919   2.42095393]\n",
      "[ Experience replay ] starts\n",
      "[ episode 370 ][ timestamp 9 ] state=[-0.13207884 -1.56307402  0.1725919   2.42095393], action=0, reward=-1.0, next_state=[-0.16334032 -1.75921704  0.22101098  2.76128779]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 370: Exploration_rate=0.01. Score=9.\n",
      "[ episode 371 ] state=[0.04619625 0.04160066 0.0261302  0.00472285]\n",
      "[ episode 371 ][ timestamp 1 ] state=[0.04619625 0.04160066 0.0261302  0.00472285], action=0, reward=1.0, next_state=[ 0.04702826 -0.15388611  0.02622466  0.3055343 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 371 ][ timestamp 2 ] state=[ 0.04702826 -0.15388611  0.02622466  0.3055343 ], action=0, reward=1.0, next_state=[ 0.04395054 -0.34937176  0.03233534  0.60637112]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 371 ][ timestamp 3 ] state=[ 0.04395054 -0.34937176  0.03233534  0.60637112], action=0, reward=1.0, next_state=[ 0.0369631  -0.54493057  0.04446277  0.90906093]\n",
      "[ Experience replay ] starts\n",
      "[ episode 371 ][ timestamp 4 ] state=[ 0.0369631  -0.54493057  0.04446277  0.90906093], action=0, reward=1.0, next_state=[ 0.02606449 -0.74062522  0.06264398  1.21538054]\n",
      "[ Experience replay ] starts\n",
      "[ episode 371 ][ timestamp 5 ] state=[ 0.02606449 -0.74062522  0.06264398  1.21538054], action=0, reward=1.0, next_state=[ 0.01125199 -0.93649684  0.0869516   1.52701696]\n",
      "[ Experience replay ] starts\n",
      "[ episode 371 ][ timestamp 6 ] state=[ 0.01125199 -0.93649684  0.0869516   1.52701696], action=0, reward=1.0, next_state=[-0.00747795 -1.13255378  0.11749193  1.84552291]\n",
      "[ Experience replay ] starts\n",
      "[ episode 371 ][ timestamp 7 ] state=[-0.00747795 -1.13255378  0.11749193  1.84552291], action=0, reward=1.0, next_state=[-0.03012902 -1.32875849  0.15440239  2.17226416]\n",
      "[ Experience replay ] starts\n",
      "[ episode 371 ][ timestamp 8 ] state=[-0.03012902 -1.32875849  0.15440239  2.17226416], action=0, reward=1.0, next_state=[-0.05670419 -1.52501215  0.19784768  2.50835673]\n",
      "[ Experience replay ] starts\n",
      "[ episode 371 ][ timestamp 9 ] state=[-0.05670419 -1.52501215  0.19784768  2.50835673], action=0, reward=-1.0, next_state=[-0.08720444 -1.72113705  0.24801481  2.85459353]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 371: Exploration_rate=0.01. Score=9.\n",
      "[ episode 372 ] state=[ 0.01510503 -0.02968416 -0.00854017 -0.0391101 ]\n",
      "[ episode 372 ][ timestamp 1 ] state=[ 0.01510503 -0.02968416 -0.00854017 -0.0391101 ], action=0, reward=1.0, next_state=[ 0.01451135 -0.22468261 -0.00932237  0.25086613]\n",
      "[ Experience replay ] starts\n",
      "[ episode 372 ][ timestamp 2 ] state=[ 0.01451135 -0.22468261 -0.00932237  0.25086613], action=0, reward=1.0, next_state=[ 0.01001769 -0.4196702  -0.00430505  0.54059407]\n",
      "[ Experience replay ] starts\n",
      "[ episode 372 ][ timestamp 3 ] state=[ 0.01001769 -0.4196702  -0.00430505  0.54059407], action=0, reward=1.0, next_state=[ 0.00162429 -0.61473138  0.00650683  0.83191744]\n",
      "[ Experience replay ] starts\n",
      "[ episode 372 ][ timestamp 4 ] state=[ 0.00162429 -0.61473138  0.00650683  0.83191744], action=0, reward=1.0, next_state=[-0.01067034 -0.80994165  0.02314518  1.12663964]\n",
      "[ Experience replay ] starts\n",
      "[ episode 372 ][ timestamp 5 ] state=[-0.01067034 -0.80994165  0.02314518  1.12663964], action=0, reward=1.0, next_state=[-0.02686917 -1.0053591   0.04567797  1.42649138]\n",
      "[ Experience replay ] starts\n",
      "[ episode 372 ][ timestamp 6 ] state=[-0.02686917 -1.0053591   0.04567797  1.42649138], action=0, reward=1.0, next_state=[-0.04697635 -1.20101473  0.0742078   1.73309336]\n",
      "[ Experience replay ] starts\n",
      "[ episode 372 ][ timestamp 7 ] state=[-0.04697635 -1.20101473  0.0742078   1.73309336], action=0, reward=1.0, next_state=[-0.07099665 -1.39690098  0.10886967  2.04791116]\n",
      "[ Experience replay ] starts\n",
      "[ episode 372 ][ timestamp 8 ] state=[-0.07099665 -1.39690098  0.10886967  2.04791116], action=0, reward=1.0, next_state=[-0.09893467 -1.59295804  0.14982789  2.37220012]\n",
      "[ Experience replay ] starts\n",
      "[ episode 372 ][ timestamp 9 ] state=[-0.09893467 -1.59295804  0.14982789  2.37220012], action=0, reward=1.0, next_state=[-0.13079383 -1.78905753  0.1972719   2.70693872]\n",
      "[ Experience replay ] starts\n",
      "[ episode 372 ][ timestamp 10 ] state=[-0.13079383 -1.78905753  0.1972719   2.70693872], action=0, reward=-1.0, next_state=[-0.16657498 -1.98498398  0.25141067  3.05275088]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 372: Exploration_rate=0.01. Score=10.\n",
      "[ episode 373 ] state=[ 0.01940411  0.02938758 -0.00075734 -0.01783753]\n",
      "[ episode 373 ][ timestamp 1 ] state=[ 0.01940411  0.02938758 -0.00075734 -0.01783753], action=0, reward=1.0, next_state=[ 0.01999186 -0.1657235  -0.00111409  0.27460635]\n",
      "[ Experience replay ] starts\n",
      "[ episode 373 ][ timestamp 2 ] state=[ 0.01999186 -0.1657235  -0.00111409  0.27460635], action=0, reward=1.0, next_state=[ 0.01667739 -0.36082954  0.00437804  0.56693769]\n",
      "[ Experience replay ] starts\n",
      "[ episode 373 ][ timestamp 3 ] state=[ 0.01667739 -0.36082954  0.00437804  0.56693769], action=0, reward=1.0, next_state=[ 0.0094608  -0.55601263  0.01571679  0.86099666]\n",
      "[ Experience replay ] starts\n",
      "[ episode 373 ][ timestamp 4 ] state=[ 0.0094608  -0.55601263  0.01571679  0.86099666], action=0, reward=1.0, next_state=[-0.00165945 -0.75134505  0.03293672  1.15857964]\n",
      "[ Experience replay ] starts\n",
      "[ episode 373 ][ timestamp 5 ] state=[-0.00165945 -0.75134505  0.03293672  1.15857964], action=0, reward=1.0, next_state=[-0.01668635 -0.94688038  0.05610832  1.46140521]\n",
      "[ Experience replay ] starts\n",
      "[ episode 373 ][ timestamp 6 ] state=[-0.01668635 -0.94688038  0.05610832  1.46140521], action=0, reward=1.0, next_state=[-0.03562396 -1.14264342  0.08533642  1.77107487]\n",
      "[ Experience replay ] starts\n",
      "[ episode 373 ][ timestamp 7 ] state=[-0.03562396 -1.14264342  0.08533642  1.77107487], action=0, reward=1.0, next_state=[-0.05847683 -1.33861829  0.12075792  2.08902592]\n",
      "[ Experience replay ] starts\n",
      "[ episode 373 ][ timestamp 8 ] state=[-0.05847683 -1.33861829  0.12075792  2.08902592], action=0, reward=1.0, next_state=[-0.08524919 -1.53473416  0.16253844  2.41647406]\n",
      "[ Experience replay ] starts\n",
      "[ episode 373 ][ timestamp 9 ] state=[-0.08524919 -1.53473416  0.16253844  2.41647406], action=0, reward=-1.0, next_state=[-0.11594387 -1.73084864  0.21086792  2.75434467]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 373: Exploration_rate=0.01. Score=9.\n",
      "[ episode 374 ] state=[-0.01874733  0.00178629  0.00691624 -0.03995185]\n",
      "[ episode 374 ][ timestamp 1 ] state=[-0.01874733  0.00178629  0.00691624 -0.03995185], action=0, reward=1.0, next_state=[-0.01871161 -0.19343416  0.0061172   0.25490517]\n",
      "[ Experience replay ] starts\n",
      "[ episode 374 ][ timestamp 2 ] state=[-0.01871161 -0.19343416  0.0061172   0.25490517], action=0, reward=1.0, next_state=[-0.02258029 -0.38864291  0.0112153   0.54951127]\n",
      "[ Experience replay ] starts\n",
      "[ episode 374 ][ timestamp 3 ] state=[-0.02258029 -0.38864291  0.0112153   0.54951127], action=0, reward=1.0, next_state=[-0.03035315 -0.58392059  0.02220553  0.8457066 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 374 ][ timestamp 4 ] state=[-0.03035315 -0.58392059  0.02220553  0.8457066 ], action=0, reward=1.0, next_state=[-0.04203156 -0.77933836  0.03911966  1.14528887]\n",
      "[ Experience replay ] starts\n",
      "[ episode 374 ][ timestamp 5 ] state=[-0.04203156 -0.77933836  0.03911966  1.14528887], action=0, reward=1.0, next_state=[-0.05761833 -0.97494883  0.06202544  1.44997834]\n",
      "[ Experience replay ] starts\n",
      "[ episode 374 ][ timestamp 6 ] state=[-0.05761833 -0.97494883  0.06202544  1.44997834], action=0, reward=1.0, next_state=[-0.0771173  -1.17077584  0.091025    1.76137779]\n",
      "[ Experience replay ] starts\n",
      "[ episode 374 ][ timestamp 7 ] state=[-0.0771173  -1.17077584  0.091025    1.76137779], action=0, reward=1.0, next_state=[-0.10053282 -1.36680237  0.12625256  2.08092471]\n",
      "[ Experience replay ] starts\n",
      "[ episode 374 ][ timestamp 8 ] state=[-0.10053282 -1.36680237  0.12625256  2.08092471], action=0, reward=1.0, next_state=[-0.12786887 -1.56295627  0.16787105  2.40983341]\n",
      "[ Experience replay ] starts\n",
      "[ episode 374 ][ timestamp 9 ] state=[-0.12786887 -1.56295627  0.16787105  2.40983341], action=0, reward=-1.0, next_state=[-0.15912799 -1.75909347  0.21606772  2.74902609]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 374: Exploration_rate=0.01. Score=9.\n",
      "[ episode 375 ] state=[ 0.04198757 -0.00181623  0.00350302  0.01033977]\n",
      "[ episode 375 ][ timestamp 1 ] state=[ 0.04198757 -0.00181623  0.00350302  0.01033977], action=0, reward=1.0, next_state=[ 0.04195124 -0.19698825  0.00370981  0.30412588]\n",
      "[ Experience replay ] starts\n",
      "[ episode 375 ][ timestamp 2 ] state=[ 0.04195124 -0.19698825  0.00370981  0.30412588], action=0, reward=1.0, next_state=[ 0.03801148 -0.39216287  0.00979233  0.59797648]\n",
      "[ Experience replay ] starts\n",
      "[ episode 375 ][ timestamp 3 ] state=[ 0.03801148 -0.39216287  0.00979233  0.59797648], action=0, reward=1.0, next_state=[ 0.03016822 -0.58742046  0.02175186  0.89372773]\n",
      "[ Experience replay ] starts\n",
      "[ episode 375 ][ timestamp 4 ] state=[ 0.03016822 -0.58742046  0.02175186  0.89372773], action=0, reward=1.0, next_state=[ 0.01841981 -0.78283056  0.03962641  1.19316807]\n",
      "[ Experience replay ] starts\n",
      "[ episode 375 ][ timestamp 5 ] state=[ 0.01841981 -0.78283056  0.03962641  1.19316807], action=0, reward=1.0, next_state=[ 0.0027632  -0.97844273  0.06348978  1.4980031 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 375 ][ timestamp 6 ] state=[ 0.0027632  -0.97844273  0.06348978  1.4980031 ], action=0, reward=1.0, next_state=[-0.01680566 -1.17427618  0.09344984  1.8098149 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 375 ][ timestamp 7 ] state=[-0.01680566 -1.17427618  0.09344984  1.8098149 ], action=0, reward=1.0, next_state=[-0.04029118 -1.37030747  0.12964614  2.1300131 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 375 ][ timestamp 8 ] state=[-0.04029118 -1.37030747  0.12964614  2.1300131 ], action=0, reward=1.0, next_state=[-0.06769733 -1.56645583  0.1722464   2.45977573]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 375 ][ timestamp 9 ] state=[-0.06769733 -1.56645583  0.1722464   2.45977573], action=0, reward=-1.0, next_state=[-0.09902645 -1.7625662   0.22144191  2.79997869]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 375: Exploration_rate=0.01. Score=9.\n",
      "[ episode 376 ] state=[ 0.04914428 -0.01022997  0.00467768 -0.01603384]\n",
      "[ episode 376 ][ timestamp 1 ] state=[ 0.04914428 -0.01022997  0.00467768 -0.01603384], action=0, reward=1.0, next_state=[ 0.04893969 -0.20541869  0.004357    0.27812127]\n",
      "[ Experience replay ] starts\n",
      "[ episode 376 ][ timestamp 2 ] state=[ 0.04893969 -0.20541869  0.004357    0.27812127], action=0, reward=1.0, next_state=[ 0.04483131 -0.40060252  0.00991942  0.5721752 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 376 ][ timestamp 3 ] state=[ 0.04483131 -0.40060252  0.00991942  0.5721752 ], action=0, reward=1.0, next_state=[ 0.03681926 -0.59586215  0.02136293  0.86796649]\n",
      "[ Experience replay ] starts\n",
      "[ episode 376 ][ timestamp 4 ] state=[ 0.03681926 -0.59586215  0.02136293  0.86796649], action=0, reward=1.0, next_state=[ 0.02490202 -0.79126816  0.03872226  1.16728884]\n",
      "[ Experience replay ] starts\n",
      "[ episode 376 ][ timestamp 5 ] state=[ 0.02490202 -0.79126816  0.03872226  1.16728884], action=0, reward=1.0, next_state=[ 0.00907665 -0.98687198  0.06206803  1.47185613]\n",
      "[ Experience replay ] starts\n",
      "[ episode 376 ][ timestamp 6 ] state=[ 0.00907665 -0.98687198  0.06206803  1.47185613], action=0, reward=1.0, next_state=[-0.01066078 -1.18269557  0.09150516  1.78326218]\n",
      "[ Experience replay ] starts\n",
      "[ episode 376 ][ timestamp 7 ] state=[-0.01066078 -1.18269557  0.09150516  1.78326218], action=0, reward=1.0, next_state=[-0.0343147  -1.37871926  0.1271704   2.10293256]\n",
      "[ Experience replay ] starts\n",
      "[ episode 376 ][ timestamp 8 ] state=[-0.0343147  -1.37871926  0.1271704   2.10293256], action=0, reward=1.0, next_state=[-0.06188908 -1.57486737  0.16922905  2.43206621]\n",
      "[ Experience replay ] starts\n",
      "[ episode 376 ][ timestamp 9 ] state=[-0.06188908 -1.57486737  0.16922905  2.43206621], action=0, reward=-1.0, next_state=[-0.09338643 -1.77099126  0.21787038  2.77156577]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 376: Exploration_rate=0.01. Score=9.\n",
      "[ episode 377 ] state=[-0.01194689  0.00963379 -0.01794574 -0.04313926]\n",
      "[ episode 377 ][ timestamp 1 ] state=[-0.01194689  0.00963379 -0.01794574 -0.04313926], action=0, reward=1.0, next_state=[-0.01175421 -0.18522629 -0.01880853  0.24382803]\n",
      "[ Experience replay ] starts\n",
      "[ episode 377 ][ timestamp 2 ] state=[-0.01175421 -0.18522629 -0.01880853  0.24382803], action=0, reward=1.0, next_state=[-0.01545874 -0.38007461 -0.01393197  0.53051943]\n",
      "[ Experience replay ] starts\n",
      "[ episode 377 ][ timestamp 3 ] state=[-0.01545874 -0.38007461 -0.01393197  0.53051943], action=0, reward=1.0, next_state=[-0.02306023 -0.57499784 -0.00332158  0.81878004]\n",
      "[ Experience replay ] starts\n",
      "[ episode 377 ][ timestamp 4 ] state=[-0.02306023 -0.57499784 -0.00332158  0.81878004], action=0, reward=1.0, next_state=[-0.03456019 -0.77007417  0.01305402  1.11041638]\n",
      "[ Experience replay ] starts\n",
      "[ episode 377 ][ timestamp 5 ] state=[-0.03456019 -0.77007417  0.01305402  1.11041638], action=0, reward=1.0, next_state=[-0.04996167 -0.96536518  0.03526235  1.4071657 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 377 ][ timestamp 6 ] state=[-0.04996167 -0.96536518  0.03526235  1.4071657 ], action=0, reward=1.0, next_state=[-0.06926898 -1.16090654  0.06340567  1.71066038]\n",
      "[ Experience replay ] starts\n",
      "[ episode 377 ][ timestamp 7 ] state=[-0.06926898 -1.16090654  0.06340567  1.71066038], action=0, reward=1.0, next_state=[-0.09248711 -1.35669696  0.09761887  2.02238465]\n",
      "[ Experience replay ] starts\n",
      "[ episode 377 ][ timestamp 8 ] state=[-0.09248711 -1.35669696  0.09761887  2.02238465], action=0, reward=1.0, next_state=[-0.11962105 -1.5526849   0.13806657  2.34362131]\n",
      "[ Experience replay ] starts\n",
      "[ episode 377 ][ timestamp 9 ] state=[-0.11962105 -1.5526849   0.13806657  2.34362131], action=0, reward=1.0, next_state=[-0.15067474 -1.74875262  0.18493899  2.67538694]\n",
      "[ Experience replay ] starts\n",
      "[ episode 377 ][ timestamp 10 ] state=[-0.15067474 -1.74875262  0.18493899  2.67538694], action=0, reward=-1.0, next_state=[-0.1856498  -1.94469789  0.23844673  3.01835545]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 377: Exploration_rate=0.01. Score=10.\n",
      "[ episode 378 ] state=[ 0.00377455 -0.00400807  0.04628002 -0.04300929]\n",
      "[ episode 378 ][ timestamp 1 ] state=[ 0.00377455 -0.00400807  0.04628002 -0.04300929], action=0, reward=1.0, next_state=[ 0.00369439 -0.19976206  0.04541983  0.26390876]\n",
      "[ Experience replay ] starts\n",
      "[ episode 378 ][ timestamp 2 ] state=[ 0.00369439 -0.19976206  0.04541983  0.26390876], action=0, reward=1.0, next_state=[-3.00850901e-04 -3.95501888e-01  5.06980080e-02  5.70564548e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 378 ][ timestamp 3 ] state=[-3.00850901e-04 -3.95501888e-01  5.06980080e-02  5.70564548e-01], action=0, reward=1.0, next_state=[-0.00821089 -0.59129679  0.0621093   0.87877838]\n",
      "[ Experience replay ] starts\n",
      "[ episode 378 ][ timestamp 4 ] state=[-0.00821089 -0.59129679  0.0621093   0.87877838], action=0, reward=1.0, next_state=[-0.02003682 -0.7872052   0.07968487  1.19032277]\n",
      "[ Experience replay ] starts\n",
      "[ episode 378 ][ timestamp 5 ] state=[-0.02003682 -0.7872052   0.07968487  1.19032277], action=0, reward=1.0, next_state=[-0.03578093 -0.98326419  0.10349132  1.50688063]\n",
      "[ Experience replay ] starts\n",
      "[ episode 378 ][ timestamp 6 ] state=[-0.03578093 -0.98326419  0.10349132  1.50688063], action=0, reward=1.0, next_state=[-0.05544621 -1.1794777   0.13362893  1.82999832]\n",
      "[ Experience replay ] starts\n",
      "[ episode 378 ][ timestamp 7 ] state=[-0.05544621 -1.1794777   0.13362893  1.82999832], action=0, reward=1.0, next_state=[-0.07903577 -1.37580307  0.1702289   2.16103107]\n",
      "[ Experience replay ] starts\n",
      "[ episode 378 ][ timestamp 8 ] state=[-0.07903577 -1.37580307  0.1702289   2.16103107], action=0, reward=-1.0, next_state=[-0.10655183 -1.57213533  0.21344952  2.50107872]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 378: Exploration_rate=0.01. Score=8.\n",
      "[ episode 379 ] state=[ 0.02668648 -0.04133266 -0.02360786 -0.00420709]\n",
      "[ episode 379 ][ timestamp 1 ] state=[ 0.02668648 -0.04133266 -0.02360786 -0.00420709], action=0, reward=1.0, next_state=[ 0.02585982 -0.23610823 -0.02369201  0.28093478]\n",
      "[ Experience replay ] starts\n",
      "[ episode 379 ][ timestamp 2 ] state=[ 0.02585982 -0.23610823 -0.02369201  0.28093478], action=0, reward=1.0, next_state=[ 0.02113766 -0.43088435 -0.01807331  0.56605218]\n",
      "[ Experience replay ] starts\n",
      "[ episode 379 ][ timestamp 3 ] state=[ 0.02113766 -0.43088435 -0.01807331  0.56605218], action=0, reward=1.0, next_state=[ 0.01251997 -0.62574815 -0.00675227  0.85298688]\n",
      "[ Experience replay ] starts\n",
      "[ episode 379 ][ timestamp 4 ] state=[ 0.01251997 -0.62574815 -0.00675227  0.85298688], action=0, reward=1.0, next_state=[ 5.00975739e-06 -8.20777413e-01  1.03074718e-02  1.14353895e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 379 ][ timestamp 5 ] state=[ 5.00975739e-06 -8.20777413e-01  1.03074718e-02  1.14353895e+00], action=0, reward=1.0, next_state=[-0.01641054 -1.01603251  0.03317825  1.43943638]\n",
      "[ Experience replay ] starts\n",
      "[ episode 379 ][ timestamp 6 ] state=[-0.01641054 -1.01603251  0.03317825  1.43943638], action=0, reward=1.0, next_state=[-0.03673119 -1.21154714  0.06196698  1.74229954]\n",
      "[ Experience replay ] starts\n",
      "[ episode 379 ][ timestamp 7 ] state=[-0.03673119 -1.21154714  0.06196698  1.74229954], action=0, reward=1.0, next_state=[-0.06096213 -1.40731718  0.09681297  2.05359761]\n",
      "[ Experience replay ] starts\n",
      "[ episode 379 ][ timestamp 8 ] state=[-0.06096213 -1.40731718  0.09681297  2.05359761], action=0, reward=1.0, next_state=[-0.08910848 -1.60328722  0.13788492  2.37459474]\n",
      "[ Experience replay ] starts\n",
      "[ episode 379 ][ timestamp 9 ] state=[-0.08910848 -1.60328722  0.13788492  2.37459474], action=0, reward=1.0, next_state=[-0.12117422 -1.79933455  0.18537682  2.70628451]\n",
      "[ Experience replay ] starts\n",
      "[ episode 379 ][ timestamp 10 ] state=[-0.12117422 -1.79933455  0.18537682  2.70628451], action=0, reward=-1.0, next_state=[-0.15716091 -1.99525056  0.23950251  3.04931272]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 379: Exploration_rate=0.01. Score=10.\n",
      "[ episode 380 ] state=[ 0.01407644  0.01696458 -0.02522911 -0.0097562 ]\n",
      "[ episode 380 ][ timestamp 1 ] state=[ 0.01407644  0.01696458 -0.02522911 -0.0097562 ], action=0, reward=1.0, next_state=[ 0.01441574 -0.17778664 -0.02542424  0.27486108]\n",
      "[ Experience replay ] starts\n",
      "[ episode 380 ][ timestamp 2 ] state=[ 0.01441574 -0.17778664 -0.02542424  0.27486108], action=0, reward=1.0, next_state=[ 0.01086    -0.37253679 -0.01992702  0.55941798]\n",
      "[ Experience replay ] starts\n",
      "[ episode 380 ][ timestamp 3 ] state=[ 0.01086    -0.37253679 -0.01992702  0.55941798], action=0, reward=1.0, next_state=[ 0.00340927 -0.56737346 -0.00873866  0.84575681]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 380 ][ timestamp 4 ] state=[ 0.00340927 -0.56737346 -0.00873866  0.84575681], action=0, reward=1.0, next_state=[-0.0079382  -0.7623751   0.00817648  1.13567897]\n",
      "[ Experience replay ] starts\n",
      "[ episode 380 ][ timestamp 5 ] state=[-0.0079382  -0.7623751   0.00817648  1.13567897], action=0, reward=1.0, next_state=[-0.0231857  -0.95760307  0.03089006  1.43091498]\n",
      "[ Experience replay ] starts\n",
      "[ episode 380 ][ timestamp 6 ] state=[-0.0231857  -0.95760307  0.03089006  1.43091498], action=0, reward=1.0, next_state=[-0.04233777 -1.1530924   0.05950836  1.73308933]\n",
      "[ Experience replay ] starts\n",
      "[ episode 380 ][ timestamp 7 ] state=[-0.04233777 -1.1530924   0.05950836  1.73308933], action=0, reward=1.0, next_state=[-0.06539961 -1.34884084  0.09417014  2.04367738]\n",
      "[ Experience replay ] starts\n",
      "[ episode 380 ][ timestamp 8 ] state=[-0.06539961 -1.34884084  0.09417014  2.04367738], action=0, reward=1.0, next_state=[-0.09237643 -1.54479543  0.13504369  2.36395205]\n",
      "[ Experience replay ] starts\n",
      "[ episode 380 ][ timestamp 9 ] state=[-0.09237643 -1.54479543  0.13504369  2.36395205], action=0, reward=1.0, next_state=[-0.12327234 -1.74083662  0.18232273  2.69491881]\n",
      "[ Experience replay ] starts\n",
      "[ episode 380 ][ timestamp 10 ] state=[-0.12327234 -1.74083662  0.18232273  2.69491881], action=0, reward=-1.0, next_state=[-0.15808907 -1.9367598   0.23622111  3.0372389 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 380: Exploration_rate=0.01. Score=10.\n",
      "[ episode 381 ] state=[ 0.02105984 -0.0238547   0.0176838   0.00956677]\n",
      "[ episode 381 ][ timestamp 1 ] state=[ 0.02105984 -0.0238547   0.0176838   0.00956677], action=0, reward=1.0, next_state=[ 0.02058275 -0.21922574  0.01787514  0.30777628]\n",
      "[ Experience replay ] starts\n",
      "[ episode 381 ][ timestamp 2 ] state=[ 0.02058275 -0.21922574  0.01787514  0.30777628], action=0, reward=1.0, next_state=[ 0.01619824 -0.41459778  0.02403067  0.60604252]\n",
      "[ Experience replay ] starts\n",
      "[ episode 381 ][ timestamp 3 ] state=[ 0.01619824 -0.41459778  0.02403067  0.60604252], action=0, reward=1.0, next_state=[ 0.00790628 -0.61004736  0.03615152  0.90619659]\n",
      "[ Experience replay ] starts\n",
      "[ episode 381 ][ timestamp 4 ] state=[ 0.00790628 -0.61004736  0.03615152  0.90619659], action=0, reward=1.0, next_state=[-0.00429467 -0.80563967  0.05427545  1.21001959]\n",
      "[ Experience replay ] starts\n",
      "[ episode 381 ][ timestamp 5 ] state=[-0.00429467 -0.80563967  0.05427545  1.21001959], action=0, reward=1.0, next_state=[-0.02040746 -1.00141884  0.07847584  1.51920504]\n",
      "[ Experience replay ] starts\n",
      "[ episode 381 ][ timestamp 6 ] state=[-0.02040746 -1.00141884  0.07847584  1.51920504], action=0, reward=1.0, next_state=[-0.04043584 -1.19739698  0.10885994  1.83531575]\n",
      "[ Experience replay ] starts\n",
      "[ episode 381 ][ timestamp 7 ] state=[-0.04043584 -1.19739698  0.10885994  1.83531575], action=0, reward=1.0, next_state=[-0.06438378 -1.3935414   0.14556626  2.15973244]\n",
      "[ Experience replay ] starts\n",
      "[ episode 381 ][ timestamp 8 ] state=[-0.06438378 -1.3935414   0.14556626  2.15973244], action=0, reward=1.0, next_state=[-0.0922546  -1.58975944  0.1887609   2.49359218]\n",
      "[ Experience replay ] starts\n",
      "[ episode 381 ][ timestamp 9 ] state=[-0.0922546  -1.58975944  0.1887609   2.49359218], action=0, reward=-1.0, next_state=[-0.12404979 -1.78588108  0.23863275  2.83771596]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 381: Exploration_rate=0.01. Score=9.\n",
      "[ episode 382 ] state=[-0.0135594  -0.01377565 -0.04866951 -0.01519499]\n",
      "[ episode 382 ][ timestamp 1 ] state=[-0.0135594  -0.01377565 -0.04866951 -0.01519499], action=0, reward=1.0, next_state=[-0.01383491 -0.20816706 -0.04897341  0.26174366]\n",
      "[ Experience replay ] starts\n",
      "[ episode 382 ][ timestamp 2 ] state=[-0.01383491 -0.20816706 -0.04897341  0.26174366], action=0, reward=1.0, next_state=[-0.01799826 -0.40255697 -0.04373854  0.5385865 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 382 ][ timestamp 3 ] state=[-0.01799826 -0.40255697 -0.04373854  0.5385865 ], action=0, reward=1.0, next_state=[-0.02604939 -0.59703761 -0.03296681  0.81717344]\n",
      "[ Experience replay ] starts\n",
      "[ episode 382 ][ timestamp 4 ] state=[-0.02604939 -0.59703761 -0.03296681  0.81717344], action=0, reward=1.0, next_state=[-0.03799015 -0.79169311 -0.01662334  1.09930756]\n",
      "[ Experience replay ] starts\n",
      "[ episode 382 ][ timestamp 5 ] state=[-0.03799015 -0.79169311 -0.01662334  1.09930756], action=0, reward=1.0, next_state=[-0.05382401 -0.98659236  0.00536281  1.386729  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 382 ][ timestamp 6 ] state=[-0.05382401 -0.98659236  0.00536281  1.386729  ], action=0, reward=1.0, next_state=[-0.07355586 -1.18178075  0.03309739  1.68108403]\n",
      "[ Experience replay ] starts\n",
      "[ episode 382 ][ timestamp 7 ] state=[-0.07355586 -1.18178075  0.03309739  1.68108403], action=0, reward=1.0, next_state=[-0.09719147 -1.37727012  0.06671908  1.98388635]\n",
      "[ Experience replay ] starts\n",
      "[ episode 382 ][ timestamp 8 ] state=[-0.09719147 -1.37727012  0.06671908  1.98388635], action=0, reward=1.0, next_state=[-0.12473687 -1.57302642  0.1063968   2.29646835]\n",
      "[ Experience replay ] starts\n",
      "[ episode 382 ][ timestamp 9 ] state=[-0.12473687 -1.57302642  0.1063968   2.29646835], action=0, reward=1.0, next_state=[-0.1561974  -1.76895469  0.15232617  2.61992054]\n",
      "[ Experience replay ] starts\n",
      "[ episode 382 ][ timestamp 10 ] state=[-0.1561974  -1.76895469  0.15232617  2.61992054], action=0, reward=1.0, next_state=[-0.1915765  -1.96488139  0.20472458  2.95501847]\n",
      "[ Experience replay ] starts\n",
      "[ episode 382 ][ timestamp 11 ] state=[-0.1915765  -1.96488139  0.20472458  2.95501847], action=0, reward=-1.0, next_state=[-0.23087412 -2.16053443  0.26382495  3.30213877]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 382: Exploration_rate=0.01. Score=11.\n",
      "[ episode 383 ] state=[-0.02667823 -0.02239001  0.00012285  0.01136863]\n",
      "[ episode 383 ][ timestamp 1 ] state=[-0.02667823 -0.02239001  0.00012285  0.01136863], action=0, reward=1.0, next_state=[-0.02712603 -0.21751372  0.00035022  0.30409032]\n",
      "[ Experience replay ] starts\n",
      "[ episode 383 ][ timestamp 2 ] state=[-0.02712603 -0.21751372  0.00035022  0.30409032], action=0, reward=1.0, next_state=[-0.03147631 -0.41264066  0.00643203  0.59688367]\n",
      "[ Experience replay ] starts\n",
      "[ episode 383 ][ timestamp 3 ] state=[-0.03147631 -0.41264066  0.00643203  0.59688367], action=0, reward=1.0, next_state=[-0.03972912 -0.60785203  0.0183697   0.89158567]\n",
      "[ Experience replay ] starts\n",
      "[ episode 383 ][ timestamp 4 ] state=[-0.03972912 -0.60785203  0.0183697   0.89158567], action=0, reward=1.0, next_state=[-0.05188616 -0.8032183   0.03620142  1.18998602]\n",
      "[ Experience replay ] starts\n",
      "[ episode 383 ][ timestamp 5 ] state=[-0.05188616 -0.8032183   0.03620142  1.18998602], action=1, reward=1.0, next_state=[-0.06795053 -0.60858373  0.06000114  0.90886635]\n",
      "[ Experience replay ] starts\n",
      "[ episode 383 ][ timestamp 6 ] state=[-0.06795053 -0.60858373  0.06000114  0.90886635], action=0, reward=1.0, next_state=[-0.0801222  -0.80446426  0.07817846  1.21978815]\n",
      "[ Experience replay ] starts\n",
      "[ episode 383 ][ timestamp 7 ] state=[-0.0801222  -0.80446426  0.07817846  1.21978815], action=0, reward=1.0, next_state=[-0.09621148 -1.00050198  0.10257423  1.53590764]\n",
      "[ Experience replay ] starts\n",
      "[ episode 383 ][ timestamp 8 ] state=[-0.09621148 -1.00050198  0.10257423  1.53590764], action=0, reward=1.0, next_state=[-0.11622152 -1.19669853  0.13329238  1.85875957]\n",
      "[ Experience replay ] starts\n",
      "[ episode 383 ][ timestamp 9 ] state=[-0.11622152 -1.19669853  0.13329238  1.85875957], action=0, reward=1.0, next_state=[-0.1401555  -1.39300787  0.17046757  2.18968362]\n",
      "[ Experience replay ] starts\n",
      "[ episode 383 ][ timestamp 10 ] state=[-0.1401555  -1.39300787  0.17046757  2.18968362], action=0, reward=-1.0, next_state=[-0.16801565 -1.58932054  0.21426124  2.52975957]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 383: Exploration_rate=0.01. Score=10.\n",
      "[ episode 384 ] state=[ 0.04442253 -0.03423677 -0.03778446 -0.0046924 ]\n",
      "[ episode 384 ][ timestamp 1 ] state=[ 0.04442253 -0.03423677 -0.03778446 -0.0046924 ], action=0, reward=1.0, next_state=[ 0.04373779 -0.22879704 -0.03787831  0.27583371]\n",
      "[ Experience replay ] starts\n",
      "[ episode 384 ][ timestamp 2 ] state=[ 0.04373779 -0.22879704 -0.03787831  0.27583371], action=0, reward=1.0, next_state=[ 0.03916185 -0.42335868 -0.03236164  0.55633326]\n",
      "[ Experience replay ] starts\n",
      "[ episode 384 ][ timestamp 3 ] state=[ 0.03916185 -0.42335868 -0.03236164  0.55633326], action=0, reward=1.0, next_state=[ 0.03069468 -0.6180117  -0.02123497  0.83864726]\n",
      "[ Experience replay ] starts\n",
      "[ episode 384 ][ timestamp 4 ] state=[ 0.03069468 -0.6180117  -0.02123497  0.83864726], action=0, reward=1.0, next_state=[ 0.01833444 -0.81283734 -0.00446203  1.12457723]\n",
      "[ Experience replay ] starts\n",
      "[ episode 384 ][ timestamp 5 ] state=[ 0.01833444 -0.81283734 -0.00446203  1.12457723], action=0, reward=1.0, next_state=[ 0.0020777  -1.00790052  0.01802952  1.41585725]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 384 ][ timestamp 6 ] state=[ 0.0020777  -1.00790052  0.01802952  1.41585725], action=0, reward=1.0, next_state=[-0.01808032 -1.20324108  0.04634666  1.71412086]\n",
      "[ Experience replay ] starts\n",
      "[ episode 384 ][ timestamp 7 ] state=[-0.01808032 -1.20324108  0.04634666  1.71412086], action=0, reward=1.0, next_state=[-0.04214514 -1.39886323  0.08062908  2.02086003]\n",
      "[ Experience replay ] starts\n",
      "[ episode 384 ][ timestamp 8 ] state=[-0.04214514 -1.39886323  0.08062908  2.02086003], action=0, reward=1.0, next_state=[-0.0701224  -1.59472265  0.12104628  2.33737398]\n",
      "[ Experience replay ] starts\n",
      "[ episode 384 ][ timestamp 9 ] state=[-0.0701224  -1.59472265  0.12104628  2.33737398], action=0, reward=1.0, next_state=[-0.10201685 -1.79071097  0.16779376  2.6647061 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 384 ][ timestamp 10 ] state=[-0.10201685 -1.79071097  0.16779376  2.6647061 ], action=0, reward=-1.0, next_state=[-0.13783107 -1.98663762  0.22108788  3.00356878]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 384: Exploration_rate=0.01. Score=10.\n",
      "[ episode 385 ] state=[ 0.04395201 -0.04532626 -0.01418438 -0.04444648]\n",
      "[ episode 385 ][ timestamp 1 ] state=[ 0.04395201 -0.04532626 -0.01418438 -0.04444648], action=0, reward=1.0, next_state=[ 0.04304548 -0.24024197 -0.01507331  0.2437276 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 385 ][ timestamp 2 ] state=[ 0.04304548 -0.24024197 -0.01507331  0.2437276 ], action=0, reward=1.0, next_state=[ 0.03824064 -0.43514541 -0.01019876  0.53161817]\n",
      "[ Experience replay ] starts\n",
      "[ episode 385 ][ timestamp 3 ] state=[ 0.03824064 -0.43514541 -0.01019876  0.53161817], action=0, reward=1.0, next_state=[ 2.95377366e-02 -6.30122438e-01  4.33601414e-04  8.21070113e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 385 ][ timestamp 4 ] state=[ 2.95377366e-02 -6.30122438e-01  4.33601414e-04  8.21070113e-01], action=0, reward=1.0, next_state=[ 0.01693529 -0.82525032  0.016855    1.11388939]\n",
      "[ Experience replay ] starts\n",
      "[ episode 385 ][ timestamp 5 ] state=[ 0.01693529 -0.82525032  0.016855    1.11388939], action=0, reward=1.0, next_state=[ 4.30281413e-04 -1.02058949e+00  3.91327914e-02  1.41181166e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 385 ][ timestamp 6 ] state=[ 4.30281413e-04 -1.02058949e+00  3.91327914e-02  1.41181166e+00], action=0, reward=1.0, next_state=[-0.01998151 -1.21617411  0.06736902  1.71646608]\n",
      "[ Experience replay ] starts\n",
      "[ episode 385 ][ timestamp 7 ] state=[-0.01998151 -1.21617411  0.06736902  1.71646608], action=0, reward=1.0, next_state=[-0.04430499 -1.41200087  0.10169835  2.02933142]\n",
      "[ Experience replay ] starts\n",
      "[ episode 385 ][ timestamp 8 ] state=[-0.04430499 -1.41200087  0.10169835  2.02933142], action=0, reward=1.0, next_state=[-0.07254501 -1.60801557  0.14228497  2.3516821 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 385 ][ timestamp 9 ] state=[-0.07254501 -1.60801557  0.14228497  2.3516821 ], action=0, reward=1.0, next_state=[-0.10470532 -1.80409704  0.18931862  2.68452286]\n",
      "[ Experience replay ] starts\n",
      "[ episode 385 ][ timestamp 10 ] state=[-0.10470532 -1.80409704  0.18931862  2.68452286], action=0, reward=-1.0, next_state=[-0.14078726 -2.00003879  0.24300907  3.02851183]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 385: Exploration_rate=0.01. Score=10.\n",
      "[ episode 386 ] state=[ 0.04203509  0.0237281  -0.00472403  0.01567174]\n",
      "[ episode 386 ][ timestamp 1 ] state=[ 0.04203509  0.0237281  -0.00472403  0.01567174], action=0, reward=1.0, next_state=[ 0.04250965 -0.17132579 -0.0044106   0.30686045]\n",
      "[ Experience replay ] starts\n",
      "[ episode 386 ][ timestamp 2 ] state=[ 0.04250965 -0.17132579 -0.0044106   0.30686045], action=0, reward=1.0, next_state=[ 0.03908314 -0.36638461  0.00172661  0.59814913]\n",
      "[ Experience replay ] starts\n",
      "[ episode 386 ][ timestamp 3 ] state=[ 0.03908314 -0.36638461  0.00172661  0.59814913], action=0, reward=1.0, next_state=[ 0.03175545 -0.56153068  0.01368959  0.89137541]\n",
      "[ Experience replay ] starts\n",
      "[ episode 386 ][ timestamp 4 ] state=[ 0.03175545 -0.56153068  0.01368959  0.89137541], action=0, reward=1.0, next_state=[ 0.02052483 -0.75683565  0.0315171   1.18833003]\n",
      "[ Experience replay ] starts\n",
      "[ episode 386 ][ timestamp 5 ] state=[ 0.02052483 -0.75683565  0.0315171   1.18833003], action=0, reward=1.0, next_state=[ 0.00538812 -0.95235168  0.0552837   1.49072293]\n",
      "[ Experience replay ] starts\n",
      "[ episode 386 ][ timestamp 6 ] state=[ 0.00538812 -0.95235168  0.0552837   1.49072293], action=0, reward=1.0, next_state=[-0.01365891 -1.14810134  0.08509816  1.80014396]\n",
      "[ Experience replay ] starts\n",
      "[ episode 386 ][ timestamp 7 ] state=[-0.01365891 -1.14810134  0.08509816  1.80014396], action=0, reward=1.0, next_state=[-0.03662094 -1.34406554  0.12110104  2.11801524]\n",
      "[ Experience replay ] starts\n",
      "[ episode 386 ][ timestamp 8 ] state=[-0.03662094 -1.34406554  0.12110104  2.11801524], action=0, reward=1.0, next_state=[-0.06350225 -1.54016921  0.16346134  2.44553315]\n",
      "[ Experience replay ] starts\n",
      "[ episode 386 ][ timestamp 9 ] state=[-0.06350225 -1.54016921  0.16346134  2.44553315], action=0, reward=-1.0, next_state=[-0.09430564 -1.7362644   0.21237201  2.7835989 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 386: Exploration_rate=0.01. Score=9.\n",
      "[ episode 387 ] state=[-0.03650584 -0.00536431  0.00224658  0.02259196]\n",
      "[ episode 387 ][ timestamp 1 ] state=[-0.03650584 -0.00536431  0.00224658  0.02259196], action=0, reward=1.0, next_state=[-0.03661312 -0.2005184   0.00269842  0.31598286]\n",
      "[ Experience replay ] starts\n",
      "[ episode 387 ][ timestamp 2 ] state=[-0.03661312 -0.2005184   0.00269842  0.31598286], action=0, reward=1.0, next_state=[-0.04062349 -0.39567869  0.00901808  0.60951556]\n",
      "[ Experience replay ] starts\n",
      "[ episode 387 ][ timestamp 3 ] state=[-0.04062349 -0.39567869  0.00901808  0.60951556], action=0, reward=1.0, next_state=[-0.04853706 -0.59092553  0.02120839  0.9050252 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 387 ][ timestamp 4 ] state=[-0.04853706 -0.59092553  0.02120839  0.9050252 ], action=0, reward=1.0, next_state=[-0.06035557 -0.78632818  0.03930889  1.20429805]\n",
      "[ Experience replay ] starts\n",
      "[ episode 387 ][ timestamp 5 ] state=[-0.06035557 -0.78632818  0.03930889  1.20429805], action=0, reward=1.0, next_state=[-0.07608214 -0.98193558  0.06339486  1.50903634]\n",
      "[ Experience replay ] starts\n",
      "[ episode 387 ][ timestamp 6 ] state=[-0.07608214 -0.98193558  0.06339486  1.50903634], action=0, reward=1.0, next_state=[-0.09572085 -1.17776602  0.09357558  1.82081753]\n",
      "[ Experience replay ] starts\n",
      "[ episode 387 ][ timestamp 7 ] state=[-0.09572085 -1.17776602  0.09357558  1.82081753], action=0, reward=1.0, next_state=[-0.11927617 -1.3737947   0.12999193  2.1410452 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 387 ][ timestamp 8 ] state=[-0.11927617 -1.3737947   0.12999193  2.1410452 ], action=0, reward=1.0, next_state=[-0.14675206 -1.56993912  0.17281284  2.47088958]\n",
      "[ Experience replay ] starts\n",
      "[ episode 387 ][ timestamp 9 ] state=[-0.14675206 -1.56993912  0.17281284  2.47088958], action=0, reward=-1.0, next_state=[-0.17815085 -1.76604191  0.22223063  2.81121681]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 387: Exploration_rate=0.01. Score=9.\n",
      "[ episode 388 ] state=[-0.02148553  0.0238992   0.01967705  0.04651592]\n",
      "[ episode 388 ][ timestamp 1 ] state=[-0.02148553  0.0238992   0.01967705  0.04651592], action=0, reward=1.0, next_state=[-0.02100755 -0.1714993   0.02060737  0.34534161]\n",
      "[ Experience replay ] starts\n",
      "[ episode 388 ][ timestamp 2 ] state=[-0.02100755 -0.1714993   0.02060737  0.34534161], action=0, reward=1.0, next_state=[-0.02443753 -0.36690824  0.0275142   0.64445092]\n",
      "[ Experience replay ] starts\n",
      "[ episode 388 ][ timestamp 3 ] state=[-0.02443753 -0.36690824  0.0275142   0.64445092], action=0, reward=1.0, next_state=[-0.0317757  -0.56240261  0.04040322  0.94566965]\n",
      "[ Experience replay ] starts\n",
      "[ episode 388 ][ timestamp 4 ] state=[-0.0317757  -0.56240261  0.04040322  0.94566965], action=0, reward=1.0, next_state=[-0.04302375 -0.75804478  0.05931661  1.25076871]\n",
      "[ Experience replay ] starts\n",
      "[ episode 388 ][ timestamp 5 ] state=[-0.04302375 -0.75804478  0.05931661  1.25076871], action=0, reward=1.0, next_state=[-0.05818465 -0.95387458  0.08433198  1.56142567]\n",
      "[ Experience replay ] starts\n",
      "[ episode 388 ][ timestamp 6 ] state=[-0.05818465 -0.95387458  0.08433198  1.56142567], action=0, reward=1.0, next_state=[-0.07726214 -1.14989814  0.1155605   1.87918027]\n",
      "[ Experience replay ] starts\n",
      "[ episode 388 ][ timestamp 7 ] state=[-0.07726214 -1.14989814  0.1155605   1.87918027], action=0, reward=1.0, next_state=[-0.1002601  -1.3460746   0.1531441   2.20538152]\n",
      "[ Experience replay ] starts\n",
      "[ episode 388 ][ timestamp 8 ] state=[-0.1002601  -1.3460746   0.1531441   2.20538152], action=0, reward=1.0, next_state=[-0.12718159 -1.54230072  0.19725173  2.54112444]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 388 ][ timestamp 9 ] state=[-0.12718159 -1.54230072  0.19725173  2.54112444], action=0, reward=-1.0, next_state=[-0.15802761 -1.73839307  0.24807422  2.88717597]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 388: Exploration_rate=0.01. Score=9.\n",
      "[ episode 389 ] state=[0.00574797 0.0184636  0.03173773 0.02296388]\n",
      "[ episode 389 ][ timestamp 1 ] state=[0.00574797 0.0184636  0.03173773 0.02296388], action=0, reward=1.0, next_state=[ 0.00611724 -0.17709878  0.03219701  0.32548906]\n",
      "[ Experience replay ] starts\n",
      "[ episode 389 ][ timestamp 2 ] state=[ 0.00611724 -0.17709878  0.03219701  0.32548906], action=0, reward=1.0, next_state=[ 0.00257527 -0.37266401  0.03870679  0.62814915]\n",
      "[ Experience replay ] starts\n",
      "[ episode 389 ][ timestamp 3 ] state=[ 0.00257527 -0.37266401  0.03870679  0.62814915], action=0, reward=1.0, next_state=[-0.00487801 -0.56830419  0.05126977  0.93276656]\n",
      "[ Experience replay ] starts\n",
      "[ episode 389 ][ timestamp 4 ] state=[-0.00487801 -0.56830419  0.05126977  0.93276656], action=0, reward=1.0, next_state=[-0.0162441  -0.76407901  0.0699251   1.24110963]\n",
      "[ Experience replay ] starts\n",
      "[ episode 389 ][ timestamp 5 ] state=[-0.0162441  -0.76407901  0.0699251   1.24110963], action=0, reward=1.0, next_state=[-0.03152568 -0.96002554  0.0947473   1.55485239]\n",
      "[ Experience replay ] starts\n",
      "[ episode 389 ][ timestamp 6 ] state=[-0.03152568 -0.96002554  0.0947473   1.55485239], action=0, reward=1.0, next_state=[-0.05072619 -1.15614661  0.12584434  1.87552859]\n",
      "[ Experience replay ] starts\n",
      "[ episode 389 ][ timestamp 7 ] state=[-0.05072619 -1.15614661  0.12584434  1.87552859], action=0, reward=1.0, next_state=[-0.07384912 -1.35239741  0.16335491  2.20447753]\n",
      "[ Experience replay ] starts\n",
      "[ episode 389 ][ timestamp 8 ] state=[-0.07384912 -1.35239741  0.16335491  2.20447753], action=0, reward=1.0, next_state=[-0.10089707 -1.54866976  0.20744447  2.5427797 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 389 ][ timestamp 9 ] state=[-0.10089707 -1.54866976  0.20744447  2.5427797 ], action=0, reward=-1.0, next_state=[-0.13187046 -1.74477427  0.25830006  2.89118206]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 389: Exploration_rate=0.01. Score=9.\n",
      "[ episode 390 ] state=[0.03577995 0.02675959 0.03264444 0.00290604]\n",
      "[ episode 390 ][ timestamp 1 ] state=[0.03577995 0.02675959 0.03264444 0.00290604], action=0, reward=1.0, next_state=[ 0.03631514 -0.16881496  0.03270256  0.30570732]\n",
      "[ Experience replay ] starts\n",
      "[ episode 390 ][ timestamp 2 ] state=[ 0.03631514 -0.16881496  0.03270256  0.30570732], action=0, reward=1.0, next_state=[ 0.03293885 -0.3643873   0.03881671  0.60852182]\n",
      "[ Experience replay ] starts\n",
      "[ episode 390 ][ timestamp 3 ] state=[ 0.03293885 -0.3643873   0.03881671  0.60852182], action=0, reward=1.0, next_state=[ 0.0256511  -0.5600298   0.05098715  0.91317376]\n",
      "[ Experience replay ] starts\n",
      "[ episode 390 ][ timestamp 4 ] state=[ 0.0256511  -0.5600298   0.05098715  0.91317376], action=0, reward=1.0, next_state=[ 0.0144505  -0.75580304  0.06925062  1.22143573]\n",
      "[ Experience replay ] starts\n",
      "[ episode 390 ][ timestamp 5 ] state=[ 0.0144505  -0.75580304  0.06925062  1.22143573], action=0, reward=1.0, next_state=[-6.65557606e-04 -9.51745616e-01  9.36793366e-02  1.53498853e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 390 ][ timestamp 6 ] state=[-6.65557606e-04 -9.51745616e-01  9.36793366e-02  1.53498853e+00], action=0, reward=1.0, next_state=[-0.01970047 -1.14786263  0.12437911  1.85537563]\n",
      "[ Experience replay ] starts\n",
      "[ episode 390 ][ timestamp 7 ] state=[-0.01970047 -1.14786263  0.12437911  1.85537563], action=0, reward=1.0, next_state=[-0.04265772 -1.34411237  0.16148662  2.18394941]\n",
      "[ Experience replay ] starts\n",
      "[ episode 390 ][ timestamp 8 ] state=[-0.04265772 -1.34411237  0.16148662  2.18394941], action=0, reward=1.0, next_state=[-0.06953997 -1.54039075  0.20516561  2.5218074 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 390 ][ timestamp 9 ] state=[-0.06953997 -1.54039075  0.20516561  2.5218074 ], action=0, reward=-1.0, next_state=[-0.10034778 -1.73651356  0.25560176  2.86971818]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 390: Exploration_rate=0.01. Score=9.\n",
      "[ episode 391 ] state=[ 0.02220055 -0.03402163  0.04348112  0.02993309]\n",
      "[ episode 391 ][ timestamp 1 ] state=[ 0.02220055 -0.03402163  0.04348112  0.02993309], action=0, reward=1.0, next_state=[ 0.02152012 -0.22973928  0.04407978  0.33601152]\n",
      "[ Experience replay ] starts\n",
      "[ episode 391 ][ timestamp 2 ] state=[ 0.02152012 -0.22973928  0.04407978  0.33601152], action=0, reward=1.0, next_state=[ 0.01692533 -0.42545992  0.05080001  0.64226257]\n",
      "[ Experience replay ] starts\n",
      "[ episode 391 ][ timestamp 3 ] state=[ 0.01692533 -0.42545992  0.05080001  0.64226257], action=0, reward=1.0, next_state=[ 0.00841613 -0.62125179  0.06364527  0.95050028]\n",
      "[ Experience replay ] starts\n",
      "[ episode 391 ][ timestamp 4 ] state=[ 0.00841613 -0.62125179  0.06364527  0.95050028], action=0, reward=1.0, next_state=[-0.0040089  -0.81716999  0.08265527  1.26248166]\n",
      "[ Experience replay ] starts\n",
      "[ episode 391 ][ timestamp 5 ] state=[-0.0040089  -0.81716999  0.08265527  1.26248166], action=0, reward=1.0, next_state=[-0.0203523  -1.01324578  0.10790491  1.57986422]\n",
      "[ Experience replay ] starts\n",
      "[ episode 391 ][ timestamp 6 ] state=[-0.0203523  -1.01324578  0.10790491  1.57986422], action=0, reward=1.0, next_state=[-0.04061722 -1.20947448  0.13950219  1.90415786]\n",
      "[ Experience replay ] starts\n",
      "[ episode 391 ][ timestamp 7 ] state=[-0.04061722 -1.20947448  0.13950219  1.90415786], action=0, reward=1.0, next_state=[-0.06480671 -1.40580161  0.17758535  2.23666843]\n",
      "[ Experience replay ] starts\n",
      "[ episode 391 ][ timestamp 8 ] state=[-0.06480671 -1.40580161  0.17758535  2.23666843], action=0, reward=-1.0, next_state=[-0.09292274 -1.60210676  0.22231872  2.57843135]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 391: Exploration_rate=0.01. Score=8.\n",
      "[ episode 392 ] state=[-0.0060234  -0.02809661 -0.01609852 -0.02402387]\n",
      "[ episode 392 ][ timestamp 1 ] state=[-0.0060234  -0.02809661 -0.01609852 -0.02402387], action=0, reward=1.0, next_state=[-0.00658533 -0.22298404 -0.016579    0.26353663]\n",
      "[ Experience replay ] starts\n",
      "[ episode 392 ][ timestamp 2 ] state=[-0.00658533 -0.22298404 -0.016579    0.26353663], action=0, reward=1.0, next_state=[-0.01104501 -0.41786547 -0.01130826  0.5509446 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 392 ][ timestamp 3 ] state=[-0.01104501 -0.41786547 -0.01130826  0.5509446 ], action=0, reward=1.0, next_state=[-1.94023188e-02 -6.12826783e-01 -2.89371473e-04  8.40043317e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 392 ][ timestamp 4 ] state=[-1.94023188e-02 -6.12826783e-01 -2.89371473e-04  8.40043317e-01], action=0, reward=1.0, next_state=[-0.03165885 -0.80794478  0.01651149  1.13263523]\n",
      "[ Experience replay ] starts\n",
      "[ episode 392 ][ timestamp 5 ] state=[-0.03165885 -0.80794478  0.01651149  1.13263523], action=0, reward=1.0, next_state=[-0.04781775 -1.00327893  0.0391642   1.43045067]\n",
      "[ Experience replay ] starts\n",
      "[ episode 392 ][ timestamp 6 ] state=[-0.04781775 -1.00327893  0.0391642   1.43045067], action=0, reward=1.0, next_state=[-0.06788333 -1.19886188  0.06777321  1.73511145]\n",
      "[ Experience replay ] starts\n",
      "[ episode 392 ][ timestamp 7 ] state=[-0.06788333 -1.19886188  0.06777321  1.73511145], action=0, reward=1.0, next_state=[-0.09186057 -1.3946882   0.10247544  2.04808666]\n",
      "[ Experience replay ] starts\n",
      "[ episode 392 ][ timestamp 8 ] state=[-0.09186057 -1.3946882   0.10247544  2.04808666], action=0, reward=1.0, next_state=[-0.11975433 -1.59070081  0.14343718  2.37063824]\n",
      "[ Experience replay ] starts\n",
      "[ episode 392 ][ timestamp 9 ] state=[-0.11975433 -1.59070081  0.14343718  2.37063824], action=0, reward=1.0, next_state=[-0.15156835 -1.78677485  0.19084994  2.703755  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 392 ][ timestamp 10 ] state=[-0.15156835 -1.78677485  0.19084994  2.703755  ], action=0, reward=-1.0, next_state=[-0.18730384 -1.98269914  0.24492504  3.04807533]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 392: Exploration_rate=0.01. Score=10.\n",
      "[ episode 393 ] state=[-0.02379255  0.02334618 -0.02989493  0.02165392]\n",
      "[ episode 393 ][ timestamp 1 ] state=[-0.02379255  0.02334618 -0.02989493  0.02165392], action=0, reward=1.0, next_state=[-0.02332563 -0.17133458 -0.02946186  0.30475678]\n",
      "[ Experience replay ] starts\n",
      "[ episode 393 ][ timestamp 2 ] state=[-0.02332563 -0.17133458 -0.02946186  0.30475678], action=0, reward=1.0, next_state=[-0.02675232 -0.36602456 -0.02336672  0.58800447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 393 ][ timestamp 3 ] state=[-0.02675232 -0.36602456 -0.02336672  0.58800447], action=0, reward=1.0, next_state=[-0.03407281 -0.56081162 -0.01160663  0.87323611]\n",
      "[ Experience replay ] starts\n",
      "[ episode 393 ][ timestamp 4 ] state=[-0.03407281 -0.56081162 -0.01160663  0.87323611], action=0, reward=1.0, next_state=[-0.04528904 -0.75577384  0.00585809  1.16224748]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 393 ][ timestamp 5 ] state=[-0.04528904 -0.75577384  0.00585809  1.16224748], action=0, reward=1.0, next_state=[-0.06040452 -0.95097159  0.02910304  1.45676135]\n",
      "[ Experience replay ] starts\n",
      "[ episode 393 ][ timestamp 6 ] state=[-0.06040452 -0.95097159  0.02910304  1.45676135], action=0, reward=1.0, next_state=[-0.07942395 -1.14643833  0.05823827  1.75839239]\n",
      "[ Experience replay ] starts\n",
      "[ episode 393 ][ timestamp 7 ] state=[-0.07942395 -1.14643833  0.05823827  1.75839239], action=0, reward=1.0, next_state=[-0.10235272 -1.34216952  0.09340612  2.06860379]\n",
      "[ Experience replay ] starts\n",
      "[ episode 393 ][ timestamp 8 ] state=[-0.10235272 -1.34216952  0.09340612  2.06860379], action=0, reward=1.0, next_state=[-0.12919611 -1.53810917  0.13477819  2.38865354]\n",
      "[ Experience replay ] starts\n",
      "[ episode 393 ][ timestamp 9 ] state=[-0.12919611 -1.53810917  0.13477819  2.38865354], action=0, reward=1.0, next_state=[-0.15995829 -1.73413377  0.18255126  2.71952879]\n",
      "[ Experience replay ] starts\n",
      "[ episode 393 ][ timestamp 10 ] state=[-0.15995829 -1.73413377  0.18255126  2.71952879], action=0, reward=-1.0, next_state=[-0.19464097 -1.9300337   0.23694184  3.06186849]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 393: Exploration_rate=0.01. Score=10.\n",
      "[ episode 394 ] state=[-0.02651149 -0.03486877 -0.03903556  0.02588279]\n",
      "[ episode 394 ][ timestamp 1 ] state=[-0.02651149 -0.03486877 -0.03903556  0.02588279], action=0, reward=1.0, next_state=[-0.02720886 -0.2294098  -0.03851791  0.30599851]\n",
      "[ Experience replay ] starts\n",
      "[ episode 394 ][ timestamp 2 ] state=[-0.02720886 -0.2294098  -0.03851791  0.30599851], action=0, reward=1.0, next_state=[-0.03179706 -0.42396231 -0.03239794  0.58628934]\n",
      "[ Experience replay ] starts\n",
      "[ episode 394 ][ timestamp 3 ] state=[-0.03179706 -0.42396231 -0.03239794  0.58628934], action=0, reward=1.0, next_state=[-0.04027631 -0.61861587 -0.02067215  0.86859313]\n",
      "[ Experience replay ] starts\n",
      "[ episode 394 ][ timestamp 4 ] state=[-0.04027631 -0.61861587 -0.02067215  0.86859313], action=0, reward=1.0, next_state=[-0.05264862 -0.81345056 -0.00330029  1.15470555]\n",
      "[ Experience replay ] starts\n",
      "[ episode 394 ][ timestamp 5 ] state=[-0.05264862 -0.81345056 -0.00330029  1.15470555], action=0, reward=1.0, next_state=[-0.06891764 -1.00852932  0.01979382  1.44635181]\n",
      "[ Experience replay ] starts\n",
      "[ episode 394 ][ timestamp 6 ] state=[-0.06891764 -1.00852932  0.01979382  1.44635181], action=0, reward=1.0, next_state=[-0.08908822 -1.20388907  0.04872086  1.74515304]\n",
      "[ Experience replay ] starts\n",
      "[ episode 394 ][ timestamp 7 ] state=[-0.08908822 -1.20388907  0.04872086  1.74515304], action=0, reward=1.0, next_state=[-0.113166   -1.39952999  0.08362392  2.05258445]\n",
      "[ Experience replay ] starts\n",
      "[ episode 394 ][ timestamp 8 ] state=[-0.113166   -1.39952999  0.08362392  2.05258445], action=0, reward=1.0, next_state=[-0.1411566  -1.59540233  0.12467561  2.36992306]\n",
      "[ Experience replay ] starts\n",
      "[ episode 394 ][ timestamp 9 ] state=[-0.1411566  -1.59540233  0.12467561  2.36992306], action=0, reward=1.0, next_state=[-0.17306465 -1.79139069  0.17207407  2.69818346]\n",
      "[ Experience replay ] starts\n",
      "[ episode 394 ][ timestamp 10 ] state=[-0.17306465 -1.79139069  0.17207407  2.69818346], action=0, reward=-1.0, next_state=[-0.20889246 -1.98729558  0.22603774  3.03804154]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 394: Exploration_rate=0.01. Score=10.\n",
      "[ episode 395 ] state=[ 0.00949625  0.00803086 -0.00724516  0.00613726]\n",
      "[ episode 395 ][ timestamp 1 ] state=[ 0.00949625  0.00803086 -0.00724516  0.00613726], action=0, reward=1.0, next_state=[ 0.00965687 -0.18698644 -0.00712242  0.29652547]\n",
      "[ Experience replay ] starts\n",
      "[ episode 395 ][ timestamp 2 ] state=[ 0.00965687 -0.18698644 -0.00712242  0.29652547], action=0, reward=1.0, next_state=[ 0.00591714 -0.38200614 -0.00119191  0.58695362]\n",
      "[ Experience replay ] starts\n",
      "[ episode 395 ][ timestamp 3 ] state=[ 0.00591714 -0.38200614 -0.00119191  0.58695362], action=0, reward=1.0, next_state=[-0.00172299 -0.57711138  0.01054716  0.87926085]\n",
      "[ Experience replay ] starts\n",
      "[ episode 395 ][ timestamp 4 ] state=[-0.00172299 -0.57711138  0.01054716  0.87926085], action=0, reward=1.0, next_state=[-0.01326521 -0.77237504  0.02813238  1.17524085]\n",
      "[ Experience replay ] starts\n",
      "[ episode 395 ][ timestamp 5 ] state=[-0.01326521 -0.77237504  0.02813238  1.17524085], action=0, reward=1.0, next_state=[-0.02871271 -0.96785101  0.0516372   1.47660862]\n",
      "[ Experience replay ] starts\n",
      "[ episode 395 ][ timestamp 6 ] state=[-0.02871271 -0.96785101  0.0516372   1.47660862], action=0, reward=1.0, next_state=[-0.04806973 -1.16356426  0.08116937  1.78496178]\n",
      "[ Experience replay ] starts\n",
      "[ episode 395 ][ timestamp 7 ] state=[-0.04806973 -1.16356426  0.08116937  1.78496178], action=0, reward=1.0, next_state=[-0.07134102 -1.35949891  0.1168686   2.10173371]\n",
      "[ Experience replay ] starts\n",
      "[ episode 395 ][ timestamp 8 ] state=[-0.07134102 -1.35949891  0.1168686   2.10173371], action=0, reward=1.0, next_state=[-0.098531   -1.55558413  0.15890328  2.42813639]\n",
      "[ Experience replay ] starts\n",
      "[ episode 395 ][ timestamp 9 ] state=[-0.098531   -1.55558413  0.15890328  2.42813639], action=0, reward=1.0, next_state=[-0.12964268 -1.75167738  0.20746601  2.76509173]\n",
      "[ Experience replay ] starts\n",
      "[ episode 395 ][ timestamp 10 ] state=[-0.12964268 -1.75167738  0.20746601  2.76509173], action=0, reward=-1.0, next_state=[-0.16467623 -1.94754552  0.26276784  3.11315203]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 395: Exploration_rate=0.01. Score=10.\n",
      "[ episode 396 ] state=[ 0.04893417 -0.03575539  0.0437746  -0.02888069]\n",
      "[ episode 396 ][ timestamp 1 ] state=[ 0.04893417 -0.03575539  0.0437746  -0.02888069], action=0, reward=1.0, next_state=[ 0.04821906 -0.23147687  0.04319698  0.27728592]\n",
      "[ Experience replay ] starts\n",
      "[ episode 396 ][ timestamp 2 ] state=[ 0.04821906 -0.23147687  0.04319698  0.27728592], action=0, reward=1.0, next_state=[ 0.04358952 -0.42718762  0.0487427   0.58327415]\n",
      "[ Experience replay ] starts\n",
      "[ episode 396 ][ timestamp 3 ] state=[ 0.04358952 -0.42718762  0.0487427   0.58327415], action=0, reward=1.0, next_state=[ 0.03504577 -0.62295733  0.06040818  0.89090462]\n",
      "[ Experience replay ] starts\n",
      "[ episode 396 ][ timestamp 4 ] state=[ 0.03504577 -0.62295733  0.06040818  0.89090462], action=0, reward=1.0, next_state=[ 0.02258662 -0.81884452  0.07822628  1.20194867]\n",
      "[ Experience replay ] starts\n",
      "[ episode 396 ][ timestamp 5 ] state=[ 0.02258662 -0.81884452  0.07822628  1.20194867], action=0, reward=1.0, next_state=[ 0.00620973 -1.01488604  0.10226525  1.51808674]\n",
      "[ Experience replay ] starts\n",
      "[ episode 396 ][ timestamp 6 ] state=[ 0.00620973 -1.01488604  0.10226525  1.51808674], action=0, reward=1.0, next_state=[-0.01408799 -1.21108527  0.13262698  1.84086162]\n",
      "[ Experience replay ] starts\n",
      "[ episode 396 ][ timestamp 7 ] state=[-0.01408799 -1.21108527  0.13262698  1.84086162], action=0, reward=1.0, next_state=[-0.0383097  -1.40739868  0.16944422  2.17162379]\n",
      "[ Experience replay ] starts\n",
      "[ episode 396 ][ timestamp 8 ] state=[-0.0383097  -1.40739868  0.16944422  2.17162379], action=0, reward=-1.0, next_state=[-0.06645767 -1.60372008  0.21287669  2.51146707]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 396: Exploration_rate=0.01. Score=8.\n",
      "[ episode 397 ] state=[-0.04833168  0.00299927  0.01087302 -0.00131523]\n",
      "[ episode 397 ][ timestamp 1 ] state=[-0.04833168  0.00299927  0.01087302 -0.00131523], action=0, reward=1.0, next_state=[-0.0482717  -0.19227691  0.01084671  0.29477834]\n",
      "[ Experience replay ] starts\n",
      "[ episode 397 ][ timestamp 2 ] state=[-0.0482717  -0.19227691  0.01084671  0.29477834], action=0, reward=1.0, next_state=[-0.05211724 -0.38755181  0.01674228  0.59086232]\n",
      "[ Experience replay ] starts\n",
      "[ episode 397 ][ timestamp 3 ] state=[-0.05211724 -0.38755181  0.01674228  0.59086232], action=0, reward=1.0, next_state=[-0.05986827 -0.58290412  0.02855953  0.88877172]\n",
      "[ Experience replay ] starts\n",
      "[ episode 397 ][ timestamp 4 ] state=[-0.05986827 -0.58290412  0.02855953  0.88877172], action=0, reward=1.0, next_state=[-0.07152636 -0.77840176  0.04633496  1.19029396]\n",
      "[ Experience replay ] starts\n",
      "[ episode 397 ][ timestamp 5 ] state=[-0.07152636 -0.77840176  0.04633496  1.19029396], action=0, reward=1.0, next_state=[-0.08709439 -0.97409253  0.07014084  1.49713267]\n",
      "[ Experience replay ] starts\n",
      "[ episode 397 ][ timestamp 6 ] state=[-0.08709439 -0.97409253  0.07014084  1.49713267], action=0, reward=1.0, next_state=[-0.10657624 -1.16999345  0.10008349  1.81086601]\n",
      "[ Experience replay ] starts\n",
      "[ episode 397 ][ timestamp 7 ] state=[-0.10657624 -1.16999345  0.10008349  1.81086601], action=0, reward=1.0, next_state=[-0.12997611 -1.36607828  0.13630081  2.13289685]\n",
      "[ Experience replay ] starts\n",
      "[ episode 397 ][ timestamp 8 ] state=[-0.12997611 -1.36607828  0.13630081  2.13289685], action=0, reward=1.0, next_state=[-0.15729768 -1.56226276  0.17895875  2.46439274]\n",
      "[ Experience replay ] starts\n",
      "[ episode 397 ][ timestamp 9 ] state=[-0.15729768 -1.56226276  0.17895875  2.46439274], action=0, reward=-1.0, next_state=[-0.18854293 -1.75838738  0.22824661  2.80621488]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Ended! ] Episode 397: Exploration_rate=0.01. Score=9.\n",
      "[ episode 398 ] state=[ 4.32657751e-02 -2.64634405e-03 -2.07686765e-05  1.89438636e-02]\n",
      "[ episode 398 ][ timestamp 1 ] state=[ 4.32657751e-02 -2.64634405e-03 -2.07686765e-05  1.89438636e-02], action=0, reward=1.0, next_state=[ 0.04321285 -0.197768    0.00035811  0.31162024]\n",
      "[ Experience replay ] starts\n",
      "[ episode 398 ][ timestamp 2 ] state=[ 0.04321285 -0.197768    0.00035811  0.31162024], action=0, reward=1.0, next_state=[ 0.03925749 -0.39289505  0.00659051  0.60441608]\n",
      "[ Experience replay ] starts\n",
      "[ episode 398 ][ timestamp 3 ] state=[ 0.03925749 -0.39289505  0.00659051  0.60441608], action=0, reward=1.0, next_state=[ 0.03139959 -0.58810855  0.01867883  0.89916756]\n",
      "[ Experience replay ] starts\n",
      "[ episode 398 ][ timestamp 4 ] state=[ 0.03139959 -0.58810855  0.01867883  0.89916756], action=0, reward=1.0, next_state=[ 0.01963742 -0.7834786   0.03666219  1.19766278]\n",
      "[ Experience replay ] starts\n",
      "[ episode 398 ][ timestamp 5 ] state=[ 0.01963742 -0.7834786   0.03666219  1.19766278], action=0, reward=1.0, next_state=[ 0.00396784 -0.97905535  0.06061544  1.50160703]\n",
      "[ Experience replay ] starts\n",
      "[ episode 398 ][ timestamp 6 ] state=[ 0.00396784 -0.97905535  0.06061544  1.50160703], action=0, reward=1.0, next_state=[-0.01561326 -1.17485864  0.09064758  1.8125826 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 398 ][ timestamp 7 ] state=[-0.01561326 -1.17485864  0.09064758  1.8125826 ], action=1, reward=1.0, next_state=[-0.03911044 -0.98085577  0.12689923  1.54938697]\n",
      "[ Experience replay ] starts\n",
      "[ episode 398 ][ timestamp 8 ] state=[-0.03911044 -0.98085577  0.12689923  1.54938697], action=0, reward=1.0, next_state=[-0.05872755 -1.17725159  0.15788697  1.87882021]\n",
      "[ Experience replay ] starts\n",
      "[ episode 398 ][ timestamp 9 ] state=[-0.05872755 -1.17725159  0.15788697  1.87882021], action=0, reward=1.0, next_state=[-0.08227258 -1.37370357  0.19546338  2.21605906]\n",
      "[ Experience replay ] starts\n",
      "[ episode 398 ][ timestamp 10 ] state=[-0.08227258 -1.37370357  0.19546338  2.21605906], action=1, reward=-1.0, next_state=[-0.10974665 -1.18091558  0.23978456  1.98948474]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 398: Exploration_rate=0.01. Score=10.\n",
      "[ episode 399 ] state=[ 0.00061265  0.03035556  0.04177823 -0.04057364]\n",
      "[ episode 399 ][ timestamp 1 ] state=[ 0.00061265  0.03035556  0.04177823 -0.04057364], action=0, reward=1.0, next_state=[ 0.00121976 -0.16533981  0.04096676  0.2649925 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 399 ][ timestamp 2 ] state=[ 0.00121976 -0.16533981  0.04096676  0.2649925 ], action=0, reward=1.0, next_state=[-0.00208703 -0.3610218   0.04626661  0.57031008]\n",
      "[ Experience replay ] starts\n",
      "[ episode 399 ][ timestamp 3 ] state=[-0.00208703 -0.3610218   0.04626661  0.57031008], action=0, reward=1.0, next_state=[-0.00930747 -0.55676103  0.05767281  0.87720226]\n",
      "[ Experience replay ] starts\n",
      "[ episode 399 ][ timestamp 4 ] state=[-0.00930747 -0.55676103  0.05767281  0.87720226], action=0, reward=1.0, next_state=[-0.02044269 -0.75261737  0.07521686  1.18744473]\n",
      "[ Experience replay ] starts\n",
      "[ episode 399 ][ timestamp 5 ] state=[-0.02044269 -0.75261737  0.07521686  1.18744473], action=0, reward=1.0, next_state=[-0.03549504 -0.94862962  0.09896575  1.50272468]\n",
      "[ Experience replay ] starts\n",
      "[ episode 399 ][ timestamp 6 ] state=[-0.03549504 -0.94862962  0.09896575  1.50272468], action=0, reward=1.0, next_state=[-0.05446763 -1.14480383  0.12902024  1.8245946 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 399 ][ timestamp 7 ] state=[-0.05446763 -1.14480383  0.12902024  1.8245946 ], action=0, reward=1.0, next_state=[-0.07736371 -1.34109999  0.16551214  2.15441836]\n",
      "[ Experience replay ] starts\n",
      "[ episode 399 ][ timestamp 8 ] state=[-0.07736371 -1.34109999  0.16551214  2.15441836], action=0, reward=1.0, next_state=[-0.10418571 -1.53741652  0.2086005   2.49330758]\n",
      "[ Experience replay ] starts\n",
      "[ episode 399 ][ timestamp 9 ] state=[-0.10418571 -1.53741652  0.2086005   2.49330758], action=0, reward=-1.0, next_state=[-0.13493404 -1.73357252  0.25846666  2.84204784]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 399: Exploration_rate=0.01. Score=9.\n",
      "[ episode 400 ] state=[-0.03923337 -0.01531974  0.02162771 -0.02222305]\n",
      "[ episode 400 ][ timestamp 1 ] state=[-0.03923337 -0.01531974  0.02162771 -0.02222305], action=0, reward=1.0, next_state=[-0.03953977 -0.21074507  0.02118325  0.27720444]\n",
      "[ Experience replay ] starts\n",
      "[ episode 400 ][ timestamp 2 ] state=[-0.03953977 -0.21074507  0.02118325  0.27720444], action=0, reward=1.0, next_state=[-0.04375467 -0.40616272  0.02672734  0.57649257]\n",
      "[ Experience replay ] starts\n",
      "[ episode 400 ][ timestamp 3 ] state=[-0.04375467 -0.40616272  0.02672734  0.57649257], action=0, reward=1.0, next_state=[-0.05187792 -0.60164892  0.03825719  0.87747404]\n",
      "[ Experience replay ] starts\n",
      "[ episode 400 ][ timestamp 4 ] state=[-0.05187792 -0.60164892  0.03825719  0.87747404], action=0, reward=1.0, next_state=[-0.0639109  -0.79726933  0.05580667  1.18193481]\n",
      "[ Experience replay ] starts\n",
      "[ episode 400 ][ timestamp 5 ] state=[-0.0639109  -0.79726933  0.05580667  1.18193481], action=0, reward=1.0, next_state=[-0.07985629 -0.99306938  0.07944537  1.4915763 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 400 ][ timestamp 6 ] state=[-0.07985629 -0.99306938  0.07944537  1.4915763 ], action=0, reward=1.0, next_state=[-0.09971768 -1.18906332  0.10927689  1.80797231]\n",
      "[ Experience replay ] starts\n",
      "[ episode 400 ][ timestamp 7 ] state=[-0.09971768 -1.18906332  0.10927689  1.80797231], action=0, reward=1.0, next_state=[-0.12349894 -1.38522151  0.14543634  2.13251803]\n",
      "[ Experience replay ] starts\n",
      "[ episode 400 ][ timestamp 8 ] state=[-0.12349894 -1.38522151  0.14543634  2.13251803], action=1, reward=1.0, next_state=[-0.15120337 -1.19181029  0.1880867   1.88807174]\n",
      "[ Experience replay ] starts\n",
      "[ episode 400 ][ timestamp 9 ] state=[-0.15120337 -1.19181029  0.1880867   1.88807174], action=0, reward=-1.0, next_state=[-0.17503958 -1.38841328  0.22584813  2.23274725]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 400: Exploration_rate=0.01. Score=9.\n",
      "[ episode 401 ] state=[0.01334968 0.02089291 0.02611125 0.001198  ]\n",
      "[ episode 401 ][ timestamp 1 ] state=[0.01334968 0.02089291 0.02611125 0.001198  ], action=0, reward=1.0, next_state=[ 0.01376754 -0.17459359  0.02613521  0.30200363]\n",
      "[ Experience replay ] starts\n",
      "[ episode 401 ][ timestamp 2 ] state=[ 0.01376754 -0.17459359  0.02613521  0.30200363], action=0, reward=1.0, next_state=[ 0.01027566 -0.3700781   0.03217529  0.60281313]\n",
      "[ Experience replay ] starts\n",
      "[ episode 401 ][ timestamp 3 ] state=[ 0.01027566 -0.3700781   0.03217529  0.60281313], action=0, reward=1.0, next_state=[ 0.0028741  -0.56563496  0.04423155  0.9054545 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 401 ][ timestamp 4 ] state=[ 0.0028741  -0.56563496  0.04423155  0.9054545 ], action=0, reward=1.0, next_state=[-0.0084386  -0.76132707  0.06234064  1.2117054 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 401 ][ timestamp 5 ] state=[-0.0084386  -0.76132707  0.06234064  1.2117054 ], action=0, reward=1.0, next_state=[-0.02366514 -0.95719589  0.08657475  1.52325419]\n",
      "[ Experience replay ] starts\n",
      "[ episode 401 ][ timestamp 6 ] state=[-0.02366514 -0.95719589  0.08657475  1.52325419], action=0, reward=1.0, next_state=[-0.04280906 -1.15325027  0.11703983  1.84165553]\n",
      "[ Experience replay ] starts\n",
      "[ episode 401 ][ timestamp 7 ] state=[-0.04280906 -1.15325027  0.11703983  1.84165553], action=0, reward=1.0, next_state=[-0.06587406 -1.3494533   0.15387294  2.16827784]\n",
      "[ Experience replay ] starts\n",
      "[ episode 401 ][ timestamp 8 ] state=[-0.06587406 -1.3494533   0.15387294  2.16827784], action=0, reward=1.0, next_state=[-0.09286313 -1.54570702  0.1972385   2.50424063]\n",
      "[ Experience replay ] starts\n",
      "[ episode 401 ][ timestamp 9 ] state=[-0.09286313 -1.54570702  0.1972385   2.50424063], action=0, reward=-1.0, next_state=[-0.12377727 -1.74183483  0.24732331  2.85034126]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 401: Exploration_rate=0.01. Score=9.\n",
      "[ episode 402 ] state=[-0.03232046  0.02087078 -0.00563106  0.01520174]\n",
      "[ episode 402 ][ timestamp 1 ] state=[-0.03232046  0.02087078 -0.00563106  0.01520174], action=0, reward=1.0, next_state=[-0.03190305 -0.17416997 -0.00532702  0.3061027 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 402 ][ timestamp 2 ] state=[-0.03190305 -0.17416997 -0.00532702  0.3061027 ], action=0, reward=1.0, next_state=[-0.03538645 -0.36921561  0.00079503  0.59710086]\n",
      "[ Experience replay ] starts\n",
      "[ episode 402 ][ timestamp 3 ] state=[-0.03538645 -0.36921561  0.00079503  0.59710086], action=0, reward=1.0, next_state=[-0.04277076 -0.56434867  0.01273705  0.89003411]\n",
      "[ Experience replay ] starts\n",
      "[ episode 402 ][ timestamp 4 ] state=[-0.04277076 -0.56434867  0.01273705  0.89003411], action=0, reward=1.0, next_state=[-0.05405773 -0.75964111  0.03053773  1.1866936 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 402 ][ timestamp 5 ] state=[-0.05405773 -0.75964111  0.03053773  1.1866936 ], action=0, reward=1.0, next_state=[-0.06925055 -0.95514546  0.05427161  1.48879009]\n",
      "[ Experience replay ] starts\n",
      "[ episode 402 ][ timestamp 6 ] state=[-0.06925055 -0.95514546  0.05427161  1.48879009], action=0, reward=1.0, next_state=[-0.08835346 -1.15088477  0.08404741  1.79791479]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 402 ][ timestamp 7 ] state=[-0.08835346 -1.15088477  0.08404741  1.79791479], action=0, reward=1.0, next_state=[-0.11137116 -1.34684063  0.1200057   2.11549187]\n",
      "[ Experience replay ] starts\n",
      "[ episode 402 ][ timestamp 8 ] state=[-0.11137116 -1.34684063  0.1200057   2.11549187], action=0, reward=1.0, next_state=[-0.13830797 -1.5429388   0.16231554  2.44272067]\n",
      "[ Experience replay ] starts\n",
      "[ episode 402 ][ timestamp 9 ] state=[-0.13830797 -1.5429388   0.16231554  2.44272067], action=0, reward=-1.0, next_state=[-0.16916675 -1.73903246  0.21116995  2.78050638]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 402: Exploration_rate=0.01. Score=9.\n",
      "[ episode 403 ] state=[-0.00335713  0.02232431  0.03636318  0.04782606]\n",
      "[ episode 403 ][ timestamp 1 ] state=[-0.00335713  0.02232431  0.03636318  0.04782606], action=0, reward=1.0, next_state=[-0.00291065 -0.17329968  0.0373197   0.35175648]\n",
      "[ Experience replay ] starts\n",
      "[ episode 403 ][ timestamp 2 ] state=[-0.00291065 -0.17329968  0.0373197   0.35175648], action=0, reward=1.0, next_state=[-0.00637664 -0.36893192  0.04435483  0.65596996]\n",
      "[ Experience replay ] starts\n",
      "[ episode 403 ][ timestamp 3 ] state=[-0.00637664 -0.36893192  0.04435483  0.65596996], action=0, reward=1.0, next_state=[-0.01375528 -0.56464238  0.05747423  0.96228297]\n",
      "[ Experience replay ] starts\n",
      "[ episode 403 ][ timestamp 4 ] state=[-0.01375528 -0.56464238  0.05747423  0.96228297], action=0, reward=1.0, next_state=[-0.02504813 -0.76048761  0.07671989  1.27245387]\n",
      "[ Experience replay ] starts\n",
      "[ episode 403 ][ timestamp 5 ] state=[-0.02504813 -0.76048761  0.07671989  1.27245387], action=0, reward=1.0, next_state=[-0.04025788 -0.9565002   0.10216897  1.58814141]\n",
      "[ Experience replay ] starts\n",
      "[ episode 403 ][ timestamp 6 ] state=[-0.04025788 -0.9565002   0.10216897  1.58814141], action=0, reward=1.0, next_state=[-0.05938788 -1.15267694  0.13393179  1.91085747]\n",
      "[ Experience replay ] starts\n",
      "[ episode 403 ][ timestamp 7 ] state=[-0.05938788 -1.15267694  0.13393179  1.91085747], action=0, reward=1.0, next_state=[-0.08244142 -1.34896501  0.17214894  2.24191112]\n",
      "[ Experience replay ] starts\n",
      "[ episode 403 ][ timestamp 8 ] state=[-0.08244142 -1.34896501  0.17214894  2.24191112], action=0, reward=-1.0, next_state=[-0.10942072 -1.54524592  0.21698717  2.58234281]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 403: Exploration_rate=0.01. Score=8.\n",
      "[ episode 404 ] state=[ 0.03786795 -0.00286554 -0.02704748 -0.02605094]\n",
      "[ episode 404 ][ timestamp 1 ] state=[ 0.03786795 -0.00286554 -0.02704748 -0.02605094], action=0, reward=1.0, next_state=[ 0.03781064 -0.19758938 -0.0275685   0.25797699]\n",
      "[ Experience replay ] starts\n",
      "[ episode 404 ][ timestamp 2 ] state=[ 0.03781064 -0.19758938 -0.0275685   0.25797699], action=0, reward=1.0, next_state=[ 0.03385885 -0.39230712 -0.02240896  0.5418385 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 404 ][ timestamp 3 ] state=[ 0.03385885 -0.39230712 -0.02240896  0.5418385 ], action=0, reward=1.0, next_state=[ 0.02601271 -0.58710706 -0.01157219  0.82737737]\n",
      "[ Experience replay ] starts\n",
      "[ episode 404 ][ timestamp 4 ] state=[ 0.02601271 -0.58710706 -0.01157219  0.82737737], action=0, reward=1.0, next_state=[ 0.01427057 -0.78206888  0.00497536  1.11639838]\n",
      "[ Experience replay ] starts\n",
      "[ episode 404 ][ timestamp 5 ] state=[ 0.01427057 -0.78206888  0.00497536  1.11639838], action=0, reward=1.0, next_state=[-1.37081193e-03 -9.77255785e-01  2.73033241e-02  1.41063785e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 404 ][ timestamp 6 ] state=[-1.37081193e-03 -9.77255785e-01  2.73033241e-02  1.41063785e+00], action=0, reward=1.0, next_state=[-0.02091593 -1.17270545  0.05551608  1.71172926]\n",
      "[ Experience replay ] starts\n",
      "[ episode 404 ][ timestamp 7 ] state=[-0.02091593 -1.17270545  0.05551608  1.71172926], action=0, reward=1.0, next_state=[-0.04437004 -1.36841925  0.08975067  2.02116102]\n",
      "[ Experience replay ] starts\n",
      "[ episode 404 ][ timestamp 8 ] state=[-0.04437004 -1.36841925  0.08975067  2.02116102], action=0, reward=1.0, next_state=[-0.07173842 -1.56434906  0.13017389  2.34022414]\n",
      "[ Experience replay ] starts\n",
      "[ episode 404 ][ timestamp 9 ] state=[-0.07173842 -1.56434906  0.13017389  2.34022414], action=0, reward=1.0, next_state=[-0.1030254  -1.76038162  0.17697837  2.66994826]\n",
      "[ Experience replay ] starts\n",
      "[ episode 404 ][ timestamp 10 ] state=[-0.1030254  -1.76038162  0.17697837  2.66994826], action=0, reward=-1.0, next_state=[-0.13823304 -1.95632023  0.23037733  3.01102582]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 404: Exploration_rate=0.01. Score=10.\n",
      "[ episode 405 ] state=[ 0.0311923  -0.03170164  0.03971482 -0.04678889]\n",
      "[ episode 405 ][ timestamp 1 ] state=[ 0.0311923  -0.03170164  0.03971482 -0.04678889], action=0, reward=1.0, next_state=[ 0.03055827 -0.2273699   0.03877904  0.25815516]\n",
      "[ Experience replay ] starts\n",
      "[ episode 405 ][ timestamp 2 ] state=[ 0.03055827 -0.2273699   0.03877904  0.25815516], action=0, reward=1.0, next_state=[ 0.02601087 -0.42302341  0.04394214  0.56281295]\n",
      "[ Experience replay ] starts\n",
      "[ episode 405 ][ timestamp 3 ] state=[ 0.02601087 -0.42302341  0.04394214  0.56281295], action=0, reward=1.0, next_state=[ 0.0175504  -0.61873353  0.0551984   0.86900959]\n",
      "[ Experience replay ] starts\n",
      "[ episode 405 ][ timestamp 4 ] state=[ 0.0175504  -0.61873353  0.0551984   0.86900959], action=0, reward=1.0, next_state=[ 0.00517573 -0.81456124  0.07257859  1.17852387]\n",
      "[ Experience replay ] starts\n",
      "[ episode 405 ][ timestamp 5 ] state=[ 0.00517573 -0.81456124  0.07257859  1.17852387], action=0, reward=1.0, next_state=[-0.01111549 -1.01054676  0.09614907  1.49304757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 405 ][ timestamp 6 ] state=[-0.01111549 -1.01054676  0.09614907  1.49304757], action=0, reward=1.0, next_state=[-0.03132643 -1.20669808  0.12601002  1.81413988]\n",
      "[ Experience replay ] starts\n",
      "[ episode 405 ][ timestamp 7 ] state=[-0.03132643 -1.20669808  0.12601002  1.81413988], action=0, reward=1.0, next_state=[-0.05546039 -1.40297776  0.16229282  2.14317401]\n",
      "[ Experience replay ] starts\n",
      "[ episode 405 ][ timestamp 8 ] state=[-0.05546039 -1.40297776  0.16229282  2.14317401], action=0, reward=1.0, next_state=[-0.08351995 -1.59928748  0.2051563   2.48127404]\n",
      "[ Experience replay ] starts\n",
      "[ episode 405 ][ timestamp 9 ] state=[-0.08351995 -1.59928748  0.2051563   2.48127404], action=0, reward=-1.0, next_state=[-0.1155057  -1.79545046  0.25478178  2.82924169]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 405: Exploration_rate=0.01. Score=9.\n",
      "[ episode 406 ] state=[-0.00828258 -0.02456686  0.04453854 -0.02969711]\n",
      "[ episode 406 ][ timestamp 1 ] state=[-0.00828258 -0.02456686  0.04453854 -0.02969711], action=0, reward=1.0, next_state=[-0.00877392 -0.22029828  0.0439446   0.27669888]\n",
      "[ Experience replay ] starts\n",
      "[ episode 406 ][ timestamp 2 ] state=[-0.00877392 -0.22029828  0.0439446   0.27669888], action=0, reward=1.0, next_state=[-0.01317988 -0.41601873  0.04947857  0.58291168]\n",
      "[ Experience replay ] starts\n",
      "[ episode 406 ][ timestamp 3 ] state=[-0.01317988 -0.41601873  0.04947857  0.58291168], action=0, reward=1.0, next_state=[-0.02150026 -0.61179768  0.06113681  0.89076148]\n",
      "[ Experience replay ] starts\n",
      "[ episode 406 ][ timestamp 4 ] state=[-0.02150026 -0.61179768  0.06113681  0.89076148], action=0, reward=1.0, next_state=[-0.03373621 -0.80769344  0.07895204  1.20201916]\n",
      "[ Experience replay ] starts\n",
      "[ episode 406 ][ timestamp 5 ] state=[-0.03373621 -0.80769344  0.07895204  1.20201916], action=0, reward=1.0, next_state=[-0.04989008 -1.00374257  0.10299242  1.51836458]\n",
      "[ Experience replay ] starts\n",
      "[ episode 406 ][ timestamp 6 ] state=[-0.04989008 -1.00374257  0.10299242  1.51836458], action=0, reward=1.0, next_state=[-0.06996493 -1.19994817  0.13335971  1.8413397 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 406 ][ timestamp 7 ] state=[-0.06996493 -1.19994817  0.13335971  1.8413397 ], action=0, reward=1.0, next_state=[-0.09396389 -1.39626631  0.1701865   2.17229381]\n",
      "[ Experience replay ] starts\n",
      "[ episode 406 ][ timestamp 8 ] state=[-0.09396389 -1.39626631  0.1701865   2.17229381], action=0, reward=-1.0, next_state=[-0.12188922 -1.59259034  0.21363238  2.51231913]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 406: Exploration_rate=0.01. Score=8.\n",
      "[ episode 407 ] state=[ 0.03959267  0.0320604   0.0190368  -0.04736731]\n",
      "[ episode 407 ][ timestamp 1 ] state=[ 0.03959267  0.0320604   0.0190368  -0.04736731], action=0, reward=1.0, next_state=[ 0.04023388 -0.16332928  0.01808945  0.25126059]\n",
      "[ Experience replay ] starts\n",
      "[ episode 407 ][ timestamp 2 ] state=[ 0.04023388 -0.16332928  0.01808945  0.25126059], action=0, reward=1.0, next_state=[ 0.03696729 -0.35870481  0.02311466  0.54959394]\n",
      "[ Experience replay ] starts\n",
      "[ episode 407 ][ timestamp 3 ] state=[ 0.03696729 -0.35870481  0.02311466  0.54959394], action=0, reward=1.0, next_state=[ 0.0297932  -0.5541437   0.03410654  0.84946906]\n",
      "[ Experience replay ] starts\n",
      "[ episode 407 ][ timestamp 4 ] state=[ 0.0297932  -0.5541437   0.03410654  0.84946906], action=0, reward=1.0, next_state=[ 0.01871032 -0.74971376  0.05109592  1.15267893]\n",
      "[ Experience replay ] starts\n",
      "[ episode 407 ][ timestamp 5 ] state=[ 0.01871032 -0.74971376  0.05109592  1.15267893], action=0, reward=1.0, next_state=[ 0.00371605 -0.94546366  0.0741495   1.46093623]\n",
      "[ Experience replay ] starts\n",
      "[ episode 407 ][ timestamp 6 ] state=[ 0.00371605 -0.94546366  0.0741495   1.46093623], action=0, reward=1.0, next_state=[-0.01519323 -1.14141218  0.10336823  1.77583135]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 407 ][ timestamp 7 ] state=[-0.01519323 -1.14141218  0.10336823  1.77583135], action=0, reward=1.0, next_state=[-0.03802147 -1.3375358   0.13888485  2.09878265]\n",
      "[ Experience replay ] starts\n",
      "[ episode 407 ][ timestamp 8 ] state=[-0.03802147 -1.3375358   0.13888485  2.09878265], action=0, reward=1.0, next_state=[-0.06477219 -1.53375395  0.18086051  2.4309768 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 407 ][ timestamp 9 ] state=[-0.06477219 -1.53375395  0.18086051  2.4309768 ], action=0, reward=-1.0, next_state=[-0.09544727 -1.72991203  0.22948004  2.77329829]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 407: Exploration_rate=0.01. Score=9.\n",
      "[ episode 408 ] state=[ 0.04543535 -0.00285593  0.01156972 -0.00165772]\n",
      "[ episode 408 ][ timestamp 1 ] state=[ 0.04543535 -0.00285593  0.01156972 -0.00165772], action=0, reward=1.0, next_state=[ 0.04537823 -0.19814188  0.01153656  0.29465302]\n",
      "[ Experience replay ] starts\n",
      "[ episode 408 ][ timestamp 2 ] state=[ 0.04537823 -0.19814188  0.01153656  0.29465302], action=0, reward=1.0, next_state=[ 0.04141539 -0.39342639  0.01742962  0.59095196]\n",
      "[ Experience replay ] starts\n",
      "[ episode 408 ][ timestamp 3 ] state=[ 0.04141539 -0.39342639  0.01742962  0.59095196], action=0, reward=1.0, next_state=[ 0.03354686 -0.58878798  0.02924866  0.88907388]\n",
      "[ Experience replay ] starts\n",
      "[ episode 408 ][ timestamp 4 ] state=[ 0.03354686 -0.58878798  0.02924866  0.88907388], action=0, reward=1.0, next_state=[ 0.0217711  -0.78429437  0.04703014  1.19080592]\n",
      "[ Experience replay ] starts\n",
      "[ episode 408 ][ timestamp 5 ] state=[ 0.0217711  -0.78429437  0.04703014  1.19080592], action=0, reward=1.0, next_state=[ 0.00608522 -0.97999312  0.07084626  1.49785123]\n",
      "[ Experience replay ] starts\n",
      "[ episode 408 ][ timestamp 6 ] state=[ 0.00608522 -0.97999312  0.07084626  1.49785123], action=0, reward=1.0, next_state=[-0.01351465 -1.17590095  0.10080328  1.81178718]\n",
      "[ Experience replay ] starts\n",
      "[ episode 408 ][ timestamp 7 ] state=[-0.01351465 -1.17590095  0.10080328  1.81178718], action=0, reward=1.0, next_state=[-0.03703267 -1.37199121  0.13703903  2.13401545]\n",
      "[ Experience replay ] starts\n",
      "[ episode 408 ][ timestamp 8 ] state=[-0.03703267 -1.37199121  0.13703903  2.13401545], action=0, reward=1.0, next_state=[-0.06447249 -1.56817913  0.17971934  2.46570188]\n",
      "[ Experience replay ] starts\n",
      "[ episode 408 ][ timestamp 9 ] state=[-0.06447249 -1.56817913  0.17971934  2.46570188], action=0, reward=-1.0, next_state=[-0.09583607 -1.76430458  0.22903337  2.80770533]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 408: Exploration_rate=0.01. Score=9.\n",
      "[ episode 409 ] state=[ 0.02717572  0.0020247  -0.00627202 -0.00724566]\n",
      "[ episode 409 ][ timestamp 1 ] state=[ 0.02717572  0.0020247  -0.00627202 -0.00724566], action=0, reward=1.0, next_state=[ 0.02721621 -0.19300674 -0.00641693  0.28345179]\n",
      "[ Experience replay ] starts\n",
      "[ episode 409 ][ timestamp 2 ] state=[ 0.02721621 -0.19300674 -0.00641693  0.28345179], action=0, reward=1.0, next_state=[ 0.02335608 -0.38803658 -0.0007479   0.57410396]\n",
      "[ Experience replay ] starts\n",
      "[ episode 409 ][ timestamp 3 ] state=[ 0.02335608 -0.38803658 -0.0007479   0.57410396], action=0, reward=1.0, next_state=[ 0.01559535 -0.58314804  0.01073418  0.86655118]\n",
      "[ Experience replay ] starts\n",
      "[ episode 409 ][ timestamp 4 ] state=[ 0.01559535 -0.58314804  0.01073418  0.86655118], action=0, reward=1.0, next_state=[ 0.00393239 -0.77841441  0.02806521  1.16258966]\n",
      "[ Experience replay ] starts\n",
      "[ episode 409 ][ timestamp 5 ] state=[ 0.00393239 -0.77841441  0.02806521  1.16258966], action=0, reward=1.0, next_state=[-0.0116359  -0.97389038  0.051317    1.46393823]\n",
      "[ Experience replay ] starts\n",
      "[ episode 409 ][ timestamp 6 ] state=[-0.0116359  -0.97389038  0.051317    1.46393823], action=0, reward=1.0, next_state=[-0.03111371 -1.16960208  0.08059576  1.77219989]\n",
      "[ Experience replay ] starts\n",
      "[ episode 409 ][ timestamp 7 ] state=[-0.03111371 -1.16960208  0.08059576  1.77219989], action=0, reward=1.0, next_state=[-0.05450575 -1.36553528  0.11603976  2.08881518]\n",
      "[ Experience replay ] starts\n",
      "[ episode 409 ][ timestamp 8 ] state=[-0.05450575 -1.36553528  0.11603976  2.08881518], action=0, reward=1.0, next_state=[-0.08181646 -1.56162134  0.15781607  2.41500541]\n",
      "[ Experience replay ] starts\n",
      "[ episode 409 ][ timestamp 9 ] state=[-0.08181646 -1.56162134  0.15781607  2.41500541], action=0, reward=1.0, next_state=[-0.11304888 -1.75772055  0.20611617  2.75170437]\n",
      "[ Experience replay ] starts\n",
      "[ episode 409 ][ timestamp 10 ] state=[-0.11304888 -1.75772055  0.20611617  2.75170437], action=0, reward=-1.0, next_state=[-0.1482033  -1.95360329  0.26115026  3.09947914]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 409: Exploration_rate=0.01. Score=10.\n",
      "[ episode 410 ] state=[ 2.27140574e-02  1.16454457e-02  1.93784661e-02 -9.78073253e-05]\n",
      "[ episode 410 ][ timestamp 1 ] state=[ 2.27140574e-02  1.16454457e-02  1.93784661e-02 -9.78073253e-05], action=0, reward=1.0, next_state=[ 0.02294697 -0.18374898  0.01937651  0.29863572]\n",
      "[ Experience replay ] starts\n",
      "[ episode 410 ][ timestamp 2 ] state=[ 0.02294697 -0.18374898  0.01937651  0.29863572], action=0, reward=1.0, next_state=[ 0.01927199 -0.3791417   0.02534922  0.59736611]\n",
      "[ Experience replay ] starts\n",
      "[ episode 410 ][ timestamp 3 ] state=[ 0.01927199 -0.3791417   0.02534922  0.59736611], action=0, reward=1.0, next_state=[ 0.01168915 -0.57460903  0.03729655  0.89792478]\n",
      "[ Experience replay ] starts\n",
      "[ episode 410 ][ timestamp 4 ] state=[ 0.01168915 -0.57460903  0.03729655  0.89792478], action=0, reward=1.0, next_state=[ 1.96972131e-04 -7.70216139e-01  5.52550421e-02  1.20209404e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 410 ][ timestamp 5 ] state=[ 1.96972131e-04 -7.70216139e-01  5.52550421e-02  1.20209404e+00], action=0, reward=1.0, next_state=[-0.01520735 -0.96600736  0.07929692  1.51156938]\n",
      "[ Experience replay ] starts\n",
      "[ episode 410 ][ timestamp 6 ] state=[-0.01520735 -0.96600736  0.07929692  1.51156938], action=0, reward=1.0, next_state=[-0.0345275  -1.16199523  0.10952831  1.82791625]\n",
      "[ Experience replay ] starts\n",
      "[ episode 410 ][ timestamp 7 ] state=[-0.0345275  -1.16199523  0.10952831  1.82791625], action=0, reward=1.0, next_state=[-0.0577674  -1.35814762  0.14608664  2.15251874]\n",
      "[ Experience replay ] starts\n",
      "[ episode 410 ][ timestamp 8 ] state=[-0.0577674  -1.35814762  0.14608664  2.15251874], action=0, reward=1.0, next_state=[-0.08493035 -1.55437273  0.18913701  2.48651807]\n",
      "[ Experience replay ] starts\n",
      "[ episode 410 ][ timestamp 9 ] state=[-0.08493035 -1.55437273  0.18913701  2.48651807], action=0, reward=-1.0, next_state=[-0.11601781 -1.7505016   0.23886737  2.83074032]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 410: Exploration_rate=0.01. Score=9.\n",
      "[ episode 411 ] state=[ 0.03740902 -0.03942261 -0.00426484 -0.02461485]\n",
      "[ episode 411 ][ timestamp 1 ] state=[ 0.03740902 -0.03942261 -0.00426484 -0.02461485], action=0, reward=1.0, next_state=[ 0.03662057 -0.23448314 -0.00475714  0.26671943]\n",
      "[ Experience replay ] starts\n",
      "[ episode 411 ][ timestamp 2 ] state=[ 0.03662057 -0.23448314 -0.00475714  0.26671943], action=0, reward=1.0, next_state=[ 0.0319309  -0.42953688  0.00057725  0.55789813]\n",
      "[ Experience replay ] starts\n",
      "[ episode 411 ][ timestamp 3 ] state=[ 0.0319309  -0.42953688  0.00057725  0.55789813], action=0, reward=1.0, next_state=[ 0.02334017 -0.62466693  0.01173521  0.85076286]\n",
      "[ Experience replay ] starts\n",
      "[ episode 411 ][ timestamp 4 ] state=[ 0.02334017 -0.62466693  0.01173521  0.85076286], action=0, reward=1.0, next_state=[ 0.01084683 -0.81994691  0.02875047  1.14711274]\n",
      "[ Experience replay ] starts\n",
      "[ episode 411 ][ timestamp 5 ] state=[ 0.01084683 -0.81994691  0.02875047  1.14711274], action=0, reward=1.0, next_state=[-0.00555211 -1.01543224  0.05169272  1.44867102]\n",
      "[ Experience replay ] starts\n",
      "[ episode 411 ][ timestamp 6 ] state=[-0.00555211 -1.01543224  0.05169272  1.44867102], action=0, reward=1.0, next_state=[-0.02586075 -1.21115019  0.08066614  1.75704669]\n",
      "[ Experience replay ] starts\n",
      "[ episode 411 ][ timestamp 7 ] state=[-0.02586075 -1.21115019  0.08066614  1.75704669], action=0, reward=1.0, next_state=[-0.05008376 -1.40708822  0.11580708  2.07368815]\n",
      "[ Experience replay ] starts\n",
      "[ episode 411 ][ timestamp 8 ] state=[-0.05008376 -1.40708822  0.11580708  2.07368815], action=0, reward=1.0, next_state=[-0.07822552 -1.60317987  0.15728084  2.39982669]\n",
      "[ Experience replay ] starts\n",
      "[ episode 411 ][ timestamp 9 ] state=[-0.07822552 -1.60317987  0.15728084  2.39982669], action=0, reward=1.0, next_state=[-0.11028912 -1.79928832  0.20527737  2.73640862]\n",
      "[ Experience replay ] starts\n",
      "[ episode 411 ][ timestamp 10 ] state=[-0.11028912 -1.79928832  0.20527737  2.73640862], action=0, reward=-1.0, next_state=[-0.14627489 -1.99518755  0.26000555  3.08401655]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Ended! ] Episode 411: Exploration_rate=0.01. Score=10.\n",
      "[ episode 412 ] state=[-0.01539732 -0.04664588 -0.03585672 -0.01576753]\n",
      "[ episode 412 ][ timestamp 1 ] state=[-0.01539732 -0.04664588 -0.03585672 -0.01576753], action=0, reward=1.0, next_state=[-0.01633024 -0.24123575 -0.03617207  0.26539003]\n",
      "[ Experience replay ] starts\n",
      "[ episode 412 ][ timestamp 2 ] state=[-0.01633024 -0.24123575 -0.03617207  0.26539003], action=0, reward=1.0, next_state=[-0.02115496 -0.43582325 -0.03086427  0.54644809]\n",
      "[ Experience replay ] starts\n",
      "[ episode 412 ][ timestamp 3 ] state=[-0.02115496 -0.43582325 -0.03086427  0.54644809], action=0, reward=1.0, next_state=[-0.02987142 -0.63049827 -0.01993531  0.82924889]\n",
      "[ Experience replay ] starts\n",
      "[ episode 412 ][ timestamp 4 ] state=[-0.02987142 -0.63049827 -0.01993531  0.82924889], action=0, reward=1.0, next_state=[-0.04248139 -0.8253421  -0.00335033  1.11559598]\n",
      "[ Experience replay ] starts\n",
      "[ episode 412 ][ timestamp 5 ] state=[-0.04248139 -0.8253421  -0.00335033  1.11559598], action=0, reward=1.0, next_state=[-0.05898823 -1.02041992  0.01896159  1.40722606]\n",
      "[ Experience replay ] starts\n",
      "[ episode 412 ][ timestamp 6 ] state=[-0.05898823 -1.02041992  0.01896159  1.40722606], action=0, reward=1.0, next_state=[-0.07939663 -1.21577197  0.04710611  1.70577583]\n",
      "[ Experience replay ] starts\n",
      "[ episode 412 ][ timestamp 7 ] state=[-0.07939663 -1.21577197  0.04710611  1.70577583], action=0, reward=1.0, next_state=[-0.10371207 -1.41140308  0.08122163  2.01274106]\n",
      "[ Experience replay ] starts\n",
      "[ episode 412 ][ timestamp 8 ] state=[-0.10371207 -1.41140308  0.08122163  2.01274106], action=0, reward=1.0, next_state=[-0.13194013 -1.60726975  0.12147645  2.32942542]\n",
      "[ Experience replay ] starts\n",
      "[ episode 412 ][ timestamp 9 ] state=[-0.13194013 -1.60726975  0.12147645  2.32942542], action=0, reward=1.0, next_state=[-0.16408552 -1.80326468  0.16806496  2.65687763]\n",
      "[ Experience replay ] starts\n",
      "[ episode 412 ][ timestamp 10 ] state=[-0.16408552 -1.80326468  0.16806496  2.65687763], action=0, reward=-1.0, next_state=[-0.20015082 -1.99919865  0.22120251  2.99581643]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 412: Exploration_rate=0.01. Score=10.\n",
      "[ episode 413 ] state=[-0.01384995  0.0381569   0.02052204  0.01389128]\n",
      "[ episode 413 ][ timestamp 1 ] state=[-0.01384995  0.0381569   0.02052204  0.01389128], action=0, reward=1.0, next_state=[-0.01308681 -0.15725326  0.02079986  0.31297785]\n",
      "[ Experience replay ] starts\n",
      "[ episode 413 ][ timestamp 2 ] state=[-0.01308681 -0.15725326  0.02079986  0.31297785], action=0, reward=1.0, next_state=[-0.01623188 -0.35266526  0.02705942  0.61214716]\n",
      "[ Experience replay ] starts\n",
      "[ episode 413 ][ timestamp 3 ] state=[-0.01623188 -0.35266526  0.02705942  0.61214716], action=0, reward=1.0, next_state=[-0.02328518 -0.54815473  0.03930236  0.91322851]\n",
      "[ Experience replay ] starts\n",
      "[ episode 413 ][ timestamp 4 ] state=[-0.02328518 -0.54815473  0.03930236  0.91322851], action=0, reward=1.0, next_state=[-0.03424828 -0.74378568  0.05756693  1.21800025]\n",
      "[ Experience replay ] starts\n",
      "[ episode 413 ][ timestamp 5 ] state=[-0.03424828 -0.74378568  0.05756693  1.21800025], action=0, reward=1.0, next_state=[-0.04912399 -0.9396007   0.08192694  1.52815156]\n",
      "[ Experience replay ] starts\n",
      "[ episode 413 ][ timestamp 6 ] state=[-0.04912399 -0.9396007   0.08192694  1.52815156], action=0, reward=1.0, next_state=[-0.067916   -1.13560986  0.11248997  1.84523872]\n",
      "[ Experience replay ] starts\n",
      "[ episode 413 ][ timestamp 7 ] state=[-0.067916   -1.13560986  0.11248997  1.84523872], action=0, reward=1.0, next_state=[-0.0906282  -1.33177775  0.14939474  2.17063313]\n",
      "[ Experience replay ] starts\n",
      "[ episode 413 ][ timestamp 8 ] state=[-0.0906282  -1.33177775  0.14939474  2.17063313], action=0, reward=1.0, next_state=[-0.11726376 -1.52800825  0.19280741  2.50545912]\n",
      "[ Experience replay ] starts\n",
      "[ episode 413 ][ timestamp 9 ] state=[-0.11726376 -1.52800825  0.19280741  2.50545912], action=0, reward=-1.0, next_state=[-0.14782392 -1.72412694  0.24291659  2.8505209 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 413: Exploration_rate=0.01. Score=9.\n",
      "[ episode 414 ] state=[-0.02333114  0.03738597  0.04255427  0.0145073 ]\n",
      "[ episode 414 ][ timestamp 1 ] state=[-0.02333114  0.03738597  0.04255427  0.0145073 ], action=0, reward=1.0, next_state=[-0.02258342 -0.15831961  0.04284442  0.3203071 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 414 ][ timestamp 2 ] state=[-0.02258342 -0.15831961  0.04284442  0.3203071 ], action=0, reward=1.0, next_state=[-0.02574981 -0.3540247   0.04925056  0.62618775]\n",
      "[ Experience replay ] starts\n",
      "[ episode 414 ][ timestamp 3 ] state=[-0.02574981 -0.3540247   0.04925056  0.62618775], action=0, reward=1.0, next_state=[-0.03283031 -0.54979829  0.06177432  0.93396586]\n",
      "[ Experience replay ] starts\n",
      "[ episode 414 ][ timestamp 4 ] state=[-0.03283031 -0.54979829  0.06177432  0.93396586], action=0, reward=1.0, next_state=[-0.04382627 -0.74569675  0.08045363  1.24540316]\n",
      "[ Experience replay ] starts\n",
      "[ episode 414 ][ timestamp 5 ] state=[-0.04382627 -0.74569675  0.08045363  1.24540316], action=0, reward=1.0, next_state=[-0.05874021 -0.94175329  0.1053617   1.56216456]\n",
      "[ Experience replay ] starts\n",
      "[ episode 414 ][ timestamp 6 ] state=[-0.05874021 -0.94175329  0.1053617   1.56216456], action=0, reward=1.0, next_state=[-0.07757527 -1.13796595  0.13660499  1.88577049]\n",
      "[ Experience replay ] starts\n",
      "[ episode 414 ][ timestamp 7 ] state=[-0.07757527 -1.13796595  0.13660499  1.88577049], action=0, reward=1.0, next_state=[-0.10033459 -1.33428387  0.1743204   2.21754111]\n",
      "[ Experience replay ] starts\n",
      "[ episode 414 ][ timestamp 8 ] state=[-0.10033459 -1.33428387  0.1743204   2.21754111], action=0, reward=-1.0, next_state=[-0.12702027 -1.53059133  0.21867122  2.55853066]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 414: Exploration_rate=0.01. Score=8.\n",
      "[ episode 415 ] state=[-0.00122195 -0.00926345 -0.02172034  0.04678947]\n",
      "[ episode 415 ][ timestamp 1 ] state=[-0.00122195 -0.00926345 -0.02172034  0.04678947], action=0, reward=1.0, next_state=[-0.00140722 -0.20406732 -0.02078455  0.33254107]\n",
      "[ Experience replay ] starts\n",
      "[ episode 415 ][ timestamp 2 ] state=[-0.00140722 -0.20406732 -0.02078455  0.33254107], action=0, reward=1.0, next_state=[-0.00548856 -0.39888736 -0.01413373  0.6185978 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 415 ][ timestamp 3 ] state=[-0.00548856 -0.39888736 -0.01413373  0.6185978 ], action=0, reward=1.0, next_state=[-0.01346631 -0.59380907 -0.00176177  0.90679598]\n",
      "[ Experience replay ] starts\n",
      "[ episode 415 ][ timestamp 4 ] state=[-0.01346631 -0.59380907 -0.00176177  0.90679598], action=0, reward=1.0, next_state=[-0.02534249 -0.78890713  0.01637415  1.19892464]\n",
      "[ Experience replay ] starts\n",
      "[ episode 415 ][ timestamp 5 ] state=[-0.02534249 -0.78890713  0.01637415  1.19892464], action=0, reward=1.0, next_state=[-0.04112064 -0.98423707  0.04035264  1.49669407]\n",
      "[ Experience replay ] starts\n",
      "[ episode 415 ][ timestamp 6 ] state=[-0.04112064 -0.98423707  0.04035264  1.49669407], action=0, reward=1.0, next_state=[-0.06080538 -1.17982565  0.07028652  1.80169857]\n",
      "[ Experience replay ] starts\n",
      "[ episode 415 ][ timestamp 7 ] state=[-0.06080538 -1.17982565  0.07028652  1.80169857], action=0, reward=1.0, next_state=[-0.08440189 -1.37565922  0.10632049  2.11537085]\n",
      "[ Experience replay ] starts\n",
      "[ episode 415 ][ timestamp 8 ] state=[-0.08440189 -1.37565922  0.10632049  2.11537085], action=0, reward=1.0, next_state=[-0.11191507 -1.5716697   0.14862791  2.43892571]\n",
      "[ Experience replay ] starts\n",
      "[ episode 415 ][ timestamp 9 ] state=[-0.11191507 -1.5716697   0.14862791  2.43892571], action=0, reward=1.0, next_state=[-0.14334847 -1.76771802  0.19740642  2.77329201]\n",
      "[ Experience replay ] starts\n",
      "[ episode 415 ][ timestamp 10 ] state=[-0.14334847 -1.76771802  0.19740642  2.77329201], action=0, reward=-1.0, next_state=[-0.17870283 -1.96357519  0.25287226  3.11903329]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 415: Exploration_rate=0.01. Score=10.\n",
      "[ episode 416 ] state=[-0.02592012  0.04711902 -0.00618985  0.00090968]\n",
      "[ episode 416 ][ timestamp 1 ] state=[-0.02592012  0.04711902 -0.00618985  0.00090968], action=0, reward=1.0, next_state=[-0.02497774 -0.14791362 -0.00617166  0.29163322]\n",
      "[ Experience replay ] starts\n",
      "[ episode 416 ][ timestamp 2 ] state=[-0.02497774 -0.14791362 -0.00617166  0.29163322], action=0, reward=1.0, next_state=[-2.79360167e-02 -3.42947032e-01 -3.38993219e-04  5.82363311e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 416 ][ timestamp 3 ] state=[-2.79360167e-02 -3.42947032e-01 -3.38993219e-04  5.82363311e-01], action=0, reward=1.0, next_state=[-0.03479496 -0.53806423  0.01130827  0.87493943]\n",
      "[ Experience replay ] starts\n",
      "[ episode 416 ][ timestamp 4 ] state=[-0.03479496 -0.53806423  0.01130827  0.87493943], action=0, reward=1.0, next_state=[-0.04555624 -0.73333807  0.02880706  1.17115603]\n",
      "[ Experience replay ] starts\n",
      "[ episode 416 ][ timestamp 5 ] state=[-0.04555624 -0.73333807  0.02880706  1.17115603], action=0, reward=1.0, next_state=[-0.060223   -0.92882252  0.05223018  1.47272915]\n",
      "[ Experience replay ] starts\n",
      "[ episode 416 ][ timestamp 6 ] state=[-0.060223   -0.92882252  0.05223018  1.47272915], action=0, reward=1.0, next_state=[-0.07879945 -1.12454267  0.08168477  1.7812577 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 416 ][ timestamp 7 ] state=[-0.07879945 -1.12454267  0.08168477  1.7812577 ], action=0, reward=1.0, next_state=[-0.10129031 -1.32048287  0.11730992  2.09817663]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 416 ][ timestamp 8 ] state=[-0.10129031 -1.32048287  0.11730992  2.09817663], action=0, reward=1.0, next_state=[-0.12769996 -1.51657259  0.15927345  2.42469971]\n",
      "[ Experience replay ] starts\n",
      "[ episode 416 ][ timestamp 9 ] state=[-0.12769996 -1.51657259  0.15927345  2.42469971], action=0, reward=1.0, next_state=[-0.15803142 -1.71266969  0.20776745  2.76175097]\n",
      "[ Experience replay ] starts\n",
      "[ episode 416 ][ timestamp 10 ] state=[-0.15803142 -1.71266969  0.20776745  2.76175097], action=0, reward=-1.0, next_state=[-0.19228481 -1.90854155  0.26300247  3.10988519]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 416: Exploration_rate=0.01. Score=10.\n",
      "[ episode 417 ] state=[0.03667538 0.0320063  0.0452019  0.03397243]\n",
      "[ episode 417 ][ timestamp 1 ] state=[0.03667538 0.0320063  0.0452019  0.03397243], action=0, reward=1.0, next_state=[ 0.0373155  -0.16373373  0.04588135  0.34056742]\n",
      "[ Experience replay ] starts\n",
      "[ episode 417 ][ timestamp 2 ] state=[ 0.0373155  -0.16373373  0.04588135  0.34056742], action=0, reward=1.0, next_state=[ 0.03404083 -0.35947745  0.05269269  0.64735838]\n",
      "[ Experience replay ] starts\n",
      "[ episode 417 ][ timestamp 3 ] state=[ 0.03404083 -0.35947745  0.05269269  0.64735838], action=0, reward=1.0, next_state=[ 0.02685128 -0.55529241  0.06563986  0.95615764]\n",
      "[ Experience replay ] starts\n",
      "[ episode 417 ][ timestamp 4 ] state=[ 0.02685128 -0.55529241  0.06563986  0.95615764], action=0, reward=1.0, next_state=[ 0.01574543 -0.75123284  0.08476301  1.26871961]\n",
      "[ Experience replay ] starts\n",
      "[ episode 417 ][ timestamp 5 ] state=[ 0.01574543 -0.75123284  0.08476301  1.26871961], action=0, reward=1.0, next_state=[ 7.20773363e-04 -9.47328812e-01  1.10137407e-01  1.58669802e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 417 ][ timestamp 6 ] state=[ 7.20773363e-04 -9.47328812e-01  1.10137407e-01  1.58669802e+00], action=0, reward=1.0, next_state=[-0.0182258  -1.1435741   0.14187137  1.91159734]\n",
      "[ Experience replay ] starts\n",
      "[ episode 417 ][ timestamp 7 ] state=[-0.0182258  -1.1435741   0.14187137  1.91159734], action=0, reward=1.0, next_state=[-0.04109728 -1.33991219  0.18010331  2.24471601]\n",
      "[ Experience replay ] starts\n",
      "[ episode 417 ][ timestamp 8 ] state=[-0.04109728 -1.33991219  0.18010331  2.24471601], action=0, reward=-1.0, next_state=[-0.06789553 -1.5362201   0.22499763  2.58707959]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 417: Exploration_rate=0.01. Score=8.\n",
      "[ episode 418 ] state=[ 0.04489471  0.02184922  0.03357894 -0.0155246 ]\n",
      "[ episode 418 ][ timestamp 1 ] state=[ 0.04489471  0.02184922  0.03357894 -0.0155246 ], action=0, reward=1.0, next_state=[ 0.0453317  -0.1737378   0.03326845  0.2875609 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 418 ][ timestamp 2 ] state=[ 0.0453317  -0.1737378   0.03326845  0.2875609 ], action=0, reward=1.0, next_state=[ 0.04185694 -0.369318    0.03901966  0.59054799]\n",
      "[ Experience replay ] starts\n",
      "[ episode 418 ][ timestamp 3 ] state=[ 0.04185694 -0.369318    0.03901966  0.59054799], action=0, reward=1.0, next_state=[ 0.03447058 -0.56496393  0.05083062  0.89526236]\n",
      "[ Experience replay ] starts\n",
      "[ episode 418 ][ timestamp 4 ] state=[ 0.03447058 -0.56496393  0.05083062  0.89526236], action=0, reward=1.0, next_state=[ 0.0231713  -0.7607369   0.06873587  1.2034803 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 418 ][ timestamp 5 ] state=[ 0.0231713  -0.7607369   0.06873587  1.2034803 ], action=0, reward=1.0, next_state=[ 0.00795656 -0.95667684  0.09280548  1.51688862]\n",
      "[ Experience replay ] starts\n",
      "[ episode 418 ][ timestamp 6 ] state=[ 0.00795656 -0.95667684  0.09280548  1.51688862], action=0, reward=1.0, next_state=[-0.01117697 -1.15279086  0.12314325  1.83703938]\n",
      "[ Experience replay ] starts\n",
      "[ episode 418 ][ timestamp 7 ] state=[-0.01117697 -1.15279086  0.12314325  1.83703938], action=0, reward=1.0, next_state=[-0.03423279 -1.34903999  0.15988404  2.16529661]\n",
      "[ Experience replay ] starts\n",
      "[ episode 418 ][ timestamp 8 ] state=[-0.03423279 -1.34903999  0.15988404  2.16529661], action=0, reward=1.0, next_state=[-0.06121359 -1.54532379  0.20318997  2.50277303]\n",
      "[ Experience replay ] starts\n",
      "[ episode 418 ][ timestamp 9 ] state=[-0.06121359 -1.54532379  0.20318997  2.50277303], action=0, reward=-1.0, next_state=[-0.09212007 -1.74146264  0.25324543  2.85025644]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 418: Exploration_rate=0.01. Score=9.\n",
      "[ episode 419 ] state=[-0.04922806  0.02835245 -0.00520521 -0.03838087]\n",
      "[ episode 419 ][ timestamp 1 ] state=[-0.04922806  0.02835245 -0.00520521 -0.03838087], action=0, reward=1.0, next_state=[-0.04866101 -0.16669447 -0.00597282  0.25265523]\n",
      "[ Experience replay ] starts\n",
      "[ episode 419 ][ timestamp 2 ] state=[-0.04866101 -0.16669447 -0.00597282  0.25265523], action=0, reward=1.0, next_state=[-0.0519949  -0.36173063 -0.00091972  0.54344825]\n",
      "[ Experience replay ] starts\n",
      "[ episode 419 ][ timestamp 3 ] state=[-0.0519949  -0.36173063 -0.00091972  0.54344825], action=0, reward=1.0, next_state=[-0.05922951 -0.55683964  0.00994924  0.83584125]\n",
      "[ Experience replay ] starts\n",
      "[ episode 419 ][ timestamp 4 ] state=[-0.05922951 -0.55683964  0.00994924  0.83584125], action=0, reward=1.0, next_state=[-0.0703663  -0.75209608  0.02666607  1.13163643]\n",
      "[ Experience replay ] starts\n",
      "[ episode 419 ][ timestamp 5 ] state=[-0.0703663  -0.75209608  0.02666607  1.13163643], action=0, reward=1.0, next_state=[-0.08540823 -0.9475568   0.0492988   1.43256217]\n",
      "[ Experience replay ] starts\n",
      "[ episode 419 ][ timestamp 6 ] state=[-0.08540823 -0.9475568   0.0492988   1.43256217], action=0, reward=1.0, next_state=[-0.10435936 -1.14325118  0.07795004  1.74023509]\n",
      "[ Experience replay ] starts\n",
      "[ episode 419 ][ timestamp 7 ] state=[-0.10435936 -1.14325118  0.07795004  1.74023509], action=0, reward=1.0, next_state=[-0.12722439 -1.33916953  0.11275474  2.05611434]\n",
      "[ Experience replay ] starts\n",
      "[ episode 419 ][ timestamp 8 ] state=[-0.12722439 -1.33916953  0.11275474  2.05611434], action=0, reward=1.0, next_state=[-0.15400778 -1.53524917  0.15387703  2.38144582]\n",
      "[ Experience replay ] starts\n",
      "[ episode 419 ][ timestamp 9 ] state=[-0.15400778 -1.53524917  0.15387703  2.38144582], action=0, reward=1.0, next_state=[-0.18471276 -1.73135814  0.20150595  2.71719504]\n",
      "[ Experience replay ] starts\n",
      "[ episode 419 ][ timestamp 10 ] state=[-0.18471276 -1.73135814  0.20150595  2.71719504], action=0, reward=-1.0, next_state=[-0.21933992 -1.92727647  0.25584985  3.06396895]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 419: Exploration_rate=0.01. Score=10.\n",
      "[ episode 420 ] state=[-0.03530953 -0.04877165  0.04878321 -0.04118491]\n",
      "[ episode 420 ][ timestamp 1 ] state=[-0.03530953 -0.04877165  0.04878321 -0.04118491], action=0, reward=1.0, next_state=[-0.03628496 -0.24455797  0.04795951  0.26648176]\n",
      "[ Experience replay ] starts\n",
      "[ episode 420 ][ timestamp 2 ] state=[-0.03628496 -0.24455797  0.04795951  0.26648176], action=0, reward=1.0, next_state=[-0.04117612 -0.44033043  0.05328914  0.57389749]\n",
      "[ Experience replay ] starts\n",
      "[ episode 420 ][ timestamp 3 ] state=[-0.04117612 -0.44033043  0.05328914  0.57389749], action=0, reward=1.0, next_state=[-0.04998273 -0.63615741  0.06476709  0.88288058]\n",
      "[ Experience replay ] starts\n",
      "[ episode 420 ][ timestamp 4 ] state=[-0.04998273 -0.63615741  0.06476709  0.88288058], action=0, reward=1.0, next_state=[-0.06270588 -0.83209635  0.08242471  1.19520098]\n",
      "[ Experience replay ] starts\n",
      "[ episode 420 ][ timestamp 5 ] state=[-0.06270588 -0.83209635  0.08242471  1.19520098], action=0, reward=1.0, next_state=[-0.07934781 -1.02818304  0.10632872  1.51253789]\n",
      "[ Experience replay ] starts\n",
      "[ episode 420 ][ timestamp 6 ] state=[-0.07934781 -1.02818304  0.10632872  1.51253789], action=0, reward=1.0, next_state=[-0.09991147 -1.22441983  0.13657948  1.83643245]\n",
      "[ Experience replay ] starts\n",
      "[ episode 420 ][ timestamp 7 ] state=[-0.09991147 -1.22441983  0.13657948  1.83643245], action=0, reward=1.0, next_state=[-0.12439986 -1.42076195  0.17330813  2.16823263]\n",
      "[ Experience replay ] starts\n",
      "[ episode 420 ][ timestamp 8 ] state=[-0.12439986 -1.42076195  0.17330813  2.16823263], action=0, reward=-1.0, next_state=[-0.1528151  -1.61710181  0.21667278  2.50902848]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 420: Exploration_rate=0.01. Score=8.\n",
      "[ episode 421 ] state=[-0.02306671  0.02624877 -0.0131663  -0.04612614]\n",
      "[ episode 421 ][ timestamp 1 ] state=[-0.02306671  0.02624877 -0.0131663  -0.04612614], action=0, reward=1.0, next_state=[-0.02254173 -0.16868193 -0.01408882  0.2423738 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 421 ][ timestamp 2 ] state=[-0.02254173 -0.16868193 -0.01408882  0.2423738 ], action=0, reward=1.0, next_state=[-0.02591537 -0.36359983 -0.00924135  0.53057965]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 421 ][ timestamp 3 ] state=[-0.02591537 -0.36359983 -0.00924135  0.53057965], action=0, reward=1.0, next_state=[-0.03318737 -0.55859057  0.00137025  0.82033636]\n",
      "[ Experience replay ] starts\n",
      "[ episode 421 ][ timestamp 4 ] state=[-0.03318737 -0.55859057  0.00137025  0.82033636], action=0, reward=1.0, next_state=[-0.04435918 -0.75373125  0.01777697  1.11344995]\n",
      "[ Experience replay ] starts\n",
      "[ episode 421 ][ timestamp 5 ] state=[-0.04435918 -0.75373125  0.01777697  1.11344995], action=0, reward=1.0, next_state=[-0.0594338  -0.94908208  0.04004597  1.41165605]\n",
      "[ Experience replay ] starts\n",
      "[ episode 421 ][ timestamp 6 ] state=[-0.0594338  -0.94908208  0.04004597  1.41165605], action=0, reward=1.0, next_state=[-0.07841545 -1.14467696  0.06827909  1.71658352]\n",
      "[ Experience replay ] starts\n",
      "[ episode 421 ][ timestamp 7 ] state=[-0.07841545 -1.14467696  0.06827909  1.71658352], action=0, reward=1.0, next_state=[-0.10130899 -1.34051226  0.10261076  2.02971044]\n",
      "[ Experience replay ] starts\n",
      "[ episode 421 ][ timestamp 8 ] state=[-0.10130899 -1.34051226  0.10261076  2.02971044], action=0, reward=1.0, next_state=[-0.12811923 -1.53653331  0.14320497  2.35231011]\n",
      "[ Experience replay ] starts\n",
      "[ episode 421 ][ timestamp 9 ] state=[-0.12811923 -1.53653331  0.14320497  2.35231011], action=0, reward=1.0, next_state=[-0.1588499  -1.73261839  0.19025117  2.68538546]\n",
      "[ Experience replay ] starts\n",
      "[ episode 421 ][ timestamp 10 ] state=[-0.1588499  -1.73261839  0.19025117  2.68538546], action=0, reward=-1.0, next_state=[-0.19350226 -1.92856031  0.24395888  3.02959223]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 421: Exploration_rate=0.01. Score=10.\n",
      "[ episode 422 ] state=[ 0.00694751 -0.03161084 -0.01934916 -0.0003649 ]\n",
      "[ episode 422 ][ timestamp 1 ] state=[ 0.00694751 -0.03161084 -0.01934916 -0.0003649 ], action=0, reward=1.0, next_state=[ 0.00631529 -0.22645002 -0.01935646  0.28615088]\n",
      "[ Experience replay ] starts\n",
      "[ episode 422 ][ timestamp 2 ] state=[ 0.00631529 -0.22645002 -0.01935646  0.28615088], action=0, reward=1.0, next_state=[ 0.00178629 -0.42129065 -0.01363344  0.57266662]\n",
      "[ Experience replay ] starts\n",
      "[ episode 422 ][ timestamp 3 ] state=[ 0.00178629 -0.42129065 -0.01363344  0.57266662], action=0, reward=1.0, next_state=[-0.00663952 -0.61621881 -0.00218011  0.86102359]\n",
      "[ Experience replay ] starts\n",
      "[ episode 422 ][ timestamp 4 ] state=[-0.00663952 -0.61621881 -0.00218011  0.86102359], action=0, reward=1.0, next_state=[-0.0189639  -0.81131101  0.01504036  1.15302023]\n",
      "[ Experience replay ] starts\n",
      "[ episode 422 ][ timestamp 5 ] state=[-0.0189639  -0.81131101  0.01504036  1.15302023], action=0, reward=1.0, next_state=[-0.03519012 -1.00662589  0.03810077  1.45038112]\n",
      "[ Experience replay ] starts\n",
      "[ episode 422 ][ timestamp 6 ] state=[-0.03519012 -1.00662589  0.03810077  1.45038112], action=0, reward=1.0, next_state=[-0.05532264 -1.20219479  0.06710839  1.75472049]\n",
      "[ Experience replay ] starts\n",
      "[ episode 422 ][ timestamp 7 ] state=[-0.05532264 -1.20219479  0.06710839  1.75472049], action=0, reward=1.0, next_state=[-0.07936653 -1.39801042  0.1022028   2.06749784]\n",
      "[ Experience replay ] starts\n",
      "[ episode 422 ][ timestamp 8 ] state=[-0.07936653 -1.39801042  0.1022028   2.06749784], action=0, reward=1.0, next_state=[-0.10732674 -1.59401317  0.14355276  2.38996314]\n",
      "[ Experience replay ] starts\n",
      "[ episode 422 ][ timestamp 9 ] state=[-0.10732674 -1.59401317  0.14355276  2.38996314], action=0, reward=1.0, next_state=[-0.13920701 -1.79007488  0.19135202  2.72309038]\n",
      "[ Experience replay ] starts\n",
      "[ episode 422 ][ timestamp 10 ] state=[-0.13920701 -1.79007488  0.19135202  2.72309038], action=0, reward=-1.0, next_state=[-0.1750085  -1.98598023  0.24581383  3.06749971]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 422: Exploration_rate=0.01. Score=10.\n",
      "[ episode 423 ] state=[-0.02316017 -0.03747686 -0.04896684  0.01281779]\n",
      "[ episode 423 ][ timestamp 1 ] state=[-0.02316017 -0.03747686 -0.04896684  0.01281779], action=0, reward=1.0, next_state=[-0.02390971 -0.23186361 -0.04871049  0.28965791]\n",
      "[ Experience replay ] starts\n",
      "[ episode 423 ][ timestamp 2 ] state=[-0.02390971 -0.23186361 -0.04871049  0.28965791], action=0, reward=1.0, next_state=[-0.02854698 -0.42625835 -0.04291733  0.56658894]\n",
      "[ Experience replay ] starts\n",
      "[ episode 423 ][ timestamp 3 ] state=[-0.02854698 -0.42625835 -0.04291733  0.56658894], action=0, reward=1.0, next_state=[-0.03707215 -0.6207528  -0.03158555  0.84544815]\n",
      "[ Experience replay ] starts\n",
      "[ episode 423 ][ timestamp 4 ] state=[-0.03707215 -0.6207528  -0.03158555  0.84544815], action=0, reward=1.0, next_state=[-0.0494872  -0.81542988 -0.01467659  1.12803352]\n",
      "[ Experience replay ] starts\n",
      "[ episode 423 ][ timestamp 5 ] state=[-0.0494872  -0.81542988 -0.01467659  1.12803352], action=0, reward=1.0, next_state=[-0.0657958  -1.01035653  0.00788408  1.41607724]\n",
      "[ Experience replay ] starts\n",
      "[ episode 423 ][ timestamp 6 ] state=[-0.0657958  -1.01035653  0.00788408  1.41607724], action=0, reward=1.0, next_state=[-0.08600293 -1.20557523  0.03620563  1.71121409]\n",
      "[ Experience replay ] starts\n",
      "[ episode 423 ][ timestamp 7 ] state=[-0.08600293 -1.20557523  0.03620563  1.71121409], action=0, reward=1.0, next_state=[-0.11011444 -1.40109381  0.07042991  2.01494189]\n",
      "[ Experience replay ] starts\n",
      "[ episode 423 ][ timestamp 8 ] state=[-0.11011444 -1.40109381  0.07042991  2.01494189], action=0, reward=1.0, next_state=[-0.13813631 -1.59687281  0.11072875  2.32857162]\n",
      "[ Experience replay ] starts\n",
      "[ episode 423 ][ timestamp 9 ] state=[-0.13813631 -1.59687281  0.11072875  2.32857162], action=0, reward=1.0, next_state=[-0.17007377 -1.79281021  0.15730018  2.65316555]\n",
      "[ Experience replay ] starts\n",
      "[ episode 423 ][ timestamp 10 ] state=[-0.17007377 -1.79281021  0.15730018  2.65316555], action=0, reward=-1.0, next_state=[-0.20592997 -1.98872344  0.21036349  2.98946299]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 423: Exploration_rate=0.01. Score=10.\n",
      "[ episode 424 ] state=[-0.00927164  0.02516961 -0.01475027 -0.04966785]\n",
      "[ episode 424 ][ timestamp 1 ] state=[-0.00927164  0.02516961 -0.01475027 -0.04966785], action=0, reward=1.0, next_state=[-0.00876825 -0.16973777 -0.01574363  0.23832498]\n",
      "[ Experience replay ] starts\n",
      "[ episode 424 ][ timestamp 2 ] state=[-0.00876825 -0.16973777 -0.01574363  0.23832498], action=0, reward=1.0, next_state=[-0.012163   -0.36463131 -0.01097713  0.52600063]\n",
      "[ Experience replay ] starts\n",
      "[ episode 424 ][ timestamp 3 ] state=[-0.012163   -0.36463131 -0.01097713  0.52600063], action=0, reward=1.0, next_state=[-1.94556275e-02 -5.59597086e-01 -4.57117944e-04  8.15204464e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 424 ][ timestamp 4 ] state=[-1.94556275e-02 -5.59597086e-01 -4.57117944e-04  8.15204464e-01], action=0, reward=1.0, next_state=[-0.03064757 -0.75471277  0.01584697  1.10774357]\n",
      "[ Experience replay ] starts\n",
      "[ episode 424 ][ timestamp 5 ] state=[-0.03064757 -0.75471277  0.01584697  1.10774357], action=0, reward=1.0, next_state=[-0.04574182 -0.9500394   0.03800184  1.40535553]\n",
      "[ Experience replay ] starts\n",
      "[ episode 424 ][ timestamp 6 ] state=[-0.04574182 -0.9500394   0.03800184  1.40535553], action=0, reward=1.0, next_state=[-0.06474261 -1.14561196  0.06610895  1.70967244]\n",
      "[ Experience replay ] starts\n",
      "[ episode 424 ][ timestamp 7 ] state=[-0.06474261 -1.14561196  0.06610895  1.70967244], action=0, reward=1.0, next_state=[-0.08765485 -1.34142834  0.1003024   2.02217727]\n",
      "[ Experience replay ] starts\n",
      "[ episode 424 ][ timestamp 8 ] state=[-0.08765485 -1.34142834  0.1003024   2.02217727], action=0, reward=1.0, next_state=[-0.11448342 -1.53743586  0.14074595  2.34415031]\n",
      "[ Experience replay ] starts\n",
      "[ episode 424 ][ timestamp 9 ] state=[-0.11448342 -1.53743586  0.14074595  2.34415031], action=0, reward=1.0, next_state=[-0.14523214 -1.73351537  0.18762895  2.67660404]\n",
      "[ Experience replay ] starts\n",
      "[ episode 424 ][ timestamp 10 ] state=[-0.14523214 -1.73351537  0.18762895  2.67660404], action=0, reward=-1.0, next_state=[-0.17990244 -1.92946288  0.24116103  3.02020659]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 424: Exploration_rate=0.01. Score=10.\n",
      "[ episode 425 ] state=[ 0.0364642   0.01265388  0.03156671 -0.00341019]\n",
      "[ episode 425 ][ timestamp 1 ] state=[ 0.0364642   0.01265388  0.03156671 -0.00341019], action=0, reward=1.0, next_state=[ 0.03671728 -0.18290623  0.0314985   0.2990629 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 425 ][ timestamp 2 ] state=[ 0.03671728 -0.18290623  0.0314985   0.2990629 ], action=0, reward=1.0, next_state=[ 0.03305915 -0.37846268  0.03747976  0.6015111 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 425 ][ timestamp 3 ] state=[ 0.03305915 -0.37846268  0.03747976  0.6015111 ], action=0, reward=1.0, next_state=[ 0.0254899  -0.57408831  0.04950998  0.90575995]\n",
      "[ Experience replay ] starts\n",
      "[ episode 425 ][ timestamp 4 ] state=[ 0.0254899  -0.57408831  0.04950998  0.90575995], action=0, reward=1.0, next_state=[ 0.01400813 -0.76984446  0.06762518  1.21358434]\n",
      "[ Experience replay ] starts\n",
      "[ episode 425 ][ timestamp 5 ] state=[ 0.01400813 -0.76984446  0.06762518  1.21358434], action=0, reward=1.0, next_state=[-1.38875546e-03 -9.65770743e-01  9.18968708e-02  1.52666868e+00]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 425 ][ timestamp 6 ] state=[-1.38875546e-03 -9.65770743e-01  9.18968708e-02  1.52666868e+00], action=0, reward=1.0, next_state=[-0.02070417 -1.16187372  0.12243024  1.84656161]\n",
      "[ Experience replay ] starts\n",
      "[ episode 425 ][ timestamp 7 ] state=[-0.02070417 -1.16187372  0.12243024  1.84656161], action=0, reward=1.0, next_state=[-0.04394164 -1.35811359  0.15936148  2.1746227 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 425 ][ timestamp 8 ] state=[-0.04394164 -1.35811359  0.15936148  2.1746227 ], action=0, reward=1.0, next_state=[-0.07110392 -1.55438873  0.20285393  2.51195908]\n",
      "[ Experience replay ] starts\n",
      "[ episode 425 ][ timestamp 9 ] state=[-0.07110392 -1.55438873  0.20285393  2.51195908], action=0, reward=-1.0, next_state=[-0.10219169 -1.75051804  0.25309311  2.85935165]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 425: Exploration_rate=0.01. Score=9.\n",
      "[ episode 426 ] state=[ 0.03805554 -0.04290938 -0.04362569  0.0465677 ]\n",
      "[ episode 426 ][ timestamp 1 ] state=[ 0.03805554 -0.04290938 -0.04362569  0.0465677 ], action=0, reward=1.0, next_state=[ 0.03719735 -0.23737949 -0.04269433  0.32517345]\n",
      "[ Experience replay ] starts\n",
      "[ episode 426 ][ timestamp 2 ] state=[ 0.03719735 -0.23737949 -0.04269433  0.32517345], action=0, reward=1.0, next_state=[ 0.03244976 -0.43186837 -0.03619087  0.6040926 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 426 ][ timestamp 3 ] state=[ 0.03244976 -0.43186837 -0.03619087  0.6040926 ], action=0, reward=1.0, next_state=[ 0.02381239 -0.62646598 -0.02410901  0.88516009]\n",
      "[ Experience replay ] starts\n",
      "[ episode 426 ][ timestamp 4 ] state=[ 0.02381239 -0.62646598 -0.02410901  0.88516009], action=0, reward=1.0, next_state=[ 0.01128307 -0.82125245 -0.00640581  1.17016752]\n",
      "[ Experience replay ] starts\n",
      "[ episode 426 ][ timestamp 5 ] state=[ 0.01128307 -0.82125245 -0.00640581  1.17016752], action=0, reward=1.0, next_state=[-0.00514198 -1.01629051  0.01699754  1.46083531]\n",
      "[ Experience replay ] starts\n",
      "[ episode 426 ][ timestamp 6 ] state=[-0.00514198 -1.01629051  0.01699754  1.46083531], action=0, reward=1.0, next_state=[-0.02546779 -1.21161667  0.04621424  1.75877926]\n",
      "[ Experience replay ] starts\n",
      "[ episode 426 ][ timestamp 7 ] state=[-0.02546779 -1.21161667  0.04621424  1.75877926], action=0, reward=1.0, next_state=[-0.04970012 -1.40723049  0.08138983  2.06546886]\n",
      "[ Experience replay ] starts\n",
      "[ episode 426 ][ timestamp 8 ] state=[-0.04970012 -1.40723049  0.08138983  2.06546886], action=0, reward=1.0, next_state=[-0.07784473 -1.60308144  0.12269921  2.38217499]\n",
      "[ Experience replay ] starts\n",
      "[ episode 426 ][ timestamp 9 ] state=[-0.07784473 -1.60308144  0.12269921  2.38217499], action=0, reward=1.0, next_state=[-0.10990636 -1.79905307  0.17034271  2.70990555]\n",
      "[ Experience replay ] starts\n",
      "[ episode 426 ][ timestamp 10 ] state=[-0.10990636 -1.79905307  0.17034271  2.70990555], action=0, reward=-1.0, next_state=[-0.14588742 -1.99494457  0.22454082  3.04932893]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 426: Exploration_rate=0.01. Score=10.\n",
      "[ episode 427 ] state=[-0.02133081 -0.00490283  0.03255266  0.03035862]\n",
      "[ episode 427 ][ timestamp 1 ] state=[-0.02133081 -0.00490283  0.03255266  0.03035862], action=0, reward=1.0, next_state=[-0.02142886 -0.20047611  0.03315984  0.33313192]\n",
      "[ Experience replay ] starts\n",
      "[ episode 427 ][ timestamp 2 ] state=[-0.02142886 -0.20047611  0.03315984  0.33313192], action=0, reward=1.0, next_state=[-0.02543838 -0.39605396  0.03982247  0.63608462]\n",
      "[ Experience replay ] starts\n",
      "[ episode 427 ][ timestamp 3 ] state=[-0.02543838 -0.39605396  0.03982247  0.63608462], action=0, reward=1.0, next_state=[-0.03335946 -0.59170801  0.05254417  0.94103774]\n",
      "[ Experience replay ] starts\n",
      "[ episode 427 ][ timestamp 4 ] state=[-0.03335946 -0.59170801  0.05254417  0.94103774], action=0, reward=1.0, next_state=[-0.04519362 -0.78749724  0.07136492  1.24975714]\n",
      "[ Experience replay ] starts\n",
      "[ episode 427 ][ timestamp 5 ] state=[-0.04519362 -0.78749724  0.07136492  1.24975714], action=0, reward=1.0, next_state=[-0.06094357 -0.98345765  0.09636006  1.56391303]\n",
      "[ Experience replay ] starts\n",
      "[ episode 427 ][ timestamp 6 ] state=[-0.06094357 -0.98345765  0.09636006  1.56391303], action=0, reward=1.0, next_state=[-0.08061272 -1.17959058  0.12763833  1.88503368]\n",
      "[ Experience replay ] starts\n",
      "[ episode 427 ][ timestamp 7 ] state=[-0.08061272 -1.17959058  0.12763833  1.88503368], action=0, reward=1.0, next_state=[-0.10420453 -1.37584927  0.165339    2.2144508 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 427 ][ timestamp 8 ] state=[-0.10420453 -1.37584927  0.165339    2.2144508 ], action=0, reward=-1.0, next_state=[-0.13172152 -1.572123    0.20962802  2.5532349 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 427: Exploration_rate=0.01. Score=8.\n",
      "[ episode 428 ] state=[-0.0025916  -0.02068934  0.01104216 -0.02994387]\n",
      "[ episode 428 ][ timestamp 1 ] state=[-0.0025916  -0.02068934  0.01104216 -0.02994387], action=0, reward=1.0, next_state=[-0.00300539 -0.21596789  0.01044328  0.26620242]\n",
      "[ Experience replay ] starts\n",
      "[ episode 428 ][ timestamp 2 ] state=[-0.00300539 -0.21596789  0.01044328  0.26620242], action=0, reward=1.0, next_state=[-0.00732474 -0.41123732  0.01576733  0.56216087]\n",
      "[ Experience replay ] starts\n",
      "[ episode 428 ][ timestamp 3 ] state=[-0.00732474 -0.41123732  0.01576733  0.56216087], action=0, reward=1.0, next_state=[-0.01554949 -0.60657695  0.02701055  0.85976929]\n",
      "[ Experience replay ] starts\n",
      "[ episode 428 ][ timestamp 4 ] state=[-0.01554949 -0.60657695  0.02701055  0.85976929], action=0, reward=1.0, next_state=[-0.02768103 -0.80205617  0.04420593  1.1608213 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 428 ][ timestamp 5 ] state=[-0.02768103 -0.80205617  0.04420593  1.1608213 ], action=0, reward=1.0, next_state=[-0.04372215 -0.99772522  0.06742236  1.46703046]\n",
      "[ Experience replay ] starts\n",
      "[ episode 428 ][ timestamp 6 ] state=[-0.04372215 -0.99772522  0.06742236  1.46703046], action=0, reward=1.0, next_state=[-0.06367666 -1.19360466  0.09676297  1.77998922]\n",
      "[ Experience replay ] starts\n",
      "[ episode 428 ][ timestamp 7 ] state=[-0.06367666 -1.19360466  0.09676297  1.77998922], action=0, reward=1.0, next_state=[-0.08754875 -1.38967315  0.13236275  2.1011201 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 428 ][ timestamp 8 ] state=[-0.08754875 -1.38967315  0.13236275  2.1011201 ], action=0, reward=1.0, next_state=[-0.11534221 -1.58585283  0.17438516  2.43161672]\n",
      "[ Experience replay ] starts\n",
      "[ episode 428 ][ timestamp 9 ] state=[-0.11534221 -1.58585283  0.17438516  2.43161672], action=0, reward=-1.0, next_state=[-0.14705927 -1.78199243  0.22301749  2.77237375]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 428: Exploration_rate=0.01. Score=9.\n",
      "[ episode 429 ] state=[-0.01859776  0.01692502 -0.03408402 -0.00025044]\n",
      "[ episode 429 ][ timestamp 1 ] state=[-0.01859776  0.01692502 -0.03408402 -0.00025044], action=0, reward=1.0, next_state=[-0.01825926 -0.17769195 -0.03408903  0.28148672]\n",
      "[ Experience replay ] starts\n",
      "[ episode 429 ][ timestamp 2 ] state=[-0.01825926 -0.17769195 -0.03408903  0.28148672], action=0, reward=1.0, next_state=[-0.0218131  -0.37231149 -0.02845929  0.56322618]\n",
      "[ Experience replay ] starts\n",
      "[ episode 429 ][ timestamp 3 ] state=[-0.0218131  -0.37231149 -0.02845929  0.56322618], action=0, reward=1.0, next_state=[-0.02925933 -0.56702278 -0.01719477  0.84680895]\n",
      "[ Experience replay ] starts\n",
      "[ episode 429 ][ timestamp 4 ] state=[-0.02925933 -0.56702278 -0.01719477  0.84680895], action=0, reward=1.0, next_state=[-4.05997829e-02 -7.61905999e-01 -2.58591612e-04  1.13403555e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 429 ][ timestamp 5 ] state=[-4.05997829e-02 -7.61905999e-01 -2.58591612e-04  1.13403555e+00], action=0, reward=1.0, next_state=[-0.0558379  -0.95702457  0.02242212  1.42663736]\n",
      "[ Experience replay ] starts\n",
      "[ episode 429 ][ timestamp 6 ] state=[-0.0558379  -0.95702457  0.02242212  1.42663736], action=0, reward=1.0, next_state=[-0.07497839 -1.15241627  0.05095487  1.7262428 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 429 ][ timestamp 7 ] state=[-0.07497839 -1.15241627  0.05095487  1.7262428 ], action=0, reward=1.0, next_state=[-0.09802672 -1.34808251  0.08547972  2.03433547]\n",
      "[ Experience replay ] starts\n",
      "[ episode 429 ][ timestamp 8 ] state=[-0.09802672 -1.34808251  0.08547972  2.03433547], action=0, reward=1.0, next_state=[-0.12498837 -1.54397521  0.12616643  2.35220211]\n",
      "[ Experience replay ] starts\n",
      "[ episode 429 ][ timestamp 9 ] state=[-0.12498837 -1.54397521  0.12616643  2.35220211], action=0, reward=1.0, next_state=[-0.15586787 -1.73998112  0.17321047  2.68086867]\n",
      "[ Experience replay ] starts\n",
      "[ episode 429 ][ timestamp 10 ] state=[-0.15586787 -1.73998112  0.17321047  2.68086867], action=0, reward=-1.0, next_state=[-0.1906675  -1.93590351  0.22682785  3.02102436]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 429: Exploration_rate=0.01. Score=10.\n",
      "[ episode 430 ] state=[ 0.03197505  0.00993022  0.00400075 -0.0400504 ]\n",
      "[ episode 430 ][ timestamp 1 ] state=[ 0.03197505  0.00993022  0.00400075 -0.0400504 ], action=0, reward=1.0, next_state=[ 0.03217366 -0.18524888  0.00319974  0.25389211]\n",
      "[ Experience replay ] starts\n",
      "[ episode 430 ][ timestamp 2 ] state=[ 0.03217366 -0.18524888  0.00319974  0.25389211], action=0, reward=1.0, next_state=[ 0.02846868 -0.38041637  0.00827758  0.54758258]\n",
      "[ Experience replay ] starts\n",
      "[ episode 430 ][ timestamp 3 ] state=[ 0.02846868 -0.38041637  0.00827758  0.54758258], action=0, reward=1.0, next_state=[ 0.02086035 -0.57565363  0.01922923  0.84286201]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 430 ][ timestamp 4 ] state=[ 0.02086035 -0.57565363  0.01922923  0.84286201], action=0, reward=1.0, next_state=[ 0.00934728 -0.77103267  0.03608647  1.14152945]\n",
      "[ Experience replay ] starts\n",
      "[ episode 430 ][ timestamp 5 ] state=[ 0.00934728 -0.77103267  0.03608647  1.14152945], action=0, reward=1.0, next_state=[-0.00607337 -0.96660721  0.05891706  1.44530738]\n",
      "[ Experience replay ] starts\n",
      "[ episode 430 ][ timestamp 6 ] state=[-0.00607337 -0.96660721  0.05891706  1.44530738], action=0, reward=1.0, next_state=[-0.02540552 -1.16240249  0.08782321  1.75580231]\n",
      "[ Experience replay ] starts\n",
      "[ episode 430 ][ timestamp 7 ] state=[-0.02540552 -1.16240249  0.08782321  1.75580231], action=0, reward=1.0, next_state=[-0.04865357 -1.35840336  0.12293926  2.07445738]\n",
      "[ Experience replay ] starts\n",
      "[ episode 430 ][ timestamp 8 ] state=[-0.04865357 -1.35840336  0.12293926  2.07445738], action=0, reward=1.0, next_state=[-0.07582163 -1.55454006  0.1644284   2.40249507]\n",
      "[ Experience replay ] starts\n",
      "[ episode 430 ][ timestamp 9 ] state=[-0.07582163 -1.55454006  0.1644284   2.40249507], action=0, reward=-1.0, next_state=[-0.10691244 -1.75067159  0.2124783   2.74084868]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 430: Exploration_rate=0.01. Score=9.\n",
      "[ episode 431 ] state=[-0.02440875  0.00151841  0.02068447  0.02174421]\n",
      "[ episode 431 ][ timestamp 1 ] state=[-0.02440875  0.00151841  0.02068447  0.02174421], action=1, reward=1.0, next_state=[-0.02437838  0.19633771  0.02111935 -0.26434143]\n",
      "[ Experience replay ] starts\n",
      "[ episode 431 ][ timestamp 2 ] state=[-0.02437838  0.19633771  0.02111935 -0.26434143], action=1, reward=1.0, next_state=[-0.02045162  0.39115196  0.01583252 -0.550289  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 431 ][ timestamp 3 ] state=[-0.02045162  0.39115196  0.01583252 -0.550289  ], action=1, reward=1.0, next_state=[-0.01262859  0.58604799  0.00482674 -0.83794184]\n",
      "[ Experience replay ] starts\n",
      "[ episode 431 ][ timestamp 4 ] state=[-0.01262859  0.58604799  0.00482674 -0.83794184], action=1, reward=1.0, next_state=[-9.07625512e-04  7.81103689e-01 -1.19320933e-02 -1.12910293e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 431 ][ timestamp 5 ] state=[-9.07625512e-04  7.81103689e-01 -1.19320933e-02 -1.12910293e+00], action=0, reward=1.0, next_state=[ 0.01471445  0.58614004 -0.03451415 -0.84018622]\n",
      "[ Experience replay ] starts\n",
      "[ episode 431 ][ timestamp 6 ] state=[ 0.01471445  0.58614004 -0.03451415 -0.84018622], action=0, reward=1.0, next_state=[ 0.02643725  0.39150587 -0.05131788 -0.55855399]\n",
      "[ Experience replay ] starts\n",
      "[ episode 431 ][ timestamp 7 ] state=[ 0.02643725  0.39150587 -0.05131788 -0.55855399], action=0, reward=1.0, next_state=[ 0.03426737  0.19714041 -0.06248896 -0.28247044]\n",
      "[ Experience replay ] starts\n",
      "[ episode 431 ][ timestamp 8 ] state=[ 0.03426737  0.19714041 -0.06248896 -0.28247044], action=0, reward=1.0, next_state=[ 0.03821017  0.00296286 -0.06813836 -0.01013241]\n",
      "[ Experience replay ] starts\n",
      "[ episode 431 ][ timestamp 9 ] state=[ 0.03821017  0.00296286 -0.06813836 -0.01013241], action=0, reward=1.0, next_state=[ 0.03826943 -0.19111909 -0.06834101  0.26029777]\n",
      "[ Experience replay ] starts\n",
      "[ episode 431 ][ timestamp 10 ] state=[ 0.03826943 -0.19111909 -0.06834101  0.26029777], action=0, reward=1.0, next_state=[ 0.03444705 -0.38520227 -0.06313506  0.53066634]\n",
      "[ Experience replay ] starts\n",
      "[ episode 431 ][ timestamp 11 ] state=[ 0.03444705 -0.38520227 -0.06313506  0.53066634], action=1, reward=1.0, next_state=[ 0.026743   -0.18925168 -0.05252173  0.21877668]\n",
      "[ Experience replay ] starts\n",
      "[ episode 431 ][ timestamp 12 ] state=[ 0.026743   -0.18925168 -0.05252173  0.21877668], action=0, reward=1.0, next_state=[ 0.02295797 -0.38358504 -0.0481462   0.49444047]\n",
      "[ Experience replay ] starts\n",
      "[ episode 431 ][ timestamp 13 ] state=[ 0.02295797 -0.38358504 -0.0481462   0.49444047], action=0, reward=1.0, next_state=[ 0.01528627 -0.5779961  -0.03825739  0.77156962]\n",
      "[ Experience replay ] starts\n",
      "[ episode 431 ][ timestamp 14 ] state=[ 0.01528627 -0.5779961  -0.03825739  0.77156962], action=0, reward=1.0, next_state=[ 0.00372635 -0.77257131 -0.022826    1.05197394]\n",
      "[ Experience replay ] starts\n",
      "[ episode 431 ][ timestamp 15 ] state=[ 0.00372635 -0.77257131 -0.022826    1.05197394], action=0, reward=1.0, next_state=[-0.01172508 -0.96738323 -0.00178652  1.33740544]\n",
      "[ Experience replay ] starts\n",
      "[ episode 431 ][ timestamp 16 ] state=[-0.01172508 -0.96738323 -0.00178652  1.33740544], action=0, reward=1.0, next_state=[-0.03107274 -1.16248263  0.02496159  1.62952884]\n",
      "[ Experience replay ] starts\n",
      "[ episode 431 ][ timestamp 17 ] state=[-0.03107274 -1.16248263  0.02496159  1.62952884], action=0, reward=1.0, next_state=[-0.0543224  -1.35788886  0.05755217  1.92988481]\n",
      "[ Experience replay ] starts\n",
      "[ episode 431 ][ timestamp 18 ] state=[-0.0543224  -1.35788886  0.05755217  1.92988481], action=0, reward=1.0, next_state=[-0.08148017 -1.55357799  0.09614986  2.23984351]\n",
      "[ Experience replay ] starts\n",
      "[ episode 431 ][ timestamp 19 ] state=[-0.08148017 -1.55357799  0.09614986  2.23984351], action=0, reward=1.0, next_state=[-0.11255173 -1.7494684   0.14094673  2.56054647]\n",
      "[ Experience replay ] starts\n",
      "[ episode 431 ][ timestamp 20 ] state=[-0.11255173 -1.7494684   0.14094673  2.56054647], action=0, reward=1.0, next_state=[-0.1475411  -1.94540355  0.19215766  2.89283597]\n",
      "[ Experience replay ] starts\n",
      "[ episode 431 ][ timestamp 21 ] state=[-0.1475411  -1.94540355  0.19215766  2.89283597], action=0, reward=-1.0, next_state=[-0.18644917 -2.14113243  0.25001438  3.23717287]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 431: Exploration_rate=0.01. Score=21.\n",
      "[ episode 432 ] state=[-0.02414151  0.02373822 -0.0178307   0.03606876]\n",
      "[ episode 432 ][ timestamp 1 ] state=[-0.02414151  0.02373822 -0.0178307   0.03606876], action=0, reward=1.0, next_state=[-0.02366675 -0.17112356 -0.01710932  0.32307302]\n",
      "[ Experience replay ] starts\n",
      "[ episode 432 ][ timestamp 2 ] state=[-0.02366675 -0.17112356 -0.01710932  0.32307302], action=0, reward=1.0, next_state=[-0.02708922 -0.36599775 -0.01064786  0.61031163]\n",
      "[ Experience replay ] starts\n",
      "[ episode 432 ][ timestamp 3 ] state=[-0.02708922 -0.36599775 -0.01064786  0.61031163], action=1, reward=1.0, next_state=[-0.03440917 -0.1707286   0.00155837  0.31429409]\n",
      "[ Experience replay ] starts\n",
      "[ episode 432 ][ timestamp 4 ] state=[-0.03440917 -0.1707286   0.00155837  0.31429409], action=1, reward=1.0, next_state=[-0.03782375  0.02437112  0.00784425  0.02210303]\n",
      "[ Experience replay ] starts\n",
      "[ episode 432 ][ timestamp 5 ] state=[-0.03782375  0.02437112  0.00784425  0.02210303], action=0, reward=1.0, next_state=[-0.03733632 -0.17086244  0.00828631  0.31725055]\n",
      "[ Experience replay ] starts\n",
      "[ episode 432 ][ timestamp 6 ] state=[-0.03733632 -0.17086244  0.00828631  0.31725055], action=1, reward=1.0, next_state=[-0.04075357  0.02414051  0.01463133  0.02719231]\n",
      "[ Experience replay ] starts\n",
      "[ episode 432 ][ timestamp 7 ] state=[-0.04075357  0.02414051  0.01463133  0.02719231], action=0, reward=1.0, next_state=[-0.04027076 -0.17118818  0.01517517  0.32445544]\n",
      "[ Experience replay ] starts\n",
      "[ episode 432 ][ timestamp 8 ] state=[-0.04027076 -0.17118818  0.01517517  0.32445544], action=1, reward=1.0, next_state=[-0.04369453  0.02371445  0.02166428  0.03659649]\n",
      "[ Experience replay ] starts\n",
      "[ episode 432 ][ timestamp 9 ] state=[-0.04369453  0.02371445  0.02166428  0.03659649], action=1, reward=1.0, next_state=[-0.04322024  0.21851914  0.02239621 -0.24917317]\n",
      "[ Experience replay ] starts\n",
      "[ episode 432 ][ timestamp 10 ] state=[-0.04322024  0.21851914  0.02239621 -0.24917317], action=1, reward=1.0, next_state=[-0.03884985  0.41331421  0.01741275 -0.53470857]\n",
      "[ Experience replay ] starts\n",
      "[ episode 432 ][ timestamp 11 ] state=[-0.03884985  0.41331421  0.01741275 -0.53470857], action=1, reward=1.0, next_state=[-0.03058357  0.60818702  0.00671858 -0.82185438]\n",
      "[ Experience replay ] starts\n",
      "[ episode 432 ][ timestamp 12 ] state=[-0.03058357  0.60818702  0.00671858 -0.82185438], action=0, reward=1.0, next_state=[-0.01841983  0.41297379 -0.00971851 -0.5270659 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 432 ][ timestamp 13 ] state=[-0.01841983  0.41297379 -0.00971851 -0.5270659 ], action=0, reward=1.0, next_state=[-0.01016035  0.21798992 -0.02025983 -0.23746111]\n",
      "[ Experience replay ] starts\n",
      "[ episode 432 ][ timestamp 14 ] state=[-0.01016035  0.21798992 -0.02025983 -0.23746111], action=0, reward=1.0, next_state=[-0.00580056  0.02316319 -0.02500905  0.04876304]\n",
      "[ Experience replay ] starts\n",
      "[ episode 432 ][ timestamp 15 ] state=[-0.00580056  0.02316319 -0.02500905  0.04876304], action=0, reward=1.0, next_state=[-0.00533729 -0.1715914  -0.02403379  0.33345167]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 432 ][ timestamp 16 ] state=[-0.00533729 -0.1715914  -0.02403379  0.33345167], action=0, reward=1.0, next_state=[-0.00876912 -0.36636318 -0.01736476  0.61845971]\n",
      "[ Experience replay ] starts\n",
      "[ episode 432 ][ timestamp 17 ] state=[-0.00876912 -0.36636318 -0.01736476  0.61845971], action=0, reward=1.0, next_state=[-0.01609638 -0.56123832 -0.00499556  0.90562338]\n",
      "[ Experience replay ] starts\n",
      "[ episode 432 ][ timestamp 18 ] state=[-0.01609638 -0.56123832 -0.00499556  0.90562338], action=0, reward=1.0, next_state=[-0.02732115 -0.75629227  0.0131169   1.19673196]\n",
      "[ Experience replay ] starts\n",
      "[ episode 432 ][ timestamp 19 ] state=[-0.02732115 -0.75629227  0.0131169   1.19673196], action=0, reward=1.0, next_state=[-0.04244699 -0.95158153  0.03705154  1.49349691]\n",
      "[ Experience replay ] starts\n",
      "[ episode 432 ][ timestamp 20 ] state=[-0.04244699 -0.95158153  0.03705154  1.49349691], action=0, reward=1.0, next_state=[-0.06147863 -1.14713412  0.06692148  1.79751514]\n",
      "[ Experience replay ] starts\n",
      "[ episode 432 ][ timestamp 21 ] state=[-0.06147863 -1.14713412  0.06692148  1.79751514], action=0, reward=1.0, next_state=[-0.08442131 -1.3429381   0.10287178  2.11022391]\n",
      "[ Experience replay ] starts\n",
      "[ episode 432 ][ timestamp 22 ] state=[-0.08442131 -1.3429381   0.10287178  2.11022391], action=0, reward=1.0, next_state=[-0.11128007 -1.53892764  0.14507626  2.43284501]\n",
      "[ Experience replay ] starts\n",
      "[ episode 432 ][ timestamp 23 ] state=[-0.11128007 -1.53892764  0.14507626  2.43284501], action=0, reward=1.0, next_state=[-0.14205862 -1.73496654  0.19373316  2.76631721]\n",
      "[ Experience replay ] starts\n",
      "[ episode 432 ][ timestamp 24 ] state=[-0.14205862 -1.73496654  0.19373316  2.76631721], action=0, reward=-1.0, next_state=[-0.17675795 -1.93082935  0.24905951  3.11121715]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 432: Exploration_rate=0.01. Score=24.\n",
      "[ episode 433 ] state=[ 0.00532407 -0.03626935 -0.0168856   0.01587504]\n",
      "[ episode 433 ][ timestamp 1 ] state=[ 0.00532407 -0.03626935 -0.0168856   0.01587504], action=0, reward=1.0, next_state=[ 0.00459868 -0.23114512 -0.0165681   0.3031829 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 433 ][ timestamp 2 ] state=[ 0.00459868 -0.23114512 -0.0165681   0.3031829 ], action=0, reward=1.0, next_state=[-2.42205841e-05 -4.26027073e-01 -1.05044449e-02  5.90594911e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 433 ][ timestamp 3 ] state=[-2.42205841e-05 -4.26027073e-01 -1.05044449e-02  5.90594911e-01], action=0, reward=1.0, next_state=[-0.00854476 -0.62100039  0.00130745  0.8799505 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 433 ][ timestamp 4 ] state=[-0.00854476 -0.62100039  0.00130745  0.8799505 ], action=0, reward=1.0, next_state=[-0.02096477 -0.81614008  0.01890646  1.17304417]\n",
      "[ Experience replay ] starts\n",
      "[ episode 433 ][ timestamp 5 ] state=[-0.02096477 -0.81614008  0.01890646  1.17304417], action=0, reward=1.0, next_state=[-0.03728757 -1.01150262  0.04236735  1.47159378]\n",
      "[ Experience replay ] starts\n",
      "[ episode 433 ][ timestamp 6 ] state=[-0.03728757 -1.01150262  0.04236735  1.47159378], action=0, reward=1.0, next_state=[-0.05751762 -1.20711629  0.07179922  1.77720326]\n",
      "[ Experience replay ] starts\n",
      "[ episode 433 ][ timestamp 7 ] state=[-0.05751762 -1.20711629  0.07179922  1.77720326], action=0, reward=1.0, next_state=[-0.08165995 -1.40296961  0.10734329  2.09131717]\n",
      "[ Experience replay ] starts\n",
      "[ episode 433 ][ timestamp 8 ] state=[-0.08165995 -1.40296961  0.10734329  2.09131717], action=0, reward=1.0, next_state=[-0.10971934 -1.59899744  0.14916963  2.41516482]\n",
      "[ Experience replay ] starts\n",
      "[ episode 433 ][ timestamp 9 ] state=[-0.10971934 -1.59899744  0.14916963  2.41516482], action=0, reward=1.0, next_state=[-0.14169929 -1.79506449  0.19747293  2.74969278]\n",
      "[ Experience replay ] starts\n",
      "[ episode 433 ][ timestamp 10 ] state=[-0.14169929 -1.79506449  0.19747293  2.74969278], action=0, reward=-1.0, next_state=[-0.17760058 -1.99094656  0.25246678  3.09548603]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 433: Exploration_rate=0.01. Score=10.\n",
      "[ episode 434 ] state=[-0.00600417 -0.00741824 -0.01354694 -0.00261444]\n",
      "[ episode 434 ][ timestamp 1 ] state=[-0.00600417 -0.00741824 -0.01354694 -0.00261444], action=0, reward=1.0, next_state=[-0.00615254 -0.20234332 -0.01359923  0.28576366]\n",
      "[ Experience replay ] starts\n",
      "[ episode 434 ][ timestamp 2 ] state=[-0.00615254 -0.20234332 -0.01359923  0.28576366], action=1, reward=1.0, next_state=[-0.0101994  -0.00703008 -0.00788395 -0.01117715]\n",
      "[ Experience replay ] starts\n",
      "[ episode 434 ][ timestamp 3 ] state=[-0.0101994  -0.00703008 -0.00788395 -0.01117715], action=0, reward=1.0, next_state=[-0.01034    -0.20203809 -0.0081075   0.27900791]\n",
      "[ Experience replay ] starts\n",
      "[ episode 434 ][ timestamp 4 ] state=[-0.01034    -0.20203809 -0.0081075   0.27900791], action=1, reward=1.0, next_state=[-0.01438077 -0.00680142 -0.00252734 -0.01622104]\n",
      "[ Experience replay ] starts\n",
      "[ episode 434 ][ timestamp 5 ] state=[-0.01438077 -0.00680142 -0.00252734 -0.01622104], action=0, reward=1.0, next_state=[-0.01451679 -0.20188704 -0.00285176  0.27566341]\n",
      "[ Experience replay ] starts\n",
      "[ episode 434 ][ timestamp 6 ] state=[-0.01451679 -0.20188704 -0.00285176  0.27566341], action=0, reward=1.0, next_state=[-0.01855453 -0.39696819  0.00266151  0.56744552]\n",
      "[ Experience replay ] starts\n",
      "[ episode 434 ][ timestamp 7 ] state=[-0.01855453 -0.39696819  0.00266151  0.56744552], action=0, reward=1.0, next_state=[-0.0264939  -0.59212737  0.01401042  0.86096575]\n",
      "[ Experience replay ] starts\n",
      "[ episode 434 ][ timestamp 8 ] state=[-0.0264939  -0.59212737  0.01401042  0.86096575], action=0, reward=1.0, next_state=[-0.03833645 -0.78743729  0.03122973  1.1580208 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 434 ][ timestamp 9 ] state=[-0.03833645 -0.78743729  0.03122973  1.1580208 ], action=0, reward=1.0, next_state=[-0.05408519 -0.98295203  0.05439015  1.46032996]\n",
      "[ Experience replay ] starts\n",
      "[ episode 434 ][ timestamp 10 ] state=[-0.05408519 -0.98295203  0.05439015  1.46032996], action=0, reward=1.0, next_state=[-0.07374423 -1.17869704  0.08359675  1.7694961 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 434 ][ timestamp 11 ] state=[-0.07374423 -1.17869704  0.08359675  1.7694961 ], action=0, reward=1.0, next_state=[-0.09731817 -1.37465727  0.11898667  2.08695879]\n",
      "[ Experience replay ] starts\n",
      "[ episode 434 ][ timestamp 12 ] state=[-0.09731817 -1.37465727  0.11898667  2.08695879], action=0, reward=1.0, next_state=[-0.12481132 -1.57076301  0.16072585  2.41393712]\n",
      "[ Experience replay ] starts\n",
      "[ episode 434 ][ timestamp 13 ] state=[-0.12481132 -1.57076301  0.16072585  2.41393712], action=0, reward=1.0, next_state=[-0.15622658 -1.76687321  0.20900459  2.75136125]\n",
      "[ Experience replay ] starts\n",
      "[ episode 434 ][ timestamp 14 ] state=[-0.15622658 -1.76687321  0.20900459  2.75136125], action=0, reward=-1.0, next_state=[-0.19156404 -1.96275666  0.26403181  3.09979314]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 434: Exploration_rate=0.01. Score=14.\n",
      "[ episode 435 ] state=[-0.00703852  0.04601186 -0.01895237  0.03344641]\n",
      "[ episode 435 ][ timestamp 1 ] state=[-0.00703852  0.04601186 -0.01895237  0.03344641], action=0, reward=1.0, next_state=[-0.00611829 -0.14883325 -0.01828344  0.32008992]\n",
      "[ Experience replay ] starts\n",
      "[ episode 435 ][ timestamp 2 ] state=[-0.00611829 -0.14883325 -0.01828344  0.32008992], action=0, reward=1.0, next_state=[-0.00909495 -0.34369011 -0.01188164  0.60695133]\n",
      "[ Experience replay ] starts\n",
      "[ episode 435 ][ timestamp 3 ] state=[-0.00909495 -0.34369011 -0.01188164  0.60695133], action=0, reward=1.0, next_state=[-1.59687523e-02 -5.38643932e-01  2.57384077e-04  8.95868302e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 435 ][ timestamp 4 ] state=[-1.59687523e-02 -5.38643932e-01  2.57384077e-04  8.95868302e-01], action=1, reward=1.0, next_state=[-0.02674163 -0.34352547  0.01817475  0.60326629]\n",
      "[ Experience replay ] starts\n",
      "[ episode 435 ][ timestamp 5 ] state=[-0.02674163 -0.34352547  0.01817475  0.60326629], action=1, reward=1.0, next_state=[-0.03361214 -0.14866237  0.03024008  0.316363  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 435 ][ timestamp 6 ] state=[-0.03361214 -0.14866237  0.03024008  0.316363  ], action=1, reward=1.0, next_state=[-0.03658539  0.04601609  0.03656734  0.03336805]\n",
      "[ Experience replay ] starts\n",
      "[ episode 435 ][ timestamp 7 ] state=[-0.03658539  0.04601609  0.03656734  0.03336805], action=1, reward=1.0, next_state=[-0.03566507  0.24059509  0.0372347  -0.24755693]\n",
      "[ Experience replay ] starts\n",
      "[ episode 435 ][ timestamp 8 ] state=[-0.03566507  0.24059509  0.0372347  -0.24755693], action=1, reward=1.0, next_state=[-0.03085316  0.43516603  0.03228356 -0.52826658]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 435 ][ timestamp 9 ] state=[-0.03085316  0.43516603  0.03228356 -0.52826658], action=1, reward=1.0, next_state=[-0.02214984  0.62981926  0.02171823 -0.81060456]\n",
      "[ Experience replay ] starts\n",
      "[ episode 435 ][ timestamp 10 ] state=[-0.02214984  0.62981926  0.02171823 -0.81060456], action=1, reward=1.0, next_state=[-0.00955346  0.82463703  0.00550614 -1.09637765]\n",
      "[ Experience replay ] starts\n",
      "[ episode 435 ][ timestamp 11 ] state=[-0.00955346  0.82463703  0.00550614 -1.09637765], action=1, reward=1.0, next_state=[ 0.00693928  1.01968604 -0.01642142 -1.38732794]\n",
      "[ Experience replay ] starts\n",
      "[ episode 435 ][ timestamp 12 ] state=[ 0.00693928  1.01968604 -0.01642142 -1.38732794], action=1, reward=1.0, next_state=[ 0.027333    1.21500877 -0.04416798 -1.68510021]\n",
      "[ Experience replay ] starts\n",
      "[ episode 435 ][ timestamp 13 ] state=[ 0.027333    1.21500877 -0.04416798 -1.68510021], action=1, reward=1.0, next_state=[ 0.05163318  1.41061311 -0.07786998 -1.99120174]\n",
      "[ Experience replay ] starts\n",
      "[ episode 435 ][ timestamp 14 ] state=[ 0.05163318  1.41061311 -0.07786998 -1.99120174], action=1, reward=1.0, next_state=[ 0.07984544  1.60645969 -0.11769402 -2.30695204]\n",
      "[ Experience replay ] starts\n",
      "[ episode 435 ][ timestamp 15 ] state=[ 0.07984544  1.60645969 -0.11769402 -2.30695204], action=1, reward=1.0, next_state=[ 0.11197463  1.80244659 -0.16383306 -2.63342086]\n",
      "[ Experience replay ] starts\n",
      "[ episode 435 ][ timestamp 16 ] state=[ 0.11197463  1.80244659 -0.16383306 -2.63342086], action=0, reward=-1.0, next_state=[ 0.14802357  1.60890565 -0.21650147 -2.39494863]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 435: Exploration_rate=0.01. Score=16.\n",
      "[ episode 436 ] state=[-0.03745194  0.00881022  0.02691618  0.0269662 ]\n",
      "[ episode 436 ][ timestamp 1 ] state=[-0.03745194  0.00881022  0.02691618  0.0269662 ], action=0, reward=1.0, next_state=[-0.03727574 -0.18668718  0.02745551  0.32801849]\n",
      "[ Experience replay ] starts\n",
      "[ episode 436 ][ timestamp 2 ] state=[-0.03727574 -0.18668718  0.02745551  0.32801849], action=0, reward=1.0, next_state=[-0.04100948 -0.38218902  0.03401587  0.62923164]\n",
      "[ Experience replay ] starts\n",
      "[ episode 436 ][ timestamp 3 ] state=[-0.04100948 -0.38218902  0.03401587  0.62923164], action=0, reward=1.0, next_state=[-0.04865326 -0.57776875  0.04660051  0.93243025]\n",
      "[ Experience replay ] starts\n",
      "[ episode 436 ][ timestamp 4 ] state=[-0.04865326 -0.57776875  0.04660051  0.93243025], action=0, reward=1.0, next_state=[-0.06020864 -0.77348746  0.06524911  1.2393852 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 436 ][ timestamp 5 ] state=[-0.06020864 -0.77348746  0.06524911  1.2393852 ], action=0, reward=1.0, next_state=[-0.07567839 -0.96938387  0.09003682  1.55177416]\n",
      "[ Experience replay ] starts\n",
      "[ episode 436 ][ timestamp 6 ] state=[-0.07567839 -0.96938387  0.09003682  1.55177416], action=0, reward=1.0, next_state=[-0.09506606 -1.16546289  0.1210723   1.8711364 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 436 ][ timestamp 7 ] state=[-0.09506606 -1.16546289  0.1210723   1.8711364 ], action=0, reward=1.0, next_state=[-0.11837532 -1.3616823   0.15849503  2.1988193 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 436 ][ timestamp 8 ] state=[-0.11837532 -1.3616823   0.15849503  2.1988193 ], action=0, reward=1.0, next_state=[-0.14560897 -1.55793722  0.20247141  2.53591456]\n",
      "[ Experience replay ] starts\n",
      "[ episode 436 ][ timestamp 9 ] state=[-0.14560897 -1.55793722  0.20247141  2.53591456], action=0, reward=-1.0, next_state=[-0.17676771 -1.75404231  0.25318971  2.88318403]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 436: Exploration_rate=0.01. Score=9.\n",
      "[ episode 437 ] state=[-0.01458795  0.01291906  0.02851962 -0.03061753]\n",
      "[ episode 437 ][ timestamp 1 ] state=[-0.01458795  0.01291906  0.02851962 -0.03061753], action=0, reward=1.0, next_state=[-0.01432957 -0.18260002  0.02790727  0.27092547]\n",
      "[ Experience replay ] starts\n",
      "[ episode 437 ][ timestamp 2 ] state=[-0.01432957 -0.18260002  0.02790727  0.27092547], action=0, reward=1.0, next_state=[-0.01798157 -0.37810886  0.03332578  0.5722782 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 437 ][ timestamp 3 ] state=[-0.01798157 -0.37810886  0.03332578  0.5722782 ], action=0, reward=1.0, next_state=[-0.02554374 -0.57368186  0.04477134  0.87527079]\n",
      "[ Experience replay ] starts\n",
      "[ episode 437 ][ timestamp 4 ] state=[-0.02554374 -0.57368186  0.04477134  0.87527079], action=0, reward=1.0, next_state=[-0.03701738 -0.76938291  0.06227676  1.18168657]\n",
      "[ Experience replay ] starts\n",
      "[ episode 437 ][ timestamp 5 ] state=[-0.03701738 -0.76938291  0.06227676  1.18168657], action=0, reward=1.0, next_state=[-0.05240504 -0.96525539  0.08591049  1.49322326]\n",
      "[ Experience replay ] starts\n",
      "[ episode 437 ][ timestamp 6 ] state=[-0.05240504 -0.96525539  0.08591049  1.49322326], action=0, reward=1.0, next_state=[-0.07171015 -1.1613111   0.11577496  1.81144886]\n",
      "[ Experience replay ] starts\n",
      "[ episode 437 ][ timestamp 7 ] state=[-0.07171015 -1.1613111   0.11577496  1.81144886], action=0, reward=1.0, next_state=[-0.09493637 -1.35751725  0.15200393  2.13774971]\n",
      "[ Experience replay ] starts\n",
      "[ episode 437 ][ timestamp 8 ] state=[-0.09493637 -1.35751725  0.15200393  2.13774971], action=0, reward=1.0, next_state=[-0.12208671 -1.55378137  0.19475893  2.47326866]\n",
      "[ Experience replay ] starts\n",
      "[ episode 437 ][ timestamp 9 ] state=[-0.12208671 -1.55378137  0.19475893  2.47326866], action=0, reward=-1.0, next_state=[-0.15316234 -1.74993383  0.2442243   2.81883259]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 437: Exploration_rate=0.01. Score=9.\n",
      "[ episode 438 ] state=[-0.04410069 -0.03306915 -0.00646341  0.01392381]\n",
      "[ episode 438 ][ timestamp 1 ] state=[-0.04410069 -0.03306915 -0.00646341  0.01392381], action=1, reward=1.0, next_state=[-0.04476207  0.16214489 -0.00618493 -0.28079137]\n",
      "[ Experience replay ] starts\n",
      "[ episode 438 ][ timestamp 2 ] state=[-0.04476207  0.16214489 -0.00618493 -0.28079137], action=0, reward=1.0, next_state=[-0.04151917 -0.03288829 -0.01180076  0.00993445]\n",
      "[ Experience replay ] starts\n",
      "[ episode 438 ][ timestamp 3 ] state=[-0.04151917 -0.03288829 -0.01180076  0.00993445], action=1, reward=1.0, next_state=[-0.04217694  0.16240089 -0.01160207 -0.28644828]\n",
      "[ Experience replay ] starts\n",
      "[ episode 438 ][ timestamp 4 ] state=[-0.04217694  0.16240089 -0.01160207 -0.28644828], action=0, reward=1.0, next_state=[-0.03892892 -0.03255369 -0.01733104  0.00255298]\n",
      "[ Experience replay ] starts\n",
      "[ episode 438 ][ timestamp 5 ] state=[-0.03892892 -0.03255369 -0.01733104  0.00255298], action=0, reward=1.0, next_state=[-0.03957999 -0.22742286 -0.01727998  0.28971777]\n",
      "[ Experience replay ] starts\n",
      "[ episode 438 ][ timestamp 6 ] state=[-0.03957999 -0.22742286 -0.01727998  0.28971777], action=0, reward=1.0, next_state=[-0.04412845 -0.42229419 -0.01148562  0.57690107]\n",
      "[ Experience replay ] starts\n",
      "[ episode 438 ][ timestamp 7 ] state=[-0.04412845 -0.42229419 -0.01148562  0.57690107], action=0, reward=1.0, next_state=[-5.25743333e-02 -6.17253286e-01  5.23986964e-05  8.65943723e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 438 ][ timestamp 8 ] state=[-5.25743333e-02 -6.17253286e-01  5.23986964e-05  8.65943723e-01], action=0, reward=1.0, next_state=[-0.0649194  -0.81237595  0.01737127  1.15864312]\n",
      "[ Experience replay ] starts\n",
      "[ episode 438 ][ timestamp 9 ] state=[-0.0649194  -0.81237595  0.01737127  1.15864312], action=0, reward=1.0, next_state=[-0.08116692 -1.00771992  0.04054414  1.45672176]\n",
      "[ Experience replay ] starts\n",
      "[ episode 438 ][ timestamp 10 ] state=[-0.08116692 -1.00771992  0.04054414  1.45672176], action=0, reward=1.0, next_state=[-0.10132132 -1.20331527  0.06967857  1.76179039]\n",
      "[ Experience replay ] starts\n",
      "[ episode 438 ][ timestamp 11 ] state=[-0.10132132 -1.20331527  0.06967857  1.76179039], action=0, reward=1.0, next_state=[-0.12538762 -1.39915299  0.10491438  2.07530308]\n",
      "[ Experience replay ] starts\n",
      "[ episode 438 ][ timestamp 12 ] state=[-0.12538762 -1.39915299  0.10491438  2.07530308], action=0, reward=1.0, next_state=[-0.15337068 -1.59517123  0.14642044  2.39850202]\n",
      "[ Experience replay ] starts\n",
      "[ episode 438 ][ timestamp 13 ] state=[-0.15337068 -1.59517123  0.14642044  2.39850202], action=0, reward=1.0, next_state=[-0.18527411 -1.79123895  0.19439048  2.73235057]\n",
      "[ Experience replay ] starts\n",
      "[ episode 438 ][ timestamp 14 ] state=[-0.18527411 -1.79123895  0.19439048  2.73235057], action=0, reward=-1.0, next_state=[-0.22109889 -1.98713721  0.24903749  3.07745506]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 438: Exploration_rate=0.01. Score=14.\n",
      "[ episode 439 ] state=[-0.01010819 -0.0048242   0.03368278  0.04014778]\n",
      "[ episode 439 ][ timestamp 1 ] state=[-0.01010819 -0.0048242   0.03368278  0.04014778], action=1, reward=1.0, next_state=[-0.01020468  0.18979896  0.03448573 -0.2417205 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 439 ][ timestamp 2 ] state=[-0.01020468  0.18979896  0.03448573 -0.2417205 ], action=1, reward=1.0, next_state=[-0.0064087   0.38441176  0.02965132 -0.52332934]\n",
      "[ Experience replay ] starts\n",
      "[ episode 439 ][ timestamp 3 ] state=[-0.0064087   0.38441176  0.02965132 -0.52332934], action=1, reward=1.0, next_state=[ 0.00127954  0.57910412  0.01918474 -0.8065233 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 439 ][ timestamp 4 ] state=[ 0.00127954  0.57910412  0.01918474 -0.8065233 ], action=1, reward=1.0, next_state=[ 0.01286162  0.77395793  0.00305427 -1.09311026]\n",
      "[ Experience replay ] starts\n",
      "[ episode 439 ][ timestamp 5 ] state=[ 0.01286162  0.77395793  0.00305427 -1.09311026], action=1, reward=1.0, next_state=[ 0.02834078  0.9690395  -0.01880793 -1.3848333 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 439 ][ timestamp 6 ] state=[ 0.02834078  0.9690395  -0.01880793 -1.3848333 ], action=0, reward=1.0, next_state=[ 0.04772157  0.77415708 -0.0465046  -1.09809057]\n",
      "[ Experience replay ] starts\n",
      "[ episode 439 ][ timestamp 7 ] state=[ 0.04772157  0.77415708 -0.0465046  -1.09809057], action=0, reward=1.0, next_state=[ 0.06320471  0.57967718 -0.06846641 -0.82035353]\n",
      "[ Experience replay ] starts\n",
      "[ episode 439 ][ timestamp 8 ] state=[ 0.06320471  0.57967718 -0.06846641 -0.82035353], action=0, reward=1.0, next_state=[ 0.07479825  0.38555563 -0.08487348 -0.54996683]\n",
      "[ Experience replay ] starts\n",
      "[ episode 439 ][ timestamp 9 ] state=[ 0.07479825  0.38555563 -0.08487348 -0.54996683], action=1, reward=1.0, next_state=[ 0.08250937  0.58176077 -0.09587282 -0.86813801]\n",
      "[ Experience replay ] starts\n",
      "[ episode 439 ][ timestamp 10 ] state=[ 0.08250937  0.58176077 -0.09587282 -0.86813801], action=1, reward=1.0, next_state=[ 0.09414458  0.77804727 -0.11323558 -1.18935911]\n",
      "[ Experience replay ] starts\n",
      "[ episode 439 ][ timestamp 11 ] state=[ 0.09414458  0.77804727 -0.11323558 -1.18935911], action=1, reward=1.0, next_state=[ 0.10970553  0.97443993 -0.13702276 -1.51528161]\n",
      "[ Experience replay ] starts\n",
      "[ episode 439 ][ timestamp 12 ] state=[ 0.10970553  0.97443993 -0.13702276 -1.51528161], action=1, reward=1.0, next_state=[ 0.12919433  1.17092825 -0.16732839 -1.84741034]\n",
      "[ Experience replay ] starts\n",
      "[ episode 439 ][ timestamp 13 ] state=[ 0.12919433  1.17092825 -0.16732839 -1.84741034], action=1, reward=1.0, next_state=[ 0.15261289  1.36745196 -0.2042766  -2.18704401]\n",
      "[ Experience replay ] starts\n",
      "[ episode 439 ][ timestamp 14 ] state=[ 0.15261289  1.36745196 -0.2042766  -2.18704401], action=0, reward=-1.0, next_state=[ 0.17996193  1.17481223 -0.24801748 -1.96373297]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 439: Exploration_rate=0.01. Score=14.\n",
      "[ episode 440 ] state=[-0.02200286 -0.02958999 -0.00094328 -0.00311095]\n",
      "[ episode 440 ][ timestamp 1 ] state=[-0.02200286 -0.02958999 -0.00094328 -0.00311095], action=1, reward=1.0, next_state=[-0.02259466  0.16554548 -0.00100549 -0.29609135]\n",
      "[ Experience replay ] starts\n",
      "[ episode 440 ][ timestamp 2 ] state=[-0.02259466  0.16554548 -0.00100549 -0.29609135], action=1, reward=1.0, next_state=[-0.01928375  0.36068175 -0.00692732 -0.58909122]\n",
      "[ Experience replay ] starts\n",
      "[ episode 440 ][ timestamp 3 ] state=[-0.01928375  0.36068175 -0.00692732 -0.58909122], action=1, reward=1.0, next_state=[-0.01207012  0.55590002 -0.01870915 -0.88394821]\n",
      "[ Experience replay ] starts\n",
      "[ episode 440 ][ timestamp 4 ] state=[-0.01207012  0.55590002 -0.01870915 -0.88394821], action=1, reward=1.0, next_state=[-9.52117320e-04  7.51270957e-01 -3.63881104e-02 -1.18245350e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 440 ][ timestamp 5 ] state=[-9.52117320e-04  7.51270957e-01 -3.63881104e-02 -1.18245350e+00], action=0, reward=1.0, next_state=[ 0.0140733   0.55663963 -0.06003718 -0.90139551]\n",
      "[ Experience replay ] starts\n",
      "[ episode 440 ][ timestamp 6 ] state=[ 0.0140733   0.55663963 -0.06003718 -0.90139551], action=0, reward=1.0, next_state=[ 0.02520609  0.36238024 -0.07806509 -0.62817175]\n",
      "[ Experience replay ] starts\n",
      "[ episode 440 ][ timestamp 7 ] state=[ 0.02520609  0.36238024 -0.07806509 -0.62817175], action=1, reward=1.0, next_state=[ 0.0324537   0.55849992 -0.09062853 -0.94438317]\n",
      "[ Experience replay ] starts\n",
      "[ episode 440 ][ timestamp 8 ] state=[ 0.0324537   0.55849992 -0.09062853 -0.94438317], action=1, reward=1.0, next_state=[ 0.0436237   0.75471816 -0.10951619 -1.26411095]\n",
      "[ Experience replay ] starts\n",
      "[ episode 440 ][ timestamp 9 ] state=[ 0.0436237   0.75471816 -0.10951619 -1.26411095], action=1, reward=1.0, next_state=[ 0.05871806  0.95105619 -0.13479841 -1.58898707]\n",
      "[ Experience replay ] starts\n",
      "[ episode 440 ][ timestamp 10 ] state=[ 0.05871806  0.95105619 -0.13479841 -1.58898707], action=1, reward=1.0, next_state=[ 0.07773918  1.14749737 -0.16657815 -1.92048662]\n",
      "[ Experience replay ] starts\n",
      "[ episode 440 ][ timestamp 11 ] state=[ 0.07773918  1.14749737 -0.16657815 -1.92048662], action=1, reward=1.0, next_state=[ 0.10068913  1.34397248 -0.20498788 -2.25986764]\n",
      "[ Experience replay ] starts\n",
      "[ episode 440 ][ timestamp 12 ] state=[ 0.10068913  1.34397248 -0.20498788 -2.25986764], action=1, reward=-1.0, next_state=[ 0.12756858  1.54034303 -0.25018523 -2.60810174]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 440: Exploration_rate=0.01. Score=12.\n",
      "[ episode 441 ] state=[-0.02857598 -0.01149701 -0.04402029  0.01849966]\n",
      "[ episode 441 ][ timestamp 1 ] state=[-0.02857598 -0.01149701 -0.04402029  0.01849966], action=1, reward=1.0, next_state=[-0.02880592  0.1842277  -0.0436503  -0.28774077]\n",
      "[ Experience replay ] starts\n",
      "[ episode 441 ][ timestamp 2 ] state=[-0.02880592  0.1842277  -0.0436503  -0.28774077], action=1, reward=1.0, next_state=[-0.02512137  0.37994407 -0.04940511 -0.5938648 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 441 ][ timestamp 3 ] state=[-0.02512137  0.37994407 -0.04940511 -0.5938648 ], action=1, reward=1.0, next_state=[-0.01752249  0.57572148 -0.06128241 -0.90169179]\n",
      "[ Experience replay ] starts\n",
      "[ episode 441 ][ timestamp 4 ] state=[-0.01752249  0.57572148 -0.06128241 -0.90169179], action=1, reward=1.0, next_state=[-0.00600806  0.77161777 -0.07931624 -1.21299037]\n",
      "[ Experience replay ] starts\n",
      "[ episode 441 ][ timestamp 5 ] state=[-0.00600806  0.77161777 -0.07931624 -1.21299037], action=1, reward=1.0, next_state=[ 0.0094243   0.96766867 -0.10357605 -1.52943672]\n",
      "[ Experience replay ] starts\n",
      "[ episode 441 ][ timestamp 6 ] state=[ 0.0094243   0.96766867 -0.10357605 -1.52943672], action=1, reward=1.0, next_state=[ 0.02877767  1.16387604 -0.13416479 -1.85256743]\n",
      "[ Experience replay ] starts\n",
      "[ episode 441 ][ timestamp 7 ] state=[ 0.02877767  1.16387604 -0.13416479 -1.85256743], action=0, reward=1.0, next_state=[ 0.05205519  0.97046054 -0.17121613 -1.60437763]\n",
      "[ Experience replay ] starts\n",
      "[ episode 441 ][ timestamp 8 ] state=[ 0.05205519  0.97046054 -0.17121613 -1.60437763], action=0, reward=1.0, next_state=[ 0.0714644   0.77772789 -0.20330369 -1.36959774]\n",
      "[ Experience replay ] starts\n",
      "[ episode 441 ][ timestamp 9 ] state=[ 0.0714644   0.77772789 -0.20330369 -1.36959774], action=0, reward=-1.0, next_state=[ 0.08701896  0.58564539 -0.23069564 -1.14676832]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 441: Exploration_rate=0.01. Score=9.\n",
      "[ episode 442 ] state=[ 0.02861171 -0.01263247 -0.03777074 -0.02682661]\n",
      "[ episode 442 ][ timestamp 1 ] state=[ 0.02861171 -0.01263247 -0.03777074 -0.02682661], action=0, reward=1.0, next_state=[ 0.02835906 -0.20719297 -0.03830727  0.25370404]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 2 ] state=[ 0.02835906 -0.20719297 -0.03830727  0.25370404], action=0, reward=1.0, next_state=[ 0.0242152  -0.4017476  -0.03323319  0.5340623 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 3 ] state=[ 0.0242152  -0.4017476  -0.03323319  0.5340623 ], action=1, reward=1.0, next_state=[ 0.01618025 -0.20617443 -0.02255195  0.23109577]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 4 ] state=[ 0.01618025 -0.20617443 -0.02255195  0.23109577], action=0, reward=1.0, next_state=[ 0.01205676 -0.40096699 -0.01793003  0.5165806 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 5 ] state=[ 0.01205676 -0.40096699 -0.01793003  0.5165806 ], action=1, reward=1.0, next_state=[ 0.00403742 -0.20559722 -0.00759842  0.2183019 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 6 ] state=[ 0.00403742 -0.20559722 -0.00759842  0.2183019 ], action=0, reward=1.0, next_state=[-7.45246045e-05 -4.00609730e-01 -3.23238187e-03  5.08578308e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 7 ] state=[-7.45246045e-05 -4.00609730e-01 -3.23238187e-03  5.08578308e-01], action=1, reward=1.0, next_state=[-0.00808672 -0.20544239  0.00693918  0.2148785 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 8 ] state=[-0.00808672 -0.20544239  0.00693918  0.2148785 ], action=0, reward=1.0, next_state=[-0.01219557 -0.40066285  0.01123675  0.50974226]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 9 ] state=[-0.01219557 -0.40066285  0.01123675  0.50974226], action=1, reward=1.0, next_state=[-0.02020882 -0.20570099  0.0214316   0.22062146]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 10 ] state=[-0.02020882 -0.20570099  0.0214316   0.22062146], action=1, reward=1.0, next_state=[-0.02432284 -0.01089184  0.02584403 -0.06522476]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 11 ] state=[-0.02432284 -0.01089184  0.02584403 -0.06522476], action=0, reward=1.0, next_state=[-0.02454068 -0.2063746   0.02453953  0.23549878]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 12 ] state=[-0.02454068 -0.2063746   0.02453953  0.23549878], action=1, reward=1.0, next_state=[-0.02866817 -0.0116117   0.02924951 -0.04934373]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 13 ] state=[-0.02866817 -0.0116117   0.02924951 -0.04934373], action=1, reward=1.0, next_state=[-0.02890041  0.1830789   0.02826263 -0.33265658]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 442 ][ timestamp 14 ] state=[-0.02890041  0.1830789   0.02826263 -0.33265658], action=1, reward=1.0, next_state=[-0.02523883  0.37778741  0.0216095  -0.6162946 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 15 ] state=[-0.02523883  0.37778741  0.0216095  -0.6162946 ], action=1, reward=1.0, next_state=[-0.01768308  0.5726009   0.00928361 -0.9020939 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 16 ] state=[-0.01768308  0.5726009   0.00928361 -0.9020939 ], action=0, reward=1.0, next_state=[-0.00623106  0.37735442 -0.00875827 -0.60650746]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 17 ] state=[-0.00623106  0.37735442 -0.00875827 -0.60650746], action=0, reward=1.0, next_state=[ 0.00131603  0.18235602 -0.02088842 -0.31659597]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 18 ] state=[ 0.00131603  0.18235602 -0.02088842 -0.31659597], action=0, reward=1.0, next_state=[ 0.00496315 -0.01246228 -0.02722034 -0.03057303]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 19 ] state=[ 0.00496315 -0.01246228 -0.02722034 -0.03057303], action=1, reward=1.0, next_state=[ 0.0047139   0.18303924 -0.0278318  -0.33171846]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 20 ] state=[ 0.0047139   0.18303924 -0.0278318  -0.33171846], action=1, reward=1.0, next_state=[ 0.00837469  0.37854607 -0.03446617 -0.63304662]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 21 ] state=[ 0.00837469  0.37854607 -0.03446617 -0.63304662], action=1, reward=1.0, next_state=[ 0.01594561  0.57413146 -0.0471271  -0.93638152]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 22 ] state=[ 0.01594561  0.57413146 -0.0471271  -0.93638152], action=1, reward=1.0, next_state=[ 0.02742824  0.7698562  -0.06585473 -1.2434929 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 23 ] state=[ 0.02742824  0.7698562  -0.06585473 -1.2434929 ], action=1, reward=1.0, next_state=[ 0.04282536  0.96575852 -0.09072459 -1.55605672]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 24 ] state=[ 0.04282536  0.96575852 -0.09072459 -1.55605672], action=1, reward=1.0, next_state=[ 0.06214053  1.16184268 -0.12184572 -1.87560977]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 25 ] state=[ 0.06214053  1.16184268 -0.12184572 -1.87560977], action=1, reward=1.0, next_state=[ 0.08537738  1.35806557 -0.15935792 -2.20349598]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 26 ] state=[ 0.08537738  1.35806557 -0.15935792 -2.20349598], action=1, reward=1.0, next_state=[ 0.1125387   1.55432117 -0.20342784 -2.54080253]\n",
      "[ Experience replay ] starts\n",
      "[ episode 442 ][ timestamp 27 ] state=[ 0.1125387   1.55432117 -0.20342784 -2.54080253], action=1, reward=-1.0, next_state=[ 0.14362512  1.75042268 -0.25424389 -2.88828546]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 442: Exploration_rate=0.01. Score=27.\n",
      "[ episode 443 ] state=[ 0.00260746 -0.02711163  0.04004485  0.03542954]\n",
      "[ episode 443 ][ timestamp 1 ] state=[ 0.00260746 -0.02711163  0.04004485  0.03542954], action=1, reward=1.0, next_state=[ 0.00206523  0.16741386  0.04075344 -0.24435474]\n",
      "[ Experience replay ] starts\n",
      "[ episode 443 ][ timestamp 2 ] state=[ 0.00206523  0.16741386  0.04075344 -0.24435474], action=1, reward=1.0, next_state=[ 0.00541351  0.36193075  0.03586635 -0.52390961]\n",
      "[ Experience replay ] starts\n",
      "[ episode 443 ][ timestamp 3 ] state=[ 0.00541351  0.36193075  0.03586635 -0.52390961], action=1, reward=1.0, next_state=[ 0.01265212  0.55653006  0.02538815 -0.8050784 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 443 ][ timestamp 4 ] state=[ 0.01265212  0.55653006  0.02538815 -0.8050784 ], action=1, reward=1.0, next_state=[ 0.02378272  0.75129493  0.00928659 -1.08966824]\n",
      "[ Experience replay ] starts\n",
      "[ episode 443 ][ timestamp 5 ] state=[ 0.02378272  0.75129493  0.00928659 -1.08966824], action=1, reward=1.0, next_state=[ 0.03880862  0.94629323 -0.01250678 -1.37942287]\n",
      "[ Experience replay ] starts\n",
      "[ episode 443 ][ timestamp 6 ] state=[ 0.03880862  0.94629323 -0.01250678 -1.37942287], action=1, reward=1.0, next_state=[ 0.05773448  1.14156908 -0.04009524 -1.67599063]\n",
      "[ Experience replay ] starts\n",
      "[ episode 443 ][ timestamp 7 ] state=[ 0.05773448  1.14156908 -0.04009524 -1.67599063], action=1, reward=1.0, next_state=[ 0.08056587  1.3371326  -0.07361505 -1.98088498]\n",
      "[ Experience replay ] starts\n",
      "[ episode 443 ][ timestamp 8 ] state=[ 0.08056587  1.3371326  -0.07361505 -1.98088498], action=1, reward=1.0, next_state=[ 0.10730852  1.53294742 -0.11323275 -2.29543499]\n",
      "[ Experience replay ] starts\n",
      "[ episode 443 ][ timestamp 9 ] state=[ 0.10730852  1.53294742 -0.11323275 -2.29543499], action=0, reward=1.0, next_state=[ 0.13796747  1.33903586 -0.15914145 -2.0396497 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 443 ][ timestamp 10 ] state=[ 0.13796747  1.33903586 -0.15914145 -2.0396497 ], action=1, reward=1.0, next_state=[ 0.16474818  1.53539779 -0.19993444 -2.377061  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 443 ][ timestamp 11 ] state=[ 0.16474818  1.53539779 -0.19993444 -2.377061  ], action=1, reward=-1.0, next_state=[ 0.19545614  1.73165005 -0.24747566 -2.72396515]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 443: Exploration_rate=0.01. Score=11.\n",
      "[ episode 444 ] state=[-0.04957972  0.04183148  0.01060152  0.0030338 ]\n",
      "[ episode 444 ][ timestamp 1 ] state=[-0.04957972  0.04183148  0.01060152  0.0030338 ], action=1, reward=1.0, next_state=[-0.04874309  0.2367998   0.0106622  -0.28628545]\n",
      "[ Experience replay ] starts\n",
      "[ episode 444 ][ timestamp 2 ] state=[-0.04874309  0.2367998   0.0106622  -0.28628545], action=1, reward=1.0, next_state=[-0.0440071   0.43176808  0.00493649 -0.57558662]\n",
      "[ Experience replay ] starts\n",
      "[ episode 444 ][ timestamp 3 ] state=[-0.0440071   0.43176808  0.00493649 -0.57558662], action=1, reward=1.0, next_state=[-0.03537174  0.62682048 -0.00657524 -0.86671034]\n",
      "[ Experience replay ] starts\n",
      "[ episode 444 ][ timestamp 4 ] state=[-0.03537174  0.62682048 -0.00657524 -0.86671034], action=1, reward=1.0, next_state=[-0.02283533  0.82203129 -0.02390945 -1.16145333]\n",
      "[ Experience replay ] starts\n",
      "[ episode 444 ][ timestamp 5 ] state=[-0.02283533  0.82203129 -0.02390945 -1.16145333], action=1, reward=1.0, next_state=[-0.0063947   1.01745637 -0.04713851 -1.46153588]\n",
      "[ Experience replay ] starts\n",
      "[ episode 444 ][ timestamp 6 ] state=[-0.0063947   1.01745637 -0.04713851 -1.46153588], action=1, reward=1.0, next_state=[ 0.01395443  1.21312337 -0.07636923 -1.76856394]\n",
      "[ Experience replay ] starts\n",
      "[ episode 444 ][ timestamp 7 ] state=[ 0.01395443  1.21312337 -0.07636923 -1.76856394], action=1, reward=1.0, next_state=[ 0.03821689  1.40902006 -0.11174051 -2.08398324]\n",
      "[ Experience replay ] starts\n",
      "[ episode 444 ][ timestamp 8 ] state=[ 0.03821689  1.40902006 -0.11174051 -2.08398324], action=1, reward=1.0, next_state=[ 0.06639729  1.60508039 -0.15342018 -2.40902302]\n",
      "[ Experience replay ] starts\n",
      "[ episode 444 ][ timestamp 9 ] state=[ 0.06639729  1.60508039 -0.15342018 -2.40902302], action=1, reward=1.0, next_state=[ 0.0984989   1.80116793 -0.20160064 -2.74462832]\n",
      "[ Experience replay ] starts\n",
      "[ episode 444 ][ timestamp 10 ] state=[ 0.0984989   1.80116793 -0.20160064 -2.74462832], action=1, reward=-1.0, next_state=[ 0.13452226  1.99705713 -0.2564932  -3.09138111]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 444: Exploration_rate=0.01. Score=10.\n",
      "[ episode 445 ] state=[-0.03788301  0.00439245  0.04069883  0.04488159]\n",
      "[ episode 445 ][ timestamp 1 ] state=[-0.03788301  0.00439245  0.04069883  0.04488159], action=1, reward=1.0, next_state=[-0.03779516  0.19890788  0.04159646 -0.23468779]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 2 ] state=[-0.03779516  0.19890788  0.04159646 -0.23468779], action=1, reward=1.0, next_state=[-0.03381701  0.39341159  0.0369027  -0.51396515]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 3 ] state=[-0.03381701  0.39341159  0.0369027  -0.51396515], action=1, reward=1.0, next_state=[-0.02594877  0.58799491  0.0266234  -0.79479449]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 4 ] state=[-0.02594877  0.58799491  0.0266234  -0.79479449], action=1, reward=1.0, next_state=[-0.01418888  0.78274153  0.01072751 -1.07898454]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 5 ] state=[-0.01418888  0.78274153  0.01072751 -1.07898454], action=0, reward=1.0, next_state=[ 0.00146595  0.58747957 -0.01085218 -0.78295463]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 6 ] state=[ 0.00146595  0.58747957 -0.01085218 -0.78295463], action=1, reward=1.0, next_state=[ 0.01321555  0.78274897 -0.02651127 -1.07903196]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 7 ] state=[ 0.01321555  0.78274897 -0.02651127 -1.07903196], action=1, reward=1.0, next_state=[ 0.02887053  0.9782108  -0.04809191 -1.37991507]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 445 ][ timestamp 8 ] state=[ 0.02887053  0.9782108  -0.04809191 -1.37991507], action=0, reward=1.0, next_state=[ 0.04843474  0.78372107 -0.07569021 -1.10265136]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 9 ] state=[ 0.04843474  0.78372107 -0.07569021 -1.10265136], action=1, reward=1.0, next_state=[ 0.06410916  0.97975268 -0.09774324 -1.41808854]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 10 ] state=[ 0.06410916  0.97975268 -0.09774324 -1.41808854], action=0, reward=1.0, next_state=[ 0.08370422  0.78596717 -0.12610501 -1.15748849]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 11 ] state=[ 0.08370422  0.78596717 -0.12610501 -1.15748849], action=0, reward=1.0, next_state=[ 0.09942356  0.59269394 -0.14925478 -0.90685743]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 12 ] state=[ 0.09942356  0.59269394 -0.14925478 -0.90685743], action=0, reward=1.0, next_state=[ 0.11127744  0.39987354 -0.16739193 -0.66456061]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 13 ] state=[ 0.11127744  0.39987354 -0.16739193 -0.66456061], action=0, reward=1.0, next_state=[ 0.11927491  0.20742675 -0.18068314 -0.42890898]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 14 ] state=[ 0.11927491  0.20742675 -0.18068314 -0.42890898], action=0, reward=1.0, next_state=[ 0.12342344  0.01526184 -0.18926132 -0.19818623]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 15 ] state=[ 0.12342344  0.01526184 -0.18926132 -0.19818623], action=0, reward=1.0, next_state=[ 0.12372868 -0.17672019 -0.19322505  0.02933338]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 16 ] state=[ 0.12372868 -0.17672019 -0.19322505  0.02933338], action=0, reward=1.0, next_state=[ 0.12019428 -0.36862187 -0.19263838  0.25537366]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 17 ] state=[ 0.12019428 -0.36862187 -0.19263838  0.25537366], action=0, reward=1.0, next_state=[ 0.11282184 -0.56054641 -0.18753091  0.48164924]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 18 ] state=[ 0.11282184 -0.56054641 -0.18753091  0.48164924], action=0, reward=1.0, next_state=[ 0.10161091 -0.75259528 -0.17789792  0.70986041]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 19 ] state=[ 0.10161091 -0.75259528 -0.17789792  0.70986041], action=0, reward=1.0, next_state=[ 0.08655901 -0.94486584 -0.16370071  0.94168803]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 20 ] state=[ 0.08655901 -0.94486584 -0.16370071  0.94168803], action=1, reward=1.0, next_state=[ 0.06766169 -0.74796155 -0.14486695  0.6023669 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 21 ] state=[ 0.06766169 -0.74796155 -0.14486695  0.6023669 ], action=0, reward=1.0, next_state=[ 0.05270246 -0.9407919  -0.13281961  0.84614055]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 22 ] state=[ 0.05270246 -0.9407919  -0.13281961  0.84614055], action=1, reward=1.0, next_state=[ 0.03388662 -0.7441323  -0.1158968   0.51481504]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 23 ] state=[ 0.03388662 -0.7441323  -0.1158968   0.51481504], action=0, reward=1.0, next_state=[ 0.01900397 -0.93744772 -0.1056005   0.76884544]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 24 ] state=[ 0.01900397 -0.93744772 -0.1056005   0.76884544], action=1, reward=1.0, next_state=[ 2.55020122e-04 -7.41042991e-01 -9.02235942e-02  4.44890590e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 25 ] state=[ 2.55020122e-04 -7.41042991e-01 -9.02235942e-02  4.44890590e-01], action=0, reward=1.0, next_state=[-0.01456584 -0.93478033 -0.08132578  0.70782483]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 26 ] state=[-0.01456584 -0.93478033 -0.08132578  0.70782483], action=1, reward=1.0, next_state=[-0.03326145 -0.73863161 -0.06716929  0.39069075]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 27 ] state=[-0.03326145 -0.73863161 -0.06716929  0.39069075], action=0, reward=1.0, next_state=[-0.04803408 -0.93273916 -0.05935547  0.66146259]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 28 ] state=[-0.04803408 -0.93273916 -0.05935547  0.66146259], action=1, reward=1.0, next_state=[-0.06668886 -0.73684372 -0.04612622  0.35069663]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 29 ] state=[-0.06668886 -0.73684372 -0.04612622  0.35069663], action=0, reward=1.0, next_state=[-0.08142574 -0.93128038 -0.03911229  0.6284851 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 30 ] state=[-0.08142574 -0.93128038 -0.03911229  0.6284851 ], action=1, reward=1.0, next_state=[-0.10005134 -0.73563503 -0.02654258  0.32374544]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 31 ] state=[-0.10005134 -0.73563503 -0.02654258  0.32374544], action=0, reward=1.0, next_state=[-0.11476404 -0.93036918 -0.02006768  0.60794117]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 32 ] state=[-0.11476404 -0.93036918 -0.02006768  0.60794117], action=1, reward=1.0, next_state=[-0.13337143 -0.7349725  -0.00790885  0.30900566]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 33 ] state=[-0.13337143 -0.7349725  -0.00790885  0.30900566], action=1, reward=1.0, next_state=[-0.14807088 -0.53973875 -0.00172874  0.01383903]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 34 ] state=[-0.14807088 -0.53973875 -0.00172874  0.01383903], action=0, reward=1.0, next_state=[-0.15886565 -0.73483587 -0.00145196  0.30597602]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 35 ] state=[-0.15886565 -0.73483587 -0.00145196  0.30597602], action=1, reward=1.0, next_state=[-0.17356237 -0.53969326  0.00466756  0.01283553]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 36 ] state=[-0.17356237 -0.53969326  0.00466756  0.01283553], action=0, reward=1.0, next_state=[-0.18435624 -0.73488184  0.00492427  0.30698747]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 37 ] state=[-0.18435624 -0.73488184  0.00492427  0.30698747], action=1, reward=1.0, next_state=[-0.19905387 -0.5398304   0.01106402  0.01586159]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 38 ] state=[-0.19905387 -0.5398304   0.01106402  0.01586159], action=0, reward=1.0, next_state=[-0.20985048 -0.73510926  0.01138125  0.31201471]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 39 ] state=[-0.20985048 -0.73510926  0.01138125  0.31201471], action=1, reward=1.0, next_state=[-0.22455267 -0.54015129  0.01762155  0.0229427 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 40 ] state=[-0.22455267 -0.54015129  0.01762155  0.0229427 ], action=1, reward=1.0, next_state=[-0.23535569 -0.34528642  0.0180804  -0.26412875]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 41 ] state=[-0.23535569 -0.34528642  0.0180804  -0.26412875], action=1, reward=1.0, next_state=[-0.24226142 -0.15042714  0.01279783 -0.55105455]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 42 ] state=[-0.24226142 -0.15042714  0.01279783 -0.55105455], action=1, reward=1.0, next_state=[-0.24526996  0.04451275  0.00177674 -0.83967797]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 43 ] state=[-0.24526996  0.04451275  0.00177674 -0.83967797], action=1, reward=1.0, next_state=[-0.24437971  0.23961039 -0.01501682 -1.13180162]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 44 ] state=[-0.24437971  0.23961039 -0.01501682 -1.13180162], action=1, reward=1.0, next_state=[-0.2395875   0.43492569 -0.03765286 -1.42915631]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 45 ] state=[-0.2395875   0.43492569 -0.03765286 -1.42915631], action=1, reward=1.0, next_state=[-0.23088899  0.63049184 -0.06623598 -1.73336493]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 46 ] state=[-0.23088899  0.63049184 -0.06623598 -1.73336493], action=1, reward=1.0, next_state=[-0.21827915  0.82630415 -0.10090328 -2.04589848]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 47 ] state=[-0.21827915  0.82630415 -0.10090328 -2.04589848], action=0, reward=1.0, next_state=[-0.20175307  0.63235215 -0.14182125 -1.78606551]\n",
      "[ Experience replay ] starts\n",
      "[ episode 445 ][ timestamp 48 ] state=[-0.20175307  0.63235215 -0.14182125 -1.78606551], action=1, reward=1.0, next_state=[-0.18910602  0.82875385 -0.17754256 -2.11926613]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 445 ][ timestamp 49 ] state=[-0.18910602  0.82875385 -0.17754256 -2.11926613], action=1, reward=-1.0, next_state=[-0.17253095  1.02514675 -0.21992788 -2.46114846]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 445: Exploration_rate=0.01. Score=49.\n",
      "[ episode 446 ] state=[-0.04706975  0.02789885 -0.00396621 -0.04816886]\n",
      "[ episode 446 ][ timestamp 1 ] state=[-0.04706975  0.02789885 -0.00396621 -0.04816886], action=1, reward=1.0, next_state=[-0.04651177  0.22307744 -0.00492959 -0.34210051]\n",
      "[ Experience replay ] starts\n",
      "[ episode 446 ][ timestamp 2 ] state=[-0.04651177  0.22307744 -0.00492959 -0.34210051], action=1, reward=1.0, next_state=[-0.04205022  0.41826918 -0.0117716  -0.63633386]\n",
      "[ Experience replay ] starts\n",
      "[ episode 446 ][ timestamp 3 ] state=[-0.04205022  0.41826918 -0.0117716  -0.63633386], action=1, reward=1.0, next_state=[-0.03368484  0.61355331 -0.02449828 -0.93270052]\n",
      "[ Experience replay ] starts\n",
      "[ episode 446 ][ timestamp 4 ] state=[-0.03368484  0.61355331 -0.02449828 -0.93270052], action=1, reward=1.0, next_state=[-0.02141377  0.80899709 -0.04315229 -1.23298   ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 446 ][ timestamp 5 ] state=[-0.02141377  0.80899709 -0.04315229 -1.23298   ], action=1, reward=1.0, next_state=[-0.00523383  1.00464652 -0.06781189 -1.53886378]\n",
      "[ Experience replay ] starts\n",
      "[ episode 446 ][ timestamp 6 ] state=[-0.00523383  1.00464652 -0.06781189 -1.53886378], action=1, reward=1.0, next_state=[ 0.0148591   1.20051566 -0.09858916 -1.85191365]\n",
      "[ Experience replay ] starts\n",
      "[ episode 446 ][ timestamp 7 ] state=[ 0.0148591   1.20051566 -0.09858916 -1.85191365], action=1, reward=1.0, next_state=[ 0.03886941  1.39657405 -0.13562744 -2.17351144]\n",
      "[ Experience replay ] starts\n",
      "[ episode 446 ][ timestamp 8 ] state=[ 0.03886941  1.39657405 -0.13562744 -2.17351144], action=1, reward=1.0, next_state=[ 0.06680089  1.59273175 -0.17909767 -2.50479824]\n",
      "[ Experience replay ] starts\n",
      "[ episode 446 ][ timestamp 9 ] state=[ 0.06680089  1.59273175 -0.17909767 -2.50479824], action=1, reward=-1.0, next_state=[ 0.09865553  1.78882191 -0.22919363 -2.84660243]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 446: Exploration_rate=0.01. Score=9.\n",
      "[ episode 447 ] state=[-2.99879499e-02 -3.49982260e-02 -3.89098213e-02  8.72230980e-05]\n",
      "[ episode 447 ][ timestamp 1 ] state=[-2.99879499e-02 -3.49982260e-02 -3.89098213e-02  8.72230980e-05], action=1, reward=1.0, next_state=[-0.03068791  0.16065952 -0.03890808 -0.30461386]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 2 ] state=[-0.03068791  0.16065952 -0.03890808 -0.30461386], action=1, reward=1.0, next_state=[-0.02747472  0.35631373 -0.04500035 -0.60930914]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 3 ] state=[-0.02747472  0.35631373 -0.04500035 -0.60930914], action=1, reward=1.0, next_state=[-0.02034845  0.5520349  -0.05718654 -0.91581934]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 4 ] state=[-0.02034845  0.5520349  -0.05718654 -0.91581934], action=0, reward=1.0, next_state=[-0.00930775  0.35773098 -0.07550292 -0.64164358]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 5 ] state=[-0.00930775  0.35773098 -0.07550292 -0.64164358], action=0, reward=1.0, next_state=[-0.00215313  0.16373821 -0.0883358  -0.37366022]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 6 ] state=[-0.00215313  0.16373821 -0.0883358  -0.37366022], action=0, reward=1.0, next_state=[ 0.00112163 -0.03002513 -0.095809   -0.11008542]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 7 ] state=[ 0.00112163 -0.03002513 -0.095809   -0.11008542], action=0, reward=1.0, next_state=[ 0.00052113 -0.22365291 -0.09801071  0.15089946]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 8 ] state=[ 0.00052113 -0.22365291 -0.09801071  0.15089946], action=0, reward=1.0, next_state=[-0.00395193 -0.41724477 -0.09499272  0.41112457]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 9 ] state=[-0.00395193 -0.41724477 -0.09499272  0.41112457], action=1, reward=1.0, next_state=[-0.01229682 -0.22091354 -0.08677023  0.09006958]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 10 ] state=[-0.01229682 -0.22091354 -0.08677023  0.09006958], action=0, reward=1.0, next_state=[-0.01671509 -0.41469152 -0.08496884  0.35416455]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 11 ] state=[-0.01671509 -0.41469152 -0.08496884  0.35416455], action=1, reward=1.0, next_state=[-0.02500893 -0.21847064 -0.07788554  0.0359443 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 12 ] state=[-0.02500893 -0.21847064 -0.07788554  0.0359443 ], action=0, reward=1.0, next_state=[-0.02937834 -0.41239431 -0.07716666  0.30307276]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 13 ] state=[-0.02937834 -0.41239431 -0.07716666  0.30307276], action=0, reward=1.0, next_state=[-0.03762622 -0.60633654 -0.0711052   0.5704559 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 14 ] state=[-0.03762622 -0.60633654 -0.0711052   0.5704559 ], action=0, reward=1.0, next_state=[-0.04975295 -0.80039305 -0.05969609  0.83991781]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 15 ] state=[-0.04975295 -0.80039305 -0.05969609  0.83991781], action=1, reward=1.0, next_state=[-0.06576082 -0.60450909 -0.04289773  0.52907502]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 16 ] state=[-0.06576082 -0.60450909 -0.04289773  0.52907502], action=0, reward=1.0, next_state=[-0.077851   -0.79900211 -0.03231623  0.8079381 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 17 ] state=[-0.077851   -0.79900211 -0.03231623  0.8079381 ], action=1, reward=1.0, next_state=[-0.09383104 -0.60345254 -0.01615747  0.50526757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 18 ] state=[-0.09383104 -0.60345254 -0.01615747  0.50526757], action=1, reward=1.0, next_state=[-0.10590009 -0.40810666 -0.00605212  0.20753692]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 19 ] state=[-0.10590009 -0.40810666 -0.00605212  0.20753692], action=1, reward=1.0, next_state=[-0.11406222 -0.21289869 -0.00190138 -0.08704898]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 20 ] state=[-0.11406222 -0.21289869 -0.00190138 -0.08704898], action=1, reward=1.0, next_state=[-0.1183202  -0.01774954 -0.00364236 -0.38033119]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 21 ] state=[-0.1183202  -0.01774954 -0.00364236 -0.38033119], action=1, reward=1.0, next_state=[-0.11867519  0.17742395 -0.01124898 -0.67416033]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 22 ] state=[-0.11867519  0.17742395 -0.01124898 -0.67416033], action=1, reward=1.0, next_state=[-0.11512671  0.37270041 -0.02473219 -0.97036363]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 23 ] state=[-0.11512671  0.37270041 -0.02473219 -0.97036363], action=1, reward=1.0, next_state=[-0.1076727   0.56814545 -0.04413946 -1.27071205]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 24 ] state=[-0.1076727   0.56814545 -0.04413946 -1.27071205], action=1, reward=1.0, next_state=[-0.09630979  0.76380222 -0.0695537  -1.57688414]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 25 ] state=[-0.09630979  0.76380222 -0.0695537  -1.57688414], action=1, reward=1.0, next_state=[-0.08103375  0.95968064 -0.10109138 -1.89042365]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 26 ] state=[-0.08103375  0.95968064 -0.10109138 -1.89042365], action=1, reward=1.0, next_state=[-0.06184013  1.15574456 -0.13889986 -2.21268833]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 27 ] state=[-0.06184013  1.15574456 -0.13889986 -2.21268833], action=1, reward=1.0, next_state=[-0.03872524  1.35189655 -0.18315362 -2.54478796]\n",
      "[ Experience replay ] starts\n",
      "[ episode 447 ][ timestamp 28 ] state=[-0.03872524  1.35189655 -0.18315362 -2.54478796], action=1, reward=-1.0, next_state=[-0.01168731  1.54796025 -0.23404938 -2.88751115]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 447: Exploration_rate=0.01. Score=28.\n",
      "[ episode 448 ] state=[-0.02442486 -0.02681433  0.04970562  0.01886878]\n",
      "[ episode 448 ][ timestamp 1 ] state=[-0.02442486 -0.02681433  0.04970562  0.01886878], action=1, reward=1.0, next_state=[-0.02496114  0.16756085  0.05008299 -0.25772646]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 2 ] state=[-0.02496114  0.16756085  0.05008299 -0.25772646], action=1, reward=1.0, next_state=[-0.02160993  0.36193334  0.04492846 -0.53420136]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 3 ] state=[-0.02160993  0.36193334  0.04492846 -0.53420136], action=1, reward=1.0, next_state=[-0.01437126  0.55639562  0.03424444 -0.8123959 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 4 ] state=[-0.01437126  0.55639562  0.03424444 -0.8123959 ], action=1, reward=1.0, next_state=[-0.00324335  0.75103219  0.01799652 -1.09411369]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 5 ] state=[-0.00324335  0.75103219  0.01799652 -1.09411369], action=1, reward=1.0, next_state=[ 0.0117773   0.9459125  -0.00388576 -1.38109613]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 6 ] state=[ 0.0117773   0.9459125  -0.00388576 -1.38109613], action=0, reward=1.0, next_state=[ 0.03069555  0.75083926 -0.03150768 -1.08963089]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 448 ][ timestamp 7 ] state=[ 0.03069555  0.75083926 -0.03150768 -1.08963089], action=0, reward=1.0, next_state=[ 0.04571233  0.55614652 -0.0533003  -0.80699846]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 8 ] state=[ 0.04571233  0.55614652 -0.0533003  -0.80699846], action=0, reward=1.0, next_state=[ 0.05683526  0.36179404 -0.06944027 -0.53154661]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 9 ] state=[ 0.05683526  0.36179404 -0.06944027 -0.53154661], action=0, reward=1.0, next_state=[ 0.06407114  0.16771401 -0.0800712  -0.2615272 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 10 ] state=[ 0.06407114  0.16771401 -0.0800712  -0.2615272 ], action=0, reward=1.0, next_state=[ 0.06742542 -0.02617908 -0.08530174  0.0048648 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 11 ] state=[ 0.06742542 -0.02617908 -0.08530174  0.0048648 ], action=0, reward=1.0, next_state=[ 0.06690184 -0.21998067 -0.08520445  0.26946189]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 12 ] state=[ 0.06690184 -0.21998067 -0.08520445  0.26946189], action=1, reward=1.0, next_state=[ 0.06250223 -0.02375268 -0.07981521 -0.04883211]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 13 ] state=[ 0.06250223 -0.02375268 -0.07981521 -0.04883211], action=0, reward=1.0, next_state=[ 0.06202717 -0.21764482 -0.08079185  0.21763943]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 14 ] state=[ 0.06202717 -0.21764482 -0.08079185  0.21763943], action=1, reward=1.0, next_state=[ 0.05767428 -0.02146645 -0.07643906 -0.09939522]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 15 ] state=[ 0.05767428 -0.02146645 -0.07643906 -0.09939522], action=0, reward=1.0, next_state=[ 0.05724495 -0.2154144  -0.07842697  0.16822599]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 16 ] state=[ 0.05724495 -0.2154144  -0.07842697  0.16822599], action=1, reward=1.0, next_state=[ 0.05293666 -0.01926256 -0.07506245 -0.14813127]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 17 ] state=[ 0.05293666 -0.01926256 -0.07506245 -0.14813127], action=0, reward=1.0, next_state=[ 0.05255141 -0.21323383 -0.07802507  0.11995871]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 18 ] state=[ 0.05255141 -0.21323383 -0.07802507  0.11995871], action=0, reward=1.0, next_state=[ 0.04828673 -0.40715621 -0.0756259   0.38704118]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 19 ] state=[ 0.04828673 -0.40715621 -0.0756259   0.38704118], action=0, reward=1.0, next_state=[ 0.04014361 -0.60112773 -0.06788507  0.654954  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 20 ] state=[ 0.04014361 -0.60112773 -0.06788507  0.654954  ], action=0, reward=1.0, next_state=[ 0.02812105 -0.79524213 -0.05478599  0.92551206]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 21 ] state=[ 0.02812105 -0.79524213 -0.05478599  0.92551206], action=1, reward=1.0, next_state=[ 0.01221621 -0.59942476 -0.03627575  0.61612768]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 22 ] state=[ 0.01221621 -0.59942476 -0.03627575  0.61612768], action=1, reward=1.0, next_state=[ 2.27714906e-04 -4.03815281e-01 -2.39531983e-02  3.12243759e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 23 ] state=[ 2.27714906e-04 -4.03815281e-01 -2.39531983e-02  3.12243759e-01], action=0, reward=1.0, next_state=[-0.00784859 -0.59858794 -0.01770832  0.59727737]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 24 ] state=[-0.00784859 -0.59858794 -0.01770832  0.59727737], action=1, reward=1.0, next_state=[-0.01982035 -0.40322273 -0.00576278  0.29906952]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 25 ] state=[-0.01982035 -0.40322273 -0.00576278  0.29906952], action=1, reward=1.0, next_state=[-0.0278848  -0.20801911  0.00021861  0.00457471]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 26 ] state=[-0.0278848  -0.20801911  0.00021861  0.00457471], action=1, reward=1.0, next_state=[-0.03204519 -0.01290029  0.00031011 -0.28803923]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 27 ] state=[-0.03204519 -0.01290029  0.00031011 -0.28803923], action=1, reward=1.0, next_state=[-0.03230319  0.18221724 -0.00545068 -0.58062434]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 28 ] state=[-0.03230319  0.18221724 -0.00545068 -0.58062434], action=1, reward=1.0, next_state=[-0.02865885  0.37741514 -0.01706316 -0.87501934]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 29 ] state=[-0.02865885  0.37741514 -0.01706316 -0.87501934], action=1, reward=1.0, next_state=[-0.02111054  0.57276485 -0.03456355 -1.17301757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 30 ] state=[-0.02111054  0.57276485 -0.03456355 -1.17301757], action=0, reward=1.0, next_state=[-0.00965525  0.37810881 -0.0580239  -0.89136757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 31 ] state=[-0.00965525  0.37810881 -0.0580239  -0.89136757], action=0, reward=1.0, next_state=[-0.00209307  0.18382    -0.07585125 -0.61747426]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 32 ] state=[-0.00209307  0.18382    -0.07585125 -0.61747426], action=0, reward=1.0, next_state=[ 0.00158333 -0.01016498 -0.08820074 -0.34961233]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 33 ] state=[ 0.00158333 -0.01016498 -0.08820074 -0.34961233], action=0, reward=1.0, next_state=[ 0.00138003 -0.20392905 -0.09519298 -0.08599343]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 34 ] state=[ 0.00138003 -0.20392905 -0.09519298 -0.08599343], action=0, reward=1.0, next_state=[-0.00269855 -0.39756671 -0.09691285  0.17520355]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 35 ] state=[-0.00269855 -0.39756671 -0.09691285  0.17520355], action=0, reward=1.0, next_state=[-0.01064989 -0.59117773 -0.09340878  0.43580954]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 36 ] state=[-0.01064989 -0.59117773 -0.09340878  0.43580954], action=0, reward=1.0, next_state=[-0.02247344 -0.78486183 -0.08469259  0.6976469 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 37 ] state=[-0.02247344 -0.78486183 -0.08469259  0.6976469 ], action=0, reward=1.0, next_state=[-0.03817068 -0.97871364 -0.07073965  0.96251251]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 38 ] state=[-0.03817068 -0.97871364 -0.07073965  0.96251251], action=0, reward=1.0, next_state=[-0.05774495 -1.17281739 -0.0514894   1.23215984]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 39 ] state=[-0.05774495 -1.17281739 -0.0514894   1.23215984], action=0, reward=1.0, next_state=[-0.0812013  -1.36724076 -0.02684621  1.5082772 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 40 ] state=[-0.0812013  -1.36724076 -0.02684621  1.5082772 ], action=0, reward=1.0, next_state=[-0.10854611 -1.56202719  0.00331934  1.79245973]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 41 ] state=[-0.10854611 -1.56202719  0.00331934  1.79245973], action=0, reward=1.0, next_state=[-0.13978666 -1.75718618  0.03916853  2.08617249]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 42 ] state=[-0.13978666 -1.75718618  0.03916853  2.08617249], action=0, reward=1.0, next_state=[-0.17493038 -1.95268109  0.08089198  2.39070255]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 43 ] state=[-0.17493038 -1.95268109  0.08089198  2.39070255], action=0, reward=1.0, next_state=[-0.213984   -2.14841402  0.12870603  2.70709819]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 44 ] state=[-0.213984   -2.14841402  0.12870603  2.70709819], action=0, reward=1.0, next_state=[-0.25695228 -2.34420781  0.182848    3.0360949 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 448 ][ timestamp 45 ] state=[-0.25695228 -2.34420781  0.182848    3.0360949 ], action=0, reward=-1.0, next_state=[-0.30383644 -2.53978566  0.2435699   3.37802945]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 448: Exploration_rate=0.01. Score=45.\n",
      "[ episode 449 ] state=[ 0.03621963  0.02846863 -0.01506048 -0.00203763]\n",
      "[ episode 449 ][ timestamp 1 ] state=[ 0.03621963  0.02846863 -0.01506048 -0.00203763], action=1, reward=1.0, next_state=[ 0.036789    0.2238033  -0.01510123 -0.29943402]\n",
      "[ Experience replay ] starts\n",
      "[ episode 449 ][ timestamp 2 ] state=[ 0.036789    0.2238033  -0.01510123 -0.29943402], action=1, reward=1.0, next_state=[ 0.04126507  0.41913721 -0.02108991 -0.59684107]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 449 ][ timestamp 3 ] state=[ 0.04126507  0.41913721 -0.02108991 -0.59684107], action=1, reward=1.0, next_state=[ 0.04964781  0.61454784 -0.03302673 -0.89609181]\n",
      "[ Experience replay ] starts\n",
      "[ episode 449 ][ timestamp 4 ] state=[ 0.04964781  0.61454784 -0.03302673 -0.89609181], action=1, reward=1.0, next_state=[ 0.06193877  0.81010163 -0.05094857 -1.19897063]\n",
      "[ Experience replay ] starts\n",
      "[ episode 449 ][ timestamp 5 ] state=[ 0.06193877  0.81010163 -0.05094857 -1.19897063], action=0, reward=1.0, next_state=[ 0.0781408   0.61567457 -0.07492798 -0.92268086]\n",
      "[ Experience replay ] starts\n",
      "[ episode 449 ][ timestamp 6 ] state=[ 0.0781408   0.61567457 -0.07492798 -0.92268086], action=0, reward=1.0, next_state=[ 0.09045429  0.42164056 -0.0933816  -0.6544547 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 449 ][ timestamp 7 ] state=[ 0.09045429  0.42164056 -0.0933816  -0.6544547 ], action=1, reward=1.0, next_state=[ 0.0988871   0.61793012 -0.1064707  -0.97502053]\n",
      "[ Experience replay ] starts\n",
      "[ episode 449 ][ timestamp 8 ] state=[ 0.0988871   0.61793012 -0.1064707  -0.97502053], action=1, reward=1.0, next_state=[ 0.1112457   0.81430667 -0.12597111 -1.29916062]\n",
      "[ Experience replay ] starts\n",
      "[ episode 449 ][ timestamp 9 ] state=[ 0.1112457   0.81430667 -0.12597111 -1.29916062], action=0, reward=1.0, next_state=[ 0.12753184  0.62098868 -0.15195432 -1.048419  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 449 ][ timestamp 10 ] state=[ 0.12753184  0.62098868 -0.15195432 -1.048419  ], action=0, reward=1.0, next_state=[ 0.13995161  0.42817352 -0.1729227  -0.80703178]\n",
      "[ Experience replay ] starts\n",
      "[ episode 449 ][ timestamp 11 ] state=[ 0.13995161  0.42817352 -0.1729227  -0.80703178], action=0, reward=1.0, next_state=[ 0.14851508  0.23578981 -0.18906333 -0.57334629]\n",
      "[ Experience replay ] starts\n",
      "[ episode 449 ][ timestamp 12 ] state=[ 0.14851508  0.23578981 -0.18906333 -0.57334629], action=0, reward=1.0, next_state=[ 0.15323088  0.0437512  -0.20053026 -0.34567542]\n",
      "[ Experience replay ] starts\n",
      "[ episode 449 ][ timestamp 13 ] state=[ 0.15323088  0.0437512  -0.20053026 -0.34567542], action=0, reward=1.0, next_state=[ 0.1541059  -0.14803777 -0.20744377 -0.1223184 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 449 ][ timestamp 14 ] state=[ 0.1541059  -0.14803777 -0.20744377 -0.1223184 ], action=1, reward=-1.0, next_state=[ 0.15114515  0.04935892 -0.20989014 -0.47261732]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 449: Exploration_rate=0.01. Score=14.\n",
      "[ episode 450 ] state=[-0.00328254  0.04287901  0.02494572  0.01389791]\n",
      "[ episode 450 ][ timestamp 1 ] state=[-0.00328254  0.04287901  0.02494572  0.01389791], action=1, reward=1.0, next_state=[-0.00242496  0.23763449  0.02522368 -0.27081114]\n",
      "[ Experience replay ] starts\n",
      "[ episode 450 ][ timestamp 2 ] state=[-0.00242496  0.23763449  0.02522368 -0.27081114], action=0, reward=1.0, next_state=[0.00232773 0.04216185 0.01980746 0.02971953]\n",
      "[ Experience replay ] starts\n",
      "[ episode 450 ][ timestamp 3 ] state=[0.00232773 0.04216185 0.01980746 0.02971953], action=1, reward=1.0, next_state=[ 0.00317097  0.23699423  0.02040185 -0.25664871]\n",
      "[ Experience replay ] starts\n",
      "[ episode 450 ][ timestamp 4 ] state=[ 0.00317097  0.23699423  0.02040185 -0.25664871], action=1, reward=1.0, next_state=[ 0.00791086  0.43181905  0.01526887 -0.54282739]\n",
      "[ Experience replay ] starts\n",
      "[ episode 450 ][ timestamp 5 ] state=[ 0.00791086  0.43181905  0.01526887 -0.54282739], action=1, reward=1.0, next_state=[ 0.01654724  0.62672312  0.00441232 -0.83066054]\n",
      "[ Experience replay ] starts\n",
      "[ episode 450 ][ timestamp 6 ] state=[ 0.01654724  0.62672312  0.00441232 -0.83066054], action=1, reward=1.0, next_state=[ 0.0290817   0.82178449 -0.01220089 -1.12195252]\n",
      "[ Experience replay ] starts\n",
      "[ episode 450 ][ timestamp 7 ] state=[ 0.0290817   0.82178449 -0.01220089 -1.12195252], action=1, reward=1.0, next_state=[ 0.04551739  1.01706429 -0.03463994 -1.4184374 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 450 ][ timestamp 8 ] state=[ 0.04551739  1.01706429 -0.03463994 -1.4184374 ], action=1, reward=1.0, next_state=[ 0.06585867  1.21259749 -0.06300869 -1.72174335]\n",
      "[ Experience replay ] starts\n",
      "[ episode 450 ][ timestamp 9 ] state=[ 0.06585867  1.21259749 -0.06300869 -1.72174335], action=1, reward=1.0, next_state=[ 0.09011062  1.40838178 -0.09744355 -2.03334932]\n",
      "[ Experience replay ] starts\n",
      "[ episode 450 ][ timestamp 10 ] state=[ 0.09011062  1.40838178 -0.09744355 -2.03334932], action=1, reward=1.0, next_state=[ 0.11827826  1.60436422 -0.13811054 -2.35453149]\n",
      "[ Experience replay ] starts\n",
      "[ episode 450 ][ timestamp 11 ] state=[ 0.11827826  1.60436422 -0.13811054 -2.35453149], action=1, reward=1.0, next_state=[ 0.15036554  1.80042527 -0.18520117 -2.68629823]\n",
      "[ Experience replay ] starts\n",
      "[ episode 450 ][ timestamp 12 ] state=[ 0.15036554  1.80042527 -0.18520117 -2.68629823], action=1, reward=-1.0, next_state=[ 0.18637405  1.9963604  -0.23892713 -3.02931338]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 450: Exploration_rate=0.01. Score=12.\n",
      "[ episode 451 ] state=[ 0.03320444 -0.02730783  0.04244243  0.02528213]\n",
      "[ episode 451 ][ timestamp 1 ] state=[ 0.03320444 -0.02730783  0.04244243  0.02528213], action=1, reward=1.0, next_state=[ 0.03265828  0.16718057  0.04294807 -0.25371342]\n",
      "[ Experience replay ] starts\n",
      "[ episode 451 ][ timestamp 2 ] state=[ 0.03265828  0.16718057  0.04294807 -0.25371342], action=1, reward=1.0, next_state=[ 0.03600189  0.3616638   0.0378738  -0.53254641]\n",
      "[ Experience replay ] starts\n",
      "[ episode 451 ][ timestamp 3 ] state=[ 0.03600189  0.3616638   0.0378738  -0.53254641], action=1, reward=1.0, next_state=[ 0.04323517  0.55623317  0.02722288 -0.81305894]\n",
      "[ Experience replay ] starts\n",
      "[ episode 451 ][ timestamp 4 ] state=[ 0.04323517  0.55623317  0.02722288 -0.81305894], action=1, reward=1.0, next_state=[ 0.05435983  0.7509719   0.0109617  -1.09705626]\n",
      "[ Experience replay ] starts\n",
      "[ episode 451 ][ timestamp 5 ] state=[ 0.05435983  0.7509719   0.0109617  -1.09705626], action=1, reward=1.0, next_state=[ 0.06937927  0.94594781 -0.01097943 -1.38627988]\n",
      "[ Experience replay ] starts\n",
      "[ episode 451 ][ timestamp 6 ] state=[ 0.06937927  0.94594781 -0.01097943 -1.38627988], action=1, reward=1.0, next_state=[ 0.08829823  1.1412049  -0.03870502 -1.68237575]\n",
      "[ Experience replay ] starts\n",
      "[ episode 451 ][ timestamp 7 ] state=[ 0.08829823  1.1412049  -0.03870502 -1.68237575], action=1, reward=1.0, next_state=[ 0.11112232  1.33675311 -0.07235254 -1.98685482]\n",
      "[ Experience replay ] starts\n",
      "[ episode 451 ][ timestamp 8 ] state=[ 0.11112232  1.33675311 -0.07235254 -1.98685482], action=1, reward=1.0, next_state=[ 0.13785739  1.53255581 -0.11208964 -2.30104354]\n",
      "[ Experience replay ] starts\n",
      "[ episode 451 ][ timestamp 9 ] state=[ 0.13785739  1.53255581 -0.11208964 -2.30104354], action=1, reward=1.0, next_state=[ 0.1685085   1.72851466 -0.15811051 -2.6260226 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 451 ][ timestamp 10 ] state=[ 0.1685085   1.72851466 -0.15811051 -2.6260226 ], action=1, reward=-1.0, next_state=[ 0.2030788   1.9244518  -0.21063096 -2.96255335]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 451: Exploration_rate=0.01. Score=10.\n",
      "[ episode 452 ] state=[-0.00018595 -0.02243502 -0.04068645 -0.00104953]\n",
      "[ episode 452 ][ timestamp 1 ] state=[-0.00018595 -0.02243502 -0.04068645 -0.00104953], action=1, reward=1.0, next_state=[-0.00063465  0.1732461  -0.04070744 -0.30628682]\n",
      "[ Experience replay ] starts\n",
      "[ episode 452 ][ timestamp 2 ] state=[-0.00063465  0.1732461  -0.04070744 -0.30628682], action=1, reward=1.0, next_state=[ 0.00283027  0.36892377 -0.04683318 -0.61152485]\n",
      "[ Experience replay ] starts\n",
      "[ episode 452 ][ timestamp 3 ] state=[ 0.00283027  0.36892377 -0.04683318 -0.61152485], action=1, reward=1.0, next_state=[ 0.01020875  0.56466793 -0.05906367 -0.91858306]\n",
      "[ Experience replay ] starts\n",
      "[ episode 452 ][ timestamp 4 ] state=[ 0.01020875  0.56466793 -0.05906367 -0.91858306], action=1, reward=1.0, next_state=[ 0.0215021   0.76053644 -0.07743534 -1.22922813]\n",
      "[ Experience replay ] starts\n",
      "[ episode 452 ][ timestamp 5 ] state=[ 0.0215021   0.76053644 -0.07743534 -1.22922813], action=1, reward=1.0, next_state=[ 0.03671283  0.95656462 -0.1020199  -1.54513251]\n",
      "[ Experience replay ] starts\n",
      "[ episode 452 ][ timestamp 6 ] state=[ 0.03671283  0.95656462 -0.1020199  -1.54513251], action=1, reward=1.0, next_state=[ 0.05584413  1.15275345 -0.13292255 -1.86782747]\n",
      "[ Experience replay ] starts\n",
      "[ episode 452 ][ timestamp 7 ] state=[ 0.05584413  1.15275345 -0.13292255 -1.86782747], action=1, reward=1.0, next_state=[ 0.07889919  1.34905594 -0.1702791  -2.19864803]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 452 ][ timestamp 8 ] state=[ 0.07889919  1.34905594 -0.1702791  -2.19864803], action=1, reward=-1.0, next_state=[ 0.10588031  1.54536136 -0.21425206 -2.53866804]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 452: Exploration_rate=0.01. Score=8.\n",
      "[ episode 453 ] state=[-0.03449875  0.00239387  0.04390228  0.00128242]\n",
      "[ episode 453 ][ timestamp 1 ] state=[-0.03449875  0.00239387  0.04390228  0.00128242], action=1, reward=1.0, next_state=[-0.03445087  0.1968596   0.04392792 -0.27723198]\n",
      "[ Experience replay ] starts\n",
      "[ episode 453 ][ timestamp 2 ] state=[-0.03445087  0.1968596   0.04392792 -0.27723198], action=1, reward=1.0, next_state=[-0.03051368  0.39132822  0.03838328 -0.55574286]\n",
      "[ Experience replay ] starts\n",
      "[ episode 453 ][ timestamp 3 ] state=[-0.03051368  0.39132822  0.03838328 -0.55574286], action=1, reward=1.0, next_state=[-0.02268711  0.58589084  0.02726843 -0.83608992]\n",
      "[ Experience replay ] starts\n",
      "[ episode 453 ][ timestamp 4 ] state=[-0.02268711  0.58589084  0.02726843 -0.83608992], action=1, reward=1.0, next_state=[-0.01096929  0.78062992  0.01054663 -1.12007401]\n",
      "[ Experience replay ] starts\n",
      "[ episode 453 ][ timestamp 5 ] state=[-0.01096929  0.78062992  0.01054663 -1.12007401], action=1, reward=1.0, next_state=[ 0.0046433   0.97561195 -0.01185485 -1.40943014]\n",
      "[ Experience replay ] starts\n",
      "[ episode 453 ][ timestamp 6 ] state=[ 0.0046433   0.97561195 -0.01185485 -1.40943014], action=1, reward=1.0, next_state=[ 0.02415554  1.17087891 -0.04004345 -1.70579526]\n",
      "[ Experience replay ] starts\n",
      "[ episode 453 ][ timestamp 7 ] state=[ 0.02415554  1.17087891 -0.04004345 -1.70579526], action=1, reward=1.0, next_state=[ 0.04757312  1.36643796 -0.07415936 -2.0106683 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 453 ][ timestamp 8 ] state=[ 0.04757312  1.36643796 -0.07415936 -2.0106683 ], action=1, reward=1.0, next_state=[ 0.07490188  1.56224869 -0.11437273 -2.32535998]\n",
      "[ Experience replay ] starts\n",
      "[ episode 453 ][ timestamp 9 ] state=[ 0.07490188  1.56224869 -0.11437273 -2.32535998], action=1, reward=1.0, next_state=[ 0.10614685  1.75820785 -0.16087993 -2.65093061]\n",
      "[ Experience replay ] starts\n",
      "[ episode 453 ][ timestamp 10 ] state=[ 0.10614685  1.75820785 -0.16087993 -2.65093061], action=1, reward=-1.0, next_state=[ 0.14131101  1.95413134 -0.21389854 -2.98811576]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 453: Exploration_rate=0.01. Score=10.\n",
      "[ episode 454 ] state=[-0.0274767   0.03900106 -0.02788021  0.03194701]\n",
      "[ episode 454 ][ timestamp 1 ] state=[-0.0274767   0.03900106 -0.02788021  0.03194701], action=1, reward=1.0, next_state=[-0.02669668  0.23451151 -0.02724127 -0.2694004 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 454 ][ timestamp 2 ] state=[-0.02669668  0.23451151 -0.02724127 -0.2694004 ], action=1, reward=1.0, next_state=[-0.02200645  0.4300114  -0.03262928 -0.57054939]\n",
      "[ Experience replay ] starts\n",
      "[ episode 454 ][ timestamp 3 ] state=[-0.02200645  0.4300114  -0.03262928 -0.57054939], action=1, reward=1.0, next_state=[-0.01340622  0.62557538 -0.04404026 -0.87333052]\n",
      "[ Experience replay ] starts\n",
      "[ episode 454 ][ timestamp 4 ] state=[-0.01340622  0.62557538 -0.04404026 -0.87333052], action=1, reward=1.0, next_state=[-8.94717074e-04  8.21267604e-01 -6.15068733e-02 -1.17952789e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 454 ][ timestamp 5 ] state=[-8.94717074e-04  8.21267604e-01 -6.15068733e-02 -1.17952789e+00], action=1, reward=1.0, next_state=[ 0.01553064  1.01713185 -0.08509743 -1.49084032]\n",
      "[ Experience replay ] starts\n",
      "[ episode 454 ][ timestamp 6 ] state=[ 0.01553064  1.01713185 -0.08509743 -1.49084032], action=1, reward=1.0, next_state=[ 0.03587327  1.21318041 -0.11491424 -1.80883748]\n",
      "[ Experience replay ] starts\n",
      "[ episode 454 ][ timestamp 7 ] state=[ 0.03587327  1.21318041 -0.11491424 -1.80883748], action=1, reward=1.0, next_state=[ 0.06013688  1.40938117 -0.15109099 -2.13490808]\n",
      "[ Experience replay ] starts\n",
      "[ episode 454 ][ timestamp 8 ] state=[ 0.06013688  1.40938117 -0.15109099 -2.13490808], action=1, reward=1.0, next_state=[ 0.0883245   1.60564252 -0.19378915 -2.47019816]\n",
      "[ Experience replay ] starts\n",
      "[ episode 454 ][ timestamp 9 ] state=[ 0.0883245   1.60564252 -0.19378915 -2.47019816], action=1, reward=-1.0, next_state=[ 0.12043735  1.80179591 -0.24319311 -2.8155388 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 454: Exploration_rate=0.01. Score=9.\n",
      "[ episode 455 ] state=[ 0.01285169 -0.01487094 -0.02515056 -0.03185892]\n",
      "[ episode 455 ][ timestamp 1 ] state=[ 0.01285169 -0.01487094 -0.02515056 -0.03185892], action=1, reward=1.0, next_state=[ 0.01255427  0.18060248 -0.02578774 -0.33236981]\n",
      "[ Experience replay ] starts\n",
      "[ episode 455 ][ timestamp 2 ] state=[ 0.01255427  0.18060248 -0.02578774 -0.33236981], action=1, reward=1.0, next_state=[ 0.01616632  0.37608181 -0.03243514 -0.63307208]\n",
      "[ Experience replay ] starts\n",
      "[ episode 455 ][ timestamp 3 ] state=[ 0.01616632  0.37608181 -0.03243514 -0.63307208], action=1, reward=1.0, next_state=[ 0.02368796  0.57164088 -0.04509658 -0.93579064]\n",
      "[ Experience replay ] starts\n",
      "[ episode 455 ][ timestamp 4 ] state=[ 0.02368796  0.57164088 -0.04509658 -0.93579064], action=1, reward=1.0, next_state=[ 0.03512077  0.76734109 -0.06381239 -1.24229641]\n",
      "[ Experience replay ] starts\n",
      "[ episode 455 ][ timestamp 5 ] state=[ 0.03512077  0.76734109 -0.06381239 -1.24229641], action=1, reward=1.0, next_state=[ 0.0504676   0.96322142 -0.08865832 -1.55426701]\n",
      "[ Experience replay ] starts\n",
      "[ episode 455 ][ timestamp 6 ] state=[ 0.0504676   0.96322142 -0.08865832 -1.55426701], action=1, reward=1.0, next_state=[ 0.06973202  1.15928706 -0.11974366 -1.87324178]\n",
      "[ Experience replay ] starts\n",
      "[ episode 455 ][ timestamp 7 ] state=[ 0.06973202  1.15928706 -0.11974366 -1.87324178], action=1, reward=1.0, next_state=[ 0.09291777  1.35549611 -0.1572085  -2.20056843]\n",
      "[ Experience replay ] starts\n",
      "[ episode 455 ][ timestamp 8 ] state=[ 0.09291777  1.35549611 -0.1572085  -2.20056843], action=1, reward=1.0, next_state=[ 0.12002769  1.55174406 -0.20121987 -2.53733937]\n",
      "[ Experience replay ] starts\n",
      "[ episode 455 ][ timestamp 9 ] state=[ 0.12002769  1.55174406 -0.20121987 -2.53733937], action=0, reward=-1.0, next_state=[ 0.15106257  1.3587394  -0.25196665 -2.31243384]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 455: Exploration_rate=0.01. Score=9.\n",
      "[ episode 456 ] state=[-0.00361846 -0.0291291  -0.04530412 -0.01307229]\n",
      "[ episode 456 ][ timestamp 1 ] state=[-0.00361846 -0.0291291  -0.04530412 -0.01307229], action=1, reward=1.0, next_state=[-0.00420104  0.1666123  -0.04556556 -0.31969799]\n",
      "[ Experience replay ] starts\n",
      "[ episode 456 ][ timestamp 2 ] state=[-0.00420104  0.1666123  -0.04556556 -0.31969799], action=1, reward=1.0, next_state=[-0.0008688   0.36235257 -0.05195952 -0.62639528]\n",
      "[ Experience replay ] starts\n",
      "[ episode 456 ][ timestamp 3 ] state=[-0.0008688   0.36235257 -0.05195952 -0.62639528], action=1, reward=1.0, next_state=[ 0.00637825  0.55815983 -0.06448743 -0.93497901]\n",
      "[ Experience replay ] starts\n",
      "[ episode 456 ][ timestamp 4 ] state=[ 0.00637825  0.55815983 -0.06448743 -0.93497901], action=0, reward=1.0, next_state=[ 0.01754145  0.36396421 -0.08318701 -0.66323722]\n",
      "[ Experience replay ] starts\n",
      "[ episode 456 ][ timestamp 5 ] state=[ 0.01754145  0.36396421 -0.08318701 -0.66323722], action=1, reward=1.0, next_state=[ 0.02482073  0.56013892 -0.09645175 -0.98091051]\n",
      "[ Experience replay ] starts\n",
      "[ episode 456 ][ timestamp 6 ] state=[ 0.02482073  0.56013892 -0.09645175 -0.98091051], action=1, reward=1.0, next_state=[ 0.03602351  0.75641198 -0.11606996 -1.30226459]\n",
      "[ Experience replay ] starts\n",
      "[ episode 456 ][ timestamp 7 ] state=[ 0.03602351  0.75641198 -0.11606996 -1.30226459], action=1, reward=1.0, next_state=[ 0.05115175  0.9527993  -0.14211525 -1.62891146]\n",
      "[ Experience replay ] starts\n",
      "[ episode 456 ][ timestamp 8 ] state=[ 0.05115175  0.9527993  -0.14211525 -1.62891146], action=1, reward=1.0, next_state=[ 0.07020774  1.149277   -0.17469348 -1.96229825]\n",
      "[ Experience replay ] starts\n",
      "[ episode 456 ][ timestamp 9 ] state=[ 0.07020774  1.149277   -0.17469348 -1.96229825], action=1, reward=-1.0, next_state=[ 0.09319328  1.3457664  -0.21393945 -2.30364549]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 456: Exploration_rate=0.01. Score=9.\n",
      "[ episode 457 ] state=[-0.02965558  0.04184406 -0.04010398  0.03092722]\n",
      "[ episode 457 ][ timestamp 1 ] state=[-0.02965558  0.04184406 -0.04010398  0.03092722], action=0, reward=1.0, next_state=[-0.02881869 -0.15268051 -0.03948544  0.31069206]\n",
      "[ Experience replay ] starts\n",
      "[ episode 457 ][ timestamp 2 ] state=[-0.02881869 -0.15268051 -0.03948544  0.31069206], action=1, reward=1.0, next_state=[-0.03187231  0.0429811  -0.0332716   0.0058227 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 457 ][ timestamp 3 ] state=[-0.03187231  0.0429811  -0.0332716   0.0058227 ], action=1, reward=1.0, next_state=[-0.03101268  0.23856403 -0.03315514 -0.29716936]\n",
      "[ Experience replay ] starts\n",
      "[ episode 457 ][ timestamp 4 ] state=[-0.03101268  0.23856403 -0.03315514 -0.29716936], action=1, reward=1.0, next_state=[-0.0262414   0.43414254 -0.03909853 -0.60012173]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 457 ][ timestamp 5 ] state=[-0.0262414   0.43414254 -0.03909853 -0.60012173], action=1, reward=1.0, next_state=[-0.01755855  0.62978905 -0.05110097 -0.90485924]\n",
      "[ Experience replay ] starts\n",
      "[ episode 457 ][ timestamp 6 ] state=[-0.01755855  0.62978905 -0.05110097 -0.90485924], action=1, reward=1.0, next_state=[-0.00496277  0.82556441 -0.06919815 -1.21315609]\n",
      "[ Experience replay ] starts\n",
      "[ episode 457 ][ timestamp 7 ] state=[-0.00496277  0.82556441 -0.06919815 -1.21315609], action=1, reward=1.0, next_state=[ 0.01154852  1.02150778 -0.09346127 -1.52669576]\n",
      "[ Experience replay ] starts\n",
      "[ episode 457 ][ timestamp 8 ] state=[ 0.01154852  1.02150778 -0.09346127 -1.52669576], action=1, reward=1.0, next_state=[ 0.03197867  1.21762511 -0.12399519 -1.84702551]\n",
      "[ Experience replay ] starts\n",
      "[ episode 457 ][ timestamp 9 ] state=[ 0.03197867  1.21762511 -0.12399519 -1.84702551], action=0, reward=1.0, next_state=[ 0.05633118  1.02406826 -0.1609357  -1.59528054]\n",
      "[ Experience replay ] starts\n",
      "[ episode 457 ][ timestamp 10 ] state=[ 0.05633118  1.02406826 -0.1609357  -1.59528054], action=1, reward=1.0, next_state=[ 0.07681254  1.22069133 -0.19284131 -1.93351506]\n",
      "[ Experience replay ] starts\n",
      "[ episode 457 ][ timestamp 11 ] state=[ 0.07681254  1.22069133 -0.19284131 -1.93351506], action=1, reward=-1.0, next_state=[ 0.10122637  1.41728352 -0.23151161 -2.27928179]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 457: Exploration_rate=0.01. Score=11.\n",
      "[ episode 458 ] state=[-0.04721018  0.01877772 -0.00932344  0.01597481]\n",
      "[ episode 458 ][ timestamp 1 ] state=[-0.04721018  0.01877772 -0.00932344  0.01597481], action=1, reward=1.0, next_state=[-0.04683463  0.21403213 -0.00900395 -0.27963513]\n",
      "[ Experience replay ] starts\n",
      "[ episode 458 ][ timestamp 2 ] state=[-0.04683463  0.21403213 -0.00900395 -0.27963513], action=1, reward=1.0, next_state=[-0.04255399  0.40928136 -0.01459665 -0.57514423]\n",
      "[ Experience replay ] starts\n",
      "[ episode 458 ][ timestamp 3 ] state=[-0.04255399  0.40928136 -0.01459665 -0.57514423], action=1, reward=1.0, next_state=[-0.03436836  0.60460486 -0.02609954 -0.87238953]\n",
      "[ Experience replay ] starts\n",
      "[ episode 458 ][ timestamp 4 ] state=[-0.03436836  0.60460486 -0.02609954 -0.87238953], action=1, reward=1.0, next_state=[-0.02227626  0.80007183 -0.04354733 -1.17316252]\n",
      "[ Experience replay ] starts\n",
      "[ episode 458 ][ timestamp 5 ] state=[-0.02227626  0.80007183 -0.04354733 -1.17316252], action=1, reward=1.0, next_state=[-0.00627483  0.99573194 -0.06701058 -1.47917331]\n",
      "[ Experience replay ] starts\n",
      "[ episode 458 ][ timestamp 6 ] state=[-0.00627483  0.99573194 -0.06701058 -1.47917331], action=1, reward=1.0, next_state=[ 0.01363981  1.19160485 -0.09659404 -1.79200963]\n",
      "[ Experience replay ] starts\n",
      "[ episode 458 ][ timestamp 7 ] state=[ 0.01363981  1.19160485 -0.09659404 -1.79200963], action=1, reward=1.0, next_state=[ 0.03747191  1.38766791 -0.13243424 -2.11308778]\n",
      "[ Experience replay ] starts\n",
      "[ episode 458 ][ timestamp 8 ] state=[ 0.03747191  1.38766791 -0.13243424 -2.11308778], action=1, reward=1.0, next_state=[ 0.06522527  1.58384151 -0.17469599 -2.44359342]\n",
      "[ Experience replay ] starts\n",
      "[ episode 458 ][ timestamp 9 ] state=[ 0.06522527  1.58384151 -0.17469599 -2.44359342], action=1, reward=-1.0, next_state=[ 0.0969021   1.77997213 -0.22356786 -2.78441129]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 458: Exploration_rate=0.01. Score=9.\n",
      "[ episode 459 ] state=[ 0.0060631   0.0498606  -0.02808524 -0.024025  ]\n",
      "[ episode 459 ][ timestamp 1 ] state=[ 0.0060631   0.0498606  -0.02808524 -0.024025  ], action=1, reward=1.0, next_state=[ 0.00706031  0.24537383 -0.02856574 -0.32543516]\n",
      "[ Experience replay ] starts\n",
      "[ episode 459 ][ timestamp 2 ] state=[ 0.00706031  0.24537383 -0.02856574 -0.32543516], action=1, reward=1.0, next_state=[ 0.01196779  0.44089061 -0.03507444 -0.62698787]\n",
      "[ Experience replay ] starts\n",
      "[ episode 459 ][ timestamp 3 ] state=[ 0.01196779  0.44089061 -0.03507444 -0.62698787], action=1, reward=1.0, next_state=[ 0.0207856   0.63648412 -0.0476142  -0.93050746]\n",
      "[ Experience replay ] starts\n",
      "[ episode 459 ][ timestamp 4 ] state=[ 0.0207856   0.63648412 -0.0476142  -0.93050746], action=1, reward=1.0, next_state=[ 0.03351528  0.83221524 -0.06622435 -1.23776469]\n",
      "[ Experience replay ] starts\n",
      "[ episode 459 ][ timestamp 5 ] state=[ 0.03351528  0.83221524 -0.06622435 -1.23776469], action=1, reward=1.0, next_state=[ 0.05015959  1.02812249 -0.09097964 -1.55043713]\n",
      "[ Experience replay ] starts\n",
      "[ episode 459 ][ timestamp 6 ] state=[ 0.05015959  1.02812249 -0.09097964 -1.55043713], action=1, reward=1.0, next_state=[ 0.07072204  1.22421052 -0.12198839 -1.87006385]\n",
      "[ Experience replay ] starts\n",
      "[ episode 459 ][ timestamp 7 ] state=[ 0.07072204  1.22421052 -0.12198839 -1.87006385], action=1, reward=1.0, next_state=[ 0.09520625  1.42043688 -0.15938966 -2.19799174]\n",
      "[ Experience replay ] starts\n",
      "[ episode 459 ][ timestamp 8 ] state=[ 0.09520625  1.42043688 -0.15938966 -2.19799174], action=1, reward=1.0, next_state=[ 0.12361498  1.61669637 -0.2033495  -2.53531179]\n",
      "[ Experience replay ] starts\n",
      "[ episode 459 ][ timestamp 9 ] state=[ 0.12361498  1.61669637 -0.2033495  -2.53531179], action=1, reward=-1.0, next_state=[ 0.15594891  1.81280325 -0.25405573 -2.8827847 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 459: Exploration_rate=0.01. Score=9.\n",
      "[ episode 460 ] state=[-0.03767338 -0.02141363 -0.00016216  0.04981019]\n",
      "[ episode 460 ][ timestamp 1 ] state=[-0.03767338 -0.02141363 -0.00016216  0.04981019], action=1, reward=1.0, next_state=[-0.03810165  0.17371065  0.00083404 -0.24292389]\n",
      "[ Experience replay ] starts\n",
      "[ episode 460 ][ timestamp 2 ] state=[-0.03810165  0.17371065  0.00083404 -0.24292389], action=1, reward=1.0, next_state=[-0.03462744  0.36882067 -0.00402444 -0.53534363]\n",
      "[ Experience replay ] starts\n",
      "[ episode 460 ][ timestamp 3 ] state=[-0.03462744  0.36882067 -0.00402444 -0.53534363], action=1, reward=1.0, next_state=[-0.02725102  0.56399898 -0.01473131 -0.8292919 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 460 ][ timestamp 4 ] state=[-0.02725102  0.56399898 -0.01473131 -0.8292919 ], action=1, reward=1.0, next_state=[-0.01597104  0.75931919 -0.03131715 -1.12657127]\n",
      "[ Experience replay ] starts\n",
      "[ episode 460 ][ timestamp 5 ] state=[-0.01597104  0.75931919 -0.03131715 -1.12657127], action=1, reward=1.0, next_state=[-7.84660515e-04  9.54837183e-01 -5.38485741e-02 -1.42891019e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 460 ][ timestamp 6 ] state=[-7.84660515e-04  9.54837183e-01 -5.38485741e-02 -1.42891019e+00], action=1, reward=1.0, next_state=[ 0.01831208  1.15058121 -0.08242678 -1.73792446]\n",
      "[ Experience replay ] starts\n",
      "[ episode 460 ][ timestamp 7 ] state=[ 0.01831208  1.15058121 -0.08242678 -1.73792446], action=1, reward=1.0, next_state=[ 0.04132371  1.34654012 -0.11718527 -2.05507091]\n",
      "[ Experience replay ] starts\n",
      "[ episode 460 ][ timestamp 8 ] state=[ 0.04132371  1.34654012 -0.11718527 -2.05507091], action=1, reward=1.0, next_state=[ 0.06825451  1.54264945 -0.15828669 -2.38159109]\n",
      "[ Experience replay ] starts\n",
      "[ episode 460 ][ timestamp 9 ] state=[ 0.06825451  1.54264945 -0.15828669 -2.38159109], action=1, reward=1.0, next_state=[ 0.0991075   1.73877493 -0.20591851 -2.71844381]\n",
      "[ Experience replay ] starts\n",
      "[ episode 460 ][ timestamp 10 ] state=[ 0.0991075   1.73877493 -0.20591851 -2.71844381], action=1, reward=-1.0, next_state=[ 0.133883    1.93469382 -0.26028738 -3.06622666]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 460: Exploration_rate=0.01. Score=10.\n",
      "[ episode 461 ] state=[-0.0189321  -0.00129765  0.03548872 -0.02527558]\n",
      "[ episode 461 ][ timestamp 1 ] state=[-0.0189321  -0.00129765  0.03548872 -0.02527558], action=1, reward=1.0, next_state=[-0.01895806  0.19329786  0.0349832  -0.30655356]\n",
      "[ Experience replay ] starts\n",
      "[ episode 461 ][ timestamp 2 ] state=[-0.01895806  0.19329786  0.0349832  -0.30655356], action=1, reward=1.0, next_state=[-0.0150921   0.3879043   0.02885213 -0.58800165]\n",
      "[ Experience replay ] starts\n",
      "[ episode 461 ][ timestamp 3 ] state=[-0.0150921   0.3879043   0.02885213 -0.58800165], action=1, reward=1.0, next_state=[-0.00733401  0.58261057  0.0170921  -0.87145816]\n",
      "[ Experience replay ] starts\n",
      "[ episode 461 ][ timestamp 4 ] state=[-0.00733401  0.58261057  0.0170921  -0.87145816], action=1, reward=1.0, next_state=[ 4.31819824e-03  7.77495943e-01 -3.37062356e-04 -1.15871868e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 461 ][ timestamp 5 ] state=[ 4.31819824e-03  7.77495943e-01 -3.37062356e-04 -1.15871868e+00], action=1, reward=1.0, next_state=[ 0.01986812  0.97262228 -0.02351144 -1.45150727]\n",
      "[ Experience replay ] starts\n",
      "[ episode 461 ][ timestamp 6 ] state=[ 0.01986812  0.97262228 -0.02351144 -1.45150727], action=1, reward=1.0, next_state=[ 0.03932056  1.16802508 -0.05254158 -1.75144217]\n",
      "[ Experience replay ] starts\n",
      "[ episode 461 ][ timestamp 7 ] state=[ 0.03932056  1.16802508 -0.05254158 -1.75144217], action=1, reward=1.0, next_state=[ 0.06268106  1.3637025  -0.08757042 -2.05999339]\n",
      "[ Experience replay ] starts\n",
      "[ episode 461 ][ timestamp 8 ] state=[ 0.06268106  1.3637025  -0.08757042 -2.05999339], action=1, reward=1.0, next_state=[ 0.08995511  1.55960221 -0.12877029 -2.37842977]\n",
      "[ Experience replay ] starts\n",
      "[ episode 461 ][ timestamp 9 ] state=[ 0.08995511  1.55960221 -0.12877029 -2.37842977], action=1, reward=1.0, next_state=[ 0.12114716  1.75560535 -0.17633889 -2.70775421]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 461 ][ timestamp 10 ] state=[ 0.12114716  1.75560535 -0.17633889 -2.70775421], action=1, reward=-1.0, next_state=[ 0.15625927  1.95150818 -0.23049397 -3.0486269 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 461: Exploration_rate=0.01. Score=10.\n",
      "[ episode 462 ] state=[-0.04173002  0.00806664  0.02601074  0.02639256]\n",
      "[ episode 462 ][ timestamp 1 ] state=[-0.04173002  0.00806664  0.02601074  0.02639256], action=1, reward=1.0, next_state=[-0.04156869  0.2028061   0.02653859 -0.25797154]\n",
      "[ Experience replay ] starts\n",
      "[ episode 462 ][ timestamp 2 ] state=[-0.04156869  0.2028061   0.02653859 -0.25797154], action=1, reward=1.0, next_state=[-0.03751256  0.39753932  0.02137916 -0.54216708]\n",
      "[ Experience replay ] starts\n",
      "[ episode 462 ][ timestamp 3 ] state=[-0.03751256  0.39753932  0.02137916 -0.54216708], action=1, reward=1.0, next_state=[-0.02956178  0.59235437  0.01053582 -0.82803788]\n",
      "[ Experience replay ] starts\n",
      "[ episode 462 ][ timestamp 4 ] state=[-0.02956178  0.59235437  0.01053582 -0.82803788], action=1, reward=1.0, next_state=[-0.01771469  0.7873307  -0.00602494 -1.11738867]\n",
      "[ Experience replay ] starts\n",
      "[ episode 462 ][ timestamp 5 ] state=[-0.01771469  0.7873307  -0.00602494 -1.11738867], action=1, reward=1.0, next_state=[-0.00196808  0.9825312  -0.02837271 -1.41195542]\n",
      "[ Experience replay ] starts\n",
      "[ episode 462 ][ timestamp 6 ] state=[-0.00196808  0.9825312  -0.02837271 -1.41195542], action=1, reward=1.0, next_state=[ 0.01768255  1.17799315 -0.05661182 -1.7133708 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 462 ][ timestamp 7 ] state=[ 0.01768255  1.17799315 -0.05661182 -1.7133708 ], action=1, reward=1.0, next_state=[ 0.04124241  1.37371736 -0.09087924 -2.02312178]\n",
      "[ Experience replay ] starts\n",
      "[ episode 462 ][ timestamp 8 ] state=[ 0.04124241  1.37371736 -0.09087924 -2.02312178], action=1, reward=1.0, next_state=[ 0.06871676  1.56965501 -0.13134167 -2.34249712]\n",
      "[ Experience replay ] starts\n",
      "[ episode 462 ][ timestamp 9 ] state=[ 0.06871676  1.56965501 -0.13134167 -2.34249712], action=1, reward=1.0, next_state=[ 0.10010986  1.76569187 -0.17819161 -2.67252326]\n",
      "[ Experience replay ] starts\n",
      "[ episode 462 ][ timestamp 10 ] state=[ 0.10010986  1.76569187 -0.17819161 -2.67252326], action=1, reward=-1.0, next_state=[ 0.1354237   1.96163005 -0.23164208 -3.01388829]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 462: Exploration_rate=0.01. Score=10.\n",
      "[ episode 463 ] state=[ 0.04557761  0.02238793 -0.01928822 -0.01410237]\n",
      "[ episode 463 ][ timestamp 1 ] state=[ 0.04557761  0.02238793 -0.01928822 -0.01410237], action=1, reward=1.0, next_state=[ 0.04602537  0.21778111 -0.01957027 -0.31280801]\n",
      "[ Experience replay ] starts\n",
      "[ episode 463 ][ timestamp 2 ] state=[ 0.04602537  0.21778111 -0.01957027 -0.31280801], action=1, reward=1.0, next_state=[ 0.05038099  0.41317631 -0.02582643 -0.61159799]\n",
      "[ Experience replay ] starts\n",
      "[ episode 463 ][ timestamp 3 ] state=[ 0.05038099  0.41317631 -0.02582643 -0.61159799], action=1, reward=1.0, next_state=[ 0.05864451  0.60864953 -0.03805839 -0.91230215]\n",
      "[ Experience replay ] starts\n",
      "[ episode 463 ][ timestamp 4 ] state=[ 0.05864451  0.60864953 -0.03805839 -0.91230215], action=1, reward=1.0, next_state=[ 0.0708175   0.80426515 -0.05630443 -1.21669957]\n",
      "[ Experience replay ] starts\n",
      "[ episode 463 ][ timestamp 5 ] state=[ 0.0708175   0.80426515 -0.05630443 -1.21669957], action=1, reward=1.0, next_state=[ 0.08690281  1.00006623 -0.08063843 -1.52648054]\n",
      "[ Experience replay ] starts\n",
      "[ episode 463 ][ timestamp 6 ] state=[ 0.08690281  1.00006623 -0.08063843 -1.52648054], action=1, reward=1.0, next_state=[ 0.10690413  1.19606348 -0.11116804 -1.84320308]\n",
      "[ Experience replay ] starts\n",
      "[ episode 463 ][ timestamp 7 ] state=[ 0.10690413  1.19606348 -0.11116804 -1.84320308], action=1, reward=1.0, next_state=[ 0.1308254   1.39222228 -0.1480321  -2.16824112]\n",
      "[ Experience replay ] starts\n",
      "[ episode 463 ][ timestamp 8 ] state=[ 0.1308254   1.39222228 -0.1480321  -2.16824112], action=1, reward=1.0, next_state=[ 0.15866985  1.58844752 -0.19139692 -2.50272254]\n",
      "[ Experience replay ] starts\n",
      "[ episode 463 ][ timestamp 9 ] state=[ 0.15866985  1.58844752 -0.19139692 -2.50272254], action=1, reward=-1.0, next_state=[ 0.1904388   1.78456607 -0.24145137 -2.84745628]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 463: Exploration_rate=0.01. Score=9.\n",
      "[ episode 464 ] state=[ 0.01383776  0.01933425 -0.03577043  0.04161407]\n",
      "[ episode 464 ][ timestamp 1 ] state=[ 0.01383776  0.01933425 -0.03577043  0.04161407], action=1, reward=1.0, next_state=[ 0.01422444  0.2149504  -0.03493815 -0.26213671]\n",
      "[ Experience replay ] starts\n",
      "[ episode 464 ][ timestamp 2 ] state=[ 0.01422444  0.2149504  -0.03493815 -0.26213671], action=1, reward=1.0, next_state=[ 0.01852345  0.4105532  -0.04018089 -0.56563158]\n",
      "[ Experience replay ] starts\n",
      "[ episode 464 ][ timestamp 3 ] state=[ 0.01852345  0.4105532  -0.04018089 -0.56563158], action=1, reward=1.0, next_state=[ 0.02673452  0.60621515 -0.05149352 -0.87069761]\n",
      "[ Experience replay ] starts\n",
      "[ episode 464 ][ timestamp 4 ] state=[ 0.02673452  0.60621515 -0.05149352 -0.87069761], action=1, reward=1.0, next_state=[ 0.03885882  0.80199826 -0.06890747 -1.17911542]\n",
      "[ Experience replay ] starts\n",
      "[ episode 464 ][ timestamp 5 ] state=[ 0.03885882  0.80199826 -0.06890747 -1.17911542], action=1, reward=1.0, next_state=[ 0.05489878  0.99794396 -0.09248978 -1.49257922]\n",
      "[ Experience replay ] starts\n",
      "[ episode 464 ][ timestamp 6 ] state=[ 0.05489878  0.99794396 -0.09248978 -1.49257922], action=1, reward=1.0, next_state=[ 0.07485766  1.19406166 -0.12234136 -1.81265165]\n",
      "[ Experience replay ] starts\n",
      "[ episode 464 ][ timestamp 7 ] state=[ 0.07485766  1.19406166 -0.12234136 -1.81265165], action=1, reward=1.0, next_state=[ 0.0987389   1.39031565 -0.1585944  -2.14071103]\n",
      "[ Experience replay ] starts\n",
      "[ episode 464 ][ timestamp 8 ] state=[ 0.0987389   1.39031565 -0.1585944  -2.14071103], action=1, reward=1.0, next_state=[ 0.12654521  1.5866098  -0.20140862 -2.47788863]\n",
      "[ Experience replay ] starts\n",
      "[ episode 464 ][ timestamp 9 ] state=[ 0.12654521  1.5866098  -0.20140862 -2.47788863], action=1, reward=-1.0, next_state=[ 0.15827741  1.78277002 -0.25096639 -2.82499571]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 464: Exploration_rate=0.01. Score=9.\n",
      "[ episode 465 ] state=[0.04229299 0.02012182 0.01816137 0.01283357]\n",
      "[ episode 465 ][ timestamp 1 ] state=[0.04229299 0.02012182 0.01816137 0.01283357], action=1, reward=1.0, next_state=[ 0.04269542  0.21497867  0.01841805 -0.27406435]\n",
      "[ Experience replay ] starts\n",
      "[ episode 465 ][ timestamp 2 ] state=[ 0.04269542  0.21497867  0.01841805 -0.27406435], action=1, reward=1.0, next_state=[ 0.046995    0.40983305  0.01293676 -0.56088176]\n",
      "[ Experience replay ] starts\n",
      "[ episode 465 ][ timestamp 3 ] state=[ 0.046995    0.40983305  0.01293676 -0.56088176], action=1, reward=1.0, next_state=[ 0.05519166  0.60477108  0.00171912 -0.84946102]\n",
      "[ Experience replay ] starts\n",
      "[ episode 465 ][ timestamp 4 ] state=[ 0.05519166  0.60477108  0.00171912 -0.84946102], action=1, reward=1.0, next_state=[ 0.06728708  0.79986954 -0.0152701  -1.14160286]\n",
      "[ Experience replay ] starts\n",
      "[ episode 465 ][ timestamp 5 ] state=[ 0.06728708  0.79986954 -0.0152701  -1.14160286], action=1, reward=1.0, next_state=[ 0.08328447  0.99518771 -0.03810215 -1.43903519]\n",
      "[ Experience replay ] starts\n",
      "[ episode 465 ][ timestamp 6 ] state=[ 0.08328447  0.99518771 -0.03810215 -1.43903519], action=1, reward=1.0, next_state=[ 0.10318822  1.19075785 -0.06688286 -1.7433768 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 465 ][ timestamp 7 ] state=[ 0.10318822  1.19075785 -0.06688286 -1.7433768 ], action=1, reward=1.0, next_state=[ 0.12700338  1.38657396 -0.10175039 -2.05609317]\n",
      "[ Experience replay ] starts\n",
      "[ episode 465 ][ timestamp 8 ] state=[ 0.12700338  1.38657396 -0.10175039 -2.05609317], action=1, reward=1.0, next_state=[ 0.15473486  1.58257821 -0.14287226 -2.37844192]\n",
      "[ Experience replay ] starts\n",
      "[ episode 465 ][ timestamp 9 ] state=[ 0.15473486  1.58257821 -0.14287226 -2.37844192], action=1, reward=1.0, next_state=[ 0.18638642  1.77864469 -0.1904411  -2.71140679]\n",
      "[ Experience replay ] starts\n",
      "[ episode 465 ][ timestamp 10 ] state=[ 0.18638642  1.77864469 -0.1904411  -2.71140679], action=1, reward=-1.0, next_state=[ 0.22195932  1.97456095 -0.24466923 -3.05562003]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 465: Exploration_rate=0.01. Score=10.\n",
      "[ episode 466 ] state=[-0.03239982 -0.01327381  0.02033378  0.02580358]\n",
      "[ episode 466 ][ timestamp 1 ] state=[-0.03239982 -0.01327381  0.02033378  0.02580358], action=1, reward=1.0, next_state=[-0.0326653   0.18155073  0.02084986 -0.26039509]\n",
      "[ Experience replay ] starts\n",
      "[ episode 466 ][ timestamp 2 ] state=[-0.0326653   0.18155073  0.02084986 -0.26039509], action=1, reward=1.0, next_state=[-0.02903428  0.37636893  0.01564195 -0.54642947]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 466 ][ timestamp 3 ] state=[-0.02903428  0.37636893  0.01564195 -0.54642947], action=1, reward=1.0, next_state=[-0.0215069   0.57126766  0.00471337 -0.83414325]\n",
      "[ Experience replay ] starts\n",
      "[ episode 466 ][ timestamp 4 ] state=[-0.0215069   0.57126766  0.00471337 -0.83414325], action=1, reward=1.0, next_state=[-0.01008155  0.7663249  -0.0119695  -1.12534013]\n",
      "[ Experience replay ] starts\n",
      "[ episode 466 ][ timestamp 5 ] state=[-0.01008155  0.7663249  -0.0119695  -1.12534013], action=1, reward=1.0, next_state=[ 0.00524495  0.96160166 -0.0344763  -1.42175324]\n",
      "[ Experience replay ] starts\n",
      "[ episode 466 ][ timestamp 6 ] state=[ 0.00524495  0.96160166 -0.0344763  -1.42175324], action=1, reward=1.0, next_state=[ 0.02447698  1.15713268 -0.06291137 -1.72500951]\n",
      "[ Experience replay ] starts\n",
      "[ episode 466 ][ timestamp 7 ] state=[ 0.02447698  1.15713268 -0.06291137 -1.72500951], action=1, reward=1.0, next_state=[ 0.04761964  1.35291536 -0.09741156 -2.0365863 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 466 ][ timestamp 8 ] state=[ 0.04761964  1.35291536 -0.09741156 -2.0365863 ], action=1, reward=1.0, next_state=[ 0.07467794  1.54889631 -0.13814328 -2.35775781]\n",
      "[ Experience replay ] starts\n",
      "[ episode 466 ][ timestamp 9 ] state=[ 0.07467794  1.54889631 -0.13814328 -2.35775781], action=1, reward=1.0, next_state=[ 0.10565587  1.74495547 -0.18529844 -2.68952995]\n",
      "[ Experience replay ] starts\n",
      "[ episode 466 ][ timestamp 10 ] state=[ 0.10565587  1.74495547 -0.18529844 -2.68952995], action=1, reward=-1.0, next_state=[ 0.14055498  1.94088761 -0.23908904 -3.03256352]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 466: Exploration_rate=0.01. Score=10.\n",
      "[ episode 467 ] state=[-0.00520935 -0.02247635  0.03905072 -0.03697431]\n",
      "[ episode 467 ][ timestamp 1 ] state=[-0.00520935 -0.02247635  0.03905072 -0.03697431], action=1, reward=1.0, next_state=[-0.00565888  0.17206448  0.03831124 -0.31708509]\n",
      "[ Experience replay ] starts\n",
      "[ episode 467 ][ timestamp 2 ] state=[-0.00565888  0.17206448  0.03831124 -0.31708509], action=1, reward=1.0, next_state=[-0.00221759  0.3666204   0.03196953 -0.59744408]\n",
      "[ Experience replay ] starts\n",
      "[ episode 467 ][ timestamp 3 ] state=[-0.00221759  0.3666204   0.03196953 -0.59744408], action=1, reward=1.0, next_state=[ 0.00511482  0.56128076  0.02002065 -0.87988797]\n",
      "[ Experience replay ] starts\n",
      "[ episode 467 ][ timestamp 4 ] state=[ 0.00511482  0.56128076  0.02002065 -0.87988797], action=1, reward=1.0, next_state=[ 0.01634043  0.75612507  0.00242289 -1.16621018]\n",
      "[ Experience replay ] starts\n",
      "[ episode 467 ][ timestamp 5 ] state=[ 0.01634043  0.75612507  0.00242289 -1.16621018], action=1, reward=1.0, next_state=[ 0.03146293  0.9512154  -0.02090131 -1.45813249]\n",
      "[ Experience replay ] starts\n",
      "[ episode 467 ][ timestamp 6 ] state=[ 0.03146293  0.9512154  -0.02090131 -1.45813249], action=1, reward=1.0, next_state=[ 0.05048724  1.14658742 -0.05006396 -1.75727105]\n",
      "[ Experience replay ] starts\n",
      "[ episode 467 ][ timestamp 7 ] state=[ 0.05048724  1.14658742 -0.05006396 -1.75727105], action=1, reward=1.0, next_state=[ 0.07341899  1.34223955 -0.08520938 -2.06509419]\n",
      "[ Experience replay ] starts\n",
      "[ episode 467 ][ timestamp 8 ] state=[ 0.07341899  1.34223955 -0.08520938 -2.06509419], action=1, reward=1.0, next_state=[ 0.10026378  1.53811972 -0.12651126 -2.38286968]\n",
      "[ Experience replay ] starts\n",
      "[ episode 467 ][ timestamp 9 ] state=[ 0.10026378  1.53811972 -0.12651126 -2.38286968], action=1, reward=1.0, next_state=[ 0.13102618  1.73410947 -0.17416866 -2.7116    ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 467 ][ timestamp 10 ] state=[ 0.13102618  1.73410947 -0.17416866 -2.7116    ], action=1, reward=-1.0, next_state=[ 0.16570837  1.93000553 -0.22840066 -3.0519456 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 467: Exploration_rate=0.01. Score=10.\n",
      "[ episode 468 ] state=[-0.03349611  0.00271707 -0.01041745 -0.01814043]\n",
      "[ episode 468 ][ timestamp 1 ] state=[-0.03349611  0.00271707 -0.01041745 -0.01814043], action=1, reward=1.0, next_state=[-0.03344177  0.19798686 -0.01078026 -0.31409189]\n",
      "[ Experience replay ] starts\n",
      "[ episode 468 ][ timestamp 2 ] state=[-0.03344177  0.19798686 -0.01078026 -0.31409189], action=1, reward=1.0, next_state=[-0.02948203  0.3932607  -0.0170621  -0.61015498]\n",
      "[ Experience replay ] starts\n",
      "[ episode 468 ][ timestamp 3 ] state=[-0.02948203  0.3932607  -0.0170621  -0.61015498], action=1, reward=1.0, next_state=[-0.02161682  0.58861694 -0.0292652  -0.9081627 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 468 ][ timestamp 4 ] state=[-0.02161682  0.58861694 -0.0292652  -0.9081627 ], action=1, reward=1.0, next_state=[-0.00984448  0.78412257 -0.04742845 -1.2098983 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 468 ][ timestamp 5 ] state=[-0.00984448  0.78412257 -0.04742845 -1.2098983 ], action=1, reward=1.0, next_state=[ 0.00583797  0.9798238  -0.07162642 -1.51705878]\n",
      "[ Experience replay ] starts\n",
      "[ episode 468 ][ timestamp 6 ] state=[ 0.00583797  0.9798238  -0.07162642 -1.51705878], action=1, reward=1.0, next_state=[ 0.02543445  1.17573536 -0.10196759 -1.83121279]\n",
      "[ Experience replay ] starts\n",
      "[ episode 468 ][ timestamp 7 ] state=[ 0.02543445  1.17573536 -0.10196759 -1.83121279], action=1, reward=1.0, next_state=[ 0.04894916  1.37182788 -0.13859185 -2.15375031]\n",
      "[ Experience replay ] starts\n",
      "[ episode 468 ][ timestamp 8 ] state=[ 0.04894916  1.37182788 -0.13859185 -2.15375031], action=1, reward=1.0, next_state=[ 0.07638572  1.56801293 -0.18166686 -2.48582189]\n",
      "[ Experience replay ] starts\n",
      "[ episode 468 ][ timestamp 9 ] state=[ 0.07638572  1.56801293 -0.18166686 -2.48582189], action=1, reward=-1.0, next_state=[ 0.10774597  1.76412573 -0.23138329 -2.82826697]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 468: Exploration_rate=0.01. Score=9.\n",
      "[ episode 469 ] state=[ 0.01310594 -0.01944706 -0.00068433 -0.02857793]\n",
      "[ episode 469 ][ timestamp 1 ] state=[ 0.01310594 -0.01944706 -0.00068433 -0.02857793], action=1, reward=1.0, next_state=[ 0.012717    0.1756847  -0.00125589 -0.32147669]\n",
      "[ Experience replay ] starts\n",
      "[ episode 469 ][ timestamp 2 ] state=[ 0.012717    0.1756847  -0.00125589 -0.32147669], action=1, reward=1.0, next_state=[ 0.01623069  0.37082451 -0.00768542 -0.61455541]\n",
      "[ Experience replay ] starts\n",
      "[ episode 469 ][ timestamp 3 ] state=[ 0.01623069  0.37082451 -0.00768542 -0.61455541], action=1, reward=1.0, next_state=[ 0.02364718  0.56605301 -0.01997653 -0.90964899]\n",
      "[ Experience replay ] starts\n",
      "[ episode 469 ][ timestamp 4 ] state=[ 0.02364718  0.56605301 -0.01997653 -0.90964899], action=1, reward=1.0, next_state=[ 0.03496824  0.76143954 -0.03816951 -1.20854302]\n",
      "[ Experience replay ] starts\n",
      "[ episode 469 ][ timestamp 5 ] state=[ 0.03496824  0.76143954 -0.03816951 -1.20854302], action=1, reward=1.0, next_state=[ 0.05019704  0.95703315 -0.06234037 -1.51293885]\n",
      "[ Experience replay ] starts\n",
      "[ episode 469 ][ timestamp 6 ] state=[ 0.05019704  0.95703315 -0.06234037 -1.51293885], action=1, reward=1.0, next_state=[ 0.0693377   1.1528521  -0.09259914 -1.8244129 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 469 ][ timestamp 7 ] state=[ 0.0693377   1.1528521  -0.09259914 -1.8244129 ], action=1, reward=1.0, next_state=[ 0.09239474  1.34887158 -0.1290874  -2.14436769]\n",
      "[ Experience replay ] starts\n",
      "[ episode 469 ][ timestamp 8 ] state=[ 0.09239474  1.34887158 -0.1290874  -2.14436769], action=1, reward=1.0, next_state=[ 0.11937217  1.54500902 -0.17197476 -2.47397236]\n",
      "[ Experience replay ] starts\n",
      "[ episode 469 ][ timestamp 9 ] state=[ 0.11937217  1.54500902 -0.17197476 -2.47397236], action=1, reward=-1.0, next_state=[ 0.15027235  1.74110695 -0.2214542  -2.81409195]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 469: Exploration_rate=0.01. Score=9.\n",
      "[ episode 470 ] state=[-0.04613994 -0.03716912 -0.01647214 -0.0234245 ]\n",
      "[ episode 470 ][ timestamp 1 ] state=[-0.04613994 -0.03716912 -0.01647214 -0.0234245 ], action=1, reward=1.0, next_state=[-0.04688332  0.15818514 -0.01694063 -0.32125872]\n",
      "[ Experience replay ] starts\n",
      "[ episode 470 ][ timestamp 2 ] state=[-0.04688332  0.15818514 -0.01694063 -0.32125872], action=1, reward=1.0, next_state=[-0.04371962  0.35354419 -0.0233658  -0.61923555]\n",
      "[ Experience replay ] starts\n",
      "[ episode 470 ][ timestamp 3 ] state=[-0.04371962  0.35354419 -0.0233658  -0.61923555], action=1, reward=1.0, next_state=[-0.03664873  0.54898457 -0.03575051 -0.91918502]\n",
      "[ Experience replay ] starts\n",
      "[ episode 470 ][ timestamp 4 ] state=[-0.03664873  0.54898457 -0.03575051 -0.91918502], action=1, reward=1.0, next_state=[-0.02566904  0.74457106 -0.05413421 -1.22288569]\n",
      "[ Experience replay ] starts\n",
      "[ episode 470 ][ timestamp 5 ] state=[-0.02566904  0.74457106 -0.05413421 -1.22288569], action=1, reward=1.0, next_state=[-0.01077762  0.94034698 -0.07859193 -1.53202707]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 470 ][ timestamp 6 ] state=[-0.01077762  0.94034698 -0.07859193 -1.53202707], action=1, reward=1.0, next_state=[ 0.00802932  1.13632325 -0.10923247 -1.84816632]\n",
      "[ Experience replay ] starts\n",
      "[ episode 470 ][ timestamp 7 ] state=[ 0.00802932  1.13632325 -0.10923247 -1.84816632], action=1, reward=1.0, next_state=[ 0.03075578  1.33246547 -0.14619579 -2.17267669]\n",
      "[ Experience replay ] starts\n",
      "[ episode 470 ][ timestamp 8 ] state=[ 0.03075578  1.33246547 -0.14619579 -2.17267669], action=1, reward=1.0, next_state=[ 0.05740509  1.52867877 -0.18964933 -2.50668557]\n",
      "[ Experience replay ] starts\n",
      "[ episode 470 ][ timestamp 9 ] state=[ 0.05740509  1.52867877 -0.18964933 -2.50668557], action=1, reward=-1.0, next_state=[ 0.08797867  1.72479023 -0.23978304 -2.85100174]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 470: Exploration_rate=0.01. Score=9.\n",
      "[ episode 471 ] state=[-0.03427269 -0.02845209 -0.0185573  -0.01794599]\n",
      "[ episode 471 ][ timestamp 1 ] state=[-0.03427269 -0.02845209 -0.0185573  -0.01794599], action=1, reward=1.0, next_state=[-0.03484173  0.16693101 -0.01891622 -0.31642571]\n",
      "[ Experience replay ] starts\n",
      "[ episode 471 ][ timestamp 2 ] state=[-0.03484173  0.16693101 -0.01891622 -0.31642571], action=1, reward=1.0, next_state=[-0.03150311  0.36231722 -0.02524473 -0.61501363]\n",
      "[ Experience replay ] starts\n",
      "[ episode 471 ][ timestamp 3 ] state=[-0.03150311  0.36231722 -0.02524473 -0.61501363], action=1, reward=1.0, next_state=[-0.02425676  0.55778263 -0.037545   -0.9155395 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 471 ][ timestamp 4 ] state=[-0.02425676  0.55778263 -0.037545   -0.9155395 ], action=1, reward=1.0, next_state=[-0.01310111  0.75339167 -0.05585579 -1.21978191]\n",
      "[ Experience replay ] starts\n",
      "[ episode 471 ][ timestamp 5 ] state=[-0.01310111  0.75339167 -0.05585579 -1.21978191], action=1, reward=1.0, next_state=[ 0.00196672  0.94918732 -0.08025143 -1.52943043]\n",
      "[ Experience replay ] starts\n",
      "[ episode 471 ][ timestamp 6 ] state=[ 0.00196672  0.94918732 -0.08025143 -1.52943043], action=1, reward=1.0, next_state=[ 0.02095047  1.14518015 -0.11084004 -1.8460421 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 471 ][ timestamp 7 ] state=[ 0.02095047  1.14518015 -0.11084004 -1.8460421 ], action=1, reward=1.0, next_state=[ 0.04385407  1.34133535 -0.14776088 -2.17098964]\n",
      "[ Experience replay ] starts\n",
      "[ episode 471 ][ timestamp 8 ] state=[ 0.04385407  1.34133535 -0.14776088 -2.17098964], action=1, reward=1.0, next_state=[ 0.07068078  1.53755753 -0.19118068 -2.50539941]\n",
      "[ Experience replay ] starts\n",
      "[ episode 471 ][ timestamp 9 ] state=[ 0.07068078  1.53755753 -0.19118068 -2.50539941], action=1, reward=-1.0, next_state=[ 0.10143193  1.73367318 -0.24128866 -2.85007856]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 471: Exploration_rate=0.01. Score=9.\n",
      "[ episode 472 ] state=[ 0.04527834  0.03373745  0.04413948 -0.03222035]\n",
      "[ episode 472 ][ timestamp 1 ] state=[ 0.04527834  0.03373745  0.04413948 -0.03222035], action=1, reward=1.0, next_state=[ 0.04595309  0.22819954  0.04349507 -0.31065659]\n",
      "[ Experience replay ] starts\n",
      "[ episode 472 ][ timestamp 2 ] state=[ 0.04595309  0.22819954  0.04349507 -0.31065659], action=1, reward=1.0, next_state=[ 0.05051708  0.42267568  0.03728194 -0.58931139]\n",
      "[ Experience replay ] starts\n",
      "[ episode 472 ][ timestamp 3 ] state=[ 0.05051708  0.42267568  0.03728194 -0.58931139], action=1, reward=1.0, next_state=[ 0.0589706   0.6172563   0.02549571 -0.87002115]\n",
      "[ Experience replay ] starts\n",
      "[ episode 472 ][ timestamp 4 ] state=[ 0.0589706   0.6172563   0.02549571 -0.87002115], action=1, reward=1.0, next_state=[ 0.07131572  0.81202233  0.00809529 -1.15458032]\n",
      "[ Experience replay ] starts\n",
      "[ episode 472 ][ timestamp 5 ] state=[ 0.07131572  0.81202233  0.00809529 -1.15458032], action=1, reward=1.0, next_state=[ 0.08755617  1.00703778 -0.01499632 -1.44471392]\n",
      "[ Experience replay ] starts\n",
      "[ episode 472 ][ timestamp 6 ] state=[ 0.08755617  1.00703778 -0.01499632 -1.44471392], action=1, reward=1.0, next_state=[ 0.10769692  1.20234102 -0.0438906  -1.74204459]\n",
      "[ Experience replay ] starts\n",
      "[ episode 472 ][ timestamp 7 ] state=[ 0.10769692  1.20234102 -0.0438906  -1.74204459], action=1, reward=1.0, next_state=[ 0.13174375  1.39793416 -0.07873149 -2.04805145]\n",
      "[ Experience replay ] starts\n",
      "[ episode 472 ][ timestamp 8 ] state=[ 0.13174375  1.39793416 -0.07873149 -2.04805145], action=1, reward=1.0, next_state=[ 0.15970243  1.59377008 -0.11969252 -2.36401852]\n",
      "[ Experience replay ] starts\n",
      "[ episode 472 ][ timestamp 9 ] state=[ 0.15970243  1.59377008 -0.11969252 -2.36401852], action=0, reward=1.0, next_state=[ 0.19157783  1.39989957 -0.16697289 -2.11039899]\n",
      "[ Experience replay ] starts\n",
      "[ episode 472 ][ timestamp 10 ] state=[ 0.19157783  1.39989957 -0.16697289 -2.11039899], action=0, reward=1.0, next_state=[ 0.21957582  1.20679612 -0.20918087 -1.87363447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 472 ][ timestamp 11 ] state=[ 0.21957582  1.20679612 -0.20918087 -1.87363447], action=0, reward=-1.0, next_state=[ 0.24371174  1.01448297 -0.24665356 -1.65250466]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 472: Exploration_rate=0.01. Score=11.\n",
      "[ episode 473 ] state=[-0.00973007 -0.02843988  0.00461624 -0.02979333]\n",
      "[ episode 473 ][ timestamp 1 ] state=[-0.00973007 -0.02843988  0.00461624 -0.02979333], action=1, reward=1.0, next_state=[-0.01029887  0.16661557  0.00402038 -0.32101621]\n",
      "[ Experience replay ] starts\n",
      "[ episode 473 ][ timestamp 2 ] state=[-0.01029887  0.16661557  0.00402038 -0.32101621], action=1, reward=1.0, next_state=[-0.00696656  0.36168004 -0.00239995 -0.61242856]\n",
      "[ Experience replay ] starts\n",
      "[ episode 473 ][ timestamp 3 ] state=[-0.00696656  0.36168004 -0.00239995 -0.61242856], action=1, reward=1.0, next_state=[ 2.67041781e-04  5.56835450e-01 -1.46485205e-02 -9.05866417e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 473 ][ timestamp 4 ] state=[ 2.67041781e-04  5.56835450e-01 -1.46485205e-02 -9.05866417e-01], action=1, reward=1.0, next_state=[ 0.01140375  0.75215266 -0.03276585 -1.20311731]\n",
      "[ Experience replay ] starts\n",
      "[ episode 473 ][ timestamp 5 ] state=[ 0.01140375  0.75215266 -0.03276585 -1.20311731], action=1, reward=1.0, next_state=[ 0.0264468   0.94768257 -0.05682819 -1.50588618]\n",
      "[ Experience replay ] starts\n",
      "[ episode 473 ][ timestamp 6 ] state=[ 0.0264468   0.94768257 -0.05682819 -1.50588618], action=0, reward=1.0, next_state=[ 0.04540046  0.75329409 -0.08694592 -1.23147266]\n",
      "[ Experience replay ] starts\n",
      "[ episode 473 ][ timestamp 7 ] state=[ 0.04540046  0.75329409 -0.08694592 -1.23147266], action=1, reward=1.0, next_state=[ 0.06046634  0.94942001 -0.11157537 -1.55008017]\n",
      "[ Experience replay ] starts\n",
      "[ episode 473 ][ timestamp 8 ] state=[ 0.06046634  0.94942001 -0.11157537 -1.55008017], action=0, reward=1.0, next_state=[ 0.07945474  0.75579959 -0.14257698 -1.29419061]\n",
      "[ Experience replay ] starts\n",
      "[ episode 473 ][ timestamp 9 ] state=[ 0.07945474  0.75579959 -0.14257698 -1.29419061], action=0, reward=1.0, next_state=[ 0.09457073  0.56274782 -0.16846079 -1.04932702]\n",
      "[ Experience replay ] starts\n",
      "[ episode 473 ][ timestamp 10 ] state=[ 0.09457073  0.56274782 -0.16846079 -1.04932702], action=1, reward=1.0, next_state=[ 0.10582569  0.75965516 -0.18944733 -1.38980045]\n",
      "[ Experience replay ] starts\n",
      "[ episode 473 ][ timestamp 11 ] state=[ 0.10582569  0.75965516 -0.18944733 -1.38980045], action=0, reward=-1.0, next_state=[ 0.12101879  0.56732981 -0.21724334 -1.16183885]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 473: Exploration_rate=0.01. Score=11.\n",
      "[ episode 474 ] state=[-0.03225057  0.03870376  0.03148297  0.01922168]\n",
      "[ episode 474 ][ timestamp 1 ] state=[-0.03225057  0.03870376  0.03148297  0.01922168], action=1, reward=1.0, next_state=[-0.0314765   0.23336039  0.0318674  -0.26336412]\n",
      "[ Experience replay ] starts\n",
      "[ episode 474 ][ timestamp 2 ] state=[-0.0314765   0.23336039  0.0318674  -0.26336412], action=1, reward=1.0, next_state=[-0.02680929  0.42801332  0.02660012 -0.54582784]\n",
      "[ Experience replay ] starts\n",
      "[ episode 474 ][ timestamp 3 ] state=[-0.02680929  0.42801332  0.02660012 -0.54582784], action=1, reward=1.0, next_state=[-0.01824902  0.62275162  0.01568356 -0.83001243]\n",
      "[ Experience replay ] starts\n",
      "[ episode 474 ][ timestamp 4 ] state=[-0.01824902  0.62275162  0.01568356 -0.83001243], action=1, reward=1.0, next_state=[-5.79398939e-03  8.17655718e-01 -9.16685025e-04 -1.11772184e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 474 ][ timestamp 5 ] state=[-5.79398939e-03  8.17655718e-01 -9.16685025e-04 -1.11772184e+00], action=0, reward=1.0, next_state=[ 0.01055912  0.62254581 -0.02327112 -0.82532661]\n",
      "[ Experience replay ] starts\n",
      "[ episode 474 ][ timestamp 6 ] state=[ 0.01055912  0.62254581 -0.02327112 -0.82532661], action=1, reward=1.0, next_state=[ 0.02301004  0.81797817 -0.03977765 -1.12523688]\n",
      "[ Experience replay ] starts\n",
      "[ episode 474 ][ timestamp 7 ] state=[ 0.02301004  0.81797817 -0.03977765 -1.12523688], action=0, reward=1.0, next_state=[ 0.0393696   0.62339948 -0.06228239 -0.84529126]\n",
      "[ Experience replay ] starts\n",
      "[ episode 474 ][ timestamp 8 ] state=[ 0.0393696   0.62339948 -0.06228239 -0.84529126], action=0, reward=1.0, next_state=[ 0.05183759  0.42918011 -0.07918822 -0.57282626]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 474 ][ timestamp 9 ] state=[ 0.05183759  0.42918011 -0.07918822 -0.57282626], action=1, reward=1.0, next_state=[ 0.0604212   0.62531787 -0.09064474 -0.88936793]\n",
      "[ Experience replay ] starts\n",
      "[ episode 474 ][ timestamp 10 ] state=[ 0.0604212   0.62531787 -0.09064474 -0.88936793], action=0, reward=1.0, next_state=[ 0.07292755  0.43153511 -0.1084321  -0.62650022]\n",
      "[ Experience replay ] starts\n",
      "[ episode 474 ][ timestamp 11 ] state=[ 0.07292755  0.43153511 -0.1084321  -0.62650022], action=1, reward=1.0, next_state=[ 0.08155826  0.62799019 -0.1209621  -0.95126876]\n",
      "[ Experience replay ] starts\n",
      "[ episode 474 ][ timestamp 12 ] state=[ 0.08155826  0.62799019 -0.1209621  -0.95126876], action=0, reward=1.0, next_state=[ 0.09411806  0.43468556 -0.13998748 -0.69890673]\n",
      "[ Experience replay ] starts\n",
      "[ episode 474 ][ timestamp 13 ] state=[ 0.09411806  0.43468556 -0.13998748 -0.69890673], action=0, reward=1.0, next_state=[ 0.10281177  0.24175345 -0.15396561 -0.45336157]\n",
      "[ Experience replay ] starts\n",
      "[ episode 474 ][ timestamp 14 ] state=[ 0.10281177  0.24175345 -0.15396561 -0.45336157], action=1, reward=1.0, next_state=[ 0.10764684  0.43867913 -0.16303285 -0.7903431 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 474 ][ timestamp 15 ] state=[ 0.10764684  0.43867913 -0.16303285 -0.7903431 ], action=0, reward=1.0, next_state=[ 0.11642042  0.24612636 -0.17883971 -0.55306356]\n",
      "[ Experience replay ] starts\n",
      "[ episode 474 ][ timestamp 16 ] state=[ 0.11642042  0.24612636 -0.17883971 -0.55306356], action=0, reward=1.0, next_state=[ 0.12134295  0.05390688 -0.18990098 -0.32163201]\n",
      "[ Experience replay ] starts\n",
      "[ episode 474 ][ timestamp 17 ] state=[ 0.12134295  0.05390688 -0.18990098 -0.32163201], action=1, reward=1.0, next_state=[ 0.12242109  0.25115398 -0.19633362 -0.6676797 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 474 ][ timestamp 18 ] state=[ 0.12242109  0.25115398 -0.19633362 -0.6676797 ], action=0, reward=-1.0, next_state=[ 0.12744417  0.05922551 -0.20968721 -0.44266985]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 474: Exploration_rate=0.01. Score=18.\n",
      "[ episode 475 ] state=[-0.02655221  0.01193835 -0.00471651 -0.01277468]\n",
      "[ episode 475 ][ timestamp 1 ] state=[-0.02655221  0.01193835 -0.00471651 -0.01277468], action=1, reward=1.0, next_state=[-0.02631344  0.20712763 -0.00497201 -0.30694198]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 2 ] state=[-0.02631344  0.20712763 -0.00497201 -0.30694198], action=1, reward=1.0, next_state=[-0.02217089  0.40232007 -0.01111085 -0.6011888 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 3 ] state=[-0.02217089  0.40232007 -0.01111085 -0.6011888 ], action=0, reward=1.0, next_state=[-0.01412449  0.2073553  -0.02313462 -0.31202621]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 4 ] state=[-0.01412449  0.2073553  -0.02313462 -0.31202621], action=0, reward=1.0, next_state=[-0.00997738  0.01257044 -0.02937515 -0.02672808]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 5 ] state=[-0.00997738  0.01257044 -0.02937515 -0.02672808], action=1, reward=1.0, next_state=[-0.00972597  0.20810107 -0.02990971 -0.32853254]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 6 ] state=[-0.00972597  0.20810107 -0.02990971 -0.32853254], action=0, reward=1.0, next_state=[-0.00556395  0.0134174  -0.03648036 -0.0454298 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 7 ] state=[-0.00556395  0.0134174  -0.03648036 -0.0454298 ], action=1, reward=1.0, next_state=[-0.0052956   0.20904296 -0.03738896 -0.34939574]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 8 ] state=[-0.0052956   0.20904296 -0.03738896 -0.34939574], action=0, reward=1.0, next_state=[-0.00111474  0.01447216 -0.04437687 -0.06873331]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 9 ] state=[-0.00111474  0.01447216 -0.04437687 -0.06873331], action=1, reward=1.0, next_state=[-0.0008253   0.21020131 -0.04575154 -0.37508052]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 10 ] state=[-0.0008253   0.21020131 -0.04575154 -0.37508052], action=0, reward=1.0, next_state=[ 0.00337873  0.01575808 -0.05325315 -0.09716713]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 11 ] state=[ 0.00337873  0.01575808 -0.05325315 -0.09716713], action=0, reward=1.0, next_state=[ 0.00369389 -0.1785618  -0.05519649  0.17825045]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 12 ] state=[ 0.00369389 -0.1785618  -0.05519649  0.17825045], action=1, reward=1.0, next_state=[ 1.22651301e-04  1.73048162e-02 -5.16314813e-02 -1.31321560e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 13 ] state=[ 1.22651301e-04  1.73048162e-02 -5.16314813e-02 -1.31321560e-01], action=0, reward=1.0, next_state=[ 0.00046875 -0.17704097 -0.05425791  0.14463573]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 14 ] state=[ 0.00046875 -0.17704097 -0.05425791  0.14463573], action=1, reward=1.0, next_state=[-0.00307207  0.01881434 -0.0513652  -0.1646589 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 15 ] state=[-0.00307207  0.01881434 -0.0513652  -0.1646589 ], action=0, reward=1.0, next_state=[-0.00269579 -0.17553613 -0.05465838  0.11138757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 16 ] state=[-0.00269579 -0.17553613 -0.05465838  0.11138757], action=1, reward=1.0, next_state=[-0.00620651  0.02032471 -0.05243062 -0.1980265 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 17 ] state=[-0.00620651  0.02032471 -0.05243062 -0.1980265 ], action=0, reward=1.0, next_state=[-0.00580001 -0.17400964 -0.05639115  0.07766691]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 18 ] state=[-0.00580001 -0.17400964 -0.05639115  0.07766691], action=1, reward=1.0, next_state=[-0.00928021  0.02187347 -0.05483782 -0.23226092]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 19 ] state=[-0.00928021  0.02187347 -0.05483782 -0.23226092], action=0, reward=1.0, next_state=[-0.00884274 -0.17242378 -0.05948303  0.04263261]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 20 ] state=[-0.00884274 -0.17242378 -0.05948303  0.04263261], action=0, reward=1.0, next_state=[-0.01229121 -0.36664455 -0.05863038  0.31597082]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 21 ] state=[-0.01229121 -0.36664455 -0.05863038  0.31597082], action=1, reward=1.0, next_state=[-0.0196241  -0.1707386  -0.05231097  0.00538937]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 22 ] state=[-0.0196241  -0.1707386  -0.05231097  0.00538937], action=0, reward=1.0, next_state=[-0.02303888 -0.36507283 -0.05220318  0.28111956]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 23 ] state=[-0.02303888 -0.36507283 -0.05220318  0.28111956], action=1, reward=1.0, next_state=[-0.03034033 -0.16924661 -0.04658079 -0.02756039]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 24 ] state=[-0.03034033 -0.16924661 -0.04658079 -0.02756039], action=0, reward=1.0, next_state=[-0.03372526 -0.36367068 -0.047132    0.25006958]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 25 ] state=[-0.03372526 -0.36367068 -0.047132    0.25006958], action=1, reward=1.0, next_state=[-0.04099868 -0.16790846 -0.0421306  -0.05709934]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 26 ] state=[-0.04099868 -0.16790846 -0.0421306  -0.05709934], action=0, reward=1.0, next_state=[-0.04435685 -0.3624018  -0.04327259  0.22199906]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 27 ] state=[-0.04435685 -0.3624018  -0.04327259  0.22199906], action=1, reward=1.0, next_state=[-0.05160488 -0.16668891 -0.03883261 -0.08401363]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 28 ] state=[-0.05160488 -0.16668891 -0.03883261 -0.08401363], action=0, reward=1.0, next_state=[-0.05493866 -0.36123331 -0.04051288  0.19616906]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 29 ] state=[-0.05493866 -0.36123331 -0.04051288  0.19616906], action=1, reward=1.0, next_state=[-0.06216333 -0.16555599 -0.0365895  -0.10901361]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 30 ] state=[-0.06216333 -0.16555599 -0.0365895  -0.10901361], action=0, reward=1.0, next_state=[-0.06547445 -0.36013503 -0.03876977  0.17190469]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 31 ] state=[-0.06547445 -0.36013503 -0.03876977  0.17190469], action=1, reward=1.0, next_state=[-0.07267715 -0.16448025 -0.03533168 -0.13275241]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 32 ] state=[-0.07267715 -0.16448025 -0.03533168 -0.13275241], action=0, reward=1.0, next_state=[-0.07596675 -0.35907875 -0.03798673  0.14857782]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 33 ] state=[-0.07596675 -0.35907875 -0.03798673  0.14857782], action=0, reward=1.0, next_state=[-0.08314833 -0.55363673 -0.03501517  0.42903884]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 34 ] state=[-0.08314833 -0.55363673 -0.03501517  0.42903884], action=1, reward=1.0, next_state=[-0.09422106 -0.35803685 -0.02643439  0.1255265 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 35 ] state=[-0.09422106 -0.35803685 -0.02643439  0.1255265 ], action=0, reward=1.0, next_state=[-0.1013818  -0.55277032 -0.02392386  0.40975385]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 36 ] state=[-0.1013818  -0.55277032 -0.02392386  0.40975385], action=1, reward=1.0, next_state=[-0.11243721 -0.3573175  -0.01572879  0.10962557]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 37 ] state=[-0.11243721 -0.3573175  -0.01572879  0.10962557], action=0, reward=1.0, next_state=[-0.11958356 -0.55221057 -0.01353628  0.39730495]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 38 ] state=[-0.11958356 -0.55221057 -0.01353628  0.39730495], action=1, reward=1.0, next_state=[-0.13062777 -0.35689921 -0.00559018  0.10038521]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 475 ][ timestamp 39 ] state=[-0.13062777 -0.35689921 -0.00559018  0.10038521], action=0, reward=1.0, next_state=[-0.13776575 -0.55194061 -0.00358247  0.39129922]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 40 ] state=[-0.13776575 -0.55194061 -0.00358247  0.39129922], action=1, reward=1.0, next_state=[-0.14880456 -0.356768    0.00424351  0.09748894]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 41 ] state=[-0.14880456 -0.356768    0.00424351  0.09748894], action=0, reward=1.0, next_state=[-0.15593992 -0.55195051  0.00619329  0.39150766]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 42 ] state=[-0.15593992 -0.55195051  0.00619329  0.39150766], action=1, reward=1.0, next_state=[-0.16697893 -0.356917    0.01402344  0.10078382]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 43 ] state=[-0.16697893 -0.356917    0.01402344  0.10078382], action=0, reward=1.0, next_state=[-0.17411727 -0.55223709  0.01603912  0.39785791]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 44 ] state=[-0.17411727 -0.55223709  0.01603912  0.39785791], action=1, reward=1.0, next_state=[-0.18516202 -0.35734631  0.02399628  0.11027465]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 45 ] state=[-0.18516202 -0.35734631  0.02399628  0.11027465], action=0, reward=1.0, next_state=[-0.19230894 -0.55280376  0.02620177  0.41043063]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 46 ] state=[-0.19230894 -0.55280376  0.02620177  0.41043063], action=0, reward=1.0, next_state=[-0.20336502 -0.74828718  0.03441038  0.71125757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 47 ] state=[-0.20336502 -0.74828718  0.03441038  0.71125757], action=0, reward=1.0, next_state=[-0.21833076 -0.94386832  0.04863554  1.01457026]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 48 ] state=[-0.21833076 -0.94386832  0.04863554  1.01457026], action=0, reward=1.0, next_state=[-0.23720813 -1.139604    0.06892694  1.32211982]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 49 ] state=[-0.23720813 -1.139604    0.06892694  1.32211982], action=0, reward=1.0, next_state=[-0.26000021 -1.33552589  0.09536934  1.63555329]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 50 ] state=[-0.26000021 -1.33552589  0.09536934  1.63555329], action=0, reward=1.0, next_state=[-0.28671072 -1.53162867  0.1280804   1.95636686]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 51 ] state=[-0.28671072 -1.53162867  0.1280804   1.95636686], action=0, reward=1.0, next_state=[-0.3173433  -1.72785625  0.16720774  2.28585003]\n",
      "[ Experience replay ] starts\n",
      "[ episode 475 ][ timestamp 52 ] state=[-0.3173433  -1.72785625  0.16720774  2.28585003], action=0, reward=-1.0, next_state=[-0.35190042 -1.92408565  0.21292474  2.62501934]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 475: Exploration_rate=0.01. Score=52.\n",
      "[ episode 476 ] state=[-0.01122156 -0.01182863  0.04725232 -0.00617169]\n",
      "[ episode 476 ][ timestamp 1 ] state=[-0.01122156 -0.01182863  0.04725232 -0.00617169], action=0, reward=1.0, next_state=[-0.01145813 -0.20759528  0.04712888  0.30103753]\n",
      "[ Experience replay ] starts\n",
      "[ episode 476 ][ timestamp 2 ] state=[-0.01145813 -0.20759528  0.04712888  0.30103753], action=0, reward=1.0, next_state=[-0.01561004 -0.40335617  0.05314963  0.60820358]\n",
      "[ Experience replay ] starts\n",
      "[ episode 476 ][ timestamp 3 ] state=[-0.01561004 -0.40335617  0.05314963  0.60820358], action=1, reward=1.0, next_state=[-0.02367716 -0.20901599  0.0653137   0.33272359]\n",
      "[ Experience replay ] starts\n",
      "[ episode 476 ][ timestamp 4 ] state=[-0.02367716 -0.20901599  0.0653137   0.33272359], action=0, reward=1.0, next_state=[-0.02785748 -0.40500384  0.07196818  0.64526711]\n",
      "[ Experience replay ] starts\n",
      "[ episode 476 ][ timestamp 5 ] state=[-0.02785748 -0.40500384  0.07196818  0.64526711], action=0, reward=1.0, next_state=[-0.03595756 -0.60105098  0.08487352  0.95971698]\n",
      "[ Experience replay ] starts\n",
      "[ episode 476 ][ timestamp 6 ] state=[-0.03595756 -0.60105098  0.08487352  0.95971698], action=0, reward=1.0, next_state=[-0.04797858 -0.79720499  0.10406786  1.27781174]\n",
      "[ Experience replay ] starts\n",
      "[ episode 476 ][ timestamp 7 ] state=[-0.04797858 -0.79720499  0.10406786  1.27781174], action=0, reward=1.0, next_state=[-0.06392268 -0.99348821  0.12962409  1.60118444]\n",
      "[ Experience replay ] starts\n",
      "[ episode 476 ][ timestamp 8 ] state=[-0.06392268 -0.99348821  0.12962409  1.60118444], action=0, reward=1.0, next_state=[-0.08379244 -1.18988498  0.16164778  1.93131096]\n",
      "[ Experience replay ] starts\n",
      "[ episode 476 ][ timestamp 9 ] state=[-0.08379244 -1.18988498  0.16164778  1.93131096], action=0, reward=1.0, next_state=[-0.10759014 -1.38632702  0.200274    2.26945037]\n",
      "[ Experience replay ] starts\n",
      "[ episode 476 ][ timestamp 10 ] state=[-0.10759014 -1.38632702  0.200274    2.26945037], action=0, reward=-1.0, next_state=[-0.13531668 -1.58267679  0.24566301  2.61657583]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 476: Exploration_rate=0.01. Score=10.\n",
      "[ episode 477 ] state=[ 0.00673636 -0.00034825 -0.0293587  -0.04579987]\n",
      "[ episode 477 ][ timestamp 1 ] state=[ 0.00673636 -0.00034825 -0.0293587  -0.04579987], action=0, reward=1.0, next_state=[ 0.0067294  -0.19503718 -0.0302747   0.23747746]\n",
      "[ Experience replay ] starts\n",
      "[ episode 477 ][ timestamp 2 ] state=[ 0.0067294  -0.19503718 -0.0302747   0.23747746], action=0, reward=1.0, next_state=[ 0.00282866 -0.38971383 -0.02552515  0.52045921]\n",
      "[ Experience replay ] starts\n",
      "[ episode 477 ][ timestamp 3 ] state=[ 0.00282866 -0.38971383 -0.02552515  0.52045921], action=0, reward=1.0, next_state=[-0.00496562 -0.58446733 -0.01511597  0.80499073]\n",
      "[ Experience replay ] starts\n",
      "[ episode 477 ][ timestamp 4 ] state=[-0.00496562 -0.58446733 -0.01511597  0.80499073], action=0, reward=1.0, next_state=[-1.66549674e-02 -7.79378830e-01  9.83848452e-04  1.09288065e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 477 ][ timestamp 5 ] state=[-1.66549674e-02 -7.79378830e-01  9.83848452e-04  1.09288065e+00], action=0, reward=1.0, next_state=[-0.03224254 -0.97451373  0.02284146  1.38587211]\n",
      "[ Experience replay ] starts\n",
      "[ episode 477 ][ timestamp 6 ] state=[-0.03224254 -0.97451373  0.02284146  1.38587211], action=0, reward=1.0, next_state=[-0.05173282 -1.16991289  0.0505589   1.6856092 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 477 ][ timestamp 7 ] state=[-0.05173282 -1.16991289  0.0505589   1.6856092 ], action=0, reward=1.0, next_state=[-0.07513108 -1.36558204  0.08427109  1.99359586]\n",
      "[ Experience replay ] starts\n",
      "[ episode 477 ][ timestamp 8 ] state=[-0.07513108 -1.36558204  0.08427109  1.99359586], action=0, reward=1.0, next_state=[-0.10244272 -1.56147892  0.124143    2.3111448 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 477 ][ timestamp 9 ] state=[-0.10244272 -1.56147892  0.124143    2.3111448 ], action=0, reward=1.0, next_state=[-0.1336723  -1.75749786  0.1703659   2.63931477]\n",
      "[ Experience replay ] starts\n",
      "[ episode 477 ][ timestamp 10 ] state=[-0.1336723  -1.75749786  0.1703659   2.63931477], action=0, reward=-1.0, next_state=[-0.16882225 -1.95345173  0.2231522   2.97883591]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 477: Exploration_rate=0.01. Score=10.\n",
      "[ episode 478 ] state=[ 0.0038968   0.02870409 -0.00756142 -0.03911513]\n",
      "[ episode 478 ][ timestamp 1 ] state=[ 0.0038968   0.02870409 -0.00756142 -0.03911513], action=0, reward=1.0, next_state=[ 0.00447088 -0.16630862 -0.00834372  0.25117254]\n",
      "[ Experience replay ] starts\n",
      "[ episode 478 ][ timestamp 2 ] state=[ 0.00447088 -0.16630862 -0.00834372  0.25117254], action=0, reward=1.0, next_state=[ 0.00114471 -0.36131043 -0.00332027  0.54121206]\n",
      "[ Experience replay ] starts\n",
      "[ episode 478 ][ timestamp 3 ] state=[ 0.00114471 -0.36131043 -0.00332027  0.54121206], action=0, reward=1.0, next_state=[-0.0060815  -0.55638556  0.00750397  0.83284697]\n",
      "[ Experience replay ] starts\n",
      "[ episode 478 ][ timestamp 4 ] state=[-0.0060815  -0.55638556  0.00750397  0.83284697], action=1, reward=1.0, next_state=[-0.01720921 -0.36136695  0.02416091  0.54253344]\n",
      "[ Experience replay ] starts\n",
      "[ episode 478 ][ timestamp 5 ] state=[-0.01720921 -0.36136695  0.02416091  0.54253344], action=0, reward=1.0, next_state=[-0.02443655 -0.55681998  0.03501158  0.84273004]\n",
      "[ Experience replay ] starts\n",
      "[ episode 478 ][ timestamp 6 ] state=[-0.02443655 -0.55681998  0.03501158  0.84273004], action=0, reward=1.0, next_state=[-0.03557295 -0.75240185  0.05186618  1.14621435]\n",
      "[ Experience replay ] starts\n",
      "[ episode 478 ][ timestamp 7 ] state=[-0.03557295 -0.75240185  0.05186618  1.14621435], action=0, reward=1.0, next_state=[-0.05062099 -0.94816136  0.07479046  1.45470056]\n",
      "[ Experience replay ] starts\n",
      "[ episode 478 ][ timestamp 8 ] state=[-0.05062099 -0.94816136  0.07479046  1.45470056], action=0, reward=1.0, next_state=[-0.06958422 -1.1441176   0.10388448  1.76978113]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 478 ][ timestamp 9 ] state=[-0.06958422 -1.1441176   0.10388448  1.76978113], action=0, reward=1.0, next_state=[-0.09246657 -1.34024752  0.1392801   2.0928771 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 478 ][ timestamp 10 ] state=[-0.09246657 -1.34024752  0.1392801   2.0928771 ], action=0, reward=1.0, next_state=[-0.11927152 -1.53647122  0.18113764  2.42517845]\n",
      "[ Experience replay ] starts\n",
      "[ episode 478 ][ timestamp 11 ] state=[-0.11927152 -1.53647122  0.18113764  2.42517845], action=0, reward=-1.0, next_state=[-0.15000094 -1.73263498  0.22964121  2.76757377]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 478: Exploration_rate=0.01. Score=11.\n",
      "[ episode 479 ] state=[-0.02874712  0.02182374  0.01536018 -0.04528169]\n",
      "[ episode 479 ][ timestamp 1 ] state=[-0.02874712  0.02182374  0.01536018 -0.04528169], action=0, reward=1.0, next_state=[-0.02831064 -0.17351506  0.01445454  0.25220766]\n",
      "[ Experience replay ] starts\n",
      "[ episode 479 ][ timestamp 2 ] state=[-0.02831064 -0.17351506  0.01445454  0.25220766], action=0, reward=1.0, next_state=[-0.03178095 -0.3688404   0.0194987   0.54941455]\n",
      "[ Experience replay ] starts\n",
      "[ episode 479 ][ timestamp 3 ] state=[-0.03178095 -0.3688404   0.0194987   0.54941455], action=0, reward=1.0, next_state=[-0.03915775 -0.56423074  0.03048699  0.84817661]\n",
      "[ Experience replay ] starts\n",
      "[ episode 479 ][ timestamp 4 ] state=[-0.03915775 -0.56423074  0.03048699  0.84817661], action=0, reward=1.0, next_state=[-0.05044237 -0.75975496  0.04745052  1.15028844]\n",
      "[ Experience replay ] starts\n",
      "[ episode 479 ][ timestamp 5 ] state=[-0.05044237 -0.75975496  0.04745052  1.15028844], action=0, reward=1.0, next_state=[-0.06563747 -0.95546295  0.07045629  1.45746522]\n",
      "[ Experience replay ] starts\n",
      "[ episode 479 ][ timestamp 6 ] state=[-0.06563747 -0.95546295  0.07045629  1.45746522], action=0, reward=1.0, next_state=[-0.08474673 -1.15137508  0.09960559  1.77130132]\n",
      "[ Experience replay ] starts\n",
      "[ episode 479 ][ timestamp 7 ] state=[-0.08474673 -1.15137508  0.09960559  1.77130132], action=0, reward=1.0, next_state=[-0.10777423 -1.34746982  0.13503162  2.09322115]\n",
      "[ Experience replay ] starts\n",
      "[ episode 479 ][ timestamp 8 ] state=[-0.10777423 -1.34746982  0.13503162  2.09322115], action=0, reward=1.0, next_state=[-0.13472362 -1.54366922  0.17689604  2.42442004]\n",
      "[ Experience replay ] starts\n",
      "[ episode 479 ][ timestamp 9 ] state=[-0.13472362 -1.54366922  0.17689604  2.42442004], action=0, reward=-1.0, next_state=[-0.16559701 -1.73982195  0.22538444  2.7657942 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 479: Exploration_rate=0.01. Score=9.\n",
      "[ episode 480 ] state=[-0.02477404 -0.04879437  0.04860002 -0.01717717]\n",
      "[ episode 480 ][ timestamp 1 ] state=[-0.02477404 -0.04879437  0.04860002 -0.01717717], action=0, reward=1.0, next_state=[-0.02574993 -0.24457839  0.04825648  0.2904349 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 480 ][ timestamp 2 ] state=[-0.02574993 -0.24457839  0.04825648  0.2904349 ], action=0, reward=1.0, next_state=[-0.03064149 -0.44035404  0.05406518  0.5979384 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 480 ][ timestamp 3 ] state=[-0.03064149 -0.44035404  0.05406518  0.5979384 ], action=1, reward=1.0, next_state=[-0.03944857 -0.24602862  0.06602395  0.32276361]\n",
      "[ Experience replay ] starts\n",
      "[ episode 480 ][ timestamp 4 ] state=[-0.03944857 -0.24602862  0.06602395  0.32276361], action=0, reward=1.0, next_state=[-0.04436915 -0.44202557  0.07247922  0.63551543]\n",
      "[ Experience replay ] starts\n",
      "[ episode 480 ][ timestamp 5 ] state=[-0.04436915 -0.44202557  0.07247922  0.63551543], action=1, reward=1.0, next_state=[-0.05320966 -0.24798537  0.08518953  0.36650952]\n",
      "[ Experience replay ] starts\n",
      "[ episode 480 ][ timestamp 6 ] state=[-0.05320966 -0.24798537  0.08518953  0.36650952], action=0, reward=1.0, next_state=[-0.05816937 -0.44420806  0.09251972  0.68479162]\n",
      "[ Experience replay ] starts\n",
      "[ episode 480 ][ timestamp 7 ] state=[-0.05816937 -0.44420806  0.09251972  0.68479162], action=1, reward=1.0, next_state=[-0.06705353 -0.25048414  0.10621555  0.42261055]\n",
      "[ Experience replay ] starts\n",
      "[ episode 480 ][ timestamp 8 ] state=[-0.06705353 -0.25048414  0.10621555  0.42261055], action=1, reward=1.0, next_state=[-0.07206321 -0.05701468  0.11466776  0.16521052]\n",
      "[ Experience replay ] starts\n",
      "[ episode 480 ][ timestamp 9 ] state=[-0.07206321 -0.05701468  0.11466776  0.16521052], action=0, reward=1.0, next_state=[-0.0732035  -0.25357543  0.11797197  0.49175386]\n",
      "[ Experience replay ] starts\n",
      "[ episode 480 ][ timestamp 10 ] state=[-0.0732035  -0.25357543  0.11797197  0.49175386], action=1, reward=1.0, next_state=[-0.07827501 -0.06029786  0.12780705  0.23845597]\n",
      "[ Experience replay ] starts\n",
      "[ episode 480 ][ timestamp 11 ] state=[-0.07827501 -0.06029786  0.12780705  0.23845597], action=0, reward=1.0, next_state=[-0.07948097 -0.25699197  0.13257617  0.56856378]\n",
      "[ Experience replay ] starts\n",
      "[ episode 480 ][ timestamp 12 ] state=[-0.07948097 -0.25699197  0.13257617  0.56856378], action=1, reward=1.0, next_state=[-0.08462081 -0.06395427  0.14394744  0.32041151]\n",
      "[ Experience replay ] starts\n",
      "[ episode 480 ][ timestamp 13 ] state=[-0.08462081 -0.06395427  0.14394744  0.32041151], action=1, reward=1.0, next_state=[-0.08589989  0.12885571  0.15035567  0.07636231]\n",
      "[ Experience replay ] starts\n",
      "[ episode 480 ][ timestamp 14 ] state=[-0.08589989  0.12885571  0.15035567  0.07636231], action=0, reward=1.0, next_state=[-0.08332278 -0.06806606  0.15188292  0.41245062]\n",
      "[ Experience replay ] starts\n",
      "[ episode 480 ][ timestamp 15 ] state=[-0.08332278 -0.06806606  0.15188292  0.41245062], action=1, reward=1.0, next_state=[-0.0846841   0.12461338  0.16013193  0.17124075]\n",
      "[ Experience replay ] starts\n",
      "[ episode 480 ][ timestamp 16 ] state=[-0.0846841   0.12461338  0.16013193  0.17124075], action=0, reward=1.0, next_state=[-0.08219183 -0.07239498  0.16355675  0.50985044]\n",
      "[ Experience replay ] starts\n",
      "[ episode 480 ][ timestamp 17 ] state=[-0.08219183 -0.07239498  0.16355675  0.50985044], action=1, reward=1.0, next_state=[-0.08363973  0.12009081  0.17375376  0.27284659]\n",
      "[ Experience replay ] starts\n",
      "[ episode 480 ][ timestamp 18 ] state=[-0.08363973  0.12009081  0.17375376  0.27284659], action=1, reward=1.0, next_state=[-0.08123792  0.3123628   0.17921069  0.03960818]\n",
      "[ Experience replay ] starts\n",
      "[ episode 480 ][ timestamp 19 ] state=[-0.08123792  0.3123628   0.17921069  0.03960818], action=0, reward=1.0, next_state=[-0.07499066  0.11518409  0.18000285  0.38304581]\n",
      "[ Experience replay ] starts\n",
      "[ episode 480 ][ timestamp 20 ] state=[-0.07499066  0.11518409  0.18000285  0.38304581], action=1, reward=1.0, next_state=[-0.07268698  0.30735486  0.18766377  0.15208247]\n",
      "[ Experience replay ] starts\n",
      "[ episode 480 ][ timestamp 21 ] state=[-0.07268698  0.30735486  0.18766377  0.15208247], action=0, reward=1.0, next_state=[-0.06653988  0.11011083  0.19070542  0.4976038 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 480 ][ timestamp 22 ] state=[-0.06653988  0.11011083  0.19070542  0.4976038 ], action=1, reward=1.0, next_state=[-0.06433767  0.30210466  0.20065749  0.27056228]\n",
      "[ Experience replay ] starts\n",
      "[ episode 480 ][ timestamp 23 ] state=[-0.06433767  0.30210466  0.20065749  0.27056228], action=0, reward=1.0, next_state=[-0.05829557  0.10476945  0.20606874  0.61922423]\n",
      "[ Experience replay ] starts\n",
      "[ episode 480 ][ timestamp 24 ] state=[-0.05829557  0.10476945  0.20606874  0.61922423], action=1, reward=-1.0, next_state=[-0.05620018  0.2965082   0.21845322  0.39785741]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 480: Exploration_rate=0.01. Score=24.\n",
      "[ episode 481 ] state=[ 0.0080799  -0.00871976  0.04931254 -0.04807941]\n",
      "[ episode 481 ][ timestamp 1 ] state=[ 0.0080799  -0.00871976  0.04931254 -0.04807941], action=1, reward=1.0, next_state=[ 0.0079055   0.18566167  0.04835095 -0.32480511]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 2 ] state=[ 0.0079055   0.18566167  0.04835095 -0.32480511], action=0, reward=1.0, next_state=[ 0.01161874 -0.01011418  0.04185485 -0.01727488]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 3 ] state=[ 0.01161874 -0.01011418  0.04185485 -0.01727488], action=0, reward=1.0, next_state=[ 0.01141645 -0.20581061  0.04150935  0.28831441]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 4 ] state=[ 0.01141645 -0.20581061  0.04150935  0.28831441], action=0, reward=1.0, next_state=[ 0.00730024 -0.40149916  0.04727564  0.59379463]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 5 ] state=[ 0.00730024 -0.40149916  0.04727564  0.59379463], action=1, reward=1.0, next_state=[-0.00072974 -0.20706972  0.05915153  0.31637018]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 6 ] state=[-0.00072974 -0.20706972  0.05915153  0.31637018], action=1, reward=1.0, next_state=[-0.00487114 -0.012838    0.06547894  0.04291257]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 481 ][ timestamp 7 ] state=[-0.00487114 -0.012838    0.06547894  0.04291257], action=0, reward=1.0, next_state=[-0.0051279  -0.20883482  0.06633719  0.35551482]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 8 ] state=[-0.0051279  -0.20883482  0.06633719  0.35551482], action=1, reward=1.0, next_state=[-0.00930459 -0.01471569  0.07344749  0.08446541]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 9 ] state=[-0.00930459 -0.01471569  0.07344749  0.08446541], action=0, reward=1.0, next_state=[-0.00959891 -0.21080943  0.07513679  0.39938715]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 10 ] state=[-0.00959891 -0.21080943  0.07513679  0.39938715], action=1, reward=1.0, next_state=[-0.0138151  -0.0168293   0.08312454  0.13130735]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 11 ] state=[-0.0138151  -0.0168293   0.08312454  0.13130735], action=0, reward=1.0, next_state=[-0.01415168 -0.21303752  0.08575068  0.44901395]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 12 ] state=[-0.01415168 -0.21303752  0.08575068  0.44901395], action=1, reward=1.0, next_state=[-0.01841243 -0.01922652  0.09473096  0.18454545]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 13 ] state=[-0.01841243 -0.01922652  0.09473096  0.18454545], action=0, reward=1.0, next_state=[-0.01879696 -0.21556726  0.09842187  0.50554534]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 14 ] state=[-0.01879696 -0.21556726  0.09842187  0.50554534], action=1, reward=1.0, next_state=[-0.02310831 -0.02196001  0.10853278  0.24542925]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 15 ] state=[-0.02310831 -0.02196001  0.10853278  0.24542925], action=1, reward=1.0, next_state=[-0.02354751  0.17145793  0.11344136 -0.01114455]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 16 ] state=[-0.02354751  0.17145793  0.11344136 -0.01114455], action=0, reward=1.0, next_state=[-0.02011835 -0.02509271  0.11321847  0.31506668]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 17 ] state=[-0.02011835 -0.02509271  0.11321847  0.31506668], action=1, reward=1.0, next_state=[-0.0206202   0.16824973  0.11951981  0.06012495]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 18 ] state=[-0.0206202   0.16824973  0.11951981  0.06012495], action=0, reward=1.0, next_state=[-0.01725521 -0.02836508  0.12072231  0.38799841]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 19 ] state=[-0.01725521 -0.02836508  0.12072231  0.38799841], action=1, reward=1.0, next_state=[-0.01782251  0.16485495  0.12848227  0.13568399]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 20 ] state=[-0.01782251  0.16485495  0.12848227  0.13568399], action=1, reward=1.0, next_state=[-0.01452541  0.35792487  0.13119595 -0.11386387]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 21 ] state=[-0.01452541  0.35792487  0.13119595 -0.11386387], action=0, reward=1.0, next_state=[-0.00736691  0.1611909   0.12891868  0.21716208]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 22 ] state=[-0.00736691  0.1611909   0.12891868  0.21716208], action=1, reward=1.0, next_state=[-0.0041431   0.35425682  0.13326192 -0.03223636]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 23 ] state=[-0.0041431   0.35425682  0.13326192 -0.03223636], action=0, reward=1.0, next_state=[0.00294204 0.15750053 0.13261719 0.29934448]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 24 ] state=[0.00294204 0.15750053 0.13261719 0.29934448], action=1, reward=1.0, next_state=[0.00609205 0.35050742 0.13860408 0.05125154]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 25 ] state=[0.00609205 0.35050742 0.13860408 0.05125154], action=0, reward=1.0, next_state=[0.0131022  0.15369828 0.13962911 0.38425335]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 26 ] state=[0.0131022  0.15369828 0.13962911 0.38425335], action=1, reward=1.0, next_state=[0.01617617 0.34659033 0.14731418 0.1386489 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 27 ] state=[0.01617617 0.34659033 0.14731418 0.1386489 ], action=1, reward=1.0, next_state=[ 0.02310797  0.53932893  0.15008716 -0.10417374]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 28 ] state=[ 0.02310797  0.53932893  0.15008716 -0.10417374], action=0, reward=1.0, next_state=[0.03389455 0.34241041 0.14800368 0.23184356]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 29 ] state=[0.03389455 0.34241041 0.14800368 0.23184356], action=1, reward=1.0, next_state=[ 0.04074276  0.5351417   0.15264055 -0.01073841]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 30 ] state=[ 0.04074276  0.5351417   0.15264055 -0.01073841], action=0, reward=1.0, next_state=[0.05144559 0.33819768 0.15242578 0.32594509]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 31 ] state=[0.05144559 0.33819768 0.15242578 0.32594509], action=1, reward=1.0, next_state=[0.05820955 0.53085806 0.15894469 0.08494502]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 32 ] state=[0.05820955 0.53085806 0.15894469 0.08494502], action=0, reward=1.0, next_state=[0.06882671 0.33385703 0.16064359 0.42325496]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 33 ] state=[0.06882671 0.33385703 0.16064359 0.42325496], action=1, reward=1.0, next_state=[0.07550385 0.52638208 0.16910868 0.18521199]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 34 ] state=[0.07550385 0.52638208 0.16910868 0.18521199], action=1, reward=1.0, next_state=[ 0.08603149  0.7187317   0.17281292 -0.04971536]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 35 ] state=[ 0.08603149  0.7187317   0.17281292 -0.04971536], action=0, reward=1.0, next_state=[0.10040612 0.52160731 0.17181862 0.29212146]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 36 ] state=[0.10040612 0.52160731 0.17181862 0.29212146], action=1, reward=1.0, next_state=[0.11083827 0.71391624 0.17766105 0.05817205]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 37 ] state=[0.11083827 0.71391624 0.17766105 0.05817205], action=1, reward=1.0, next_state=[ 0.12511659  0.90610492  0.17882449 -0.17361531]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 38 ] state=[ 0.12511659  0.90610492  0.17882449 -0.17361531], action=0, reward=1.0, next_state=[0.14323869 0.70893437 0.17535218 0.16971887]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 39 ] state=[0.14323869 0.70893437 0.17535218 0.16971887], action=1, reward=1.0, next_state=[ 0.15741738  0.90116956  0.17874656 -0.06292233]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 40 ] state=[ 0.15741738  0.90116956  0.17874656 -0.06292233], action=0, reward=1.0, next_state=[0.17544077 0.70399514 0.17748811 0.28039913]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 41 ] state=[0.17544077 0.70399514 0.17748811 0.28039913], action=1, reward=1.0, next_state=[0.18952067 0.89619991 0.18309609 0.04852916]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 42 ] state=[0.18952067 0.89619991 0.18309609 0.04852916], action=1, reward=1.0, next_state=[ 0.20744467  1.08828876  0.18406668 -0.18125789]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 43 ] state=[ 0.20744467  1.08828876  0.18406668 -0.18125789], action=0, reward=1.0, next_state=[0.22921045 0.89107552 0.18044152 0.16337538]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 44 ] state=[0.22921045 0.89107552 0.18044152 0.16337538], action=1, reward=1.0, next_state=[ 0.24703196  1.08321735  0.18370903 -0.0673957 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 45 ] state=[ 0.24703196  1.08321735  0.18370903 -0.0673957 ], action=0, reward=1.0, next_state=[0.26869631 0.88600199 0.18236111 0.27715667]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 46 ] state=[0.26869631 0.88600199 0.18236111 0.27715667], action=1, reward=1.0, next_state=[0.28641635 1.07811748 0.18790425 0.04707935]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 47 ] state=[0.28641635 1.07811748 0.18790425 0.04707935], action=1, reward=1.0, next_state=[ 0.30797869  1.27011777  0.18884583 -0.18093235]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 48 ] state=[ 0.30797869  1.27011777  0.18884583 -0.18093235], action=0, reward=1.0, next_state=[0.33338105 1.07286606 0.18522719 0.16487624]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 481 ][ timestamp 49 ] state=[0.33338105 1.07286606 0.18522719 0.16487624], action=1, reward=1.0, next_state=[ 0.35483837  1.26492016  0.18852471 -0.06413119]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 50 ] state=[ 0.35483837  1.26492016  0.18852471 -0.06413119], action=0, reward=1.0, next_state=[0.38013677 1.06766578 0.18724209 0.28160641]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 51 ] state=[0.38013677 1.06766578 0.18724209 0.28160641], action=1, reward=1.0, next_state=[0.40149009 1.25969225 0.19287422 0.05332932]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 52 ] state=[0.40149009 1.25969225 0.19287422 0.05332932], action=1, reward=1.0, next_state=[ 0.42668394  1.4516008   0.1939408  -0.17284167]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 53 ] state=[ 0.42668394  1.4516008   0.1939408  -0.17284167], action=0, reward=1.0, next_state=[0.45571595 1.2543085  0.19048397 0.17421046]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 54 ] state=[0.45571595 1.2543085  0.19048397 0.17421046], action=1, reward=1.0, next_state=[ 0.48080212  1.44626634  0.19396818 -0.05285407]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 55 ] state=[ 0.48080212  1.44626634  0.19396818 -0.05285407], action=1, reward=1.0, next_state=[ 0.50972745  1.63815462  0.1929111  -0.27861906]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 56 ] state=[ 0.50972745  1.63815462  0.1929111  -0.27861906], action=1, reward=1.0, next_state=[ 0.54249054  1.83007643  0.18733872 -0.5047969 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 57 ] state=[ 0.54249054  1.83007643  0.18733872 -0.5047969 ], action=0, reward=1.0, next_state=[ 0.57909207  1.632877    0.17724278 -0.15941726]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 58 ] state=[ 0.57909207  1.632877    0.17724278 -0.15941726], action=1, reward=1.0, next_state=[ 0.61174961  1.82507711  0.17405443 -0.39136383]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 59 ] state=[ 0.61174961  1.82507711  0.17405443 -0.39136383], action=1, reward=1.0, next_state=[ 0.64825115  2.01735691  0.16622716 -0.6245117 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 60 ] state=[ 0.64825115  2.01735691  0.16622716 -0.6245117 ], action=0, reward=1.0, next_state=[ 0.68859829  1.82035231  0.15373692 -0.28443201]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 61 ] state=[ 0.68859829  1.82035231  0.15373692 -0.28443201], action=1, reward=1.0, next_state=[ 0.72500534  2.01298559  0.14804828 -0.52495316]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 62 ] state=[ 0.72500534  2.01298559  0.14804828 -0.52495316], action=0, reward=1.0, next_state=[ 0.76526505  1.81612437  0.13754922 -0.18952421]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 63 ] state=[ 0.76526505  1.81612437  0.13754922 -0.18952421], action=1, reward=1.0, next_state=[ 0.80158754  2.00903788  0.13375873 -0.4358493 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 64 ] state=[ 0.80158754  2.00903788  0.13375873 -0.4358493 ], action=1, reward=1.0, next_state=[ 0.84176829  2.20203789  0.12504175 -0.68355548]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 65 ] state=[ 0.84176829  2.20203789  0.12504175 -0.68355548], action=0, reward=1.0, next_state=[ 0.88580905  2.00542191  0.11137064 -0.3542676 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 66 ] state=[ 0.88580905  2.00542191  0.11137064 -0.3542676 ], action=1, reward=1.0, next_state=[ 0.92591749  2.19879865  0.10428529 -0.60986034]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 67 ] state=[ 0.92591749  2.19879865  0.10428529 -0.60986034], action=1, reward=1.0, next_state=[ 0.96989346  2.39232015  0.09208808 -0.86796122]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 68 ] state=[ 0.96989346  2.39232015  0.09208808 -0.86796122], action=1, reward=1.0, next_state=[ 1.01773986  2.58607658  0.07472886 -1.13032876]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 69 ] state=[ 1.01773986  2.58607658  0.07472886 -1.13032876], action=0, reward=1.0, next_state=[ 1.0694614   2.3900599   0.05212228 -0.8151745 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 70 ] state=[ 1.0694614   2.3900599   0.05212228 -0.8151745 ], action=1, reward=1.0, next_state=[ 1.11726259  2.58443087  0.03581879 -1.09101799]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 71 ] state=[ 1.11726259  2.58443087  0.03581879 -1.09101799], action=1, reward=1.0, next_state=[ 1.16895121  2.77906289  0.01399843 -1.37225028]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 72 ] state=[ 1.16895121  2.77906289  0.01399843 -1.37225028], action=0, reward=1.0, next_state=[ 1.22453247  2.58376872 -0.01344657 -1.07522233]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 73 ] state=[ 1.22453247  2.58376872 -0.01344657 -1.07522233], action=1, reward=1.0, next_state=[ 1.27620784  2.77906574 -0.03495102 -1.37209455]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 74 ] state=[ 1.27620784  2.77906574 -0.03495102 -1.37209455], action=0, reward=1.0, next_state=[ 1.33178916  2.58439784 -0.06239291 -1.09054454]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 75 ] state=[ 1.33178916  2.58439784 -0.06239291 -1.09054454], action=0, reward=1.0, next_state=[ 1.38347712  2.39015129 -0.0842038  -0.81807328]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 76 ] state=[ 1.38347712  2.39015129 -0.0842038  -0.81807328], action=1, reward=1.0, next_state=[ 1.43128014  2.5863187  -0.10056527 -1.13600852]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 77 ] state=[ 1.43128014  2.5863187  -0.10056527 -1.13600852], action=0, reward=1.0, next_state=[ 1.48300651  2.39264573 -0.12328544 -0.87648323]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 78 ] state=[ 1.48300651  2.39264573 -0.12328544 -0.87648323], action=1, reward=1.0, next_state=[ 1.53085943  2.58920825 -0.1408151  -1.2052433 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 79 ] state=[ 1.53085943  2.58920825 -0.1408151  -1.2052433 ], action=0, reward=1.0, next_state=[ 1.58264359  2.39615855 -0.16491997 -0.95979793]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 80 ] state=[ 1.58264359  2.39615855 -0.16491997 -0.95979793], action=0, reward=1.0, next_state=[ 1.63056677  2.20359136 -0.18411593 -0.7231334 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 81 ] state=[ 1.63056677  2.20359136 -0.18411593 -0.7231334 ], action=1, reward=1.0, next_state=[ 1.67463859  2.40071768 -0.1985786  -1.06765006]\n",
      "[ Experience replay ] starts\n",
      "[ episode 481 ][ timestamp 82 ] state=[ 1.67463859  2.40071768 -0.1985786  -1.06765006], action=0, reward=-1.0, next_state=[ 1.72265295  2.20869678 -0.2199316  -0.84327827]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 481: Exploration_rate=0.01. Score=82.\n",
      "[ episode 482 ] state=[ 0.04555751  0.02815393 -0.02097067 -0.02565428]\n",
      "[ episode 482 ][ timestamp 1 ] state=[ 0.04555751  0.02815393 -0.02097067 -0.02565428], action=1, reward=1.0, next_state=[ 0.04612059  0.22357024 -0.02148376 -0.32487922]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 2 ] state=[ 0.04612059  0.22357024 -0.02148376 -0.32487922], action=0, reward=1.0, next_state=[ 0.050592    0.02876067 -0.02798134 -0.03904804]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 3 ] state=[ 0.050592    0.02876067 -0.02798134 -0.03904804], action=1, reward=1.0, next_state=[ 0.05116721  0.22427246 -0.0287623  -0.34042637]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 4 ] state=[ 0.05116721  0.22427246 -0.0287623  -0.34042637], action=0, reward=1.0, next_state=[ 0.05565266  0.02957131 -0.03557083 -0.05695038]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 5 ] state=[ 0.05565266  0.02957131 -0.03557083 -0.05695038], action=1, reward=1.0, next_state=[ 0.05624409  0.22518475 -0.03670984 -0.36064056]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 6 ] state=[ 0.05624409  0.22518475 -0.03670984 -0.36064056], action=0, reward=1.0, next_state=[ 0.06074778  0.03060332 -0.04392265 -0.07975532]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 7 ] state=[ 0.06074778  0.03060332 -0.04392265 -0.07975532], action=0, reward=1.0, next_state=[ 0.06135985 -0.16386236 -0.04551775  0.19875277]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 482 ][ timestamp 8 ] state=[ 0.06135985 -0.16386236 -0.04551775  0.19875277], action=1, reward=1.0, next_state=[ 0.0580826   0.03188007 -0.0415427  -0.10793437]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 9 ] state=[ 0.0580826   0.03188007 -0.0415427  -0.10793437], action=0, reward=1.0, next_state=[ 0.0587202  -0.16262271 -0.04370139  0.17135804]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 10 ] state=[ 0.0587202  -0.16262271 -0.04370139  0.17135804], action=1, reward=1.0, next_state=[ 0.05546775  0.0330966  -0.04027423 -0.13478475]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 11 ] state=[ 0.05546775  0.0330966  -0.04027423 -0.13478475], action=0, reward=1.0, next_state=[ 0.05612968 -0.16142602 -0.04296992  0.14492516]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 12 ] state=[ 0.05612968 -0.16142602 -0.04296992  0.14492516], action=1, reward=1.0, next_state=[ 0.05290116  0.03428412 -0.04007142 -0.16099834]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 13 ] state=[ 0.05290116  0.03428412 -0.04007142 -0.16099834], action=0, reward=1.0, next_state=[ 0.05358684 -0.16024193 -0.04329138  0.11877866]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 14 ] state=[ 0.05358684 -0.16024193 -0.04329138  0.11877866], action=1, reward=1.0, next_state=[ 0.050382    0.03547269 -0.04091581 -0.18724191]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 15 ] state=[ 0.050382    0.03547269 -0.04091581 -0.18724191], action=0, reward=1.0, next_state=[ 0.05109146 -0.15904071 -0.04466065  0.09225811]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 16 ] state=[ 0.05109146 -0.15904071 -0.04466065  0.09225811], action=0, reward=1.0, next_state=[ 0.04791064 -0.35349503 -0.04281549  0.37052287]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 17 ] state=[ 0.04791064 -0.35349503 -0.04281549  0.37052287], action=1, reward=1.0, next_state=[ 0.04084074 -0.15779176 -0.03540503  0.06465308]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 18 ] state=[ 0.04084074 -0.15779176 -0.03540503  0.06465308], action=1, reward=1.0, next_state=[ 0.03768491  0.03781945 -0.03411197 -0.23898675]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 19 ] state=[ 0.03768491  0.03781945 -0.03411197 -0.23898675], action=0, reward=1.0, next_state=[ 0.03844129 -0.156799   -0.0388917   0.04274412]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 20 ] state=[ 0.03844129 -0.156799   -0.0388917   0.04274412], action=1, reward=1.0, next_state=[ 0.03530532  0.03885844 -0.03803682 -0.26195139]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 21 ] state=[ 0.03530532  0.03885844 -0.03803682 -0.26195139], action=0, reward=1.0, next_state=[ 0.03608248 -0.15570049 -0.04327585  0.01849579]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 22 ] state=[ 0.03608248 -0.15570049 -0.04327585  0.01849579], action=1, reward=1.0, next_state=[ 0.03296847  0.04001451 -0.04290593 -0.28752098]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 23 ] state=[ 0.03296847  0.04001451 -0.04290593 -0.28752098], action=0, reward=1.0, next_state=[ 0.03376876 -0.15447014 -0.04865635 -0.00867296]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 24 ] state=[ 0.03376876 -0.15447014 -0.04865635 -0.00867296], action=1, reward=1.0, next_state=[ 0.03067936  0.04131462 -0.04882981 -0.31630186]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 25 ] state=[ 0.03067936  0.04131462 -0.04882981 -0.31630186], action=0, reward=1.0, next_state=[ 0.03150565 -0.15307903 -0.05515585 -0.03940921]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 26 ] state=[ 0.03150565 -0.15307903 -0.05515585 -0.03940921], action=1, reward=1.0, next_state=[ 0.02844407  0.04278869 -0.05594403 -0.34897161]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 27 ] state=[ 0.02844407  0.04278869 -0.05594403 -0.34897161], action=0, reward=1.0, next_state=[ 0.02929985 -0.15149481 -0.06292347 -0.07444124]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 28 ] state=[ 0.02929985 -0.15149481 -0.06292347 -0.07444124], action=1, reward=1.0, next_state=[ 0.02626995  0.04447014 -0.06441229 -0.38629423]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 29 ] state=[ 0.02626995  0.04447014 -0.06441229 -0.38629423], action=0, reward=1.0, next_state=[ 0.02715935 -0.14968111 -0.07213817 -0.11459541]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 30 ] state=[ 0.02715935 -0.14968111 -0.07213817 -0.11459541], action=0, reward=1.0, next_state=[ 0.02416573 -0.34369925 -0.07443008  0.15448466]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 31 ] state=[ 0.02416573 -0.34369925 -0.07443008  0.15448466], action=1, reward=1.0, next_state=[ 0.01729175 -0.14759488 -0.07134039 -0.16071973]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 32 ] state=[ 0.01729175 -0.14759488 -0.07134039 -0.16071973], action=0, reward=1.0, next_state=[ 0.01433985 -0.34162684 -0.07455478  0.10863159]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 33 ] state=[ 0.01433985 -0.34162684 -0.07455478  0.10863159], action=1, reward=1.0, next_state=[ 0.00750731 -0.14552009 -0.07238215 -0.20661019]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 34 ] state=[ 0.00750731 -0.14552009 -0.07238215 -0.20661019], action=0, reward=1.0, next_state=[ 0.00459691 -0.33953636 -0.07651436  0.06239042]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 35 ] state=[ 0.00459691 -0.33953636 -0.07651436  0.06239042], action=1, reward=1.0, next_state=[-0.00219382 -0.1434055  -0.07526655 -0.25341839]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 36 ] state=[-0.00219382 -0.1434055  -0.07526655 -0.25341839], action=0, reward=1.0, next_state=[-0.00506193 -0.33737655 -0.08033492  0.01460695]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 37 ] state=[-0.00506193 -0.33737655 -0.08033492  0.01460695], action=1, reward=1.0, next_state=[-0.01180946 -0.14119989 -0.08004278 -0.30230207]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 38 ] state=[-0.01180946 -0.14119989 -0.08004278 -0.30230207], action=0, reward=1.0, next_state=[-0.01463346 -0.33509523 -0.08608882 -0.03589771]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 39 ] state=[-0.01463346 -0.33509523 -0.08608882 -0.03589771], action=0, reward=1.0, next_state=[-0.02133536 -0.52888391 -0.08680677  0.22842995]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 40 ] state=[-0.02133536 -0.52888391 -0.08680677  0.22842995], action=1, reward=1.0, next_state=[-0.03191304 -0.33263562 -0.08223817 -0.09032323]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 41 ] state=[-0.03191304 -0.33263562 -0.08223817 -0.09032323], action=0, reward=1.0, next_state=[-0.03856575 -0.5264884  -0.08404464  0.17532243]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 42 ] state=[-0.03856575 -0.5264884  -0.08404464  0.17532243], action=1, reward=1.0, next_state=[-0.04909552 -0.33027049 -0.08053819 -0.14264561]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 43 ] state=[-0.04909552 -0.33027049 -0.08053819 -0.14264561], action=0, reward=1.0, next_state=[-0.05570093 -0.52415217 -0.0833911   0.12358159]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 44 ] state=[-0.05570093 -0.52415217 -0.0833911   0.12358159], action=1, reward=1.0, next_state=[-0.06618397 -0.32794065 -0.08091947 -0.1942015 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 45 ] state=[-0.06618397 -0.32794065 -0.08091947 -0.1942015 ], action=0, reward=1.0, next_state=[-0.07274279 -0.52181745 -0.0848035   0.07189772]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 46 ] state=[-0.07274279 -0.52181745 -0.0848035   0.07189772], action=1, reward=1.0, next_state=[-0.08317913 -0.32558855 -0.08336555 -0.24629021]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 47 ] state=[-0.08317913 -0.32558855 -0.08336555 -0.24629021], action=0, reward=1.0, next_state=[-0.08969091 -0.51942703 -0.08829135  0.01897665]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 48 ] state=[-0.08969091 -0.51942703 -0.08829135  0.01897665], action=0, reward=1.0, next_state=[-0.10007945 -0.71317913 -0.08791182  0.28254881]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 49 ] state=[-0.10007945 -0.71317913 -0.08791182  0.28254881], action=1, reward=1.0, next_state=[-0.11434303 -0.51692042 -0.08226084 -0.0365152 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 482 ][ timestamp 50 ] state=[-0.11434303 -0.51692042 -0.08226084 -0.0365152 ], action=0, reward=1.0, next_state=[-0.12468144 -0.71077228 -0.08299114  0.22912191]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 51 ] state=[-0.12468144 -0.71077228 -0.08299114  0.22912191], action=1, reward=1.0, next_state=[-0.13889688 -0.51456848 -0.07840871 -0.08854226]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 52 ] state=[-0.13889688 -0.51456848 -0.07840871 -0.08854226], action=0, reward=1.0, next_state=[-0.14918825 -0.70848408 -0.08017955  0.17840892]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 53 ] state=[-0.14918825 -0.70848408 -0.08017955  0.17840892], action=1, reward=1.0, next_state=[-0.16335793 -0.51231173 -0.07661137 -0.1384518 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 54 ] state=[-0.16335793 -0.51231173 -0.07661137 -0.1384518 ], action=0, reward=1.0, next_state=[-0.17360417 -0.70625755 -0.07938041  0.1291119 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 55 ] state=[-0.17360417 -0.70625755 -0.07938041  0.1291119 ], action=1, reward=1.0, next_state=[-0.18772932 -0.51009349 -0.07679817 -0.18752096]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 56 ] state=[-0.18772932 -0.51009349 -0.07679817 -0.18752096], action=0, reward=1.0, next_state=[-0.19793119 -0.70403747 -0.08054859  0.07998105]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 57 ] state=[-0.19793119 -0.70403747 -0.08054859  0.07998105], action=1, reward=1.0, next_state=[-0.21201194 -0.50785876 -0.07894897 -0.2369886 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 58 ] state=[-0.21201194 -0.50785876 -0.07894897 -0.2369886 ], action=0, reward=1.0, next_state=[-0.22216911 -0.70176923 -0.08368874  0.02978421]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 59 ] state=[-0.22216911 -0.70176923 -0.08368874  0.02978421], action=1, reward=1.0, next_state=[-0.2362045  -0.50555305 -0.08309306 -0.28808574]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 60 ] state=[-0.2362045  -0.50555305 -0.08309306 -0.28808574], action=0, reward=1.0, next_state=[-0.24631556 -0.69939782 -0.08885477 -0.02272307]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 61 ] state=[-0.24631556 -0.69939782 -0.08885477 -0.02272307], action=1, reward=1.0, next_state=[-0.26030352 -0.50312139 -0.08930923 -0.34206519]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 62 ] state=[-0.26030352 -0.50312139 -0.08930923 -0.34206519], action=0, reward=1.0, next_state=[-0.27036594 -0.69686673 -0.09615054 -0.07882744]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 63 ] state=[-0.27036594 -0.69686673 -0.09615054 -0.07882744], action=1, reward=1.0, next_state=[-0.28430328 -0.50050732 -0.09772709 -0.40023083]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 64 ] state=[-0.28430328 -0.50050732 -0.09772709 -0.40023083], action=0, reward=1.0, next_state=[-0.29431342 -0.69411701 -0.1057317  -0.13988805]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 65 ] state=[-0.29431342 -0.69411701 -0.1057317  -0.13988805], action=1, reward=1.0, next_state=[-0.30819576 -0.49765211 -0.10852946 -0.46396693]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 66 ] state=[-0.30819576 -0.49765211 -0.10852946 -0.46396693], action=0, reward=1.0, next_state=[-0.31814881 -0.69108647 -0.1178088  -0.20736757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 67 ] state=[-0.31814881 -0.69108647 -0.1178088  -0.20736757], action=1, reward=1.0, next_state=[-0.33197054 -0.49449421 -0.12195615 -0.53476768]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 68 ] state=[-0.33197054 -0.49449421 -0.12195615 -0.53476768], action=0, reward=1.0, next_state=[-0.34186042 -0.68770914 -0.13265151 -0.28286423]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 69 ] state=[-0.34186042 -0.68770914 -0.13265151 -0.28286423], action=1, reward=1.0, next_state=[-0.3556146  -0.49096916 -0.13830879 -0.61426683]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 70 ] state=[-0.3556146  -0.49096916 -0.13830879 -0.61426683], action=0, reward=1.0, next_state=[-0.36543399 -0.68391516 -0.15059413 -0.36814488]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 71 ] state=[-0.36543399 -0.68391516 -0.15059413 -0.36814488], action=0, reward=1.0, next_state=[-0.37911229 -0.87661236 -0.15795703 -0.12647798]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 72 ] state=[-0.37911229 -0.87661236 -0.15795703 -0.12647798], action=1, reward=1.0, next_state=[-0.39664454 -0.67962168 -0.16048659 -0.46453191]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 73 ] state=[-0.39664454 -0.67962168 -0.16048659 -0.46453191], action=0, reward=1.0, next_state=[-0.41023697 -0.87215523 -0.16977722 -0.22642354]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 74 ] state=[-0.41023697 -0.87215523 -0.16977722 -0.22642354], action=1, reward=1.0, next_state=[-0.42768008 -0.6750651  -0.17430569 -0.56748328]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 75 ] state=[-0.42768008 -0.6750651  -0.17430569 -0.56748328], action=0, reward=1.0, next_state=[-0.44118138 -0.86736887 -0.18565536 -0.33438531]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 76 ] state=[-0.44118138 -0.86736887 -0.18565536 -0.33438531], action=0, reward=1.0, next_state=[-0.45852875 -1.0594303  -0.19234307 -0.10551354]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 77 ] state=[-0.45852875 -1.0594303  -0.19234307 -0.10551354], action=1, reward=1.0, next_state=[-0.47971736 -0.86214697 -0.19445334 -0.45218222]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 78 ] state=[-0.47971736 -0.86214697 -0.19445334 -0.45218222], action=0, reward=1.0, next_state=[-0.4969603  -1.05406417 -0.20349698 -0.22654157]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 79 ] state=[-0.4969603  -1.05406417 -0.20349698 -0.22654157], action=1, reward=1.0, next_state=[-0.51804158 -0.85670361 -0.20802781 -0.57588991]\n",
      "[ Experience replay ] starts\n",
      "[ episode 482 ][ timestamp 80 ] state=[-0.51804158 -0.85670361 -0.20802781 -0.57588991], action=0, reward=-1.0, next_state=[-0.53517566 -1.048396   -0.21954561 -0.35527062]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 482: Exploration_rate=0.01. Score=80.\n",
      "[ episode 483 ] state=[ 0.00488177  0.01661468  0.00729722 -0.04345077]\n",
      "[ episode 483 ][ timestamp 1 ] state=[ 0.00488177  0.01661468  0.00729722 -0.04345077], action=1, reward=1.0, next_state=[ 0.00521406  0.21163123  0.00642821 -0.33382245]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 2 ] state=[ 0.00521406  0.21163123  0.00642821 -0.33382245], action=1, reward=1.0, next_state=[ 9.44668649e-03  4.06661107e-01 -2.48242242e-04 -6.24471332e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 3 ] state=[ 9.44668649e-03  4.06661107e-01 -2.48242242e-04 -6.24471332e-01], action=1, reward=1.0, next_state=[ 0.01757991  0.60178652 -0.01273767 -0.91723243]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 4 ] state=[ 0.01757991  0.60178652 -0.01273767 -0.91723243], action=0, reward=1.0, next_state=[ 0.02961564  0.40683909 -0.03108232 -0.62857977]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 5 ] state=[ 0.02961564  0.40683909 -0.03108232 -0.62857977], action=0, reward=1.0, next_state=[ 0.03775242  0.21216439 -0.04365391 -0.34584551]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 6 ] state=[ 0.03775242  0.21216439 -0.04365391 -0.34584551], action=1, reward=1.0, next_state=[ 0.04199571  0.40787925 -0.05057082 -0.65196828]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 7 ] state=[ 0.04199571  0.40787925 -0.05057082 -0.65196828], action=1, reward=1.0, next_state=[ 0.05015329  0.60366765 -0.06361019 -0.96013692]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 8 ] state=[ 0.05015329  0.60366765 -0.06361019 -0.96013692], action=0, reward=1.0, next_state=[ 0.06222665  0.40945576 -0.08281293 -0.68809704]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 9 ] state=[ 0.06222665  0.40945576 -0.08281293 -0.68809704], action=0, reward=1.0, next_state=[ 0.07041576  0.2155749  -0.09657487 -0.42259159]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 10 ] state=[ 0.07041576  0.2155749  -0.09657487 -0.42259159], action=0, reward=1.0, next_state=[ 0.07472726  0.02194431 -0.1050267  -0.161848  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 11 ] state=[ 0.07472726  0.02194431 -0.1050267  -0.161848  ], action=0, reward=1.0, next_state=[ 0.07516615 -0.17152955 -0.10826366  0.09594255]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 12 ] state=[ 0.07516615 -0.17152955 -0.10826366  0.09594255], action=1, reward=1.0, next_state=[ 0.07173555  0.02496412 -0.10634481 -0.22883969]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 13 ] state=[ 0.07173555  0.02496412 -0.10634481 -0.22883969], action=0, reward=1.0, next_state=[ 0.07223484 -0.16849011 -0.1109216   0.02849586]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 14 ] state=[ 0.07223484 -0.16849011 -0.1109216   0.02849586], action=1, reward=1.0, next_state=[ 0.06886504  0.02803332 -0.11035169 -0.2970218 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 15 ] state=[ 0.06886504  0.02803332 -0.11035169 -0.2970218 ], action=0, reward=1.0, next_state=[ 0.0694257  -0.1653567  -0.11629212 -0.04107882]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 16 ] state=[ 0.0694257  -0.1653567  -0.11629212 -0.04107882], action=0, reward=1.0, next_state=[ 0.06611857 -0.35863564 -0.1171137   0.21276852]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 483 ][ timestamp 17 ] state=[ 0.06611857 -0.35863564 -0.1171137   0.21276852], action=1, reward=1.0, next_state=[ 0.05894585 -0.16205097 -0.11285833 -0.11444136]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 18 ] state=[ 0.05894585 -0.16205097 -0.11285833 -0.11444136], action=0, reward=1.0, next_state=[ 0.05570484 -0.3553901  -0.11514715  0.14061241]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 19 ] state=[ 0.05570484 -0.3553901  -0.11514715  0.14061241], action=1, reward=1.0, next_state=[ 0.04859703 -0.1588234  -0.11233491 -0.18606361]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 20 ] state=[ 0.04859703 -0.1588234  -0.11233491 -0.18606361], action=0, reward=1.0, next_state=[ 0.04542057 -0.35217383 -0.11605618  0.06917698]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 21 ] state=[ 0.04542057 -0.35217383 -0.11605618  0.06917698], action=1, reward=1.0, next_state=[ 0.03837709 -0.15559578 -0.11467264 -0.25775052]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 22 ] state=[ 0.03837709 -0.15559578 -0.11467264 -0.25775052], action=0, reward=1.0, next_state=[ 0.03526517 -0.34890976 -0.11982765 -0.0033239 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 23 ] state=[ 0.03526517 -0.34890976 -0.11982765 -0.0033239 ], action=0, reward=1.0, next_state=[ 0.02828698 -0.54212758 -0.11989413  0.24927947]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 24 ] state=[ 0.02828698 -0.54212758 -0.11989413  0.24927947], action=1, reward=1.0, next_state=[ 0.01744443 -0.34551566 -0.11490854 -0.07868577]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 25 ] state=[ 0.01744443 -0.34551566 -0.11490854 -0.07868577], action=0, reward=1.0, next_state=[ 0.01053411 -0.53881888 -0.11648225  0.17564807]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 26 ] state=[ 0.01053411 -0.53881888 -0.11648225  0.17564807], action=1, reward=1.0, next_state=[-2.42264301e-04 -3.42239249e-01 -1.12969292e-01 -1.51391611e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 27 ] state=[-2.42264301e-04 -3.42239249e-01 -1.12969292e-01 -1.51391611e-01], action=0, reward=1.0, next_state=[-0.00708705 -0.53557755 -0.11599712  0.10362489]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 28 ] state=[-0.00708705 -0.53557755 -0.11599712  0.10362489], action=0, reward=1.0, next_state=[-0.0177986  -0.72886258 -0.11392463  0.35757736]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 29 ] state=[-0.0177986  -0.72886258 -0.11392463  0.35757736], action=1, reward=1.0, next_state=[-0.03237585 -0.53232091 -0.10677308  0.03125451]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 30 ] state=[-0.03237585 -0.53232091 -0.10677308  0.03125451], action=0, reward=1.0, next_state=[-0.04302227 -0.72576254 -0.10614799  0.28843285]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 31 ] state=[-0.04302227 -0.72576254 -0.10614799  0.28843285], action=1, reward=1.0, next_state=[-0.05753752 -0.52929965 -0.10037933 -0.03575177]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 32 ] state=[-0.05753752 -0.52929965 -0.10037933 -0.03575177], action=0, reward=1.0, next_state=[-0.06812351 -0.72284957 -0.10109437  0.2236497 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 33 ] state=[-0.06812351 -0.72284957 -0.10109437  0.2236497 ], action=1, reward=1.0, next_state=[-0.08258051 -0.52643894 -0.09662137 -0.09913318]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 34 ] state=[-0.08258051 -0.52643894 -0.09662137 -0.09913318], action=0, reward=1.0, next_state=[-0.09310928 -0.72005288 -0.09860404  0.16157065]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 35 ] state=[-0.09310928 -0.72005288 -0.09860404  0.16157065], action=1, reward=1.0, next_state=[-0.10751034 -0.52366772 -0.09537262 -0.16051883]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 36 ] state=[-0.10751034 -0.52366772 -0.09537262 -0.16051883], action=0, reward=1.0, next_state=[-0.1179837  -0.71730408 -0.098583    0.10061866]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 37 ] state=[-0.1179837  -0.71730408 -0.098583    0.10061866], action=1, reward=1.0, next_state=[-0.13232978 -0.52091762 -0.09657063 -0.22146721]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 38 ] state=[-0.13232978 -0.52091762 -0.09657063 -0.22146721], action=0, reward=1.0, next_state=[-0.14274813 -0.7145361  -0.10099997  0.03925966]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 39 ] state=[-0.14274813 -0.7145361  -0.10099997  0.03925966], action=1, reward=1.0, next_state=[-0.15703885 -0.51812175 -0.10021478 -0.28350395]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 40 ] state=[-0.15703885 -0.51812175 -0.10021478 -0.28350395], action=0, reward=1.0, next_state=[-0.16740129 -0.71168216 -0.10588486 -0.02403391]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 41 ] state=[-0.16740129 -0.71168216 -0.10588486 -0.02403391], action=1, reward=1.0, next_state=[-0.18163493 -0.51521363 -0.10636554 -0.34815823]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 42 ] state=[-0.18163493 -0.51521363 -0.10636554 -0.34815823], action=0, reward=1.0, next_state=[-0.1919392  -0.70867462 -0.1133287  -0.09081928]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 43 ] state=[-0.1919392  -0.70867462 -0.1133287  -0.09081928], action=0, reward=1.0, next_state=[-0.2061127  -0.90200516 -0.11514509  0.16406889]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 44 ] state=[-0.2061127  -0.90200516 -0.11514509  0.16406889], action=1, reward=1.0, next_state=[-0.2241528  -0.70543929 -0.11186371 -0.16260537]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 45 ] state=[-0.2241528  -0.70543929 -0.11186371 -0.16260537], action=0, reward=1.0, next_state=[-0.23826158 -0.89879687 -0.11511581  0.09279884]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 46 ] state=[-0.23826158 -0.89879687 -0.11511581  0.09279884], action=1, reward=1.0, next_state=[-0.25623752 -0.70222926 -0.11325984 -0.23387045]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 47 ] state=[-0.25623752 -0.70222926 -0.11325984 -0.23387045], action=0, reward=1.0, next_state=[-0.27028211 -0.89556609 -0.11793725  0.02104947]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 48 ] state=[-0.27028211 -0.89556609 -0.11793725  0.02104947], action=1, reward=1.0, next_state=[-0.28819343 -0.6989676  -0.11751626 -0.30639297]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 49 ] state=[-0.28819343 -0.6989676  -0.11751626 -0.30639297], action=0, reward=1.0, next_state=[-0.30217278 -0.89223601 -0.12364412 -0.05296016]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 50 ] state=[-0.30217278 -0.89223601 -0.12364412 -0.05296016], action=0, reward=1.0, next_state=[-0.3200175  -1.08538811 -0.12470332  0.19829734]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 51 ] state=[-0.3200175  -1.08538811 -0.12470332  0.19829734], action=1, reward=1.0, next_state=[-0.34172526 -0.88872361 -0.12073737 -0.13097648]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 52 ] state=[-0.34172526 -0.88872361 -0.12073737 -0.13097648], action=0, reward=1.0, next_state=[-0.35949974 -1.08192772 -0.1233569   0.12130932]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 53 ] state=[-0.35949974 -1.08192772 -0.1233569   0.12130932], action=1, reward=1.0, next_state=[-0.38113829 -0.88527417 -0.12093072 -0.20760454]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 54 ] state=[-0.38113829 -0.88527417 -0.12093072 -0.20760454], action=0, reward=1.0, next_state=[-0.39884377 -1.07847799 -0.12508281  0.04461765]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 55 ] state=[-0.39884377 -1.07847799 -0.12508281  0.04461765], action=1, reward=1.0, next_state=[-0.42041333 -0.88180504 -0.12419045 -0.28476549]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 56 ] state=[-0.42041333 -0.88180504 -0.12419045 -0.28476549], action=0, reward=1.0, next_state=[-0.43804943 -1.07495712 -0.12988576 -0.03368699]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 57 ] state=[-0.43804943 -1.07495712 -0.12988576 -0.03368699], action=1, reward=1.0, next_state=[-0.45954858 -0.87823491 -0.1305595  -0.36436386]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 58 ] state=[-0.45954858 -0.87823491 -0.1305595  -0.36436386], action=0, reward=1.0, next_state=[-0.47711327 -1.0712831  -0.13784678 -0.1155316 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 59 ] state=[-0.47711327 -1.0712831  -0.13784678 -0.1155316 ], action=1, reward=1.0, next_state=[-0.49853894 -0.87448286 -0.14015741 -0.44833047]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 60 ] state=[-0.49853894 -0.87448286 -0.14015741 -0.44833047], action=0, reward=1.0, next_state=[-0.51602859 -1.06737292 -0.14912402 -0.2029041 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 61 ] state=[-0.51602859 -1.06737292 -0.14912402 -0.2029041 ], action=1, reward=1.0, next_state=[-0.53737605 -0.87046788 -0.1531821  -0.53866382]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 62 ] state=[-0.53737605 -0.87046788 -0.1531821  -0.53866382], action=0, reward=1.0, next_state=[-0.55478541 -1.06314214 -0.16395538 -0.29789621]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 63 ] state=[-0.55478541 -1.06314214 -0.16395538 -0.29789621], action=0, reward=1.0, next_state=[-0.57604825 -1.25559362 -0.16991331 -0.06107754]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 64 ] state=[-0.57604825 -1.25559362 -0.16991331 -0.06107754], action=1, reward=1.0, next_state=[-0.60116012 -1.05849448 -0.17113486 -0.40218322]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 483 ][ timestamp 65 ] state=[-0.60116012 -1.05849448 -0.17113486 -0.40218322], action=0, reward=1.0, next_state=[-0.62233001 -1.25082847 -0.17917852 -0.16796505]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 66 ] state=[-0.62233001 -1.25082847 -0.17917852 -0.16796505], action=1, reward=1.0, next_state=[-0.64734658 -1.05365464 -0.18253782 -0.51138785]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 67 ] state=[-0.64734658 -1.05365464 -0.18253782 -0.51138785], action=0, reward=1.0, next_state=[-0.66841968 -1.24579953 -0.19276558 -0.28132749]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 68 ] state=[-0.66841968 -1.24579953 -0.19276558 -0.28132749], action=1, reward=1.0, next_state=[-0.69333567 -1.04852557 -0.19839213 -0.62808037]\n",
      "[ Experience replay ] starts\n",
      "[ episode 483 ][ timestamp 69 ] state=[-0.69333567 -1.04852557 -0.19839213 -0.62808037], action=0, reward=-1.0, next_state=[-0.71430618 -1.24040682 -0.21095374 -0.40384962]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 483: Exploration_rate=0.01. Score=69.\n",
      "[ episode 484 ] state=[-0.00397595 -0.03229174  0.0123115   0.03619275]\n",
      "[ episode 484 ][ timestamp 1 ] state=[-0.00397595 -0.03229174  0.0123115   0.03619275], action=1, reward=1.0, next_state=[-0.00462179  0.16265152  0.01303536 -0.25258049]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 2 ] state=[-0.00462179  0.16265152  0.01303536 -0.25258049], action=1, reward=1.0, next_state=[-0.00136876  0.35758494  0.00798375 -0.54112348]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 3 ] state=[-0.00136876  0.35758494  0.00798375 -0.54112348], action=1, reward=1.0, next_state=[ 0.00578294  0.55259376 -0.00283872 -0.8312802 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 4 ] state=[ 0.00578294  0.55259376 -0.00283872 -0.8312802 ], action=0, reward=1.0, next_state=[ 0.01683482  0.35751073 -0.01946432 -0.53949141]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 5 ] state=[ 0.01683482  0.35751073 -0.01946432 -0.53949141], action=1, reward=1.0, next_state=[ 0.02398503  0.55290081 -0.03025415 -0.83824317]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 6 ] state=[ 0.02398503  0.55290081 -0.03025415 -0.83824317], action=0, reward=1.0, next_state=[ 0.03504305  0.35820478 -0.04701902 -0.55522614]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 7 ] state=[ 0.03504305  0.35820478 -0.04701902 -0.55522614], action=0, reward=1.0, next_state=[ 0.04220714  0.16377345 -0.05812354 -0.27771997]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 8 ] state=[ 0.04220714  0.16377345 -0.05812354 -0.27771997], action=1, reward=1.0, next_state=[ 0.04548261  0.35967436 -0.06367794 -0.5881538 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 9 ] state=[ 0.04548261  0.35967436 -0.06367794 -0.5881538 ], action=0, reward=1.0, next_state=[ 0.0526761   0.16549924 -0.07544101 -0.3161901 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 10 ] state=[ 0.0526761   0.16549924 -0.07544101 -0.3161901 ], action=0, reward=1.0, next_state=[ 0.05598608 -0.02847161 -0.08176482 -0.04822003]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 11 ] state=[ 0.05598608 -0.02847161 -0.08176482 -0.04822003], action=0, reward=1.0, next_state=[ 0.05541665 -0.22233172 -0.08272922  0.21758656]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 12 ] state=[ 0.05541665 -0.22233172 -0.08272922  0.21758656], action=1, reward=1.0, next_state=[ 0.05097002 -0.02613056 -0.07837749 -0.10000329]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 13 ] state=[ 0.05097002 -0.02613056 -0.07837749 -0.10000329], action=0, reward=1.0, next_state=[ 0.05044741 -0.22004684 -0.08037755  0.16695877]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 14 ] state=[ 0.05044741 -0.22004684 -0.08037755  0.16695877], action=1, reward=1.0, next_state=[ 0.04604647 -0.02387184 -0.07703838 -0.14995926]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 15 ] state=[ 0.04604647 -0.02387184 -0.07703838 -0.14995926], action=0, reward=1.0, next_state=[ 0.04556903 -0.21781095 -0.08003756  0.11745968]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 16 ] state=[ 0.04556903 -0.21781095 -0.08003756  0.11745968], action=1, reward=1.0, next_state=[ 0.04121281 -0.02163887 -0.07768837 -0.19936236]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 17 ] state=[ 0.04121281 -0.02163887 -0.07768837 -0.19936236], action=0, reward=1.0, next_state=[ 0.04078004 -0.21556867 -0.08167562  0.06783753]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 18 ] state=[ 0.04078004 -0.21556867 -0.08167562  0.06783753], action=0, reward=1.0, next_state=[ 0.03646866 -0.40943042 -0.08031886  0.33367484]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 19 ] state=[ 0.03646866 -0.40943042 -0.08031886  0.33367484], action=1, reward=1.0, next_state=[ 0.02828005 -0.21326264 -0.07364537  0.01678342]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 20 ] state=[ 0.02828005 -0.21326264 -0.07364537  0.01678342], action=1, reward=1.0, next_state=[ 0.0240148  -0.01716603 -0.0733097  -0.29819637]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 21 ] state=[ 0.0240148  -0.01716603 -0.0733097  -0.29819637], action=0, reward=1.0, next_state=[ 0.02367148 -0.21117057 -0.07927363 -0.02950494]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 22 ] state=[ 0.02367148 -0.21117057 -0.07927363 -0.02950494], action=1, reward=1.0, next_state=[ 0.01944807 -0.01500656 -0.07986373 -0.34610892]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 23 ] state=[ 0.01944807 -0.01500656 -0.07986373 -0.34610892], action=0, reward=1.0, next_state=[ 0.01914794 -0.20890703 -0.0867859  -0.07964025]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 24 ] state=[ 0.01914794 -0.20890703 -0.0867859  -0.07964025], action=1, reward=1.0, next_state=[ 0.0149698  -0.01265511 -0.08837871 -0.39839328]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 25 ] state=[ 0.0149698  -0.01265511 -0.08837871 -0.39839328], action=0, reward=1.0, next_state=[ 0.0147167  -0.20641938 -0.09634657 -0.13483074]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 26 ] state=[ 0.0147167  -0.20641938 -0.09634657 -0.13483074], action=0, reward=1.0, next_state=[ 0.01058831 -0.40003872 -0.09904319  0.12596924]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 27 ] state=[ 0.01058831 -0.40003872 -0.09904319  0.12596924], action=1, reward=1.0, next_state=[ 0.00258753 -0.20364765 -0.0965238  -0.19624478]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 28 ] state=[ 0.00258753 -0.20364765 -0.0965238  -0.19624478], action=0, reward=1.0, next_state=[-0.00148542 -0.39726593 -0.1004487   0.0644968 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 29 ] state=[-0.00148542 -0.39726593 -0.1004487   0.0644968 ], action=1, reward=1.0, next_state=[-0.00943074 -0.20085799 -0.09915876 -0.25811233]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 30 ] state=[-0.00943074 -0.20085799 -0.09915876 -0.25811233], action=1, reward=1.0, next_state=[-0.0134479  -0.00447052 -0.10432101 -0.58035141]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 31 ] state=[-0.0134479  -0.00447052 -0.10432101 -0.58035141], action=0, reward=1.0, next_state=[-0.01353731 -0.19798787 -0.11592804 -0.32226826]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 32 ] state=[-0.01353731 -0.19798787 -0.11592804 -0.32226826], action=0, reward=1.0, next_state=[-0.01749707 -0.39128459 -0.1223734  -0.06827589]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 33 ] state=[-0.01749707 -0.39128459 -0.1223734  -0.06827589], action=0, reward=1.0, next_state=[-0.02532276 -0.58445893 -0.12373892  0.18343066]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 34 ] state=[-0.02532276 -0.58445893 -0.12373892  0.18343066], action=1, reward=1.0, next_state=[-0.03701194 -0.38780369 -0.12007031 -0.14558327]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 35 ] state=[-0.03701194 -0.38780369 -0.12007031 -0.14558327], action=0, reward=1.0, next_state=[-0.04476801 -0.58101979 -0.12298197  0.1069383 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 36 ] state=[-0.04476801 -0.58101979 -0.12298197  0.1069383 ], action=0, reward=1.0, next_state=[-0.05638841 -0.77418441 -0.12084321  0.35843121]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 37 ] state=[-0.05638841 -0.77418441 -0.12084321  0.35843121], action=1, reward=1.0, next_state=[-0.07187209 -0.57757038 -0.11367458  0.03021944]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 38 ] state=[-0.07187209 -0.57757038 -0.11367458  0.03021944], action=0, reward=1.0, next_state=[-0.0834235  -0.77089418 -0.1130702   0.28498517]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 39 ] state=[-0.0834235  -0.77089418 -0.1130702   0.28498517], action=1, reward=1.0, next_state=[-0.09884139 -0.57435644 -0.10737049 -0.04111077]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 40 ] state=[-0.09884139 -0.57435644 -0.10737049 -0.04111077], action=0, reward=1.0, next_state=[-0.11032851 -0.76778798 -0.10819271  0.21585937]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 484 ][ timestamp 41 ] state=[-0.11032851 -0.76778798 -0.10819271  0.21585937], action=1, reward=1.0, next_state=[-0.12568427 -0.57129902 -0.10387552 -0.10889737]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 42 ] state=[-0.12568427 -0.57129902 -0.10387552 -0.10889737], action=0, reward=1.0, next_state=[-0.13711025 -0.76479092 -0.10605347  0.14929153]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 43 ] state=[-0.13711025 -0.76479092 -0.10605347  0.14929153], action=1, reward=1.0, next_state=[-0.15240607 -0.56832277 -0.10306764 -0.17487624]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 44 ] state=[-0.15240607 -0.56832277 -0.10306764 -0.17487624], action=0, reward=1.0, next_state=[-0.16377253 -0.76183022 -0.10656516  0.08359631]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 45 ] state=[-0.16377253 -0.76183022 -0.10656516  0.08359631], action=0, reward=1.0, next_state=[-0.17900913 -0.955276   -0.10489324  0.34084806]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 46 ] state=[-0.17900913 -0.955276   -0.10489324  0.34084806], action=1, reward=1.0, next_state=[-0.19811465 -0.75883018 -0.09807627  0.01701681]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 47 ] state=[-0.19811465 -0.75883018 -0.09807627  0.01701681], action=1, reward=1.0, next_state=[-0.21329126 -0.56244848 -0.09773594 -0.30492836]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 48 ] state=[-0.21329126 -0.56244848 -0.09773594 -0.30492836], action=0, reward=1.0, next_state=[-0.22454023 -0.75605162 -0.1038345  -0.0445982 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 49 ] state=[-0.22454023 -0.75605162 -0.1038345  -0.0445982 ], action=0, reward=1.0, next_state=[-0.23966126 -0.94954322 -0.10472647  0.21360347]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 50 ] state=[-0.23966126 -0.94954322 -0.10472647  0.21360347], action=1, reward=1.0, next_state=[-0.25865212 -0.75309203 -0.1004544  -0.11019217]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 51 ] state=[-0.25865212 -0.75309203 -0.1004544  -0.11019217], action=0, reward=1.0, next_state=[-0.27371396 -0.94664174 -0.10265824  0.14918484]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 52 ] state=[-0.27371396 -0.94664174 -0.10265824  0.14918484], action=1, reward=1.0, next_state=[-0.2926468  -0.75021101 -0.09967455 -0.17403856]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 53 ] state=[-0.2926468  -0.75021101 -0.09967455 -0.17403856], action=0, reward=1.0, next_state=[-0.30765102 -0.94377563 -0.10315532  0.08561144]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 54 ] state=[-0.30765102 -0.94377563 -0.10315532  0.08561144], action=1, reward=1.0, next_state=[-0.32652653 -0.74733789 -0.10144309 -0.23775273]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 55 ] state=[-0.32652653 -0.74733789 -0.10144309 -0.23775273], action=0, reward=1.0, next_state=[-0.34147329 -0.94087532 -0.10619814  0.02128781]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 56 ] state=[-0.34147329 -0.94087532 -0.10619814  0.02128781], action=0, reward=1.0, next_state=[-0.36029079 -1.13432666 -0.10577239  0.27866646]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 57 ] state=[-0.36029079 -1.13432666 -0.10577239  0.27866646], action=1, reward=1.0, next_state=[-0.38297733 -0.9378673  -0.10019906 -0.0454148 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 58 ] state=[-0.38297733 -0.9378673  -0.10019906 -0.0454148 ], action=0, reward=1.0, next_state=[-0.40173467 -1.13142033 -0.10110735  0.21404928]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 59 ] state=[-0.40173467 -1.13142033 -0.10110735  0.21404928], action=1, reward=1.0, next_state=[-0.42436308 -0.93500914 -0.09682637 -0.10873783]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 60 ] state=[-0.42436308 -0.93500914 -0.09682637 -0.10873783], action=0, reward=1.0, next_state=[-0.44306326 -1.12861984 -0.09900112  0.15189541]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 61 ] state=[-0.44306326 -1.12861984 -0.09900112  0.15189541], action=1, reward=1.0, next_state=[-0.46563566 -0.93222993 -0.09596322 -0.17030579]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 62 ] state=[-0.46563566 -0.93222993 -0.09596322 -0.17030579], action=0, reward=1.0, next_state=[-0.48428026 -1.12585671 -0.09936933  0.09062817]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 63 ] state=[-0.48428026 -1.12585671 -0.09936933  0.09062817], action=1, reward=1.0, next_state=[-0.50679739 -0.92946124 -0.09755677 -0.23167831]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 64 ] state=[-0.50679739 -0.92946124 -0.09755677 -0.23167831], action=0, reward=1.0, next_state=[-0.52538662 -1.12306365 -0.10219034  0.02870825]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 65 ] state=[-0.52538662 -1.12306365 -0.10219034  0.02870825], action=1, reward=1.0, next_state=[-0.54784789 -0.92663598 -0.10161617 -0.29438784]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 66 ] state=[-0.54784789 -0.92663598 -0.10161617 -0.29438784], action=1, reward=1.0, next_state=[-0.56638061 -0.73022318 -0.10750393 -0.61731102]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 67 ] state=[-0.56638061 -0.73022318 -0.10750393 -0.61731102], action=0, reward=1.0, next_state=[-0.58098507 -0.92369213 -0.11985015 -0.36032824]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 68 ] state=[-0.58098507 -0.92369213 -0.11985015 -0.36032824], action=0, reward=1.0, next_state=[-0.59945892 -1.11692469 -0.12705671 -0.10771026]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 69 ] state=[-0.59945892 -1.11692469 -0.12705671 -0.10771026], action=1, reward=1.0, next_state=[-0.62179741 -0.92023263 -0.12921092 -0.43762434]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 70 ] state=[-0.62179741 -0.92023263 -0.12921092 -0.43762434], action=0, reward=1.0, next_state=[-0.64020206 -1.11331158 -0.1379634  -0.1883026 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 71 ] state=[-0.64020206 -1.11331158 -0.1379634  -0.1883026 ], action=1, reward=1.0, next_state=[-0.66246829 -0.91651316 -0.14172946 -0.52112801]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 72 ] state=[-0.66246829 -0.91651316 -0.14172946 -0.52112801], action=0, reward=1.0, next_state=[-0.68079856 -1.10938544 -0.15215202 -0.27624951]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 73 ] state=[-0.68079856 -1.10938544 -0.15215202 -0.27624951], action=1, reward=1.0, next_state=[-0.70298627 -0.91245723 -0.15767701 -0.61278952]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 74 ] state=[-0.70298627 -0.91245723 -0.15767701 -0.61278952], action=0, reward=1.0, next_state=[-0.72123541 -1.10506519 -0.1699328  -0.37362681]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 75 ] state=[-0.72123541 -1.10506519 -0.1699328  -0.37362681], action=0, reward=1.0, next_state=[-0.74333672 -1.2974172  -0.17740533 -0.13897485]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 76 ] state=[-0.74333672 -1.2974172  -0.17740533 -0.13897485], action=1, reward=1.0, next_state=[-0.76928506 -1.10025666 -0.18018483 -0.48195798]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 77 ] state=[-0.76928506 -1.10025666 -0.18018483 -0.48195798], action=0, reward=1.0, next_state=[-0.79129019 -1.29243905 -0.18982399 -0.25103953]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 78 ] state=[-0.79129019 -1.29243905 -0.18982399 -0.25103953], action=1, reward=1.0, next_state=[-0.81713897 -1.09518513 -0.19484478 -0.59707934]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 79 ] state=[-0.81713897 -1.09518513 -0.19484478 -0.59707934], action=0, reward=1.0, next_state=[-0.83904268 -1.28712379 -0.20678637 -0.37154183]\n",
      "[ Experience replay ] starts\n",
      "[ episode 484 ][ timestamp 80 ] state=[-0.83904268 -1.28712379 -0.20678637 -0.37154183], action=1, reward=-1.0, next_state=[-0.86478515 -1.08975646 -0.2142172  -0.72164852]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 484: Exploration_rate=0.01. Score=80.\n",
      "[ episode 485 ] state=[ 0.0433995   0.01400498 -0.03108177  0.00359071]\n",
      "[ episode 485 ][ timestamp 1 ] state=[ 0.0433995   0.01400498 -0.03108177  0.00359071], action=1, reward=1.0, next_state=[ 0.0436796   0.20955858 -0.03100995 -0.29873458]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 2 ] state=[ 0.0436796   0.20955858 -0.03100995 -0.29873458], action=1, reward=1.0, next_state=[ 0.04787077  0.40510852 -0.03698464 -0.60103393]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 485 ][ timestamp 3 ] state=[ 0.04787077  0.40510852 -0.03698464 -0.60103393], action=1, reward=1.0, next_state=[ 0.05597294  0.6007278  -0.04900532 -0.9051332 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 4 ] state=[ 0.05597294  0.6007278  -0.04900532 -0.9051332 ], action=0, reward=1.0, next_state=[ 0.06798749  0.40630252 -0.06710799 -0.62824719]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 5 ] state=[ 0.06798749  0.40630252 -0.06710799 -0.62824719], action=0, reward=1.0, next_state=[ 0.07611354  0.21217815 -0.07967293 -0.35743102]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 6 ] state=[ 0.07611354  0.21217815 -0.07967293 -0.35743102], action=0, reward=1.0, next_state=[ 0.08035711  0.01827395 -0.08682155 -0.09089644]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 7 ] state=[ 0.08035711  0.01827395 -0.08682155 -0.09089644], action=1, reward=1.0, next_state=[ 0.08072259  0.21452613 -0.08863948 -0.40965938]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 8 ] state=[ 0.08072259  0.21452613 -0.08863948 -0.40965938], action=0, reward=1.0, next_state=[ 0.08501311  0.02076536 -0.09683267 -0.14618515]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 9 ] state=[ 0.08501311  0.02076536 -0.09683267 -0.14618515], action=0, reward=1.0, next_state=[ 0.08542842 -0.17284613 -0.09975637  0.11444726]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 10 ] state=[ 0.08542842 -0.17284613 -0.09975637  0.11444726], action=1, reward=1.0, next_state=[ 0.08197149  0.0235532  -0.09746743 -0.20796687]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 11 ] state=[ 0.08197149  0.0235532  -0.09746743 -0.20796687], action=0, reward=1.0, next_state=[ 0.08244256 -0.17004972 -0.10162676  0.05244911]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 12 ] state=[ 0.08244256 -0.17004972 -0.10162676  0.05244911], action=1, reward=1.0, next_state=[ 0.07904156  0.02637149 -0.10057778 -0.2704894 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 13 ] state=[ 0.07904156  0.02637149 -0.10057778 -0.2704894 ], action=0, reward=1.0, next_state=[ 0.07956899 -0.16718212 -0.10598757 -0.01114626]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 14 ] state=[ 0.07956899 -0.16718212 -0.10598757 -0.01114626], action=1, reward=1.0, next_state=[ 0.07622535  0.0292876  -0.10621049 -0.33529916]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 15 ] state=[ 0.07622535  0.0292876  -0.10621049 -0.33529916], action=0, reward=1.0, next_state=[ 0.0768111  -0.16417511 -0.11291648 -0.07790756]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 16 ] state=[ 0.0768111  -0.16417511 -0.11291648 -0.07790756], action=0, reward=1.0, next_state=[ 0.0735276  -0.35751247 -0.11447463  0.17712468]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 17 ] state=[ 0.0735276  -0.35751247 -0.11447463  0.17712468], action=0, reward=1.0, next_state=[ 0.06637735 -0.55082593 -0.11093214  0.43161492]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 18 ] state=[ 0.06637735 -0.55082593 -0.11093214  0.43161492], action=0, reward=1.0, next_state=[ 0.05536083 -0.74421667 -0.10229984  0.68737078]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 19 ] state=[ 0.05536083 -0.74421667 -0.10229984  0.68737078], action=1, reward=1.0, next_state=[ 0.0404765  -0.54783474 -0.08855242  0.3643142 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 20 ] state=[ 0.0404765  -0.54783474 -0.08855242  0.3643142 ], action=0, reward=1.0, next_state=[ 0.0295198  -0.74159391 -0.08126614  0.62781378]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 21 ] state=[ 0.0295198  -0.74159391 -0.08126614  0.62781378], action=1, reward=1.0, next_state=[ 0.01468793 -0.54543741 -0.06870986  0.31068412]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 22 ] state=[ 0.01468793 -0.54543741 -0.06870986  0.31068412], action=1, reward=1.0, next_state=[ 0.00377918 -0.34940723 -0.06249618 -0.00285212]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 23 ] state=[ 0.00377918 -0.34940723 -0.06249618 -0.00285212], action=0, reward=1.0, next_state=[-0.00320897 -0.54357981 -0.06255322  0.26947621]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 24 ] state=[-0.00320897 -0.54357981 -0.06255322  0.26947621], action=1, reward=1.0, next_state=[-0.01408056 -0.34762355 -0.0571637  -0.04226195]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 25 ] state=[-0.01408056 -0.34762355 -0.0571637  -0.04226195], action=1, reward=1.0, next_state=[-0.02103303 -0.15173046 -0.05800894 -0.3524186 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 26 ] state=[-0.02103303 -0.15173046 -0.05800894 -0.3524186 ], action=0, reward=1.0, next_state=[-0.02406764 -0.34598159 -0.06505731 -0.07857708]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 27 ] state=[-0.02406764 -0.34598159 -0.06505731 -0.07857708], action=0, reward=1.0, next_state=[-0.03098728 -0.54011351 -0.06662885  0.19289142]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 28 ] state=[-0.03098728 -0.54011351 -0.06662885  0.19289142], action=0, reward=1.0, next_state=[-0.04178955 -0.73422219 -0.06277102  0.46383399]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 29 ] state=[-0.04178955 -0.73422219 -0.06277102  0.46383399], action=1, reward=1.0, next_state=[-0.05647399 -0.53827196 -0.05349434  0.15204495]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 30 ] state=[-0.05647399 -0.53827196 -0.05349434  0.15204495], action=0, reward=1.0, next_state=[-0.06723943 -0.73258874 -0.05045344  0.42738334]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 31 ] state=[-0.06723943 -0.73258874 -0.05045344  0.42738334], action=1, reward=1.0, next_state=[-0.0818912  -0.53678987 -0.04190578  0.11923174]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 32 ] state=[-0.0818912  -0.53678987 -0.04190578  0.11923174], action=1, reward=1.0, next_state=[-0.092627   -0.34109334 -0.03952114 -0.18637203]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 33 ] state=[-0.092627   -0.34109334 -0.03952114 -0.18637203], action=0, reward=1.0, next_state=[-0.09944887 -0.53562821 -0.04324858  0.09358623]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 34 ] state=[-0.09944887 -0.53562821 -0.04324858  0.09358623], action=1, reward=1.0, next_state=[-0.11016143 -0.33991392 -0.04137686 -0.21242181]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 35 ] state=[-0.11016143 -0.33991392 -0.04137686 -0.21242181], action=1, reward=1.0, next_state=[-0.11695971 -0.14422557 -0.04562529 -0.51786444]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 36 ] state=[-0.11695971 -0.14422557 -0.04562529 -0.51786444], action=1, reward=1.0, next_state=[-0.11984422  0.05150809 -0.05598258 -0.82456856]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 37 ] state=[-0.11984422  0.05150809 -0.05598258 -0.82456856], action=0, reward=1.0, next_state=[-0.11881406 -0.14280527 -0.07247395 -0.55000543]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 38 ] state=[-0.11881406 -0.14280527 -0.07247395 -0.55000543], action=0, reward=1.0, next_state=[-0.12167016 -0.3368384  -0.08347406 -0.28100846]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 39 ] state=[-0.12167016 -0.3368384  -0.08347406 -0.28100846], action=0, reward=1.0, next_state=[-0.12840693 -0.53067659 -0.08909423 -0.01577646]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 40 ] state=[-0.12840693 -0.53067659 -0.08909423 -0.01577646], action=0, reward=1.0, next_state=[-0.13902046 -0.72441535 -0.08940976  0.24751998]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 41 ] state=[-0.13902046 -0.72441535 -0.08940976  0.24751998], action=0, reward=1.0, next_state=[-0.15350877 -0.91815417 -0.08445936  0.51071595]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 42 ] state=[-0.15350877 -0.91815417 -0.08445936  0.51071595], action=1, reward=1.0, next_state=[-0.17187186 -0.72195033 -0.07424504  0.19265773]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 43 ] state=[-0.17187186 -0.72195033 -0.07424504  0.19265773], action=1, reward=1.0, next_state=[-0.18631086 -0.52584914 -0.07039189 -0.12249169]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 44 ] state=[-0.18631086 -0.52584914 -0.07039189 -0.12249169], action=0, reward=1.0, next_state=[-0.19682784 -0.71989569 -0.07284172  0.14717918]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 45 ] state=[-0.19682784 -0.71989569 -0.07284172  0.14717918], action=1, reward=1.0, next_state=[-0.21122576 -0.52381031 -0.06989814 -0.16756547]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 485 ][ timestamp 46 ] state=[-0.21122576 -0.52381031 -0.06989814 -0.16756547], action=0, reward=1.0, next_state=[-0.22170196 -0.71786573 -0.07324945  0.10227355]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 47 ] state=[-0.22170196 -0.71786573 -0.07324945  0.10227355], action=1, reward=1.0, next_state=[-0.23605928 -0.52177463 -0.07120398 -0.21259046]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 48 ] state=[-0.23605928 -0.52177463 -0.07120398 -0.21259046], action=0, reward=1.0, next_state=[-0.24649477 -0.71581013 -0.07545579  0.05680901]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 49 ] state=[-0.24649477 -0.71581013 -0.07545579  0.05680901], action=1, reward=1.0, next_state=[-0.26081097 -0.51969192 -0.0743196  -0.2586942 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 50 ] state=[-0.26081097 -0.51969192 -0.0743196  -0.2586942 ], action=1, reward=1.0, next_state=[-0.27120481 -0.323592   -0.07949349 -0.57386196]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 51 ] state=[-0.27120481 -0.323592   -0.07949349 -0.57386196], action=0, reward=1.0, next_state=[-0.27767665 -0.51751473 -0.09097073 -0.30724294]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 52 ] state=[-0.27767665 -0.51751473 -0.09097073 -0.30724294], action=1, reward=1.0, next_state=[-0.28802695 -0.32122221 -0.09711559 -0.62717273]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 53 ] state=[-0.28802695 -0.32122221 -0.09711559 -0.62717273], action=1, reward=1.0, next_state=[-0.29445139 -0.12488849 -0.10965904 -0.94879274]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 54 ] state=[-0.29445139 -0.12488849 -0.10965904 -0.94879274], action=1, reward=1.0, next_state=[-0.29694916  0.07152531 -0.1286349  -1.27381897]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 55 ] state=[-0.29694916  0.07152531 -0.1286349  -1.27381897], action=1, reward=1.0, next_state=[-0.29551866  0.2680321  -0.15411128 -1.60385829]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 56 ] state=[-0.29551866  0.2680321  -0.15411128 -1.60385829], action=1, reward=1.0, next_state=[-0.29015801  0.46460526 -0.18618844 -1.94035303]\n",
      "[ Experience replay ] starts\n",
      "[ episode 485 ][ timestamp 57 ] state=[-0.29015801  0.46460526 -0.18618844 -1.94035303], action=1, reward=-1.0, next_state=[-0.28086591  0.66116334 -0.2249955  -2.28451815]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 485: Exploration_rate=0.01. Score=57.\n",
      "[ episode 486 ] state=[ 0.04409692  0.02002167 -0.00989796 -0.02209916]\n",
      "[ episode 486 ][ timestamp 1 ] state=[ 0.04409692  0.02002167 -0.00989796 -0.02209916], action=1, reward=1.0, next_state=[ 0.04449735  0.21528416 -0.01033994 -0.31788849]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 2 ] state=[ 0.04449735  0.21528416 -0.01033994 -0.31788849], action=1, reward=1.0, next_state=[ 0.04880303  0.41055184 -0.01669771 -0.61381425]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 3 ] state=[ 0.04880303  0.41055184 -0.01669771 -0.61381425], action=0, reward=1.0, next_state=[ 0.05701407  0.21566716 -0.028974   -0.32643687]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 4 ] state=[ 0.05701407  0.21566716 -0.028974   -0.32643687], action=0, reward=1.0, next_state=[ 0.06132741  0.02096945 -0.03550274 -0.04303005]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 5 ] state=[ 0.06132741  0.02096945 -0.03550274 -0.04303005], action=0, reward=1.0, next_state=[ 0.0617468  -0.1736259  -0.03636334  0.23824342]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 6 ] state=[ 0.0617468  -0.1736259  -0.03636334  0.23824342], action=1, reward=1.0, next_state=[ 0.05827428  0.02199617 -0.03159847 -0.06568416]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 7 ] state=[ 0.05827428  0.02199617 -0.03159847 -0.06568416], action=1, reward=1.0, next_state=[ 0.05871421  0.21755657 -0.03291215 -0.36816674]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 8 ] state=[ 0.05871421  0.21755657 -0.03291215 -0.36816674], action=0, reward=1.0, next_state=[ 0.06306534  0.02291736 -0.04027549 -0.08604046]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 9 ] state=[ 0.06306534  0.02291736 -0.04027549 -0.08604046], action=0, reward=1.0, next_state=[ 0.06352369 -0.17160483 -0.0419963   0.1936684 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 10 ] state=[ 0.06352369 -0.17160483 -0.0419963   0.1936684 ], action=0, reward=1.0, next_state=[ 0.06009159 -0.36610165 -0.03812293  0.47281312]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 11 ] state=[ 0.06009159 -0.36610165 -0.03812293  0.47281312], action=1, reward=1.0, next_state=[ 0.05276956 -0.1704626  -0.02866667  0.16836234]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 12 ] state=[ 0.05276956 -0.1704626  -0.02866667  0.16836234], action=0, reward=1.0, next_state=[ 0.04936031 -0.36516274 -0.02529942  0.45186571]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 13 ] state=[ 0.04936031 -0.36516274 -0.02529942  0.45186571], action=1, reward=1.0, next_state=[ 0.04205705 -0.1696923  -0.0162621   0.15131665]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 14 ] state=[ 0.04205705 -0.1696923  -0.0162621   0.15131665], action=1, reward=1.0, next_state=[ 0.0386632   0.02565869 -0.01323577 -0.14645193]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 15 ] state=[ 0.0386632   0.02565869 -0.01323577 -0.14645193], action=1, reward=1.0, next_state=[ 0.03917638  0.22096766 -0.01616481 -0.44328093]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 16 ] state=[ 0.03917638  0.22096766 -0.01616481 -0.44328093], action=0, reward=1.0, next_state=[ 0.04359573  0.02607812 -0.02503043 -0.15573706]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 17 ] state=[ 0.04359573  0.02607812 -0.02503043 -0.15573706], action=0, reward=1.0, next_state=[ 0.04411729 -0.16867668 -0.02814517  0.12894544]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 18 ] state=[ 0.04411729 -0.16867668 -0.02814517  0.12894544], action=1, reward=1.0, next_state=[ 0.04074376  0.02683692 -0.02556626 -0.17248239]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 19 ] state=[ 0.04074376  0.02683692 -0.02556626 -0.17248239], action=1, reward=1.0, next_state=[ 0.0412805   0.22231528 -0.02901591 -0.47311977]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 20 ] state=[ 0.0412805   0.22231528 -0.02901591 -0.47311977], action=1, reward=1.0, next_state=[ 0.0457268   0.41783475 -0.0384783  -0.774805  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 21 ] state=[ 0.0457268   0.41783475 -0.0384783  -0.774805  ], action=0, reward=1.0, next_state=[ 0.0540835   0.22326263 -0.0539744  -0.49447268]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 22 ] state=[ 0.0540835   0.22326263 -0.0539744  -0.49447268], action=0, reward=1.0, next_state=[ 0.05854875  0.02894176 -0.06386386 -0.21927663]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 23 ] state=[ 0.05854875  0.02894176 -0.06386386 -0.21927663], action=0, reward=1.0, next_state=[ 0.05912759 -0.16521191 -0.06824939  0.05259696]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 24 ] state=[ 0.05912759 -0.16521191 -0.06824939  0.05259696], action=0, reward=1.0, next_state=[ 0.05582335 -0.35929224 -0.06719745  0.32298996]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 25 ] state=[ 0.05582335 -0.35929224 -0.06719745  0.32298996], action=1, reward=1.0, next_state=[ 0.0486375  -0.16328098 -0.06073765  0.00989544]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 26 ] state=[ 0.0486375  -0.16328098 -0.06073765  0.00989544], action=0, reward=1.0, next_state=[ 0.04537188 -0.35748165 -0.06053974  0.28281341]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 27 ] state=[ 0.04537188 -0.35748165 -0.06053974  0.28281341], action=0, reward=1.0, next_state=[ 0.03822225 -0.5516902  -0.05488348  0.55580474]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 28 ] state=[ 0.03822225 -0.5516902  -0.05488348  0.55580474], action=1, reward=1.0, next_state=[ 0.02718845 -0.35584237 -0.04376738  0.2463477 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 29 ] state=[ 0.02718845 -0.35584237 -0.04376738  0.2463477 ], action=0, reward=1.0, next_state=[ 0.0200716  -0.55031279 -0.03884043  0.52491047]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 30 ] state=[ 0.0200716  -0.55031279 -0.03884043  0.52491047], action=1, reward=1.0, next_state=[ 0.00906534 -0.35466639 -0.02834222  0.22024599]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 486 ][ timestamp 31 ] state=[ 0.00906534 -0.35466639 -0.02834222  0.22024599], action=1, reward=1.0, next_state=[ 0.00197202 -0.15915102 -0.0239373  -0.08124078]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 32 ] state=[ 0.00197202 -0.15915102 -0.0239373  -0.08124078], action=1, reward=1.0, next_state=[-0.001211    0.03630575 -0.02556211 -0.38137884]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 33 ] state=[-0.001211    0.03630575 -0.02556211 -0.38137884], action=1, reward=1.0, next_state=[-4.84889065e-04  2.31781171e-01 -3.31896902e-02 -6.82010617e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 34 ] state=[-4.84889065e-04  2.31781171e-01 -3.31896902e-02 -6.82010617e-01], action=1, reward=1.0, next_state=[ 0.00415073  0.42734795 -0.0468299  -0.9849552 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 35 ] state=[ 0.00415073  0.42734795 -0.0468299  -0.9849552 ], action=1, reward=1.0, next_state=[ 0.01269769  0.62306483 -0.06652901 -1.29197164]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 36 ] state=[ 0.01269769  0.62306483 -0.06652901 -1.29197164], action=1, reward=1.0, next_state=[ 0.02515899  0.81896648 -0.09236844 -1.60471915]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 37 ] state=[ 0.02515899  0.81896648 -0.09236844 -1.60471915], action=0, reward=1.0, next_state=[ 0.04153832  0.6250507  -0.12446282 -1.34220316]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 38 ] state=[ 0.04153832  0.6250507  -0.12446282 -1.34220316], action=0, reward=1.0, next_state=[ 0.05403933  0.4316952  -0.15130689 -1.09091113]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 39 ] state=[ 0.05403933  0.4316952  -0.15130689 -1.09091113], action=0, reward=1.0, next_state=[ 0.06267324  0.23885581 -0.17312511 -0.84927154]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 40 ] state=[ 0.06267324  0.23885581 -0.17312511 -0.84927154], action=0, reward=1.0, next_state=[ 0.06745035  0.04646393 -0.19011054 -0.61564265]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 41 ] state=[ 0.06745035  0.04646393 -0.19011054 -0.61564265], action=0, reward=1.0, next_state=[ 0.06837963 -0.14556491 -0.20242339 -0.38834539]\n",
      "[ Experience replay ] starts\n",
      "[ episode 486 ][ timestamp 42 ] state=[ 0.06837963 -0.14556491 -0.20242339 -0.38834539], action=0, reward=-1.0, next_state=[ 0.06546833 -0.33732493 -0.2101903  -0.16568522]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 486: Exploration_rate=0.01. Score=42.\n",
      "[ episode 487 ] state=[-0.03151087  0.03891122  0.04718198  0.00684368]\n",
      "[ episode 487 ][ timestamp 1 ] state=[-0.03151087  0.03891122  0.04718198  0.00684368], action=0, reward=1.0, next_state=[-0.03073265 -0.15685452  0.04731886  0.31403186]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 2 ] state=[-0.03073265 -0.15685452  0.04731886  0.31403186], action=1, reward=1.0, next_state=[-0.03386974  0.03756255  0.05359949  0.03663924]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 3 ] state=[-0.03386974  0.03756255  0.05359949  0.03663924], action=1, reward=1.0, next_state=[-0.03311849  0.23187654  0.05433228 -0.23866245]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 4 ] state=[-0.03311849  0.23187654  0.05433228 -0.23866245], action=1, reward=1.0, next_state=[-0.02848096  0.4261819   0.04955903 -0.51372459]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 5 ] state=[-0.02848096  0.4261819   0.04955903 -0.51372459], action=0, reward=1.0, next_state=[-0.01995732  0.23039828  0.03928454 -0.20584533]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 6 ] state=[-0.01995732  0.23039828  0.03928454 -0.20584533], action=0, reward=1.0, next_state=[-0.01534935  0.03473722  0.03516763  0.0989665 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 7 ] state=[-0.01534935  0.03473722  0.03516763  0.0989665 ], action=0, reward=1.0, next_state=[-0.01465461 -0.16087065  0.03714696  0.40253403]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 8 ] state=[-0.01465461 -0.16087065  0.03714696  0.40253403], action=1, reward=1.0, next_state=[-0.01787202  0.03370529  0.04519764  0.12179017]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 9 ] state=[-0.01787202  0.03370529  0.04519764  0.12179017], action=1, reward=1.0, next_state=[-0.01719792  0.22815153  0.04763344 -0.15629775]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 10 ] state=[-0.01719792  0.22815153  0.04763344 -0.15629775], action=0, reward=1.0, next_state=[-0.01263489  0.03238109  0.04450749  0.15102377]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 11 ] state=[-0.01263489  0.03238109  0.04450749  0.15102377], action=0, reward=1.0, next_state=[-0.01198726 -0.16334898  0.04752796  0.45740901]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 12 ] state=[-0.01198726 -0.16334898  0.04752796  0.45740901], action=0, reward=1.0, next_state=[-0.01525424 -0.35910949  0.05667614  0.76468615]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 13 ] state=[-0.01525424 -0.35910949  0.05667614  0.76468615], action=1, reward=1.0, next_state=[-0.02243643 -0.16481193  0.07196987  0.49036163]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 14 ] state=[-0.02243643 -0.16481193  0.07196987  0.49036163], action=1, reward=1.0, next_state=[-0.02573267  0.02922489  0.0817771   0.22120075]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 15 ] state=[-0.02573267  0.02922489  0.0817771   0.22120075], action=1, reward=1.0, next_state=[-0.02514817  0.22308851  0.08620112 -0.04460719]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 16 ] state=[-0.02514817  0.22308851  0.08620112 -0.04460719], action=1, reward=1.0, next_state=[-0.0206864   0.41687539  0.08530897 -0.30889646]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 17 ] state=[-0.0206864   0.41687539  0.08530897 -0.30889646], action=1, reward=1.0, next_state=[-0.0123489   0.61068479  0.07913104 -0.57350292]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 18 ] state=[-0.0123489   0.61068479  0.07913104 -0.57350292], action=1, reward=1.0, next_state=[-1.35200594e-04  8.04613312e-01  6.76609837e-02 -8.40245186e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 19 ] state=[-1.35200594e-04  8.04613312e-01  6.76609837e-02 -8.40245186e-01], action=1, reward=1.0, next_state=[ 0.01595707  0.9987495   0.05085608 -1.110906  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 20 ] state=[ 0.01595707  0.9987495   0.05085608 -1.110906  ], action=1, reward=1.0, next_state=[ 0.03593206  1.1931678   0.02863796 -1.38721117]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 21 ] state=[ 0.03593206  1.1931678   0.02863796 -1.38721117], action=1, reward=1.0, next_state=[ 5.97954117e-02  1.38792134e+00  8.93736654e-04 -1.67080328e+00]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 22 ] state=[ 5.97954117e-02  1.38792134e+00  8.93736654e-04 -1.67080328e+00], action=1, reward=1.0, next_state=[ 0.08755384  1.5830329  -0.03252233 -1.96320774]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 23 ] state=[ 0.08755384  1.5830329  -0.03252233 -1.96320774], action=1, reward=1.0, next_state=[ 0.1192145   1.77848355 -0.07178648 -2.26578857]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 24 ] state=[ 0.1192145   1.77848355 -0.07178648 -2.26578857], action=1, reward=1.0, next_state=[ 0.15478417  1.97419858 -0.11710226 -2.5796921 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 25 ] state=[ 0.15478417  1.97419858 -0.11710226 -2.5796921 ], action=1, reward=1.0, next_state=[ 0.19426814  2.17003049 -0.1686961  -2.90577763]\n",
      "[ Experience replay ] starts\n",
      "[ episode 487 ][ timestamp 26 ] state=[ 0.19426814  2.17003049 -0.1686961  -2.90577763], action=0, reward=-1.0, next_state=[ 0.23766875  1.97629878 -0.22681165 -2.66866698]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 487: Exploration_rate=0.01. Score=26.\n",
      "[ episode 488 ] state=[ 0.0093075   0.00035532 -0.030812   -0.04039587]\n",
      "[ episode 488 ][ timestamp 1 ] state=[ 0.0093075   0.00035532 -0.030812   -0.04039587], action=1, reward=1.0, next_state=[ 0.00931461  0.19590525 -0.03161991 -0.34263883]\n",
      "[ Experience replay ] starts\n",
      "[ episode 488 ][ timestamp 2 ] state=[ 0.00931461  0.19590525 -0.03161991 -0.34263883], action=1, reward=1.0, next_state=[ 0.01323271  0.39146245 -0.03847269 -0.64512271]\n",
      "[ Experience replay ] starts\n",
      "[ episode 488 ][ timestamp 3 ] state=[ 0.01323271  0.39146245 -0.03847269 -0.64512271], action=1, reward=1.0, next_state=[ 0.02106196  0.58709882 -0.05137514 -0.94966829]\n",
      "[ Experience replay ] starts\n",
      "[ episode 488 ][ timestamp 4 ] state=[ 0.02106196  0.58709882 -0.05137514 -0.94966829], action=1, reward=1.0, next_state=[ 0.03280394  0.78287331 -0.07036851 -1.25804021]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 488 ][ timestamp 5 ] state=[ 0.03280394  0.78287331 -0.07036851 -1.25804021], action=1, reward=1.0, next_state=[ 0.04846141  0.97882168 -0.09552931 -1.57190663]\n",
      "[ Experience replay ] starts\n",
      "[ episode 488 ][ timestamp 6 ] state=[ 0.04846141  0.97882168 -0.09552931 -1.57190663], action=1, reward=1.0, next_state=[ 0.06803784  1.17494484 -0.12696745 -1.89279297]\n",
      "[ Experience replay ] starts\n",
      "[ episode 488 ][ timestamp 7 ] state=[ 0.06803784  1.17494484 -0.12696745 -1.89279297], action=1, reward=1.0, next_state=[ 0.09153674  1.37119534 -0.16482331 -2.22202734]\n",
      "[ Experience replay ] starts\n",
      "[ episode 488 ][ timestamp 8 ] state=[ 0.09153674  1.37119534 -0.16482331 -2.22202734], action=1, reward=1.0, next_state=[ 0.11896064  1.56746154 -0.20926385 -2.5606757 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 488 ][ timestamp 9 ] state=[ 0.11896064  1.56746154 -0.20926385 -2.5606757 ], action=0, reward=-1.0, next_state=[ 0.15030987  1.37453416 -0.26047737 -2.33867348]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 488: Exploration_rate=0.01. Score=9.\n",
      "[ episode 489 ] state=[ 0.04651049  0.00307224 -0.04138262  0.03482332]\n",
      "[ episode 489 ][ timestamp 1 ] state=[ 0.04651049  0.00307224 -0.04138262  0.03482332], action=1, reward=1.0, next_state=[ 0.04657194  0.19876244 -0.04068615 -0.2706237 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 2 ] state=[ 0.04657194  0.19876244 -0.04068615 -0.2706237 ], action=1, reward=1.0, next_state=[ 0.05054719  0.39444066 -0.04609863 -0.57585654]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 3 ] state=[ 0.05054719  0.39444066 -0.04609863 -0.57585654], action=1, reward=1.0, next_state=[ 0.058436    0.59017747 -0.05761576 -0.88269805]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 4 ] state=[ 0.058436    0.59017747 -0.05761576 -0.88269805], action=0, reward=1.0, next_state=[ 0.07023955  0.39588335 -0.07526972 -0.60867013]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 5 ] state=[ 0.07023955  0.39588335 -0.07526972 -0.60867013], action=0, reward=1.0, next_state=[ 0.07815722  0.2018899  -0.08744312 -0.34061227]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 6 ] state=[ 0.07815722  0.2018899  -0.08744312 -0.34061227], action=0, reward=1.0, next_state=[ 0.08219501  0.00811388 -0.09425537 -0.07673432]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 7 ] state=[ 0.08219501  0.00811388 -0.09425537 -0.07673432], action=0, reward=1.0, next_state=[ 0.08235729 -0.18553933 -0.09579005  0.18478607]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 8 ] state=[ 0.08235729 -0.18553933 -0.09579005  0.18478607], action=1, reward=1.0, next_state=[ 0.07864651  0.01081339 -0.09209433 -0.13651201]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 9 ] state=[ 0.07864651  0.01081339 -0.09209433 -0.13651201], action=0, reward=1.0, next_state=[ 0.07886277 -0.18287706 -0.09482457  0.12575499]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 10 ] state=[ 0.07886277 -0.18287706 -0.09482457  0.12575499], action=1, reward=1.0, next_state=[ 0.07520523  0.01346642 -0.09230947 -0.1952738 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 11 ] state=[ 0.07520523  0.01346642 -0.09230947 -0.1952738 ], action=0, reward=1.0, next_state=[ 0.07547456 -0.18022219 -0.09621495  0.06692172]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 12 ] state=[ 0.07547456 -0.18022219 -0.09621495  0.06692172], action=1, reward=1.0, next_state=[ 0.07187012  0.0161381  -0.09487651 -0.25450002]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 13 ] state=[ 0.07187012  0.0161381  -0.09487651 -0.25450002], action=0, reward=1.0, next_state=[ 0.07219288 -0.17751015 -0.09996652  0.00681413]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 14 ] state=[ 0.07219288 -0.17751015 -0.09996652  0.00681413], action=0, reward=1.0, next_state=[ 0.06864268 -0.37106691 -0.09983023  0.26635854]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 15 ] state=[ 0.06864268 -0.37106691 -0.09983023  0.26635854], action=1, reward=1.0, next_state=[ 0.06122134 -0.17467238 -0.09450306 -0.05606787]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 16 ] state=[ 0.06122134 -0.17467238 -0.09450306 -0.05606787], action=0, reward=1.0, next_state=[ 0.05772789 -0.3683212  -0.09562442  0.20536667]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 17 ] state=[ 0.05772789 -0.3683212  -0.09562442  0.20536667], action=1, reward=1.0, next_state=[ 0.05036147 -0.1719711  -0.09151709 -0.11588368]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 18 ] state=[ 0.05036147 -0.1719711  -0.09151709 -0.11588368], action=0, reward=1.0, next_state=[ 0.04692204 -0.3656707  -0.09383476  0.14658136]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 19 ] state=[ 0.04692204 -0.3656707  -0.09383476  0.14658136], action=1, reward=1.0, next_state=[ 0.03960863 -0.16933899 -0.09090313 -0.17416759]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 20 ] state=[ 0.03960863 -0.16933899 -0.09090313 -0.17416759], action=0, reward=1.0, next_state=[ 0.03622185 -0.36305031 -0.09438648  0.08851095]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 21 ] state=[ 0.03622185 -0.36305031 -0.09438648  0.08851095], action=0, reward=1.0, next_state=[ 0.02896084 -0.55670151 -0.09261627  0.34998637]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 22 ] state=[ 0.02896084 -0.55670151 -0.09261627  0.34998637], action=1, reward=1.0, next_state=[ 0.01782681 -0.3603928  -0.08561654  0.02959505]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 23 ] state=[ 0.01782681 -0.3603928  -0.08561654  0.02959505], action=0, reward=1.0, next_state=[ 0.01061896 -0.55418926 -0.08502464  0.29408444]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 24 ] state=[ 0.01061896 -0.55418926 -0.08502464  0.29408444], action=1, reward=1.0, next_state=[-0.00046483 -0.35796451 -0.07914295 -0.02415655]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 25 ] state=[-0.00046483 -0.35796451 -0.07914295 -0.02415655], action=0, reward=1.0, next_state=[-0.00762412 -0.55186753 -0.07962608  0.24254381]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 26 ] state=[-0.00762412 -0.55186753 -0.07962608  0.24254381], action=1, reward=1.0, next_state=[-0.01866147 -0.35570382 -0.0747752  -0.07415477]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 27 ] state=[-0.01866147 -0.35570382 -0.0747752  -0.07415477], action=0, reward=1.0, next_state=[-0.02577554 -0.54967857 -0.0762583   0.19403087]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 28 ] state=[-0.02577554 -0.54967857 -0.0762583   0.19403087], action=1, reward=1.0, next_state=[-0.03676912 -0.35355329 -0.07237768 -0.12170029]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 29 ] state=[-0.03676912 -0.35355329 -0.07237768 -0.12170029], action=0, reward=1.0, next_state=[-0.04384018 -0.54756767 -0.07481169  0.14729889]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 30 ] state=[-0.04384018 -0.54756767 -0.07481169  0.14729889], action=1, reward=1.0, next_state=[-0.05479154 -0.35145855 -0.07186571 -0.16801611]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 31 ] state=[-0.05479154 -0.35145855 -0.07186571 -0.16801611], action=0, reward=1.0, next_state=[-0.06182071 -0.54548217 -0.07522603  0.10115775]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 32 ] state=[-0.06182071 -0.54548217 -0.07522603  0.10115775], action=1, reward=1.0, next_state=[-0.07273035 -0.34936723 -0.07320288 -0.21427829]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 33 ] state=[-0.07273035 -0.34936723 -0.07320288 -0.21427829], action=0, reward=1.0, next_state=[-0.07971769 -0.54337044 -0.07748844  0.05444474]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 34 ] state=[-0.07971769 -0.54337044 -0.07748844  0.05444474], action=1, reward=1.0, next_state=[-0.0905851  -0.34722786 -0.07639955 -0.26164509]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 35 ] state=[-0.0905851  -0.34722786 -0.07639955 -0.26164509], action=0, reward=1.0, next_state=[-0.09752966 -0.54118081 -0.08163245  0.00599607]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 36 ] state=[-0.09752966 -0.54118081 -0.08163245  0.00599607], action=1, reward=1.0, next_state=[-0.10835328 -0.34498878 -0.08151253 -0.31128527]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 37 ] state=[-0.10835328 -0.34498878 -0.08151253 -0.31128527], action=0, reward=1.0, next_state=[-0.11525305 -0.53886054 -0.08773823 -0.04538135]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 489 ][ timestamp 38 ] state=[-0.11525305 -0.53886054 -0.08773823 -0.04538135], action=0, reward=1.0, next_state=[-0.12603026 -0.73262196 -0.08864586  0.21838086]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 39 ] state=[-0.12603026 -0.73262196 -0.08864586  0.21838086], action=1, reward=1.0, next_state=[-0.1406827  -0.53635203 -0.08427824 -0.10089583]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 40 ] state=[-0.1406827  -0.53635203 -0.08427824 -0.10089583], action=0, reward=1.0, next_state=[-0.15140974 -0.73017136 -0.08629616  0.1640528 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 41 ] state=[-0.15140974 -0.73017136 -0.08629616  0.1640528 ], action=1, reward=1.0, next_state=[-0.16601317 -0.53392687 -0.0830151  -0.15455814]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 42 ] state=[-0.16601317 -0.53392687 -0.0830151  -0.15455814], action=0, reward=1.0, next_state=[-0.17669171 -0.72776813 -0.08610627  0.11082403]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 43 ] state=[-0.17669171 -0.72776813 -0.08610627  0.11082403], action=1, reward=1.0, next_state=[-0.19124707 -0.53152462 -0.08388979 -0.20773464]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 44 ] state=[-0.19124707 -0.53152462 -0.08388979 -0.20773464], action=0, reward=1.0, next_state=[-0.20187756 -0.72535305 -0.08804448  0.05735089]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 45 ] state=[-0.20187756 -0.72535305 -0.08804448  0.05735089], action=1, reward=1.0, next_state=[-0.21638462 -0.52908625 -0.08689746 -0.26176063]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 46 ] state=[-0.21638462 -0.52908625 -0.08689746 -0.26176063], action=0, reward=1.0, next_state=[-0.22696635 -0.72286723 -0.09213267  0.00229836]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 47 ] state=[-0.22696635 -0.72286723 -0.09213267  0.00229836], action=1, reward=1.0, next_state=[-0.24142369 -0.52655302 -0.09208671 -0.31797275]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 48 ] state=[-0.24142369 -0.52655302 -0.09208671 -0.31797275], action=0, reward=1.0, next_state=[-0.25195475 -0.72025099 -0.09844616 -0.05569208]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 49 ] state=[-0.25195475 -0.72025099 -0.09844616 -0.05569208], action=1, reward=1.0, next_state=[-0.26635977 -0.52386539 -0.09956    -0.37774059]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 50 ] state=[-0.26635977 -0.52386539 -0.09956    -0.37774059], action=0, reward=1.0, next_state=[-0.27683708 -0.71744282 -0.10711481 -0.11803464]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 51 ] state=[-0.27683708 -0.71744282 -0.10711481 -0.11803464], action=1, reward=1.0, next_state=[-0.29118594 -0.52096219 -0.10947551 -0.44249803]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 52 ] state=[-0.29118594 -0.52096219 -0.10947551 -0.44249803], action=0, reward=1.0, next_state=[-0.30160518 -0.71437855 -0.11832547 -0.18623186]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 53 ] state=[-0.30160518 -0.71437855 -0.11832547 -0.18623186], action=0, reward=1.0, next_state=[-0.31589275 -0.90762628 -0.1220501   0.0669063 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 54 ] state=[-0.31589275 -0.90762628 -0.1220501   0.0669063 ], action=1, reward=1.0, next_state=[-0.33404528 -0.71098508 -0.12071198 -0.26165502]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 55 ] state=[-0.33404528 -0.71098508 -0.12071198 -0.26165502], action=0, reward=1.0, next_state=[-0.34826498 -0.90419565 -0.12594508 -0.0093513 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 56 ] state=[-0.34826498 -0.90419565 -0.12594508 -0.0093513 ], action=1, reward=1.0, next_state=[-0.36634889 -0.70751359 -0.12613211 -0.33896768]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 57 ] state=[-0.36634889 -0.70751359 -0.12613211 -0.33896768], action=0, reward=1.0, next_state=[-0.38049916 -0.90063623 -0.13291146 -0.0885696 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 58 ] state=[-0.38049916 -0.90063623 -0.13291146 -0.0885696 ], action=1, reward=1.0, next_state=[-0.39851189 -0.70388434 -0.13468285 -0.4200555 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 59 ] state=[-0.39851189 -0.70388434 -0.13468285 -0.4200555 ], action=0, reward=1.0, next_state=[-0.41258958 -0.89686653 -0.14308396 -0.17268084]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 60 ] state=[-0.41258958 -0.89686653 -0.14308396 -0.17268084], action=1, reward=1.0, next_state=[-0.43052691 -0.70001747 -0.14653758 -0.50686032]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 61 ] state=[-0.43052691 -0.70001747 -0.14653758 -0.50686032], action=0, reward=1.0, next_state=[-0.44452726 -0.89280366 -0.15667478 -0.26370831]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 62 ] state=[-0.44452726 -0.89280366 -0.15667478 -0.26370831], action=1, reward=1.0, next_state=[-0.46238333 -0.69583284 -0.16194895 -0.60141984]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 63 ] state=[-0.46238333 -0.69583284 -0.16194895 -0.60141984], action=0, reward=1.0, next_state=[-0.47629999 -0.88836324 -0.17397735 -0.3638083 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 64 ] state=[-0.47629999 -0.88836324 -0.17397735 -0.3638083 ], action=0, reward=1.0, next_state=[-0.49406725 -1.08064093 -0.18125351 -0.13063737]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 65 ] state=[-0.49406725 -1.08064093 -0.18125351 -0.13063737], action=1, reward=1.0, next_state=[-0.51568007 -0.88344796 -0.18386626 -0.47457859]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 66 ] state=[-0.51568007 -0.88344796 -0.18386626 -0.47457859], action=0, reward=1.0, next_state=[-0.53334903 -1.07556231 -0.19335783 -0.24501705]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 67 ] state=[-0.53334903 -1.07556231 -0.19335783 -0.24501705], action=1, reward=1.0, next_state=[-0.55486027 -0.87828018 -0.19825817 -0.59191922]\n",
      "[ Experience replay ] starts\n",
      "[ episode 489 ][ timestamp 68 ] state=[-0.55486027 -0.87828018 -0.19825817 -0.59191922], action=0, reward=-1.0, next_state=[-0.57242588 -1.07015542 -0.21009656 -0.3676511 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 489: Exploration_rate=0.01. Score=68.\n",
      "[ episode 490 ] state=[-0.01926769 -0.01177748  0.04198573 -0.00242845]\n",
      "[ episode 490 ][ timestamp 1 ] state=[-0.01926769 -0.01177748  0.04198573 -0.00242845], action=1, reward=1.0, next_state=[-0.01950324  0.18271797  0.04193716 -0.28157434]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 2 ] state=[-0.01950324  0.18271797  0.04193716 -0.28157434], action=1, reward=1.0, next_state=[-0.01584888  0.37721741  0.03630567 -0.56074108]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 3 ] state=[-0.01584888  0.37721741  0.03630567 -0.56074108], action=0, reward=1.0, next_state=[-0.00830453  0.18160524  0.02509085 -0.25684465]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 4 ] state=[-0.00830453  0.18160524  0.02509085 -0.25684465], action=1, reward=1.0, next_state=[-0.00467243  0.37636014  0.01995396 -0.54150912]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 5 ] state=[-0.00467243  0.37636014  0.01995396 -0.54150912], action=0, reward=1.0, next_state=[ 0.00285477  0.1809635   0.00912378 -0.24260643]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 6 ] state=[ 0.00285477  0.1809635   0.00912378 -0.24260643], action=1, reward=1.0, next_state=[ 0.00647404  0.37595395  0.00427165 -0.53239757]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 7 ] state=[ 0.00647404  0.37595395  0.00427165 -0.53239757], action=0, reward=1.0, next_state=[ 0.01399312  0.18077217 -0.0063763  -0.23837172]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 8 ] state=[ 0.01399312  0.18077217 -0.0063763  -0.23837172], action=1, reward=1.0, next_state=[ 0.01760857  0.37598463 -0.01114374 -0.53305908]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 9 ] state=[ 0.01760857  0.37598463 -0.01114374 -0.53305908], action=0, reward=1.0, next_state=[ 0.02512826  0.18102117 -0.02180492 -0.24390823]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 10 ] state=[ 0.02512826  0.18102117 -0.02180492 -0.24390823], action=0, reward=1.0, next_state=[ 0.02874868 -0.01378266 -0.02668308  0.0418179 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 490 ][ timestamp 11 ] state=[ 0.02874868 -0.01378266 -0.02668308  0.0418179 ], action=1, reward=1.0, next_state=[ 0.02847303  0.18171156 -0.02584673 -0.25916293]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 12 ] state=[ 0.02847303  0.18171156 -0.02584673 -0.25916293], action=0, reward=1.0, next_state=[ 0.03210726 -0.01303205 -0.03102999  0.02525683]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 13 ] state=[ 0.03210726 -0.01303205 -0.03102999  0.02525683], action=1, reward=1.0, next_state=[ 0.03184662  0.18252084 -0.03052485 -0.27705265]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 14 ] state=[ 0.03184662  0.18252084 -0.03052485 -0.27705265], action=1, reward=1.0, next_state=[ 0.03549704  0.37806467 -0.0360659  -0.57920468]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 15 ] state=[ 0.03549704  0.37806467 -0.0360659  -0.57920468], action=0, reward=1.0, next_state=[ 0.04305833  0.18346622 -0.04765    -0.2980979 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 16 ] state=[ 0.04305833  0.18346622 -0.04765    -0.2980979 ], action=1, reward=1.0, next_state=[ 0.04672765  0.37923388 -0.05361195 -0.60541988]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 17 ] state=[ 0.04672765  0.37923388 -0.05361195 -0.60541988], action=0, reward=1.0, next_state=[ 0.05431233  0.184901   -0.06572035 -0.33009375]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 18 ] state=[ 0.05431233  0.184901   -0.06572035 -0.33009375], action=1, reward=1.0, next_state=[ 0.05801035  0.38089394 -0.07232223 -0.64275636]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 19 ] state=[ 0.05801035  0.38089394 -0.07232223 -0.64275636], action=0, reward=1.0, next_state=[ 0.06562823  0.18685059 -0.08517735 -0.37369641]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 20 ] state=[ 0.06562823  0.18685059 -0.08517735 -0.37369641], action=0, reward=1.0, next_state=[ 0.06936524 -0.00696464 -0.09265128 -0.10903942]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 21 ] state=[ 0.06936524 -0.00696464 -0.09265128 -0.10903942], action=0, reward=1.0, next_state=[ 0.06922595 -0.2006452  -0.09483207  0.15303482]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 22 ] state=[ 0.06922595 -0.2006452  -0.09483207  0.15303482], action=1, reward=1.0, next_state=[ 0.06521305 -0.00430233 -0.09177137 -0.16799502]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 23 ] state=[ 0.06521305 -0.00430233 -0.09177137 -0.16799502], action=0, reward=1.0, next_state=[ 0.065127   -0.19799902 -0.09513127  0.09438445]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 24 ] state=[ 0.065127   -0.19799902 -0.09513127  0.09438445], action=1, reward=1.0, next_state=[ 0.06116702 -0.00165142 -0.09324358 -0.22673168]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 25 ] state=[ 0.06116702 -0.00165142 -0.09324358 -0.22673168], action=0, reward=1.0, next_state=[ 0.06113399 -0.19532567 -0.09777822  0.0351438 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 26 ] state=[ 0.06113399 -0.19532567 -0.09777822  0.0351438 ], action=1, reward=1.0, next_state=[ 0.05722748  0.00105258 -0.09707534 -0.28671759]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 27 ] state=[ 0.05722748  0.00105258 -0.09707534 -0.28671759], action=1, reward=1.0, next_state=[ 0.05724853  0.19741526 -0.10280969 -0.60837021]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 28 ] state=[ 0.05724853  0.19741526 -0.10280969 -0.60837021], action=0, reward=1.0, next_state=[ 0.06119683  0.00386949 -0.1149771  -0.34975734]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 29 ] state=[ 0.06119683  0.00386949 -0.1149771  -0.34975734], action=1, reward=1.0, next_state=[ 0.06127422  0.2004229  -0.12197225 -0.67636966]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 30 ] state=[ 0.06127422  0.2004229  -0.12197225 -0.67636966], action=0, reward=1.0, next_state=[ 0.06528268  0.00718792 -0.13549964 -0.42444161]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 31 ] state=[ 0.06528268  0.00718792 -0.13549964 -0.42444161], action=1, reward=1.0, next_state=[ 0.06542644  0.20394298 -0.14398847 -0.7565841 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 32 ] state=[ 0.06542644  0.20394298 -0.14398847 -0.7565841 ], action=0, reward=1.0, next_state=[ 0.0695053   0.01106806 -0.15912015 -0.51245213]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 33 ] state=[ 0.0695053   0.01106806 -0.15912015 -0.51245213], action=0, reward=1.0, next_state=[ 0.06972666 -0.18149705 -0.1693692  -0.27383763]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 34 ] state=[ 0.06972666 -0.18149705 -0.1693692  -0.27383763], action=0, reward=1.0, next_state=[ 0.06609672 -0.37384862 -0.17484595 -0.03899554]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 35 ] state=[ 0.06609672 -0.37384862 -0.17484595 -0.03899554], action=0, reward=1.0, next_state=[ 0.05861975 -0.56608842 -0.17562586  0.19382444]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 36 ] state=[ 0.05861975 -0.56608842 -0.17562586  0.19382444], action=1, reward=1.0, next_state=[ 0.04729798 -0.36894599 -0.17174937 -0.14870933]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 37 ] state=[ 0.04729798 -0.36894599 -0.17174937 -0.14870933], action=0, reward=1.0, next_state=[ 0.03991906 -0.56124565 -0.17472356  0.08524986]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 38 ] state=[ 0.03991906 -0.56124565 -0.17472356  0.08524986], action=1, reward=1.0, next_state=[ 0.02869415 -0.36410574 -0.17301856 -0.25706548]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 39 ] state=[ 0.02869415 -0.36410574 -0.17301856 -0.25706548], action=0, reward=1.0, next_state=[ 0.02141203 -0.55638968 -0.17815987 -0.02355993]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 40 ] state=[ 0.02141203 -0.55638968 -0.17815987 -0.02355993], action=0, reward=1.0, next_state=[ 0.01028424 -0.74856874 -0.17863107  0.20804345]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 41 ] state=[ 0.01028424 -0.74856874 -0.17863107  0.20804345], action=1, reward=1.0, next_state=[-0.00468714 -0.55140208 -0.1744702  -0.1352392 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 42 ] state=[-0.00468714 -0.55140208 -0.1744702  -0.1352392 ], action=0, reward=1.0, next_state=[-0.01571518 -0.74365157 -0.17717498  0.09772272]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 43 ] state=[-0.01571518 -0.74365157 -0.17717498  0.09772272], action=1, reward=1.0, next_state=[-0.03058821 -0.5464913  -0.17522053 -0.24520539]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 44 ] state=[-0.03058821 -0.5464913  -0.17522053 -0.24520539], action=0, reward=1.0, next_state=[-0.04151804 -0.7387342  -0.18012464 -0.01250807]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 45 ] state=[-0.04151804 -0.7387342  -0.18012464 -0.01250807], action=0, reward=1.0, next_state=[-0.05629272 -0.9308772  -0.1803748   0.21837278]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 46 ] state=[-0.05629272 -0.9308772  -0.1803748   0.21837278], action=1, reward=1.0, next_state=[-0.07491026 -0.73369685 -0.17600734 -0.12534243]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 47 ] state=[-0.07491026 -0.73369685 -0.17600734 -0.12534243], action=0, reward=1.0, next_state=[-0.0895842  -0.92591786 -0.17851419  0.10705515]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 48 ] state=[-0.0895842  -0.92591786 -0.17851419  0.10705515], action=1, reward=1.0, next_state=[-0.10810256 -0.72874667 -0.17637309 -0.23620651]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 49 ] state=[-0.10810256 -0.72874667 -0.17637309 -0.23620651], action=1, reward=1.0, next_state=[-0.12267749 -0.53160106 -0.18109722 -0.57892259]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 50 ] state=[-0.12267749 -0.53160106 -0.18109722 -0.57892259], action=1, reward=1.0, next_state=[-0.13330951 -0.33446513 -0.19267567 -0.92274276]\n",
      "[ Experience replay ] starts\n",
      "[ episode 490 ][ timestamp 51 ] state=[-0.13330951 -0.33446513 -0.19267567 -0.92274276], action=0, reward=-1.0, next_state=[-0.13999881 -0.52653547 -0.21113052 -0.69626534]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 490: Exploration_rate=0.01. Score=51.\n",
      "[ episode 491 ] state=[ 0.0414077  -0.01825149  0.01662078  0.02090414]\n",
      "[ episode 491 ][ timestamp 1 ] state=[ 0.0414077  -0.01825149  0.01662078  0.02090414], action=1, reward=1.0, next_state=[ 0.04104267  0.17662821  0.01703886 -0.26648875]\n",
      "[ Experience replay ] starts\n",
      "[ episode 491 ][ timestamp 2 ] state=[ 0.04104267  0.17662821  0.01703886 -0.26648875], action=1, reward=1.0, next_state=[ 0.04457523  0.37150289  0.01170908 -0.55374915]\n",
      "[ Experience replay ] starts\n",
      "[ episode 491 ][ timestamp 3 ] state=[ 0.04457523  0.37150289  0.01170908 -0.55374915], action=0, reward=1.0, next_state=[ 0.05200529  0.17621849  0.0006341  -0.25740024]\n",
      "[ Experience replay ] starts\n",
      "[ episode 491 ][ timestamp 4 ] state=[ 0.05200529  0.17621849  0.0006341  -0.25740024], action=1, reward=1.0, next_state=[ 0.05552966  0.37133138 -0.0045139  -0.5498831 ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 491 ][ timestamp 5 ] state=[ 0.05552966  0.37133138 -0.0045139  -0.5498831 ], action=0, reward=1.0, next_state=[ 0.06295629  0.17627313 -0.01551157 -0.25862577]\n",
      "[ Experience replay ] starts\n",
      "[ episode 491 ][ timestamp 6 ] state=[ 0.06295629  0.17627313 -0.01551157 -0.25862577], action=1, reward=1.0, next_state=[ 0.06648175  0.37161305 -0.02068408 -0.55616063]\n",
      "[ Experience replay ] starts\n",
      "[ episode 491 ][ timestamp 7 ] state=[ 0.06648175  0.37161305 -0.02068408 -0.55616063], action=0, reward=1.0, next_state=[ 0.07391401  0.17678751 -0.03180729 -0.27006552]\n",
      "[ Experience replay ] starts\n",
      "[ episode 491 ][ timestamp 8 ] state=[ 0.07391401  0.17678751 -0.03180729 -0.27006552], action=1, reward=1.0, next_state=[ 0.07744976  0.37234858 -0.0372086  -0.57260852]\n",
      "[ Experience replay ] starts\n",
      "[ episode 491 ][ timestamp 9 ] state=[ 0.07744976  0.37234858 -0.0372086  -0.57260852], action=0, reward=1.0, next_state=[ 0.08489673  0.17776757 -0.04866077 -0.29187582]\n",
      "[ Experience replay ] starts\n",
      "[ episode 491 ][ timestamp 10 ] state=[ 0.08489673  0.17776757 -0.04866077 -0.29187582], action=1, reward=1.0, next_state=[ 0.08845208  0.37354835 -0.05449829 -0.5995    ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 491 ][ timestamp 11 ] state=[ 0.08845208  0.37354835 -0.05449829 -0.5995    ], action=0, reward=1.0, next_state=[ 0.09592305  0.17922952 -0.06648829 -0.32446907]\n",
      "[ Experience replay ] starts\n",
      "[ episode 491 ][ timestamp 12 ] state=[ 0.09592305  0.17922952 -0.06648829 -0.32446907], action=0, reward=1.0, next_state=[ 0.09950764 -0.01488582 -0.07297767 -0.05347257]\n",
      "[ Experience replay ] starts\n",
      "[ episode 491 ][ timestamp 13 ] state=[ 0.09950764 -0.01488582 -0.07297767 -0.05347257], action=1, reward=1.0, next_state=[ 0.09920993  0.18120254 -0.07404712 -0.36825862]\n",
      "[ Experience replay ] starts\n",
      "[ episode 491 ][ timestamp 14 ] state=[ 0.09920993  0.18120254 -0.07404712 -0.36825862], action=0, reward=1.0, next_state=[ 0.10283398 -0.01279344 -0.0814123  -0.099812  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 491 ][ timestamp 15 ] state=[ 0.10283398 -0.01279344 -0.0814123  -0.099812  ], action=1, reward=1.0, next_state=[ 0.10257811  0.1833952  -0.08340854 -0.41702904]\n",
      "[ Experience replay ] starts\n",
      "[ episode 491 ][ timestamp 16 ] state=[ 0.10257811  0.1833952  -0.08340854 -0.41702904], action=1, reward=1.0, next_state=[ 0.10624601  0.37959403 -0.09174912 -0.73479786]\n",
      "[ Experience replay ] starts\n",
      "[ episode 491 ][ timestamp 17 ] state=[ 0.10624601  0.37959403 -0.09174912 -0.73479786], action=1, reward=1.0, next_state=[ 0.11383789  0.57585562 -0.10644507 -1.05488844]\n",
      "[ Experience replay ] starts\n",
      "[ episode 491 ][ timestamp 18 ] state=[ 0.11383789  0.57585562 -0.10644507 -1.05488844], action=1, reward=1.0, next_state=[ 0.125355    0.77221512 -0.12754284 -1.3789964 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 491 ][ timestamp 19 ] state=[ 0.125355    0.77221512 -0.12754284 -1.3789964 ], action=0, reward=1.0, next_state=[ 0.14079931  0.57889542 -0.15512277 -1.12876825]\n",
      "[ Experience replay ] starts\n",
      "[ episode 491 ][ timestamp 20 ] state=[ 0.14079931  0.57889542 -0.15512277 -1.12876825], action=0, reward=1.0, next_state=[ 0.15237722  0.38610732 -0.17769814 -0.88848184]\n",
      "[ Experience replay ] starts\n",
      "[ episode 491 ][ timestamp 21 ] state=[ 0.15237722  0.38610732 -0.17769814 -0.88848184], action=0, reward=1.0, next_state=[ 0.16009936  0.19378408 -0.19546777 -0.65650844]\n",
      "[ Experience replay ] starts\n",
      "[ episode 491 ][ timestamp 22 ] state=[ 0.16009936  0.19378408 -0.19546777 -0.65650844], action=0, reward=1.0, next_state=[ 0.16397504  0.0018427  -0.20859794 -0.43118137]\n",
      "[ Experience replay ] starts\n",
      "[ episode 491 ][ timestamp 23 ] state=[ 0.16397504  0.0018427  -0.20859794 -0.43118137], action=0, reward=-1.0, next_state=[ 0.1640119  -0.18980985 -0.21722157 -0.21081846]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 491: Exploration_rate=0.01. Score=23.\n",
      "[ episode 492 ] state=[-0.01702912 -0.01621851 -0.00480425 -0.0461946 ]\n",
      "[ episode 492 ][ timestamp 1 ] state=[-0.01702912 -0.01621851 -0.00480425 -0.0461946 ], action=1, reward=1.0, next_state=[-0.01735349  0.178972   -0.00572814 -0.34038943]\n",
      "[ Experience replay ] starts\n",
      "[ episode 492 ][ timestamp 2 ] state=[-0.01735349  0.178972   -0.00572814 -0.34038943], action=1, reward=1.0, next_state=[-0.01377405  0.37417499 -0.01253593 -0.63487316]\n",
      "[ Experience replay ] starts\n",
      "[ episode 492 ][ timestamp 3 ] state=[-0.01377405  0.37417499 -0.01253593 -0.63487316], action=1, reward=1.0, next_state=[-0.00629055  0.56946953 -0.02523339 -0.93147742]\n",
      "[ Experience replay ] starts\n",
      "[ episode 492 ][ timestamp 4 ] state=[-0.00629055  0.56946953 -0.02523339 -0.93147742], action=1, reward=1.0, next_state=[ 0.00509884  0.76492275 -0.04386294 -1.23198175]\n",
      "[ Experience replay ] starts\n",
      "[ episode 492 ][ timestamp 5 ] state=[ 0.00509884  0.76492275 -0.04386294 -1.23198175], action=1, reward=1.0, next_state=[ 0.0203973   0.96058049 -0.06850257 -1.53807765]\n",
      "[ Experience replay ] starts\n",
      "[ episode 492 ][ timestamp 6 ] state=[ 0.0203973   0.96058049 -0.06850257 -1.53807765], action=1, reward=1.0, next_state=[ 0.03960891  1.15645667 -0.09926413 -1.85132682]\n",
      "[ Experience replay ] starts\n",
      "[ episode 492 ][ timestamp 7 ] state=[ 0.03960891  1.15645667 -0.09926413 -1.85132682], action=0, reward=1.0, next_state=[ 0.06273804  0.96255694 -0.13629066 -1.59104473]\n",
      "[ Experience replay ] starts\n",
      "[ episode 492 ][ timestamp 8 ] state=[ 0.06273804  0.96255694 -0.13629066 -1.59104473], action=0, reward=1.0, next_state=[ 0.08198918  0.76929099 -0.16811156 -1.34377961]\n",
      "[ Experience replay ] starts\n",
      "[ episode 492 ][ timestamp 9 ] state=[ 0.08198918  0.76929099 -0.16811156 -1.34377961], action=0, reward=1.0, next_state=[ 0.097375    0.57663472 -0.19498715 -1.10806149]\n",
      "[ Experience replay ] starts\n",
      "[ episode 492 ][ timestamp 10 ] state=[ 0.097375    0.57663472 -0.19498715 -1.10806149], action=0, reward=-1.0, next_state=[ 0.10890769  0.38453442 -0.21714838 -0.88233509]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 492: Exploration_rate=0.01. Score=10.\n",
      "[ episode 493 ] state=[-0.03108555  0.04457828 -0.00598915 -0.02032313]\n",
      "[ episode 493 ][ timestamp 1 ] state=[-0.03108555  0.04457828 -0.00598915 -0.02032313], action=1, reward=1.0, next_state=[-0.03019398  0.2397856  -0.00639561 -0.31488967]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 2 ] state=[-0.03019398  0.2397856  -0.00639561 -0.31488967], action=1, reward=1.0, next_state=[-0.02539827  0.43499807 -0.0126934  -0.60958268]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 3 ] state=[-0.02539827  0.43499807 -0.0126934  -0.60958268], action=1, reward=1.0, next_state=[-0.01669831  0.63029514 -0.02488506 -0.90623644]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 4 ] state=[-0.01669831  0.63029514 -0.02488506 -0.90623644], action=1, reward=1.0, next_state=[-0.0040924   0.82574504 -0.04300978 -1.20663597]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 5 ] state=[-0.0040924   0.82574504 -0.04300978 -1.20663597], action=1, reward=1.0, next_state=[ 0.0124225   1.02139551 -0.0671425  -1.51248126]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 6 ] state=[ 0.0124225   1.02139551 -0.0671425  -1.51248126], action=0, reward=1.0, next_state=[ 0.03285041  0.82714784 -0.09739213 -1.24149133]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 7 ] state=[ 0.03285041  0.82714784 -0.09739213 -1.24149133], action=0, reward=1.0, next_state=[ 0.04939336  0.63340164 -0.12222196 -0.98083729]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 8 ] state=[ 0.04939336  0.63340164 -0.12222196 -0.98083729], action=0, reward=1.0, next_state=[ 0.0620614   0.44011092 -0.1418387  -0.72890794]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 9 ] state=[ 0.0620614   0.44011092 -0.1418387  -0.72890794], action=0, reward=1.0, next_state=[ 0.07086361  0.24720477 -0.15641686 -0.48401542]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 10 ] state=[ 0.07086361  0.24720477 -0.15641686 -0.48401542], action=0, reward=1.0, next_state=[ 0.07580771  0.05459606 -0.16609717 -0.24442873]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 11 ] state=[ 0.07580771  0.05459606 -0.16609717 -0.24442873], action=0, reward=1.0, next_state=[ 0.07689963 -0.13781218 -0.17098574 -0.00839673]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 12 ] state=[ 0.07689963 -0.13781218 -0.17098574 -0.00839673], action=1, reward=1.0, next_state=[ 0.07414339  0.05929686 -0.17115368 -0.349774  ]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 493 ][ timestamp 13 ] state=[ 0.07414339  0.05929686 -0.17115368 -0.349774  ], action=1, reward=1.0, next_state=[ 0.07532932  0.25638708 -0.17814916 -0.69116365]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 14 ] state=[ 0.07532932  0.25638708 -0.17814916 -0.69116365], action=0, reward=1.0, next_state=[ 0.08045707  0.06412552 -0.19197243 -0.45943483]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 15 ] state=[ 0.08045707  0.06412552 -0.19197243 -0.45943483], action=0, reward=1.0, next_state=[ 0.08173958 -0.12783838 -0.20116113 -0.23287247]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 16 ] state=[ 0.08173958 -0.12783838 -0.20116113 -0.23287247], action=0, reward=1.0, next_state=[ 0.07918281 -0.31960298 -0.20581858 -0.00976921]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 17 ] state=[ 0.07918281 -0.31960298 -0.20581858 -0.00976921], action=0, reward=1.0, next_state=[ 0.07279075 -0.51127012 -0.20601396  0.21157916]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 18 ] state=[ 0.07279075 -0.51127012 -0.20601396  0.21157916], action=0, reward=1.0, next_state=[ 0.06256535 -0.70294249 -0.20178238  0.4328675 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 19 ] state=[ 0.06256535 -0.70294249 -0.20178238  0.4328675 ], action=1, reward=1.0, next_state=[ 0.0485065  -0.50562116 -0.19312503  0.08396846]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 20 ] state=[ 0.0485065  -0.50562116 -0.19312503  0.08396846], action=1, reward=1.0, next_state=[ 0.03839407 -0.30833095 -0.19144566 -0.26289167]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 21 ] state=[ 0.03839407 -0.30833095 -0.19144566 -0.26289167], action=0, reward=1.0, next_state=[ 0.03222746 -0.50027821 -0.19670349 -0.03617288]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 22 ] state=[ 0.03222746 -0.50027821 -0.19670349 -0.03617288], action=0, reward=1.0, next_state=[ 0.02222189 -0.69211545 -0.19742695  0.18857534]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 23 ] state=[ 0.02222189 -0.69211545 -0.19742695  0.18857534], action=1, reward=1.0, next_state=[ 0.00837958 -0.49479739 -0.19365544 -0.15931945]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 24 ] state=[ 0.00837958 -0.49479739 -0.19365544 -0.15931945], action=0, reward=1.0, next_state=[-0.00151637 -0.68669569 -0.19684183  0.06656786]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 25 ] state=[-0.00151637 -0.68669569 -0.19684183  0.06656786], action=0, reward=1.0, next_state=[-0.01525028 -0.87853096 -0.19551047  0.2912655 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 26 ] state=[-0.01525028 -0.87853096 -0.19551047  0.2912655 ], action=0, reward=1.0, next_state=[-0.0328209  -1.07040614 -0.18968516  0.51648045]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 27 ] state=[-0.0328209  -1.07040614 -0.18968516  0.51648045], action=1, reward=1.0, next_state=[-0.05422902 -0.87319069 -0.17935556  0.17052965]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 28 ] state=[-0.05422902 -0.87319069 -0.17935556  0.17052965], action=0, reward=1.0, next_state=[-0.07169284 -1.06535281 -0.17594496  0.40170081]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 29 ] state=[-0.07169284 -1.06535281 -0.17594496  0.40170081], action=1, reward=1.0, next_state=[-0.09299989 -0.86822878 -0.16791095  0.05911834]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 30 ] state=[-0.09299989 -0.86822878 -0.16791095  0.05911834], action=1, reward=1.0, next_state=[-0.11036447 -0.67114709 -0.16672858 -0.28148075]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 31 ] state=[-0.11036447 -0.67114709 -0.16672858 -0.28148075], action=1, reward=1.0, next_state=[-0.12378741 -0.47408803 -0.17235819 -0.62176182]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 32 ] state=[-0.12378741 -0.47408803 -0.17235819 -0.62176182], action=1, reward=1.0, next_state=[-0.13326917 -0.27703168 -0.18479343 -0.96338947]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 33 ] state=[-0.13326917 -0.27703168 -0.18479343 -0.96338947], action=0, reward=1.0, next_state=[-0.1388098  -0.46925486 -0.20406122 -0.73398441]\n",
      "[ Experience replay ] starts\n",
      "[ episode 493 ][ timestamp 34 ] state=[-0.1388098  -0.46925486 -0.20406122 -0.73398441], action=0, reward=-1.0, next_state=[-0.1481949  -0.661061   -0.21874091 -0.51182317]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 493: Exploration_rate=0.01. Score=34.\n",
      "[ episode 494 ] state=[ 0.00297802 -0.03604983  0.02846389  0.02567089]\n",
      "[ episode 494 ][ timestamp 1 ] state=[ 0.00297802 -0.03604983  0.02846389  0.02567089], action=1, reward=1.0, next_state=[ 0.00225702  0.15865261  0.02897731 -0.25789721]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 2 ] state=[ 0.00225702  0.15865261  0.02897731 -0.25789721], action=1, reward=1.0, next_state=[ 0.00543008  0.35334914  0.02381936 -0.54130126]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 3 ] state=[ 0.00543008  0.35334914  0.02381936 -0.54130126], action=0, reward=1.0, next_state=[ 0.01249706  0.15790063  0.01299334 -0.24120944]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 4 ] state=[ 0.01249706  0.15790063  0.01299334 -0.24120944], action=0, reward=1.0, next_state=[ 0.01565507 -0.03740449  0.00816915  0.05554346]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 5 ] state=[ 0.01565507 -0.03740449  0.00816915  0.05554346], action=1, reward=1.0, next_state=[ 0.01490698  0.15759938  0.00928002 -0.23455089]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 6 ] state=[ 0.01490698  0.15759938  0.00928002 -0.23455089], action=0, reward=1.0, next_state=[ 0.01805897 -0.03765393  0.004589    0.06104474]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 7 ] state=[ 0.01805897 -0.03765393  0.004589    0.06104474], action=0, reward=1.0, next_state=[ 0.01730589 -0.23284137  0.0058099   0.35517199]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 8 ] state=[ 0.01730589 -0.23284137  0.0058099   0.35517199], action=1, reward=1.0, next_state=[ 0.01264906 -0.03780251  0.01291334  0.06432673]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 9 ] state=[ 0.01264906 -0.03780251  0.01291334  0.06432673], action=0, reward=1.0, next_state=[ 0.01189301 -0.2331072   0.01419987  0.36105576]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 10 ] state=[ 0.01189301 -0.2331072   0.01419987  0.36105576], action=1, reward=1.0, next_state=[ 0.00723087 -0.03818994  0.02142099  0.07288397]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 11 ] state=[ 0.00723087 -0.03818994  0.02142099  0.07288397], action=0, reward=1.0, next_state=[ 0.00646707 -0.23361233  0.02287867  0.37224759]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 12 ] state=[ 0.00646707 -0.23361233  0.02287867  0.37224759], action=1, reward=1.0, next_state=[ 0.00179482 -0.03882275  0.03032362  0.08686542]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 13 ] state=[ 0.00179482 -0.03882275  0.03032362  0.08686542], action=0, reward=1.0, next_state=[ 0.00101837 -0.23436594  0.03206093  0.38895915]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 14 ] state=[ 0.00101837 -0.23436594  0.03206093  0.38895915], action=0, reward=1.0, next_state=[-0.00366895 -0.42992794  0.03984011  0.69157569]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 15 ] state=[-0.00366895 -0.42992794  0.03984011  0.69157569], action=1, reward=1.0, next_state=[-0.01226751 -0.23538075  0.05367162  0.41169636]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 16 ] state=[-0.01226751 -0.23538075  0.05367162  0.41169636], action=1, reward=1.0, next_state=[-0.01697512 -0.0410591   0.06190555  0.1364055 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 17 ] state=[-0.01697512 -0.0410591   0.06190555  0.1364055 ], action=0, reward=1.0, next_state=[-0.01779631 -0.2370106   0.06463366  0.44795833]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 18 ] state=[-0.01779631 -0.2370106   0.06463366  0.44795833], action=1, reward=1.0, next_state=[-0.02253652 -0.04285963  0.07359283  0.17632903]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 19 ] state=[-0.02253652 -0.04285963  0.07359283  0.17632903], action=0, reward=1.0, next_state=[-0.02339371 -0.23895341  0.07711941  0.4912903 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 20 ] state=[-0.02339371 -0.23895341  0.07711941  0.4912903 ], action=1, reward=1.0, next_state=[-0.02817278 -0.04499917  0.08694521  0.2238743 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 21 ] state=[-0.02817278 -0.04499917  0.08694521  0.2238743 ], action=1, reward=1.0, next_state=[-0.02907276  0.14877946  0.0914227  -0.04016599]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 494 ][ timestamp 22 ] state=[-0.02907276  0.14877946  0.0914227  -0.04016599], action=1, reward=1.0, next_state=[-0.02609717  0.34247958  0.09061938 -0.30266195]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 23 ] state=[-0.02609717  0.34247958  0.09061938 -0.30266195], action=1, reward=1.0, next_state=[-0.01924758  0.53620103  0.08456614 -0.56544617]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 24 ] state=[-0.01924758  0.53620103  0.08456614 -0.56544617], action=0, reward=1.0, next_state=[-0.00852356  0.34000082  0.07325722 -0.24736474]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 25 ] state=[-0.00852356  0.34000082  0.07325722 -0.24736474], action=1, reward=1.0, next_state=[-0.00172354  0.53400423  0.06830992 -0.51607099]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 26 ] state=[-0.00172354  0.53400423  0.06830992 -0.51607099], action=1, reward=1.0, next_state=[ 0.00895654  0.72810113  0.0579885  -0.78646983]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 27 ] state=[ 0.00895654  0.72810113  0.0579885  -0.78646983], action=1, reward=1.0, next_state=[ 0.02351856  0.92238053  0.04225911 -1.06036002]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 28 ] state=[ 0.02351856  0.92238053  0.04225911 -1.06036002], action=0, reward=1.0, next_state=[ 0.04196617  0.72672514  0.0210519  -0.75471847]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 29 ] state=[ 0.04196617  0.72672514  0.0210519  -0.75471847], action=0, reward=1.0, next_state=[ 0.05650068  0.53131939  0.00595754 -0.455486  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 30 ] state=[ 0.05650068  0.53131939  0.00595754 -0.455486  ], action=1, reward=1.0, next_state=[ 0.06712706  0.72635661 -0.00315218 -0.74628512]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 31 ] state=[ 0.06712706  0.72635661 -0.00315218 -0.74628512], action=0, reward=1.0, next_state=[ 0.0816542   0.53127829 -0.01807789 -0.45459584]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 32 ] state=[ 0.0816542   0.53127829 -0.01807789 -0.45459584], action=0, reward=1.0, next_state=[ 0.09227976  0.33641656 -0.0271698  -0.16766562]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 33 ] state=[ 0.09227976  0.33641656 -0.0271698  -0.16766562], action=0, reward=1.0, next_state=[ 0.09900809  0.14169384 -0.03052312  0.11632372]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 34 ] state=[ 0.09900809  0.14169384 -0.03052312  0.11632372], action=0, reward=1.0, next_state=[ 0.10184197 -0.05297777 -0.02819664  0.39922272]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 35 ] state=[ 0.10184197 -0.05297777 -0.02819664  0.39922272], action=1, reward=1.0, next_state=[ 0.10078241  0.14253259 -0.02021219  0.09778503]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 36 ] state=[ 0.10078241  0.14253259 -0.02021219  0.09778503], action=1, reward=1.0, next_state=[ 0.10363307  0.33793831 -0.01825649 -0.20120565]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 37 ] state=[ 0.10363307  0.33793831 -0.01825649 -0.20120565], action=1, reward=1.0, next_state=[ 0.11039183  0.53331654 -0.0222806  -0.49959127]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 38 ] state=[ 0.11039183  0.53331654 -0.0222806  -0.49959127], action=1, reward=1.0, next_state=[ 0.12105816  0.7287454  -0.03227243 -0.79921175]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 39 ] state=[ 0.12105816  0.7287454  -0.03227243 -0.79921175], action=1, reward=1.0, next_state=[ 0.13563307  0.92429486 -0.04825666 -1.10186965]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 40 ] state=[ 0.13563307  0.92429486 -0.04825666 -1.10186965], action=1, reward=1.0, next_state=[ 0.15411897  1.12001735 -0.07029405 -1.40929356]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 41 ] state=[ 0.15411897  1.12001735 -0.07029405 -1.40929356], action=1, reward=1.0, next_state=[ 0.17651932  1.31593729 -0.09847992 -1.72309714]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 42 ] state=[ 0.17651932  1.31593729 -0.09847992 -1.72309714], action=1, reward=1.0, next_state=[ 0.20283806  1.51203896 -0.13294187 -2.04473073]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 43 ] state=[ 0.20283806  1.51203896 -0.13294187 -2.04473073], action=1, reward=1.0, next_state=[ 0.23307884  1.70825219 -0.17383648 -2.37542346]\n",
      "[ Experience replay ] starts\n",
      "[ episode 494 ][ timestamp 44 ] state=[ 0.23307884  1.70825219 -0.17383648 -2.37542346], action=1, reward=-1.0, next_state=[ 0.26724388  1.90443569 -0.22134495 -2.71611443]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 494: Exploration_rate=0.01. Score=44.\n",
      "[ episode 495 ] state=[ 0.04026722 -0.01149839  0.03924938  0.01519607]\n",
      "[ episode 495 ][ timestamp 1 ] state=[ 0.04026722 -0.01149839  0.03924938  0.01519607], action=1, reward=1.0, next_state=[ 0.04003726  0.18303934  0.0395533  -0.26484943]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 2 ] state=[ 0.04003726  0.18303934  0.0395533  -0.26484943], action=1, reward=1.0, next_state=[ 0.04369804  0.37757508  0.03425631 -0.54479917]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 3 ] state=[ 0.04369804  0.37757508  0.03425631 -0.54479917], action=0, reward=1.0, next_state=[ 0.05124954  0.18198893  0.02336033 -0.24152268]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 4 ] state=[ 0.05124954  0.18198893  0.02336033 -0.24152268], action=0, reward=1.0, next_state=[ 0.05488932 -0.01345879  0.01852988  0.05843622]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 5 ] state=[ 0.05488932 -0.01345879  0.01852988  0.05843622], action=0, reward=1.0, next_state=[ 0.05462015 -0.20884145  0.0196986   0.35690738]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 6 ] state=[ 0.05462015 -0.20884145  0.0196986   0.35690738], action=0, reward=1.0, next_state=[ 0.05044332 -0.40423784  0.02683675  0.65573611]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 7 ] state=[ 0.05044332 -0.40423784  0.02683675  0.65573611], action=1, reward=1.0, next_state=[ 0.04235856 -0.20949959  0.03995147  0.37162297]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 8 ] state=[ 0.04235856 -0.20949959  0.03995147  0.37162297], action=1, reward=1.0, next_state=[ 0.03816857 -0.01496732  0.04738393  0.09180001]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 9 ] state=[ 0.03816857 -0.01496732  0.04738393  0.09180001], action=1, reward=1.0, next_state=[ 0.03786922  0.17944457  0.04921993 -0.18556484]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 10 ] state=[ 0.03786922  0.17944457  0.04921993 -0.18556484], action=1, reward=1.0, next_state=[ 0.04145811  0.373829    0.04550863 -0.46232355]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 11 ] state=[ 0.04145811  0.373829    0.04550863 -0.46232355], action=0, reward=1.0, next_state=[ 0.04893469  0.17809441  0.03626216 -0.15565074]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 12 ] state=[ 0.04893469  0.17809441  0.03626216 -0.15565074], action=0, reward=1.0, next_state=[ 0.05249658 -0.01752746  0.03314915  0.14824792]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 13 ] state=[ 0.05249658 -0.01752746  0.03314915  0.14824792], action=0, reward=1.0, next_state=[ 0.05214603 -0.21310804  0.03611411  0.45120168]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 14 ] state=[ 0.05214603 -0.21310804  0.03611411  0.45120168], action=1, reward=1.0, next_state=[ 0.04788387 -0.01851496  0.04513814  0.17011762]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 15 ] state=[ 0.04788387 -0.01851496  0.04513814  0.17011762], action=1, reward=1.0, next_state=[ 0.04751357  0.17593283  0.04854049 -0.10799087]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 16 ] state=[ 0.04751357  0.17593283  0.04854049 -0.10799087], action=1, reward=1.0, next_state=[ 0.05103223  0.37032679  0.04638067 -0.38497307]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 17 ] state=[ 0.05103223  0.37032679  0.04638067 -0.38497307], action=0, reward=1.0, next_state=[ 0.05843876  0.17457812  0.03868121 -0.07803479]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 18 ] state=[ 0.05843876  0.17457812  0.03868121 -0.07803479], action=1, reward=1.0, next_state=[ 0.06193033  0.36912482  0.03712052 -0.35826711]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 19 ] state=[ 0.06193033  0.36912482  0.03712052 -0.35826711], action=1, reward=1.0, next_state=[ 0.06931282  0.56369994  0.02995517 -0.6390178 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 20 ] state=[ 0.06931282  0.56369994  0.02995517 -0.6390178 ], action=1, reward=1.0, next_state=[ 0.08058682  0.7583917   0.01717482 -0.92211892]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 21 ] state=[ 0.08058682  0.7583917   0.01717482 -0.92211892], action=1, reward=1.0, next_state=[ 0.09575466  0.95327743 -0.00126756 -1.20935525]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 22 ] state=[ 0.09575466  0.95327743 -0.00126756 -1.20935525], action=1, reward=1.0, next_state=[ 0.11482021  1.14841573 -0.02545466 -1.50243513]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 23 ] state=[ 0.11482021  1.14841573 -0.02545466 -1.50243513], action=1, reward=1.0, next_state=[ 0.13778852  1.34383726 -0.05550337 -1.80295533]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 24 ] state=[ 0.13778852  1.34383726 -0.05550337 -1.80295533], action=1, reward=1.0, next_state=[ 0.16466526  1.53953358 -0.09156247 -2.11235739]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 495 ][ timestamp 25 ] state=[ 0.16466526  1.53953358 -0.09156247 -2.11235739], action=1, reward=1.0, next_state=[ 0.19545594  1.73544347 -0.13380962 -2.43187301]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 26 ] state=[ 0.19545594  1.73544347 -0.13380962 -2.43187301], action=1, reward=1.0, next_state=[ 0.23016481  1.93143662 -0.18244708 -2.76245747]\n",
      "[ Experience replay ] starts\n",
      "[ episode 495 ][ timestamp 27 ] state=[ 0.23016481  1.93143662 -0.18244708 -2.76245747], action=1, reward=-1.0, next_state=[ 0.26879354  2.12729488 -0.23769623 -3.10471111]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 495: Exploration_rate=0.01. Score=27.\n",
      "[ episode 496 ] state=[-0.03093138 -0.0454347   0.01369762  0.03757432]\n",
      "[ episode 496 ][ timestamp 1 ] state=[-0.03093138 -0.0454347   0.01369762  0.03757432], action=1, reward=1.0, next_state=[-0.03184008  0.14948817  0.01444911 -0.25075559]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 2 ] state=[-0.03184008  0.14948817  0.01444911 -0.25075559], action=1, reward=1.0, next_state=[-0.02885031  0.34440084  0.009434   -0.53884618]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 3 ] state=[-0.02885031  0.34440084  0.009434   -0.53884618], action=1, reward=1.0, next_state=[-0.0219623   0.5393889  -0.00134293 -0.82854171]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 4 ] state=[-0.0219623   0.5393889  -0.00134293 -0.82854171], action=0, reward=1.0, next_state=[-0.01117452  0.34428534 -0.01791376 -0.53628144]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 5 ] state=[-0.01117452  0.34428534 -0.01791376 -0.53628144], action=0, reward=1.0, next_state=[-0.00428881  0.14941979 -0.02863939 -0.24929638]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 6 ] state=[-0.00428881  0.14941979 -0.02863939 -0.24929638], action=1, reward=1.0, next_state=[-0.00130042  0.34493878 -0.03362532 -0.55087343]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 7 ] state=[-0.00130042  0.34493878 -0.03362532 -0.55087343], action=1, reward=1.0, next_state=[ 0.00559836  0.54051648 -0.04464278 -0.85395812]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 8 ] state=[ 0.00559836  0.54051648 -0.04464278 -0.85395812], action=1, reward=1.0, next_state=[ 0.01640869  0.73621755 -0.06172195 -1.16033788]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 9 ] state=[ 0.01640869  0.73621755 -0.06172195 -1.16033788], action=0, reward=1.0, next_state=[ 0.03113304  0.5419516  -0.0849287  -0.88762856]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 10 ] state=[ 0.03113304  0.5419516  -0.0849287  -0.88762856], action=0, reward=1.0, next_state=[ 0.04197207  0.34807867 -0.10268128 -0.62280636]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 11 ] state=[ 0.04197207  0.34807867 -0.10268128 -0.62280636], action=0, reward=1.0, next_state=[ 0.04893365  0.154529   -0.1151374  -0.36414629]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 12 ] state=[ 0.04893365  0.154529   -0.1151374  -0.36414629], action=0, reward=1.0, next_state=[ 0.05202423 -0.03878445 -0.12242033 -0.10987167]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 13 ] state=[ 0.05202423 -0.03878445 -0.12242033 -0.10987167], action=1, reward=1.0, next_state=[ 0.05124854  0.1578597  -0.12461776 -0.43853211]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 14 ] state=[ 0.05124854  0.1578597  -0.12461776 -0.43853211], action=1, reward=1.0, next_state=[ 0.05440573  0.35450479 -0.1333884  -0.76775521]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 15 ] state=[ 0.05440573  0.35450479 -0.1333884  -0.76775521], action=0, reward=1.0, next_state=[ 0.06149583  0.16144659 -0.14874351 -0.51984032]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 16 ] state=[ 0.06149583  0.16144659 -0.14874351 -0.51984032], action=0, reward=1.0, next_state=[ 0.06472476 -0.03130282 -0.15914032 -0.27747822]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 17 ] state=[ 0.06472476 -0.03130282 -0.15914032 -0.27747822], action=1, reward=1.0, next_state=[ 0.0640987   0.16568922 -0.16468988 -0.61582248]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 18 ] state=[ 0.0640987   0.16568922 -0.16468988 -0.61582248], action=0, reward=1.0, next_state=[ 0.06741249 -0.02679541 -0.17700633 -0.37920247]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 19 ] state=[ 0.06741249 -0.02679541 -0.17700633 -0.37920247], action=0, reward=1.0, next_state=[ 0.06687658 -0.21902013 -0.18459038 -0.14713913]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 20 ] state=[ 0.06687658 -0.21902013 -0.18459038 -0.14713913], action=0, reward=1.0, next_state=[ 0.06249618 -0.41108495 -0.18753316  0.08210186]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 21 ] state=[ 0.06249618 -0.41108495 -0.18753316  0.08210186], action=1, reward=1.0, next_state=[ 0.05427448 -0.21383899 -0.18589112 -0.26339179]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 22 ] state=[ 0.05427448 -0.21383899 -0.18589112 -0.26339179], action=0, reward=1.0, next_state=[ 0.0499977  -0.40588847 -0.19115896 -0.03461831]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 23 ] state=[ 0.0499977  -0.40588847 -0.19115896 -0.03461831], action=0, reward=1.0, next_state=[ 0.04187993 -0.59782846 -0.19185133  0.19218823]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 24 ] state=[ 0.04187993 -0.59782846 -0.19185133  0.19218823], action=0, reward=1.0, next_state=[ 0.02992336 -0.78976227 -0.18800756  0.41874789]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 25 ] state=[ 0.02992336 -0.78976227 -0.18800756  0.41874789], action=1, reward=1.0, next_state=[ 0.01412811 -0.59254317 -0.1796326   0.073183  ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 26 ] state=[ 0.01412811 -0.59254317 -0.1796326   0.073183  ], action=0, reward=1.0, next_state=[ 0.00227725 -0.7846961  -0.17816894  0.30424618]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 27 ] state=[ 0.00227725 -0.7846961  -0.17816894  0.30424618], action=0, reward=1.0, next_state=[-0.01341667 -0.97689087 -0.17208402  0.53586966]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 28 ] state=[-0.01341667 -0.97689087 -0.17208402  0.53586966], action=0, reward=1.0, next_state=[-0.03295449 -1.16922863 -0.16136663  0.7697717 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 29 ] state=[-0.03295449 -1.16922863 -0.16136663  0.7697717 ], action=1, reward=1.0, next_state=[-0.05633906 -0.97229726 -0.14597119  0.43097609]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 30 ] state=[-0.05633906 -0.97229726 -0.14597119  0.43097609], action=1, reward=1.0, next_state=[-0.07578501 -0.77544258 -0.13735167  0.09607107]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 31 ] state=[-0.07578501 -0.77544258 -0.13735167  0.09607107], action=1, reward=1.0, next_state=[-0.09129386 -0.57864667 -0.13543025 -0.23659722]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 32 ] state=[-0.09129386 -0.57864667 -0.13543025 -0.23659722], action=0, reward=1.0, next_state=[-0.10286679 -0.77160002 -0.14016219  0.01048769]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 33 ] state=[-0.10286679 -0.77160002 -0.14016219  0.01048769], action=0, reward=1.0, next_state=[-0.11829879 -0.96446265 -0.13995244  0.25587174]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 34 ] state=[-0.11829879 -0.96446265 -0.13995244  0.25587174], action=1, reward=1.0, next_state=[-0.13758805 -0.76764883 -0.13483501 -0.07747434]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 35 ] state=[-0.13758805 -0.76764883 -0.13483501 -0.07747434], action=0, reward=1.0, next_state=[-0.15294102 -0.96060605 -0.13638449  0.16981295]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 36 ] state=[-0.15294102 -0.96060605 -0.13638449  0.16981295], action=1, reward=1.0, next_state=[-0.17215314 -0.76382231 -0.13298823 -0.16259453]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 37 ] state=[-0.17215314 -0.76382231 -0.13298823 -0.16259453], action=0, reward=1.0, next_state=[-0.18742959 -0.95681456 -0.13624012  0.0853543 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 38 ] state=[-0.18742959 -0.95681456 -0.13624012  0.0853543 ], action=1, reward=1.0, next_state=[-0.20656588 -0.7600294  -0.13453304 -0.24701903]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 39 ] state=[-0.20656588 -0.7600294  -0.13453304 -0.24701903], action=0, reward=1.0, next_state=[-2.21766469e-01 -9.52999109e-01 -1.39473419e-01  3.86543004e-04]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 40 ] state=[-2.21766469e-01 -9.52999109e-01 -1.39473419e-01  3.86543004e-04], action=1, reward=1.0, next_state=[-0.24082645 -0.75618108 -0.13946569 -0.33284602]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 41 ] state=[-0.24082645 -0.75618108 -0.13946569 -0.33284602], action=1, reward=1.0, next_state=[-0.25595007 -0.55937813 -0.14612261 -0.66605425]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 496 ][ timestamp 42 ] state=[-0.25595007 -0.55937813 -0.14612261 -0.66605425], action=1, reward=1.0, next_state=[-0.26713764 -0.3625586  -0.15944369 -1.00094464]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 43 ] state=[-0.26713764 -0.3625586  -0.15944369 -1.00094464], action=1, reward=1.0, next_state=[-0.27438881 -0.16570683 -0.17946259 -1.33915501]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 44 ] state=[-0.27438881 -0.16570683 -0.17946259 -1.33915501], action=0, reward=1.0, next_state=[-0.27770294 -0.35817352 -0.20624569 -1.10757081]\n",
      "[ Experience replay ] starts\n",
      "[ episode 496 ][ timestamp 45 ] state=[-0.27770294 -0.35817352 -0.20624569 -1.10757081], action=0, reward=-1.0, next_state=[-0.28486641 -0.55007698 -0.2283971  -0.88602349]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 496: Exploration_rate=0.01. Score=45.\n",
      "[ episode 497 ] state=[ 0.03105517 -0.00774151 -0.00283979  0.02913354]\n",
      "[ episode 497 ][ timestamp 1 ] state=[ 0.03105517 -0.00774151 -0.00283979  0.02913354], action=1, reward=1.0, next_state=[ 0.03090034  0.18742105 -0.00225711 -0.26444401]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 2 ] state=[ 0.03090034  0.18742105 -0.00225711 -0.26444401], action=0, reward=1.0, next_state=[ 0.03464876 -0.00766861 -0.00754599  0.02752615]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 3 ] state=[ 0.03464876 -0.00766861 -0.00754599  0.02752615], action=0, reward=1.0, next_state=[ 0.03449539 -0.20268154 -0.00699547  0.31781871]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 4 ] state=[ 0.03449539 -0.20268154 -0.00699547  0.31781871], action=1, reward=1.0, next_state=[ 0.03044176 -0.00746065 -0.0006391   0.02293789]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 5 ] state=[ 0.03044176 -0.00746065 -0.0006391   0.02293789], action=0, reward=1.0, next_state=[ 3.02925447e-02 -2.02573434e-01 -1.80339404e-04  3.15419110e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 6 ] state=[ 3.02925447e-02 -2.02573434e-01 -1.80339404e-04  3.15419110e-01], action=1, reward=1.0, next_state=[ 0.02624108 -0.00744891  0.00612804  0.02267932]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 7 ] state=[ 0.02624108 -0.00744891  0.00612804  0.02267932], action=1, reward=1.0, next_state=[ 0.0260921   0.18758462  0.00658163 -0.26806386]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 8 ] state=[ 0.0260921   0.18758462  0.00658163 -0.26806386], action=0, reward=1.0, next_state=[ 0.02984379 -0.00763064  0.00122035  0.02668767]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 9 ] state=[ 0.02984379 -0.00763064  0.00122035  0.02668767], action=1, reward=1.0, next_state=[ 0.02969118  0.18747379  0.00175411 -0.26560997]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 10 ] state=[ 0.02969118  0.18747379  0.00175411 -0.26560997], action=0, reward=1.0, next_state=[ 0.03344065 -0.00767315 -0.00355809  0.0276257 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 11 ] state=[ 0.03344065 -0.00767315 -0.00355809  0.0276257 ], action=1, reward=1.0, next_state=[ 0.03328719  0.18749964 -0.00300558 -0.26617772]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 12 ] state=[ 0.03328719  0.18749964 -0.00300558 -0.26617772], action=0, reward=1.0, next_state=[ 0.03703718 -0.00757928 -0.00832913  0.02555571]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 13 ] state=[ 0.03703718 -0.00757928 -0.00832913  0.02555571], action=1, reward=1.0, next_state=[ 0.0368856   0.18766112 -0.00781802 -0.26974347]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 14 ] state=[ 0.0368856   0.18766112 -0.00781802 -0.26974347], action=0, reward=1.0, next_state=[ 0.04063882 -0.0073484  -0.01321289  0.02046339]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 15 ] state=[ 0.04063882 -0.0073484  -0.01321289  0.02046339], action=1, reward=1.0, next_state=[ 0.04049185  0.18796052 -0.01280362 -0.2763589 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 16 ] state=[ 0.04049185  0.18796052 -0.01280362 -0.2763589 ], action=0, reward=1.0, next_state=[ 0.04425106 -0.00697644 -0.0183308   0.01225842]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 17 ] state=[ 0.04425106 -0.00697644 -0.0183308   0.01225842], action=0, reward=1.0, next_state=[ 0.04411153 -0.20183077 -0.01808563  0.29910186]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 18 ] state=[ 0.04411153 -0.20183077 -0.01808563  0.29910186], action=1, reward=1.0, next_state=[ 0.04007492 -0.00645576 -0.01210359  0.00077038]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 19 ] state=[ 0.04007492 -0.00645576 -0.01210359  0.00077038], action=0, reward=1.0, next_state=[ 0.0399458  -0.20140205 -0.01208819  0.28961003]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 20 ] state=[ 0.0399458  -0.20140205 -0.01208819  0.28961003], action=1, reward=1.0, next_state=[ 0.03591776 -0.00610983 -0.00629599 -0.00686073]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 21 ] state=[ 0.03591776 -0.00610983 -0.00629599 -0.00686073], action=1, reward=1.0, next_state=[ 0.03579557  0.18910184 -0.0064332  -0.30152345]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 22 ] state=[ 0.03579557  0.18910184 -0.0064332  -0.30152345], action=0, reward=1.0, next_state=[ 0.0395776  -0.00592783 -0.01246367 -0.01087635]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 23 ] state=[ 0.0395776  -0.00592783 -0.01246367 -0.01087635], action=1, reward=1.0, next_state=[ 0.03945905  0.18937063 -0.0126812  -0.3074655 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 24 ] state=[ 0.03945905  0.18937063 -0.0126812  -0.3074655 ], action=0, reward=1.0, next_state=[ 0.04324646 -0.00556835 -0.01883051 -0.01880872]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 25 ] state=[ 0.04324646 -0.00556835 -0.01883051 -0.01880872], action=1, reward=1.0, next_state=[ 0.04313509  0.18981852 -0.01920668 -0.3173729 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 26 ] state=[ 0.04313509  0.18981852 -0.01920668 -0.3173729 ], action=0, reward=1.0, next_state=[ 0.04693146 -0.00502468 -0.02555414 -0.03080843]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 27 ] state=[ 0.04693146 -0.00502468 -0.02555414 -0.03080843], action=1, reward=1.0, next_state=[ 0.04683097  0.19045424 -0.02617031 -0.33144317]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 28 ] state=[ 0.04683097  0.19045424 -0.02617031 -0.33144317], action=0, reward=1.0, next_state=[ 0.05064005 -0.00428561 -0.03279917 -0.04712661]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 29 ] state=[ 0.05064005 -0.00428561 -0.03279917 -0.04712661], action=0, reward=1.0, next_state=[ 0.05055434 -0.19892227 -0.0337417   0.23503012]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 30 ] state=[ 0.05055434 -0.19892227 -0.0337417   0.23503012], action=1, reward=1.0, next_state=[ 0.04657589 -0.00333488 -0.0290411  -0.06810215]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 31 ] state=[ 0.04657589 -0.00333488 -0.0290411  -0.06810215], action=0, reward=1.0, next_state=[ 0.0465092  -0.1980287  -0.03040314  0.21527855]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 32 ] state=[ 0.0465092  -0.1980287  -0.03040314  0.21527855], action=1, reward=1.0, next_state=[ 0.04254862 -0.00248559 -0.02609757 -0.08683772]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 33 ] state=[ 0.04254862 -0.00248559 -0.02609757 -0.08683772], action=0, reward=1.0, next_state=[ 0.04249891 -0.19722392 -0.02783433  0.1974985 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 34 ] state=[ 0.04249891 -0.19722392 -0.02783433  0.1974985 ], action=1, reward=1.0, next_state=[ 0.03855443 -0.00171513 -0.02388436 -0.10383333]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 35 ] state=[ 0.03855443 -0.00171513 -0.02388436 -0.10383333], action=1, reward=1.0, next_state=[ 0.03852013  0.19374082 -0.02596102 -0.40395496]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 36 ] state=[ 0.03852013  0.19374082 -0.02596102 -0.40395496], action=1, reward=1.0, next_state=[ 0.04239495  0.38922115 -0.03404012 -0.70470834]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 37 ] state=[ 0.04239495  0.38922115 -0.03404012 -0.70470834], action=1, reward=1.0, next_state=[ 0.05017937  0.58479785 -0.04813429 -1.0079093 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 38 ] state=[ 0.05017937  0.58479785 -0.04813429 -1.0079093 ], action=0, reward=1.0, next_state=[ 0.06187533  0.3903504  -0.06829248 -0.73072196]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 497 ][ timestamp 39 ] state=[ 0.06187533  0.3903504  -0.06829248 -0.73072196], action=0, reward=1.0, next_state=[ 0.06968233  0.19623541 -0.08290692 -0.4602906 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 40 ] state=[ 0.06968233  0.19623541 -0.08290692 -0.4602906 ], action=0, reward=1.0, next_state=[ 0.07360704  0.00237718 -0.09211273 -0.19484878]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 41 ] state=[ 0.07360704  0.00237718 -0.09211273 -0.19484878], action=0, reward=1.0, next_state=[ 0.07365459 -0.19131469 -0.0960097   0.06741447]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 42 ] state=[ 0.07365459 -0.19131469 -0.0960097   0.06741447], action=1, reward=1.0, next_state=[ 0.06982829  0.00504328 -0.09466141 -0.25394953]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 43 ] state=[ 0.06982829  0.00504328 -0.09466141 -0.25394953], action=1, reward=1.0, next_state=[ 0.06992916  0.20138042 -0.0997404  -0.57492564]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 44 ] state=[ 0.06992916  0.20138042 -0.0997404  -0.57492564], action=0, reward=1.0, next_state=[ 0.07395677  0.00778777 -0.11123892 -0.31525496]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 45 ] state=[ 0.07395677  0.00778777 -0.11123892 -0.31525496], action=0, reward=1.0, next_state=[ 0.07411252 -0.18558838 -0.11754402 -0.05962036]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 46 ] state=[ 0.07411252 -0.18558838 -0.11754402 -0.05962036], action=0, reward=1.0, next_state=[ 0.07040075 -0.37884599 -0.11873642  0.19378732]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 47 ] state=[ 0.07040075 -0.37884599 -0.11873642  0.19378732], action=1, reward=1.0, next_state=[ 0.06282383 -0.18224335 -0.11486068 -0.1338668 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 48 ] state=[ 0.06282383 -0.18224335 -0.11486068 -0.1338668 ], action=0, reward=1.0, next_state=[ 0.05917897 -0.3755487  -0.11753801  0.12048578]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 49 ] state=[ 0.05917897 -0.3755487  -0.11753801  0.12048578], action=1, reward=1.0, next_state=[ 0.05166799 -0.17895604 -0.1151283  -0.20684524]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 50 ] state=[ 0.05166799 -0.17895604 -0.1151283  -0.20684524], action=1, reward=1.0, next_state=[ 0.04808887  0.01760788 -0.1192652  -0.53351225]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 51 ] state=[ 0.04808887  0.01760788 -0.1192652  -0.53351225], action=0, reward=1.0, next_state=[ 0.04844103 -0.17565265 -0.12993545 -0.28066164]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 52 ] state=[ 0.04844103 -0.17565265 -0.12993545 -0.28066164], action=0, reward=1.0, next_state=[ 0.04492798 -0.36870485 -0.13554868 -0.03161803]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 53 ] state=[ 0.04492798 -0.36870485 -0.13554868 -0.03161803], action=0, reward=1.0, next_state=[ 0.03755388 -0.56164888 -0.13618104  0.21541391]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 54 ] state=[ 0.03755388 -0.56164888 -0.13618104  0.21541391], action=1, reward=1.0, next_state=[ 0.0263209  -0.36486948 -0.13187276 -0.11693602]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 55 ] state=[ 0.0263209  -0.36486948 -0.13187276 -0.11693602], action=1, reward=1.0, next_state=[ 0.01902351 -0.16812878 -0.13421148 -0.44814305]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 56 ] state=[ 0.01902351 -0.16812878 -0.13421148 -0.44814305], action=1, reward=1.0, next_state=[ 0.01566094  0.02861096 -0.14317434 -0.77993861]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 57 ] state=[ 0.01566094  0.02861096 -0.14317434 -0.77993861], action=0, reward=1.0, next_state=[ 0.01623316 -0.16428293 -0.15877312 -0.53550788]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 58 ] state=[ 0.01623316 -0.16428293 -0.15877312 -0.53550788], action=0, reward=1.0, next_state=[ 0.0129475  -0.35685792 -0.16948327 -0.29676214]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 59 ] state=[ 0.0129475  -0.35685792 -0.16948327 -0.29676214], action=1, reward=1.0, next_state=[ 0.00581034 -0.15977637 -0.17541852 -0.63773867]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 60 ] state=[ 0.00581034 -0.15977637 -0.17541852 -0.63773867], action=1, reward=1.0, next_state=[ 0.00261481  0.03730159 -0.18817329 -0.98012791]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 61 ] state=[ 0.00261481  0.03730159 -0.18817329 -0.98012791], action=1, reward=1.0, next_state=[ 0.00336084  0.23437903 -0.20777585 -1.32552277]\n",
      "[ Experience replay ] starts\n",
      "[ episode 497 ][ timestamp 62 ] state=[ 0.00336084  0.23437903 -0.20777585 -1.32552277], action=0, reward=-1.0, next_state=[ 0.00804843  0.04239603 -0.2342863  -1.10438951]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 497: Exploration_rate=0.01. Score=62.\n",
      "[ episode 498 ] state=[-0.00566927 -0.03913795  0.02265929 -0.03673683]\n",
      "[ episode 498 ][ timestamp 1 ] state=[-0.00566927 -0.03913795  0.02265929 -0.03673683], action=1, reward=1.0, next_state=[-0.00645203  0.15565186  0.02192455 -0.32218527]\n",
      "[ Experience replay ] starts\n",
      "[ episode 498 ][ timestamp 2 ] state=[-0.00645203  0.15565186  0.02192455 -0.32218527], action=1, reward=1.0, next_state=[-0.00333899  0.35045485  0.01548085 -0.60787423]\n",
      "[ Experience replay ] starts\n",
      "[ episode 498 ][ timestamp 3 ] state=[-0.00333899  0.35045485  0.01548085 -0.60787423], action=1, reward=1.0, next_state=[ 0.0036701   0.54535698  0.00332336 -0.89564121]\n",
      "[ Experience replay ] starts\n",
      "[ episode 498 ][ timestamp 4 ] state=[ 0.0036701   0.54535698  0.00332336 -0.89564121], action=1, reward=1.0, next_state=[ 0.01457724  0.74043372 -0.01458946 -1.18727763]\n",
      "[ Experience replay ] starts\n",
      "[ episode 498 ][ timestamp 5 ] state=[ 0.01457724  0.74043372 -0.01458946 -1.18727763], action=1, reward=1.0, next_state=[ 0.02938592  0.93574177 -0.03833502 -1.48449767]\n",
      "[ Experience replay ] starts\n",
      "[ episode 498 ][ timestamp 6 ] state=[ 0.02938592  0.93574177 -0.03833502 -1.48449767], action=1, reward=1.0, next_state=[ 0.04810075  1.13130954 -0.06802497 -1.78890155]\n",
      "[ Experience replay ] starts\n",
      "[ episode 498 ][ timestamp 7 ] state=[ 0.04810075  1.13130954 -0.06802497 -1.78890155], action=1, reward=1.0, next_state=[ 0.07072695  1.32712565 -0.103803   -2.1019303 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 498 ][ timestamp 8 ] state=[ 0.07072695  1.32712565 -0.103803   -2.1019303 ], action=0, reward=1.0, next_state=[ 0.09726946  1.13318744 -0.14584161 -1.84305216]\n",
      "[ Experience replay ] starts\n",
      "[ episode 498 ][ timestamp 9 ] state=[ 0.09726946  1.13318744 -0.14584161 -1.84305216], action=0, reward=1.0, next_state=[ 0.11993321  0.93994456 -0.18270265 -1.59899064]\n",
      "[ Experience replay ] starts\n",
      "[ episode 498 ][ timestamp 10 ] state=[ 0.11993321  0.93994456 -0.18270265 -1.59899064], action=0, reward=-1.0, next_state=[ 0.1387321   0.74739679 -0.21468246 -1.3683923 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 498: Exploration_rate=0.01. Score=10.\n",
      "[ episode 499 ] state=[ 0.01903759  0.00611457  0.01988708 -0.03473839]\n",
      "[ episode 499 ][ timestamp 1 ] state=[ 0.01903759  0.00611457  0.01988708 -0.03473839], action=0, reward=1.0, next_state=[ 0.01915989 -0.18928684  0.01919232  0.26415218]\n",
      "[ Experience replay ] starts\n",
      "[ episode 499 ][ timestamp 2 ] state=[ 0.01915989 -0.18928684  0.01919232  0.26415218], action=0, reward=1.0, next_state=[ 0.01537415 -0.3846774   0.02447536  0.56282623]\n",
      "[ Experience replay ] starts\n",
      "[ episode 499 ][ timestamp 3 ] state=[ 0.01537415 -0.3846774   0.02447536  0.56282623], action=1, reward=1.0, next_state=[ 0.0076806  -0.18990729  0.03573189  0.27795361]\n",
      "[ Experience replay ] starts\n",
      "[ episode 499 ][ timestamp 4 ] state=[ 0.0076806  -0.18990729  0.03573189  0.27795361], action=1, reward=1.0, next_state=[ 0.00388245  0.00468717  0.04129096 -0.00324882]\n",
      "[ Experience replay ] starts\n",
      "[ episode 499 ][ timestamp 5 ] state=[ 0.00388245  0.00468717  0.04129096 -0.00324882], action=0, reward=1.0, next_state=[ 0.0039762  -0.19100188  0.04122598  0.30217066]\n",
      "[ Experience replay ] starts\n",
      "[ episode 499 ][ timestamp 6 ] state=[ 0.0039762  -0.19100188  0.04122598  0.30217066], action=0, reward=1.0, next_state=[ 1.56160665e-04 -3.86686410e-01  4.72693944e-02  6.07565056e-01]\n",
      "[ Experience replay ] starts\n",
      "[ episode 499 ][ timestamp 7 ] state=[ 1.56160665e-04 -3.86686410e-01  4.72693944e-02  6.07565056e-01], action=0, reward=1.0, next_state=[-0.00757757 -0.58243627  0.0594207   0.91475389]\n",
      "[ Experience replay ] starts\n",
      "[ episode 499 ][ timestamp 8 ] state=[-0.00757757 -0.58243627  0.0594207   0.91475389], action=0, reward=1.0, next_state=[-0.01922629 -0.77830937  0.07771577  1.22550441]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 499 ][ timestamp 9 ] state=[-0.01922629 -0.77830937  0.07771577  1.22550441], action=0, reward=1.0, next_state=[-0.03479248 -0.97434118  0.10222586  1.54149003]\n",
      "[ Experience replay ] starts\n",
      "[ episode 499 ][ timestamp 10 ] state=[-0.03479248 -0.97434118  0.10222586  1.54149003], action=0, reward=1.0, next_state=[-0.0542793  -1.17053294  0.13305566  1.86424343]\n",
      "[ Experience replay ] starts\n",
      "[ episode 499 ][ timestamp 11 ] state=[-0.0542793  -1.17053294  0.13305566  1.86424343], action=0, reward=1.0, next_state=[-0.07768996 -1.36683805  0.17034053  2.19510147]\n",
      "[ Experience replay ] starts\n",
      "[ episode 499 ][ timestamp 12 ] state=[-0.07768996 -1.36683805  0.17034053  2.19510147], action=0, reward=-1.0, next_state=[-0.10502672 -1.56314629  0.21424256  2.53514038]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 499: Exploration_rate=0.01. Score=12.\n",
      "[ episode 500 ] state=[-0.03948708 -0.01641352 -0.0173079  -0.03202856]\n",
      "[ episode 500 ][ timestamp 1 ] state=[-0.03948708 -0.01641352 -0.0173079  -0.03202856], action=0, reward=1.0, next_state=[-0.03981535 -0.21128304 -0.01794847  0.25514368]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 2 ] state=[-0.03981535 -0.21128304 -0.01794847  0.25514368], action=0, reward=1.0, next_state=[-0.04404101 -0.40614419 -0.0128456   0.54211175]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 3 ] state=[-0.04404101 -0.40614419 -0.0128456   0.54211175], action=1, reward=1.0, next_state=[-0.0521639  -0.21084407 -0.00200336  0.24540925]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 4 ] state=[-0.0521639  -0.21084407 -0.00200336  0.24540925], action=0, reward=1.0, next_state=[-0.05638078 -0.40593735  0.00290482  0.53745959]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 5 ] state=[-0.05638078 -0.40593735  0.00290482  0.53745959], action=1, reward=1.0, next_state=[-0.06449953 -0.21085636  0.01365402  0.24569336]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 6 ] state=[-0.06449953 -0.21085636  0.01365402  0.24569336], action=1, reward=1.0, next_state=[-0.06871665 -0.01593206  0.01856788 -0.04265168]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 7 ] state=[-0.06871665 -0.01593206  0.01856788 -0.04265168], action=1, reward=1.0, next_state=[-0.06903529  0.17891878  0.01771485 -0.32941892]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 8 ] state=[-0.06903529  0.17891878  0.01771485 -0.32941892], action=0, reward=1.0, next_state=[-0.06545692 -0.01645082  0.01112647 -0.03120261]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 9 ] state=[-0.06545692 -0.01645082  0.01112647 -0.03120261], action=1, reward=1.0, next_state=[-0.06578594  0.17850982  0.01050242 -0.32035435]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 10 ] state=[-0.06578594  0.17850982  0.01050242 -0.32035435], action=0, reward=1.0, next_state=[-0.06221574 -0.01676011  0.00409533 -0.02437795]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 11 ] state=[-0.06221574 -0.01676011  0.00409533 -0.02437795], action=1, reward=1.0, next_state=[-0.06255094  0.17830287  0.00360777 -0.31576595]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 12 ] state=[-0.06255094  0.17830287  0.00360777 -0.31576595], action=0, reward=1.0, next_state=[-0.05898488 -0.01687028 -0.00270755 -0.02194744]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 13 ] state=[-0.05898488 -0.01687028 -0.00270755 -0.02194744], action=1, reward=1.0, next_state=[-0.05932229  0.17829039 -0.00314649 -0.3154834 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 14 ] state=[-0.05932229  0.17829039 -0.00314649 -0.3154834 ], action=0, reward=1.0, next_state=[-0.05575648 -0.0167866  -0.00945616 -0.02379443]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 15 ] state=[-0.05575648 -0.0167866  -0.00945616 -0.02379443], action=1, reward=1.0, next_state=[-0.05609221  0.17846968 -0.00993205 -0.31944582]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 16 ] state=[-0.05609221  0.17846968 -0.00993205 -0.31944582], action=0, reward=1.0, next_state=[-0.05252282 -0.01650942 -0.01632097 -0.02991156]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 17 ] state=[-0.05252282 -0.01650942 -0.01632097 -0.02991156], action=1, reward=1.0, next_state=[-0.05285301  0.17884273 -0.0169192  -0.32769892]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 18 ] state=[-0.05285301  0.17884273 -0.0169192  -0.32769892], action=0, reward=1.0, next_state=[-0.04927615 -0.01603431 -0.02347318 -0.0403992 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 19 ] state=[-0.04927615 -0.01603431 -0.02347318 -0.0403992 ], action=1, reward=1.0, next_state=[-0.04959684  0.17941624 -0.02428116 -0.34039475]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 20 ] state=[-0.04959684  0.17941624 -0.02428116 -0.34039475], action=0, reward=1.0, next_state=[-0.04600851 -0.01535197 -0.03108906 -0.05546652]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 21 ] state=[-0.04600851 -0.01535197 -0.03108906 -0.05546652], action=1, reward=1.0, next_state=[-0.04631555  0.18020164 -0.03219839 -0.35779389]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 22 ] state=[-0.04631555  0.18020164 -0.03219839 -0.35779389], action=0, reward=1.0, next_state=[-0.04271152 -0.01444812 -0.03935426 -0.07543528]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 23 ] state=[-0.04271152 -0.01444812 -0.03935426 -0.07543528], action=1, reward=1.0, next_state=[-0.04300048  0.18121527 -0.04086297 -0.38027028]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 24 ] state=[-0.04300048  0.18121527 -0.04086297 -0.38027028], action=1, reward=1.0, next_state=[-0.03937618  0.37689295 -0.04846838 -0.68555215]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 25 ] state=[-0.03937618  0.37689295 -0.04846838 -0.68555215], action=1, reward=1.0, next_state=[-0.03183832  0.57265308 -0.06217942 -0.99309163]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 26 ] state=[-0.03183832  0.57265308 -0.06217942 -0.99309163], action=0, reward=1.0, next_state=[-0.02038526  0.37841566 -0.08204125 -0.72056753]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 27 ] state=[-0.02038526  0.37841566 -0.08204125 -0.72056753], action=0, reward=1.0, next_state=[-0.01281694  0.1845188  -0.0964526  -0.45479357]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 28 ] state=[-0.01281694  0.1845188  -0.0964526  -0.45479357], action=0, reward=1.0, next_state=[-0.00912657 -0.00911647 -0.10554847 -0.19400379]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 29 ] state=[-0.00912657 -0.00911647 -0.10554847 -0.19400379], action=0, reward=1.0, next_state=[-0.0093089  -0.20258267 -0.10942855  0.06360686]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 30 ] state=[-0.0093089  -0.20258267 -0.10942855  0.06360686], action=0, reward=1.0, next_state=[-0.01336055 -0.39597941 -0.10815641  0.31985899]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 31 ] state=[-0.01336055 -0.39597941 -0.10815641  0.31985899], action=1, reward=1.0, next_state=[-0.02128014 -0.1994967  -0.10175923 -0.00487896]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 32 ] state=[-0.02128014 -0.1994967  -0.10175923 -0.00487896], action=0, reward=1.0, next_state=[-0.02527007 -0.39302321 -0.10185681  0.25404351]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 33 ] state=[-0.02527007 -0.39302321 -0.10185681  0.25404351], action=0, reward=1.0, next_state=[-0.03313054 -0.58655446 -0.09677594  0.51294166]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 34 ] state=[-0.03313054 -0.58655446 -0.09677594  0.51294166], action=1, reward=1.0, next_state=[-0.04486163 -0.39021218 -0.08651711  0.19139858]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 35 ] state=[-0.04486163 -0.39021218 -0.08651711  0.19139858], action=1, reward=1.0, next_state=[-0.05266587 -0.19396594 -0.08268914 -0.12727408]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 36 ] state=[-0.05266587 -0.19396594 -0.08268914 -0.12727408], action=0, reward=1.0, next_state=[-0.05654519 -0.38781195 -0.08523462  0.13821852]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 37 ] state=[-0.05654519 -0.38781195 -0.08523462  0.13821852], action=1, reward=1.0, next_state=[-0.06430143 -0.19157917 -0.08247025 -0.18009073]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 38 ] state=[-0.06430143 -0.19157917 -0.08247025 -0.18009073], action=0, reward=1.0, next_state=[-0.06813301 -0.38543008 -0.08607206  0.08547859]\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 500 ][ timestamp 39 ] state=[-0.06813301 -0.38543008 -0.08607206  0.08547859], action=1, reward=1.0, next_state=[-0.07584161 -0.18918655 -0.08436249 -0.23307094]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 40 ] state=[-0.07584161 -0.18918655 -0.08436249 -0.23307094], action=0, reward=1.0, next_state=[-0.07962534 -0.38300812 -0.08902391  0.03185429]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 41 ] state=[-0.07962534 -0.38300812 -0.08902391  0.03185429], action=1, reward=1.0, next_state=[-0.08728551 -0.18672978 -0.08838682 -0.2875358 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 42 ] state=[-0.08728551 -0.18672978 -0.08838682 -0.2875358 ], action=1, reward=1.0, next_state=[-0.0910201   0.00953416 -0.09413754 -0.60673441]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 43 ] state=[-0.0910201   0.00953416 -0.09413754 -0.60673441], action=0, reward=1.0, next_state=[-0.09082942 -0.18415422 -0.10627223 -0.3451238 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 44 ] state=[-0.09082942 -0.18415422 -0.10627223 -0.3451238 ], action=1, reward=1.0, next_state=[-0.0945125   0.01230631 -0.1131747  -0.66933734]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 45 ] state=[-0.0945125   0.01230631 -0.1131747  -0.66933734], action=1, reward=1.0, next_state=[-0.09426638  0.2088048  -0.12656145 -0.99540181]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 46 ] state=[-0.09426638  0.2088048  -0.12656145 -0.99540181], action=1, reward=1.0, next_state=[-0.09009028  0.40537133 -0.14646949 -1.32500316]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 47 ] state=[-0.09009028  0.40537133 -0.14646949 -1.32500316], action=1, reward=1.0, next_state=[-0.08198285  0.60200751 -0.17296955 -1.65970742]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 48 ] state=[-0.08198285  0.60200751 -0.17296955 -1.65970742], action=0, reward=1.0, next_state=[-0.0699427   0.40927222 -0.2061637  -1.42551833]\n",
      "[ Experience replay ] starts\n",
      "[ episode 500 ][ timestamp 49 ] state=[-0.0699427   0.40927222 -0.2061637  -1.42551833], action=0, reward=-1.0, next_state=[-0.06175726  0.21720695 -0.23467407 -1.20370503]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 500: Exploration_rate=0.01. Score=49.\n",
      "[ episode 501 ] state=[-0.02193741 -0.00997843  0.04887671 -0.04478592]\n",
      "[ episode 501 ][ timestamp 1 ] state=[-0.02193741 -0.00997843  0.04887671 -0.04478592], action=1, reward=1.0, next_state=[-0.02213697  0.18440981  0.04798099 -0.32165604]\n",
      "[ Experience replay ] starts\n",
      "[ episode 501 ][ timestamp 2 ] state=[-0.02213697  0.18440981  0.04798099 -0.32165604], action=1, reward=1.0, next_state=[-0.01844878  0.37881682  0.04154787 -0.59882995]\n",
      "[ Experience replay ] starts\n",
      "[ episode 501 ][ timestamp 3 ] state=[-0.01844878  0.37881682  0.04154787 -0.59882995], action=0, reward=1.0, next_state=[-0.01087244  0.18313893  0.02957127 -0.29335485]\n",
      "[ Experience replay ] starts\n",
      "[ episode 501 ][ timestamp 4 ] state=[-0.01087244  0.18313893  0.02957127 -0.29335485], action=0, reward=1.0, next_state=[-0.00720966 -0.01239188  0.02370417  0.00850582]\n",
      "[ Experience replay ] starts\n",
      "[ episode 501 ][ timestamp 5 ] state=[-0.00720966 -0.01239188  0.02370417  0.00850582], action=1, reward=1.0, next_state=[-0.0074575   0.18238224  0.02387429 -0.27660491]\n",
      "[ Experience replay ] starts\n",
      "[ episode 501 ][ timestamp 6 ] state=[-0.0074575   0.18238224  0.02387429 -0.27660491], action=1, reward=1.0, next_state=[-0.00380986  0.37715559  0.01834219 -0.5616633 ]\n",
      "[ Experience replay ] starts\n",
      "[ episode 501 ][ timestamp 7 ] state=[-0.00380986  0.37715559  0.01834219 -0.5616633 ], action=1, reward=1.0, next_state=[ 0.00373326  0.5720154   0.00710893 -0.84851154]\n",
      "[ Experience replay ] starts\n",
      "[ episode 501 ][ timestamp 8 ] state=[ 0.00373326  0.5720154   0.00710893 -0.84851154], action=0, reward=1.0, next_state=[ 0.01517356  0.37679721 -0.0098613  -0.55360165]\n",
      "[ Experience replay ] starts\n",
      "[ episode 501 ][ timestamp 9 ] state=[ 0.01517356  0.37679721 -0.0098613  -0.55360165], action=1, reward=1.0, next_state=[ 0.02270951  0.57205624 -0.02093334 -0.84937513]\n",
      "[ Experience replay ] starts\n",
      "[ episode 501 ][ timestamp 10 ] state=[ 0.02270951  0.57205624 -0.02093334 -0.84937513], action=1, reward=1.0, next_state=[ 0.03415063  0.76745732 -0.03792084 -1.14856649]\n",
      "[ Experience replay ] starts\n",
      "[ episode 501 ][ timestamp 11 ] state=[ 0.03415063  0.76745732 -0.03792084 -1.14856649], action=0, reward=1.0, next_state=[ 0.04949978  0.57285037 -0.06089217 -0.86801196]\n",
      "[ Experience replay ] starts\n",
      "[ episode 501 ][ timestamp 12 ] state=[ 0.04949978  0.57285037 -0.06089217 -0.86801196], action=0, reward=1.0, next_state=[ 0.06095679  0.37860744 -0.07825241 -0.59507882]\n",
      "[ Experience replay ] starts\n",
      "[ episode 501 ][ timestamp 13 ] state=[ 0.06095679  0.37860744 -0.07825241 -0.59507882], action=1, reward=1.0, next_state=[ 0.06852894  0.57473237 -0.09015398 -0.91134868]\n",
      "[ Experience replay ] starts\n",
      "[ episode 501 ][ timestamp 14 ] state=[ 0.06852894  0.57473237 -0.09015398 -0.91134868], action=1, reward=1.0, next_state=[ 0.08002358  0.77095093 -0.10838096 -1.23095061]\n",
      "[ Experience replay ] starts\n",
      "[ episode 501 ][ timestamp 15 ] state=[ 0.08002358  0.77095093 -0.10838096 -1.23095061], action=1, reward=1.0, next_state=[ 0.0954426   0.96728709 -0.13299997 -1.55552851]\n",
      "[ Experience replay ] starts\n",
      "[ episode 501 ][ timestamp 16 ] state=[ 0.0954426   0.96728709 -0.13299997 -1.55552851], action=1, reward=1.0, next_state=[ 0.11478834  1.16372824 -0.16411054 -1.88657475]\n",
      "[ Experience replay ] starts\n",
      "[ episode 501 ][ timestamp 17 ] state=[ 0.11478834  1.16372824 -0.16411054 -1.88657475], action=1, reward=1.0, next_state=[ 0.13806291  1.36021071 -0.20184204 -2.22537079]\n",
      "[ Experience replay ] starts\n",
      "[ episode 501 ][ timestamp 18 ] state=[ 0.13806291  1.36021071 -0.20184204 -2.22537079], action=1, reward=-1.0, next_state=[ 0.16526712  1.55660326 -0.24634945 -2.5729186 ]\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 501: Exploration_rate=0.01. Score=18.\n",
      "[ Failed! ] took more than 500 episodes to converge\n"
     ]
    }
   ],
   "source": [
    "# create policy\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "policy = Policy(observation_space, action_space)\n",
    "\n",
    "# create agent\n",
    "agent = Agent(policy)\n",
    "\n",
    "# play game\n",
    "for i_episode in count(1):\n",
    "    state = env.reset()\n",
    "    print(\"[ episode {} ] state={}\".format(i_episode, state))\n",
    "    for t in range(1, 10000):\n",
    "        action = agent.select_action(state)\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            reward *= -1\n",
    "        agent.remember(state, action, reward, state_next, done)\n",
    "        print(\"[ episode {} ][ timestamp {} ] state={}, action={}, reward={}, next_state={}\".format(i_episode, t, state, action, reward, state_next))\n",
    "        state = state_next\n",
    "        agent.experience_replay()\n",
    "        if done:\n",
    "            break\n",
    "    print(\"[ Ended! ] Episode {}: Exploration_rate={}. Score={}.\".format(i_episode, agent.exploration_rate, t))\n",
    "\n",
    "    # end game criteria\n",
    "    if t > env.spec.reward_threshold:\n",
    "        print(\"[ Solved! ] Score is now {}\".format(t))\n",
    "        break\n",
    "    elif i_episode > 500:\n",
    "        print(\"[ Failed! ] took more than 500 episodes to converge\")\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('pytorch_p36': conda)",
   "language": "python",
   "name": "python361064bitpytorchp36conda143b13e29122453f97130b8bdfe91e87"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
