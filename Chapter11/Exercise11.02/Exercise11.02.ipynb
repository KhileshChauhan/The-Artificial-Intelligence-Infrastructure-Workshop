{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate synthetic data for linear regression\n",
    "# set random seed\n",
    "np.random.seed(9)\n",
    "# draw 100 random numbers from uniform dist [0, 1]\n",
    "x = np.random.uniform(0, 1, (100, 1))\n",
    "# draw random noise from standard normal\n",
    "z = np.random.normal(0, .1, (100, 1))\n",
    "# create ground truth for y = 8x - 3\n",
    "y = 3 * x - 1 + z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# split data into train and test\n",
    "x_train, y_train = x[:80], y[:80]\n",
    "x_val, y_val = x[80:], y[80:]\n",
    "\n",
    "print(type(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# move data from numpy to torch\n",
    "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "x_val_tensor = torch.from_numpy(x_val).float().to(device)\n",
    "y_val_tensor = torch.from_numpy(y_val).float().to(device)\n",
    "print(type(x_train_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7823], requires_grad=True) tensor([-2.3965], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# create trainable parameters for the model\n",
    "weight = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "bias = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "print(weight, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ epoch ] 0\n",
      "[ training ] training loss = 11.882405281066895\n",
      "[ eval ] validation loss = 5.58413028717041\n",
      "[ epoch ] 1\n",
      "[ training ] training loss = 6.844976902008057\n",
      "[ eval ] validation loss = 3.155251979827881\n",
      "[ epoch ] 2\n",
      "[ training ] training loss = 4.0197625160217285\n",
      "[ eval ] validation loss = 1.834886908531189\n",
      "[ epoch ] 3\n",
      "[ training ] training loss = 2.433295726776123\n",
      "[ eval ] validation loss = 1.123799443244934\n",
      "[ epoch ] 4\n",
      "[ training ] training loss = 1.540527105331421\n",
      "[ eval ] validation loss = 0.7454460263252258\n",
      "[ epoch ] 5\n",
      "[ training ] training loss = 1.0362762212753296\n",
      "[ eval ] validation loss = 0.5471951961517334\n",
      "[ epoch ] 6\n",
      "[ training ] training loss = 0.7496689558029175\n",
      "[ eval ] validation loss = 0.4452219009399414\n",
      "[ epoch ] 7\n",
      "[ training ] training loss = 0.5850287675857544\n",
      "[ eval ] validation loss = 0.3938012719154358\n",
      "[ epoch ] 8\n",
      "[ training ] training loss = 0.48878344893455505\n",
      "[ eval ] validation loss = 0.3682265877723694\n",
      "[ epoch ] 9\n",
      "[ training ] training loss = 0.4309348165988922\n",
      "[ eval ] validation loss = 0.3553265333175659\n",
      "[ epoch ] 10\n",
      "[ training ] training loss = 0.39468449354171753\n",
      "[ eval ] validation loss = 0.34821635484695435\n",
      "[ epoch ] 11\n",
      "[ training ] training loss = 0.37062424421310425\n",
      "[ eval ] validation loss = 0.343403160572052\n",
      "[ epoch ] 12\n",
      "[ training ] training loss = 0.3534838557243347\n",
      "[ eval ] validation loss = 0.33919912576675415\n",
      "[ epoch ] 13\n",
      "[ training ] training loss = 0.34031036496162415\n",
      "[ eval ] validation loss = 0.33485689759254456\n",
      "[ epoch ] 14\n",
      "[ training ] training loss = 0.32944780588150024\n",
      "[ eval ] validation loss = 0.3301038146018982\n",
      "[ epoch ] 15\n",
      "[ training ] training loss = 0.31996697187423706\n",
      "[ eval ] validation loss = 0.3248947560787201\n",
      "[ epoch ] 16\n",
      "[ training ] training loss = 0.3113452196121216\n",
      "[ eval ] validation loss = 0.319283664226532\n",
      "[ epoch ] 17\n",
      "[ training ] training loss = 0.3032877445220947\n",
      "[ eval ] validation loss = 0.3133586049079895\n",
      "[ epoch ] 18\n",
      "[ training ] training loss = 0.295627623796463\n",
      "[ eval ] validation loss = 0.30721166729927063\n",
      "[ epoch ] 19\n",
      "[ training ] training loss = 0.28826904296875\n",
      "[ eval ] validation loss = 0.30092522501945496\n",
      "[ epoch ] 20\n",
      "[ training ] training loss = 0.28115659952163696\n",
      "[ eval ] validation loss = 0.29456809163093567\n",
      "[ epoch ] 21\n",
      "[ training ] training loss = 0.27425724267959595\n",
      "[ eval ] validation loss = 0.28819483518600464\n",
      "[ epoch ] 22\n",
      "[ training ] training loss = 0.2675504982471466\n",
      "[ eval ] validation loss = 0.28184765577316284\n",
      "[ epoch ] 23\n",
      "[ training ] training loss = 0.26102322340011597\n",
      "[ eval ] validation loss = 0.27555832266807556\n",
      "[ epoch ] 24\n",
      "[ training ] training loss = 0.2546660304069519\n",
      "[ eval ] validation loss = 0.26935014128685\n",
      "[ epoch ] 25\n",
      "[ training ] training loss = 0.24847225844860077\n",
      "[ eval ] validation loss = 0.26323992013931274\n",
      "[ epoch ] 26\n",
      "[ training ] training loss = 0.24243609607219696\n",
      "[ eval ] validation loss = 0.25723931193351746\n",
      "[ epoch ] 27\n",
      "[ training ] training loss = 0.23655278980731964\n",
      "[ eval ] validation loss = 0.2513563930988312\n",
      "[ epoch ] 28\n",
      "[ training ] training loss = 0.23081818222999573\n",
      "[ eval ] validation loss = 0.24559617042541504\n",
      "[ epoch ] 29\n",
      "[ training ] training loss = 0.22522802650928497\n",
      "[ eval ] validation loss = 0.2399614304304123\n",
      "[ epoch ] 30\n",
      "[ training ] training loss = 0.21977880597114563\n",
      "[ eval ] validation loss = 0.23445363342761993\n",
      "[ epoch ] 31\n",
      "[ training ] training loss = 0.2144666612148285\n",
      "[ eval ] validation loss = 0.22907276451587677\n",
      "[ epoch ] 32\n",
      "[ training ] training loss = 0.20928826928138733\n",
      "[ eval ] validation loss = 0.22381822764873505\n",
      "[ epoch ] 33\n",
      "[ training ] training loss = 0.20424017310142517\n",
      "[ eval ] validation loss = 0.21868857741355896\n",
      "[ epoch ] 34\n",
      "[ training ] training loss = 0.1993190199136734\n",
      "[ eval ] validation loss = 0.2136821746826172\n",
      "[ epoch ] 35\n",
      "[ training ] training loss = 0.1945217400789261\n",
      "[ eval ] validation loss = 0.208796888589859\n",
      "[ epoch ] 36\n",
      "[ training ] training loss = 0.18984512984752655\n",
      "[ eval ] validation loss = 0.20403039455413818\n",
      "[ epoch ] 37\n",
      "[ training ] training loss = 0.18528616428375244\n",
      "[ eval ] validation loss = 0.19938035309314728\n",
      "[ epoch ] 38\n",
      "[ training ] training loss = 0.180841863155365\n",
      "[ eval ] validation loss = 0.19484423100948334\n",
      "[ epoch ] 39\n",
      "[ training ] training loss = 0.17650935053825378\n",
      "[ eval ] validation loss = 0.1904195100069046\n",
      "[ epoch ] 40\n",
      "[ training ] training loss = 0.17228588461875916\n",
      "[ eval ] validation loss = 0.18610361218452454\n",
      "[ epoch ] 41\n",
      "[ training ] training loss = 0.16816860437393188\n",
      "[ eval ] validation loss = 0.18189403414726257\n",
      "[ epoch ] 42\n",
      "[ training ] training loss = 0.16415490210056305\n",
      "[ eval ] validation loss = 0.17778818309307098\n",
      "[ epoch ] 43\n",
      "[ training ] training loss = 0.16024218499660492\n",
      "[ eval ] validation loss = 0.17378364503383636\n",
      "[ epoch ] 44\n",
      "[ training ] training loss = 0.15642787516117096\n",
      "[ eval ] validation loss = 0.16987788677215576\n",
      "[ epoch ] 45\n",
      "[ training ] training loss = 0.15270957350730896\n",
      "[ eval ] validation loss = 0.16606852412223816\n",
      "[ epoch ] 46\n",
      "[ training ] training loss = 0.14908471703529358\n",
      "[ eval ] validation loss = 0.16235318779945374\n",
      "[ epoch ] 47\n",
      "[ training ] training loss = 0.14555111527442932\n",
      "[ eval ] validation loss = 0.15872958302497864\n",
      "[ epoch ] 48\n",
      "[ training ] training loss = 0.14210639894008636\n",
      "[ eval ] validation loss = 0.15519540011882782\n",
      "[ epoch ] 49\n",
      "[ training ] training loss = 0.13874828815460205\n",
      "[ eval ] validation loss = 0.15174849331378937\n",
      "[ epoch ] 50\n",
      "[ training ] training loss = 0.13547469675540924\n",
      "[ eval ] validation loss = 0.148386612534523\n",
      "[ epoch ] 51\n",
      "[ training ] training loss = 0.13228341937065125\n",
      "[ eval ] validation loss = 0.14510773122310638\n",
      "[ epoch ] 52\n",
      "[ training ] training loss = 0.12917247414588928\n",
      "[ eval ] validation loss = 0.14190974831581116\n",
      "[ epoch ] 53\n",
      "[ training ] training loss = 0.12613971531391144\n",
      "[ eval ] validation loss = 0.13879060745239258\n",
      "[ epoch ] 54\n",
      "[ training ] training loss = 0.12318328768014908\n",
      "[ eval ] validation loss = 0.13574843108654022\n",
      "[ epoch ] 55\n",
      "[ training ] training loss = 0.1203012466430664\n",
      "[ eval ] validation loss = 0.1327812820672989\n",
      "[ epoch ] 56\n",
      "[ training ] training loss = 0.11749168485403061\n",
      "[ eval ] validation loss = 0.12988725304603577\n",
      "[ epoch ] 57\n",
      "[ training ] training loss = 0.11475275456905365\n",
      "[ eval ] validation loss = 0.1270645707845688\n",
      "[ epoch ] 58\n",
      "[ training ] training loss = 0.11208277940750122\n",
      "[ eval ] validation loss = 0.1243114247918129\n",
      "[ epoch ] 59\n",
      "[ training ] training loss = 0.10947994142770767\n",
      "[ eval ] validation loss = 0.121626116335392\n",
      "[ epoch ] 60\n",
      "[ training ] training loss = 0.10694260895252228\n",
      "[ eval ] validation loss = 0.11900695413351059\n",
      "[ epoch ] 61\n",
      "[ training ] training loss = 0.10446908324956894\n",
      "[ eval ] validation loss = 0.11645226180553436\n",
      "[ epoch ] 62\n",
      "[ training ] training loss = 0.10205775499343872\n",
      "[ eval ] validation loss = 0.11396048218011856\n",
      "[ epoch ] 63\n",
      "[ training ] training loss = 0.09970713406801224\n",
      "[ eval ] validation loss = 0.11152998358011246\n",
      "[ epoch ] 64\n",
      "[ training ] training loss = 0.09741562604904175\n",
      "[ eval ] validation loss = 0.10915931314229965\n",
      "[ epoch ] 65\n",
      "[ training ] training loss = 0.09518177062273026\n",
      "[ eval ] validation loss = 0.10684694349765778\n",
      "[ epoch ] 66\n",
      "[ training ] training loss = 0.09300409257411957\n",
      "[ eval ] validation loss = 0.10459142923355103\n",
      "[ epoch ] 67\n",
      "[ training ] training loss = 0.09088117629289627\n",
      "[ eval ] validation loss = 0.10239138454198837\n",
      "[ epoch ] 68\n",
      "[ training ] training loss = 0.08881168812513351\n",
      "[ eval ] validation loss = 0.1002453938126564\n",
      "[ epoch ] 69\n",
      "[ training ] training loss = 0.08679427206516266\n",
      "[ eval ] validation loss = 0.09815212339162827\n",
      "[ epoch ] 70\n",
      "[ training ] training loss = 0.08482759445905685\n",
      "[ eval ] validation loss = 0.0961102694272995\n",
      "[ epoch ] 71\n",
      "[ training ] training loss = 0.08291041105985641\n",
      "[ eval ] validation loss = 0.09411855041980743\n",
      "[ epoch ] 72\n",
      "[ training ] training loss = 0.08104139566421509\n",
      "[ eval ] validation loss = 0.09217570722103119\n",
      "[ epoch ] 73\n",
      "[ training ] training loss = 0.07921943068504333\n",
      "[ eval ] validation loss = 0.09028055518865585\n",
      "[ epoch ] 74\n",
      "[ training ] training loss = 0.07744325697422028\n",
      "[ eval ] validation loss = 0.08843187242746353\n",
      "[ epoch ] 75\n",
      "[ training ] training loss = 0.07571182399988174\n",
      "[ eval ] validation loss = 0.08662854880094528\n",
      "[ epoch ] 76\n",
      "[ training ] training loss = 0.07402391731739044\n",
      "[ eval ] validation loss = 0.08486944437026978\n",
      "[ epoch ] 77\n",
      "[ training ] training loss = 0.0723784789443016\n",
      "[ eval ] validation loss = 0.08315343409776688\n",
      "[ epoch ] 78\n",
      "[ training ] training loss = 0.07077442109584808\n",
      "[ eval ] validation loss = 0.08147948235273361\n",
      "[ epoch ] 79\n",
      "[ training ] training loss = 0.06921076029539108\n",
      "[ eval ] validation loss = 0.07984653860330582\n",
      "[ epoch ] 80\n",
      "[ training ] training loss = 0.06768639385700226\n",
      "[ eval ] validation loss = 0.07825355976819992\n",
      "[ epoch ] 81\n",
      "[ training ] training loss = 0.0662003606557846\n",
      "[ eval ] validation loss = 0.07669956982135773\n",
      "[ epoch ] 82\n",
      "[ training ] training loss = 0.06475172191858292\n",
      "[ eval ] validation loss = 0.07518361508846283\n",
      "[ epoch ] 83\n",
      "[ training ] training loss = 0.06333952397108078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ eval ] validation loss = 0.07370473444461823\n",
      "[ epoch ] 84\n",
      "[ training ] training loss = 0.06196286156773567\n",
      "[ eval ] validation loss = 0.07226201891899109\n",
      "[ epoch ] 85\n",
      "[ training ] training loss = 0.06062082573771477\n",
      "[ eval ] validation loss = 0.0708545595407486\n",
      "[ epoch ] 86\n",
      "[ training ] training loss = 0.05931253358721733\n",
      "[ eval ] validation loss = 0.0694814994931221\n",
      "[ epoch ] 87\n",
      "[ training ] training loss = 0.058037154376506805\n",
      "[ eval ] validation loss = 0.06814197450876236\n",
      "[ epoch ] 88\n",
      "[ training ] training loss = 0.05679388716816902\n",
      "[ eval ] validation loss = 0.06683517247438431\n",
      "[ epoch ] 89\n",
      "[ training ] training loss = 0.05558185651898384\n",
      "[ eval ] validation loss = 0.0655602440237999\n",
      "[ epoch ] 90\n",
      "[ training ] training loss = 0.05440034717321396\n",
      "[ eval ] validation loss = 0.06431642919778824\n",
      "[ epoch ] 91\n",
      "[ training ] training loss = 0.05324854701757431\n",
      "[ eval ] validation loss = 0.06310296058654785\n",
      "[ epoch ] 92\n",
      "[ training ] training loss = 0.0521257221698761\n",
      "[ eval ] validation loss = 0.06191907078027725\n",
      "[ epoch ] 93\n",
      "[ training ] training loss = 0.05103112384676933\n",
      "[ eval ] validation loss = 0.06076402589678764\n",
      "[ epoch ] 94\n",
      "[ training ] training loss = 0.04996407777070999\n",
      "[ eval ] validation loss = 0.05963714048266411\n",
      "[ epoch ] 95\n",
      "[ training ] training loss = 0.04892387241125107\n",
      "[ eval ] validation loss = 0.05853767320513725\n",
      "[ epoch ] 96\n",
      "[ training ] training loss = 0.04790984466671944\n",
      "[ eval ] validation loss = 0.05746499449014664\n",
      "[ epoch ] 97\n",
      "[ training ] training loss = 0.046921297907829285\n",
      "[ eval ] validation loss = 0.056418418884277344\n",
      "[ epoch ] 98\n",
      "[ training ] training loss = 0.04595763981342316\n",
      "[ eval ] validation loss = 0.055397290736436844\n",
      "[ epoch ] 99\n",
      "[ training ] training loss = 0.045018233358860016\n",
      "[ eval ] validation loss = 0.05440099164843559\n",
      "[ epoch ] 100\n",
      "[ training ] training loss = 0.04410242289304733\n",
      "[ eval ] validation loss = 0.05342889949679375\n",
      "[ epoch ] 101\n",
      "[ training ] training loss = 0.043209683150053024\n",
      "[ eval ] validation loss = 0.05248042941093445\n",
      "[ epoch ] 102\n",
      "[ training ] training loss = 0.04233937710523605\n",
      "[ eval ] validation loss = 0.05155498534440994\n",
      "[ epoch ] 103\n",
      "[ training ] training loss = 0.04149096459150314\n",
      "[ eval ] validation loss = 0.05065201595425606\n",
      "[ epoch ] 104\n",
      "[ training ] training loss = 0.04066391661763191\n",
      "[ eval ] validation loss = 0.049770940095186234\n",
      "[ epoch ] 105\n",
      "[ training ] training loss = 0.0398576483130455\n",
      "[ eval ] validation loss = 0.04891124367713928\n",
      "[ epoch ] 106\n",
      "[ training ] training loss = 0.03907167166471481\n",
      "[ eval ] validation loss = 0.04807237535715103\n",
      "[ epoch ] 107\n",
      "[ training ] training loss = 0.038305483758449554\n",
      "[ eval ] validation loss = 0.047253839671611786\n",
      "[ epoch ] 108\n",
      "[ training ] training loss = 0.03755854815244675\n",
      "[ eval ] validation loss = 0.046455129981040955\n",
      "[ epoch ] 109\n",
      "[ training ] training loss = 0.03683040663599968\n",
      "[ eval ] validation loss = 0.045675747096538544\n",
      "[ epoch ] 110\n",
      "[ training ] training loss = 0.03612058609724045\n",
      "[ eval ] validation loss = 0.044915229082107544\n",
      "[ epoch ] 111\n",
      "[ training ] training loss = 0.03542860597372055\n",
      "[ eval ] validation loss = 0.04417310282588005\n",
      "[ epoch ] 112\n",
      "[ training ] training loss = 0.03475406765937805\n",
      "[ eval ] validation loss = 0.04344892129302025\n",
      "[ epoch ] 113\n",
      "[ training ] training loss = 0.03409648314118385\n",
      "[ eval ] validation loss = 0.04274223744869232\n",
      "[ epoch ] 114\n",
      "[ training ] training loss = 0.03345542401075363\n",
      "[ eval ] validation loss = 0.042052630335092545\n",
      "[ epoch ] 115\n",
      "[ training ] training loss = 0.03283047676086426\n",
      "[ eval ] validation loss = 0.0413796529173851\n",
      "[ epoch ] 116\n",
      "[ training ] training loss = 0.03222127631306648\n",
      "[ eval ] validation loss = 0.04072291776537895\n",
      "[ epoch ] 117\n",
      "[ training ] training loss = 0.03162740170955658\n",
      "[ eval ] validation loss = 0.04008202999830246\n",
      "[ epoch ] 118\n",
      "[ training ] training loss = 0.031048471108078957\n",
      "[ eval ] validation loss = 0.0394565723836422\n",
      "[ epoch ] 119\n",
      "[ training ] training loss = 0.030484085902571678\n",
      "[ eval ] validation loss = 0.03884618729352951\n",
      "[ epoch ] 120\n",
      "[ training ] training loss = 0.029933903366327286\n",
      "[ eval ] validation loss = 0.03825049474835396\n",
      "[ epoch ] 121\n",
      "[ training ] training loss = 0.02939755842089653\n",
      "[ eval ] validation loss = 0.03766913712024689\n",
      "[ epoch ] 122\n",
      "[ training ] training loss = 0.0288747139275074\n",
      "[ eval ] validation loss = 0.037101779133081436\n",
      "[ epoch ] 123\n",
      "[ training ] training loss = 0.02836501970887184\n",
      "[ eval ] validation loss = 0.03654804080724716\n",
      "[ epoch ] 124\n",
      "[ training ] training loss = 0.02786814235150814\n",
      "[ eval ] validation loss = 0.03600762411952019\n",
      "[ epoch ] 125\n",
      "[ training ] training loss = 0.02738376334309578\n",
      "[ eval ] validation loss = 0.03548016771674156\n",
      "[ epoch ] 126\n",
      "[ training ] training loss = 0.026911571621894836\n",
      "[ eval ] validation loss = 0.03496537730097771\n",
      "[ epoch ] 127\n",
      "[ training ] training loss = 0.026451250538229942\n",
      "[ eval ] validation loss = 0.034462932497262955\n",
      "[ epoch ] 128\n",
      "[ training ] training loss = 0.026002515107393265\n",
      "[ eval ] validation loss = 0.03397252410650253\n",
      "[ epoch ] 129\n",
      "[ training ] training loss = 0.025565069168806076\n",
      "[ eval ] validation loss = 0.03349388390779495\n",
      "[ epoch ] 130\n",
      "[ training ] training loss = 0.025138631463050842\n",
      "[ eval ] validation loss = 0.033026695251464844\n",
      "[ epoch ] 131\n",
      "[ training ] training loss = 0.02472292073071003\n",
      "[ eval ] validation loss = 0.03257068991661072\n",
      "[ epoch ] 132\n",
      "[ training ] training loss = 0.024317670613527298\n",
      "[ eval ] validation loss = 0.03212558478116989\n",
      "[ epoch ] 133\n",
      "[ training ] training loss = 0.02392260730266571\n",
      "[ eval ] validation loss = 0.03169113025069237\n",
      "[ epoch ] 134\n",
      "[ training ] training loss = 0.023537486791610718\n",
      "[ eval ] validation loss = 0.03126705437898636\n",
      "[ epoch ] 135\n",
      "[ training ] training loss = 0.02316204458475113\n",
      "[ eval ] validation loss = 0.03085310198366642\n",
      "[ epoch ] 136\n",
      "[ training ] training loss = 0.022796060889959335\n",
      "[ eval ] validation loss = 0.030449023470282555\n",
      "[ epoch ] 137\n",
      "[ training ] training loss = 0.02243928238749504\n",
      "[ eval ] validation loss = 0.030054587870836258\n",
      "[ epoch ] 138\n",
      "[ training ] training loss = 0.022091461345553398\n",
      "[ eval ] validation loss = 0.029669547453522682\n",
      "[ epoch ] 139\n",
      "[ training ] training loss = 0.021752405911684036\n",
      "[ eval ] validation loss = 0.029293667525053024\n",
      "[ epoch ] 140\n",
      "[ training ] training loss = 0.021421881392598152\n",
      "[ eval ] validation loss = 0.028926745057106018\n",
      "[ epoch ] 141\n",
      "[ training ] training loss = 0.02109965868294239\n",
      "[ eval ] validation loss = 0.028568539768457413\n",
      "[ epoch ] 142\n",
      "[ training ] training loss = 0.020785558968782425\n",
      "[ eval ] validation loss = 0.02821885608136654\n",
      "[ epoch ] 143\n",
      "[ training ] training loss = 0.02047935500741005\n",
      "[ eval ] validation loss = 0.027877474203705788\n",
      "[ epoch ] 144\n",
      "[ training ] training loss = 0.02018084190785885\n",
      "[ eval ] validation loss = 0.027544206008315086\n",
      "[ epoch ] 145\n",
      "[ training ] training loss = 0.019889844581484795\n",
      "[ eval ] validation loss = 0.027218827977776527\n",
      "[ epoch ] 146\n",
      "[ training ] training loss = 0.01960616558790207\n",
      "[ eval ] validation loss = 0.026901161298155785\n",
      "[ epoch ] 147\n",
      "[ training ] training loss = 0.019329622387886047\n",
      "[ eval ] validation loss = 0.026591027155518532\n",
      "[ epoch ] 148\n",
      "[ training ] training loss = 0.01906004548072815\n",
      "[ eval ] validation loss = 0.026288235560059547\n",
      "[ epoch ] 149\n",
      "[ training ] training loss = 0.018797241151332855\n",
      "[ eval ] validation loss = 0.025992607697844505\n",
      "[ epoch ] 150\n",
      "[ training ] training loss = 0.01854104921221733\n",
      "[ eval ] validation loss = 0.02570396661758423\n",
      "[ epoch ] 151\n",
      "[ training ] training loss = 0.018291305750608444\n",
      "[ eval ] validation loss = 0.02542213723063469\n",
      "[ epoch ] 152\n",
      "[ training ] training loss = 0.018047841265797615\n",
      "[ eval ] validation loss = 0.02514696680009365\n",
      "[ epoch ] 153\n",
      "[ training ] training loss = 0.017810501158237457\n",
      "[ eval ] validation loss = 0.024878276512026787\n",
      "[ epoch ] 154\n",
      "[ training ] training loss = 0.017579130828380585\n",
      "[ eval ] validation loss = 0.024615932255983353\n",
      "[ epoch ] 155\n",
      "[ training ] training loss = 0.017353590577840805\n",
      "[ eval ] validation loss = 0.02435975708067417\n",
      "[ epoch ] 156\n",
      "[ training ] training loss = 0.017133714631199837\n",
      "[ eval ] validation loss = 0.024109618738293648\n",
      "[ epoch ] 157\n",
      "[ training ] training loss = 0.016919374465942383\n",
      "[ eval ] validation loss = 0.02386535331606865\n",
      "[ epoch ] 158\n",
      "[ training ] training loss = 0.016710419207811356\n",
      "[ eval ] validation loss = 0.023626837879419327\n",
      "[ epoch ] 159\n",
      "[ training ] training loss = 0.01650671474635601\n",
      "[ eval ] validation loss = 0.023393912240862846\n",
      "[ epoch ] 160\n",
      "[ training ] training loss = 0.016308147460222244\n",
      "[ eval ] validation loss = 0.023166455328464508\n",
      "[ epoch ] 161\n",
      "[ training ] training loss = 0.01611456274986267\n",
      "[ eval ] validation loss = 0.02294432558119297\n",
      "[ epoch ] 162\n",
      "[ training ] training loss = 0.015925858169794083\n",
      "[ eval ] validation loss = 0.022727400064468384\n",
      "[ epoch ] 163\n",
      "[ training ] training loss = 0.015741895884275436\n",
      "[ eval ] validation loss = 0.02251555770635605\n",
      "[ epoch ] 164\n",
      "[ training ] training loss = 0.015562574379146099\n",
      "[ eval ] validation loss = 0.02230866067111492\n",
      "[ epoch ] 165\n",
      "[ training ] training loss = 0.015387745574116707\n",
      "[ eval ] validation loss = 0.022106600925326347\n",
      "[ epoch ] 166\n",
      "[ training ] training loss = 0.015217326581478119\n",
      "[ eval ] validation loss = 0.021909251809120178\n",
      "[ epoch ] 167\n",
      "[ training ] training loss = 0.015051184222102165\n",
      "[ eval ] validation loss = 0.021716514602303505\n",
      "[ epoch ] 168\n",
      "[ training ] training loss = 0.01488922256976366\n",
      "[ eval ] validation loss = 0.021528268232941628\n",
      "[ epoch ] 169\n",
      "[ training ] training loss = 0.014731337316334248\n",
      "[ eval ] validation loss = 0.021344395354390144\n",
      "[ epoch ] 170\n",
      "[ training ] training loss = 0.01457742415368557\n",
      "[ eval ] validation loss = 0.02116480842232704\n",
      "[ epoch ] 171\n",
      "[ training ] training loss = 0.014427381567656994\n",
      "[ eval ] validation loss = 0.020989391952753067\n",
      "[ epoch ] 172\n",
      "[ training ] training loss = 0.014281116425991058\n",
      "[ eval ] validation loss = 0.02081804908812046\n",
      "[ epoch ] 173\n",
      "[ training ] training loss = 0.014138521626591682\n",
      "[ eval ] validation loss = 0.02065068855881691\n",
      "[ epoch ] 174\n",
      "[ training ] training loss = 0.013999521732330322\n",
      "[ eval ] validation loss = 0.020487193018198013\n",
      "[ epoch ] 175\n",
      "[ training ] training loss = 0.01386401616036892\n",
      "[ eval ] validation loss = 0.0203274916857481\n",
      "[ epoch ] 176\n",
      "[ training ] training loss = 0.013731921091675758\n",
      "[ eval ] validation loss = 0.020171482115983963\n",
      "[ epoch ] 177\n",
      "[ training ] training loss = 0.013603150844573975\n",
      "[ eval ] validation loss = 0.020019087940454483\n",
      "[ epoch ] 178\n",
      "[ training ] training loss = 0.013477618806064129\n",
      "[ eval ] validation loss = 0.019870208576321602\n",
      "[ epoch ] 179\n",
      "[ training ] training loss = 0.013355238363146782\n",
      "[ eval ] validation loss = 0.01972474902868271\n",
      "[ epoch ] 180\n",
      "[ training ] training loss = 0.013235936872661114\n",
      "[ eval ] validation loss = 0.019582664594054222\n",
      "[ epoch ] 181\n",
      "[ training ] training loss = 0.013119647279381752\n",
      "[ eval ] validation loss = 0.01944384165108204\n",
      "[ epoch ] 182\n",
      "[ training ] training loss = 0.013006272725760937\n",
      "[ eval ] validation loss = 0.01930820569396019\n",
      "[ epoch ] 183\n",
      "[ training ] training loss = 0.012895760126411915\n",
      "[ eval ] validation loss = 0.0191756971180439\n",
      "[ epoch ] 184\n",
      "[ training ] training loss = 0.012788024730980396\n",
      "[ eval ] validation loss = 0.019046233966946602\n",
      "[ epoch ] 185\n",
      "[ training ] training loss = 0.012682994827628136\n",
      "[ eval ] validation loss = 0.01891973800957203\n",
      "[ epoch ] 186\n",
      "[ training ] training loss = 0.012580612674355507\n",
      "[ eval ] validation loss = 0.018796134740114212\n",
      "[ epoch ] 187\n",
      "[ training ] training loss = 0.012480802834033966\n",
      "[ eval ] validation loss = 0.018675368279218674\n",
      "[ epoch ] 188\n",
      "[ training ] training loss = 0.012383504770696163\n",
      "[ eval ] validation loss = 0.0185573548078537\n",
      "[ epoch ] 189\n",
      "[ training ] training loss = 0.012288656085729599\n",
      "[ eval ] validation loss = 0.01844203844666481\n",
      "[ epoch ] 190\n",
      "[ training ] training loss = 0.012196186929941177\n",
      "[ eval ] validation loss = 0.018329361453652382\n",
      "[ epoch ] 191\n",
      "[ training ] training loss = 0.012106047943234444\n",
      "[ eval ] validation loss = 0.018219245597720146\n",
      "[ epoch ] 192\n",
      "[ training ] training loss = 0.0120181730017066\n",
      "[ eval ] validation loss = 0.018111640587449074\n",
      "[ epoch ] 193\n",
      "[ training ] training loss = 0.011932522989809513\n",
      "[ eval ] validation loss = 0.01800648681819439\n",
      "[ epoch ] 194\n",
      "[ training ] training loss = 0.011849010363221169\n",
      "[ eval ] validation loss = 0.017903711646795273\n",
      "[ epoch ] 195\n",
      "[ training ] training loss = 0.01176760159432888\n",
      "[ eval ] validation loss = 0.01780327782034874\n",
      "[ epoch ] 196\n",
      "[ training ] training loss = 0.011688243597745895\n",
      "[ eval ] validation loss = 0.017705116420984268\n",
      "[ epoch ] 197\n",
      "[ training ] training loss = 0.011610882356762886\n",
      "[ eval ] validation loss = 0.017609182745218277\n",
      "[ epoch ] 198\n",
      "[ training ] training loss = 0.011535463854670525\n",
      "[ eval ] validation loss = 0.017515409737825394\n",
      "[ epoch ] 199\n",
      "[ training ] training loss = 0.011461945250630379\n",
      "[ eval ] validation loss = 0.017423754557967186\n",
      "[ epoch ] 200\n",
      "[ training ] training loss = 0.01139027625322342\n",
      "[ eval ] validation loss = 0.017334170639514923\n",
      "[ epoch ] 201\n",
      "[ training ] training loss = 0.011320412158966064\n",
      "[ eval ] validation loss = 0.017246609553694725\n",
      "[ epoch ] 202\n",
      "[ training ] training loss = 0.011252299882471561\n",
      "[ eval ] validation loss = 0.017161015421152115\n",
      "[ epoch ] 203\n",
      "[ training ] training loss = 0.0111859031021595\n",
      "[ eval ] validation loss = 0.01707734726369381\n",
      "[ epoch ] 204\n",
      "[ training ] training loss = 0.011121179908514023\n",
      "[ eval ] validation loss = 0.016995560377836227\n",
      "[ epoch ] 205\n",
      "[ training ] training loss = 0.011058082804083824\n",
      "[ eval ] validation loss = 0.016915608197450638\n",
      "[ epoch ] 206\n",
      "[ training ] training loss = 0.010996567085385323\n",
      "[ eval ] validation loss = 0.01683744788169861\n",
      "[ epoch ] 207\n",
      "[ training ] training loss = 0.01093660295009613\n",
      "[ eval ] validation loss = 0.01676102541387081\n",
      "[ epoch ] 208\n",
      "[ training ] training loss = 0.010878151282668114\n",
      "[ eval ] validation loss = 0.016686324030160904\n",
      "[ epoch ] 209\n",
      "[ training ] training loss = 0.010821170173585415\n",
      "[ eval ] validation loss = 0.016613289713859558\n",
      "[ epoch ] 210\n",
      "[ training ] training loss = 0.010765617713332176\n",
      "[ eval ] validation loss = 0.01654187962412834\n",
      "[ epoch ] 211\n",
      "[ training ] training loss = 0.010711463168263435\n",
      "[ eval ] validation loss = 0.01647206023335457\n",
      "[ epoch ] 212\n",
      "[ training ] training loss = 0.010658672079443932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ eval ] validation loss = 0.016403790563344955\n",
      "[ epoch ] 213\n",
      "[ training ] training loss = 0.010607206262648106\n",
      "[ eval ] validation loss = 0.016337040811777115\n",
      "[ epoch ] 214\n",
      "[ training ] training loss = 0.010557038709521294\n",
      "[ eval ] validation loss = 0.016271773725748062\n",
      "[ epoch ] 215\n",
      "[ training ] training loss = 0.010508129373192787\n",
      "[ eval ] validation loss = 0.016207944601774216\n",
      "[ epoch ] 216\n",
      "[ training ] training loss = 0.010460453107953072\n",
      "[ eval ] validation loss = 0.016145527362823486\n",
      "[ epoch ] 217\n",
      "[ training ] training loss = 0.01041397638618946\n",
      "[ eval ] validation loss = 0.016084490343928337\n",
      "[ epoch ] 218\n",
      "[ training ] training loss = 0.010368668474256992\n",
      "[ eval ] validation loss = 0.01602480374276638\n",
      "[ epoch ] 219\n",
      "[ training ] training loss = 0.010324500501155853\n",
      "[ eval ] validation loss = 0.015966439619660378\n",
      "[ epoch ] 220\n",
      "[ training ] training loss = 0.010281442664563656\n",
      "[ eval ] validation loss = 0.01590934954583645\n",
      "[ epoch ] 221\n",
      "[ training ] training loss = 0.010239469818770885\n",
      "[ eval ] validation loss = 0.015853513032197952\n",
      "[ epoch ] 222\n",
      "[ training ] training loss = 0.010198553092777729\n",
      "[ eval ] validation loss = 0.015798907727003098\n",
      "[ epoch ] 223\n",
      "[ training ] training loss = 0.010158662684261799\n",
      "[ eval ] validation loss = 0.0157454926520586\n",
      "[ epoch ] 224\n",
      "[ training ] training loss = 0.010119777172803879\n",
      "[ eval ] validation loss = 0.01569325476884842\n",
      "[ epoch ] 225\n",
      "[ training ] training loss = 0.010081874206662178\n",
      "[ eval ] validation loss = 0.015642153099179268\n",
      "[ epoch ] 226\n",
      "[ training ] training loss = 0.010044917464256287\n",
      "[ eval ] validation loss = 0.015592160634696484\n",
      "[ epoch ] 227\n",
      "[ training ] training loss = 0.010008892975747585\n",
      "[ eval ] validation loss = 0.015543264336884022\n",
      "[ epoch ] 228\n",
      "[ training ] training loss = 0.009973777458071709\n",
      "[ eval ] validation loss = 0.015495432540774345\n",
      "[ epoch ] 229\n",
      "[ training ] training loss = 0.009939543902873993\n",
      "[ eval ] validation loss = 0.01544864010065794\n",
      "[ epoch ] 230\n",
      "[ training ] training loss = 0.009906172752380371\n",
      "[ eval ] validation loss = 0.015402862802147865\n",
      "[ epoch ] 231\n",
      "[ training ] training loss = 0.009873637929558754\n",
      "[ eval ] validation loss = 0.01535807829350233\n",
      "[ epoch ] 232\n",
      "[ training ] training loss = 0.009841924533247948\n",
      "[ eval ] validation loss = 0.015314255841076374\n",
      "[ epoch ] 233\n",
      "[ training ] training loss = 0.009811008349061012\n",
      "[ eval ] validation loss = 0.015271383337676525\n",
      "[ epoch ] 234\n",
      "[ training ] training loss = 0.009780868887901306\n",
      "[ eval ] validation loss = 0.015229441225528717\n",
      "[ epoch ] 235\n",
      "[ training ] training loss = 0.009751483798027039\n",
      "[ eval ] validation loss = 0.015188397839665413\n",
      "[ epoch ] 236\n",
      "[ training ] training loss = 0.009722847491502762\n",
      "[ eval ] validation loss = 0.015148237347602844\n",
      "[ epoch ] 237\n",
      "[ training ] training loss = 0.009694918990135193\n",
      "[ eval ] validation loss = 0.01510893739759922\n",
      "[ epoch ] 238\n",
      "[ training ] training loss = 0.009667700156569481\n",
      "[ eval ] validation loss = 0.015070480294525623\n",
      "[ epoch ] 239\n",
      "[ training ] training loss = 0.00964116770774126\n",
      "[ eval ] validation loss = 0.015032842755317688\n",
      "[ epoch ] 240\n",
      "[ training ] training loss = 0.009615299291908741\n",
      "[ eval ] validation loss = 0.01499602198600769\n",
      "[ epoch ] 241\n",
      "[ training ] training loss = 0.009590083733201027\n",
      "[ eval ] validation loss = 0.014959976077079773\n",
      "[ epoch ] 242\n",
      "[ training ] training loss = 0.009565498679876328\n",
      "[ eval ] validation loss = 0.014924700371921062\n",
      "[ epoch ] 243\n",
      "[ training ] training loss = 0.009541535750031471\n",
      "[ eval ] validation loss = 0.014890165999531746\n",
      "[ epoch ] 244\n",
      "[ training ] training loss = 0.009518171660602093\n",
      "[ eval ] validation loss = 0.0148563701659441\n",
      "[ epoch ] 245\n",
      "[ training ] training loss = 0.009495401754975319\n",
      "[ eval ] validation loss = 0.014823297038674355\n",
      "[ epoch ] 246\n",
      "[ training ] training loss = 0.009473195299506187\n",
      "[ eval ] validation loss = 0.014790916815400124\n",
      "[ epoch ] 247\n",
      "[ training ] training loss = 0.009451557882130146\n",
      "[ eval ] validation loss = 0.01475922018289566\n",
      "[ epoch ] 248\n",
      "[ training ] training loss = 0.009430463425815105\n",
      "[ eval ] validation loss = 0.014728193171322346\n",
      "[ epoch ] 249\n",
      "[ training ] training loss = 0.009409895166754723\n",
      "[ eval ] validation loss = 0.014697819948196411\n",
      "[ epoch ] 250\n",
      "[ training ] training loss = 0.009389840066432953\n",
      "[ eval ] validation loss = 0.014668082818388939\n",
      "[ epoch ] 251\n",
      "[ training ] training loss = 0.00937030091881752\n",
      "[ eval ] validation loss = 0.014638969674706459\n",
      "[ epoch ] 252\n",
      "[ training ] training loss = 0.009351244196295738\n",
      "[ eval ] validation loss = 0.0146104721352458\n",
      "[ epoch ] 253\n",
      "[ training ] training loss = 0.009332673624157906\n",
      "[ eval ] validation loss = 0.014582566916942596\n",
      "[ epoch ] 254\n",
      "[ training ] training loss = 0.009314566850662231\n",
      "[ eval ] validation loss = 0.014555254951119423\n",
      "[ epoch ] 255\n",
      "[ training ] training loss = 0.00929691269993782\n",
      "[ eval ] validation loss = 0.014528505504131317\n",
      "[ epoch ] 256\n",
      "[ training ] training loss = 0.0092797065153718\n",
      "[ eval ] validation loss = 0.014502306468784809\n",
      "[ epoch ] 257\n",
      "[ training ] training loss = 0.009262929670512676\n",
      "[ eval ] validation loss = 0.0144766541197896\n",
      "[ epoch ] 258\n",
      "[ training ] training loss = 0.009246578440070152\n",
      "[ eval ] validation loss = 0.01445153821259737\n",
      "[ epoch ] 259\n",
      "[ training ] training loss = 0.009230637922883034\n",
      "[ eval ] validation loss = 0.014426936395466328\n",
      "[ epoch ] 260\n",
      "[ training ] training loss = 0.009215099737048149\n",
      "[ eval ] validation loss = 0.014402839355170727\n",
      "[ epoch ] 261\n",
      "[ training ] training loss = 0.009199952706694603\n",
      "[ eval ] validation loss = 0.014379249885678291\n",
      "[ epoch ] 262\n",
      "[ training ] training loss = 0.009185181930661201\n",
      "[ eval ] validation loss = 0.014356146566569805\n",
      "[ epoch ] 263\n",
      "[ training ] training loss = 0.00917078647762537\n",
      "[ eval ] validation loss = 0.014333519153296947\n",
      "[ epoch ] 264\n",
      "[ training ] training loss = 0.009156746789813042\n",
      "[ eval ] validation loss = 0.014311350882053375\n",
      "[ epoch ] 265\n",
      "[ training ] training loss = 0.00914306752383709\n",
      "[ eval ] validation loss = 0.014289637096226215\n",
      "[ epoch ] 266\n",
      "[ training ] training loss = 0.009129730984568596\n",
      "[ eval ] validation loss = 0.014268368482589722\n",
      "[ epoch ] 267\n",
      "[ training ] training loss = 0.009116731584072113\n",
      "[ eval ] validation loss = 0.014247531071305275\n",
      "[ epoch ] 268\n",
      "[ training ] training loss = 0.009104053489863873\n",
      "[ eval ] validation loss = 0.014227116480469704\n",
      "[ epoch ] 269\n",
      "[ training ] training loss = 0.009091700427234173\n",
      "[ eval ] validation loss = 0.014207115396857262\n",
      "[ epoch ] 270\n",
      "[ training ] training loss = 0.009079652838408947\n",
      "[ eval ] validation loss = 0.01418752409517765\n",
      "[ epoch ] 271\n",
      "[ training ] training loss = 0.009067912586033344\n",
      "[ eval ] validation loss = 0.014168331399559975\n",
      "[ epoch ] 272\n",
      "[ training ] training loss = 0.00905646476894617\n",
      "[ eval ] validation loss = 0.014149527065455914\n",
      "[ epoch ] 273\n",
      "[ training ] training loss = 0.009045304730534554\n",
      "[ eval ] validation loss = 0.014131101779639721\n",
      "[ epoch ] 274\n",
      "[ training ] training loss = 0.009034428745508194\n",
      "[ eval ] validation loss = 0.014113044366240501\n",
      "[ epoch ] 275\n",
      "[ training ] training loss = 0.009023820981383324\n",
      "[ eval ] validation loss = 0.014095346443355083\n",
      "[ epoch ] 276\n",
      "[ training ] training loss = 0.009013484232127666\n",
      "[ eval ] validation loss = 0.014078003354370594\n",
      "[ epoch ] 277\n",
      "[ training ] training loss = 0.009003409184515476\n",
      "[ eval ] validation loss = 0.014061014167964458\n",
      "[ epoch ] 278\n",
      "[ training ] training loss = 0.008993581868708134\n",
      "[ eval ] validation loss = 0.014044362120330334\n",
      "[ epoch ] 279\n",
      "[ training ] training loss = 0.008984005078673363\n",
      "[ eval ] validation loss = 0.014028036966919899\n",
      "[ epoch ] 280\n",
      "[ training ] training loss = 0.008974669501185417\n",
      "[ eval ] validation loss = 0.014012041501700878\n",
      "[ epoch ] 281\n",
      "[ training ] training loss = 0.00896556954830885\n",
      "[ eval ] validation loss = 0.013996364548802376\n",
      "[ epoch ] 282\n",
      "[ training ] training loss = 0.008956694044172764\n",
      "[ eval ] validation loss = 0.01398099772632122\n",
      "[ epoch ] 283\n",
      "[ training ] training loss = 0.008948048576712608\n",
      "[ eval ] validation loss = 0.01396593451499939\n",
      "[ epoch ] 284\n",
      "[ training ] training loss = 0.008939619176089764\n",
      "[ eval ] validation loss = 0.01395117212086916\n",
      "[ epoch ] 285\n",
      "[ training ] training loss = 0.008931394666433334\n",
      "[ eval ] validation loss = 0.013936696574091911\n",
      "[ epoch ] 286\n",
      "[ training ] training loss = 0.008923383429646492\n",
      "[ eval ] validation loss = 0.013922510668635368\n",
      "[ epoch ] 287\n",
      "[ training ] training loss = 0.008915573358535767\n",
      "[ eval ] validation loss = 0.013908597640693188\n",
      "[ epoch ] 288\n",
      "[ training ] training loss = 0.008907957933843136\n",
      "[ eval ] validation loss = 0.013894958421587944\n",
      "[ epoch ] 289\n",
      "[ training ] training loss = 0.008900534361600876\n",
      "[ eval ] validation loss = 0.013881586492061615\n",
      "[ epoch ] 290\n",
      "[ training ] training loss = 0.008893297985196114\n",
      "[ eval ] validation loss = 0.013868476264178753\n",
      "[ epoch ] 291\n",
      "[ training ] training loss = 0.008886246010661125\n",
      "[ eval ] validation loss = 0.013855625875294209\n",
      "[ epoch ] 292\n",
      "[ training ] training loss = 0.00887936819344759\n",
      "[ eval ] validation loss = 0.013843020424246788\n",
      "[ epoch ] 293\n",
      "[ training ] training loss = 0.008872661739587784\n",
      "[ eval ] validation loss = 0.013830664567649364\n",
      "[ epoch ] 294\n",
      "[ training ] training loss = 0.008866130374372005\n",
      "[ eval ] validation loss = 0.013818539679050446\n",
      "[ epoch ] 295\n",
      "[ training ] training loss = 0.008859758265316486\n",
      "[ eval ] validation loss = 0.013806653209030628\n",
      "[ epoch ] 296\n",
      "[ training ] training loss = 0.00885354820638895\n",
      "[ eval ] validation loss = 0.013794993050396442\n",
      "[ epoch ] 297\n",
      "[ training ] training loss = 0.008847493678331375\n",
      "[ eval ] validation loss = 0.013783564791083336\n",
      "[ epoch ] 298\n",
      "[ training ] training loss = 0.008841592818498611\n",
      "[ eval ] validation loss = 0.013772343285381794\n",
      "[ epoch ] 299\n",
      "[ training ] training loss = 0.008835837244987488\n",
      "[ eval ] validation loss = 0.013761350885033607\n",
      "[ epoch ] 300\n",
      "[ training ] training loss = 0.008830230683088303\n",
      "[ eval ] validation loss = 0.013750560581684113\n",
      "[ epoch ] 301\n",
      "[ training ] training loss = 0.00882476195693016\n",
      "[ eval ] validation loss = 0.013739977963268757\n",
      "[ epoch ] 302\n",
      "[ training ] training loss = 0.00881943292915821\n",
      "[ eval ] validation loss = 0.013729599304497242\n",
      "[ epoch ] 303\n",
      "[ training ] training loss = 0.008814232423901558\n",
      "[ eval ] validation loss = 0.013719411566853523\n",
      "[ epoch ] 304\n",
      "[ training ] training loss = 0.008809166960418224\n",
      "[ eval ] validation loss = 0.01370941661298275\n",
      "[ epoch ] 305\n",
      "[ training ] training loss = 0.008804232813417912\n",
      "[ eval ] validation loss = 0.013699608854949474\n",
      "[ epoch ] 306\n",
      "[ training ] training loss = 0.008799416944384575\n",
      "[ eval ] validation loss = 0.013690000399947166\n",
      "[ epoch ] 307\n",
      "[ training ] training loss = 0.008794726803898811\n",
      "[ eval ] validation loss = 0.013680562376976013\n",
      "[ epoch ] 308\n",
      "[ training ] training loss = 0.008790151216089725\n",
      "[ eval ] validation loss = 0.013671299442648888\n",
      "[ epoch ] 309\n",
      "[ training ] training loss = 0.008785691112279892\n",
      "[ eval ] validation loss = 0.013662220910191536\n",
      "[ epoch ] 310\n",
      "[ training ] training loss = 0.008781345561146736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ eval ] validation loss = 0.013653300702571869\n",
      "[ epoch ] 311\n",
      "[ training ] training loss = 0.008777105249464512\n",
      "[ eval ] validation loss = 0.01364455558359623\n",
      "[ epoch ] 312\n",
      "[ training ] training loss = 0.008772973902523518\n",
      "[ eval ] validation loss = 0.013635968789458275\n",
      "[ epoch ] 313\n",
      "[ training ] training loss = 0.00876894686371088\n",
      "[ eval ] validation loss = 0.013627542182803154\n",
      "[ epoch ] 314\n",
      "[ training ] training loss = 0.008765021339058876\n",
      "[ eval ] validation loss = 0.013619269244372845\n",
      "[ epoch ] 315\n",
      "[ training ] training loss = 0.008761194534599781\n",
      "[ eval ] validation loss = 0.013611149974167347\n",
      "[ epoch ] 316\n",
      "[ training ] training loss = 0.008757463656365871\n",
      "[ eval ] validation loss = 0.01360318623483181\n",
      "[ epoch ] 317\n",
      "[ training ] training loss = 0.008753830567002296\n",
      "[ eval ] validation loss = 0.013595369644463062\n",
      "[ epoch ] 318\n",
      "[ training ] training loss = 0.008750279434025288\n",
      "[ eval ] validation loss = 0.013587690889835358\n",
      "[ epoch ] 319\n",
      "[ training ] training loss = 0.008746826089918613\n",
      "[ eval ] validation loss = 0.013580156490206718\n",
      "[ epoch ] 320\n",
      "[ training ] training loss = 0.008743452839553356\n",
      "[ eval ] validation loss = 0.013572764582931995\n",
      "[ epoch ] 321\n",
      "[ training ] training loss = 0.008740169927477837\n",
      "[ eval ] validation loss = 0.013565504923462868\n",
      "[ epoch ] 322\n",
      "[ training ] training loss = 0.00873696617782116\n",
      "[ eval ] validation loss = 0.013558375649154186\n",
      "[ epoch ] 323\n",
      "[ training ] training loss = 0.008733844384551048\n",
      "[ eval ] validation loss = 0.013551379553973675\n",
      "[ epoch ] 324\n",
      "[ training ] training loss = 0.008730804547667503\n",
      "[ eval ] validation loss = 0.013544510118663311\n",
      "[ epoch ] 325\n",
      "[ training ] training loss = 0.008727836422622204\n",
      "[ eval ] validation loss = 0.013537759892642498\n",
      "[ epoch ] 326\n",
      "[ training ] training loss = 0.008724940940737724\n",
      "[ eval ] validation loss = 0.013531138189136982\n",
      "[ epoch ] 327\n",
      "[ training ] training loss = 0.008722124621272087\n",
      "[ eval ] validation loss = 0.013524631969630718\n",
      "[ epoch ] 328\n",
      "[ training ] training loss = 0.008719375357031822\n",
      "[ eval ] validation loss = 0.013518239371478558\n",
      "[ epoch ] 329\n",
      "[ training ] training loss = 0.00871669314801693\n",
      "[ eval ] validation loss = 0.013511965051293373\n",
      "[ epoch ] 330\n",
      "[ training ] training loss = 0.008714084513485432\n",
      "[ eval ] validation loss = 0.013505799695849419\n",
      "[ epoch ] 331\n",
      "[ training ] training loss = 0.008711538277566433\n",
      "[ eval ] validation loss = 0.013499753549695015\n",
      "[ epoch ] 332\n",
      "[ training ] training loss = 0.008709055371582508\n",
      "[ eval ] validation loss = 0.013493810780346394\n",
      "[ epoch ] 333\n",
      "[ training ] training loss = 0.008706638589501381\n",
      "[ eval ] validation loss = 0.013487974181771278\n",
      "[ epoch ] 334\n",
      "[ training ] training loss = 0.008704275824129581\n",
      "[ eval ] validation loss = 0.013482242822647095\n",
      "[ epoch ] 335\n",
      "[ training ] training loss = 0.008701978251338005\n",
      "[ eval ] validation loss = 0.013476607389748096\n",
      "[ epoch ] 336\n",
      "[ training ] training loss = 0.00869973748922348\n",
      "[ eval ] validation loss = 0.013471072539687157\n",
      "[ epoch ] 337\n",
      "[ training ] training loss = 0.008697551675140858\n",
      "[ eval ] validation loss = 0.013465638272464275\n",
      "[ epoch ] 338\n",
      "[ training ] training loss = 0.008695422671735287\n",
      "[ eval ] validation loss = 0.013460296206176281\n",
      "[ epoch ] 339\n",
      "[ training ] training loss = 0.00869334489107132\n",
      "[ eval ] validation loss = 0.013455061241984367\n",
      "[ epoch ] 340\n",
      "[ training ] training loss = 0.00869132112711668\n",
      "[ eval ] validation loss = 0.013449911959469318\n",
      "[ epoch ] 341\n",
      "[ training ] training loss = 0.008689344860613346\n",
      "[ eval ] validation loss = 0.013444843702018261\n",
      "[ epoch ] 342\n",
      "[ training ] training loss = 0.008687421679496765\n",
      "[ eval ] validation loss = 0.013439871370792389\n",
      "[ epoch ] 343\n",
      "[ training ] training loss = 0.008685546927154064\n",
      "[ eval ] validation loss = 0.013434985652565956\n",
      "[ epoch ] 344\n",
      "[ training ] training loss = 0.008683716878294945\n",
      "[ eval ] validation loss = 0.01343018002808094\n",
      "[ epoch ] 345\n",
      "[ training ] training loss = 0.008681935258209705\n",
      "[ eval ] validation loss = 0.013425464741885662\n",
      "[ epoch ] 346\n",
      "[ training ] training loss = 0.008680200204253197\n",
      "[ eval ] validation loss = 0.0134208295494318\n",
      "[ epoch ] 347\n",
      "[ training ] training loss = 0.008678505197167397\n",
      "[ eval ] validation loss = 0.013416267931461334\n",
      "[ epoch ] 348\n",
      "[ training ] training loss = 0.008676853030920029\n",
      "[ eval ] validation loss = 0.01341178733855486\n",
      "[ epoch ] 349\n",
      "[ training ] training loss = 0.008675242774188519\n",
      "[ eval ] validation loss = 0.013407377526164055\n",
      "[ epoch ] 350\n",
      "[ training ] training loss = 0.00867367722094059\n",
      "[ eval ] validation loss = 0.013403060846030712\n",
      "[ epoch ] 351\n",
      "[ training ] training loss = 0.008672147057950497\n",
      "[ eval ] validation loss = 0.013398811221122742\n",
      "[ epoch ] 352\n",
      "[ training ] training loss = 0.008670656010508537\n",
      "[ eval ] validation loss = 0.013394631445407867\n",
      "[ epoch ] 353\n",
      "[ training ] training loss = 0.008669202215969563\n",
      "[ eval ] validation loss = 0.013390518724918365\n",
      "[ epoch ] 354\n",
      "[ training ] training loss = 0.008667785674333572\n",
      "[ eval ] validation loss = 0.013386478647589684\n",
      "[ epoch ] 355\n",
      "[ training ] training loss = 0.008666403591632843\n",
      "[ eval ] validation loss = 0.013382514007389545\n",
      "[ epoch ] 356\n",
      "[ training ] training loss = 0.008665058761835098\n",
      "[ eval ] validation loss = 0.013378618285059929\n",
      "[ epoch ] 357\n",
      "[ training ] training loss = 0.008663746528327465\n",
      "[ eval ] validation loss = 0.01337477844208479\n",
      "[ epoch ] 358\n",
      "[ training ] training loss = 0.00866246409714222\n",
      "[ eval ] validation loss = 0.013371013104915619\n",
      "[ epoch ] 359\n",
      "[ training ] training loss = 0.008661218918859959\n",
      "[ eval ] validation loss = 0.013367305509746075\n",
      "[ epoch ] 360\n",
      "[ training ] training loss = 0.008660001680254936\n",
      "[ eval ] validation loss = 0.013363661244511604\n",
      "[ epoch ] 361\n",
      "[ training ] training loss = 0.008658817037940025\n",
      "[ eval ] validation loss = 0.013360077515244484\n",
      "[ epoch ] 362\n",
      "[ training ] training loss = 0.008657659403979778\n",
      "[ eval ] validation loss = 0.01335655152797699\n",
      "[ epoch ] 363\n",
      "[ training ] training loss = 0.008656533434987068\n",
      "[ eval ] validation loss = 0.013353085145354271\n",
      "[ epoch ] 364\n",
      "[ training ] training loss = 0.00865543819963932\n",
      "[ eval ] validation loss = 0.013349680230021477\n",
      "[ epoch ] 365\n",
      "[ training ] training loss = 0.008654363453388214\n",
      "[ eval ] validation loss = 0.013346327468752861\n",
      "[ epoch ] 366\n",
      "[ training ] training loss = 0.008653317578136921\n",
      "[ eval ] validation loss = 0.01334303431212902\n",
      "[ epoch ] 367\n",
      "[ training ] training loss = 0.008652301505208015\n",
      "[ eval ] validation loss = 0.013339795172214508\n",
      "[ epoch ] 368\n",
      "[ training ] training loss = 0.008651310577988625\n",
      "[ eval ] validation loss = 0.013336604461073875\n",
      "[ epoch ] 369\n",
      "[ training ] training loss = 0.008650343865156174\n",
      "[ eval ] validation loss = 0.013333478942513466\n",
      "[ epoch ] 370\n",
      "[ training ] training loss = 0.008649402298033237\n",
      "[ eval ] validation loss = 0.013330397196114063\n",
      "[ epoch ] 371\n",
      "[ training ] training loss = 0.008648483082652092\n",
      "[ eval ] validation loss = 0.013327367603778839\n",
      "[ epoch ] 372\n",
      "[ training ] training loss = 0.008647585287690163\n",
      "[ eval ] validation loss = 0.013324390165507793\n",
      "[ epoch ] 373\n",
      "[ training ] training loss = 0.008646713569760323\n",
      "[ eval ] validation loss = 0.013321456499397755\n",
      "[ epoch ] 374\n",
      "[ training ] training loss = 0.008645860478281975\n",
      "[ eval ] validation loss = 0.013318561017513275\n",
      "[ epoch ] 375\n",
      "[ training ] training loss = 0.008645033463835716\n",
      "[ eval ] validation loss = 0.013315720483660698\n",
      "[ epoch ] 376\n",
      "[ training ] training loss = 0.00864422507584095\n",
      "[ eval ] validation loss = 0.013312933035194874\n",
      "[ epoch ] 377\n",
      "[ training ] training loss = 0.008643431589007378\n",
      "[ eval ] validation loss = 0.013310184702277184\n",
      "[ epoch ] 378\n",
      "[ training ] training loss = 0.008642667904496193\n",
      "[ eval ] validation loss = 0.013307491317391396\n",
      "[ epoch ] 379\n",
      "[ training ] training loss = 0.008641917258501053\n",
      "[ eval ] validation loss = 0.013304832391440868\n",
      "[ epoch ] 380\n",
      "[ training ] training loss = 0.00864118617027998\n",
      "[ eval ] validation loss = 0.013302216306328773\n",
      "[ epoch ] 381\n",
      "[ training ] training loss = 0.008640473708510399\n",
      "[ eval ] validation loss = 0.013299639336764812\n",
      "[ epoch ] 382\n",
      "[ training ] training loss = 0.00863977987319231\n",
      "[ eval ] validation loss = 0.013297105208039284\n",
      "[ epoch ] 383\n",
      "[ training ] training loss = 0.008639100007712841\n",
      "[ eval ] validation loss = 0.013294612988829613\n",
      "[ epoch ] 384\n",
      "[ training ] training loss = 0.008638439700007439\n",
      "[ eval ] validation loss = 0.013292166404426098\n",
      "[ epoch ] 385\n",
      "[ training ] training loss = 0.008637799881398678\n",
      "[ eval ] validation loss = 0.013289752416312695\n",
      "[ epoch ] 386\n",
      "[ training ] training loss = 0.008637172169983387\n",
      "[ eval ] validation loss = 0.013287382200360298\n",
      "[ epoch ] 387\n",
      "[ training ] training loss = 0.00863656122237444\n",
      "[ eval ] validation loss = 0.013285043649375439\n",
      "[ epoch ] 388\n",
      "[ training ] training loss = 0.00863596424460411\n",
      "[ eval ] validation loss = 0.013282744213938713\n",
      "[ epoch ] 389\n",
      "[ training ] training loss = 0.008635382167994976\n",
      "[ eval ] validation loss = 0.013280478306114674\n",
      "[ epoch ] 390\n",
      "[ training ] training loss = 0.008634814992547035\n",
      "[ eval ] validation loss = 0.01327824778854847\n",
      "[ epoch ] 391\n",
      "[ training ] training loss = 0.008634263649582863\n",
      "[ eval ] validation loss = 0.013276060111820698\n",
      "[ epoch ] 392\n",
      "[ training ] training loss = 0.008633727207779884\n",
      "[ eval ] validation loss = 0.013273902237415314\n",
      "[ epoch ] 393\n",
      "[ training ] training loss = 0.008633201010525227\n",
      "[ eval ] validation loss = 0.013271776027977467\n",
      "[ epoch ] 394\n",
      "[ training ] training loss = 0.008632689714431763\n",
      "[ eval ] validation loss = 0.013269683346152306\n",
      "[ epoch ] 395\n",
      "[ training ] training loss = 0.008632193319499493\n",
      "[ eval ] validation loss = 0.013267630711197853\n",
      "[ epoch ] 396\n",
      "[ training ] training loss = 0.008631705306470394\n",
      "[ eval ] validation loss = 0.013265605084598064\n",
      "[ epoch ] 397\n",
      "[ training ] training loss = 0.00863123219460249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ eval ] validation loss = 0.013263617642223835\n",
      "[ epoch ] 398\n",
      "[ training ] training loss = 0.008630769327282906\n",
      "[ eval ] validation loss = 0.01326165534555912\n",
      "[ epoch ] 399\n",
      "[ training ] training loss = 0.008630321361124516\n",
      "[ eval ] validation loss = 0.01325971819460392\n",
      "[ epoch ] 400\n",
      "[ training ] training loss = 0.008629881776869297\n",
      "[ eval ] validation loss = 0.013257823884487152\n",
      "[ epoch ] 401\n",
      "[ training ] training loss = 0.00862945057451725\n",
      "[ eval ] validation loss = 0.013255956582725048\n",
      "[ epoch ] 402\n",
      "[ training ] training loss = 0.008629034273326397\n",
      "[ eval ] validation loss = 0.013254107907414436\n",
      "[ epoch ] 403\n",
      "[ training ] training loss = 0.008628631010651588\n",
      "[ eval ] validation loss = 0.013252297416329384\n",
      "[ epoch ] 404\n",
      "[ training ] training loss = 0.008628234267234802\n",
      "[ eval ] validation loss = 0.013250510208308697\n",
      "[ epoch ] 405\n",
      "[ training ] training loss = 0.008627844974398613\n",
      "[ eval ] validation loss = 0.013248749077320099\n",
      "[ epoch ] 406\n",
      "[ training ] training loss = 0.008627472445368767\n",
      "[ eval ] validation loss = 0.013247010298073292\n",
      "[ epoch ] 407\n",
      "[ training ] training loss = 0.008627103641629219\n",
      "[ eval ] validation loss = 0.01324530690908432\n",
      "[ epoch ] 408\n",
      "[ training ] training loss = 0.008626744151115417\n",
      "[ eval ] validation loss = 0.013243628665804863\n",
      "[ epoch ] 409\n",
      "[ training ] training loss = 0.008626394905149937\n",
      "[ eval ] validation loss = 0.013241974636912346\n",
      "[ epoch ] 410\n",
      "[ training ] training loss = 0.008626054041087627\n",
      "[ eval ] validation loss = 0.013240346685051918\n",
      "[ epoch ] 411\n",
      "[ training ] training loss = 0.008625722490251064\n",
      "[ eval ] validation loss = 0.01323874294757843\n",
      "[ epoch ] 412\n",
      "[ training ] training loss = 0.008625400252640247\n",
      "[ eval ] validation loss = 0.01323715876787901\n",
      "[ epoch ] 413\n",
      "[ training ] training loss = 0.008625084534287453\n",
      "[ eval ] validation loss = 0.013235606253147125\n",
      "[ epoch ] 414\n",
      "[ training ] training loss = 0.00862477719783783\n",
      "[ eval ] validation loss = 0.013234080746769905\n",
      "[ epoch ] 415\n",
      "[ training ] training loss = 0.008624480105936527\n",
      "[ eval ] validation loss = 0.01323256827890873\n",
      "[ epoch ] 416\n",
      "[ training ] training loss = 0.008624186739325523\n",
      "[ eval ] validation loss = 0.013231074437499046\n",
      "[ epoch ] 417\n",
      "[ training ] training loss = 0.00862390361726284\n",
      "[ eval ] validation loss = 0.01322961412370205\n",
      "[ epoch ] 418\n",
      "[ training ] training loss = 0.008623624220490456\n",
      "[ eval ] validation loss = 0.013228178024291992\n",
      "[ epoch ] 419\n",
      "[ training ] training loss = 0.008623353205621243\n",
      "[ eval ] validation loss = 0.013226757757365704\n",
      "[ epoch ] 420\n",
      "[ training ] training loss = 0.008623090572655201\n",
      "[ eval ] validation loss = 0.013225359842181206\n",
      "[ epoch ] 421\n",
      "[ training ] training loss = 0.008622832596302032\n",
      "[ eval ] validation loss = 0.013223988935351372\n",
      "[ epoch ] 422\n",
      "[ training ] training loss = 0.008622581139206886\n",
      "[ eval ] validation loss = 0.013222629204392433\n",
      "[ epoch ] 423\n",
      "[ training ] training loss = 0.008622337132692337\n",
      "[ eval ] validation loss = 0.013221291825175285\n",
      "[ epoch ] 424\n",
      "[ training ] training loss = 0.008622100576758385\n",
      "[ eval ] validation loss = 0.013219974935054779\n",
      "[ epoch ] 425\n",
      "[ training ] training loss = 0.008621865883469582\n",
      "[ eval ] validation loss = 0.01321867574006319\n",
      "[ epoch ] 426\n",
      "[ training ] training loss = 0.0086216414347291\n",
      "[ eval ] validation loss = 0.013217395171523094\n",
      "[ epoch ] 427\n",
      "[ training ] training loss = 0.008621422573924065\n",
      "[ eval ] validation loss = 0.013216136023402214\n",
      "[ epoch ] 428\n",
      "[ training ] training loss = 0.00862120557576418\n",
      "[ eval ] validation loss = 0.013214895501732826\n",
      "[ epoch ] 429\n",
      "[ training ] training loss = 0.008620998822152615\n",
      "[ eval ] validation loss = 0.013213679194450378\n",
      "[ epoch ] 430\n",
      "[ training ] training loss = 0.008620795793831348\n",
      "[ eval ] validation loss = 0.013212477788329124\n",
      "[ epoch ] 431\n",
      "[ training ] training loss = 0.008620592765510082\n",
      "[ eval ] validation loss = 0.013211289420723915\n",
      "[ epoch ] 432\n",
      "[ training ] training loss = 0.008620398119091988\n",
      "[ eval ] validation loss = 0.01321012806147337\n",
      "[ epoch ] 433\n",
      "[ training ] training loss = 0.008620207197964191\n",
      "[ eval ] validation loss = 0.01320897787809372\n",
      "[ epoch ] 434\n",
      "[ training ] training loss = 0.008620022796094418\n",
      "[ eval ] validation loss = 0.013207848183810711\n",
      "[ epoch ] 435\n",
      "[ training ] training loss = 0.008619843050837517\n",
      "[ eval ] validation loss = 0.013206723146140575\n",
      "[ epoch ] 436\n",
      "[ training ] training loss = 0.008619667962193489\n",
      "[ eval ] validation loss = 0.01320562232285738\n",
      "[ epoch ] 437\n",
      "[ training ] training loss = 0.008619499392807484\n",
      "[ eval ] validation loss = 0.013204539194703102\n",
      "[ epoch ] 438\n",
      "[ training ] training loss = 0.008619330823421478\n",
      "[ eval ] validation loss = 0.013203469105064869\n",
      "[ epoch ] 439\n",
      "[ training ] training loss = 0.008619166910648346\n",
      "[ eval ] validation loss = 0.01320241391658783\n",
      "[ epoch ] 440\n",
      "[ training ] training loss = 0.008619011379778385\n",
      "[ eval ] validation loss = 0.013201375491917133\n",
      "[ epoch ] 441\n",
      "[ training ] training loss = 0.008618855848908424\n",
      "[ eval ] validation loss = 0.013200352899730206\n",
      "[ epoch ] 442\n",
      "[ training ] training loss = 0.008618703112006187\n",
      "[ eval ] validation loss = 0.013199339620769024\n",
      "[ epoch ] 443\n",
      "[ training ] training loss = 0.008618559688329697\n",
      "[ eval ] validation loss = 0.013198347762227058\n",
      "[ epoch ] 444\n",
      "[ training ] training loss = 0.008618418127298355\n",
      "[ eval ] validation loss = 0.013197365216910839\n",
      "[ epoch ] 445\n",
      "[ training ] training loss = 0.008618277497589588\n",
      "[ eval ] validation loss = 0.013196396641433239\n",
      "[ epoch ] 446\n",
      "[ training ] training loss = 0.008618139661848545\n",
      "[ eval ] validation loss = 0.013195452280342579\n",
      "[ epoch ] 447\n",
      "[ training ] training loss = 0.008618006482720375\n",
      "[ eval ] validation loss = 0.013194508850574493\n",
      "[ epoch ] 448\n",
      "[ training ] training loss = 0.008617878891527653\n",
      "[ eval ] validation loss = 0.013193587772548199\n",
      "[ epoch ] 449\n",
      "[ training ] training loss = 0.00861775130033493\n",
      "[ eval ] validation loss = 0.0131926778703928\n",
      "[ epoch ] 450\n",
      "[ training ] training loss = 0.008617626503109932\n",
      "[ eval ] validation loss = 0.01319178193807602\n",
      "[ epoch ] 451\n",
      "[ training ] training loss = 0.008617507293820381\n",
      "[ eval ] validation loss = 0.013190890662372112\n",
      "[ epoch ] 452\n",
      "[ training ] training loss = 0.008617394603788853\n",
      "[ eval ] validation loss = 0.013190018013119698\n",
      "[ epoch ] 453\n",
      "[ training ] training loss = 0.0086172791197896\n",
      "[ eval ] validation loss = 0.013189157471060753\n",
      "[ epoch ] 454\n",
      "[ training ] training loss = 0.008617169223725796\n",
      "[ eval ] validation loss = 0.013188311830163002\n",
      "[ epoch ] 455\n",
      "[ training ] training loss = 0.008617060258984566\n",
      "[ eval ] validation loss = 0.013187477365136147\n",
      "[ epoch ] 456\n",
      "[ training ] training loss = 0.008616955950856209\n",
      "[ eval ] validation loss = 0.013186657801270485\n",
      "[ epoch ] 457\n",
      "[ training ] training loss = 0.008616853505373001\n",
      "[ eval ] validation loss = 0.01318584568798542\n",
      "[ epoch ] 458\n",
      "[ training ] training loss = 0.008616751059889793\n",
      "[ eval ] validation loss = 0.013185044750571251\n",
      "[ epoch ] 459\n",
      "[ training ] training loss = 0.008616654202342033\n",
      "[ eval ] validation loss = 0.013184254989027977\n",
      "[ epoch ] 460\n",
      "[ training ] training loss = 0.008616561070084572\n",
      "[ eval ] validation loss = 0.013183477334678173\n",
      "[ epoch ] 461\n",
      "[ training ] training loss = 0.008616464212536812\n",
      "[ eval ] validation loss = 0.013182714581489563\n",
      "[ epoch ] 462\n",
      "[ training ] training loss = 0.008616373874247074\n",
      "[ eval ] validation loss = 0.0131819574162364\n",
      "[ epoch ] 463\n",
      "[ training ] training loss = 0.008616289123892784\n",
      "[ eval ] validation loss = 0.013181209564208984\n",
      "[ epoch ] 464\n",
      "[ training ] training loss = 0.008616200648248196\n",
      "[ eval ] validation loss = 0.013180481269955635\n",
      "[ epoch ] 465\n",
      "[ training ] training loss = 0.008616117760539055\n",
      "[ eval ] validation loss = 0.013179756700992584\n",
      "[ epoch ] 466\n",
      "[ training ] training loss = 0.008616035804152489\n",
      "[ eval ] validation loss = 0.013179036788642406\n",
      "[ epoch ] 467\n",
      "[ training ] training loss = 0.008615957573056221\n",
      "[ eval ] validation loss = 0.013178333640098572\n",
      "[ epoch ] 468\n",
      "[ training ] training loss = 0.008615879341959953\n",
      "[ eval ] validation loss = 0.013177642598748207\n",
      "[ epoch ] 469\n",
      "[ training ] training loss = 0.008615801110863686\n",
      "[ eval ] validation loss = 0.01317695714533329\n",
      "[ epoch ] 470\n",
      "[ training ] training loss = 0.008615728467702866\n",
      "[ eval ] validation loss = 0.013176282867789268\n",
      "[ epoch ] 471\n",
      "[ training ] training loss = 0.00861565675586462\n",
      "[ eval ] validation loss = 0.013175616972148418\n",
      "[ epoch ] 472\n",
      "[ training ] training loss = 0.008615585044026375\n",
      "[ eval ] validation loss = 0.013174965977668762\n",
      "[ epoch ] 473\n",
      "[ training ] training loss = 0.008615517988801003\n",
      "[ eval ] validation loss = 0.013174320571124554\n",
      "[ epoch ] 474\n",
      "[ training ] training loss = 0.008615451864898205\n",
      "[ eval ] validation loss = 0.013173682615160942\n",
      "[ epoch ] 475\n",
      "[ training ] training loss = 0.008615386672317982\n",
      "[ eval ] validation loss = 0.013173049315810204\n",
      "[ epoch ] 476\n",
      "[ training ] training loss = 0.008615323342382908\n",
      "[ eval ] validation loss = 0.013172435574233532\n",
      "[ epoch ] 477\n",
      "[ training ] training loss = 0.008615263737738132\n",
      "[ eval ] validation loss = 0.01317182369530201\n",
      "[ epoch ] 478\n",
      "[ training ] training loss = 0.008615202270448208\n",
      "[ eval ] validation loss = 0.013171220198273659\n",
      "[ epoch ] 479\n",
      "[ training ] training loss = 0.008615141734480858\n",
      "[ eval ] validation loss = 0.013170626945793629\n",
      "[ epoch ] 480\n",
      "[ training ] training loss = 0.008615085855126381\n",
      "[ eval ] validation loss = 0.013170033693313599\n",
      "[ epoch ] 481\n",
      "[ training ] training loss = 0.008615029975771904\n",
      "[ eval ] validation loss = 0.013169461861252785\n",
      "[ epoch ] 482\n",
      "[ training ] training loss = 0.008614977821707726\n",
      "[ eval ] validation loss = 0.013168888166546822\n",
      "[ epoch ] 483\n",
      "[ training ] training loss = 0.008614922873675823\n",
      "[ eval ] validation loss = 0.013168330304324627\n",
      "[ epoch ] 484\n",
      "[ training ] training loss = 0.008614873513579369\n",
      "[ eval ] validation loss = 0.013167771510779858\n",
      "[ epoch ] 485\n",
      "[ training ] training loss = 0.008614824153482914\n",
      "[ eval ] validation loss = 0.013167224824428558\n",
      "[ epoch ] 486\n",
      "[ training ] training loss = 0.008614770136773586\n",
      "[ eval ] validation loss = 0.013166690245270729\n",
      "[ epoch ] 487\n",
      "[ training ] training loss = 0.00861472450196743\n",
      "[ eval ] validation loss = 0.01316615380346775\n",
      "[ epoch ] 488\n",
      "[ training ] training loss = 0.008614678867161274\n",
      "[ eval ] validation loss = 0.013165625743567944\n",
      "[ epoch ] 489\n",
      "[ training ] training loss = 0.008614631369709969\n",
      "[ eval ] validation loss = 0.013165111653506756\n",
      "[ epoch ] 490\n",
      "[ training ] training loss = 0.008614589460194111\n",
      "[ eval ] validation loss = 0.013164606876671314\n",
      "[ epoch ] 491\n",
      "[ training ] training loss = 0.00861454475671053\n",
      "[ eval ] validation loss = 0.013164103031158447\n",
      "[ epoch ] 492\n",
      "[ training ] training loss = 0.008614500984549522\n",
      "[ eval ] validation loss = 0.013163608498871326\n",
      "[ epoch ] 493\n",
      "[ training ] training loss = 0.00861446000635624\n",
      "[ eval ] validation loss = 0.01316311676055193\n",
      "[ epoch ] 494\n",
      "[ training ] training loss = 0.008614422753453255\n",
      "[ eval ] validation loss = 0.013162640854716301\n",
      "[ epoch ] 495\n",
      "[ training ] training loss = 0.008614380843937397\n",
      "[ eval ] validation loss = 0.013162165880203247\n",
      "[ epoch ] 496\n",
      "[ training ] training loss = 0.008614341728389263\n",
      "[ eval ] validation loss = 0.013161690905690193\n",
      "[ epoch ] 497\n",
      "[ training ] training loss = 0.008614307269454002\n",
      "[ eval ] validation loss = 0.013161229901015759\n",
      "[ epoch ] 498\n",
      "[ training ] training loss = 0.008614272810518742\n",
      "[ eval ] validation loss = 0.013160770758986473\n",
      "[ epoch ] 499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ training ] training loss = 0.008614237420260906\n",
      "[ eval ] validation loss = 0.01316031627357006\n",
      "linear.weight = tensor([2.9844], requires_grad=True), linear.bias = tensor([-0.9730], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# set training routine\n",
    "lr = 1e-1\n",
    "n_epochs = 500\n",
    "\n",
    "# train model\n",
    "losses = []\n",
    "val_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"[ epoch ]\", epoch)\n",
    "    yhat = bias + weight * x_train_tensor\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "    losses.append(loss.item())\n",
    "    print(\"[ training ] training loss = {}\".format(loss))\n",
    "    # calculate gradients\n",
    "    loss.backward()\n",
    "    # update weight and bias\n",
    "    with torch.no_grad():\n",
    "        bias -= lr * bias.grad\n",
    "        weight -= lr * weight.grad\n",
    "    # zero out grads\n",
    "    bias.grad.zero_()\n",
    "    weight.grad.zero_()\n",
    "\n",
    "    # eval\n",
    "    with torch.no_grad():\n",
    "        yhat = bias + weight * x_val_tensor\n",
    "        error = y_val_tensor - yhat\n",
    "        val_loss = (error ** 2).mean()\n",
    "        val_losses.append(val_loss.item())\n",
    "        print(\"[ eval ] validation loss = {}\".format(val_loss))\n",
    "    \n",
    "print(\"linear.weight = {}, linear.bias = {}\".format(weight, bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(n_epochs), losses, label='train_loss', color='b')\n",
    "plt.scatter(range(n_epochs), val_losses, label='val_loss', color='r')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('mse')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('pytorch_p36': conda)",
   "language": "python",
   "name": "python361064bitpytorchp36conda143b13e29122453f97130b8bdfe91e87"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
