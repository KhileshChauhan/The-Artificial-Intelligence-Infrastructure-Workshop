{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kevin/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1248d8650>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import module\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "\n",
    "# make game\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# seed the experiment\n",
    "env.seed(9)\n",
    "torch.manual_seed(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define util function\n",
    "def to_torch_tensor(np_arr):\n",
    "    return torch.from_numpy(np_arr).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our policy\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(Policy, self).__init__()\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.fc1 = nn.Linear(self.observation_space, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3 = nn.Linear(24, self.action_space)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "# define our agent\n",
    "class Agent:\n",
    "    def __init__(self, policy):\n",
    "        MEMORY_SIZE = 1000000\n",
    "        GAMMA = 0.95\n",
    "        EXPLORATION_MAX = 1.0\n",
    "        EXPLORATION_MIN = 0.01\n",
    "        EXPLORATION_DECAY = 0.995\n",
    "\n",
    "        self.policy = policy\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-2)\n",
    "        self.loss_fn = nn.MSELoss(reduction='mean')\n",
    "        self.memory = []\n",
    "        self.gamma = GAMMA\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.exploration_min = EXPLORATION_MIN\n",
    "        self.exploration_decay = EXPLORATION_DECAY\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.policy.action_space)\n",
    "        q_values = self.policy(to_torch_tensor(state))\n",
    "        return int(q_values.max(0)[-1])\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def experience_replay(self):\n",
    "        print(\"[ Experience replay ] starts\")\n",
    "        for state, action, reward, state_next, done in self.memory:\n",
    "            if not done:\n",
    "                # Q function (bellman eqution): q value = reward at current step + gamma * q value of next step by taking an optimal action\n",
    "                q_value_to_update = (reward + self.gamma * torch.max(self.policy(to_torch_tensor(state_next))))\n",
    "                # remove this tensor from the autograph\n",
    "                q_value_to_update = q_value_to_update.clone().detach()\n",
    "            else:\n",
    "                q_value_to_update = reward\n",
    "            q_values_hat = self.policy(to_torch_tensor(state))\n",
    "            # generate target\n",
    "            q_values_target = q_values_hat.clone().detach()\n",
    "            q_values_target[action] = q_value_to_update\n",
    "            # train policy            \n",
    "            policy_loss = self.loss_fn(q_values_target, q_values_hat)\n",
    "            policy_loss.backward()    \n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        # the more policy gets replayed, the less the agent explores\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        self.exploration_rate = max(self.exploration_min, self.exploration_rate)\n",
    "        \n",
    "        # clean up\n",
    "        self.memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 1 ] state=[-0.00551277  0.02101743  0.00884103  0.02545213]\n",
      "[ episode 1 ][ timestamp 1 ] state=[-0.00551277  0.02101743  0.00884103  0.02545213], action=0, reward=1.0, next_state=[-0.00509242 -0.17423018  0.00935007  0.32091134]\n",
      "[ episode 1 ][ timestamp 2 ] state=[-0.00509242 -0.17423018  0.00935007  0.32091134], action=1, reward=1.0, next_state=[-0.00857702  0.02075737  0.0157683   0.03119167]\n",
      "[ episode 1 ][ timestamp 3 ] state=[-0.00857702  0.02075737  0.0157683   0.03119167], action=0, reward=1.0, next_state=[-0.00816188 -0.17458711  0.01639213  0.32880766]\n",
      "[ episode 1 ][ timestamp 4 ] state=[-0.00816188 -0.17458711  0.01639213  0.32880766], action=1, reward=1.0, next_state=[-0.01165362  0.02029769  0.02296828  0.04133879]\n",
      "[ episode 1 ][ timestamp 5 ] state=[-0.01165362  0.02029769  0.02296828  0.04133879], action=0, reward=1.0, next_state=[-0.01124767 -0.17514596  0.02379506  0.34117903]\n",
      "[ episode 1 ][ timestamp 6 ] state=[-0.01124767 -0.17514596  0.02379506  0.34117903], action=0, reward=1.0, next_state=[-0.01475059 -0.37059824  0.03061864  0.64126954]\n",
      "[ episode 1 ][ timestamp 7 ] state=[-0.01475059 -0.37059824  0.03061864  0.64126954], action=0, reward=1.0, next_state=[-0.02216255 -0.56613334  0.04344403  0.94343519]\n",
      "[ episode 1 ][ timestamp 8 ] state=[-0.02216255 -0.56613334  0.04344403  0.94343519], action=0, reward=1.0, next_state=[-0.03348522 -0.76181284  0.06231273  1.24944602]\n",
      "[ episode 1 ][ timestamp 9 ] state=[-0.03348522 -0.76181284  0.06231273  1.24944602], action=1, reward=1.0, next_state=[-0.04872147 -0.56754252  0.08730165  0.97691418]\n",
      "[ episode 1 ][ timestamp 10 ] state=[-0.04872147 -0.56754252  0.08730165  0.97691418], action=1, reward=1.0, next_state=[-0.06007232 -0.3736929   0.10683994  0.71288122]\n",
      "[ episode 1 ][ timestamp 11 ] state=[-0.06007232 -0.3736929   0.10683994  0.71288122], action=0, reward=1.0, next_state=[-0.06754618 -0.57011915  0.12109756  1.0371918 ]\n",
      "[ episode 1 ][ timestamp 12 ] state=[-0.06754618 -0.57011915  0.12109756  1.0371918 ], action=0, reward=1.0, next_state=[-0.07894856 -0.76662427  0.1418414   1.3653066 ]\n",
      "[ episode 1 ][ timestamp 13 ] state=[-0.07894856 -0.76662427  0.1418414   1.3653066 ], action=0, reward=1.0, next_state=[-0.09428105 -0.96320871  0.16914753  1.69878361]\n",
      "[ episode 1 ][ timestamp 14 ] state=[-0.09428105 -0.96320871  0.16914753  1.69878361], action=1, reward=1.0, next_state=[-0.11354522 -0.77039244  0.2031232   1.4631794 ]\n",
      "[ episode 1 ][ timestamp 15 ] state=[-0.11354522 -0.77039244  0.2031232   1.4631794 ], action=1, reward=-1.0, next_state=[-0.12895307 -0.57825488  0.23238679  1.24020662]\n",
      "[ Ended! ] Episode 1: Exploration_rate=1.0. Score=15.\n",
      "[ Experience replay ] starts\n",
      "[ episode 2 ] state=[ 0.04526127 -0.00615054 -0.03334226  0.00441591]\n",
      "[ episode 2 ][ timestamp 1 ] state=[ 0.04526127 -0.00615054 -0.03334226  0.00441591], action=0, reward=1.0, next_state=[ 0.04513826 -0.20077884 -0.03325395  0.28639529]\n",
      "[ episode 2 ][ timestamp 2 ] state=[ 0.04513826 -0.20077884 -0.03325395  0.28639529], action=1, reward=1.0, next_state=[ 0.04112268 -0.00519881 -0.02752604 -0.01658742]\n",
      "[ episode 2 ][ timestamp 3 ] state=[ 0.04112268 -0.00519881 -0.02752604 -0.01658742], action=0, reward=1.0, next_state=[ 0.0410187  -0.19991541 -0.02785779  0.2672852 ]\n",
      "[ episode 2 ][ timestamp 4 ] state=[ 0.0410187  -0.19991541 -0.02785779  0.2672852 ], action=0, reward=1.0, next_state=[ 0.0370204  -0.39462894 -0.02251208  0.55105304]\n",
      "[ episode 2 ][ timestamp 5 ] state=[ 0.0370204  -0.39462894 -0.02251208  0.55105304], action=1, reward=1.0, next_state=[ 0.02912782 -0.19919815 -0.01149102  0.25136314]\n",
      "[ episode 2 ][ timestamp 6 ] state=[ 0.02912782 -0.19919815 -0.01149102  0.25136314], action=1, reward=1.0, next_state=[ 0.02514385 -0.00391401 -0.00646376 -0.04492201]\n",
      "[ episode 2 ][ timestamp 7 ] state=[ 0.02514385 -0.00391401 -0.00646376 -0.04492201], action=0, reward=1.0, next_state=[ 0.02506557 -0.19894268 -0.0073622   0.24571455]\n",
      "[ episode 2 ][ timestamp 8 ] state=[ 0.02506557 -0.19894268 -0.0073622   0.24571455], action=1, reward=1.0, next_state=[ 0.02108672 -0.00371636 -0.00244791 -0.04928147]\n",
      "[ episode 2 ][ timestamp 9 ] state=[ 0.02108672 -0.00371636 -0.00244791 -0.04928147], action=1, reward=1.0, next_state=[ 0.02101239  0.19144061 -0.00343354 -0.34273572]\n",
      "[ episode 2 ][ timestamp 10 ] state=[ 0.02101239  0.19144061 -0.00343354 -0.34273572], action=0, reward=1.0, next_state=[ 0.02484121 -0.00363233 -0.01028825 -0.05113751]\n",
      "[ episode 2 ][ timestamp 11 ] state=[ 0.02484121 -0.00363233 -0.01028825 -0.05113751], action=1, reward=1.0, next_state=[ 0.02476856  0.19163562 -0.011311   -0.34704862]\n",
      "[ episode 2 ][ timestamp 12 ] state=[ 0.02476856  0.19163562 -0.011311   -0.34704862], action=1, reward=1.0, next_state=[ 0.02860127  0.38691662 -0.01825198 -0.64327675]\n",
      "[ episode 2 ][ timestamp 13 ] state=[ 0.02860127  0.38691662 -0.01825198 -0.64327675], action=1, reward=1.0, next_state=[ 0.0363396   0.58228814 -0.03111751 -0.941651  ]\n",
      "[ episode 2 ][ timestamp 14 ] state=[ 0.0363396   0.58228814 -0.03111751 -0.941651  ], action=0, reward=1.0, next_state=[ 0.04798537  0.38759905 -0.04995053 -0.65890581]\n",
      "[ episode 2 ][ timestamp 15 ] state=[ 0.04798537  0.38759905 -0.04995053 -0.65890581], action=0, reward=1.0, next_state=[ 0.05573735  0.19320658 -0.06312865 -0.38236015]\n",
      "[ episode 2 ][ timestamp 16 ] state=[ 0.05573735  0.19320658 -0.06312865 -0.38236015], action=0, reward=1.0, next_state=[ 0.05960148 -0.00096487 -0.07077585 -0.11023064]\n",
      "[ episode 2 ][ timestamp 17 ] state=[ 0.05960148 -0.00096487 -0.07077585 -0.11023064], action=1, reward=1.0, next_state=[ 0.05958218  0.19509614 -0.07298046 -0.42437662]\n",
      "[ episode 2 ][ timestamp 18 ] state=[ 0.05958218  0.19509614 -0.07298046 -0.42437662], action=0, reward=1.0, next_state=[ 0.0634841   0.00107979 -0.081468   -0.15556398]\n",
      "[ episode 2 ][ timestamp 19 ] state=[ 0.0634841   0.00107979 -0.081468   -0.15556398], action=0, reward=1.0, next_state=[ 0.0635057  -0.19278693 -0.08457928  0.11034651]\n",
      "[ episode 2 ][ timestamp 20 ] state=[ 0.0635057  -0.19278693 -0.08457928  0.11034651], action=0, reward=1.0, next_state=[ 0.05964996 -0.38660146 -0.08237234  0.3751924 ]\n",
      "[ episode 2 ][ timestamp 21 ] state=[ 0.05964996 -0.38660146 -0.08237234  0.3751924 ], action=0, reward=1.0, next_state=[ 0.05191793 -0.58046268 -0.0748685   0.64080815]\n",
      "[ episode 2 ][ timestamp 22 ] state=[ 0.05191793 -0.58046268 -0.0748685   0.64080815], action=1, reward=1.0, next_state=[ 0.04030868 -0.38438125 -0.06205233  0.32551916]\n",
      "[ episode 2 ][ timestamp 23 ] state=[ 0.04030868 -0.38438125 -0.06205233  0.32551916], action=1, reward=1.0, next_state=[ 0.03262105 -0.18843321 -0.05554195  0.0139311 ]\n",
      "[ episode 2 ][ timestamp 24 ] state=[ 0.03262105 -0.18843321 -0.05554195  0.0139311 ], action=0, reward=1.0, next_state=[ 0.02885239 -0.38271644 -0.05526333  0.28858563]\n",
      "[ episode 2 ][ timestamp 25 ] state=[ 0.02885239 -0.38271644 -0.05526333  0.28858563], action=1, reward=1.0, next_state=[ 0.02119806 -0.18685176 -0.04949162 -0.02100203]\n",
      "[ episode 2 ][ timestamp 26 ] state=[ 0.02119806 -0.18685176 -0.04949162 -0.02100203], action=1, reward=1.0, next_state=[ 0.01746103  0.00894373 -0.04991166 -0.32888025]\n",
      "[ episode 2 ][ timestamp 27 ] state=[ 0.01746103  0.00894373 -0.04991166 -0.32888025], action=1, reward=1.0, next_state=[ 0.0176399   0.20473937 -0.05648926 -0.6368759 ]\n",
      "[ episode 2 ][ timestamp 28 ] state=[ 0.0176399   0.20473937 -0.05648926 -0.6368759 ], action=0, reward=1.0, next_state=[ 0.02173469  0.01044881 -0.06922678 -0.36250394]\n",
      "[ episode 2 ][ timestamp 29 ] state=[ 0.02173469  0.01044881 -0.06922678 -0.36250394], action=1, reward=1.0, next_state=[ 0.02194366  0.20648291 -0.07647686 -0.67618719]\n",
      "[ episode 2 ][ timestamp 30 ] state=[ 0.02194366  0.20648291 -0.07647686 -0.67618719], action=0, reward=1.0, next_state=[ 0.02607332  0.01250225 -0.0900006  -0.40852897]\n",
      "[ episode 2 ][ timestamp 31 ] state=[ 0.02607332  0.01250225 -0.0900006  -0.40852897], action=0, reward=1.0, next_state=[ 0.02632337 -0.18123604 -0.09817118 -0.14552219]\n",
      "[ episode 2 ][ timestamp 32 ] state=[ 0.02632337 -0.18123604 -0.09817118 -0.14552219], action=1, reward=1.0, next_state=[ 0.02269865  0.01514474 -0.10108163 -0.46749101]\n",
      "[ episode 2 ][ timestamp 33 ] state=[ 0.02269865  0.01514474 -0.10108163 -0.46749101], action=1, reward=1.0, next_state=[ 0.02300154  0.21153865 -0.11043145 -0.79024558]\n",
      "[ episode 2 ][ timestamp 34 ] state=[ 0.02300154  0.21153865 -0.11043145 -0.79024558], action=1, reward=1.0, next_state=[ 0.02723231  0.40798977 -0.12623636 -1.11552818]\n",
      "[ episode 2 ][ timestamp 35 ] state=[ 0.02723231  0.40798977 -0.12623636 -1.11552818], action=0, reward=1.0, next_state=[ 0.03539211  0.21473036 -0.14854692 -0.86496078]\n",
      "[ episode 2 ][ timestamp 36 ] state=[ 0.03539211  0.21473036 -0.14854692 -0.86496078], action=1, reward=1.0, next_state=[ 0.03968672  0.41152804 -0.16584614 -1.20041871]\n",
      "[ episode 2 ][ timestamp 37 ] state=[ 0.03968672  0.41152804 -0.16584614 -1.20041871], action=0, reward=1.0, next_state=[ 0.04791728  0.21889325 -0.18985451 -0.96396679]\n",
      "[ episode 2 ][ timestamp 38 ] state=[ 0.04791728  0.21889325 -0.18985451 -0.96396679], action=1, reward=1.0, next_state=[ 0.05229514  0.41598835 -0.20913385 -1.30977974]\n",
      "[ episode 2 ][ timestamp 39 ] state=[ 0.05229514  0.41598835 -0.20913385 -1.30977974], action=1, reward=-1.0, next_state=[ 0.06061491  0.61305389 -0.23532944 -1.65997544]\n",
      "[ Ended! ] Episode 2: Exploration_rate=0.995. Score=39.\n",
      "[ Experience replay ] starts\n",
      "[ episode 3 ] state=[-0.00796233  0.00449342 -0.04318868  0.00635115]\n",
      "[ episode 3 ][ timestamp 1 ] state=[-0.00796233  0.00449342 -0.04318868  0.00635115], action=1, reward=1.0, next_state=[-0.00787246  0.2002073  -0.04306165 -0.29963943]\n",
      "[ episode 3 ][ timestamp 2 ] state=[-0.00787246  0.2002073  -0.04306165 -0.29963943], action=0, reward=1.0, next_state=[-0.00386832  0.00572475 -0.04905444 -0.02084225]\n",
      "[ episode 3 ][ timestamp 3 ] state=[-0.00386832  0.00572475 -0.04905444 -0.02084225], action=1, reward=1.0, next_state=[-0.00375382  0.20151462 -0.04947129 -0.32859   ]\n",
      "[ episode 3 ][ timestamp 4 ] state=[-0.00375382  0.20151462 -0.04947129 -0.32859   ], action=1, reward=1.0, next_state=[ 2.76469671e-04  3.97304661e-01 -5.60430866e-02 -6.36454377e-01]\n",
      "[ episode 3 ][ timestamp 5 ] state=[ 2.76469671e-04  3.97304661e-01 -5.60430866e-02 -6.36454377e-01], action=0, reward=1.0, next_state=[ 0.00822256  0.20300724 -0.06877217 -0.36193386]\n",
      "[ episode 3 ][ timestamp 6 ] state=[ 0.00822256  0.20300724 -0.06877217 -0.36193386], action=1, reward=1.0, next_state=[ 0.01228271  0.39903586 -0.07601085 -0.6754848 ]\n",
      "[ episode 3 ][ timestamp 7 ] state=[ 0.01228271  0.39903586 -0.07601085 -0.6754848 ], action=1, reward=1.0, next_state=[ 0.02026342  0.59512718 -0.08952055 -0.99109815]\n",
      "[ episode 3 ][ timestamp 8 ] state=[ 0.02026342  0.59512718 -0.08952055 -0.99109815], action=1, reward=1.0, next_state=[ 0.03216597  0.79132573 -0.10934251 -1.31050142]\n",
      "[ episode 3 ][ timestamp 9 ] state=[ 0.03216597  0.79132573 -0.10934251 -1.31050142], action=1, reward=1.0, next_state=[ 0.04799248  0.98764943 -0.13555254 -1.63531101]\n",
      "[ episode 3 ][ timestamp 10 ] state=[ 0.04799248  0.98764943 -0.13555254 -1.63531101], action=1, reward=1.0, next_state=[ 0.06774547  1.18407663 -0.16825876 -1.96697952]\n",
      "[ episode 3 ][ timestamp 11 ] state=[ 0.06774547  1.18407663 -0.16825876 -1.96697952], action=1, reward=1.0, next_state=[ 0.091427    1.38053111 -0.20759835 -2.30673469]\n",
      "[ episode 3 ][ timestamp 12 ] state=[ 0.091427    1.38053111 -0.20759835 -2.30673469], action=1, reward=-1.0, next_state=[ 0.11903763  1.57686526 -0.25373304 -2.65550907]\n",
      "[ Ended! ] Episode 3: Exploration_rate=0.990025. Score=12.\n",
      "[ Experience replay ] starts\n",
      "[ episode 4 ] state=[ 0.03516278  0.02257556 -0.00252319  0.04904582]\n",
      "[ episode 4 ][ timestamp 1 ] state=[ 0.03516278  0.02257556 -0.00252319  0.04904582], action=0, reward=1.0, next_state=[ 0.03561429 -0.17251012 -0.00154227  0.34093159]\n",
      "[ episode 4 ][ timestamp 2 ] state=[ 0.03561429 -0.17251012 -0.00154227  0.34093159], action=1, reward=1.0, next_state=[0.03216409 0.02263375 0.00527636 0.04776272]\n",
      "[ episode 4 ][ timestamp 3 ] state=[0.03216409 0.02263375 0.00527636 0.04776272], action=1, reward=1.0, next_state=[ 0.03261676  0.21767964  0.00623162 -0.2432508 ]\n",
      "[ episode 4 ][ timestamp 4 ] state=[ 0.03261676  0.21767964  0.00623162 -0.2432508 ], action=1, reward=1.0, next_state=[ 0.03697036  0.41271203  0.0013666  -0.53396162]\n",
      "[ episode 4 ][ timestamp 5 ] state=[ 0.03697036  0.41271203  0.0013666  -0.53396162], action=0, reward=1.0, next_state=[ 0.0452246   0.21757089 -0.00931263 -0.2408484 ]\n",
      "[ episode 4 ][ timestamp 6 ] state=[ 0.0452246   0.21757089 -0.00931263 -0.2408484 ], action=0, reward=1.0, next_state=[ 0.04957601  0.02258319 -0.0141296   0.04888258]\n",
      "[ episode 4 ][ timestamp 7 ] state=[ 0.04957601  0.02258319 -0.0141296   0.04888258], action=1, reward=1.0, next_state=[ 0.05002768  0.21790487 -0.01315195 -0.24822465]\n",
      "[ episode 4 ][ timestamp 8 ] state=[ 0.05002768  0.21790487 -0.01315195 -0.24822465], action=1, reward=1.0, next_state=[ 0.05438578  0.41321216 -0.01811644 -0.54502681]\n",
      "[ episode 4 ][ timestamp 9 ] state=[ 0.05438578  0.41321216 -0.01811644 -0.54502681], action=1, reward=1.0, next_state=[ 0.06265002  0.60858393 -0.02901698 -0.84336231]\n",
      "[ episode 4 ][ timestamp 10 ] state=[ 0.06265002  0.60858393 -0.02901698 -0.84336231], action=0, reward=1.0, next_state=[ 0.0748217   0.41386975 -0.04588422 -0.55994379]\n",
      "[ episode 4 ][ timestamp 11 ] state=[ 0.0748217   0.41386975 -0.04588422 -0.55994379], action=1, reward=1.0, next_state=[ 0.08309909  0.60960466 -0.0570831  -0.86672237]\n",
      "[ episode 4 ][ timestamp 12 ] state=[ 0.08309909  0.60960466 -0.0570831  -0.86672237], action=1, reward=1.0, next_state=[ 0.09529119  0.80545503 -0.07441755 -1.17679275]\n",
      "[ episode 4 ][ timestamp 13 ] state=[ 0.09529119  0.80545503 -0.07441755 -1.17679275], action=0, reward=1.0, next_state=[ 0.11140029  0.61137445 -0.0979534  -0.90833618]\n",
      "[ episode 4 ][ timestamp 14 ] state=[ 0.11140029  0.61137445 -0.0979534  -0.90833618], action=1, reward=1.0, next_state=[ 0.12362777  0.80767613 -0.11612013 -1.23012948]\n",
      "[ episode 4 ][ timestamp 15 ] state=[ 0.12362777  0.80767613 -0.11612013 -1.23012948], action=1, reward=1.0, next_state=[ 0.1397813   1.00408453 -0.14072271 -1.5568207 ]\n",
      "[ episode 4 ][ timestamp 16 ] state=[ 0.1397813   1.00408453 -0.14072271 -1.5568207 ], action=1, reward=1.0, next_state=[ 0.15986299  1.20058361 -0.17185913 -1.88989176]\n",
      "[ episode 4 ][ timestamp 17 ] state=[ 0.15986299  1.20058361 -0.17185913 -1.88989176], action=0, reward=-1.0, next_state=[ 0.18387466  1.00769498 -0.20965696 -1.65509935]\n",
      "[ Ended! ] Episode 4: Exploration_rate=0.985074875. Score=17.\n",
      "[ Experience replay ] starts\n",
      "[ episode 5 ] state=[-0.0392785   0.00260751 -0.02165696 -0.00071208]\n",
      "[ episode 5 ][ timestamp 1 ] state=[-0.0392785   0.00260751 -0.02165696 -0.00071208], action=0, reward=1.0, next_state=[-0.03922635 -0.19219726 -0.0216712   0.2850599 ]\n",
      "[ episode 5 ][ timestamp 2 ] state=[-0.03922635 -0.19219726 -0.0216712   0.2850599 ], action=1, reward=1.0, next_state=[-0.04307029  0.00322696 -0.01597    -0.01437843]\n",
      "[ episode 5 ][ timestamp 3 ] state=[-0.04307029  0.00322696 -0.01597    -0.01437843], action=0, reward=1.0, next_state=[-0.04300575 -0.19166237 -0.01625757  0.2732233 ]\n",
      "[ episode 5 ][ timestamp 4 ] state=[-0.04300575 -0.19166237 -0.01625757  0.2732233 ], action=1, reward=1.0, next_state=[-0.046839    0.00368774 -0.0107931  -0.02454265]\n",
      "[ episode 5 ][ timestamp 5 ] state=[-0.046839    0.00368774 -0.0107931  -0.02454265], action=0, reward=1.0, next_state=[-0.04676525 -0.19127778 -0.01128396  0.26471549]\n",
      "[ episode 5 ][ timestamp 6 ] state=[-0.04676525 -0.19127778 -0.01128396  0.26471549], action=0, reward=1.0, next_state=[-0.0505908  -0.38623687 -0.00598965  0.55381809]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 5 ][ timestamp 7 ] state=[-0.0505908  -0.38623687 -0.00598965  0.55381809], action=1, reward=1.0, next_state=[-0.05831554 -0.19103133  0.00508672  0.25925408]\n",
      "[ episode 5 ][ timestamp 8 ] state=[-0.05831554 -0.19103133  0.00508672  0.25925408], action=1, reward=1.0, next_state=[-0.06213617  0.00401764  0.0102718  -0.03182009]\n",
      "[ episode 5 ][ timestamp 9 ] state=[-0.06213617  0.00401764  0.0102718  -0.03182009], action=1, reward=1.0, next_state=[-0.06205581  0.19899079  0.0096354  -0.32124454]\n",
      "[ episode 5 ][ timestamp 10 ] state=[-0.06205581  0.19899079  0.0096354  -0.32124454], action=0, reward=1.0, next_state=[-0.058076    0.00373296  0.00321051 -0.02553862]\n",
      "[ episode 5 ][ timestamp 11 ] state=[-0.058076    0.00373296  0.00321051 -0.02553862], action=0, reward=1.0, next_state=[-0.05800134 -0.19143488  0.00269973  0.26815552]\n",
      "[ episode 5 ][ timestamp 12 ] state=[-0.05800134 -0.19143488  0.00269973  0.26815552], action=1, reward=1.0, next_state=[-0.06183004  0.00364844  0.00806284 -0.02367467]\n",
      "[ episode 5 ][ timestamp 13 ] state=[-0.06183004  0.00364844  0.00806284 -0.02367467], action=0, reward=1.0, next_state=[-0.06175707 -0.19158821  0.00758935  0.27154123]\n",
      "[ episode 5 ][ timestamp 14 ] state=[-0.06175707 -0.19158821  0.00758935  0.27154123], action=0, reward=1.0, next_state=[-0.06558883 -0.38681763  0.01302017  0.56660818]\n",
      "[ episode 5 ][ timestamp 15 ] state=[-0.06558883 -0.38681763  0.01302017  0.56660818], action=1, reward=1.0, next_state=[-0.07332518 -0.19188073  0.02435234  0.27805543]\n",
      "[ episode 5 ][ timestamp 16 ] state=[-0.07332518 -0.19188073  0.02435234  0.27805543], action=0, reward=1.0, next_state=[-0.0771628  -0.38734147  0.02991345  0.57831849]\n",
      "[ episode 5 ][ timestamp 17 ] state=[-0.0771628  -0.38734147  0.02991345  0.57831849], action=1, reward=1.0, next_state=[-0.08490963 -0.19265125  0.04147982  0.29520705]\n",
      "[ episode 5 ][ timestamp 18 ] state=[-0.08490963 -0.19265125  0.04147982  0.29520705], action=0, reward=1.0, next_state=[-0.08876265 -0.38833925  0.04738396  0.60067814]\n",
      "[ episode 5 ][ timestamp 19 ] state=[-0.08876265 -0.38833925  0.04738396  0.60067814], action=0, reward=1.0, next_state=[-0.09652944 -0.58409093  0.05939752  0.90790176]\n",
      "[ episode 5 ][ timestamp 20 ] state=[-0.09652944 -0.58409093  0.05939752  0.90790176], action=1, reward=1.0, next_state=[-0.10821126 -0.38982118  0.07755556  0.63446364]\n",
      "[ episode 5 ][ timestamp 21 ] state=[-0.10821126 -0.38982118  0.07755556  0.63446364], action=0, reward=1.0, next_state=[-0.11600768 -0.58593437  0.09024483  0.95052765]\n",
      "[ episode 5 ][ timestamp 22 ] state=[-0.11600768 -0.58593437  0.09024483  0.95052765], action=1, reward=1.0, next_state=[-0.12772637 -0.39213538  0.10925538  0.68750809]\n",
      "[ episode 5 ][ timestamp 23 ] state=[-0.12772637 -0.39213538  0.10925538  0.68750809], action=0, reward=1.0, next_state=[-0.13556908 -0.58859059  0.12300554  1.01249109]\n",
      "[ episode 5 ][ timestamp 24 ] state=[-0.13556908 -0.58859059  0.12300554  1.01249109], action=1, reward=1.0, next_state=[-0.14734089 -0.39530517  0.14325537  0.76082606]\n",
      "[ episode 5 ][ timestamp 25 ] state=[-0.14734089 -0.39530517  0.14325537  0.76082606], action=1, reward=1.0, next_state=[-0.15524699 -0.20241676  0.15847189  0.5164304 ]\n",
      "[ episode 5 ][ timestamp 26 ] state=[-0.15524699 -0.20241676  0.15847189  0.5164304 ], action=1, reward=1.0, next_state=[-0.15929533 -0.00983953  0.16880049  0.27758013]\n",
      "[ episode 5 ][ timestamp 27 ] state=[-0.15929533 -0.00983953  0.16880049  0.27758013], action=0, reward=1.0, next_state=[-0.15949212 -0.20691698  0.1743521   0.61838671]\n",
      "[ episode 5 ][ timestamp 28 ] state=[-0.15949212 -0.20691698  0.1743521   0.61838671], action=0, reward=1.0, next_state=[-0.16363046 -0.40399038  0.18671983  0.96051532]\n",
      "[ episode 5 ][ timestamp 29 ] state=[-0.16363046 -0.40399038  0.18671983  0.96051532], action=0, reward=1.0, next_state=[-0.17171026 -0.60106434  0.20593014  1.30556528]\n",
      "[ episode 5 ][ timestamp 30 ] state=[-0.17171026 -0.60106434  0.20593014  1.30556528], action=1, reward=-1.0, next_state=[-0.18373155 -0.40906032  0.23204144  1.08376092]\n",
      "[ Ended! ] Episode 5: Exploration_rate=0.9801495006250001. Score=30.\n",
      "[ Experience replay ] starts\n",
      "[ episode 6 ] state=[ 0.02139753  0.00545731 -0.0183568  -0.03246379]\n",
      "[ episode 6 ][ timestamp 1 ] state=[ 0.02139753  0.00545731 -0.0183568  -0.03246379], action=0, reward=1.0, next_state=[ 0.02150668 -0.18939665 -0.01900608  0.25437131]\n",
      "[ episode 6 ][ timestamp 2 ] state=[ 0.02150668 -0.18939665 -0.01900608  0.25437131], action=1, reward=1.0, next_state=[ 0.01771874  0.00599144 -0.01391865 -0.04424536]\n",
      "[ episode 6 ][ timestamp 3 ] state=[ 0.01771874  0.00599144 -0.01391865 -0.04424536], action=0, reward=1.0, next_state=[ 0.01783857 -0.18892818 -0.01480356  0.24401381]\n",
      "[ episode 6 ][ timestamp 4 ] state=[ 0.01783857 -0.18892818 -0.01480356  0.24401381], action=1, reward=1.0, next_state=[ 0.01406001  0.00640205 -0.00992328 -0.05330152]\n",
      "[ episode 6 ][ timestamp 5 ] state=[ 0.01406001  0.00640205 -0.00992328 -0.05330152], action=0, reward=1.0, next_state=[ 0.01418805 -0.18857622 -0.01098931  0.23623409]\n",
      "[ episode 6 ][ timestamp 6 ] state=[ 0.01418805 -0.18857622 -0.01098931  0.23623409], action=1, reward=1.0, next_state=[ 0.01041653  0.006701   -0.00626463 -0.05989485]\n",
      "[ episode 6 ][ timestamp 7 ] state=[ 0.01041653  0.006701   -0.00626463 -0.05989485], action=1, reward=1.0, next_state=[ 0.01055055  0.20191221 -0.00746253 -0.35454771]\n",
      "[ episode 6 ][ timestamp 8 ] state=[ 0.01055055  0.20191221 -0.00746253 -0.35454771], action=0, reward=1.0, next_state=[ 0.01458879  0.00689716 -0.01455348 -0.06422724]\n",
      "[ episode 6 ][ timestamp 9 ] state=[ 0.01458879  0.00689716 -0.01455348 -0.06422724], action=1, reward=1.0, next_state=[ 0.01472673  0.20222471 -0.01583803 -0.36146611]\n",
      "[ episode 6 ][ timestamp 10 ] state=[ 0.01472673  0.20222471 -0.01583803 -0.36146611], action=0, reward=1.0, next_state=[ 0.01877123  0.00733142 -0.02306735 -0.07381903]\n",
      "[ episode 6 ][ timestamp 11 ] state=[ 0.01877123  0.00733142 -0.02306735 -0.07381903], action=1, reward=1.0, next_state=[ 0.01891786  0.20277634 -0.02454373 -0.37368962]\n",
      "[ episode 6 ][ timestamp 12 ] state=[ 0.01891786  0.20277634 -0.02454373 -0.37368962], action=0, reward=1.0, next_state=[ 0.02297338  0.00801148 -0.03201752 -0.08884545]\n",
      "[ episode 6 ][ timestamp 13 ] state=[ 0.02297338  0.00801148 -0.03201752 -0.08884545], action=1, reward=1.0, next_state=[ 0.02313361  0.20357739 -0.03379443 -0.3914555 ]\n",
      "[ episode 6 ][ timestamp 14 ] state=[ 0.02313361  0.20357739 -0.03379443 -0.3914555 ], action=1, reward=1.0, next_state=[ 0.02720516  0.39916224 -0.04162354 -0.69459894]\n",
      "[ episode 6 ][ timestamp 15 ] state=[ 0.02720516  0.39916224 -0.04162354 -0.69459894], action=0, reward=1.0, next_state=[ 0.0351884   0.2046416  -0.05551552 -0.41530449]\n",
      "[ episode 6 ][ timestamp 16 ] state=[ 0.0351884   0.2046416  -0.05551552 -0.41530449], action=1, reward=1.0, next_state=[ 0.03928124  0.40050463 -0.06382161 -0.7249596 ]\n",
      "[ episode 6 ][ timestamp 17 ] state=[ 0.03928124  0.40050463 -0.06382161 -0.7249596 ], action=1, reward=1.0, next_state=[ 0.04729133  0.59644837 -0.0783208  -1.03702765]\n",
      "[ episode 6 ][ timestamp 18 ] state=[ 0.04729133  0.59644837 -0.0783208  -1.03702765], action=1, reward=1.0, next_state=[ 0.0592203   0.79251906 -0.09906135 -1.35323487]\n",
      "[ episode 6 ][ timestamp 19 ] state=[ 0.0592203   0.79251906 -0.09906135 -1.35323487], action=0, reward=1.0, next_state=[ 0.07507068  0.59877049 -0.12612605 -1.09311324]\n",
      "[ episode 6 ][ timestamp 20 ] state=[ 0.07507068  0.59877049 -0.12612605 -1.09311324], action=0, reward=1.0, next_state=[ 0.08704609  0.40551536 -0.14798832 -0.84251601]\n",
      "[ episode 6 ][ timestamp 21 ] state=[ 0.08704609  0.40551536 -0.14798832 -0.84251601], action=1, reward=1.0, next_state=[ 0.09515639  0.60231365 -0.16483864 -1.17783678]\n",
      "[ episode 6 ][ timestamp 22 ] state=[ 0.09515639  0.60231365 -0.16483864 -1.17783678], action=0, reward=1.0, next_state=[ 0.10720267  0.40967059 -0.18839537 -0.94103254]\n",
      "[ episode 6 ][ timestamp 23 ] state=[ 0.10720267  0.40967059 -0.18839537 -0.94103254], action=1, reward=1.0, next_state=[ 0.11539608  0.6067633  -0.20721602 -1.28650174]\n",
      "[ episode 6 ][ timestamp 24 ] state=[ 0.11539608  0.6067633  -0.20721602 -1.28650174], action=1, reward=-1.0, next_state=[ 0.12753135  0.80383014 -0.23294606 -1.63626483]\n",
      "[ Ended! ] Episode 6: Exploration_rate=0.9752487531218751. Score=24.\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ] state=[-0.02530876 -0.02150658  0.00726012 -0.00122606]\n",
      "[ episode 7 ][ timestamp 1 ] state=[-0.02530876 -0.02150658  0.00726012 -0.00122606], action=0, reward=1.0, next_state=[-0.02573889 -0.2167319   0.0072356   0.29373865]\n",
      "[ episode 7 ][ timestamp 2 ] state=[-0.02573889 -0.2167319   0.0072356   0.29373865], action=1, reward=1.0, next_state=[-0.03007353 -0.02171385  0.01311038  0.00334649]\n",
      "[ episode 7 ][ timestamp 3 ] state=[-0.03007353 -0.02171385  0.01311038  0.00334649], action=0, reward=1.0, next_state=[-0.0305078  -0.21702135  0.01317731  0.30013689]\n",
      "[ episode 7 ][ timestamp 4 ] state=[-0.0305078  -0.21702135  0.01317731  0.30013689], action=0, reward=1.0, next_state=[-0.03484823 -0.41232862  0.01918004  0.59694638]\n",
      "[ episode 7 ][ timestamp 5 ] state=[-0.03484823 -0.41232862  0.01918004  0.59694638], action=0, reward=1.0, next_state=[-0.0430948  -0.60771365  0.03111897  0.8956086 ]\n",
      "[ episode 7 ][ timestamp 6 ] state=[-0.0430948  -0.60771365  0.03111897  0.8956086 ], action=1, reward=1.0, next_state=[-0.05524908 -0.41302714  0.04903114  0.61286773]\n",
      "[ episode 7 ][ timestamp 7 ] state=[-0.05524908 -0.41302714  0.04903114  0.61286773], action=1, reward=1.0, next_state=[-0.06350962 -0.21862345  0.0612885   0.33602203]\n",
      "[ episode 7 ][ timestamp 8 ] state=[-0.06350962 -0.21862345  0.0612885   0.33602203], action=1, reward=1.0, next_state=[-0.06788209 -0.02442483  0.06800894  0.06327856]\n",
      "[ episode 7 ][ timestamp 9 ] state=[-0.06788209 -0.02442483  0.06800894  0.06327856], action=0, reward=1.0, next_state=[-0.06837058 -0.22045262  0.06927451  0.37661972]\n",
      "[ episode 7 ][ timestamp 10 ] state=[-0.06837058 -0.22045262  0.06927451  0.37661972], action=1, reward=1.0, next_state=[-0.07277964 -0.02637945  0.0768069   0.10655862]\n",
      "[ episode 7 ][ timestamp 11 ] state=[-0.07277964 -0.02637945  0.0768069   0.10655862], action=1, reward=1.0, next_state=[-0.07330723  0.16756261  0.07893808 -0.16093776]\n",
      "[ episode 7 ][ timestamp 12 ] state=[-0.07330723  0.16756261  0.07893808 -0.16093776], action=1, reward=1.0, next_state=[-0.06995597  0.36147093  0.07571932 -0.4277108 ]\n",
      "[ episode 7 ][ timestamp 13 ] state=[-0.06995597  0.36147093  0.07571932 -0.4277108 ], action=1, reward=1.0, next_state=[-0.06272655  0.55544338  0.0671651  -0.69559557]\n",
      "[ episode 7 ][ timestamp 14 ] state=[-0.06272655  0.55544338  0.0671651  -0.69559557], action=1, reward=1.0, next_state=[-0.05161769  0.74957268  0.05325319 -0.96640126]\n",
      "[ episode 7 ][ timestamp 15 ] state=[-0.05161769  0.74957268  0.05325319 -0.96640126], action=1, reward=1.0, next_state=[-0.03662623  0.94394055  0.03392517 -1.24189072]\n",
      "[ episode 7 ][ timestamp 16 ] state=[-0.03662623  0.94394055  0.03392517 -1.24189072], action=1, reward=1.0, next_state=[-0.01774742  1.13861099  0.00908735 -1.52375626]\n",
      "[ episode 7 ][ timestamp 17 ] state=[-0.01774742  1.13861099  0.00908735 -1.52375626], action=1, reward=1.0, next_state=[ 0.0050248   1.33362203 -0.02138777 -1.8135891 ]\n",
      "[ episode 7 ][ timestamp 18 ] state=[ 0.0050248   1.33362203 -0.02138777 -1.8135891 ], action=1, reward=1.0, next_state=[ 0.03169724  1.52897545 -0.05765955 -2.11283975]\n",
      "[ episode 7 ][ timestamp 19 ] state=[ 0.03169724  1.52897545 -0.05765955 -2.11283975], action=1, reward=1.0, next_state=[ 0.06227675  1.72462397 -0.09991635 -2.42276733]\n",
      "[ episode 7 ][ timestamp 20 ] state=[ 0.06227675  1.72462397 -0.09991635 -2.42276733], action=1, reward=1.0, next_state=[ 0.09676923  1.92045554 -0.1483717  -2.74437618]\n",
      "[ episode 7 ][ timestamp 21 ] state=[ 0.09676923  1.92045554 -0.1483717  -2.74437618], action=0, reward=1.0, next_state=[ 0.13517834  1.72665403 -0.20325922 -2.50032924]\n",
      "[ episode 7 ][ timestamp 22 ] state=[ 0.13517834  1.72665403 -0.20325922 -2.50032924], action=1, reward=-1.0, next_state=[ 0.16971142  1.92279538 -0.2532658  -2.84783216]\n",
      "[ Ended! ] Episode 7: Exploration_rate=0.9703725093562657. Score=22.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 8 ] state=[-0.00883346  0.00371395 -0.00354461  0.02637272]\n",
      "[ episode 8 ][ timestamp 1 ] state=[-0.00883346  0.00371395 -0.00354461  0.02637272], action=1, reward=1.0, next_state=[-0.00875918  0.19888655 -0.00301715 -0.26742646]\n",
      "[ episode 8 ][ timestamp 2 ] state=[-0.00875918  0.19888655 -0.00301715 -0.26742646], action=0, reward=1.0, next_state=[-0.00478145  0.00380779 -0.00836568  0.02430331]\n",
      "[ episode 8 ][ timestamp 3 ] state=[-0.00478145  0.00380779 -0.00836568  0.02430331], action=1, reward=1.0, next_state=[-0.00470529  0.19904871 -0.00787962 -0.2710073 ]\n",
      "[ episode 8 ][ timestamp 4 ] state=[-0.00470529  0.19904871 -0.00787962 -0.2710073 ], action=1, reward=1.0, next_state=[-0.00072432  0.39428221 -0.01329976 -0.56616504]\n",
      "[ episode 8 ][ timestamp 5 ] state=[-0.00072432  0.39428221 -0.01329976 -0.56616504], action=1, reward=1.0, next_state=[ 0.00716132  0.58958819 -0.02462306 -0.86300812]\n",
      "[ episode 8 ][ timestamp 6 ] state=[ 0.00716132  0.58958819 -0.02462306 -0.86300812], action=1, reward=1.0, next_state=[ 0.01895309  0.78503657 -0.04188323 -1.16333027]\n",
      "[ episode 8 ][ timestamp 7 ] state=[ 0.01895309  0.78503657 -0.04188323 -1.16333027], action=0, reward=1.0, next_state=[ 0.03465382  0.59048425 -0.06514983 -0.88406779]\n",
      "[ episode 8 ][ timestamp 8 ] state=[ 0.03465382  0.59048425 -0.06514983 -0.88406779], action=1, reward=1.0, next_state=[ 0.0464635   0.7864275  -0.08283119 -1.19649962]\n",
      "[ episode 8 ][ timestamp 9 ] state=[ 0.0464635   0.7864275  -0.08283119 -1.19649962], action=1, reward=1.0, next_state=[ 0.06219205  0.98251817 -0.10676118 -1.5139517 ]\n",
      "[ episode 8 ][ timestamp 10 ] state=[ 0.06219205  0.98251817 -0.10676118 -1.5139517 ], action=0, reward=1.0, next_state=[ 0.08184242  0.78883835 -0.13704021 -1.25641427]\n",
      "[ episode 8 ][ timestamp 11 ] state=[ 0.08184242  0.78883835 -0.13704021 -1.25641427], action=1, reward=1.0, next_state=[ 0.09761918  0.98542231 -0.1621685  -1.58868948]\n",
      "[ episode 8 ][ timestamp 12 ] state=[ 0.09761918  0.98542231 -0.1621685  -1.58868948], action=0, reward=1.0, next_state=[ 0.11732763  0.79255585 -0.19394229 -1.35065438]\n",
      "[ episode 8 ][ timestamp 13 ] state=[ 0.11732763  0.79255585 -0.19394229 -1.35065438], action=1, reward=-1.0, next_state=[ 0.13317875  0.98951167 -0.22095538 -1.69721161]\n",
      "[ Ended! ] Episode 8: Exploration_rate=0.9655206468094844. Score=13.\n",
      "[ Experience replay ] starts\n",
      "[ episode 9 ] state=[-0.0166116  -0.0488663  -0.00318727 -0.03194849]\n",
      "[ episode 9 ][ timestamp 1 ] state=[-0.0166116  -0.0488663  -0.00318727 -0.03194849], action=0, reward=1.0, next_state=[-0.01758893 -0.2439424  -0.00382624  0.25972711]\n",
      "[ episode 9 ][ timestamp 2 ] state=[-0.01758893 -0.2439424  -0.00382624  0.25972711], action=1, reward=1.0, next_state=[-0.02246778 -0.04876604  0.0013683  -0.0341602 ]\n",
      "[ episode 9 ][ timestamp 3 ] state=[-0.02246778 -0.04876604  0.0013683  -0.0341602 ], action=0, reward=1.0, next_state=[-0.0234431  -0.24390759  0.00068509  0.25895412]\n",
      "[ episode 9 ][ timestamp 4 ] state=[-0.0234431  -0.24390759  0.00068509  0.25895412], action=0, reward=1.0, next_state=[-0.02832125 -0.43903931  0.00586418  0.55185306]\n",
      "[ episode 9 ][ timestamp 5 ] state=[-0.02832125 -0.43903931  0.00586418  0.55185306], action=0, reward=1.0, next_state=[-0.03710203 -0.63424313  0.01690124  0.84637781]\n",
      "[ episode 9 ][ timestamp 6 ] state=[-0.03710203 -0.63424313  0.01690124  0.84637781], action=0, reward=1.0, next_state=[-0.0497869  -0.82959153  0.03382879  1.14432729]\n",
      "[ episode 9 ][ timestamp 7 ] state=[-0.0497869  -0.82959153  0.03382879  1.14432729], action=1, reward=1.0, next_state=[-0.06637873 -0.63492744  0.05671534  0.86244199]\n",
      "[ episode 9 ][ timestamp 8 ] state=[-0.06637873 -0.63492744  0.05671534  0.86244199], action=0, reward=1.0, next_state=[-0.07907728 -0.83077385  0.07396418  1.17240463]\n",
      "[ episode 9 ][ timestamp 9 ] state=[-0.07907728 -0.83077385  0.07396418  1.17240463], action=0, reward=1.0, next_state=[-0.09569275 -1.02677528  0.09741227  1.48732859]\n",
      "[ episode 9 ][ timestamp 10 ] state=[-0.09569275 -1.02677528  0.09741227  1.48732859], action=0, reward=1.0, next_state=[-0.11622826 -1.22293977  0.12715884  1.80877429]\n",
      "[ episode 9 ][ timestamp 11 ] state=[-0.11622826 -1.22293977  0.12715884  1.80877429], action=1, reward=1.0, next_state=[-0.14068705 -1.02944479  0.16333433  1.55815921]\n",
      "[ episode 9 ][ timestamp 12 ] state=[-0.14068705 -1.02944479  0.16333433  1.55815921], action=1, reward=1.0, next_state=[-0.16127595 -0.83661177  0.19449751  1.32056649]\n",
      "[ episode 9 ][ timestamp 13 ] state=[-0.16127595 -0.83661177  0.19449751  1.32056649], action=0, reward=-1.0, next_state=[-0.17800819 -1.03358607  0.22090884  1.66727941]\n",
      "[ Ended! ] Episode 9: Exploration_rate=0.960693043575437. Score=13.\n",
      "[ Experience replay ] starts\n",
      "[ episode 10 ] state=[-0.00982436  0.02553941 -0.00842842  0.04125779]\n",
      "[ episode 10 ][ timestamp 1 ] state=[-0.00982436  0.02553941 -0.00842842  0.04125779], action=0, reward=1.0, next_state=[-0.00931357 -0.16946067 -0.00760326  0.3312696 ]\n",
      "[ episode 10 ][ timestamp 2 ] state=[-0.00931357 -0.16946067 -0.00760326  0.3312696 ], action=0, reward=1.0, next_state=[-0.01270278 -0.36447357 -0.00097787  0.62154516]\n",
      "[ episode 10 ][ timestamp 3 ] state=[-0.01270278 -0.36447357 -0.00097787  0.62154516], action=1, reward=1.0, next_state=[-0.01999225 -0.16933798  0.01145304  0.32855442]\n",
      "[ episode 10 ][ timestamp 4 ] state=[-0.01999225 -0.16933798  0.01145304  0.32855442], action=1, reward=1.0, next_state=[-0.02337901  0.02561907  0.01802412  0.03950514]\n",
      "[ episode 10 ][ timestamp 5 ] state=[-0.02337901  0.02561907  0.01802412  0.03950514], action=1, reward=1.0, next_state=[-0.02286663  0.22047798  0.01881423 -0.24743694]\n",
      "[ episode 10 ][ timestamp 6 ] state=[-0.02286663  0.22047798  0.01881423 -0.24743694], action=0, reward=1.0, next_state=[-0.01845707  0.02509245  0.01386549  0.05112054]\n",
      "[ episode 10 ][ timestamp 7 ] state=[-0.01845707  0.02509245  0.01386549  0.05112054], action=0, reward=1.0, next_state=[-0.01795522 -0.17022554  0.0148879   0.34814569]\n",
      "[ episode 10 ][ timestamp 8 ] state=[-0.01795522 -0.17022554  0.0148879   0.34814569], action=0, reward=1.0, next_state=[-0.02135973 -0.36555605  0.02185081  0.64548586]\n",
      "[ episode 10 ][ timestamp 9 ] state=[-0.02135973 -0.36555605  0.02185081  0.64548586], action=1, reward=1.0, next_state=[-0.02867085 -0.17074529  0.03476053  0.35976311]\n",
      "[ episode 10 ][ timestamp 10 ] state=[-0.02867085 -0.17074529  0.03476053  0.35976311], action=0, reward=1.0, next_state=[-0.03208576 -0.36634369  0.04195579  0.663201  ]\n",
      "[ episode 10 ][ timestamp 11 ] state=[-0.03208576 -0.36634369  0.04195579  0.663201  ], action=1, reward=1.0, next_state=[-0.03941263 -0.17182978  0.05521981  0.38401828]\n",
      "[ episode 10 ][ timestamp 12 ] state=[-0.03941263 -0.17182978  0.05521981  0.38401828], action=1, reward=1.0, next_state=[-0.04284923  0.02246649  0.06290018  0.10924449]\n",
      "[ episode 10 ][ timestamp 13 ] state=[-0.04284923  0.02246649  0.06290018  0.10924449], action=0, reward=1.0, next_state=[-0.0423999  -0.17349778  0.06508507  0.42109005]\n",
      "[ episode 10 ][ timestamp 14 ] state=[-0.0423999  -0.17349778  0.06508507  0.42109005], action=1, reward=1.0, next_state=[-0.04586985  0.02064455  0.07350687  0.14961464]\n",
      "[ episode 10 ][ timestamp 15 ] state=[-0.04586985  0.02064455  0.07350687  0.14961464], action=1, reward=1.0, next_state=[-0.04545696  0.21464114  0.07649916 -0.11900288]\n",
      "[ episode 10 ][ timestamp 16 ] state=[-0.04545696  0.21464114  0.07649916 -0.11900288], action=1, reward=1.0, next_state=[-0.04116414  0.40858843  0.0741191  -0.38660414]\n",
      "[ episode 10 ][ timestamp 17 ] state=[-0.04116414  0.40858843  0.0741191  -0.38660414], action=0, reward=1.0, next_state=[-0.03299237  0.21249686  0.06638702 -0.0715033 ]\n",
      "[ episode 10 ][ timestamp 18 ] state=[-0.03299237  0.21249686  0.06638702 -0.0715033 ], action=0, reward=1.0, next_state=[-0.02874244  0.01648907  0.06495695  0.24136419]\n",
      "[ episode 10 ][ timestamp 19 ] state=[-0.02874244  0.01648907  0.06495695  0.24136419], action=0, reward=1.0, next_state=[-0.02841265 -0.17949773  0.06978424  0.55380831]\n",
      "[ episode 10 ][ timestamp 20 ] state=[-0.02841265 -0.17949773  0.06978424  0.55380831], action=1, reward=1.0, next_state=[-0.03200261  0.01457848  0.0808604   0.28390247]\n",
      "[ episode 10 ][ timestamp 21 ] state=[-0.03200261  0.01457848  0.0808604   0.28390247], action=0, reward=1.0, next_state=[-0.03171104 -0.18159808  0.08653845  0.60095288]\n",
      "[ episode 10 ][ timestamp 22 ] state=[-0.03171104 -0.18159808  0.08653845  0.60095288], action=1, reward=1.0, next_state=[-0.035343    0.01221349  0.09855751  0.33673399]\n",
      "[ episode 10 ][ timestamp 23 ] state=[-0.035343    0.01221349  0.09855751  0.33673399], action=1, reward=1.0, next_state=[-0.03509873  0.20580485  0.10529219  0.07668517]\n",
      "[ episode 10 ][ timestamp 24 ] state=[-0.03509873  0.20580485  0.10529219  0.07668517], action=0, reward=1.0, next_state=[-0.03098263  0.00934338  0.10682589  0.40064409]\n",
      "[ episode 10 ][ timestamp 25 ] state=[-0.03098263  0.00934338  0.10682589  0.40064409], action=1, reward=1.0, next_state=[-0.03079577  0.20280069  0.11483878  0.14345942]\n",
      "[ episode 10 ][ timestamp 26 ] state=[-0.03079577  0.20280069  0.11483878  0.14345942], action=1, reward=1.0, next_state=[-0.02673975  0.39610671  0.11770796 -0.11090129]\n",
      "[ episode 10 ][ timestamp 27 ] state=[-0.02673975  0.39610671  0.11770796 -0.11090129], action=1, reward=1.0, next_state=[-0.01881762  0.5893625   0.11548994 -0.36425281]\n",
      "[ episode 10 ][ timestamp 28 ] state=[-0.01881762  0.5893625   0.11548994 -0.36425281], action=1, reward=1.0, next_state=[-0.00703037  0.78266994  0.10820488 -0.61840376]\n",
      "[ episode 10 ][ timestamp 29 ] state=[-0.00703037  0.78266994  0.10820488 -0.61840376], action=0, reward=1.0, next_state=[ 0.00862303  0.58621619  0.09583681 -0.29369637]\n",
      "[ episode 10 ][ timestamp 30 ] state=[ 0.00862303  0.58621619  0.09583681 -0.29369637], action=1, reward=1.0, next_state=[ 0.02034735  0.77985042  0.08996288 -0.55468198]\n",
      "[ episode 10 ][ timestamp 31 ] state=[ 0.02034735  0.77985042  0.08996288 -0.55468198], action=0, reward=1.0, next_state=[ 0.03594436  0.5835881   0.07886924 -0.23506558]\n",
      "[ episode 10 ][ timestamp 32 ] state=[ 0.03594436  0.5835881   0.07886924 -0.23506558], action=1, reward=1.0, next_state=[ 0.04761613  0.77749981  0.07416793 -0.50186543]\n",
      "[ episode 10 ][ timestamp 33 ] state=[ 0.04761613  0.77749981  0.07416793 -0.50186543], action=0, reward=1.0, next_state=[ 0.06316612  0.58141507  0.06413062 -0.18676154]\n",
      "[ episode 10 ][ timestamp 34 ] state=[ 0.06316612  0.58141507  0.06413062 -0.18676154], action=1, reward=1.0, next_state=[ 0.07479442  0.77556365  0.06039539 -0.45854427]\n",
      "[ episode 10 ][ timestamp 35 ] state=[ 0.07479442  0.77556365  0.06039539 -0.45854427], action=1, reward=1.0, next_state=[ 0.0903057   0.96978215  0.0512245  -0.73159542]\n",
      "[ episode 10 ][ timestamp 36 ] state=[ 0.0903057   0.96978215  0.0512245  -0.73159542], action=1, reward=1.0, next_state=[ 0.10970134  1.1641602   0.0365926  -1.00772663]\n",
      "[ episode 10 ][ timestamp 37 ] state=[ 0.10970134  1.1641602   0.0365926  -1.00772663], action=1, reward=1.0, next_state=[ 0.13298454  1.35877502  0.01643806 -1.28869761]\n",
      "[ episode 10 ][ timestamp 38 ] state=[ 0.13298454  1.35877502  0.01643806 -1.28869761], action=0, reward=1.0, next_state=[ 0.16016004  1.16344786 -0.00933589 -0.99091388]\n",
      "[ episode 10 ][ timestamp 39 ] state=[ 0.16016004  1.16344786 -0.00933589 -0.99091388], action=1, reward=1.0, next_state=[ 0.183429    1.3586935  -0.02915417 -1.28651429]\n",
      "[ episode 10 ][ timestamp 40 ] state=[ 0.183429    1.3586935  -0.02915417 -1.28651429], action=0, reward=1.0, next_state=[ 0.21060287  1.16395447 -0.05488445 -1.00309998]\n",
      "[ episode 10 ][ timestamp 41 ] state=[ 0.21060287  1.16395447 -0.05488445 -1.00309998], action=0, reward=1.0, next_state=[ 0.23388196  0.969607   -0.07494645 -0.72814568]\n",
      "[ episode 10 ][ timestamp 42 ] state=[ 0.23388196  0.969607   -0.07494645 -0.72814568], action=1, reward=1.0, next_state=[ 0.2532741   1.16568061 -0.08950937 -1.0434441 ]\n",
      "[ episode 10 ][ timestamp 43 ] state=[ 0.2532741   1.16568061 -0.08950937 -1.0434441 ], action=1, reward=1.0, next_state=[ 0.27658771  1.36186975 -0.11037825 -1.36283034]\n",
      "[ episode 10 ][ timestamp 44 ] state=[ 0.27658771  1.36186975 -0.11037825 -1.36283034], action=1, reward=1.0, next_state=[ 0.30382511  1.55818796 -0.13763485 -1.68790097]\n",
      "[ episode 10 ][ timestamp 45 ] state=[ 0.30382511  1.55818796 -0.13763485 -1.68790097], action=1, reward=1.0, next_state=[ 0.33498887  1.75460707 -0.17139287 -2.02008042]\n",
      "[ episode 10 ][ timestamp 46 ] state=[ 0.33498887  1.75460707 -0.17139287 -2.02008042], action=1, reward=-1.0, next_state=[ 0.37008101  1.95104207 -0.21179448 -2.36055888]\n",
      "[ Ended! ] Episode 10: Exploration_rate=0.9558895783575597. Score=46.\n",
      "[ Experience replay ] starts\n",
      "[ episode 11 ] state=[-0.04084854  0.00633337  0.04368708  0.00604724]\n",
      "[ episode 11 ][ timestamp 1 ] state=[-0.04084854  0.00633337  0.04368708  0.00604724], action=1, reward=1.0, next_state=[-0.04072187  0.20080244  0.04380803 -0.27253813]\n",
      "[ episode 11 ][ timestamp 2 ] state=[-0.04072187  0.20080244  0.04380803 -0.27253813], action=1, reward=1.0, next_state=[-0.03670582  0.39527281  0.03835726 -0.55108837]\n",
      "[ episode 11 ][ timestamp 3 ] state=[-0.03670582  0.39527281  0.03835726 -0.55108837], action=1, reward=1.0, next_state=[-0.02880037  0.58983563  0.0273355  -0.83144366]\n",
      "[ episode 11 ][ timestamp 4 ] state=[-0.02880037  0.58983563  0.0273355  -0.83144366], action=1, reward=1.0, next_state=[-0.01700365  0.78457353  0.01070662 -1.11540576]\n",
      "[ episode 11 ][ timestamp 5 ] state=[-0.01700365  0.78457353  0.01070662 -1.11540576], action=1, reward=1.0, next_state=[-1.31218430e-03  9.79553307e-01 -1.16014923e-02 -1.40471097e+00]\n",
      "[ episode 11 ][ timestamp 6 ] state=[-1.31218430e-03  9.79553307e-01 -1.16014923e-02 -1.40471097e+00], action=1, reward=1.0, next_state=[ 0.01827888  1.17481737 -0.03969571 -1.70099811]\n",
      "[ episode 11 ][ timestamp 7 ] state=[ 0.01827888  1.17481737 -0.03969571 -1.70099811], action=1, reward=1.0, next_state=[ 0.04177523  1.37037346 -0.07371567 -2.00576864]\n",
      "[ episode 11 ][ timestamp 8 ] state=[ 0.04177523  1.37037346 -0.07371567 -2.00576864], action=1, reward=1.0, next_state=[ 0.0691827   1.56618199 -0.11383105 -2.32033656]\n",
      "[ episode 11 ][ timestamp 9 ] state=[ 0.0691827   1.56618199 -0.11383105 -2.32033656], action=0, reward=1.0, next_state=[ 0.10050634  1.37226491 -0.16023778 -2.06473753]\n",
      "[ episode 11 ][ timestamp 10 ] state=[ 0.10050634  1.37226491 -0.16023778 -2.06473753], action=1, reward=1.0, next_state=[ 0.12795164  1.56861642 -0.20153253 -2.40240028]\n",
      "[ episode 11 ][ timestamp 11 ] state=[ 0.12795164  1.56861642 -0.20153253 -2.40240028], action=1, reward=-1.0, next_state=[ 0.15932396  1.76484858 -0.24958053 -2.74964149]\n",
      "[ Ended! ] Episode 11: Exploration_rate=0.9511101304657719. Score=11.\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ] state=[ 0.02016282 -0.04751182  0.04800106 -0.02684788]\n",
      "[ episode 12 ][ timestamp 1 ] state=[ 0.02016282 -0.04751182  0.04800106 -0.02684788], action=0, reward=1.0, next_state=[ 0.01921258 -0.24328811  0.04746411  0.28058519]\n",
      "[ episode 12 ][ timestamp 2 ] state=[ 0.01921258 -0.24328811  0.04746411  0.28058519], action=0, reward=1.0, next_state=[ 0.01434682 -0.43905385  0.05307581  0.5878523 ]\n",
      "[ episode 12 ][ timestamp 3 ] state=[ 0.01434682 -0.43905385  0.05307581  0.5878523 ], action=0, reward=1.0, next_state=[ 0.00556574 -0.63487735  0.06483286  0.89677087]\n",
      "[ episode 12 ][ timestamp 4 ] state=[ 0.00556574 -0.63487735  0.06483286  0.89677087], action=1, reward=1.0, next_state=[-0.00713181 -0.4406914   0.08276827  0.62515141]\n",
      "[ episode 12 ][ timestamp 5 ] state=[-0.00713181 -0.4406914   0.08276827  0.62515141], action=1, reward=1.0, next_state=[-0.01594563 -0.24681649  0.0952713   0.3596407 ]\n",
      "[ episode 12 ][ timestamp 6 ] state=[-0.01594563 -0.24681649  0.0952713   0.3596407 ], action=0, reward=1.0, next_state=[-0.02088196 -0.44315451  0.10246412  0.68077959]\n",
      "[ episode 12 ][ timestamp 7 ] state=[-0.02088196 -0.44315451  0.10246412  0.68077959], action=1, reward=1.0, next_state=[-0.02974505 -0.24959371  0.11607971  0.42203295]\n",
      "[ episode 12 ][ timestamp 8 ] state=[-0.02974505 -0.24959371  0.11607971  0.42203295], action=0, reward=1.0, next_state=[-0.03473693 -0.44615245  0.12452037  0.74893773]\n",
      "[ episode 12 ][ timestamp 9 ] state=[-0.03473693 -0.44615245  0.12452037  0.74893773], action=1, reward=1.0, next_state=[-0.04365998 -0.252948    0.13949912  0.49788939]\n",
      "[ episode 12 ][ timestamp 10 ] state=[-0.04365998 -0.252948    0.13949912  0.49788939], action=1, reward=1.0, next_state=[-0.04871894 -0.0600401   0.14945691  0.25221831]\n",
      "[ episode 12 ][ timestamp 11 ] state=[-0.04871894 -0.0600401   0.14945691  0.25221831], action=1, reward=1.0, next_state=[-0.04991974  0.13266681  0.15450128  0.01015729]\n",
      "[ episode 12 ][ timestamp 12 ] state=[-0.04991974  0.13266681  0.15450128  0.01015729], action=0, reward=1.0, next_state=[-0.0472664  -0.06429451  0.15470442  0.34732297]\n",
      "[ episode 12 ][ timestamp 13 ] state=[-0.0472664  -0.06429451  0.15470442  0.34732297], action=1, reward=1.0, next_state=[-0.04855229  0.12832747  0.16165088  0.10714257]\n",
      "[ episode 12 ][ timestamp 14 ] state=[-0.04855229  0.12832747  0.16165088  0.10714257], action=0, reward=1.0, next_state=[-0.04598574 -0.06869736  0.16379373  0.44614553]\n",
      "[ episode 12 ][ timestamp 15 ] state=[-0.04598574 -0.06869736  0.16379373  0.44614553], action=1, reward=1.0, next_state=[-0.04735969  0.12377454  0.17271664  0.20924214]\n",
      "[ episode 12 ][ timestamp 16 ] state=[-0.04735969  0.12377454  0.17271664  0.20924214], action=1, reward=1.0, next_state=[-0.0448842   0.31606023  0.17690149 -0.02436841]\n",
      "[ episode 12 ][ timestamp 17 ] state=[-0.0448842   0.31606023  0.17690149 -0.02436841], action=1, reward=1.0, next_state=[-0.038563    0.50826232  0.17641412 -0.25643401]\n",
      "[ episode 12 ][ timestamp 18 ] state=[-0.038563    0.50826232  0.17641412 -0.25643401], action=1, reward=1.0, next_state=[-0.02839775  0.70048445  0.17128544 -0.48869494]\n",
      "[ episode 12 ][ timestamp 19 ] state=[-0.02839775  0.70048445  0.17128544 -0.48869494], action=1, reward=1.0, next_state=[-0.01438806  0.89282851  0.16151154 -0.72287698]\n",
      "[ episode 12 ][ timestamp 20 ] state=[-0.01438806  0.89282851  0.16151154 -0.72287698], action=0, reward=1.0, next_state=[ 0.00346851  0.69588496  0.147054   -0.38402817]\n",
      "[ episode 12 ][ timestamp 21 ] state=[ 0.00346851  0.69588496  0.147054   -0.38402817], action=1, reward=1.0, next_state=[ 0.01738621  0.88864648  0.13937344 -0.62697152]\n",
      "[ episode 12 ][ timestamp 22 ] state=[ 0.01738621  0.88864648  0.13937344 -0.62697152], action=0, reward=1.0, next_state=[ 0.03515914  0.69188263  0.12683401 -0.29384445]\n",
      "[ episode 12 ][ timestamp 23 ] state=[ 0.03515914  0.69188263  0.12683401 -0.29384445], action=1, reward=1.0, next_state=[ 0.04899679  0.88498961  0.12095712 -0.54398888]\n",
      "[ episode 12 ][ timestamp 24 ] state=[ 0.04899679  0.88498961  0.12095712 -0.54398888], action=1, reward=1.0, next_state=[ 0.06669658  1.07822271  0.11007734 -0.79624603]\n",
      "[ episode 12 ][ timestamp 25 ] state=[ 0.06669658  1.07822271  0.11007734 -0.79624603], action=0, reward=1.0, next_state=[ 0.08826104  0.88177623  0.09415242 -0.47106234]\n",
      "[ episode 12 ][ timestamp 26 ] state=[ 0.08826104  0.88177623  0.09415242 -0.47106234], action=1, reward=1.0, next_state=[ 0.10589656  1.07545096  0.08473117 -0.73264781]\n",
      "[ episode 12 ][ timestamp 27 ] state=[ 0.10589656  1.07545096  0.08473117 -0.73264781], action=0, reward=1.0, next_state=[ 0.12740558  0.87926678  0.07007822 -0.4145461 ]\n",
      "[ episode 12 ][ timestamp 28 ] state=[ 0.12740558  0.87926678  0.07007822 -0.4145461 ], action=1, reward=1.0, next_state=[ 0.14499092  1.07332912  0.06178729 -0.68433899]\n",
      "[ episode 12 ][ timestamp 29 ] state=[ 0.14499092  1.07332912  0.06178729 -0.68433899], action=1, reward=1.0, next_state=[ 0.1664575   1.26754123  0.04810051 -0.95694735]\n",
      "[ episode 12 ][ timestamp 30 ] state=[ 0.1664575   1.26754123  0.04810051 -0.95694735], action=0, reward=1.0, next_state=[ 0.19180832  1.07180658  0.02896157 -0.64954886]\n",
      "[ episode 12 ][ timestamp 31 ] state=[ 0.19180832  1.07180658  0.02896157 -0.64954886], action=0, reward=1.0, next_state=[ 0.21324446  0.87629342  0.01597059 -0.3478886 ]\n",
      "[ episode 12 ][ timestamp 32 ] state=[ 0.21324446  0.87629342  0.01597059 -0.3478886 ], action=1, reward=1.0, next_state=[ 0.23077032  1.07118462  0.00901282 -0.63549296]\n",
      "[ episode 12 ][ timestamp 33 ] state=[ 0.23077032  1.07118462  0.00901282 -0.63549296], action=0, reward=1.0, next_state=[ 0.25219402  0.87593813 -0.00369704 -0.33998539]\n",
      "[ episode 12 ][ timestamp 34 ] state=[ 0.25219402  0.87593813 -0.00369704 -0.33998539], action=0, reward=1.0, next_state=[ 0.26971278  0.68086898 -0.01049675 -0.04847059]\n",
      "[ episode 12 ][ timestamp 35 ] state=[ 0.26971278  0.68086898 -0.01049675 -0.04847059], action=0, reward=1.0, next_state=[ 0.28333016  0.4858991  -0.01146616  0.24088213]\n",
      "[ episode 12 ][ timestamp 36 ] state=[ 0.28333016  0.4858991  -0.01146616  0.24088213], action=1, reward=1.0, next_state=[ 0.29304814  0.68118295 -0.00664852 -0.05539537]\n",
      "[ episode 12 ][ timestamp 37 ] state=[ 0.29304814  0.68118295 -0.00664852 -0.05539537], action=1, reward=1.0, next_state=[ 0.3066718   0.8763996  -0.00775643 -0.35016852]\n",
      "[ episode 12 ][ timestamp 38 ] state=[ 0.3066718   0.8763996  -0.00775643 -0.35016852], action=1, reward=1.0, next_state=[ 0.32419979  1.071631   -0.0147598  -0.64528717]\n",
      "[ episode 12 ][ timestamp 39 ] state=[ 0.32419979  1.071631   -0.0147598  -0.64528717], action=0, reward=1.0, next_state=[ 0.34563241  0.87671781 -0.02766554 -0.35728845]\n",
      "[ episode 12 ][ timestamp 40 ] state=[ 0.34563241  0.87671781 -0.02766554 -0.35728845], action=0, reward=1.0, next_state=[ 0.36316677  0.68199987 -0.03481131 -0.07345595]\n",
      "[ episode 12 ][ timestamp 41 ] state=[ 0.36316677  0.68199987 -0.03481131 -0.07345595], action=0, reward=1.0, next_state=[ 0.37680677  0.48739383 -0.03628043  0.2080438 ]\n",
      "[ episode 12 ][ timestamp 42 ] state=[ 0.37680677  0.48739383 -0.03628043  0.2080438 ], action=0, reward=1.0, next_state=[ 0.38655464  0.29280894 -0.03211955  0.48906496]\n",
      "[ episode 12 ][ timestamp 43 ] state=[ 0.38655464  0.29280894 -0.03211955  0.48906496], action=0, reward=1.0, next_state=[ 0.39241082  0.09815451 -0.02233825  0.77145449]\n",
      "[ episode 12 ][ timestamp 44 ] state=[ 0.39241082  0.09815451 -0.02233825  0.77145449], action=0, reward=1.0, next_state=[ 0.39437391 -0.09665305 -0.00690916  1.05702601]\n",
      "[ episode 12 ][ timestamp 45 ] state=[ 0.39437391 -0.09665305 -0.00690916  1.05702601], action=0, reward=1.0, next_state=[ 0.39244085 -0.29168276  0.01423136  1.34753233]\n",
      "[ episode 12 ][ timestamp 46 ] state=[ 0.39244085 -0.29168276  0.01423136  1.34753233], action=0, reward=1.0, next_state=[ 0.38660719 -0.48698068  0.041182    1.64463341]\n",
      "[ episode 12 ][ timestamp 47 ] state=[ 0.38660719 -0.48698068  0.041182    1.64463341], action=0, reward=1.0, next_state=[ 0.37686758 -0.68255968  0.07407467  1.94985726]\n",
      "[ episode 12 ][ timestamp 48 ] state=[ 0.37686758 -0.68255968  0.07407467  1.94985726], action=0, reward=1.0, next_state=[ 0.36321639 -0.8783871   0.11307182  2.26455092]\n",
      "[ episode 12 ][ timestamp 49 ] state=[ 0.36321639 -0.8783871   0.11307182  2.26455092], action=1, reward=1.0, next_state=[ 0.34564865 -0.68448909  0.15836284  2.00873352]\n",
      "[ episode 12 ][ timestamp 50 ] state=[ 0.34564865 -0.68448909  0.15836284  2.00873352], action=1, reward=1.0, next_state=[ 0.33195886 -0.4913312   0.19853751  1.76898654]\n",
      "[ episode 12 ][ timestamp 51 ] state=[ 0.33195886 -0.4913312   0.19853751  1.76898654], action=1, reward=-1.0, next_state=[ 0.32213224 -0.29892785  0.23391724  1.54403816]\n",
      "[ Ended! ] Episode 12: Exploration_rate=0.946354579813443. Score=51.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 13 ] state=[ 0.01278416  0.03839105  0.00546557 -0.03038043]\n",
      "[ episode 13 ][ timestamp 1 ] state=[ 0.01278416  0.03839105  0.00546557 -0.03038043], action=1, reward=1.0, next_state=[ 0.01355198  0.23343419  0.00485796 -0.32133391]\n",
      "[ episode 13 ][ timestamp 2 ] state=[ 0.01355198  0.23343419  0.00485796 -0.32133391], action=0, reward=1.0, next_state=[ 0.01822067  0.0382434  -0.00156871 -0.02712294]\n",
      "[ episode 13 ][ timestamp 3 ] state=[ 0.01822067  0.0382434  -0.00156871 -0.02712294], action=0, reward=1.0, next_state=[ 0.01898554 -0.15685602 -0.00211117  0.26506463]\n",
      "[ episode 13 ][ timestamp 4 ] state=[ 0.01898554 -0.15685602 -0.00211117  0.26506463], action=1, reward=1.0, next_state=[ 0.01584842  0.038296    0.00319012 -0.02828343]\n",
      "[ episode 13 ][ timestamp 5 ] state=[ 0.01584842  0.038296    0.00319012 -0.02828343], action=1, reward=1.0, next_state=[ 0.01661434  0.23337206  0.00262445 -0.31995814]\n",
      "[ episode 13 ][ timestamp 6 ] state=[ 0.01661434  0.23337206  0.00262445 -0.31995814], action=1, reward=1.0, next_state=[ 0.02128178  0.42845653 -0.00377471 -0.61181226]\n",
      "[ episode 13 ][ timestamp 7 ] state=[ 0.02128178  0.42845653 -0.00377471 -0.61181226], action=0, reward=1.0, next_state=[ 0.02985091  0.23338754 -0.01601096 -0.32032062]\n",
      "[ episode 13 ][ timestamp 8 ] state=[ 0.02985091  0.23338754 -0.01601096 -0.32032062], action=1, reward=1.0, next_state=[ 0.03451866  0.42873381 -0.02241737 -0.61800948]\n",
      "[ episode 13 ][ timestamp 9 ] state=[ 0.03451866  0.42873381 -0.02241737 -0.61800948], action=1, reward=1.0, next_state=[ 0.04309333  0.62416161 -0.03477756 -0.91766768]\n",
      "[ episode 13 ][ timestamp 10 ] state=[ 0.04309333  0.62416161 -0.03477756 -0.91766768], action=0, reward=1.0, next_state=[ 0.05557657  0.42952667 -0.05313091 -0.63611435]\n",
      "[ episode 13 ][ timestamp 11 ] state=[ 0.05557657  0.42952667 -0.05313091 -0.63611435], action=1, reward=1.0, next_state=[ 0.0641671   0.62534779 -0.0658532  -0.94504468]\n",
      "[ episode 13 ][ timestamp 12 ] state=[ 0.0641671   0.62534779 -0.0658532  -0.94504468], action=1, reward=1.0, next_state=[ 0.07667406  0.82129202 -0.08475409 -1.2576708 ]\n",
      "[ episode 13 ][ timestamp 13 ] state=[ 0.07667406  0.82129202 -0.08475409 -1.2576708 ], action=0, reward=1.0, next_state=[ 0.0930999   0.62735081 -0.10990751 -0.9926911 ]\n",
      "[ episode 13 ][ timestamp 14 ] state=[ 0.0930999   0.62735081 -0.10990751 -0.9926911 ], action=0, reward=1.0, next_state=[ 0.10564691  0.43385732 -0.12976133 -0.73644988]\n",
      "[ episode 13 ][ timestamp 15 ] state=[ 0.10564691  0.43385732 -0.12976133 -0.73644988], action=0, reward=1.0, next_state=[ 0.11432406  0.24074364 -0.14449033 -0.48725754]\n",
      "[ episode 13 ][ timestamp 16 ] state=[ 0.11432406  0.24074364 -0.14449033 -0.48725754], action=0, reward=1.0, next_state=[ 0.11913893  0.04792433 -0.15423548 -0.24337501]\n",
      "[ episode 13 ][ timestamp 17 ] state=[ 0.11913893  0.04792433 -0.15423548 -0.24337501], action=0, reward=1.0, next_state=[ 0.12009742 -0.14469682 -0.15910298 -0.00303878]\n",
      "[ episode 13 ][ timestamp 18 ] state=[ 0.12009742 -0.14469682 -0.15910298 -0.00303878], action=1, reward=1.0, next_state=[ 0.11720348  0.05230676 -0.15916375 -0.34139103]\n",
      "[ episode 13 ][ timestamp 19 ] state=[ 0.11720348  0.05230676 -0.15916375 -0.34139103], action=1, reward=1.0, next_state=[ 0.11824962  0.24929291 -0.16599158 -0.67973226]\n",
      "[ episode 13 ][ timestamp 20 ] state=[ 0.11824962  0.24929291 -0.16599158 -0.67973226], action=0, reward=1.0, next_state=[ 0.12323548  0.05681805 -0.17958622 -0.44356605]\n",
      "[ episode 13 ][ timestamp 21 ] state=[ 0.12323548  0.05681805 -0.17958622 -0.44356605], action=1, reward=1.0, next_state=[ 0.12437184  0.25396596 -0.18845754 -0.78704703]\n",
      "[ episode 13 ][ timestamp 22 ] state=[ 0.12437184  0.25396596 -0.18845754 -0.78704703], action=0, reward=1.0, next_state=[ 0.12945116  0.06186335 -0.20419848 -0.55907419]\n",
      "[ episode 13 ][ timestamp 23 ] state=[ 0.12945116  0.06186335 -0.20419848 -0.55907419], action=1, reward=-1.0, next_state=[ 0.13068842  0.25917756 -0.21537997 -0.90851436]\n",
      "[ Ended! ] Episode 13: Exploration_rate=0.9416228069143757. Score=23.\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ] state=[-0.0375153  -0.00623601 -0.00406604 -0.04050475]\n",
      "[ episode 14 ][ timestamp 1 ] state=[-0.0375153  -0.00623601 -0.00406604 -0.04050475], action=0, reward=1.0, next_state=[-0.03764002 -0.20129942 -0.00487613  0.25089253]\n",
      "[ episode 14 ][ timestamp 2 ] state=[-0.03764002 -0.20129942 -0.00487613  0.25089253], action=0, reward=1.0, next_state=[-4.16660130e-02 -3.96351401e-01  1.41717951e-04  5.42033446e-01]\n",
      "[ episode 14 ][ timestamp 3 ] state=[-4.16660130e-02 -3.96351401e-01  1.41717951e-04  5.42033446e-01], action=0, reward=1.0, next_state=[-0.04959304 -0.59147534  0.01098239  0.83476102]\n",
      "[ episode 14 ][ timestamp 4 ] state=[-0.04959304 -0.59147534  0.01098239  0.83476102], action=0, reward=1.0, next_state=[-0.06142255 -0.7867456   0.02767761  1.13087749]\n",
      "[ episode 14 ][ timestamp 5 ] state=[-0.06142255 -0.7867456   0.02767761  1.13087749], action=0, reward=1.0, next_state=[-0.07715746 -0.9822188   0.05029516  1.43211118]\n",
      "[ episode 14 ][ timestamp 6 ] state=[-0.07715746 -0.9822188   0.05029516  1.43211118], action=0, reward=1.0, next_state=[-0.09680184 -1.17792405  0.07893738  1.74007839]\n",
      "[ episode 14 ][ timestamp 7 ] state=[-0.09680184 -1.17792405  0.07893738  1.74007839], action=1, reward=1.0, next_state=[-0.12036032 -0.98378487  0.11373895  1.47295992]\n",
      "[ episode 14 ][ timestamp 8 ] state=[-0.12036032 -0.98378487  0.11373895  1.47295992], action=0, reward=1.0, next_state=[-0.14003601 -1.18009872  0.14319815  1.79889522]\n",
      "[ episode 14 ][ timestamp 9 ] state=[-0.14003601 -1.18009872  0.14319815  1.79889522], action=0, reward=1.0, next_state=[-0.16363799 -1.37650315  0.17917605  2.13244299]\n",
      "[ episode 14 ][ timestamp 10 ] state=[-0.16363799 -1.37650315  0.17917605  2.13244299], action=0, reward=-1.0, next_state=[-0.19116805 -1.57289304  0.22182491  2.47470812]\n",
      "[ Ended! ] Episode 14: Exploration_rate=0.9369146928798039. Score=10.\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ] state=[ 0.01713211 -0.02542854  0.00360869  0.04617124]\n",
      "[ episode 15 ][ timestamp 1 ] state=[ 0.01713211 -0.02542854  0.00360869  0.04617124], action=1, reward=1.0, next_state=[ 0.01662354  0.16964148  0.00453211 -0.24537093]\n",
      "[ episode 15 ][ timestamp 2 ] state=[ 0.01662354  0.16964148  0.00453211 -0.24537093], action=1, reward=1.0, next_state=[ 2.00163728e-02  3.64698412e-01 -3.75307292e-04 -5.36620881e-01]\n",
      "[ episode 15 ][ timestamp 3 ] state=[ 2.00163728e-02  3.64698412e-01 -3.75307292e-04 -5.36620881e-01], action=1, reward=1.0, next_state=[ 0.02731034  0.55982564 -0.01110772 -0.82942204]\n",
      "[ episode 15 ][ timestamp 4 ] state=[ 0.02731034  0.55982564 -0.01110772 -0.82942204], action=0, reward=1.0, next_state=[ 0.03850685  0.36485728 -0.02769617 -0.54025315]\n",
      "[ episode 15 ][ timestamp 5 ] state=[ 0.03850685  0.36485728 -0.02769617 -0.54025315], action=1, reward=1.0, next_state=[ 0.045804    0.56035738 -0.03850123 -0.84153246]\n",
      "[ episode 15 ][ timestamp 6 ] state=[ 0.045804    0.56035738 -0.03850123 -0.84153246], action=1, reward=1.0, next_state=[ 0.05701115  0.75598314 -0.05533188 -1.14607021]\n",
      "[ episode 15 ][ timestamp 7 ] state=[ 0.05701115  0.75598314 -0.05533188 -1.14607021], action=0, reward=1.0, next_state=[ 0.07213081  0.56162574 -0.07825328 -0.87123955]\n",
      "[ episode 15 ][ timestamp 8 ] state=[ 0.07213081  0.56162574 -0.07825328 -0.87123955], action=1, reward=1.0, next_state=[ 0.08336332  0.75771981 -0.09567807 -1.18746351]\n",
      "[ episode 15 ][ timestamp 9 ] state=[ 0.08336332  0.75771981 -0.09567807 -1.18746351], action=1, reward=1.0, next_state=[ 0.09851772  0.95394311 -0.11942734 -1.50853873]\n",
      "[ episode 15 ][ timestamp 10 ] state=[ 0.09851772  0.95394311 -0.11942734 -1.50853873], action=1, reward=1.0, next_state=[ 0.11759658  1.15029312 -0.14959812 -1.83599408]\n",
      "[ episode 15 ][ timestamp 11 ] state=[ 0.11759658  1.15029312 -0.14959812 -1.83599408], action=0, reward=1.0, next_state=[ 0.14060245  0.9571086  -0.186318   -1.59327179]\n",
      "[ episode 15 ][ timestamp 12 ] state=[ 0.14060245  0.9571086  -0.186318   -1.59327179], action=0, reward=-1.0, next_state=[ 0.15974462  0.76462184 -0.21818344 -1.36399983]\n",
      "[ Ended! ] Episode 15: Exploration_rate=0.9322301194154049. Score=12.\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ] state=[-0.00043132 -0.00893639  0.04828571 -0.01982985]\n",
      "[ episode 16 ][ timestamp 1 ] state=[-0.00043132 -0.00893639  0.04828571 -0.01982985], action=1, reward=1.0, next_state=[-0.00061005  0.18546103  0.04788911 -0.29689563]\n",
      "[ episode 16 ][ timestamp 2 ] state=[-0.00061005  0.18546103  0.04788911 -0.29689563], action=1, reward=1.0, next_state=[ 0.00309917  0.37986875  0.0419512  -0.57409887]\n",
      "[ episode 16 ][ timestamp 3 ] state=[ 0.00309917  0.37986875  0.0419512  -0.57409887], action=1, reward=1.0, next_state=[ 0.01069654  0.57437822  0.03046922 -0.85327634]\n",
      "[ episode 16 ][ timestamp 4 ] state=[ 0.01069654  0.57437822  0.03046922 -0.85327634], action=1, reward=1.0, next_state=[ 0.02218411  0.76907189  0.01340369 -1.13622472]\n",
      "[ episode 16 ][ timestamp 5 ] state=[ 0.02218411  0.76907189  0.01340369 -1.13622472], action=1, reward=1.0, next_state=[ 0.03756555  0.96401595 -0.0093208  -1.42467398]\n",
      "[ episode 16 ][ timestamp 6 ] state=[ 0.03756555  0.96401595 -0.0093208  -1.42467398], action=0, reward=1.0, next_state=[ 0.05684587  0.76901045 -0.03781428 -1.13491871]\n",
      "[ episode 16 ][ timestamp 7 ] state=[ 0.05684587  0.76901045 -0.03781428 -1.13491871], action=1, reward=1.0, next_state=[ 0.07222608  0.96460624 -0.06051266 -1.4392174 ]\n",
      "[ episode 16 ][ timestamp 8 ] state=[ 0.07222608  0.96460624 -0.06051266 -1.4392174 ], action=1, reward=1.0, next_state=[ 0.0915182   1.1604193  -0.089297   -1.75017925]\n",
      "[ episode 16 ][ timestamp 9 ] state=[ 0.0915182   1.1604193  -0.089297   -1.75017925], action=1, reward=1.0, next_state=[ 0.11472659  1.35643454 -0.12430059 -2.06924907]\n",
      "[ episode 16 ][ timestamp 10 ] state=[ 0.11472659  1.35643454 -0.12430059 -2.06924907], action=0, reward=1.0, next_state=[ 0.14185528  1.16277683 -0.16568557 -1.81745407]\n",
      "[ episode 16 ][ timestamp 11 ] state=[ 0.14185528  1.16277683 -0.16568557 -1.81745407], action=0, reward=1.0, next_state=[ 0.16511081  0.96984032 -0.20203465 -1.58050154]\n",
      "[ episode 16 ][ timestamp 12 ] state=[ 0.16511081  0.96984032 -0.20203465 -1.58050154], action=1, reward=-1.0, next_state=[ 0.18450762  1.16671247 -0.23364468 -1.92879823]\n",
      "[ Ended! ] Episode 16: Exploration_rate=0.9275689688183278. Score=12.\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ] state=[ 0.01971887  0.01740424  0.0294541  -0.04498197]\n",
      "[ episode 17 ][ timestamp 1 ] state=[ 0.01971887  0.01740424  0.0294541  -0.04498197], action=1, reward=1.0, next_state=[ 0.02006696  0.21209172  0.02855446 -0.32822827]\n",
      "[ episode 17 ][ timestamp 2 ] state=[ 0.02006696  0.21209172  0.02855446 -0.32822827], action=1, reward=1.0, next_state=[ 0.02430879  0.40679577  0.02198989 -0.61177142]\n",
      "[ episode 17 ][ timestamp 3 ] state=[ 0.02430879  0.40679577  0.02198989 -0.61177142], action=0, reward=1.0, next_state=[ 0.03244471  0.2113735   0.00975446 -0.31224437]\n",
      "[ episode 17 ][ timestamp 4 ] state=[ 0.03244471  0.2113735   0.00975446 -0.31224437], action=0, reward=1.0, next_state=[ 0.03667218  0.01611395  0.00350958 -0.01650122]\n",
      "[ episode 17 ][ timestamp 5 ] state=[ 0.03667218  0.01611395  0.00350958 -0.01650122], action=0, reward=1.0, next_state=[ 0.03699445 -0.17905816  0.00317955  0.27728695]\n",
      "[ episode 17 ][ timestamp 6 ] state=[ 0.03699445 -0.17905816  0.00317955  0.27728695], action=0, reward=1.0, next_state=[ 0.03341329 -0.37422532  0.00872529  0.57097101]\n",
      "[ episode 17 ][ timestamp 7 ] state=[ 0.03341329 -0.37422532  0.00872529  0.57097101], action=1, reward=1.0, next_state=[ 0.02592879 -0.17922681  0.02014471  0.28104958]\n",
      "[ episode 17 ][ timestamp 8 ] state=[ 0.02592879 -0.17922681  0.02014471  0.28104958], action=0, reward=1.0, next_state=[ 0.02234425 -0.37463023  0.0257657   0.58001739]\n",
      "[ episode 17 ][ timestamp 9 ] state=[ 0.02234425 -0.37463023  0.0257657   0.58001739], action=1, reward=1.0, next_state=[ 0.01485164 -0.17987864  0.03736605  0.29556124]\n",
      "[ episode 17 ][ timestamp 10 ] state=[ 0.01485164 -0.17987864  0.03736605  0.29556124], action=1, reward=1.0, next_state=[0.01125407 0.01469124 0.04327727 0.0148932 ]\n",
      "[ episode 17 ][ timestamp 11 ] state=[0.01125407 0.01469124 0.04327727 0.0148932 ], action=1, reward=1.0, next_state=[ 0.0115479   0.20916668  0.04357514 -0.26382728]\n",
      "[ episode 17 ][ timestamp 12 ] state=[ 0.0115479   0.20916668  0.04357514 -0.26382728], action=0, reward=1.0, next_state=[0.01573123 0.01345072 0.03829859 0.04227502]\n",
      "[ episode 17 ][ timestamp 13 ] state=[0.01573123 0.01345072 0.03829859 0.04227502], action=1, reward=1.0, next_state=[ 0.01600024  0.20800315  0.03914409 -0.23808258]\n",
      "[ episode 17 ][ timestamp 14 ] state=[ 0.01600024  0.20800315  0.03914409 -0.23808258], action=1, reward=1.0, next_state=[ 0.02016031  0.40254466  0.03438244 -0.51816588]\n",
      "[ episode 17 ][ timestamp 15 ] state=[ 0.02016031  0.40254466  0.03438244 -0.51816588], action=0, reward=1.0, next_state=[ 0.0282112   0.20695591  0.02401912 -0.21484972]\n",
      "[ episode 17 ][ timestamp 16 ] state=[ 0.0282112   0.20695591  0.02401912 -0.21484972], action=1, reward=1.0, next_state=[ 0.03235032  0.40172639  0.01972213 -0.49986021]\n",
      "[ episode 17 ][ timestamp 17 ] state=[ 0.03235032  0.40172639  0.01972213 -0.49986021], action=0, reward=1.0, next_state=[ 0.04038485  0.20633203  0.00972493 -0.20102775]\n",
      "[ episode 17 ][ timestamp 18 ] state=[ 0.04038485  0.20633203  0.00972493 -0.20102775], action=0, reward=1.0, next_state=[0.04451149 0.01107236 0.00570437 0.094707  ]\n",
      "[ episode 17 ][ timestamp 19 ] state=[0.04451149 0.01107236 0.00570437 0.094707  ], action=0, reward=1.0, next_state=[ 0.04473293 -0.18413089  0.00759851  0.38918418]\n",
      "[ episode 17 ][ timestamp 20 ] state=[ 0.04473293 -0.18413089  0.00759851  0.38918418], action=1, reward=1.0, next_state=[0.04105032 0.01088239 0.01538219 0.09890664]\n",
      "[ episode 17 ][ timestamp 21 ] state=[0.04105032 0.01088239 0.01538219 0.09890664], action=1, reward=1.0, next_state=[ 0.04126796  0.20578055  0.01736033 -0.18888382]\n",
      "[ episode 17 ][ timestamp 22 ] state=[ 0.04126796  0.20578055  0.01736033 -0.18888382], action=1, reward=1.0, next_state=[ 0.04538358  0.40064989  0.01358265 -0.4760401 ]\n",
      "[ episode 17 ][ timestamp 23 ] state=[ 0.04538358  0.40064989  0.01358265 -0.4760401 ], action=1, reward=1.0, next_state=[ 0.05339657  0.59557744  0.00406185 -0.76441128]\n",
      "[ episode 17 ][ timestamp 24 ] state=[ 0.05339657  0.59557744  0.00406185 -0.76441128], action=0, reward=1.0, next_state=[ 0.06530812  0.40039979 -0.01122638 -0.47045303]\n",
      "[ episode 17 ][ timestamp 25 ] state=[ 0.06530812  0.40039979 -0.01122638 -0.47045303], action=0, reward=1.0, next_state=[ 0.07331612  0.2054382  -0.02063544 -0.18132957]\n",
      "[ episode 17 ][ timestamp 26 ] state=[ 0.07331612  0.2054382  -0.02063544 -0.18132957], action=1, reward=1.0, next_state=[ 0.07742488  0.40084926 -0.02426203 -0.48045014]\n",
      "[ episode 17 ][ timestamp 27 ] state=[ 0.07742488  0.40084926 -0.02426203 -0.48045014], action=1, reward=1.0, next_state=[ 0.08544187  0.59630514 -0.03387103 -0.78068002]\n",
      "[ episode 17 ][ timestamp 28 ] state=[ 0.08544187  0.59630514 -0.03387103 -0.78068002], action=1, reward=1.0, next_state=[ 0.09736797  0.79187594 -0.04948463 -1.08382413]\n",
      "[ episode 17 ][ timestamp 29 ] state=[ 0.09736797  0.79187594 -0.04948463 -1.08382413], action=0, reward=1.0, next_state=[ 0.11320549  0.59744063 -0.07116111 -0.80707073]\n",
      "[ episode 17 ][ timestamp 30 ] state=[ 0.11320549  0.59744063 -0.07116111 -0.80707073], action=0, reward=1.0, next_state=[ 0.1251543   0.4033624  -0.08730253 -0.53759389]\n",
      "[ episode 17 ][ timestamp 31 ] state=[ 0.1251543   0.4033624  -0.08730253 -0.53759389], action=0, reward=1.0, next_state=[ 0.13322155  0.20956936 -0.09805441 -0.27364575]\n",
      "[ episode 17 ][ timestamp 32 ] state=[ 0.13322155  0.20956936 -0.09805441 -0.27364575], action=1, reward=1.0, next_state=[ 0.13741294  0.4059437  -0.10352732 -0.59557416]\n",
      "[ episode 17 ][ timestamp 33 ] state=[ 0.13741294  0.4059437  -0.10352732 -0.59557416], action=1, reward=1.0, next_state=[ 0.14553181  0.60235055 -0.11543881 -0.91898974]\n",
      "[ episode 17 ][ timestamp 34 ] state=[ 0.14553181  0.60235055 -0.11543881 -0.91898974], action=1, reward=1.0, next_state=[ 0.15757882  0.79882776 -0.1338186  -1.2456077 ]\n",
      "[ episode 17 ][ timestamp 35 ] state=[ 0.15757882  0.79882776 -0.1338186  -1.2456077 ], action=0, reward=1.0, next_state=[ 0.17355538  0.60565179 -0.15873075 -0.99765969]\n",
      "[ episode 17 ][ timestamp 36 ] state=[ 0.17355538  0.60565179 -0.15873075 -0.99765969], action=1, reward=1.0, next_state=[ 0.18566841  0.80249878 -0.17868395 -1.33568936]\n",
      "[ episode 17 ][ timestamp 37 ] state=[ 0.18566841  0.80249878 -0.17868395 -1.33568936], action=1, reward=1.0, next_state=[ 0.20171839  0.99936472 -0.20539774 -1.67854064]\n",
      "[ episode 17 ][ timestamp 38 ] state=[ 0.20171839  0.99936472 -0.20539774 -1.67854064], action=0, reward=-1.0, next_state=[ 0.22170568  0.8071308  -0.23896855 -1.45621415]\n",
      "[ Ended! ] Episode 17: Exploration_rate=0.9229311239742362. Score=38.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 18 ] state=[-0.03816452  0.03167719 -0.00492155 -0.03666001]\n",
      "[ episode 18 ][ timestamp 1 ] state=[-0.03816452  0.03167719 -0.00492155 -0.03666001], action=1, reward=1.0, next_state=[-0.03753098  0.22686937 -0.00565475 -0.33089167]\n",
      "[ episode 18 ][ timestamp 2 ] state=[-0.03753098  0.22686937 -0.00565475 -0.33089167], action=1, reward=1.0, next_state=[-0.03299359  0.42207135 -0.01227259 -0.62535245]\n",
      "[ episode 18 ][ timestamp 3 ] state=[-0.03299359  0.42207135 -0.01227259 -0.62535245], action=1, reward=1.0, next_state=[-0.02455217  0.61736246 -0.02477964 -0.9218751 ]\n",
      "[ episode 18 ][ timestamp 4 ] state=[-0.02455217  0.61736246 -0.02477964 -0.9218751 ], action=1, reward=1.0, next_state=[-0.01220492  0.81281032 -0.04321714 -1.22224135]\n",
      "[ episode 18 ][ timestamp 5 ] state=[-0.01220492  0.81281032 -0.04321714 -1.22224135], action=0, reward=1.0, next_state=[ 0.00405129  0.618271   -0.06766197 -0.94340672]\n",
      "[ episode 18 ][ timestamp 6 ] state=[ 0.00405129  0.618271   -0.06766197 -0.94340672], action=1, reward=1.0, next_state=[ 0.01641671  0.81423611 -0.0865301  -1.25655922]\n",
      "[ episode 18 ][ timestamp 7 ] state=[ 0.01641671  0.81423611 -0.0865301  -1.25655922], action=0, reward=1.0, next_state=[ 0.03270143  0.62032178 -0.11166128 -0.99218412]\n",
      "[ episode 18 ][ timestamp 8 ] state=[ 0.03270143  0.62032178 -0.11166128 -0.99218412], action=0, reward=1.0, next_state=[ 0.04510787  0.42685669 -0.13150497 -0.73655397]\n",
      "[ episode 18 ][ timestamp 9 ] state=[ 0.04510787  0.42685669 -0.13150497 -0.73655397], action=1, reward=1.0, next_state=[ 0.053645    0.62352611 -0.14623605 -1.06756206]\n",
      "[ episode 18 ][ timestamp 10 ] state=[ 0.053645    0.62352611 -0.14623605 -1.06756206], action=0, reward=1.0, next_state=[ 0.06611552  0.43060933 -0.16758729 -0.82411584]\n",
      "[ episode 18 ][ timestamp 11 ] state=[ 0.06611552  0.43060933 -0.16758729 -0.82411584], action=0, reward=1.0, next_state=[ 0.07472771  0.23812742 -0.1840696  -0.58847832]\n",
      "[ episode 18 ][ timestamp 12 ] state=[ 0.07472771  0.23812742 -0.1840696  -0.58847832], action=0, reward=1.0, next_state=[ 0.07949026  0.0459952  -0.19583917 -0.35895992]\n",
      "[ episode 18 ][ timestamp 13 ] state=[ 0.07949026  0.0459952  -0.19583917 -0.35895992], action=1, reward=1.0, next_state=[ 0.08041016  0.24328344 -0.20301837 -0.70644482]\n",
      "[ episode 18 ][ timestamp 14 ] state=[ 0.08041016  0.24328344 -0.20301837 -0.70644482], action=1, reward=-1.0, next_state=[ 0.08527583  0.44055265 -0.21714727 -1.05554973]\n",
      "[ Ended! ] Episode 18: Exploration_rate=0.918316468354365. Score=14.\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ] state=[0.03387186 0.02571307 0.02149082 0.00112801]\n",
      "[ episode 19 ][ timestamp 1 ] state=[0.03387186 0.02571307 0.02149082 0.00112801], action=1, reward=1.0, next_state=[ 0.03438612  0.22052032  0.02151338 -0.28469758]\n",
      "[ episode 19 ][ timestamp 2 ] state=[ 0.03438612  0.22052032  0.02151338 -0.28469758], action=0, reward=1.0, next_state=[0.03879653 0.02509825 0.01581943 0.01469214]\n",
      "[ episode 19 ][ timestamp 3 ] state=[0.03879653 0.02509825 0.01581943 0.01469214], action=0, reward=1.0, next_state=[ 0.03929849 -0.17024695  0.01611328  0.31232401]\n",
      "[ episode 19 ][ timestamp 4 ] state=[ 0.03929849 -0.17024695  0.01611328  0.31232401], action=1, reward=1.0, next_state=[0.03589355 0.02464178 0.02235976 0.02476596]\n",
      "[ episode 19 ][ timestamp 5 ] state=[0.03589355 0.02464178 0.02235976 0.02476596], action=0, reward=1.0, next_state=[ 0.03638639 -0.17079357  0.02285507  0.32441893]\n",
      "[ episode 19 ][ timestamp 6 ] state=[ 0.03638639 -0.17079357  0.02285507  0.32441893], action=0, reward=1.0, next_state=[ 0.03297052 -0.36623337  0.02934345  0.62422087]\n",
      "[ episode 19 ][ timestamp 7 ] state=[ 0.03297052 -0.36623337  0.02934345  0.62422087], action=0, reward=1.0, next_state=[ 0.02564585 -0.56175244  0.04182787  0.92599896]\n",
      "[ episode 19 ][ timestamp 8 ] state=[ 0.02564585 -0.56175244  0.04182787  0.92599896], action=0, reward=1.0, next_state=[ 0.0144108  -0.75741355  0.06034785  1.23152773]\n",
      "[ episode 19 ][ timestamp 9 ] state=[ 0.0144108  -0.75741355  0.06034785  1.23152773], action=0, reward=1.0, next_state=[-7.37470024e-04 -9.53257513e-01  8.49784044e-02  1.54249041e+00]\n",
      "[ episode 19 ][ timestamp 10 ] state=[-7.37470024e-04 -9.53257513e-01  8.49784044e-02  1.54249041e+00], action=0, reward=1.0, next_state=[-0.01980262 -1.14929197  0.11582821  1.8604346 ]\n",
      "[ episode 19 ][ timestamp 11 ] state=[-0.01980262 -1.14929197  0.11582821  1.8604346 ], action=0, reward=1.0, next_state=[-0.04278846 -1.34547826  0.1530369   2.18671959]\n",
      "[ episode 19 ][ timestamp 12 ] state=[-0.04278846 -1.34547826  0.1530369   2.18671959], action=1, reward=1.0, next_state=[-0.06969802 -1.15213458  0.1967713   1.944911  ]\n",
      "[ episode 19 ][ timestamp 13 ] state=[-0.06969802 -1.15213458  0.1967713   1.944911  ], action=1, reward=-1.0, next_state=[-0.09274072 -0.95957936  0.23566952  1.71912997]\n",
      "[ Ended! ] Episode 19: Exploration_rate=0.9137248860125932. Score=13.\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ] state=[-0.04166013  0.02156697  0.04292098 -0.0303552 ]\n",
      "[ episode 20 ][ timestamp 1 ] state=[-0.04166013  0.02156697  0.04292098 -0.0303552 ], action=1, reward=1.0, next_state=[-0.0412288   0.21604797  0.04231387 -0.30919314]\n",
      "[ episode 20 ][ timestamp 2 ] state=[-0.0412288   0.21604797  0.04231387 -0.30919314], action=1, reward=1.0, next_state=[-0.03690784  0.41054228  0.03613001 -0.5882369 ]\n",
      "[ episode 20 ][ timestamp 3 ] state=[-0.03690784  0.41054228  0.03613001 -0.5882369 ], action=1, reward=1.0, next_state=[-0.02869699  0.60514014  0.02436527 -0.86932328]\n",
      "[ episode 20 ][ timestamp 4 ] state=[-0.02869699  0.60514014  0.02436527 -0.86932328], action=0, reward=1.0, next_state=[-0.01659419  0.40969535  0.00697881 -0.56908043]\n",
      "[ episode 20 ][ timestamp 5 ] state=[-0.01659419  0.40969535  0.00697881 -0.56908043], action=1, reward=1.0, next_state=[-0.00840028  0.60471872 -0.0044028  -0.85955662]\n",
      "[ episode 20 ][ timestamp 6 ] state=[-0.00840028  0.60471872 -0.0044028  -0.85955662], action=0, reward=1.0, next_state=[ 0.00369409  0.40965702 -0.02159394 -0.56826131]\n",
      "[ episode 20 ][ timestamp 7 ] state=[ 0.00369409  0.40965702 -0.02159394 -0.56826131], action=0, reward=1.0, next_state=[ 0.01188723  0.2148445  -0.03295916 -0.28245879]\n",
      "[ episode 20 ][ timestamp 8 ] state=[ 0.01188723  0.2148445  -0.03295916 -0.28245879], action=0, reward=1.0, next_state=[ 0.01618412  0.02020779 -0.03860834 -0.00035053]\n",
      "[ episode 20 ][ timestamp 9 ] state=[ 0.01618412  0.02020779 -0.03860834 -0.00035053], action=1, reward=1.0, next_state=[ 0.01658828  0.21586156 -0.03861535 -0.30496051]\n",
      "[ episode 20 ][ timestamp 10 ] state=[ 0.01658828  0.21586156 -0.03861535 -0.30496051], action=0, reward=1.0, next_state=[ 0.02090551  0.02131058 -0.04471456 -0.02470167]\n",
      "[ episode 20 ][ timestamp 11 ] state=[ 0.02090551  0.02131058 -0.04471456 -0.02470167], action=1, reward=1.0, next_state=[ 0.02133172  0.2170443  -0.04520859 -0.3311505 ]\n",
      "[ episode 20 ][ timestamp 12 ] state=[ 0.02133172  0.2170443  -0.04520859 -0.3311505 ], action=0, reward=1.0, next_state=[ 0.02567261  0.02259405 -0.0518316  -0.05305993]\n",
      "[ episode 20 ][ timestamp 13 ] state=[ 0.02567261  0.02259405 -0.0518316  -0.05305993], action=0, reward=1.0, next_state=[ 0.02612449 -0.17174786 -0.0528928   0.22282978]\n",
      "[ episode 20 ][ timestamp 14 ] state=[ 0.02612449 -0.17174786 -0.0528928   0.22282978], action=1, reward=1.0, next_state=[ 0.02268953  0.02408862 -0.0484362  -0.08605737]\n",
      "[ episode 20 ][ timestamp 15 ] state=[ 0.02268953  0.02408862 -0.0484362  -0.08605737], action=0, reward=1.0, next_state=[ 0.02317131 -0.17030678 -0.05015735  0.19095907]\n",
      "[ episode 20 ][ timestamp 16 ] state=[ 0.02317131 -0.17030678 -0.05015735  0.19095907], action=1, reward=1.0, next_state=[ 0.01976517  0.0254955  -0.04633817 -0.11711506]\n",
      "[ episode 20 ][ timestamp 17 ] state=[ 0.01976517  0.0254955  -0.04633817 -0.11711506], action=1, reward=1.0, next_state=[ 0.02027508  0.22124971 -0.04868047 -0.42404972]\n",
      "[ episode 20 ][ timestamp 18 ] state=[ 0.02027508  0.22124971 -0.04868047 -0.42404972], action=0, reward=1.0, next_state=[ 0.02470007  0.02684995 -0.05716147 -0.14710194]\n",
      "[ episode 20 ][ timestamp 19 ] state=[ 0.02470007  0.02684995 -0.05716147 -0.14710194], action=1, reward=1.0, next_state=[ 0.02523707  0.2227419  -0.0601035  -0.45725628]\n",
      "[ episode 20 ][ timestamp 20 ] state=[ 0.02523707  0.2227419  -0.0601035  -0.45725628], action=1, reward=1.0, next_state=[ 0.02969191  0.41865978 -0.06924863 -0.76826223]\n",
      "[ episode 20 ][ timestamp 21 ] state=[ 0.02969191  0.41865978 -0.06924863 -0.76826223], action=1, reward=1.0, next_state=[ 0.03806511  0.61466318 -0.08461387 -1.08190551]\n",
      "[ episode 20 ][ timestamp 22 ] state=[ 0.03806511  0.61466318 -0.08461387 -1.08190551], action=0, reward=1.0, next_state=[ 0.05035837  0.42075376 -0.10625198 -0.81692879]\n",
      "[ episode 20 ][ timestamp 23 ] state=[ 0.05035837  0.42075376 -0.10625198 -0.81692879], action=0, reward=1.0, next_state=[ 0.05877345  0.22723438 -0.12259056 -0.55946607]\n",
      "[ episode 20 ][ timestamp 24 ] state=[ 0.05877345  0.22723438 -0.12259056 -0.55946607], action=0, reward=1.0, next_state=[ 0.06331813  0.03402705 -0.13377988 -0.30778147]\n",
      "[ episode 20 ][ timestamp 25 ] state=[ 0.06331813  0.03402705 -0.13377988 -0.30778147], action=1, reward=1.0, next_state=[ 0.06399867  0.23077635 -0.13993551 -0.6394825 ]\n",
      "[ episode 20 ][ timestamp 26 ] state=[ 0.06399867  0.23077635 -0.13993551 -0.6394825 ], action=0, reward=1.0, next_state=[ 0.0686142   0.03785415 -0.15272516 -0.39393484]\n",
      "[ episode 20 ][ timestamp 27 ] state=[ 0.0686142   0.03785415 -0.15272516 -0.39393484], action=1, reward=1.0, next_state=[ 0.06937128  0.23477599 -0.16060386 -0.73060624]\n",
      "[ episode 20 ][ timestamp 28 ] state=[ 0.06937128  0.23477599 -0.16060386 -0.73060624], action=1, reward=1.0, next_state=[ 0.0740668   0.43171006 -0.17521598 -1.0692206 ]\n",
      "[ episode 20 ][ timestamp 29 ] state=[ 0.0740668   0.43171006 -0.17521598 -1.0692206 ], action=0, reward=1.0, next_state=[ 0.082701    0.23928328 -0.1966004  -0.83625014]\n",
      "[ episode 20 ][ timestamp 30 ] state=[ 0.082701    0.23928328 -0.1966004  -0.83625014], action=1, reward=-1.0, next_state=[ 0.08748667  0.43646888 -0.2133254  -1.18375964]\n",
      "[ Ended! ] Episode 20: Exploration_rate=0.9091562615825302. Score=30.\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ] state=[-0.02014701 -0.00057976  0.02066312  0.00078902]\n",
      "[ episode 21 ][ timestamp 1 ] state=[-0.02014701 -0.00057976  0.02066312  0.00078902], action=0, reward=1.0, next_state=[-0.02015861 -0.19599186  0.0206789   0.29991913]\n",
      "[ episode 21 ][ timestamp 2 ] state=[-0.02015861 -0.19599186  0.0206789   0.29991913], action=0, reward=1.0, next_state=[-0.02407844 -0.39140236  0.02667728  0.59905138]\n",
      "[ episode 21 ][ timestamp 3 ] state=[-0.02407844 -0.39140236  0.02667728  0.59905138], action=1, reward=1.0, next_state=[-0.03190649 -0.19666362  0.03865831  0.31488939]\n",
      "[ episode 21 ][ timestamp 4 ] state=[-0.03190649 -0.19666362  0.03865831  0.31488939], action=0, reward=1.0, next_state=[-0.03583976 -0.39231431  0.04495609  0.61950887]\n",
      "[ episode 21 ][ timestamp 5 ] state=[-0.03583976 -0.39231431  0.04495609  0.61950887], action=0, reward=1.0, next_state=[-0.04368605 -0.58803438  0.05734627  0.926005  ]\n",
      "[ episode 21 ][ timestamp 6 ] state=[-0.04368605 -0.58803438  0.05734627  0.926005  ], action=0, reward=1.0, next_state=[-0.05544674 -0.78388194  0.07586637  1.23614398]\n",
      "[ episode 21 ][ timestamp 7 ] state=[-0.05544674 -0.78388194  0.07586637  1.23614398], action=1, reward=1.0, next_state=[-0.07112438 -0.58981244  0.10058925  0.9681604 ]\n",
      "[ episode 21 ][ timestamp 8 ] state=[-0.07112438 -0.58981244  0.10058925  0.9681604 ], action=1, reward=1.0, next_state=[-0.08292063 -0.39617442  0.11995246  0.70869498]\n",
      "[ episode 21 ][ timestamp 9 ] state=[-0.08292063 -0.39617442  0.11995246  0.70869498], action=0, reward=1.0, next_state=[-0.09084411 -0.59273562  0.13412636  1.03659966]\n",
      "[ episode 21 ][ timestamp 10 ] state=[-0.09084411 -0.59273562  0.13412636  1.03659966], action=1, reward=1.0, next_state=[-0.10269883 -0.39962674  0.15485835  0.78885296]\n",
      "[ episode 21 ][ timestamp 11 ] state=[-0.10269883 -0.39962674  0.15485835  0.78885296], action=0, reward=1.0, next_state=[-0.11069136 -0.5964979   0.17063541  1.12597249]\n",
      "[ episode 21 ][ timestamp 12 ] state=[-0.11069136 -0.5964979   0.17063541  1.12597249], action=1, reward=1.0, next_state=[-0.12262132 -0.40397206  0.19315486  0.8913015 ]\n",
      "[ episode 21 ][ timestamp 13 ] state=[-0.12262132 -0.40397206  0.19315486  0.8913015 ], action=0, reward=-1.0, next_state=[-0.13070076 -0.60111545  0.21098089  1.2379524 ]\n",
      "[ Ended! ] Episode 21: Exploration_rate=0.9046104802746175. Score=13.\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ] state=[-0.01978792 -0.00914173  0.04344534 -0.00079638]\n",
      "[ episode 22 ][ timestamp 1 ] state=[-0.01978792 -0.00914173  0.04344534 -0.00079638], action=0, reward=1.0, next_state=[-0.01997075 -0.20485895  0.04342941  0.30527135]\n",
      "[ episode 22 ][ timestamp 2 ] state=[-0.01997075 -0.20485895  0.04342941  0.30527135], action=0, reward=1.0, next_state=[-0.02406793 -0.40057202  0.04953484  0.61132838]\n",
      "[ episode 22 ][ timestamp 3 ] state=[-0.02406793 -0.40057202  0.04953484  0.61132838], action=1, reward=1.0, next_state=[-0.03207937 -0.20617613  0.06176141  0.3346495 ]\n",
      "[ episode 22 ][ timestamp 4 ] state=[-0.03207937 -0.20617613  0.06176141  0.3346495 ], action=0, reward=1.0, next_state=[-0.03620289 -0.40212021  0.0684544   0.64615155]\n",
      "[ episode 22 ][ timestamp 5 ] state=[-0.03620289 -0.40212021  0.0684544   0.64615155], action=0, reward=1.0, next_state=[-0.0442453  -0.59812588  0.08137743  0.95958133]\n",
      "[ episode 22 ][ timestamp 6 ] state=[-0.0442453  -0.59812588  0.08137743  0.95958133], action=0, reward=1.0, next_state=[-0.05620782 -0.79424191  0.10056906  1.27668043]\n",
      "[ episode 22 ][ timestamp 7 ] state=[-0.05620782 -0.79424191  0.10056906  1.27668043], action=0, reward=1.0, next_state=[-0.07209265 -0.99049206  0.12610267  1.59908572]\n",
      "[ episode 22 ][ timestamp 8 ] state=[-0.07209265 -0.99049206  0.12610267  1.59908572], action=0, reward=1.0, next_state=[-0.09190249 -1.18686236  0.15808438  1.92827829]\n",
      "[ episode 22 ][ timestamp 9 ] state=[-0.09190249 -1.18686236  0.15808438  1.92827829], action=1, reward=1.0, next_state=[-0.11563974 -0.99374911  0.19664995  1.68850386]\n",
      "[ episode 22 ][ timestamp 10 ] state=[-0.11563974 -0.99374911  0.19664995  1.68850386], action=1, reward=-1.0, next_state=[-0.13551472 -0.80136917  0.23042002  1.46293884]\n",
      "[ Ended! ] Episode 22: Exploration_rate=0.9000874278732445. Score=10.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 23 ] state=[-0.04716767 -0.04133847  0.00636357 -0.03126362]\n",
      "[ episode 23 ][ timestamp 1 ] state=[-0.04716767 -0.04133847  0.00636357 -0.03126362], action=0, reward=1.0, next_state=[-0.04799443 -0.23655109  0.0057383   0.26342027]\n",
      "[ episode 23 ][ timestamp 2 ] state=[-0.04799443 -0.23655109  0.0057383   0.26342027], action=0, reward=1.0, next_state=[-0.05272546 -0.43175448  0.01100671  0.55790758]\n",
      "[ episode 23 ][ timestamp 3 ] state=[-0.05272546 -0.43175448  0.01100671  0.55790758], action=1, reward=1.0, next_state=[-0.06136055 -0.23678875  0.02216486  0.26871261]\n",
      "[ episode 23 ][ timestamp 4 ] state=[-0.06136055 -0.23678875  0.02216486  0.26871261], action=0, reward=1.0, next_state=[-0.06609632 -0.43221989  0.02753911  0.56830325]\n",
      "[ episode 23 ][ timestamp 5 ] state=[-0.06609632 -0.43221989  0.02753911  0.56830325], action=0, reward=1.0, next_state=[-0.07474072 -0.62771707  0.03890518  0.86953331]\n",
      "[ episode 23 ][ timestamp 6 ] state=[-0.07474072 -0.62771707  0.03890518  0.86953331], action=0, reward=1.0, next_state=[-0.08729506 -0.82334607  0.05629584  1.17418999]\n",
      "[ episode 23 ][ timestamp 7 ] state=[-0.08729506 -0.82334607  0.05629584  1.17418999], action=1, reward=1.0, next_state=[-0.10376198 -0.62899912  0.07977964  0.89967362]\n",
      "[ episode 23 ][ timestamp 8 ] state=[-0.10376198 -0.62899912  0.07977964  0.89967362], action=1, reward=1.0, next_state=[-0.11634196 -0.43504368  0.09777312  0.63309618]\n",
      "[ episode 23 ][ timestamp 9 ] state=[-0.11634196 -0.43504368  0.09777312  0.63309618], action=0, reward=1.0, next_state=[-0.12504284 -0.63138385  0.11043504  0.95489937]\n",
      "[ episode 23 ][ timestamp 10 ] state=[-0.12504284 -0.63138385  0.11043504  0.95489937], action=1, reward=1.0, next_state=[-0.13767051 -0.43790676  0.12953303  0.69885361]\n",
      "[ episode 23 ][ timestamp 11 ] state=[-0.13767051 -0.43790676  0.12953303  0.69885361], action=0, reward=1.0, next_state=[-0.14642865 -0.63456406  0.1435101   1.02934456]\n",
      "[ episode 23 ][ timestamp 12 ] state=[-0.14642865 -0.63456406  0.1435101   1.02934456], action=0, reward=1.0, next_state=[-0.15911993 -0.83127377  0.16409699  1.36342318]\n",
      "[ episode 23 ][ timestamp 13 ] state=[-0.15911993 -0.83127377  0.16409699  1.36342318], action=1, reward=1.0, next_state=[-0.17574541 -0.63854326  0.19136545  1.12623934]\n",
      "[ episode 23 ][ timestamp 14 ] state=[-0.17574541 -0.63854326  0.19136545  1.12623934], action=0, reward=-1.0, next_state=[-0.18851627 -0.83558622  0.21389024  1.47232706]\n",
      "[ Ended! ] Episode 23: Exploration_rate=0.8955869907338783. Score=14.\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ] state=[-0.00203755 -0.01872867  0.04701859  0.03601537]\n",
      "[ episode 24 ][ timestamp 1 ] state=[-0.00203755 -0.01872867  0.04701859  0.03601537], action=0, reward=1.0, next_state=[-0.00241213 -0.21449224  0.0477389   0.34315457]\n",
      "[ episode 24 ][ timestamp 2 ] state=[-0.00241213 -0.21449224  0.0477389   0.34315457], action=1, reward=1.0, next_state=[-0.00670197 -0.02008081  0.05460199  0.06589956]\n",
      "[ episode 24 ][ timestamp 3 ] state=[-0.00670197 -0.02008081  0.05460199  0.06589956], action=0, reward=1.0, next_state=[-0.00710359 -0.21594136  0.05591998  0.37529756]\n",
      "[ episode 24 ][ timestamp 4 ] state=[-0.00710359 -0.21594136  0.05591998  0.37529756], action=1, reward=1.0, next_state=[-0.01142242 -0.02165644  0.06342593  0.10075762]\n",
      "[ episode 24 ][ timestamp 5 ] state=[-0.01142242 -0.02165644  0.06342593  0.10075762], action=0, reward=1.0, next_state=[-0.01185554 -0.21762734  0.06544108  0.41275762]\n",
      "[ episode 24 ][ timestamp 6 ] state=[-0.01185554 -0.21762734  0.06544108  0.41275762], action=1, reward=1.0, next_state=[-0.01620809 -0.02349111  0.07369624  0.14140254]\n",
      "[ episode 24 ][ timestamp 7 ] state=[-0.01620809 -0.02349111  0.07369624  0.14140254], action=0, reward=1.0, next_state=[-0.01667791 -0.21958693  0.07652429  0.45639495]\n",
      "[ episode 24 ][ timestamp 8 ] state=[-0.01667791 -0.21958693  0.07652429  0.45639495], action=0, reward=1.0, next_state=[-0.02106965 -0.41570266  0.08565219  0.77218383]\n",
      "[ episode 24 ][ timestamp 9 ] state=[-0.02106965 -0.41570266  0.08565219  0.77218383], action=0, reward=1.0, next_state=[-0.02938371 -0.61189217  0.10109586  1.09054023]\n",
      "[ episode 24 ][ timestamp 10 ] state=[-0.02938371 -0.61189217  0.10109586  1.09054023], action=0, reward=1.0, next_state=[-0.04162155 -0.80819073  0.12290667  1.41315625]\n",
      "[ episode 24 ][ timestamp 11 ] state=[-0.04162155 -0.80819073  0.12290667  1.41315625], action=1, reward=1.0, next_state=[-0.05778536 -0.61478758  0.15116979  1.16128359]\n",
      "[ episode 24 ][ timestamp 12 ] state=[-0.05778536 -0.61478758  0.15116979  1.16128359], action=0, reward=1.0, next_state=[-0.07008112 -0.81152005  0.17439546  1.49729171]\n",
      "[ episode 24 ][ timestamp 13 ] state=[-0.07008112 -0.81152005  0.17439546  1.49729171], action=0, reward=1.0, next_state=[-0.08631152 -1.00827973  0.2043413   1.8389672 ]\n",
      "[ episode 24 ][ timestamp 14 ] state=[-0.08631152 -1.00827973  0.2043413   1.8389672 ], action=0, reward=-1.0, next_state=[-0.10647711 -1.20498915  0.24112064  2.18755161]\n",
      "[ Ended! ] Episode 24: Exploration_rate=0.8911090557802088. Score=14.\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ] state=[ 0.04008623 -0.04576434  0.02428739  0.01729052]\n",
      "[ episode 25 ][ timestamp 1 ] state=[ 0.04008623 -0.04576434  0.02428739  0.01729052], action=1, reward=1.0, next_state=[ 0.03917095  0.14900103  0.0246332  -0.26763159]\n",
      "[ episode 25 ][ timestamp 2 ] state=[ 0.03917095  0.14900103  0.0246332  -0.26763159], action=1, reward=1.0, next_state=[ 0.04215097  0.34376292  0.01928056 -0.55244437]\n",
      "[ episode 25 ][ timestamp 3 ] state=[ 0.04215097  0.34376292  0.01928056 -0.55244437], action=0, reward=1.0, next_state=[ 0.04902623  0.14837558  0.00823168 -0.2537497 ]\n",
      "[ episode 25 ][ timestamp 4 ] state=[ 0.04902623  0.14837558  0.00823168 -0.2537497 ], action=0, reward=1.0, next_state=[ 0.05199374 -0.04686293  0.00315668  0.04151823]\n",
      "[ episode 25 ][ timestamp 5 ] state=[ 0.05199374 -0.04686293  0.00315668  0.04151823], action=0, reward=1.0, next_state=[ 0.05105648 -0.24203001  0.00398705  0.33519545]\n",
      "[ episode 25 ][ timestamp 6 ] state=[ 0.05105648 -0.24203001  0.00398705  0.33519545], action=0, reward=1.0, next_state=[ 0.04621588 -0.43720847  0.01069096  0.62913301]\n",
      "[ episode 25 ][ timestamp 7 ] state=[ 0.04621588 -0.43720847  0.01069096  0.62913301], action=0, reward=1.0, next_state=[ 0.03747171 -0.63247797  0.02327362  0.92516361]\n",
      "[ episode 25 ][ timestamp 8 ] state=[ 0.03747171 -0.63247797  0.02327362  0.92516361], action=0, reward=1.0, next_state=[ 0.02482215 -0.82790641  0.04177689  1.22506869]\n",
      "[ episode 25 ][ timestamp 9 ] state=[ 0.02482215 -0.82790641  0.04177689  1.22506869], action=0, reward=1.0, next_state=[ 0.00826402 -1.02354068  0.06627826  1.53054289]\n",
      "[ episode 25 ][ timestamp 10 ] state=[ 0.00826402 -1.02354068  0.06627826  1.53054289], action=1, reward=1.0, next_state=[-0.01220679 -0.82927746  0.09688912  1.2592594 ]\n",
      "[ episode 25 ][ timestamp 11 ] state=[-0.01220679 -0.82927746  0.09688912  1.2592594 ], action=1, reward=1.0, next_state=[-0.02879234 -0.63551938  0.12207431  0.99842624]\n",
      "[ episode 25 ][ timestamp 12 ] state=[-0.02879234 -0.63551938  0.12207431  0.99842624], action=1, reward=1.0, next_state=[-0.04150273 -0.44222211  0.14204283  0.74643883]\n",
      "[ episode 25 ][ timestamp 13 ] state=[-0.04150273 -0.44222211  0.14204283  0.74643883], action=0, reward=1.0, next_state=[-0.05034717 -0.63898838  0.15697161  1.08023606]\n",
      "[ episode 25 ][ timestamp 14 ] state=[-0.05034717 -0.63898838  0.15697161  1.08023606], action=0, reward=1.0, next_state=[-0.06312694 -0.83579484  0.17857633  1.41777658]\n",
      "[ episode 25 ][ timestamp 15 ] state=[-0.06312694 -0.83579484  0.17857633  1.41777658], action=0, reward=1.0, next_state=[-0.07984284 -1.03262099  0.20693186  1.76054363]\n",
      "[ episode 25 ][ timestamp 16 ] state=[-0.07984284 -1.03262099  0.20693186  1.76054363], action=0, reward=-1.0, next_state=[-0.10049525 -1.22939757  0.24214274  2.10981612]\n",
      "[ Ended! ] Episode 25: Exploration_rate=0.8866535105013078. Score=16.\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ] state=[ 0.01611798 -0.01455908 -0.00213678 -0.01430272]\n",
      "[ episode 26 ][ timestamp 1 ] state=[ 0.01611798 -0.01455908 -0.00213678 -0.01430272], action=1, reward=1.0, next_state=[ 0.0158268   0.18059345 -0.00242284 -0.30765906]\n",
      "[ episode 26 ][ timestamp 2 ] state=[ 0.0158268   0.18059345 -0.00242284 -0.30765906], action=1, reward=1.0, next_state=[ 0.01943867  0.37574984 -0.00857602 -0.6011051 ]\n",
      "[ episode 26 ][ timestamp 3 ] state=[ 0.01943867  0.37574984 -0.00857602 -0.6011051 ], action=0, reward=1.0, next_state=[ 0.02695367  0.1807489  -0.02059812 -0.31113577]\n",
      "[ episode 26 ][ timestamp 4 ] state=[ 0.02695367  0.1807489  -0.02059812 -0.31113577], action=1, reward=1.0, next_state=[ 0.03056865  0.37615817 -0.02682084 -0.61024291]\n",
      "[ episode 26 ][ timestamp 5 ] state=[ 0.03056865  0.37615817 -0.02682084 -0.61024291], action=1, reward=1.0, next_state=[ 0.03809181  0.57164455 -0.03902569 -0.9112514 ]\n",
      "[ episode 26 ][ timestamp 6 ] state=[ 0.03809181  0.57164455 -0.03902569 -0.9112514 ], action=1, reward=1.0, next_state=[ 0.0495247   0.76727222 -0.05725072 -1.21594013]\n",
      "[ episode 26 ][ timestamp 7 ] state=[ 0.0495247   0.76727222 -0.05725072 -1.21594013], action=1, reward=1.0, next_state=[ 0.06487014  0.96308399 -0.08156952 -1.52599908]\n",
      "[ episode 26 ][ timestamp 8 ] state=[ 0.06487014  0.96308399 -0.08156952 -1.52599908], action=0, reward=1.0, next_state=[ 0.08413182  0.76903584 -0.11208951 -1.2598495 ]\n",
      "[ episode 26 ][ timestamp 9 ] state=[ 0.08413182  0.76903584 -0.11208951 -1.2598495 ], action=1, reward=1.0, next_state=[ 0.09951254  0.96539891 -0.1372865  -1.58543107]\n",
      "[ episode 26 ][ timestamp 10 ] state=[ 0.09951254  0.96539891 -0.1372865  -1.58543107], action=1, reward=1.0, next_state=[ 0.11882052  1.16186026 -0.16899512 -1.91758589]\n",
      "[ episode 26 ][ timestamp 11 ] state=[ 0.11882052  1.16186026 -0.16899512 -1.91758589], action=1, reward=1.0, next_state=[ 0.14205773  1.35834987 -0.20734683 -2.25757001]\n",
      "[ episode 26 ][ timestamp 12 ] state=[ 0.14205773  1.35834987 -0.20734683 -2.25757001], action=1, reward=-1.0, next_state=[ 0.16922472  1.55472839 -0.25249823 -2.60635241]\n",
      "[ Ended! ] Episode 26: Exploration_rate=0.8822202429488013. Score=12.\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ] state=[ 0.00761592 -0.01799296  0.03406501  0.00724279]\n",
      "[ episode 27 ][ timestamp 1 ] state=[ 0.00761592 -0.01799296  0.03406501  0.00724279], action=1, reward=1.0, next_state=[ 0.00725607  0.17662432  0.03420987 -0.27450058]\n",
      "[ episode 27 ][ timestamp 2 ] state=[ 0.00725607  0.17662432  0.03420987 -0.27450058], action=1, reward=1.0, next_state=[ 0.01078855  0.37124188  0.02871986 -0.55620039]\n",
      "[ episode 27 ][ timestamp 3 ] state=[ 0.01078855  0.37124188  0.02871986 -0.55620039], action=1, reward=1.0, next_state=[ 0.01821339  0.5659491   0.01759585 -0.83969829]\n",
      "[ episode 27 ][ timestamp 4 ] state=[ 0.01821339  0.5659491   0.01759585 -0.83969829], action=0, reward=1.0, next_state=[ 0.02953237  0.37059138  0.00080188 -0.54153416]\n",
      "[ episode 27 ][ timestamp 5 ] state=[ 0.02953237  0.37059138  0.00080188 -0.54153416], action=0, reward=1.0, next_state=[ 0.0369442   0.17545816 -0.0100288  -0.24859868]\n",
      "[ episode 27 ][ timestamp 6 ] state=[ 0.0369442   0.17545816 -0.0100288  -0.24859868], action=0, reward=1.0, next_state=[ 0.04045336 -0.01951914 -0.01500078  0.04090415]\n",
      "[ episode 27 ][ timestamp 7 ] state=[ 0.04045336 -0.01951914 -0.01500078  0.04090415], action=0, reward=1.0, next_state=[ 0.04006298 -0.21442281 -0.01418269  0.32881669]\n",
      "[ episode 27 ][ timestamp 8 ] state=[ 0.04006298 -0.21442281 -0.01418269  0.32881669], action=1, reward=1.0, next_state=[ 0.03577452 -0.01910185 -0.00760636  0.03169516]\n",
      "[ episode 27 ][ timestamp 9 ] state=[ 0.03577452 -0.01910185 -0.00760636  0.03169516], action=0, reward=1.0, next_state=[ 0.03539249 -0.2141139  -0.00697246  0.32196852]\n",
      "[ episode 27 ][ timestamp 10 ] state=[ 0.03539249 -0.2141139  -0.00697246  0.32196852], action=1, reward=1.0, next_state=[ 0.03111021 -0.01889336 -0.00053308  0.02709494]\n",
      "[ episode 27 ][ timestamp 11 ] state=[ 0.03111021 -0.01889336 -0.00053308  0.02709494], action=0, reward=1.0, next_state=[ 3.07323412e-02 -2.14007662e-01  8.81399811e-06  3.19609625e-01]\n",
      "[ episode 27 ][ timestamp 12 ] state=[ 3.07323412e-02 -2.14007662e-01  8.81399811e-06  3.19609625e-01], action=0, reward=1.0, next_state=[ 0.02645219 -0.40912974  0.00640101  0.61229533]\n",
      "[ episode 27 ][ timestamp 13 ] state=[ 0.02645219 -0.40912974  0.00640101  0.61229533], action=1, reward=1.0, next_state=[ 0.01826959 -0.21409783  0.01864691  0.32163534]\n",
      "[ episode 27 ][ timestamp 14 ] state=[ 0.01826959 -0.21409783  0.01864691  0.32163534], action=0, reward=1.0, next_state=[ 0.01398764 -0.40948029  0.02507962  0.62013996]\n",
      "[ episode 27 ][ timestamp 15 ] state=[ 0.01398764 -0.40948029  0.02507962  0.62013996], action=0, reward=1.0, next_state=[ 0.00579803 -0.60494337  0.03748242  0.920615  ]\n",
      "[ episode 27 ][ timestamp 16 ] state=[ 0.00579803 -0.60494337  0.03748242  0.920615  ], action=1, reward=1.0, next_state=[-0.00630084 -0.41034747  0.05589472  0.63994344]\n",
      "[ episode 27 ][ timestamp 17 ] state=[-0.00630084 -0.41034747  0.05589472  0.63994344], action=1, reward=1.0, next_state=[-0.01450779 -0.21604752  0.06869359  0.36537315]\n",
      "[ episode 27 ][ timestamp 18 ] state=[-0.01450779 -0.21604752  0.06869359  0.36537315], action=0, reward=1.0, next_state=[-0.01882874 -0.41207502  0.07600105  0.67890095]\n",
      "[ episode 27 ][ timestamp 19 ] state=[-0.01882874 -0.41207502  0.07600105  0.67890095], action=1, reward=1.0, next_state=[-0.02707024 -0.21808653  0.08957907  0.411081  ]\n",
      "[ episode 27 ][ timestamp 20 ] state=[-0.02707024 -0.21808653  0.08957907  0.411081  ], action=0, reward=1.0, next_state=[-0.03143197 -0.41435663  0.09780069  0.73060677]\n",
      "[ episode 27 ][ timestamp 21 ] state=[-0.03143197 -0.41435663  0.09780069  0.73060677], action=1, reward=1.0, next_state=[-0.0397191  -0.22071261  0.11241283  0.47023636]\n",
      "[ episode 27 ][ timestamp 22 ] state=[-0.0397191  -0.22071261  0.11241283  0.47023636], action=0, reward=1.0, next_state=[-0.04413335 -0.41722803  0.12181755  0.7961288 ]\n",
      "[ episode 27 ][ timestamp 23 ] state=[-0.04413335 -0.41722803  0.12181755  0.7961288 ], action=0, reward=1.0, next_state=[-0.05247791 -0.61379227  0.13774013  1.12451603]\n",
      "[ episode 27 ][ timestamp 24 ] state=[-0.05247791 -0.61379227  0.13774013  1.12451603], action=0, reward=1.0, next_state=[-0.06475376 -0.81042409  0.16023045  1.45703792]\n",
      "[ episode 27 ][ timestamp 25 ] state=[-0.06475376 -0.81042409  0.16023045  1.45703792], action=1, reward=1.0, next_state=[-0.08096224 -0.61758967  0.18937121  1.21839787]\n",
      "[ episode 27 ][ timestamp 26 ] state=[-0.08096224 -0.61758967  0.18937121  1.21839787], action=1, reward=-1.0, next_state=[-0.09331403 -0.42534493  0.21373917  0.99052894]\n",
      "[ Ended! ] Episode 27: Exploration_rate=0.8778091417340573. Score=26.\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ] state=[-0.0144156  -0.0183929   0.00747609 -0.03979776]\n",
      "[ episode 28 ][ timestamp 1 ] state=[-0.0144156  -0.0183929   0.00747609 -0.03979776], action=1, reward=1.0, next_state=[-0.01478345  0.17662105  0.00668014 -0.33011256]\n",
      "[ episode 28 ][ timestamp 2 ] state=[-0.01478345  0.17662105  0.00668014 -0.33011256], action=0, reward=1.0, next_state=[-1.12510324e-02 -1.85953507e-02  7.78883356e-05 -3.53305419e-02]\n",
      "[ episode 28 ][ timestamp 3 ] state=[-1.12510324e-02 -1.85953507e-02  7.78883356e-05 -3.53305419e-02], action=0, reward=1.0, next_state=[-0.01162294 -0.21371842 -0.00062872  0.25737696]\n",
      "[ episode 28 ][ timestamp 4 ] state=[-0.01162294 -0.21371842 -0.00062872  0.25737696], action=1, reward=1.0, next_state=[-0.01589731 -0.0185875   0.00451882 -0.03550421]\n",
      "[ episode 28 ][ timestamp 5 ] state=[-0.01589731 -0.0185875   0.00451882 -0.03550421], action=0, reward=1.0, next_state=[-0.01626906 -0.21377396  0.00380873  0.25860102]\n",
      "[ episode 28 ][ timestamp 6 ] state=[-0.01626906 -0.21377396  0.00380873  0.25860102], action=0, reward=1.0, next_state=[-0.02054454 -0.40895007  0.00898075  0.55248283]\n",
      "[ episode 28 ][ timestamp 7 ] state=[-0.02054454 -0.40895007  0.00898075  0.55248283], action=1, reward=1.0, next_state=[-0.02872354 -0.21395539  0.02003041  0.26264291]\n",
      "[ episode 28 ][ timestamp 8 ] state=[-0.02872354 -0.21395539  0.02003041  0.26264291], action=0, reward=1.0, next_state=[-0.03300265 -0.40935745  0.02528327  0.56157574]\n",
      "[ episode 28 ][ timestamp 9 ] state=[-0.03300265 -0.40935745  0.02528327  0.56157574], action=1, reward=1.0, next_state=[-0.0411898  -0.21459927  0.03651478  0.27696433]\n",
      "[ episode 28 ][ timestamp 10 ] state=[-0.0411898  -0.21459927  0.03651478  0.27696433], action=1, reward=1.0, next_state=[-0.04548178 -0.02001677  0.04205407 -0.00398189]\n",
      "[ episode 28 ][ timestamp 11 ] state=[-0.04548178 -0.02001677  0.04205407 -0.00398189], action=1, reward=1.0, next_state=[-0.04588212  0.17447762  0.04197443 -0.28310529]\n",
      "[ episode 28 ][ timestamp 12 ] state=[-0.04588212  0.17447762  0.04197443 -0.28310529], action=0, reward=1.0, next_state=[-0.04239256 -0.0212171   0.03631233  0.02251511]\n",
      "[ episode 28 ][ timestamp 13 ] state=[-0.04239256 -0.0212171   0.03631233  0.02251511], action=0, reward=1.0, next_state=[-0.04281691 -0.21684048  0.03676263  0.32643022]\n",
      "[ episode 28 ][ timestamp 14 ] state=[-0.04281691 -0.21684048  0.03676263  0.32643022], action=1, reward=1.0, next_state=[-0.04715372 -0.0222607   0.04329123  0.04556353]\n",
      "[ episode 28 ][ timestamp 15 ] state=[-0.04715372 -0.0222607   0.04329123  0.04556353], action=0, reward=1.0, next_state=[-0.04759893 -0.21797583  0.0442025   0.35158481]\n",
      "[ episode 28 ][ timestamp 16 ] state=[-0.04759893 -0.21797583  0.0442025   0.35158481], action=0, reward=1.0, next_state=[-0.05195845 -0.41369759  0.0512342   0.657872  ]\n",
      "[ episode 28 ][ timestamp 17 ] state=[-0.05195845 -0.41369759  0.0512342   0.657872  ], action=1, reward=1.0, next_state=[-0.0602324  -0.2193248   0.06439164  0.38175166]\n",
      "[ episode 28 ][ timestamp 18 ] state=[-0.0602324  -0.2193248   0.06439164  0.38175166], action=0, reward=1.0, next_state=[-0.06461889 -0.41529915  0.07202667  0.69402203]\n",
      "[ episode 28 ][ timestamp 19 ] state=[-0.06461889 -0.41529915  0.07202667  0.69402203], action=0, reward=1.0, next_state=[-0.07292488 -0.61134239  0.08590711  1.00848199]\n",
      "[ episode 28 ][ timestamp 20 ] state=[-0.07292488 -0.61134239  0.08590711  1.00848199], action=0, reward=1.0, next_state=[-0.08515172 -0.80749953  0.10607675  1.32685825]\n",
      "[ episode 28 ][ timestamp 21 ] state=[-0.08515172 -0.80749953  0.10607675  1.32685825], action=1, reward=1.0, next_state=[-0.10130172 -0.61386446  0.13261392  1.06916636]\n",
      "[ episode 28 ][ timestamp 22 ] state=[-0.10130172 -0.61386446  0.13261392  1.06916636], action=1, reward=1.0, next_state=[-0.113579   -0.42072179  0.15399724  0.82087045]\n",
      "[ episode 28 ][ timestamp 23 ] state=[-0.113579   -0.42072179  0.15399724  0.82087045], action=1, reward=1.0, next_state=[-0.12199344 -0.22800457  0.17041465  0.58031205]\n",
      "[ episode 28 ][ timestamp 24 ] state=[-0.12199344 -0.22800457  0.17041465  0.58031205], action=1, reward=1.0, next_state=[-0.12655353 -0.0356287   0.18202089  0.34578799]\n",
      "[ episode 28 ][ timestamp 25 ] state=[-0.12655353 -0.0356287   0.18202089  0.34578799], action=0, reward=1.0, next_state=[-0.12726611 -0.23280994  0.18893665  0.68989279]\n",
      "[ episode 28 ][ timestamp 26 ] state=[-0.12726611 -0.23280994  0.18893665  0.68989279], action=1, reward=1.0, next_state=[-0.1319223  -0.04074212  0.20273451  0.46213546]\n",
      "[ episode 28 ][ timestamp 27 ] state=[-0.1319223  -0.04074212  0.20273451  0.46213546], action=1, reward=-1.0, next_state=[-0.13273715  0.15102443  0.21197722  0.23957329]\n",
      "[ Ended! ] Episode 28: Exploration_rate=0.8734200960253871. Score=27.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 29 ] state=[-0.01387775  0.02180851  0.04635044 -0.0242206 ]\n",
      "[ episode 29 ][ timestamp 1 ] state=[-0.01387775  0.02180851  0.04635044 -0.0242206 ], action=0, reward=1.0, next_state=[-0.01344158 -0.17394644  0.04586603  0.28271863]\n",
      "[ episode 29 ][ timestamp 2 ] state=[-0.01344158 -0.17394644  0.04586603  0.28271863], action=0, reward=1.0, next_state=[-0.01692051 -0.36969157  0.0515204   0.58950742]\n",
      "[ episode 29 ][ timestamp 3 ] state=[-0.01692051 -0.36969157  0.0515204   0.58950742], action=0, reward=1.0, next_state=[-0.02431434 -0.56549564  0.06331055  0.89796411]\n",
      "[ episode 29 ][ timestamp 4 ] state=[-0.02431434 -0.56549564  0.06331055  0.89796411], action=1, reward=1.0, next_state=[-0.03562426 -0.37128634  0.08126983  0.62583466]\n",
      "[ episode 29 ][ timestamp 5 ] state=[-0.03562426 -0.37128634  0.08126983  0.62583466], action=1, reward=1.0, next_state=[-0.04304998 -0.17738727  0.09378653  0.35981307]\n",
      "[ episode 29 ][ timestamp 6 ] state=[-0.04304998 -0.17738727  0.09378653  0.35981307], action=1, reward=1.0, next_state=[-0.04659773  0.01628506  0.10098279  0.09811411]\n",
      "[ episode 29 ][ timestamp 7 ] state=[-0.04659773  0.01628506  0.10098279  0.09811411], action=1, reward=1.0, next_state=[-0.04627203  0.20982562  0.10294507 -0.16107926]\n",
      "[ episode 29 ][ timestamp 8 ] state=[-0.04627203  0.20982562  0.10294507 -0.16107926], action=1, reward=1.0, next_state=[-0.04207552  0.40333467  0.09972348 -0.41959371]\n",
      "[ episode 29 ][ timestamp 9 ] state=[-0.04207552  0.40333467  0.09972348 -0.41959371], action=1, reward=1.0, next_state=[-0.03400882  0.59691261  0.09133161 -0.67924787]\n",
      "[ episode 29 ][ timestamp 10 ] state=[-0.03400882  0.59691261  0.09133161 -0.67924787], action=0, reward=1.0, next_state=[-0.02207057  0.40064862  0.07774665 -0.35926469]\n",
      "[ episode 29 ][ timestamp 11 ] state=[-0.02207057  0.40064862  0.07774665 -0.35926469], action=1, reward=1.0, next_state=[-0.0140576   0.59458423  0.07056136 -0.62645487]\n",
      "[ episode 29 ][ timestamp 12 ] state=[-0.0140576   0.59458423  0.07056136 -0.62645487], action=1, reward=1.0, next_state=[-0.00216591  0.788654    0.05803226 -0.8961073 ]\n",
      "[ episode 29 ][ timestamp 13 ] state=[-0.00216591  0.788654    0.05803226 -0.8961073 ], action=0, reward=1.0, next_state=[ 0.01360717  0.59279529  0.04011012 -0.58576189]\n",
      "[ episode 29 ][ timestamp 14 ] state=[ 0.01360717  0.59279529  0.04011012 -0.58576189], action=0, reward=1.0, next_state=[ 0.02546307  0.39713516  0.02839488 -0.28071854]\n",
      "[ episode 29 ][ timestamp 15 ] state=[ 0.02546307  0.39713516  0.02839488 -0.28071854], action=0, reward=1.0, next_state=[0.03340578 0.20161992 0.02278051 0.02078307]\n",
      "[ episode 29 ][ timestamp 16 ] state=[0.03340578 0.20161992 0.02278051 0.02078307], action=0, reward=1.0, next_state=[0.03743817 0.00617881 0.02319617 0.32056557]\n",
      "[ episode 29 ][ timestamp 17 ] state=[0.03743817 0.00617881 0.02319617 0.32056557], action=0, reward=1.0, next_state=[ 0.03756175 -0.18926567  0.02960748  0.62047248]\n",
      "[ episode 29 ][ timestamp 18 ] state=[ 0.03756175 -0.18926567  0.02960748  0.62047248], action=1, reward=1.0, next_state=[0.03377644 0.00543055 0.04201693 0.33725948]\n",
      "[ episode 29 ][ timestamp 19 ] state=[0.03377644 0.00543055 0.04201693 0.33725948], action=0, reward=1.0, next_state=[ 0.03388505 -0.19026336  0.04876212  0.6428906 ]\n",
      "[ episode 29 ][ timestamp 20 ] state=[ 0.03388505 -0.19026336  0.04876212  0.6428906 ], action=0, reward=1.0, next_state=[ 0.03007978 -0.38602983  0.06161993  0.95052165]\n",
      "[ episode 29 ][ timestamp 21 ] state=[ 0.03007978 -0.38602983  0.06161993  0.95052165], action=0, reward=1.0, next_state=[ 0.02235918 -0.58192463  0.08063036  1.26191096]\n",
      "[ episode 29 ][ timestamp 22 ] state=[ 0.02235918 -0.58192463  0.08063036  1.26191096], action=0, reward=1.0, next_state=[ 0.01072069 -0.77797974  0.10586858  1.57871784]\n",
      "[ episode 29 ][ timestamp 23 ] state=[ 0.01072069 -0.77797974  0.10586858  1.57871784], action=1, reward=1.0, next_state=[-0.0048389  -0.58426611  0.13744294  1.32084151]\n",
      "[ episode 29 ][ timestamp 24 ] state=[-0.0048389  -0.58426611  0.13744294  1.32084151], action=1, reward=1.0, next_state=[-0.01652423 -0.39112265  0.16385977  1.07413958]\n",
      "[ episode 29 ][ timestamp 25 ] state=[-0.01652423 -0.39112265  0.16385977  1.07413958], action=1, reward=1.0, next_state=[-0.02434668 -0.19850029  0.18534256  0.83703578]\n",
      "[ episode 29 ][ timestamp 26 ] state=[-0.02434668 -0.19850029  0.18534256  0.83703578], action=1, reward=1.0, next_state=[-0.02831668 -0.00632753  0.20208328  0.60789287]\n",
      "[ episode 29 ][ timestamp 27 ] state=[-0.02831668 -0.00632753  0.20208328  0.60789287], action=1, reward=-1.0, next_state=[-0.02844324  0.18548149  0.21424114  0.38504309]\n",
      "[ Ended! ] Episode 29: Exploration_rate=0.8690529955452602. Score=27.\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ] state=[-0.01245591 -0.00097128 -0.04308935  0.01573372]\n",
      "[ episode 30 ][ timestamp 1 ] state=[-0.01245591 -0.00097128 -0.04308935  0.01573372], action=0, reward=1.0, next_state=[-0.01247534 -0.19544963 -0.04277467  0.29451614]\n",
      "[ episode 30 ][ timestamp 2 ] state=[-0.01247534 -0.19544963 -0.04277467  0.29451614], action=1, reward=1.0, next_state=[-0.01638433  0.00025522 -0.03688435 -0.01134454]\n",
      "[ episode 30 ][ timestamp 3 ] state=[-0.01638433  0.00025522 -0.03688435 -0.01134454], action=0, reward=1.0, next_state=[-0.01637923 -0.19431888 -0.03711124  0.26947656]\n",
      "[ episode 30 ][ timestamp 4 ] state=[-0.01637923 -0.19431888 -0.03711124  0.26947656], action=0, reward=1.0, next_state=[-0.0202656  -0.38889212 -0.03172171  0.55022726]\n",
      "[ episode 30 ][ timestamp 5 ] state=[-0.0202656  -0.38889212 -0.03172171  0.55022726], action=0, reward=1.0, next_state=[-0.02804345 -0.58355448 -0.02071716  0.83274928]\n",
      "[ episode 30 ][ timestamp 6 ] state=[-0.02804345 -0.58355448 -0.02071716  0.83274928], action=0, reward=1.0, next_state=[-0.03971454 -0.7783873  -0.00406218  1.11884539]\n",
      "[ episode 30 ][ timestamp 7 ] state=[-0.03971454 -0.7783873  -0.00406218  1.11884539], action=1, reward=1.0, next_state=[-0.05528228 -0.58321229  0.01831473  0.82489101]\n",
      "[ episode 30 ][ timestamp 8 ] state=[-0.05528228 -0.58321229  0.01831473  0.82489101], action=0, reward=1.0, next_state=[-0.06694653 -0.77857989  0.03481255  1.12327749]\n",
      "[ episode 30 ][ timestamp 9 ] state=[-0.06694653 -0.77857989  0.03481255  1.12327749], action=1, reward=1.0, next_state=[-0.08251812 -0.58393121  0.0572781   0.8417142 ]\n",
      "[ episode 30 ][ timestamp 10 ] state=[-0.08251812 -0.58393121  0.0572781   0.8417142 ], action=1, reward=1.0, next_state=[-0.09419675 -0.38963593  0.07411238  0.56757979]\n",
      "[ episode 30 ][ timestamp 11 ] state=[-0.09419675 -0.38963593  0.07411238  0.56757979], action=0, reward=1.0, next_state=[-0.10198947 -0.58571495  0.08546398  0.88266003]\n",
      "[ episode 30 ][ timestamp 12 ] state=[-0.10198947 -0.58571495  0.08546398  0.88266003], action=0, reward=1.0, next_state=[-0.11370377 -0.78188715  0.10311718  1.20094018]\n",
      "[ episode 30 ][ timestamp 13 ] state=[-0.11370377 -0.78188715  0.10311718  1.20094018], action=1, reward=1.0, next_state=[-0.12934151 -0.58823888  0.12713598  0.94227348]\n",
      "[ episode 30 ][ timestamp 14 ] state=[-0.12934151 -0.58823888  0.12713598  0.94227348], action=1, reward=1.0, next_state=[-0.14110629 -0.3950381   0.14598145  0.69208863]\n",
      "[ episode 30 ][ timestamp 15 ] state=[-0.14110629 -0.3950381   0.14598145  0.69208863], action=0, reward=1.0, next_state=[-0.14900705 -0.59185133  0.15982323  1.02693468]\n",
      "[ episode 30 ][ timestamp 16 ] state=[-0.14900705 -0.59185133  0.15982323  1.02693468], action=1, reward=1.0, next_state=[-0.16084408 -0.39917596  0.18036192  0.78839322]\n",
      "[ episode 30 ][ timestamp 17 ] state=[-0.16084408 -0.39917596  0.18036192  0.78839322], action=0, reward=1.0, next_state=[-0.1688276  -0.59625602  0.19612978  1.13195738]\n",
      "[ episode 30 ][ timestamp 18 ] state=[-0.1688276  -0.59625602  0.19612978  1.13195738], action=1, reward=-1.0, next_state=[-0.18075272 -0.40416544  0.21876893  0.90663879]\n",
      "[ Ended! ] Episode 30: Exploration_rate=0.8647077305675338. Score=18.\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ] state=[-0.02638457 -0.04115002  0.0433485   0.00769043]\n",
      "[ episode 31 ][ timestamp 1 ] state=[-0.02638457 -0.04115002  0.0433485   0.00769043], action=0, reward=1.0, next_state=[-0.02720757 -0.23686597  0.04350231  0.31372905]\n",
      "[ episode 31 ][ timestamp 2 ] state=[-0.02720757 -0.23686597  0.04350231  0.31372905], action=1, reward=1.0, next_state=[-0.03194489 -0.04238986  0.04977689  0.03507651]\n",
      "[ episode 31 ][ timestamp 3 ] state=[-0.03194489 -0.04238986  0.04977689  0.03507651], action=0, reward=1.0, next_state=[-0.03279268 -0.23818898  0.05047842  0.34303977]\n",
      "[ episode 31 ][ timestamp 4 ] state=[-0.03279268 -0.23818898  0.05047842  0.34303977], action=0, reward=1.0, next_state=[-0.03755646 -0.43399137  0.05733922  0.6512036 ]\n",
      "[ episode 31 ][ timestamp 5 ] state=[-0.03755646 -0.43399137  0.05733922  0.6512036 ], action=0, reward=1.0, next_state=[-0.04623629 -0.62986307  0.07036329  0.96137679]\n",
      "[ episode 31 ][ timestamp 6 ] state=[-0.04623629 -0.62986307  0.07036329  0.96137679], action=0, reward=1.0, next_state=[-0.05883355 -0.82585652  0.08959083  1.27530924]\n",
      "[ episode 31 ][ timestamp 7 ] state=[-0.05883355 -0.82585652  0.08959083  1.27530924], action=1, reward=1.0, next_state=[-0.07535068 -0.63198416  0.11509701  1.01197149]\n",
      "[ episode 31 ][ timestamp 8 ] state=[-0.07535068 -0.63198416  0.11509701  1.01197149], action=0, reward=1.0, next_state=[-0.08799037 -0.82843791  0.13533644  1.33846625]\n",
      "[ episode 31 ][ timestamp 9 ] state=[-0.08799037 -0.82843791  0.13533644  1.33846625], action=1, reward=1.0, next_state=[-0.10455912 -0.63525477  0.16210577  1.09100882]\n",
      "[ episode 31 ][ timestamp 10 ] state=[-0.10455912 -0.63525477  0.16210577  1.09100882], action=0, reward=1.0, next_state=[-0.11726422 -0.83209843  0.18392594  1.42985389]\n",
      "[ episode 31 ][ timestamp 11 ] state=[-0.11726422 -0.83209843  0.18392594  1.42985389], action=0, reward=-1.0, next_state=[-0.13390619 -1.02895246  0.21252302  1.77392438]\n",
      "[ Ended! ] Episode 31: Exploration_rate=0.8603841919146962. Score=11.\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ] state=[-0.00978935 -0.03555889 -0.01909043 -0.03236013]\n",
      "[ episode 32 ][ timestamp 1 ] state=[-0.00978935 -0.03555889 -0.01909043 -0.03236013], action=1, reward=1.0, next_state=[-0.01050053  0.15983155 -0.01973763 -0.33100463]\n",
      "[ episode 32 ][ timestamp 2 ] state=[-0.01050053  0.15983155 -0.01973763 -0.33100463], action=1, reward=1.0, next_state=[-0.0073039   0.35522882 -0.02635772 -0.62984592]\n",
      "[ episode 32 ][ timestamp 3 ] state=[-0.0073039   0.35522882 -0.02635772 -0.62984592], action=0, reward=1.0, next_state=[-1.99320988e-04  1.60484397e-01 -3.89546406e-02 -3.45579032e-01]\n",
      "[ episode 32 ][ timestamp 4 ] state=[-1.99320988e-04  1.60484397e-01 -3.89546406e-02 -3.45579032e-01], action=1, reward=1.0, next_state=[ 0.00301037  0.3561382  -0.04586622 -0.65028686]\n",
      "[ episode 32 ][ timestamp 5 ] state=[ 0.00301037  0.3561382  -0.04586622 -0.65028686], action=0, reward=1.0, next_state=[ 0.01013313  0.16168411 -0.05887196 -0.37239242]\n",
      "[ episode 32 ][ timestamp 6 ] state=[ 0.01013313  0.16168411 -0.05887196 -0.37239242], action=1, reward=1.0, next_state=[ 0.01336681  0.35759084 -0.06631981 -0.68304177]\n",
      "[ episode 32 ][ timestamp 7 ] state=[ 0.01336681  0.35759084 -0.06631981 -0.68304177], action=0, reward=1.0, next_state=[ 0.02051863  0.16344945 -0.07998064 -0.4119536 ]\n",
      "[ episode 32 ][ timestamp 8 ] state=[ 0.02051863  0.16344945 -0.07998064 -0.4119536 ], action=0, reward=1.0, next_state=[ 0.02378762 -0.03045301 -0.08821971 -0.14551895]\n",
      "[ episode 32 ][ timestamp 9 ] state=[ 0.02378762 -0.03045301 -0.08821971 -0.14551895], action=0, reward=1.0, next_state=[ 0.02317856 -0.22420809 -0.09113009  0.11808048]\n",
      "[ episode 32 ][ timestamp 10 ] state=[ 0.02317856 -0.22420809 -0.09113009  0.11808048], action=0, reward=1.0, next_state=[ 0.0186944  -0.41791418 -0.08876848  0.38067877]\n",
      "[ episode 32 ][ timestamp 11 ] state=[ 0.0186944  -0.41791418 -0.08876848  0.38067877], action=0, reward=1.0, next_state=[ 0.01033611 -0.61167085 -0.08115491  0.64410577]\n",
      "[ episode 32 ][ timestamp 12 ] state=[ 0.01033611 -0.61167085 -0.08115491  0.64410577], action=1, reward=1.0, next_state=[-0.0018973  -0.41551726 -0.06827279  0.32701041]\n",
      "[ episode 32 ][ timestamp 13 ] state=[-0.0018973  -0.41551726 -0.06827279  0.32701041], action=1, reward=1.0, next_state=[-0.01020765 -0.21949308 -0.06173258  0.01360255]\n",
      "[ episode 32 ][ timestamp 14 ] state=[-0.01020765 -0.21949308 -0.06173258  0.01360255], action=1, reward=1.0, next_state=[-0.01459751 -0.02354263 -0.06146053 -0.29790111]\n",
      "[ episode 32 ][ timestamp 15 ] state=[-0.01459751 -0.02354263 -0.06146053 -0.29790111], action=1, reward=1.0, next_state=[-0.01506836  0.17239913 -0.06741856 -0.60931684]\n",
      "[ episode 32 ][ timestamp 16 ] state=[-0.01506836  0.17239913 -0.06741856 -0.60931684], action=0, reward=1.0, next_state=[-0.01162038 -0.0217188  -0.07960489 -0.33860746]\n",
      "[ episode 32 ][ timestamp 17 ] state=[-0.01162038 -0.0217188  -0.07960489 -0.33860746], action=0, reward=1.0, next_state=[-0.01205476 -0.2156231  -0.08637704 -0.07205123]\n",
      "[ episode 32 ][ timestamp 18 ] state=[-0.01205476 -0.2156231  -0.08637704 -0.07205123], action=0, reward=1.0, next_state=[-0.01636722 -0.40940733 -0.08781807  0.19217815]\n",
      "[ episode 32 ][ timestamp 19 ] state=[-0.01636722 -0.40940733 -0.08781807  0.19217815], action=1, reward=1.0, next_state=[-0.02455537 -0.21314603 -0.0839745  -0.12686469]\n",
      "[ episode 32 ][ timestamp 20 ] state=[-0.02455537 -0.21314603 -0.0839745  -0.12686469], action=0, reward=1.0, next_state=[-0.02881829 -0.40697086 -0.0865118   0.13818856]\n",
      "[ episode 32 ][ timestamp 21 ] state=[-0.02881829 -0.40697086 -0.0865118   0.13818856], action=1, reward=1.0, next_state=[-0.0369577  -0.2107232  -0.08374803 -0.1804848 ]\n",
      "[ episode 32 ][ timestamp 22 ] state=[-0.0369577  -0.2107232  -0.08374803 -0.1804848 ], action=1, reward=1.0, next_state=[-0.04117217 -0.01450891 -0.08735772 -0.49836784]\n",
      "[ episode 32 ][ timestamp 23 ] state=[-0.04117217 -0.01450891 -0.08735772 -0.49836784], action=1, reward=1.0, next_state=[-0.04146235  0.18172908 -0.09732508 -0.81725287]\n",
      "[ episode 32 ][ timestamp 24 ] state=[-0.04146235  0.18172908 -0.09732508 -0.81725287], action=0, reward=1.0, next_state=[-0.03782776 -0.0119354  -0.11367014 -0.55669931]\n",
      "[ episode 32 ][ timestamp 25 ] state=[-0.03782776 -0.0119354  -0.11367014 -0.55669931], action=0, reward=1.0, next_state=[-0.03806647 -0.20529344 -0.12480412 -0.30188111]\n",
      "[ episode 32 ][ timestamp 26 ] state=[-0.03806647 -0.20529344 -0.12480412 -0.30188111], action=0, reward=1.0, next_state=[-0.04217234 -0.39843613 -0.13084174 -0.05101768]\n",
      "[ episode 32 ][ timestamp 27 ] state=[-0.04217234 -0.39843613 -0.13084174 -0.05101768], action=1, reward=1.0, next_state=[-0.05014106 -0.20170442 -0.1318621  -0.38195068]\n",
      "[ episode 32 ][ timestamp 28 ] state=[-0.05014106 -0.20170442 -0.1318621  -0.38195068], action=0, reward=1.0, next_state=[-0.05417515 -0.39473167 -0.13950111 -0.13357858]\n",
      "[ episode 32 ][ timestamp 29 ] state=[-0.05417515 -0.39473167 -0.13950111 -0.13357858], action=0, reward=1.0, next_state=[-0.06206978 -0.58760838 -0.14217268  0.11204549]\n",
      "[ episode 32 ][ timestamp 30 ] state=[-0.06206978 -0.58760838 -0.14217268  0.11204549], action=1, reward=1.0, next_state=[-0.07382195 -0.39076574 -0.13993177 -0.22189749]\n",
      "[ episode 32 ][ timestamp 31 ] state=[-0.07382195 -0.39076574 -0.13993177 -0.22189749], action=1, reward=1.0, next_state=[-0.08163727 -0.19394991 -0.14436972 -0.55524139]\n",
      "[ episode 32 ][ timestamp 32 ] state=[-0.08163727 -0.19394991 -0.14436972 -0.55524139], action=0, reward=1.0, next_state=[-0.08551627 -0.38678127 -0.15547455 -0.31130087]\n",
      "[ episode 32 ][ timestamp 33 ] state=[-0.08551627 -0.38678127 -0.15547455 -0.31130087], action=0, reward=1.0, next_state=[-0.09325189 -0.57938596 -0.16170057 -0.07140417]\n",
      "[ episode 32 ][ timestamp 34 ] state=[-0.09325189 -0.57938596 -0.16170057 -0.07140417], action=0, reward=1.0, next_state=[-0.10483961 -0.77186485 -0.16312865  0.16621475]\n",
      "[ episode 32 ][ timestamp 35 ] state=[-0.10483961 -0.77186485 -0.16312865  0.16621475], action=0, reward=1.0, next_state=[-0.12027691 -0.96432153 -0.15980436  0.40331979]\n",
      "[ episode 32 ][ timestamp 36 ] state=[-0.12027691 -0.96432153 -0.15980436  0.40331979], action=0, reward=1.0, next_state=[-0.13956334 -1.156859   -0.15173796  0.64166339]\n",
      "[ episode 32 ][ timestamp 37 ] state=[-0.13956334 -1.156859   -0.15173796  0.64166339], action=1, reward=1.0, next_state=[-0.16270052 -0.95998401 -0.13890469  0.30530412]\n",
      "[ episode 32 ][ timestamp 38 ] state=[-0.16270052 -0.95998401 -0.13890469  0.30530412], action=0, reward=1.0, next_state=[-0.1819002  -1.1528814  -0.13279861  0.55115651]\n",
      "[ episode 32 ][ timestamp 39 ] state=[-0.1819002  -1.1528814  -0.13279861  0.55115651], action=0, reward=1.0, next_state=[-0.20495783 -1.34591275 -0.12177548  0.799226  ]\n",
      "[ episode 32 ][ timestamp 40 ] state=[-0.20495783 -1.34591275 -0.12177548  0.799226  ], action=0, reward=1.0, next_state=[-0.23187608 -1.53917251 -0.10579096  1.0512553 ]\n",
      "[ episode 32 ][ timestamp 41 ] state=[-0.23187608 -1.53917251 -0.10579096  1.0512553 ], action=0, reward=1.0, next_state=[-0.26265953 -1.73274449 -0.08476585  1.30894541]\n",
      "[ episode 32 ][ timestamp 42 ] state=[-0.26265953 -1.73274449 -0.08476585  1.30894541], action=1, reward=1.0, next_state=[-0.29731442 -1.53665705 -0.05858695  0.990979  ]\n",
      "[ episode 32 ][ timestamp 43 ] state=[-0.29731442 -1.53665705 -0.05858695  0.990979  ], action=0, reward=1.0, next_state=[-0.32804756 -1.73094806 -0.03876737  1.26470078]\n",
      "[ episode 32 ][ timestamp 44 ] state=[-0.32804756 -1.73094806 -0.03876737  1.26470078], action=0, reward=1.0, next_state=[-0.36266652 -1.92555367 -0.01347335  1.54499512]\n",
      "[ episode 32 ][ timestamp 45 ] state=[-0.36266652 -1.92555367 -0.01347335  1.54499512], action=0, reward=1.0, next_state=[-0.4011776  -2.12051121  0.01742655  1.83344383]\n",
      "[ episode 32 ][ timestamp 46 ] state=[-0.4011776  -2.12051121  0.01742655  1.83344383], action=0, reward=1.0, next_state=[-0.44358782 -2.31582154  0.05409543  2.131488  ]\n",
      "[ episode 32 ][ timestamp 47 ] state=[-0.44358782 -2.31582154  0.05409543  2.131488  ], action=1, reward=1.0, next_state=[-0.48990425 -2.12127585  0.09672519  1.85599263]\n",
      "[ episode 32 ][ timestamp 48 ] state=[-0.48990425 -2.12127585  0.09672519  1.85599263], action=0, reward=1.0, next_state=[-0.53232977 -2.317318    0.13384504  2.17707422]\n",
      "[ episode 32 ][ timestamp 49 ] state=[-0.53232977 -2.317318    0.13384504  2.17707422], action=1, reward=1.0, next_state=[-0.57867613 -2.12372793  0.17738653  1.92851934]\n",
      "[ episode 32 ][ timestamp 50 ] state=[-0.57867613 -2.12372793  0.17738653  1.92851934], action=0, reward=-1.0, next_state=[-0.62115069 -2.32025281  0.21595691  2.27055949]\n",
      "[ Ended! ] Episode 32: Exploration_rate=0.8560822709551227. Score=50.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 33 ] state=[-0.03803954  0.01609746 -0.00118585  0.0388231 ]\n",
      "[ episode 33 ][ timestamp 1 ] state=[-0.03803954  0.01609746 -0.00118585  0.0388231 ], action=0, reward=1.0, next_state=[-0.03771759 -0.17900746 -0.00040939  0.33113164]\n",
      "[ episode 33 ][ timestamp 2 ] state=[-0.03771759 -0.17900746 -0.00040939  0.33113164], action=1, reward=1.0, next_state=[-0.04129774  0.01612031  0.00621324  0.03831964]\n",
      "[ episode 33 ][ timestamp 3 ] state=[-0.04129774  0.01612031  0.00621324  0.03831964], action=1, reward=1.0, next_state=[-0.04097533  0.21115262  0.00697963 -0.25239649]\n",
      "[ episode 33 ][ timestamp 4 ] state=[-0.04097533  0.21115262  0.00697963 -0.25239649], action=0, reward=1.0, next_state=[-0.03675228  0.0159317   0.0019317   0.04247975]\n",
      "[ episode 33 ][ timestamp 5 ] state=[-0.03675228  0.0159317   0.0019317   0.04247975], action=1, reward=1.0, next_state=[-0.03643365  0.2110259   0.0027813  -0.24959308]\n",
      "[ episode 33 ][ timestamp 6 ] state=[-0.03643365  0.2110259   0.0027813  -0.24959308], action=1, reward=1.0, next_state=[-0.03221313  0.40610802 -0.00221056 -0.54139743]\n",
      "[ episode 33 ][ timestamp 7 ] state=[-0.03221313  0.40610802 -0.00221056 -0.54139743], action=1, reward=1.0, next_state=[-0.02409097  0.60126097 -0.01303851 -0.83477605]\n",
      "[ episode 33 ][ timestamp 8 ] state=[-0.02409097  0.60126097 -0.01303851 -0.83477605], action=0, reward=1.0, next_state=[-0.01206575  0.40631955 -0.02973403 -0.54622199]\n",
      "[ episode 33 ][ timestamp 9 ] state=[-0.01206575  0.40631955 -0.02973403 -0.54622199], action=0, reward=1.0, next_state=[-0.00393936  0.21162772 -0.04065847 -0.26305384]\n",
      "[ episode 33 ][ timestamp 10 ] state=[-0.00393936  0.21162772 -0.04065847 -0.26305384], action=1, reward=1.0, next_state=[ 2.93196340e-04  4.07305727e-01 -4.59195496e-02 -5.68278581e-01]\n",
      "[ episode 33 ][ timestamp 11 ] state=[ 2.93196340e-04  4.07305727e-01 -4.59195496e-02 -5.68278581e-01], action=0, reward=1.0, next_state=[ 0.00843931  0.21285692 -0.05728512 -0.29040843]\n",
      "[ episode 33 ][ timestamp 12 ] state=[ 0.00843931  0.21285692 -0.05728512 -0.29040843], action=1, reward=1.0, next_state=[ 0.01269645  0.40874693 -0.06309329 -0.60059408]\n",
      "[ episode 33 ][ timestamp 13 ] state=[ 0.01269645  0.40874693 -0.06309329 -0.60059408], action=1, reward=1.0, next_state=[ 0.02087139  0.60469215 -0.07510517 -0.91246421]\n",
      "[ episode 33 ][ timestamp 14 ] state=[ 0.02087139  0.60469215 -0.07510517 -0.91246421], action=0, reward=1.0, next_state=[ 0.03296523  0.41066225 -0.09335446 -0.64430001]\n",
      "[ episode 33 ][ timestamp 15 ] state=[ 0.03296523  0.41066225 -0.09335446 -0.64430001], action=1, reward=1.0, next_state=[ 0.04117848  0.60695272 -0.10624046 -0.96485998]\n",
      "[ episode 33 ][ timestamp 16 ] state=[ 0.04117848  0.60695272 -0.10624046 -0.96485998], action=0, reward=1.0, next_state=[ 0.05331753  0.4134059  -0.12553766 -0.70735262]\n",
      "[ episode 33 ][ timestamp 17 ] state=[ 0.05331753  0.4134059  -0.12553766 -0.70735262], action=1, reward=1.0, next_state=[ 0.06158565  0.61002273 -0.13968471 -1.03676815]\n",
      "[ episode 33 ][ timestamp 18 ] state=[ 0.06158565  0.61002273 -0.13968471 -1.03676815], action=0, reward=1.0, next_state=[ 0.0737861   0.41700588 -0.16042007 -0.79099675]\n",
      "[ episode 33 ][ timestamp 19 ] state=[ 0.0737861   0.41700588 -0.16042007 -0.79099675], action=0, reward=1.0, next_state=[ 0.08212622  0.22440738 -0.17624001 -0.55276985]\n",
      "[ episode 33 ][ timestamp 20 ] state=[ 0.08212622  0.22440738 -0.17624001 -0.55276985], action=0, reward=1.0, next_state=[ 0.08661437  0.03214116 -0.1872954  -0.32038459]\n",
      "[ episode 33 ][ timestamp 21 ] state=[ 0.08661437  0.03214116 -0.1872954  -0.32038459], action=1, reward=1.0, next_state=[ 0.08725719  0.22936782 -0.19370309 -0.66579424]\n",
      "[ episode 33 ][ timestamp 22 ] state=[ 0.08725719  0.22936782 -0.19370309 -0.66579424], action=1, reward=1.0, next_state=[ 0.09184455  0.42658085 -0.20701898 -1.01267465]\n",
      "[ episode 33 ][ timestamp 23 ] state=[ 0.09184455  0.42658085 -0.20701898 -1.01267465], action=1, reward=-1.0, next_state=[ 0.10037616  0.62377242 -0.22727247 -1.36257613]\n",
      "[ Ended! ] Episode 33: Exploration_rate=0.851801859600347. Score=23.\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ] state=[ 0.00347535  0.01928308 -0.01471377  0.028973  ]\n",
      "[ episode 34 ][ timestamp 1 ] state=[ 0.00347535  0.01928308 -0.01471377  0.028973  ], action=0, reward=1.0, next_state=[ 0.00386101 -0.17562481 -0.01413431  0.31697749]\n",
      "[ episode 34 ][ timestamp 2 ] state=[ 0.00386101 -0.17562481 -0.01413431  0.31697749], action=1, reward=1.0, next_state=[ 0.00034851  0.01969558 -0.00779476  0.01987083]\n",
      "[ episode 34 ][ timestamp 3 ] state=[ 0.00034851  0.01969558 -0.00779476  0.01987083], action=0, reward=1.0, next_state=[ 0.00074242 -0.17531372 -0.00739734  0.31008426]\n",
      "[ episode 34 ][ timestamp 4 ] state=[ 0.00074242 -0.17531372 -0.00739734  0.31008426], action=0, reward=1.0, next_state=[-0.00276385 -0.3703295  -0.00119566  0.60042513]\n",
      "[ episode 34 ][ timestamp 5 ] state=[-0.00276385 -0.3703295  -0.00119566  0.60042513], action=1, reward=1.0, next_state=[-0.01017044 -0.17519085  0.01081285  0.30736583]\n",
      "[ episode 34 ][ timestamp 6 ] state=[-0.01017044 -0.17519085  0.01081285  0.30736583], action=1, reward=1.0, next_state=[-0.01367426  0.01977537  0.01696016  0.01811251]\n",
      "[ episode 34 ][ timestamp 7 ] state=[-0.01367426  0.01977537  0.01696016  0.01811251], action=1, reward=1.0, next_state=[-0.01327875  0.21465004  0.01732241 -0.2691714 ]\n",
      "[ episode 34 ][ timestamp 8 ] state=[-0.01327875  0.21465004  0.01732241 -0.2691714 ], action=0, reward=1.0, next_state=[-0.00898575  0.01928523  0.01193899  0.02892439]\n",
      "[ episode 34 ][ timestamp 9 ] state=[-0.00898575  0.01928523  0.01193899  0.02892439], action=1, reward=1.0, next_state=[-0.00860005  0.21423395  0.01251747 -0.25996787]\n",
      "[ episode 34 ][ timestamp 10 ] state=[-0.00860005  0.21423395  0.01251747 -0.25996787], action=1, reward=1.0, next_state=[-0.00431537  0.40917499  0.00731812 -0.54867649]\n",
      "[ episode 34 ][ timestamp 11 ] state=[-0.00431537  0.40917499  0.00731812 -0.54867649], action=0, reward=1.0, next_state=[ 0.00386813  0.213951   -0.00365541 -0.25369684]\n",
      "[ episode 34 ][ timestamp 12 ] state=[ 0.00386813  0.213951   -0.00365541 -0.25369684], action=1, reward=1.0, next_state=[ 0.00814715  0.40912496 -0.00872935 -0.54753051]\n",
      "[ episode 34 ][ timestamp 13 ] state=[ 0.00814715  0.40912496 -0.00872935 -0.54753051], action=0, reward=1.0, next_state=[ 0.01632965  0.21412673 -0.01967996 -0.2576107 ]\n",
      "[ episode 34 ][ timestamp 14 ] state=[ 0.01632965  0.21412673 -0.01967996 -0.2576107 ], action=1, reward=1.0, next_state=[ 0.02061219  0.40952403 -0.02483217 -0.55643544]\n",
      "[ episode 34 ][ timestamp 15 ] state=[ 0.02061219  0.40952403 -0.02483217 -0.55643544], action=1, reward=1.0, next_state=[ 0.02880267  0.60498565 -0.03596088 -0.85683738]\n",
      "[ episode 34 ][ timestamp 16 ] state=[ 0.02880267  0.60498565 -0.03596088 -0.85683738], action=0, reward=1.0, next_state=[ 0.04090238  0.41037164 -0.05309763 -0.57567532]\n",
      "[ episode 34 ][ timestamp 17 ] state=[ 0.04090238  0.41037164 -0.05309763 -0.57567532], action=1, reward=1.0, next_state=[ 0.04910981  0.60619614 -0.06461114 -0.88460146]\n",
      "[ episode 34 ][ timestamp 18 ] state=[ 0.04910981  0.60619614 -0.06461114 -0.88460146], action=1, reward=1.0, next_state=[ 0.06123374  0.80213307 -0.08230317 -1.19687607]\n",
      "[ episode 34 ][ timestamp 19 ] state=[ 0.06123374  0.80213307 -0.08230317 -1.19687607], action=0, reward=1.0, next_state=[ 0.0772764   0.60816717 -0.10624069 -0.9310819 ]\n",
      "[ episode 34 ][ timestamp 20 ] state=[ 0.0772764   0.60816717 -0.10624069 -0.9310819 ], action=1, reward=1.0, next_state=[ 0.08943974  0.80455007 -0.12486233 -1.2551714 ]\n",
      "[ episode 34 ][ timestamp 21 ] state=[ 0.08943974  0.80455007 -0.12486233 -1.2551714 ], action=1, reward=1.0, next_state=[ 0.10553074  1.00102978 -0.14996575 -1.58421073]\n",
      "[ episode 34 ][ timestamp 22 ] state=[ 0.10553074  1.00102978 -0.14996575 -1.58421073], action=0, reward=1.0, next_state=[ 0.12555134  0.80797595 -0.18164997 -1.34180504]\n",
      "[ episode 34 ][ timestamp 23 ] state=[ 0.12555134  0.80797595 -0.18164997 -1.34180504], action=0, reward=1.0, next_state=[ 0.14171086  0.61554439 -0.20848607 -1.1110187 ]\n",
      "[ episode 34 ][ timestamp 24 ] state=[ 0.14171086  0.61554439 -0.20848607 -1.1110187 ], action=1, reward=-1.0, next_state=[ 0.15402174  0.81270316 -0.23070644 -1.46120459]\n",
      "[ Ended! ] Episode 34: Exploration_rate=0.8475428503023453. Score=24.\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ] state=[0.04333278 0.04405373 0.03978121 0.00656529]\n",
      "[ episode 35 ][ timestamp 1 ] state=[0.04333278 0.04405373 0.03978121 0.00656529], action=1, reward=1.0, next_state=[ 0.04421385  0.23858325  0.03991252 -0.27330554]\n",
      "[ episode 35 ][ timestamp 2 ] state=[ 0.04421385  0.23858325  0.03991252 -0.27330554], action=1, reward=1.0, next_state=[ 0.04898551  0.43311365  0.03444641 -0.55313759]\n",
      "[ episode 35 ][ timestamp 3 ] state=[ 0.04898551  0.43311365  0.03444641 -0.55313759], action=0, reward=1.0, next_state=[ 0.05764779  0.23752533  0.02338366 -0.24980391]\n",
      "[ episode 35 ][ timestamp 4 ] state=[ 0.05764779  0.23752533  0.02338366 -0.24980391], action=0, reward=1.0, next_state=[0.06239829 0.04207739 0.01838758 0.05016203]\n",
      "[ episode 35 ][ timestamp 5 ] state=[0.06239829 0.04207739 0.01838758 0.05016203], action=0, reward=1.0, next_state=[ 0.06323984 -0.15330333  0.01939082  0.3485892 ]\n",
      "[ episode 35 ][ timestamp 6 ] state=[ 0.06323984 -0.15330333  0.01939082  0.3485892 ], action=1, reward=1.0, next_state=[0.06017378 0.04153754 0.0263626  0.06208339]\n",
      "[ episode 35 ][ timestamp 7 ] state=[0.06017378 0.04153754 0.0263626  0.06208339], action=1, reward=1.0, next_state=[ 0.06100453  0.23627178  0.02760427 -0.22216677]\n",
      "[ episode 35 ][ timestamp 8 ] state=[ 0.06100453  0.23627178  0.02760427 -0.22216677], action=1, reward=1.0, next_state=[ 0.06572996  0.43098853  0.02316094 -0.50601599]\n",
      "[ episode 35 ][ timestamp 9 ] state=[ 0.06572996  0.43098853  0.02316094 -0.50601599], action=0, reward=1.0, next_state=[ 0.07434973  0.23554799  0.01304062 -0.2061251 ]\n",
      "[ episode 35 ][ timestamp 10 ] state=[ 0.07434973  0.23554799  0.01304062 -0.2061251 ], action=0, reward=1.0, next_state=[0.07906069 0.04024201 0.00891811 0.0906428 ]\n",
      "[ episode 35 ][ timestamp 11 ] state=[0.07906069 0.04024201 0.00891811 0.0906428 ], action=1, reward=1.0, next_state=[ 0.07986553  0.235235    0.01073097 -0.19921318]\n",
      "[ episode 35 ][ timestamp 12 ] state=[ 0.07986553  0.235235    0.01073097 -0.19921318], action=0, reward=1.0, next_state=[0.08457023 0.03996122 0.00674671 0.09683547]\n",
      "[ episode 35 ][ timestamp 13 ] state=[0.08457023 0.03996122 0.00674671 0.09683547], action=0, reward=1.0, next_state=[ 0.08536946 -0.15525677  0.00868342  0.39163931]\n",
      "[ episode 35 ][ timestamp 14 ] state=[ 0.08536946 -0.15525677  0.00868342  0.39163931], action=1, reward=1.0, next_state=[0.08226432 0.03974088 0.0165162  0.10170676]\n",
      "[ episode 35 ][ timestamp 15 ] state=[0.08226432 0.03974088 0.0165162  0.10170676], action=1, reward=1.0, next_state=[ 0.08305914  0.23462228  0.01855034 -0.18571994]\n",
      "[ episode 35 ][ timestamp 16 ] state=[ 0.08305914  0.23462228  0.01855034 -0.18571994], action=0, reward=1.0, next_state=[0.08775158 0.0392399  0.01483594 0.11275671]\n",
      "[ episode 35 ][ timestamp 17 ] state=[0.08775158 0.0392399  0.01483594 0.11275671], action=0, reward=1.0, next_state=[ 0.08853638 -0.15609146  0.01709107  0.41008311]\n",
      "[ episode 35 ][ timestamp 18 ] state=[ 0.08853638 -0.15609146  0.01709107  0.41008311], action=0, reward=1.0, next_state=[ 0.08541455 -0.3514515   0.02529274  0.70810489]\n",
      "[ episode 35 ][ timestamp 19 ] state=[ 0.08541455 -0.3514515   0.02529274  0.70810489], action=1, reward=1.0, next_state=[ 0.07838552 -0.15668887  0.03945483  0.42348966]\n",
      "[ episode 35 ][ timestamp 20 ] state=[ 0.07838552 -0.15668887  0.03945483  0.42348966], action=1, reward=1.0, next_state=[0.07525175 0.03785258 0.04792463 0.1435013 ]\n",
      "[ episode 35 ][ timestamp 21 ] state=[0.07525175 0.03785258 0.04792463 0.1435013 ], action=1, reward=1.0, next_state=[ 0.0760088   0.23225659  0.05079465 -0.13368546]\n",
      "[ episode 35 ][ timestamp 22 ] state=[ 0.0760088   0.23225659  0.05079465 -0.13368546], action=1, reward=1.0, next_state=[ 0.08065393  0.42661555  0.04812094 -0.40992067]\n",
      "[ episode 35 ][ timestamp 23 ] state=[ 0.08065393  0.42661555  0.04812094 -0.40992067], action=1, reward=1.0, next_state=[ 0.08918624  0.62102341  0.03992253 -0.6870528 ]\n",
      "[ episode 35 ][ timestamp 24 ] state=[ 0.08918624  0.62102341  0.03992253 -0.6870528 ], action=0, reward=1.0, next_state=[ 0.10160671  0.42537071  0.02618147 -0.38207348]\n",
      "[ episode 35 ][ timestamp 25 ] state=[ 0.10160671  0.42537071  0.02618147 -0.38207348], action=1, reward=1.0, next_state=[ 0.11011412  0.62011131  0.01854    -0.66638781]\n",
      "[ episode 35 ][ timestamp 26 ] state=[ 0.11011412  0.62011131  0.01854    -0.66638781], action=0, reward=1.0, next_state=[ 0.12251635  0.42473648  0.00521225 -0.36792547]\n",
      "[ episode 35 ][ timestamp 27 ] state=[ 0.12251635  0.42473648  0.00521225 -0.36792547], action=0, reward=1.0, next_state=[ 0.13101108  0.22954085 -0.00214626 -0.07360362]\n",
      "[ episode 35 ][ timestamp 28 ] state=[ 0.13101108  0.22954085 -0.00214626 -0.07360362], action=0, reward=1.0, next_state=[ 0.1356019   0.03444974 -0.00361833  0.21840138]\n",
      "[ episode 35 ][ timestamp 29 ] state=[ 0.1356019   0.03444974 -0.00361833  0.21840138], action=1, reward=1.0, next_state=[ 0.13629089  0.22962323  0.00074969 -0.07542072]\n",
      "[ episode 35 ][ timestamp 30 ] state=[ 0.13629089  0.22962323  0.00074969 -0.07542072], action=0, reward=1.0, next_state=[ 0.14088336  0.03449053 -0.00075872  0.21749865]\n",
      "[ episode 35 ][ timestamp 31 ] state=[ 0.14088336  0.03449053 -0.00075872  0.21749865], action=1, reward=1.0, next_state=[ 0.14157317  0.22962332  0.00359125 -0.07542352]\n",
      "[ episode 35 ][ timestamp 32 ] state=[ 0.14157317  0.22962332  0.00359125 -0.07542352], action=0, reward=1.0, next_state=[0.14616563 0.03445007 0.00208278 0.2183903 ]\n",
      "[ episode 35 ][ timestamp 33 ] state=[0.14616563 0.03445007 0.00208278 0.2183903 ], action=1, reward=1.0, next_state=[ 0.14685463  0.22954219  0.00645059 -0.07363491]\n",
      "[ episode 35 ][ timestamp 34 ] state=[ 0.14685463  0.22954219  0.00645059 -0.07363491], action=0, reward=1.0, next_state=[0.15144548 0.03432836 0.00497789 0.22107621]\n",
      "[ episode 35 ][ timestamp 35 ] state=[0.15144548 0.03432836 0.00497789 0.22107621], action=1, reward=1.0, next_state=[ 0.15213204  0.2293788   0.00939941 -0.07003234]\n",
      "[ episode 35 ][ timestamp 36 ] state=[ 0.15213204  0.2293788   0.00939941 -0.07003234], action=0, reward=1.0, next_state=[0.15671962 0.03412337 0.00799877 0.22560126]\n",
      "[ episode 35 ][ timestamp 37 ] state=[0.15671962 0.03412337 0.00799877 0.22560126], action=1, reward=1.0, next_state=[ 0.15740209  0.22913009  0.01251079 -0.06454786]\n",
      "[ episode 35 ][ timestamp 38 ] state=[ 0.15740209  0.22913009  0.01251079 -0.06454786], action=1, reward=1.0, next_state=[ 0.16198469  0.42407046  0.01121984 -0.35325744]\n",
      "[ episode 35 ][ timestamp 39 ] state=[ 0.16198469  0.42407046  0.01121984 -0.35325744], action=0, reward=1.0, next_state=[ 0.1704661   0.22879078  0.00415469 -0.0570578 ]\n",
      "[ episode 35 ][ timestamp 40 ] state=[ 0.1704661   0.22879078  0.00415469 -0.0570578 ], action=1, reward=1.0, next_state=[ 0.17504191  0.42385291  0.00301353 -0.348427  ]\n",
      "[ episode 35 ][ timestamp 41 ] state=[ 0.17504191  0.42385291  0.00301353 -0.348427  ], action=1, reward=1.0, next_state=[ 0.18351897  0.61893187 -0.00395501 -0.64015813]\n",
      "[ episode 35 ][ timestamp 42 ] state=[ 0.18351897  0.61893187 -0.00395501 -0.64015813], action=1, reward=1.0, next_state=[ 0.19589761  0.81410874 -0.01675817 -0.93408391]\n",
      "[ episode 35 ][ timestamp 43 ] state=[ 0.19589761  0.81410874 -0.01675817 -0.93408391], action=0, reward=1.0, next_state=[ 0.21217979  0.61921682 -0.03543985 -0.64671375]\n",
      "[ episode 35 ][ timestamp 44 ] state=[ 0.21217979  0.61921682 -0.03543985 -0.64671375], action=0, reward=1.0, next_state=[ 0.22456412  0.42460612 -0.04837413 -0.36539814]\n",
      "[ episode 35 ][ timestamp 45 ] state=[ 0.22456412  0.42460612 -0.04837413 -0.36539814], action=1, reward=1.0, next_state=[ 0.23305624  0.62038095 -0.05568209 -0.67293331]\n",
      "[ episode 35 ][ timestamp 46 ] state=[ 0.23305624  0.62038095 -0.05568209 -0.67293331], action=0, reward=1.0, next_state=[ 0.24546386  0.42607536 -0.06914075 -0.39828872]\n",
      "[ episode 35 ][ timestamp 47 ] state=[ 0.24546386  0.42607536 -0.06914075 -0.39828872], action=0, reward=1.0, next_state=[ 0.25398537  0.23199892 -0.07710653 -0.1281808 ]\n",
      "[ episode 35 ][ timestamp 48 ] state=[ 0.25398537  0.23199892 -0.07710653 -0.1281808 ], action=0, reward=1.0, next_state=[ 0.25862535  0.03806138 -0.07967014  0.1392143 ]\n",
      "[ episode 35 ][ timestamp 49 ] state=[ 0.25862535  0.03806138 -0.07967014  0.1392143 ], action=1, reward=1.0, next_state=[ 0.25938658  0.23422867 -0.07688586 -0.17750152]\n",
      "[ episode 35 ][ timestamp 50 ] state=[ 0.25938658  0.23422867 -0.07688586 -0.17750152], action=0, reward=1.0, next_state=[ 0.26407115  0.04028639 -0.08043589  0.08997028]\n",
      "[ episode 35 ][ timestamp 51 ] state=[ 0.26407115  0.04028639 -0.08043589  0.08997028], action=1, reward=1.0, next_state=[ 0.26487688  0.23646364 -0.07863648 -0.22696682]\n",
      "[ episode 35 ][ timestamp 52 ] state=[ 0.26487688  0.23646364 -0.07863648 -0.22696682], action=0, reward=1.0, next_state=[ 0.26960615  0.04254842 -0.08317582  0.03991183]\n",
      "[ episode 35 ][ timestamp 53 ] state=[ 0.26960615  0.04254842 -0.08317582  0.03991183], action=0, reward=1.0, next_state=[ 0.27045712 -0.1512884  -0.08237758  0.30523638]\n",
      "[ episode 35 ][ timestamp 54 ] state=[ 0.27045712 -0.1512884  -0.08237758  0.30523638], action=1, reward=1.0, next_state=[ 0.26743135  0.04490495 -0.07627286 -0.01224731]\n",
      "[ episode 35 ][ timestamp 55 ] state=[ 0.26743135  0.04490495 -0.07627286 -0.01224731], action=1, reward=1.0, next_state=[ 0.26832945  0.24103319 -0.0765178  -0.32798683]\n",
      "[ episode 35 ][ timestamp 56 ] state=[ 0.26832945  0.24103319 -0.0765178  -0.32798683], action=1, reward=1.0, next_state=[ 0.27315011  0.43715636 -0.08307754 -0.64378507]\n",
      "[ episode 35 ][ timestamp 57 ] state=[ 0.27315011  0.43715636 -0.08307754 -0.64378507], action=1, reward=1.0, next_state=[ 0.28189324  0.6333319  -0.09595324 -0.96143018]\n",
      "[ episode 35 ][ timestamp 58 ] state=[ 0.28189324  0.6333319  -0.09595324 -0.96143018], action=0, reward=1.0, next_state=[ 0.29455988  0.43962132 -0.11518184 -0.7003679 ]\n",
      "[ episode 35 ][ timestamp 59 ] state=[ 0.29455988  0.43962132 -0.11518184 -0.7003679 ], action=1, reward=1.0, next_state=[ 0.30335231  0.63613566 -0.1291892  -1.02697485]\n",
      "[ episode 35 ][ timestamp 60 ] state=[ 0.30335231  0.63613566 -0.1291892  -1.02697485], action=0, reward=1.0, next_state=[ 0.31607502  0.44294798 -0.1497287  -0.77748424]\n",
      "[ episode 35 ][ timestamp 61 ] state=[ 0.31607502  0.44294798 -0.1497287  -0.77748424], action=0, reward=1.0, next_state=[ 0.32493398  0.25016731 -0.16527838 -0.53540455]\n",
      "[ episode 35 ][ timestamp 62 ] state=[ 0.32493398  0.25016731 -0.16527838 -0.53540455], action=0, reward=1.0, next_state=[ 0.32993732  0.05770795 -0.17598647 -0.2990205 ]\n",
      "[ episode 35 ][ timestamp 63 ] state=[ 0.32993732  0.05770795 -0.17598647 -0.2990205 ], action=0, reward=1.0, next_state=[ 0.33109148 -0.134526   -0.18196688 -0.06659672]\n",
      "[ episode 35 ][ timestamp 64 ] state=[ 0.33109148 -0.134526   -0.18196688 -0.06659672], action=0, reward=1.0, next_state=[ 0.32840096 -0.32663596 -0.18329882  0.163607  ]\n",
      "[ episode 35 ][ timestamp 65 ] state=[ 0.32840096 -0.32663596 -0.18329882  0.163607  ], action=1, reward=1.0, next_state=[ 0.32186824 -0.1294279  -0.18002668 -0.18083816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 35 ][ timestamp 66 ] state=[ 0.32186824 -0.1294279  -0.18002668 -0.18083816], action=0, reward=1.0, next_state=[ 0.31927969 -0.32157837 -0.18364344  0.05008709]\n",
      "[ episode 35 ][ timestamp 67 ] state=[ 0.31927969 -0.32157837 -0.18364344  0.05008709], action=1, reward=1.0, next_state=[ 0.31284812 -0.12436318 -0.1826417  -0.2944496 ]\n",
      "[ episode 35 ][ timestamp 68 ] state=[ 0.31284812 -0.12436318 -0.1826417  -0.2944496 ], action=0, reward=1.0, next_state=[ 0.31036085 -0.31647529 -0.18853069 -0.06447307]\n",
      "[ episode 35 ][ timestamp 69 ] state=[ 0.31036085 -0.31647529 -0.18853069 -0.06447307], action=1, reward=1.0, next_state=[ 0.30403135 -0.11922087 -0.18982015 -0.41021213]\n",
      "[ episode 35 ][ timestamp 70 ] state=[ 0.30403135 -0.11922087 -0.18982015 -0.41021213], action=0, reward=1.0, next_state=[ 0.30164693 -0.31121628 -0.1980244  -0.18286448]\n",
      "[ episode 35 ][ timestamp 71 ] state=[ 0.30164693 -0.31121628 -0.1980244  -0.18286448], action=1, reward=1.0, next_state=[ 0.29542261 -0.11389327 -0.20168169 -0.53090404]\n",
      "[ episode 35 ][ timestamp 72 ] state=[ 0.29542261 -0.11389327 -0.20168169 -0.53090404], action=0, reward=-1.0, next_state=[ 0.29314474 -0.30569255 -0.21229977 -0.30792973]\n",
      "[ Ended! ] Episode 35: Exploration_rate=0.8433051360508336. Score=72.\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ] state=[-0.02017533  0.01596358  0.02347703 -0.02193699]\n",
      "[ episode 36 ][ timestamp 1 ] state=[-0.02017533  0.01596358  0.02347703 -0.02193699], action=0, reward=1.0, next_state=[-0.01985606 -0.17948705  0.02303829  0.27805977]\n",
      "[ episode 36 ][ timestamp 2 ] state=[-0.01985606 -0.17948705  0.02303829  0.27805977], action=1, reward=1.0, next_state=[-0.0234458   0.01529879  0.02859949 -0.00726879]\n",
      "[ episode 36 ][ timestamp 3 ] state=[-0.0234458   0.01529879  0.02859949 -0.00726879], action=1, reward=1.0, next_state=[-0.02313983  0.20999916  0.02845411 -0.29079281]\n",
      "[ episode 36 ][ timestamp 4 ] state=[-0.02313983  0.20999916  0.02845411 -0.29079281], action=0, reward=1.0, next_state=[-0.01893984  0.01448328  0.02263826  0.01072667]\n",
      "[ episode 36 ][ timestamp 5 ] state=[-0.01893984  0.01448328  0.02263826  0.01072667], action=1, reward=1.0, next_state=[-0.01865018  0.20927337  0.02285279 -0.27472852]\n",
      "[ episode 36 ][ timestamp 6 ] state=[-0.01865018  0.20927337  0.02285279 -0.27472852], action=0, reward=1.0, next_state=[-0.01446471  0.01383294  0.01735822  0.02507371]\n",
      "[ episode 36 ][ timestamp 7 ] state=[-0.01446471  0.01383294  0.01735822  0.02507371], action=1, reward=1.0, next_state=[-0.01418805  0.20870172  0.01785969 -0.26208236]\n",
      "[ episode 36 ][ timestamp 8 ] state=[-0.01418805  0.20870172  0.01785969 -0.26208236], action=1, reward=1.0, next_state=[-0.01001402  0.40356424  0.01261805 -0.54907905]\n",
      "[ episode 36 ][ timestamp 9 ] state=[-0.01001402  0.40356424  0.01261805 -0.54907905], action=1, reward=1.0, next_state=[-0.00194273  0.59850669  0.00163646 -0.83775984]\n",
      "[ episode 36 ][ timestamp 10 ] state=[-0.00194273  0.59850669  0.00163646 -0.83775984], action=1, reward=1.0, next_state=[ 0.0100274   0.79360625 -0.01511873 -1.12992768]\n",
      "[ episode 36 ][ timestamp 11 ] state=[ 0.0100274   0.79360625 -0.01511873 -1.12992768], action=0, reward=1.0, next_state=[ 0.02589953  0.59868552 -0.03771729 -0.84202473]\n",
      "[ episode 36 ][ timestamp 12 ] state=[ 0.02589953  0.59868552 -0.03771729 -0.84202473], action=1, reward=1.0, next_state=[ 0.03787324  0.79430144 -0.05455778 -1.14632618]\n",
      "[ episode 36 ][ timestamp 13 ] state=[ 0.03787324  0.79430144 -0.05455778 -1.14632618], action=1, reward=1.0, next_state=[ 0.05375927  0.99009177 -0.0774843  -1.45560673]\n",
      "[ episode 36 ][ timestamp 14 ] state=[ 0.05375927  0.99009177 -0.0774843  -1.45560673], action=1, reward=1.0, next_state=[ 0.0735611   1.18607459 -0.10659644 -1.77145651]\n",
      "[ episode 36 ][ timestamp 15 ] state=[ 0.0735611   1.18607459 -0.10659644 -1.77145651], action=0, reward=1.0, next_state=[ 0.09728259  0.9923046  -0.14202557 -1.51373132]\n",
      "[ episode 36 ][ timestamp 16 ] state=[ 0.09728259  0.9923046  -0.14202557 -1.51373132], action=1, reward=1.0, next_state=[ 0.11712868  1.18883157 -0.1723002  -1.84716892]\n",
      "[ episode 36 ][ timestamp 17 ] state=[ 0.11712868  1.18883157 -0.1723002  -1.84716892], action=0, reward=1.0, next_state=[ 0.14090532  0.99597612 -0.20924357 -1.61257514]\n",
      "[ episode 36 ][ timestamp 18 ] state=[ 0.14090532  0.99597612 -0.20924357 -1.61257514], action=0, reward=-1.0, next_state=[ 0.16082484  0.8038478  -0.24149508 -1.39173831]\n",
      "[ Ended! ] Episode 36: Exploration_rate=0.8390886103705794. Score=18.\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ] state=[ 0.00499993  0.03408254  0.00703154 -0.03385436]\n",
      "[ episode 37 ][ timestamp 1 ] state=[ 0.00499993  0.03408254  0.00703154 -0.03385436], action=0, reward=1.0, next_state=[ 0.00568159 -0.16113953  0.00635445  0.26103877]\n",
      "[ episode 37 ][ timestamp 2 ] state=[ 0.00568159 -0.16113953  0.00635445  0.26103877], action=1, reward=1.0, next_state=[ 0.00245879  0.03389113  0.01157523 -0.02963313]\n",
      "[ episode 37 ][ timestamp 3 ] state=[ 0.00245879  0.03389113  0.01157523 -0.02963313], action=1, reward=1.0, next_state=[ 0.00313662  0.22884519  0.01098257 -0.31864158]\n",
      "[ episode 37 ][ timestamp 4 ] state=[ 0.00313662  0.22884519  0.01098257 -0.31864158], action=0, reward=1.0, next_state=[ 0.00771352  0.03356856  0.00460973 -0.02251549]\n",
      "[ episode 37 ][ timestamp 5 ] state=[ 0.00771352  0.03356856  0.00460973 -0.02251549], action=1, reward=1.0, next_state=[ 0.00838489  0.2286241   0.00415942 -0.31374043]\n",
      "[ episode 37 ][ timestamp 6 ] state=[ 0.00838489  0.2286241   0.00415942 -0.31374043], action=0, reward=1.0, next_state=[ 0.01295737  0.03344314 -0.00211538 -0.01974866]\n",
      "[ episode 37 ][ timestamp 7 ] state=[ 0.01295737  0.03344314 -0.00211538 -0.01974866], action=0, reward=1.0, next_state=[ 0.01362624 -0.16164841 -0.00251036  0.27226608]\n",
      "[ episode 37 ][ timestamp 8 ] state=[ 0.01362624 -0.16164841 -0.00251036  0.27226608], action=0, reward=1.0, next_state=[ 0.01039327 -0.35673445  0.00293496  0.56415618]\n",
      "[ episode 37 ][ timestamp 9 ] state=[ 0.01039327 -0.35673445  0.00293496  0.56415618], action=0, reward=1.0, next_state=[ 0.00325858 -0.55189746  0.01421809  0.85776231]\n",
      "[ episode 37 ][ timestamp 10 ] state=[ 0.00325858 -0.55189746  0.01421809  0.85776231], action=1, reward=1.0, next_state=[-0.00777937 -0.35697206  0.03137333  0.56958375]\n",
      "[ episode 37 ][ timestamp 11 ] state=[-0.00777937 -0.35697206  0.03137333  0.56958375], action=1, reward=1.0, next_state=[-0.01491881 -0.16230384  0.04276501  0.28694736]\n",
      "[ episode 37 ][ timestamp 12 ] state=[-0.01491881 -0.16230384  0.04276501  0.28694736], action=0, reward=1.0, next_state=[-0.01816489 -0.35800875  0.04850396  0.59280541]\n",
      "[ episode 37 ][ timestamp 13 ] state=[-0.01816489 -0.35800875  0.04850396  0.59280541], action=1, reward=1.0, next_state=[-0.02532506 -0.16359815  0.06036006  0.31578704]\n",
      "[ episode 37 ][ timestamp 14 ] state=[-0.02532506 -0.16359815  0.06036006  0.31578704], action=1, reward=1.0, next_state=[-0.02859703  0.03061441  0.06667581  0.04273381]\n",
      "[ episode 37 ][ timestamp 15 ] state=[-0.02859703  0.03061441  0.06667581  0.04273381], action=0, reward=1.0, next_state=[-0.02798474 -0.16539715  0.06753048  0.35568602]\n",
      "[ episode 37 ][ timestamp 16 ] state=[-0.02798474 -0.16539715  0.06753048  0.35568602], action=1, reward=1.0, next_state=[-0.03129268  0.02870292  0.0746442   0.08503841]\n",
      "[ episode 37 ][ timestamp 17 ] state=[-0.03129268  0.02870292  0.0746442   0.08503841], action=0, reward=1.0, next_state=[-0.03071862 -0.16740524  0.07634497  0.40030655]\n",
      "[ episode 37 ][ timestamp 18 ] state=[-0.03071862 -0.16740524  0.07634497  0.40030655], action=0, reward=1.0, next_state=[-0.03406673 -0.36352244  0.0843511   0.71604908]\n",
      "[ episode 37 ][ timestamp 19 ] state=[-0.03406673 -0.36352244  0.0843511   0.71604908], action=1, reward=1.0, next_state=[-0.04133718 -0.16966302  0.09867208  0.45106366]\n",
      "[ episode 37 ][ timestamp 20 ] state=[-0.04133718 -0.16966302  0.09867208  0.45106366], action=0, reward=1.0, next_state=[-0.04473044 -0.36603191  0.10769336  0.77314679]\n",
      "[ episode 37 ][ timestamp 21 ] state=[-0.04473044 -0.36603191  0.10769336  0.77314679], action=0, reward=1.0, next_state=[-0.05205107 -0.5624577   0.12315629  1.09767921]\n",
      "[ episode 37 ][ timestamp 22 ] state=[-0.05205107 -0.5624577   0.12315629  1.09767921], action=0, reward=1.0, next_state=[-0.06330023 -0.75896669  0.14510988  1.42632661]\n",
      "[ episode 37 ][ timestamp 23 ] state=[-0.06330023 -0.75896669  0.14510988  1.42632661], action=0, reward=1.0, next_state=[-0.07847956 -0.95555287  0.17363641  1.76061944]\n",
      "[ episode 37 ][ timestamp 24 ] state=[-0.07847956 -0.95555287  0.17363641  1.76061944], action=0, reward=1.0, next_state=[-0.09759062 -1.1521634   0.2088488   2.10189357]\n",
      "[ episode 37 ][ timestamp 25 ] state=[-0.09759062 -1.1521634   0.2088488   2.10189357], action=1, reward=-1.0, next_state=[-0.12063389 -0.95966215  0.25088667  1.88037236]\n",
      "[ Ended! ] Episode 37: Exploration_rate=0.8348931673187264. Score=25.\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ] state=[-0.00721901 -0.003781   -0.01409408  0.04046496]\n",
      "[ episode 38 ][ timestamp 1 ] state=[-0.00721901 -0.003781   -0.01409408  0.04046496], action=1, reward=1.0, next_state=[-0.00729463  0.19154019 -0.01328478 -0.25663125]\n",
      "[ episode 38 ][ timestamp 2 ] state=[-0.00729463  0.19154019 -0.01328478 -0.25663125], action=0, reward=1.0, next_state=[-0.00346382 -0.00338959 -0.0184174   0.03183202]\n",
      "[ episode 38 ][ timestamp 3 ] state=[-0.00346382 -0.00338959 -0.0184174   0.03183202], action=1, reward=1.0, next_state=[-0.00353162  0.19199157 -0.01778076 -0.26660442]\n",
      "[ episode 38 ][ timestamp 4 ] state=[-0.00353162  0.19199157 -0.01778076 -0.26660442], action=0, reward=1.0, next_state=[ 0.00030822 -0.00287216 -0.02311285  0.0204177 ]\n",
      "[ episode 38 ][ timestamp 5 ] state=[ 0.00030822 -0.00287216 -0.02311285  0.0204177 ], action=1, reward=1.0, next_state=[ 2.50772229e-04  1.92573494e-01 -2.27044961e-02 -2.79467058e-01]\n",
      "[ episode 38 ][ timestamp 6 ] state=[ 2.50772229e-04  1.92573494e-01 -2.27044961e-02 -2.79467058e-01], action=0, reward=1.0, next_state=[ 0.00410224 -0.00221734 -0.02829384  0.00596933]\n",
      "[ episode 38 ][ timestamp 7 ] state=[ 0.00410224 -0.00221734 -0.02829384  0.00596933], action=1, reward=1.0, next_state=[ 0.0040579   0.19329872 -0.02817445 -0.29550465]\n",
      "[ episode 38 ][ timestamp 8 ] state=[ 0.0040579   0.19329872 -0.02817445 -0.29550465], action=0, reward=1.0, next_state=[ 0.00792387 -0.00141047 -0.03408454 -0.01183896]\n",
      "[ episode 38 ][ timestamp 9 ] state=[ 0.00792387 -0.00141047 -0.03408454 -0.01183896], action=1, reward=1.0, next_state=[ 0.00789566  0.1941833  -0.03432132 -0.31507813]\n",
      "[ episode 38 ][ timestamp 10 ] state=[ 0.00789566  0.1941833  -0.03432132 -0.31507813], action=0, reward=1.0, next_state=[ 0.01177933 -0.00043338 -0.04062289 -0.03341352]\n",
      "[ episode 38 ][ timestamp 11 ] state=[ 0.01177933 -0.00043338 -0.04062289 -0.03341352], action=0, reward=1.0, next_state=[ 0.01177066 -0.19494995 -0.04129116  0.24618078]\n",
      "[ episode 38 ][ timestamp 12 ] state=[ 0.01177066 -0.19494995 -0.04129116  0.24618078], action=1, reward=1.0, next_state=[ 0.00787166  0.00073667 -0.03636754 -0.0592351 ]\n",
      "[ episode 38 ][ timestamp 13 ] state=[ 0.00787166  0.00073667 -0.03636754 -0.0592351 ], action=1, reward=1.0, next_state=[ 0.00788639  0.19636067 -0.03755224 -0.36316679]\n",
      "[ episode 38 ][ timestamp 14 ] state=[ 0.00788639  0.19636067 -0.03755224 -0.36316679], action=1, reward=1.0, next_state=[ 0.01181361  0.39199566 -0.04481558 -0.66745015]\n",
      "[ episode 38 ][ timestamp 15 ] state=[ 0.01181361  0.39199566 -0.04481558 -0.66745015], action=1, reward=1.0, next_state=[ 0.01965352  0.58771126 -0.05816458 -0.97390016]\n",
      "[ episode 38 ][ timestamp 16 ] state=[ 0.01965352  0.58771126 -0.05816458 -0.97390016], action=1, reward=1.0, next_state=[ 0.03140775  0.78356328 -0.07764258 -1.28427213]\n",
      "[ episode 38 ][ timestamp 17 ] state=[ 0.03140775  0.78356328 -0.07764258 -1.28427213], action=0, reward=1.0, next_state=[ 0.04707901  0.58951098 -0.10332803 -1.01687459]\n",
      "[ episode 38 ][ timestamp 18 ] state=[ 0.04707901  0.58951098 -0.10332803 -1.01687459], action=1, reward=1.0, next_state=[ 0.05886923  0.78584739 -0.12366552 -1.34013285]\n",
      "[ episode 38 ][ timestamp 19 ] state=[ 0.05886923  0.78584739 -0.12366552 -1.34013285], action=1, reward=1.0, next_state=[ 0.07458618  0.98229003 -0.15046818 -1.66881157]\n",
      "[ episode 38 ][ timestamp 20 ] state=[ 0.07458618  0.98229003 -0.15046818 -1.66881157], action=1, reward=1.0, next_state=[ 0.09423198  1.1788071  -0.18384441 -2.00432741]\n",
      "[ episode 38 ][ timestamp 21 ] state=[ 0.09423198  1.1788071  -0.18384441 -2.00432741], action=1, reward=-1.0, next_state=[ 0.11780812  1.37530963 -0.22393096 -2.34786036]\n",
      "[ Ended! ] Episode 38: Exploration_rate=0.8307187014821328. Score=21.\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ] state=[ 0.00920935 -0.02673088  0.00030564 -0.03450336]\n",
      "[ episode 39 ][ timestamp 1 ] state=[ 0.00920935 -0.02673088  0.00030564 -0.03450336], action=1, reward=1.0, next_state=[ 0.00867473  0.16838669 -0.00038443 -0.32708984]\n",
      "[ episode 39 ][ timestamp 2 ] state=[ 0.00867473  0.16838669 -0.00038443 -0.32708984], action=1, reward=1.0, next_state=[ 0.01204246  0.36351411 -0.00692622 -0.61989397]\n",
      "[ episode 39 ][ timestamp 3 ] state=[ 0.01204246  0.36351411 -0.00692622 -0.61989397], action=0, reward=1.0, next_state=[ 0.01931275  0.16848957 -0.0193241  -0.32940048]\n",
      "[ episode 39 ][ timestamp 4 ] state=[ 0.01931275  0.16848957 -0.0193241  -0.32940048], action=0, reward=1.0, next_state=[ 0.02268254 -0.02635203 -0.02591211 -0.04287357]\n",
      "[ episode 39 ][ timestamp 5 ] state=[ 0.02268254 -0.02635203 -0.02591211 -0.04287357], action=1, reward=1.0, next_state=[ 0.0221555   0.16913172 -0.02676958 -0.34361807]\n",
      "[ episode 39 ][ timestamp 6 ] state=[ 0.0221555   0.16913172 -0.02676958 -0.34361807], action=0, reward=1.0, next_state=[ 0.02553813 -0.02559937 -0.03364195 -0.0594954 ]\n",
      "[ episode 39 ][ timestamp 7 ] state=[ 0.02553813 -0.02559937 -0.03364195 -0.0594954 ], action=1, reward=1.0, next_state=[ 0.02502614  0.16998838 -0.03483185 -0.36259989]\n",
      "[ episode 39 ][ timestamp 8 ] state=[ 0.02502614  0.16998838 -0.03483185 -0.36259989], action=1, reward=1.0, next_state=[ 0.02842591  0.36558764 -0.04208385 -0.66605931]\n",
      "[ episode 39 ][ timestamp 9 ] state=[ 0.02842591  0.36558764 -0.04208385 -0.66605931], action=0, reward=1.0, next_state=[ 0.03573767  0.17107551 -0.05540504 -0.38691844]\n",
      "[ episode 39 ][ timestamp 10 ] state=[ 0.03573767  0.17107551 -0.05540504 -0.38691844], action=0, reward=1.0, next_state=[ 0.03915918 -0.02321797 -0.06314341 -0.11220618]\n",
      "[ episode 39 ][ timestamp 11 ] state=[ 0.03915918 -0.02321797 -0.06314341 -0.11220618], action=0, reward=1.0, next_state=[ 0.03869482 -0.21738097 -0.06538753  0.15990607]\n",
      "[ episode 39 ][ timestamp 12 ] state=[ 0.03869482 -0.21738097 -0.06538753  0.15990607], action=0, reward=1.0, next_state=[ 0.0343472  -0.41150882 -0.06218941  0.43126533]\n",
      "[ episode 39 ][ timestamp 13 ] state=[ 0.0343472  -0.41150882 -0.06218941  0.43126533], action=0, reward=1.0, next_state=[ 0.02611702 -0.60569758 -0.0535641   0.70371348]\n",
      "[ episode 39 ][ timestamp 14 ] state=[ 0.02611702 -0.60569758 -0.0535641   0.70371348], action=0, reward=1.0, next_state=[ 0.01400307 -0.80003791 -0.03948983  0.97906558]\n",
      "[ episode 39 ][ timestamp 15 ] state=[ 0.01400307 -0.80003791 -0.03948983  0.97906558], action=1, reward=1.0, next_state=[-0.00199769 -0.60440944 -0.01990852  0.67424465]\n",
      "[ episode 39 ][ timestamp 16 ] state=[-0.00199769 -0.60440944 -0.01990852  0.67424465], action=0, reward=1.0, next_state=[-0.01408588 -0.79924913 -0.00642363  0.96059355]\n",
      "[ episode 39 ][ timestamp 17 ] state=[-0.01408588 -0.79924913 -0.00642363  0.96059355], action=0, reward=1.0, next_state=[-0.03007086 -0.99428415  0.01278824  1.25125151]\n",
      "[ episode 39 ][ timestamp 18 ] state=[-0.03007086 -0.99428415  0.01278824  1.25125151], action=0, reward=1.0, next_state=[-0.04995654 -1.18956762  0.03781327  1.5479124 ]\n",
      "[ episode 39 ][ timestamp 19 ] state=[-0.04995654 -1.18956762  0.03781327  1.5479124 ], action=0, reward=1.0, next_state=[-0.0737479  -1.38512253  0.06877152  1.85214953]\n",
      "[ episode 39 ][ timestamp 20 ] state=[-0.0737479  -1.38512253  0.06877152  1.85214953], action=1, reward=1.0, next_state=[-0.10145035 -1.19082091  0.10581451  1.58158894]\n",
      "[ episode 39 ][ timestamp 21 ] state=[-0.10145035 -1.19082091  0.10581451  1.58158894], action=0, reward=1.0, next_state=[-0.12526676 -1.38703119  0.13744629  1.90530967]\n",
      "[ episode 39 ][ timestamp 22 ] state=[-0.12526676 -1.38703119  0.13744629  1.90530967], action=0, reward=1.0, next_state=[-0.15300739 -1.58334467  0.17555248  2.23728487]\n",
      "[ episode 39 ][ timestamp 23 ] state=[-0.15300739 -1.58334467  0.17555248  2.23728487], action=1, reward=-1.0, next_state=[-0.18467428 -1.39026704  0.22029818  2.00346751]\n",
      "[ Ended! ] Episode 39: Exploration_rate=0.8265651079747222. Score=23.\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ] state=[-0.03524177  0.04891932  0.03192115 -0.03746048]\n",
      "[ episode 40 ][ timestamp 1 ] state=[-0.03524177  0.04891932  0.03192115 -0.03746048], action=0, reward=1.0, next_state=[-0.03426338 -0.14664549  0.03117194  0.26512052]\n",
      "[ episode 40 ][ timestamp 2 ] state=[-0.03426338 -0.14664549  0.03117194  0.26512052], action=1, reward=1.0, next_state=[-0.03719629  0.048018    0.03647435 -0.01756979]\n",
      "[ episode 40 ][ timestamp 3 ] state=[-0.03719629  0.048018    0.03647435 -0.01756979], action=0, reward=1.0, next_state=[-0.03623593 -0.14760754  0.03612296  0.28639442]\n",
      "[ episode 40 ][ timestamp 4 ] state=[-0.03623593 -0.14760754  0.03612296  0.28639442], action=0, reward=1.0, next_state=[-0.03918808 -0.34322554  0.04185084  0.59024784]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 40 ][ timestamp 5 ] state=[-0.03918808 -0.34322554  0.04185084  0.59024784], action=0, reward=1.0, next_state=[-0.04605259 -0.5389077   0.0536558   0.89581463]\n",
      "[ episode 40 ][ timestamp 6 ] state=[-0.04605259 -0.5389077   0.0536558   0.89581463], action=0, reward=1.0, next_state=[-0.05683075 -0.73471449  0.07157209  1.20486936]\n",
      "[ episode 40 ][ timestamp 7 ] state=[-0.05683075 -0.73471449  0.07157209  1.20486936], action=1, reward=1.0, next_state=[-0.07152504 -0.54058684  0.09566948  0.93544763]\n",
      "[ episode 40 ][ timestamp 8 ] state=[-0.07152504 -0.54058684  0.09566948  0.93544763], action=0, reward=1.0, next_state=[-0.08233678 -0.73685988  0.11437843  1.25659484]\n",
      "[ episode 40 ][ timestamp 9 ] state=[-0.08233678 -0.73685988  0.11437843  1.25659484], action=0, reward=1.0, next_state=[-0.09707397 -0.93324493  0.13951033  1.5828016 ]\n",
      "[ episode 40 ][ timestamp 10 ] state=[-0.09707397 -0.93324493  0.13951033  1.5828016 ], action=1, reward=1.0, next_state=[-0.11573887 -0.7400314   0.17116636  1.33668026]\n",
      "[ episode 40 ][ timestamp 11 ] state=[-0.11573887 -0.7400314   0.17116636  1.33668026], action=0, reward=1.0, next_state=[-0.1305395  -0.93684573  0.19789997  1.67766517]\n",
      "[ episode 40 ][ timestamp 12 ] state=[-0.1305395  -0.93684573  0.19789997  1.67766517], action=0, reward=-1.0, next_state=[-0.14927641 -1.13363584  0.23145327  2.02489235]\n",
      "[ Ended! ] Episode 40: Exploration_rate=0.8224322824348486. Score=12.\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ] state=[-0.00803828  0.03108819 -0.04626699 -0.0476733 ]\n",
      "[ episode 41 ][ timestamp 1 ] state=[-0.00803828  0.03108819 -0.04626699 -0.0476733 ], action=0, reward=1.0, next_state=[-0.00741652 -0.16334085 -0.04722046  0.23006051]\n",
      "[ episode 41 ][ timestamp 2 ] state=[-0.00741652 -0.16334085 -0.04722046  0.23006051], action=1, reward=1.0, next_state=[-0.01068333  0.03242296 -0.04261925 -0.07713553]\n",
      "[ episode 41 ][ timestamp 3 ] state=[-0.01068333  0.03242296 -0.04261925 -0.07713553], action=0, reward=1.0, next_state=[-0.01003487 -0.16206293 -0.04416196  0.20180212]\n",
      "[ episode 41 ][ timestamp 4 ] state=[-0.01003487 -0.16206293 -0.04416196  0.20180212], action=1, reward=1.0, next_state=[-0.01327613  0.03366188 -0.04012592 -0.10447825]\n",
      "[ episode 41 ][ timestamp 5 ] state=[-0.01327613  0.03366188 -0.04012592 -0.10447825], action=0, reward=1.0, next_state=[-0.01260289 -0.16086274 -0.04221548  0.17527997]\n",
      "[ episode 41 ][ timestamp 6 ] state=[-0.01260289 -0.16086274 -0.04221548  0.17527997], action=0, reward=1.0, next_state=[-0.01582015 -0.3553559  -0.03870988  0.45435211]\n",
      "[ episode 41 ][ timestamp 7 ] state=[-0.01582015 -0.3553559  -0.03870988  0.45435211], action=1, reward=1.0, next_state=[-0.02292727 -0.15970858 -0.02962284  0.14972312]\n",
      "[ episode 41 ][ timestamp 8 ] state=[-0.02292727 -0.15970858 -0.02962284  0.14972312], action=0, reward=1.0, next_state=[-0.02612144 -0.3543941  -0.02662838  0.43291543]\n",
      "[ episode 41 ][ timestamp 9 ] state=[-0.02612144 -0.3543941  -0.02662838  0.43291543], action=0, reward=1.0, next_state=[-0.03320932 -0.54912911 -0.01797007  0.71708657]\n",
      "[ episode 41 ][ timestamp 10 ] state=[-0.03320932 -0.54912911 -0.01797007  0.71708657], action=0, reward=1.0, next_state=[-0.0441919  -0.74399781 -0.00362834  1.00405951]\n",
      "[ episode 41 ][ timestamp 11 ] state=[-0.0441919  -0.74399781 -0.00362834  1.00405951], action=1, reward=1.0, next_state=[-0.05907186 -0.54882758  0.01645285  0.71023936]\n",
      "[ episode 41 ][ timestamp 12 ] state=[-0.05907186 -0.54882758  0.01645285  0.71023936], action=1, reward=1.0, next_state=[-0.07004841 -0.35393731  0.03065764  0.42278044]\n",
      "[ episode 41 ][ timestamp 13 ] state=[-0.07004841 -0.35393731  0.03065764  0.42278044], action=1, reward=1.0, next_state=[-0.07712716 -0.15926279  0.03911325  0.13991782]\n",
      "[ episode 41 ][ timestamp 14 ] state=[-0.07712716 -0.15926279  0.03911325  0.13991782], action=0, reward=1.0, next_state=[-0.08031241 -0.35492248  0.0419116   0.44467924]\n",
      "[ episode 41 ][ timestamp 15 ] state=[-0.08031241 -0.35492248  0.0419116   0.44467924], action=1, reward=1.0, next_state=[-0.08741086 -0.1604178   0.05080519  0.16549684]\n",
      "[ episode 41 ][ timestamp 16 ] state=[-0.08741086 -0.1604178   0.05080519  0.16549684], action=0, reward=1.0, next_state=[-0.09061922 -0.35622881  0.05411512  0.47376467]\n",
      "[ episode 41 ][ timestamp 17 ] state=[-0.09061922 -0.35622881  0.05411512  0.47376467], action=0, reward=1.0, next_state=[-0.09774379 -0.55207157  0.06359042  0.78300086]\n",
      "[ episode 41 ][ timestamp 18 ] state=[-0.09774379 -0.55207157  0.06359042  0.78300086], action=0, reward=1.0, next_state=[-0.10878522 -0.74800713  0.07925044  1.09499317]\n",
      "[ episode 41 ][ timestamp 19 ] state=[-0.10878522 -0.74800713  0.07925044  1.09499317], action=1, reward=1.0, next_state=[-0.12374537 -0.55401336  0.1011503   0.82819107]\n",
      "[ episode 41 ][ timestamp 20 ] state=[-0.12374537 -0.55401336  0.1011503   0.82819107], action=0, reward=1.0, next_state=[-0.13482563 -0.75036201  0.11771412  1.15089615]\n",
      "[ episode 41 ][ timestamp 21 ] state=[-0.13482563 -0.75036201  0.11771412  1.15089615], action=0, reward=1.0, next_state=[-0.14983287 -0.94680658  0.14073204  1.4780519 ]\n",
      "[ episode 41 ][ timestamp 22 ] state=[-0.14983287 -0.94680658  0.14073204  1.4780519 ], action=0, reward=1.0, next_state=[-0.16876901 -1.1433384   0.17029308  1.81117391]\n",
      "[ episode 41 ][ timestamp 23 ] state=[-0.16876901 -1.1433384   0.17029308  1.81117391], action=1, reward=1.0, next_state=[-0.19163577 -0.95047469  0.20651656  1.5758875 ]\n",
      "[ episode 41 ][ timestamp 24 ] state=[-0.19163577 -0.95047469  0.20651656  1.5758875 ], action=0, reward=-1.0, next_state=[-0.21064527 -1.14737232  0.23803431  1.92524338]\n",
      "[ Ended! ] Episode 41: Exploration_rate=0.8183201210226743. Score=24.\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ] state=[ 0.01031851 -0.02167815 -0.00010867  0.0408918 ]\n",
      "[ episode 42 ][ timestamp 1 ] state=[ 0.01031851 -0.02167815 -0.00010867  0.0408918 ], action=1, reward=1.0, next_state=[ 0.00988495  0.17344536  0.00070917 -0.25182541]\n",
      "[ episode 42 ][ timestamp 2 ] state=[ 0.00988495  0.17344536  0.00070917 -0.25182541], action=1, reward=1.0, next_state=[ 0.01335386  0.36855717 -0.00432734 -0.54428457]\n",
      "[ episode 42 ][ timestamp 3 ] state=[ 0.01335386  0.36855717 -0.00432734 -0.54428457], action=0, reward=1.0, next_state=[ 0.020725    0.1734963  -0.01521303 -0.25296823]\n",
      "[ episode 42 ][ timestamp 4 ] state=[ 0.020725    0.1734963  -0.01521303 -0.25296823], action=1, reward=1.0, next_state=[ 0.02419493  0.36883214 -0.0202724  -0.55041054]\n",
      "[ episode 42 ][ timestamp 5 ] state=[ 0.02419493  0.36883214 -0.0202724  -0.55041054], action=1, reward=1.0, next_state=[ 0.03157157  0.56423288 -0.03128061 -0.8494111 ]\n",
      "[ episode 42 ][ timestamp 6 ] state=[ 0.03157157  0.56423288 -0.03128061 -0.8494111 ], action=1, reward=1.0, next_state=[ 0.04285623  0.75976714 -0.04826883 -1.15176401]\n",
      "[ episode 42 ][ timestamp 7 ] state=[ 0.04285623  0.75976714 -0.04826883 -1.15176401], action=0, reward=1.0, next_state=[ 0.05805157  0.56530704 -0.07130411 -0.87459912]\n",
      "[ episode 42 ][ timestamp 8 ] state=[ 0.05805157  0.56530704 -0.07130411 -0.87459912], action=1, reward=1.0, next_state=[ 0.06935771  0.76132217 -0.08879609 -1.18882033]\n",
      "[ episode 42 ][ timestamp 9 ] state=[ 0.06935771  0.76132217 -0.08879609 -1.18882033], action=0, reward=1.0, next_state=[ 0.08458416  0.56745628 -0.1125725  -0.92523894]\n",
      "[ episode 42 ][ timestamp 10 ] state=[ 0.08458416  0.56745628 -0.1125725  -0.92523894], action=1, reward=1.0, next_state=[ 0.09593328  0.76390387 -0.13107728 -1.25107163]\n",
      "[ episode 42 ][ timestamp 11 ] state=[ 0.09593328  0.76390387 -0.13107728 -1.25107163], action=0, reward=1.0, next_state=[ 0.11121136  0.57068224 -0.15609871 -1.00215193]\n",
      "[ episode 42 ][ timestamp 12 ] state=[ 0.11121136  0.57068224 -0.15609871 -1.00215193], action=0, reward=1.0, next_state=[ 0.122625    0.3779513  -0.17614175 -0.76227742]\n",
      "[ episode 42 ][ timestamp 13 ] state=[ 0.122625    0.3779513  -0.17614175 -0.76227742], action=1, reward=1.0, next_state=[ 0.13018403  0.57500537 -0.1913873  -1.10480335]\n",
      "[ episode 42 ][ timestamp 14 ] state=[ 0.13018403  0.57500537 -0.1913873  -1.10480335], action=0, reward=-1.0, next_state=[ 0.14168414  0.38284391 -0.21348336 -0.87774907]\n",
      "[ Ended! ] Episode 42: Exploration_rate=0.8142285204175609. Score=14.\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ] state=[ 0.03391424 -0.02913143  0.01825021  0.00096216]\n",
      "[ episode 43 ][ timestamp 1 ] state=[ 0.03391424 -0.02913143  0.01825021  0.00096216], action=1, reward=1.0, next_state=[ 0.03333161  0.1657241   0.01826945 -0.28590719]\n",
      "[ episode 43 ][ timestamp 2 ] state=[ 0.03333161  0.1657241   0.01826945 -0.28590719], action=0, reward=1.0, next_state=[ 0.0366461  -0.02965358  0.01255131  0.01248134]\n",
      "[ episode 43 ][ timestamp 3 ] state=[ 0.0366461  -0.02965358  0.01255131  0.01248134], action=0, reward=1.0, next_state=[ 0.03605302 -0.22495326  0.01280094  0.30909778]\n",
      "[ episode 43 ][ timestamp 4 ] state=[ 0.03605302 -0.22495326  0.01280094  0.30909778], action=1, reward=1.0, next_state=[ 0.03155396 -0.03001602  0.01898289  0.02047924]\n",
      "[ episode 43 ][ timestamp 5 ] state=[ 0.03155396 -0.03001602  0.01898289  0.02047924], action=1, reward=1.0, next_state=[ 0.03095364  0.16482863  0.01939248 -0.26615443]\n",
      "[ episode 43 ][ timestamp 6 ] state=[ 0.03095364  0.16482863  0.01939248 -0.26615443], action=1, reward=1.0, next_state=[ 0.03425021  0.35966851  0.01406939 -0.55265827]\n",
      "[ episode 43 ][ timestamp 7 ] state=[ 0.03425021  0.35966851  0.01406939 -0.55265827], action=0, reward=1.0, next_state=[ 0.04144358  0.16435183  0.00301622 -0.25557599]\n",
      "[ episode 43 ][ timestamp 8 ] state=[ 0.04144358  0.16435183  0.00301622 -0.25557599], action=1, reward=1.0, next_state=[ 0.04473062  0.35943059 -0.0020953  -0.54730602]\n",
      "[ episode 43 ][ timestamp 9 ] state=[ 0.04473062  0.35943059 -0.0020953  -0.54730602], action=1, reward=1.0, next_state=[ 0.05191923  0.55458192 -0.01304142 -0.84064839]\n",
      "[ episode 43 ][ timestamp 10 ] state=[ 0.05191923  0.55458192 -0.01304142 -0.84064839], action=1, reward=1.0, next_state=[ 0.06301087  0.74987946 -0.02985438 -1.13740385]\n",
      "[ episode 43 ][ timestamp 11 ] state=[ 0.06301087  0.74987946 -0.02985438 -1.13740385], action=1, reward=1.0, next_state=[ 0.07800846  0.94537889 -0.05260246 -1.43929821]\n",
      "[ episode 43 ][ timestamp 12 ] state=[ 0.07800846  0.94537889 -0.05260246 -1.43929821], action=1, reward=1.0, next_state=[ 0.09691604  1.14110799 -0.08138843 -1.74794376]\n",
      "[ episode 43 ][ timestamp 13 ] state=[ 0.09691604  1.14110799 -0.08138843 -1.74794376], action=0, reward=1.0, next_state=[ 0.1197382   0.94699966 -0.1163473  -1.48164687]\n",
      "[ episode 43 ][ timestamp 14 ] state=[ 0.1197382   0.94699966 -0.1163473  -1.48164687], action=0, reward=1.0, next_state=[ 0.13867819  0.75347348 -0.14598024 -1.22744915]\n",
      "[ episode 43 ][ timestamp 15 ] state=[ 0.13867819  0.75347348 -0.14598024 -1.22744915], action=0, reward=1.0, next_state=[ 0.15374766  0.56050044 -0.17052922 -0.98383425]\n",
      "[ episode 43 ][ timestamp 16 ] state=[ 0.15374766  0.56050044 -0.17052922 -0.98383425], action=0, reward=1.0, next_state=[ 0.16495767  0.36802234 -0.19020591 -0.74919787]\n",
      "[ episode 43 ][ timestamp 17 ] state=[ 0.16495767  0.36802234 -0.19020591 -0.74919787], action=1, reward=1.0, next_state=[ 0.17231811  0.5651876  -0.20518986 -1.09519601]\n",
      "[ episode 43 ][ timestamp 18 ] state=[ 0.17231811  0.5651876  -0.20518986 -1.09519601], action=1, reward=-1.0, next_state=[ 0.18362187  0.76233307 -0.22709378 -1.44461412]\n",
      "[ Ended! ] Episode 43: Exploration_rate=0.810157377815473. Score=18.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 44 ] state=[ 0.03138339  0.00498829 -0.03730645  0.03172011]\n",
      "[ episode 44 ][ timestamp 1 ] state=[ 0.03138339  0.00498829 -0.03730645  0.03172011], action=1, reward=1.0, next_state=[ 0.03148316  0.20062483 -0.03667205 -0.27249605]\n",
      "[ episode 44 ][ timestamp 2 ] state=[ 0.03148316  0.20062483 -0.03667205 -0.27249605], action=1, reward=1.0, next_state=[ 0.03549565  0.39625034 -0.04212197 -0.57651619]\n",
      "[ episode 44 ][ timestamp 3 ] state=[ 0.03549565  0.39625034 -0.04212197 -0.57651619], action=0, reward=1.0, next_state=[ 0.04342066  0.20174335 -0.05365229 -0.29739469]\n",
      "[ episode 44 ][ timestamp 4 ] state=[ 0.04342066  0.20174335 -0.05365229 -0.29739469], action=0, reward=1.0, next_state=[ 0.04745553  0.00742563 -0.05960019 -0.02210375]\n",
      "[ episode 44 ][ timestamp 5 ] state=[ 0.04745553  0.00742563 -0.05960019 -0.02210375], action=1, reward=1.0, next_state=[ 0.04760404  0.20334942 -0.06004226 -0.3329797 ]\n",
      "[ episode 44 ][ timestamp 6 ] state=[ 0.04760404  0.20334942 -0.06004226 -0.3329797 ], action=0, reward=1.0, next_state=[ 0.05167103  0.00913118 -0.06670186 -0.05981913]\n",
      "[ episode 44 ][ timestamp 7 ] state=[ 0.05167103  0.00913118 -0.06670186 -0.05981913], action=1, reward=1.0, next_state=[ 0.05185365  0.20514295 -0.06789824 -0.37277877]\n",
      "[ episode 44 ][ timestamp 8 ] state=[ 0.05185365  0.20514295 -0.06789824 -0.37277877], action=1, reward=1.0, next_state=[ 0.05595651  0.40116045 -0.07535381 -0.68607427]\n",
      "[ episode 44 ][ timestamp 9 ] state=[ 0.05595651  0.40116045 -0.07535381 -0.68607427], action=1, reward=1.0, next_state=[ 0.06397972  0.59724311 -0.0890753  -1.00149668]\n",
      "[ episode 44 ][ timestamp 10 ] state=[ 0.06397972  0.59724311 -0.0890753  -1.00149668], action=1, reward=1.0, next_state=[ 0.07592458  0.79343514 -0.10910523 -1.32077151]\n",
      "[ episode 44 ][ timestamp 11 ] state=[ 0.07592458  0.79343514 -0.10910523 -1.32077151], action=1, reward=1.0, next_state=[ 0.09179329  0.98975379 -0.13552066 -1.64551184]\n",
      "[ episode 44 ][ timestamp 12 ] state=[ 0.09179329  0.98975379 -0.13552066 -1.64551184], action=1, reward=1.0, next_state=[ 0.11158836  1.18617635 -0.1684309  -1.97716543]\n",
      "[ episode 44 ][ timestamp 13 ] state=[ 0.11158836  1.18617635 -0.1684309  -1.97716543], action=1, reward=1.0, next_state=[ 0.13531189  1.38262514 -0.20797421 -2.31695359]\n",
      "[ episode 44 ][ timestamp 14 ] state=[ 0.13531189  1.38262514 -0.20797421 -2.31695359], action=0, reward=-1.0, next_state=[ 0.16296439  1.18992032 -0.25431328 -2.09482979]\n",
      "[ Ended! ] Episode 44: Exploration_rate=0.8061065909263957. Score=14.\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ] state=[ 0.04823659  0.04315728 -0.00840204 -0.01898929]\n",
      "[ episode 45 ][ timestamp 1 ] state=[ 0.04823659  0.04315728 -0.00840204 -0.01898929], action=1, reward=1.0, next_state=[ 0.04909973  0.23839871 -0.00878182 -0.31431127]\n",
      "[ episode 45 ][ timestamp 2 ] state=[ 0.04909973  0.23839871 -0.00878182 -0.31431127], action=0, reward=1.0, next_state=[ 0.05386771  0.04340295 -0.01506805 -0.02441073]\n",
      "[ episode 45 ][ timestamp 3 ] state=[ 0.05386771  0.04340295 -0.01506805 -0.02441073], action=0, reward=1.0, next_state=[ 0.05473577 -0.1514997  -0.01555626  0.26348023]\n",
      "[ episode 45 ][ timestamp 4 ] state=[ 0.05473577 -0.1514997  -0.01555626  0.26348023], action=1, reward=1.0, next_state=[ 0.05170577  0.0438408  -0.01028666 -0.03406843]\n",
      "[ episode 45 ][ timestamp 5 ] state=[ 0.05170577  0.0438408  -0.01028666 -0.03406843], action=0, reward=1.0, next_state=[ 0.05258259 -0.15113214 -0.01096803  0.25535128]\n",
      "[ episode 45 ][ timestamp 6 ] state=[ 0.05258259 -0.15113214 -0.01096803  0.25535128], action=1, reward=1.0, next_state=[ 0.04955994  0.04414468 -0.005861   -0.04077086]\n",
      "[ episode 45 ][ timestamp 7 ] state=[ 0.04955994  0.04414468 -0.005861   -0.04077086], action=0, reward=1.0, next_state=[ 0.05044284 -0.15089274 -0.00667642  0.25005712]\n",
      "[ episode 45 ][ timestamp 8 ] state=[ 0.05044284 -0.15089274 -0.00667642  0.25005712], action=1, reward=1.0, next_state=[ 0.04742498  0.04432392 -0.00167527 -0.04472419]\n",
      "[ episode 45 ][ timestamp 9 ] state=[ 0.04742498  0.04432392 -0.00167527 -0.04472419], action=0, reward=1.0, next_state=[ 0.04831146 -0.15077397 -0.00256976  0.2474297 ]\n",
      "[ episode 45 ][ timestamp 10 ] state=[ 0.04831146 -0.15077397 -0.00256976  0.2474297 ], action=0, reward=1.0, next_state=[ 0.04529598 -0.34585913  0.00237884  0.53930096]\n",
      "[ episode 45 ][ timestamp 11 ] state=[ 0.04529598 -0.34585913  0.00237884  0.53930096], action=1, reward=1.0, next_state=[ 0.0383788  -0.1507707   0.01316486  0.24736852]\n",
      "[ episode 45 ][ timestamp 12 ] state=[ 0.0383788  -0.1507707   0.01316486  0.24736852], action=1, reward=1.0, next_state=[ 0.03536339  0.04416079  0.01811223 -0.04113301]\n",
      "[ episode 45 ][ timestamp 13 ] state=[ 0.03536339  0.04416079  0.01811223 -0.04113301], action=0, reward=1.0, next_state=[ 0.0362466  -0.15121614  0.01728957  0.25720902]\n",
      "[ episode 45 ][ timestamp 14 ] state=[ 0.0362466  -0.15121614  0.01728957  0.25720902], action=1, reward=1.0, next_state=[ 0.03322228  0.04365475  0.02243375 -0.02997076]\n",
      "[ episode 45 ][ timestamp 15 ] state=[ 0.03322228  0.04365475  0.02243375 -0.02997076], action=1, reward=1.0, next_state=[ 0.03409537  0.23844793  0.02183433 -0.31549203]\n",
      "[ episode 45 ][ timestamp 16 ] state=[ 0.03409537  0.23844793  0.02183433 -0.31549203], action=0, reward=1.0, next_state=[ 0.03886433  0.04302188  0.01552449 -0.01600404]\n",
      "[ episode 45 ][ timestamp 17 ] state=[ 0.03886433  0.04302188  0.01552449 -0.01600404], action=1, reward=1.0, next_state=[ 0.03972477  0.23791779  0.01520441 -0.30374866]\n",
      "[ episode 45 ][ timestamp 18 ] state=[ 0.03972477  0.23791779  0.01520441 -0.30374866], action=1, reward=1.0, next_state=[ 0.04448313  0.43281979  0.00912944 -0.59159795]\n",
      "[ episode 45 ][ timestamp 19 ] state=[ 0.04448313  0.43281979  0.00912944 -0.59159795], action=0, reward=1.0, next_state=[ 0.05313952  0.23757122 -0.00270252 -0.29605329]\n",
      "[ episode 45 ][ timestamp 20 ] state=[ 0.05313952  0.23757122 -0.00270252 -0.29605329], action=1, reward=1.0, next_state=[ 0.05789095  0.4327316  -0.00862359 -0.58958732]\n",
      "[ episode 45 ][ timestamp 21 ] state=[ 0.05789095  0.4327316  -0.00862359 -0.58958732], action=0, reward=1.0, next_state=[ 0.06654558  0.23773145 -0.02041534 -0.29963328]\n",
      "[ episode 45 ][ timestamp 22 ] state=[ 0.06654558  0.23773145 -0.02041534 -0.29963328], action=1, reward=1.0, next_state=[ 0.07130021  0.43313836 -0.026408   -0.59868426]\n",
      "[ episode 45 ][ timestamp 23 ] state=[ 0.07130021  0.43313836 -0.026408   -0.59868426], action=1, reward=1.0, next_state=[ 0.07996297  0.62861966 -0.03838169 -0.89956702]\n",
      "[ episode 45 ][ timestamp 24 ] state=[ 0.07996297  0.62861966 -0.03838169 -0.89956702], action=1, reward=1.0, next_state=[ 0.09253537  0.82424015 -0.05637303 -1.20406309]\n",
      "[ episode 45 ][ timestamp 25 ] state=[ 0.09253537  0.82424015 -0.05637303 -1.20406309], action=0, reward=1.0, next_state=[ 0.10902017  0.62989041 -0.08045429 -0.92956647]\n",
      "[ episode 45 ][ timestamp 26 ] state=[ 0.10902017  0.62989041 -0.08045429 -0.92956647], action=0, reward=1.0, next_state=[ 0.12161798  0.43594123 -0.09904562 -0.6632118 ]\n",
      "[ episode 45 ][ timestamp 27 ] state=[ 0.12161798  0.43594123 -0.09904562 -0.6632118 ], action=1, reward=1.0, next_state=[ 0.1303368   0.63229145 -0.11230985 -0.98536549]\n",
      "[ episode 45 ][ timestamp 28 ] state=[ 0.1303368   0.63229145 -0.11230985 -0.98536549], action=0, reward=1.0, next_state=[ 0.14298263  0.43883831 -0.13201716 -0.72996368]\n",
      "[ episode 45 ][ timestamp 29 ] state=[ 0.14298263  0.43883831 -0.13201716 -0.72996368], action=1, reward=1.0, next_state=[ 0.1517594   0.63551387 -0.14661644 -1.06111032]\n",
      "[ episode 45 ][ timestamp 30 ] state=[ 0.1517594   0.63551387 -0.14661644 -1.06111032], action=1, reward=1.0, next_state=[ 0.16446968  0.83224081 -0.16783864 -1.39598569]\n",
      "[ episode 45 ][ timestamp 31 ] state=[ 0.16446968  0.83224081 -0.16783864 -1.39598569], action=1, reward=1.0, next_state=[ 0.18111449  1.02900548 -0.19575836 -1.73609854]\n",
      "[ episode 45 ][ timestamp 32 ] state=[ 0.18111449  1.02900548 -0.19575836 -1.73609854], action=1, reward=-1.0, next_state=[ 0.2016946   1.2257471  -0.23048033 -2.08276053]\n",
      "[ Ended! ] Episode 45: Exploration_rate=0.8020760579717637. Score=32.\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ] state=[-0.01691799 -0.0469115  -0.01121607 -0.00313979]\n",
      "[ episode 46 ][ timestamp 1 ] state=[-0.01691799 -0.0469115  -0.01121607 -0.00313979], action=0, reward=1.0, next_state=[-0.01785622 -0.24187081 -0.01127887  0.28598333]\n",
      "[ episode 46 ][ timestamp 2 ] state=[-0.01785622 -0.24187081 -0.01127887  0.28598333], action=0, reward=1.0, next_state=[-0.02269364 -0.43683011 -0.0055592   0.57508776]\n",
      "[ episode 46 ][ timestamp 3 ] state=[-0.02269364 -0.43683011 -0.0055592   0.57508776], action=0, reward=1.0, next_state=[-0.03143024 -0.63187368  0.00594256  0.86601421]\n",
      "[ episode 46 ][ timestamp 4 ] state=[-0.03143024 -0.63187368  0.00594256  0.86601421], action=0, reward=1.0, next_state=[-0.04406771 -0.82707601  0.02326284  1.16055962]\n",
      "[ episode 46 ][ timestamp 5 ] state=[-0.04406771 -0.82707601  0.02326284  1.16055962], action=0, reward=1.0, next_state=[-0.06060923 -1.02249316  0.04647403  1.4604447 ]\n",
      "[ episode 46 ][ timestamp 6 ] state=[-0.06060923 -1.02249316  0.04647403  1.4604447 ], action=1, reward=1.0, next_state=[-0.0810591  -0.8279708   0.07568293  1.18263465]\n",
      "[ episode 46 ][ timestamp 7 ] state=[-0.0810591  -0.8279708   0.07568293  1.18263465], action=0, reward=1.0, next_state=[-0.09761851 -1.02398885  0.09933562  1.49804959]\n",
      "[ episode 46 ][ timestamp 8 ] state=[-0.09761851 -1.02398885  0.09933562  1.49804959], action=0, reward=1.0, next_state=[-0.11809829 -1.22016775  0.12929661  1.82002395]\n",
      "[ episode 46 ][ timestamp 9 ] state=[-0.11809829 -1.22016775  0.12929661  1.82002395], action=1, reward=1.0, next_state=[-0.14250165 -1.02669817  0.16569709  1.57014934]\n",
      "[ episode 46 ][ timestamp 10 ] state=[-0.14250165 -1.02669817  0.16569709  1.57014934], action=0, reward=1.0, next_state=[-0.16303561 -1.22336499  0.19710008  1.90960147]\n",
      "[ episode 46 ][ timestamp 11 ] state=[-0.16303561 -1.22336499  0.19710008  1.90960147], action=0, reward=-1.0, next_state=[-0.18750291 -1.4199926   0.23529211  2.25640535]\n",
      "[ Ended! ] Episode 46: Exploration_rate=0.798065677681905. Score=11.\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ] state=[ 0.04772537 -0.01466121  0.00476804  0.02696535]\n",
      "[ episode 47 ][ timestamp 1 ] state=[ 0.04772537 -0.01466121  0.00476804  0.02696535], action=1, reward=1.0, next_state=[ 0.04743215  0.18039204  0.00530735 -0.2642094 ]\n",
      "[ episode 47 ][ timestamp 2 ] state=[ 0.04743215  0.18039204  0.00530735 -0.2642094 ], action=1, reward=1.0, next_state=[ 5.10399891e-02  3.75437835e-01  2.31585968e-05 -5.55213625e-01]\n",
      "[ episode 47 ][ timestamp 3 ] state=[ 5.10399891e-02  3.75437835e-01  2.31585968e-05 -5.55213625e-01], action=0, reward=1.0, next_state=[ 0.05854875  0.18031556 -0.01108111 -0.2625234 ]\n",
      "[ episode 47 ][ timestamp 4 ] state=[ 0.05854875  0.18031556 -0.01108111 -0.2625234 ], action=1, reward=1.0, next_state=[ 0.06215506  0.37559392 -0.01633158 -0.55868074]\n",
      "[ episode 47 ][ timestamp 5 ] state=[ 0.06215506  0.37559392 -0.01633158 -0.55868074], action=0, reward=1.0, next_state=[ 0.06966694  0.18070497 -0.0275052  -0.27118758]\n",
      "[ episode 47 ][ timestamp 6 ] state=[ 0.06966694  0.18070497 -0.0275052  -0.27118758], action=1, reward=1.0, next_state=[ 0.07328103  0.3762084  -0.03292895 -0.5724173 ]\n",
      "[ episode 47 ][ timestamp 7 ] state=[ 0.07328103  0.3762084  -0.03292895 -0.5724173 ], action=1, reward=1.0, next_state=[ 0.0808052   0.57177622 -0.04437729 -0.87528937]\n",
      "[ episode 47 ][ timestamp 8 ] state=[ 0.0808052   0.57177622 -0.04437729 -0.87528937], action=1, reward=1.0, next_state=[ 0.09224073  0.76747243 -0.06188308 -1.18158733]\n",
      "[ episode 47 ][ timestamp 9 ] state=[ 0.09224073  0.76747243 -0.06188308 -1.18158733], action=0, reward=1.0, next_state=[ 0.10759018  0.57320584 -0.08551483 -0.90892724]\n",
      "[ episode 47 ][ timestamp 10 ] state=[ 0.10759018  0.57320584 -0.08551483 -0.90892724], action=1, reward=1.0, next_state=[ 0.11905429  0.76937468 -0.10369337 -1.22721597]\n",
      "[ episode 47 ][ timestamp 11 ] state=[ 0.11905429  0.76937468 -0.10369337 -1.22721597], action=1, reward=1.0, next_state=[ 0.13444179  0.96566714 -0.12823769 -1.55050438]\n",
      "[ episode 47 ][ timestamp 12 ] state=[ 0.13444179  0.96566714 -0.12823769 -1.55050438], action=0, reward=1.0, next_state=[ 0.15375513  0.77229567 -0.15924778 -1.30042752]\n",
      "[ episode 47 ][ timestamp 13 ] state=[ 0.15375513  0.77229567 -0.15924778 -1.30042752], action=1, reward=1.0, next_state=[ 0.16920104  0.96903942 -0.18525633 -1.63843022]\n",
      "[ episode 47 ][ timestamp 14 ] state=[ 0.16920104  0.96903942 -0.18525633 -1.63843022], action=1, reward=-1.0, next_state=[ 0.18858183  1.16578698 -0.21802493 -1.98265612]\n",
      "[ Ended! ] Episode 47: Exploration_rate=0.7940753492934954. Score=14.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 48 ] state=[-0.03646513 -0.0295532   0.04321229  0.03916854]\n",
      "[ episode 48 ][ timestamp 1 ] state=[-0.03646513 -0.0295532   0.04321229  0.03916854], action=1, reward=1.0, next_state=[-0.0370562   0.16492331  0.04399566 -0.23957344]\n",
      "[ episode 48 ][ timestamp 2 ] state=[-0.0370562   0.16492331  0.04399566 -0.23957344], action=1, reward=1.0, next_state=[-0.03375773  0.35939005  0.03920419 -0.51806074]\n",
      "[ episode 48 ][ timestamp 3 ] state=[-0.03375773  0.35939005  0.03920419 -0.51806074], action=1, reward=1.0, next_state=[-0.02656993  0.55393872  0.02884298 -0.79813644]\n",
      "[ episode 48 ][ timestamp 4 ] state=[-0.02656993  0.55393872  0.02884298 -0.79813644], action=1, reward=1.0, next_state=[-0.01549116  0.74865333  0.01288025 -1.0816082 ]\n",
      "[ episode 48 ][ timestamp 5 ] state=[-0.01549116  0.74865333  0.01288025 -1.0816082 ], action=0, reward=1.0, next_state=[-5.18090237e-04  5.53363745e-01 -8.75191505e-03 -7.84911443e-01]\n",
      "[ episode 48 ][ timestamp 6 ] state=[-5.18090237e-04  5.53363745e-01 -8.75191505e-03 -7.84911443e-01], action=0, reward=1.0, next_state=[ 0.01054918  0.35836313 -0.02445014 -0.49499476]\n",
      "[ episode 48 ][ timestamp 7 ] state=[ 0.01054918  0.35836313 -0.02445014 -0.49499476], action=1, reward=1.0, next_state=[ 0.01771645  0.55382121 -0.03435004 -0.79528186]\n",
      "[ episode 48 ][ timestamp 8 ] state=[ 0.01771645  0.55382121 -0.03435004 -0.79528186], action=0, reward=1.0, next_state=[ 0.02879287  0.3591871  -0.05025568 -0.51359986]\n",
      "[ episode 48 ][ timestamp 9 ] state=[ 0.02879287  0.3591871  -0.05025568 -0.51359986], action=1, reward=1.0, next_state=[ 0.03597661  0.5549795  -0.06052767 -0.82168661]\n",
      "[ episode 48 ][ timestamp 10 ] state=[ 0.03597661  0.5549795  -0.06052767 -0.82168661], action=0, reward=1.0, next_state=[ 0.0470762   0.36073565 -0.07696141 -0.54863866]\n",
      "[ episode 48 ][ timestamp 11 ] state=[ 0.0470762   0.36073565 -0.07696141 -0.54863866], action=1, reward=1.0, next_state=[ 0.05429092  0.55684958 -0.08793418 -0.86454312]\n",
      "[ episode 48 ][ timestamp 12 ] state=[ 0.05429092  0.55684958 -0.08793418 -0.86454312], action=1, reward=1.0, next_state=[ 0.06542791  0.75305138 -0.10522504 -1.18352806]\n",
      "[ episode 48 ][ timestamp 13 ] state=[ 0.06542791  0.75305138 -0.10522504 -1.18352806], action=0, reward=1.0, next_state=[ 0.08048894  0.55944014 -0.1288956  -0.92559662]\n",
      "[ episode 48 ][ timestamp 14 ] state=[ 0.08048894  0.55944014 -0.1288956  -0.92559662], action=1, reward=1.0, next_state=[ 0.09167774  0.75604512 -0.14740753 -1.25584812]\n",
      "[ episode 48 ][ timestamp 15 ] state=[ 0.09167774  0.75604512 -0.14740753 -1.25584812], action=1, reward=1.0, next_state=[ 0.10679864  0.95271419 -0.1725245  -1.59083351]\n",
      "[ episode 48 ][ timestamp 16 ] state=[ 0.10679864  0.95271419 -0.1725245  -1.59083351], action=0, reward=1.0, next_state=[ 0.12585292  0.76000938 -0.20434117 -1.35653843]\n",
      "[ episode 48 ][ timestamp 17 ] state=[ 0.12585292  0.76000938 -0.20434117 -1.35653843], action=1, reward=-1.0, next_state=[ 0.14105311  0.95702308 -0.23147194 -1.70556973]\n",
      "[ Ended! ] Episode 48: Exploration_rate=0.7901049725470279. Score=17.\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ] state=[-0.02308932  0.04350422 -0.03366331 -0.02491041]\n",
      "[ episode 49 ][ timestamp 1 ] state=[-0.02308932  0.04350422 -0.03366331 -0.02491041], action=1, reward=1.0, next_state=[-0.02221923  0.23909235 -0.03416152 -0.32802154]\n",
      "[ episode 49 ][ timestamp 2 ] state=[-0.02221923  0.23909235 -0.03416152 -0.32802154], action=1, reward=1.0, next_state=[-0.01743739  0.43468357 -0.04072195 -0.63127872]\n",
      "[ episode 49 ][ timestamp 3 ] state=[-0.01743739  0.43468357 -0.04072195 -0.63127872], action=1, reward=1.0, next_state=[-0.00874371  0.63034933 -0.05334752 -0.93650299]\n",
      "[ episode 49 ][ timestamp 4 ] state=[-0.00874371  0.63034933 -0.05334752 -0.93650299], action=1, reward=1.0, next_state=[ 0.00386327  0.82614855 -0.07207758 -1.24546073]\n",
      "[ episode 49 ][ timestamp 5 ] state=[ 0.00386327  0.82614855 -0.07207758 -1.24546073], action=0, reward=1.0, next_state=[ 0.02038624  0.63202141 -0.0969868  -0.97619854]\n",
      "[ episode 49 ][ timestamp 6 ] state=[ 0.02038624  0.63202141 -0.0969868  -0.97619854], action=1, reward=1.0, next_state=[ 0.03302667  0.82830089 -0.11651077 -1.29770356]\n",
      "[ episode 49 ][ timestamp 7 ] state=[ 0.03302667  0.82830089 -0.11651077 -1.29770356], action=0, reward=1.0, next_state=[ 0.04959269  0.6348351  -0.14246484 -1.04364905]\n",
      "[ episode 49 ][ timestamp 8 ] state=[ 0.04959269  0.6348351  -0.14246484 -1.04364905], action=0, reward=1.0, next_state=[ 0.06228939  0.44186254 -0.16333782 -0.79886583]\n",
      "[ episode 49 ][ timestamp 9 ] state=[ 0.06228939  0.44186254 -0.16333782 -0.79886583], action=1, reward=1.0, next_state=[ 0.07112664  0.63880325 -0.17931513 -1.13815306]\n",
      "[ episode 49 ][ timestamp 10 ] state=[ 0.07112664  0.63880325 -0.17931513 -1.13815306], action=0, reward=1.0, next_state=[ 0.08390271  0.44642056 -0.2020782  -0.90664258]\n",
      "[ episode 49 ][ timestamp 11 ] state=[ 0.08390271  0.44642056 -0.2020782  -0.90664258], action=0, reward=-1.0, next_state=[ 0.09283112  0.25452311 -0.22021105 -0.6836611 ]\n",
      "[ Ended! ] Episode 49: Exploration_rate=0.7861544476842928. Score=11.\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ] state=[ 0.04479809 -0.03983746  0.00483124  0.02906465]\n",
      "[ episode 50 ][ timestamp 1 ] state=[ 0.04479809 -0.03983746  0.00483124  0.02906465], action=1, reward=1.0, next_state=[ 0.04400134  0.15521488  0.00541253 -0.26209006]\n",
      "[ episode 50 ][ timestamp 2 ] state=[ 0.04400134  0.15521488  0.00541253 -0.26209006], action=0, reward=1.0, next_state=[ 0.04710564 -0.03998391  0.00017073  0.03229511]\n",
      "[ episode 50 ][ timestamp 3 ] state=[ 0.04710564 -0.03998391  0.00017073  0.03229511], action=0, reward=1.0, next_state=[ 0.04630596 -0.23510831  0.00081663  0.3250319 ]\n",
      "[ episode 50 ][ timestamp 4 ] state=[ 0.04630596 -0.23510831  0.00081663  0.3250319 ], action=1, reward=1.0, next_state=[ 0.04160379 -0.039998    0.00731727  0.03260662]\n",
      "[ episode 50 ][ timestamp 5 ] state=[ 0.04160379 -0.039998    0.00731727  0.03260662], action=0, reward=1.0, next_state=[ 0.04080383 -0.23522411  0.0079694   0.32758921]\n",
      "[ episode 50 ][ timestamp 6 ] state=[ 0.04080383 -0.23522411  0.0079694   0.32758921], action=0, reward=1.0, next_state=[ 0.03609935 -0.43045861  0.01452118  0.62277463]\n",
      "[ episode 50 ][ timestamp 7 ] state=[ 0.03609935 -0.43045861  0.01452118  0.62277463], action=1, reward=1.0, next_state=[ 0.02749018 -0.2355424   0.02697668  0.33470022]\n",
      "[ episode 50 ][ timestamp 8 ] state=[ 0.02749018 -0.2355424   0.02697668  0.33470022], action=0, reward=1.0, next_state=[ 0.02277933 -0.43103769  0.03367068  0.63576664]\n",
      "[ episode 50 ][ timestamp 9 ] state=[ 0.02277933 -0.43103769  0.03367068  0.63576664], action=0, reward=1.0, next_state=[ 0.01415857 -0.62661267  0.04638601  0.93886014]\n",
      "[ episode 50 ][ timestamp 10 ] state=[ 0.01415857 -0.62661267  0.04638601  0.93886014], action=1, reward=1.0, next_state=[ 0.00162632 -0.43214573  0.06516322  0.66110609]\n",
      "[ episode 50 ][ timestamp 11 ] state=[ 0.00162632 -0.43214573  0.06516322  0.66110609], action=0, reward=1.0, next_state=[-0.00701659 -0.628111    0.07838534  0.97357457]\n",
      "[ episode 50 ][ timestamp 12 ] state=[-0.00701659 -0.628111    0.07838534  0.97357457], action=1, reward=1.0, next_state=[-0.01957881 -0.43412324  0.09785683  0.70650809]\n",
      "[ episode 50 ][ timestamp 13 ] state=[-0.01957881 -0.43412324  0.09785683  0.70650809], action=1, reward=1.0, next_state=[-0.02826128 -0.24048342  0.11198699  0.44616199]\n",
      "[ episode 50 ][ timestamp 14 ] state=[-0.02826128 -0.24048342  0.11198699  0.44616199], action=0, reward=1.0, next_state=[-0.03307095 -0.43699676  0.12091023  0.77194096]\n",
      "[ episode 50 ][ timestamp 15 ] state=[-0.03307095 -0.43699676  0.12091023  0.77194096], action=0, reward=1.0, next_state=[-0.04181088 -0.63355653  0.13634905  1.10008913]\n",
      "[ episode 50 ][ timestamp 16 ] state=[-0.04181088 -0.63355653  0.13634905  1.10008913], action=1, reward=1.0, next_state=[-0.05448201 -0.44046643  0.15835083  0.85310465]\n",
      "[ episode 50 ][ timestamp 17 ] state=[-0.05448201 -0.44046643  0.15835083  0.85310465], action=0, reward=1.0, next_state=[-0.06329134 -0.6373514   0.17541293  1.191098  ]\n",
      "[ episode 50 ][ timestamp 18 ] state=[-0.06329134 -0.6373514   0.17541293  1.191098  ], action=1, reward=1.0, next_state=[-0.07603837 -0.4448812   0.19923489  0.95813034]\n",
      "[ episode 50 ][ timestamp 19 ] state=[-0.07603837 -0.4448812   0.19923489  0.95813034], action=1, reward=-1.0, next_state=[-0.08493599 -0.25291452  0.21839749  0.73406476]\n",
      "[ Ended! ] Episode 50: Exploration_rate=0.7822236754458713. Score=19.\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ] state=[0.02209279 0.03323408 0.01676797 0.0412674 ]\n",
      "[ episode 51 ][ timestamp 1 ] state=[0.02209279 0.03323408 0.01676797 0.0412674 ], action=1, reward=1.0, next_state=[ 0.02275747  0.22811162  0.01759332 -0.24607826]\n",
      "[ episode 51 ][ timestamp 2 ] state=[ 0.02275747  0.22811162  0.01759332 -0.24607826], action=1, reward=1.0, next_state=[ 0.02731971  0.42297793  0.01267175 -0.53316033]\n",
      "[ episode 51 ][ timestamp 3 ] state=[ 0.02731971  0.42297793  0.01267175 -0.53316033], action=0, reward=1.0, next_state=[ 0.03577926  0.22768008  0.00200854 -0.23651168]\n",
      "[ episode 51 ][ timestamp 4 ] state=[ 0.03577926  0.22768008  0.00200854 -0.23651168], action=0, reward=1.0, next_state=[ 0.04033287  0.03252949 -0.00272169  0.05680413]\n",
      "[ episode 51 ][ timestamp 5 ] state=[ 0.04033287  0.03252949 -0.00272169  0.05680413], action=1, reward=1.0, next_state=[ 0.04098346  0.22769036 -0.00158561 -0.23673627]\n",
      "[ episode 51 ][ timestamp 6 ] state=[ 0.04098346  0.22769036 -0.00158561 -0.23673627], action=0, reward=1.0, next_state=[ 0.04553726  0.0325911  -0.00632033  0.05544609]\n",
      "[ episode 51 ][ timestamp 7 ] state=[ 0.04553726  0.0325911  -0.00632033  0.05544609], action=1, reward=1.0, next_state=[ 0.04618908  0.2278031  -0.00521141 -0.23922423]\n",
      "[ episode 51 ][ timestamp 8 ] state=[ 0.04618908  0.2278031  -0.00521141 -0.23922423], action=0, reward=1.0, next_state=[ 0.05074515  0.03275598 -0.00999589  0.05181033]\n",
      "[ episode 51 ][ timestamp 9 ] state=[ 0.05074515  0.03275598 -0.00999589  0.05181033], action=1, reward=1.0, next_state=[ 0.05140027  0.22801982 -0.00895969 -0.24400955]\n",
      "[ episode 51 ][ timestamp 10 ] state=[ 0.05140027  0.22801982 -0.00895969 -0.24400955], action=1, reward=1.0, next_state=[ 0.05596066  0.4232686  -0.01383988 -0.53950507]\n",
      "[ episode 51 ][ timestamp 11 ] state=[ 0.05596066  0.4232686  -0.01383988 -0.53950507], action=0, reward=1.0, next_state=[ 0.06442603  0.22834391 -0.02462998 -0.25121483]\n",
      "[ episode 51 ][ timestamp 12 ] state=[ 0.06442603  0.22834391 -0.02462998 -0.25121483], action=0, reward=1.0, next_state=[ 0.06899291  0.03358217 -0.02965428  0.03359869]\n",
      "[ episode 51 ][ timestamp 13 ] state=[ 0.06899291  0.03358217 -0.02965428  0.03359869], action=1, reward=1.0, next_state=[ 0.06966456  0.22911655 -0.0289823  -0.268291  ]\n",
      "[ episode 51 ][ timestamp 14 ] state=[ 0.06966456  0.22911655 -0.0289823  -0.268291  ], action=1, reward=1.0, next_state=[ 0.07424689  0.42463986 -0.03434812 -0.56997242]\n",
      "[ episode 51 ][ timestamp 15 ] state=[ 0.07424689  0.42463986 -0.03434812 -0.56997242], action=0, reward=1.0, next_state=[ 0.08273968  0.23001604 -0.04574757 -0.28830524]\n",
      "[ episode 51 ][ timestamp 16 ] state=[ 0.08273968  0.23001604 -0.04574757 -0.28830524], action=0, reward=1.0, next_state=[ 0.08734     0.03557531 -0.05151368 -0.01039438]\n",
      "[ episode 51 ][ timestamp 17 ] state=[ 0.08734     0.03557531 -0.05151368 -0.01039438], action=0, reward=1.0, next_state=[ 0.08805151 -0.15877147 -0.05172156  0.26560075]\n",
      "[ episode 51 ][ timestamp 18 ] state=[ 0.08805151 -0.15877147 -0.05172156  0.26560075], action=1, reward=1.0, next_state=[ 0.08487608  0.03704906 -0.04640955 -0.04293662]\n",
      "[ episode 51 ][ timestamp 19 ] state=[ 0.08487608  0.03704906 -0.04640955 -0.04293662], action=0, reward=1.0, next_state=[ 0.08561706 -0.15737772 -0.04726828  0.23475004]\n",
      "[ episode 51 ][ timestamp 20 ] state=[ 0.08561706 -0.15737772 -0.04726828  0.23475004], action=0, reward=1.0, next_state=[ 0.08246951 -0.35179357 -0.04257328  0.51215638]\n",
      "[ episode 51 ][ timestamp 21 ] state=[ 0.08246951 -0.35179357 -0.04257328  0.51215638], action=1, reward=1.0, next_state=[ 0.07543364 -0.15609862 -0.03233015  0.20636717]\n",
      "[ episode 51 ][ timestamp 22 ] state=[ 0.07543364 -0.15609862 -0.03233015  0.20636717], action=1, reward=1.0, next_state=[ 0.07231166  0.03947037 -0.02820281 -0.09633643]\n",
      "[ episode 51 ][ timestamp 23 ] state=[ 0.07231166  0.03947037 -0.02820281 -0.09633643], action=1, reward=1.0, next_state=[ 0.07310107  0.23498495 -0.03012954 -0.39778219]\n",
      "[ episode 51 ][ timestamp 24 ] state=[ 0.07310107  0.23498495 -0.03012954 -0.39778219], action=1, reward=1.0, next_state=[ 0.07780077  0.43052111 -0.03808518 -0.69981005]\n",
      "[ episode 51 ][ timestamp 25 ] state=[ 0.07780077  0.43052111 -0.03808518 -0.69981005], action=1, reward=1.0, next_state=[ 0.08641119  0.62614978 -0.05208138 -1.00423461]\n",
      "[ episode 51 ][ timestamp 26 ] state=[ 0.08641119  0.62614978 -0.05208138 -1.00423461], action=1, reward=1.0, next_state=[ 0.09893419  0.82192726 -0.07216608 -1.31280765]\n",
      "[ episode 51 ][ timestamp 27 ] state=[ 0.09893419  0.82192726 -0.07216608 -1.31280765], action=1, reward=1.0, next_state=[ 0.11537273  1.01788481 -0.09842223 -1.62717732]\n",
      "[ episode 51 ][ timestamp 28 ] state=[ 0.11537273  1.01788481 -0.09842223 -1.62717732], action=0, reward=1.0, next_state=[ 0.13573043  0.8240484  -0.13096577 -1.36671927]\n",
      "[ episode 51 ][ timestamp 29 ] state=[ 0.13573043  0.8240484  -0.13096577 -1.36671927], action=0, reward=1.0, next_state=[ 0.1522114   0.63078647 -0.15830016 -1.11770291]\n",
      "[ episode 51 ][ timestamp 30 ] state=[ 0.1522114   0.63078647 -0.15830016 -1.11770291], action=0, reward=1.0, next_state=[ 0.16482713  0.43805535 -0.18065422 -0.87856702]\n",
      "[ episode 51 ][ timestamp 31 ] state=[ 0.16482713  0.43805535 -0.18065422 -0.87856702], action=1, reward=1.0, next_state=[ 0.17358823  0.63511137 -0.19822556 -1.22216473]\n",
      "[ episode 51 ][ timestamp 32 ] state=[ 0.17358823  0.63511137 -0.19822556 -1.22216473], action=1, reward=-1.0, next_state=[ 0.18629046  0.83215616 -0.22266885 -1.56984139]\n",
      "[ Ended! ] Episode 51: Exploration_rate=0.778312557068642. Score=32.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 52 ] state=[ 0.0167798  -0.03774758 -0.00289243  0.00777961]\n",
      "[ episode 52 ][ timestamp 1 ] state=[ 0.0167798  -0.03774758 -0.00289243  0.00777961], action=1, reward=1.0, next_state=[ 0.01602485  0.15741574 -0.00273684 -0.28581451]\n",
      "[ episode 52 ][ timestamp 2 ] state=[ 0.01602485  0.15741574 -0.00273684 -0.28581451], action=0, reward=1.0, next_state=[ 0.01917317 -0.03766708 -0.00845313  0.00600398]\n",
      "[ episode 52 ][ timestamp 3 ] state=[ 0.01917317 -0.03766708 -0.00845313  0.00600398], action=1, reward=1.0, next_state=[ 0.01841982  0.15757508 -0.00833305 -0.28933398]\n",
      "[ episode 52 ][ timestamp 4 ] state=[ 0.01841982  0.15757508 -0.00833305 -0.28933398], action=1, reward=1.0, next_state=[ 0.02157133  0.35281486 -0.01411973 -0.58463337]\n",
      "[ episode 52 ][ timestamp 5 ] state=[ 0.02157133  0.35281486 -0.01411973 -0.58463337], action=0, reward=1.0, next_state=[ 0.02862762  0.15789351 -0.0258124  -0.29643156]\n",
      "[ episode 52 ][ timestamp 6 ] state=[ 0.02862762  0.15789351 -0.0258124  -0.29643156], action=0, reward=1.0, next_state=[ 0.03178549 -0.03685113 -0.03174103 -0.0119999 ]\n",
      "[ episode 52 ][ timestamp 7 ] state=[ 0.03178549 -0.03685113 -0.03174103 -0.0119999 ], action=0, reward=1.0, next_state=[ 0.03104847 -0.23150384 -0.03198103  0.27050179]\n",
      "[ episode 52 ][ timestamp 8 ] state=[ 0.03104847 -0.23150384 -0.03198103  0.27050179], action=0, reward=1.0, next_state=[ 0.02641839 -0.42615517 -0.02657099  0.55292866]\n",
      "[ episode 52 ][ timestamp 9 ] state=[ 0.02641839 -0.42615517 -0.02657099  0.55292866], action=0, reward=1.0, next_state=[ 0.01789529 -0.6208941  -0.01551242  0.837123  ]\n",
      "[ episode 52 ][ timestamp 10 ] state=[ 0.01789529 -0.6208941  -0.01551242  0.837123  ], action=0, reward=1.0, next_state=[ 0.00547741 -0.81580079  0.00123004  1.12488739]\n",
      "[ episode 52 ][ timestamp 11 ] state=[ 0.00547741 -0.81580079  0.00123004  1.12488739], action=0, reward=1.0, next_state=[-0.01083861 -1.01093884  0.02372779  1.41795588]\n",
      "[ episode 52 ][ timestamp 12 ] state=[-0.01083861 -1.01093884  0.02372779  1.41795588], action=1, reward=1.0, next_state=[-0.03105738 -0.81611854  0.05208691  1.132783  ]\n",
      "[ episode 52 ][ timestamp 13 ] state=[-0.03105738 -0.81611854  0.05208691  1.132783  ], action=0, reward=1.0, next_state=[-0.04737975 -1.01188213  0.07474257  1.44133678]\n",
      "[ episode 52 ][ timestamp 14 ] state=[-0.04737975 -1.01188213  0.07474257  1.44133678], action=0, reward=1.0, next_state=[-0.0676174  -1.20784072  0.1035693   1.75640786]\n",
      "[ episode 52 ][ timestamp 15 ] state=[-0.0676174  -1.20784072  0.1035693   1.75640786], action=0, reward=1.0, next_state=[-0.09177421 -1.40397286  0.13869746  2.07942457]\n",
      "[ episode 52 ][ timestamp 16 ] state=[-0.09177421 -1.40397286  0.13869746  2.07942457], action=1, reward=1.0, next_state=[-0.11985367 -1.21050198  0.18028595  1.83265157]\n",
      "[ episode 52 ][ timestamp 17 ] state=[-0.11985367 -1.21050198  0.18028595  1.83265157], action=1, reward=-1.0, next_state=[-0.14406371 -1.01777579  0.21693898  1.60096512]\n",
      "[ Ended! ] Episode 52: Exploration_rate=0.7744209942832988. Score=17.\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ] state=[ 0.00614851 -0.02761336  0.03959487  0.01187264]\n",
      "[ episode 53 ][ timestamp 1 ] state=[ 0.00614851 -0.02761336  0.03959487  0.01187264], action=1, reward=1.0, next_state=[ 0.00559624  0.16691904  0.03983232 -0.2680594 ]\n",
      "[ episode 53 ][ timestamp 2 ] state=[ 0.00559624  0.16691904  0.03983232 -0.2680594 ], action=0, reward=1.0, next_state=[ 0.00893462 -0.02874807  0.03447113  0.03691606]\n",
      "[ episode 53 ][ timestamp 3 ] state=[ 0.00893462 -0.02874807  0.03447113  0.03691606], action=1, reward=1.0, next_state=[ 0.00835966  0.16586304  0.03520945 -0.24469468]\n",
      "[ episode 53 ][ timestamp 4 ] state=[ 0.00835966  0.16586304  0.03520945 -0.24469468], action=1, reward=1.0, next_state=[ 0.01167692  0.36046487  0.03031556 -0.52606706]\n",
      "[ episode 53 ][ timestamp 5 ] state=[ 0.01167692  0.36046487  0.03031556 -0.52606706], action=0, reward=1.0, next_state=[ 0.01888622  0.16492974  0.01979422 -0.22398773]\n",
      "[ episode 53 ][ timestamp 6 ] state=[ 0.01888622  0.16492974  0.01979422 -0.22398773], action=0, reward=1.0, next_state=[ 0.02218481 -0.03046945  0.01531446  0.07487275]\n",
      "[ episode 53 ][ timestamp 7 ] state=[ 0.02218481 -0.03046945  0.01531446  0.07487275], action=0, reward=1.0, next_state=[ 0.02157542 -0.22580756  0.01681192  0.37234784]\n",
      "[ episode 53 ][ timestamp 8 ] state=[ 0.02157542 -0.22580756  0.01681192  0.37234784], action=1, reward=1.0, next_state=[ 0.01705927 -0.03092843  0.02425888  0.08501292]\n",
      "[ episode 53 ][ timestamp 9 ] state=[ 0.01705927 -0.03092843  0.02425888  0.08501292], action=1, reward=1.0, next_state=[ 0.0164407   0.16383754  0.02595913 -0.19991866]\n",
      "[ episode 53 ][ timestamp 10 ] state=[ 0.0164407   0.16383754  0.02595913 -0.19991866], action=1, reward=1.0, next_state=[ 0.01971745  0.35857878  0.02196076 -0.48430097]\n",
      "[ episode 53 ][ timestamp 11 ] state=[ 0.01971745  0.35857878  0.02196076 -0.48430097], action=1, reward=1.0, next_state=[ 0.02688903  0.55338403  0.01227474 -0.76998244]\n",
      "[ episode 53 ][ timestamp 12 ] state=[ 0.02688903  0.55338403  0.01227474 -0.76998244], action=1, reward=1.0, next_state=[ 0.03795671  0.74833492 -0.00312491 -1.05877805]\n",
      "[ episode 53 ][ timestamp 13 ] state=[ 0.03795671  0.74833492 -0.00312491 -1.05877805], action=0, reward=1.0, next_state=[ 0.05292341  0.5532545  -0.02430047 -0.76707758]\n",
      "[ episode 53 ][ timestamp 14 ] state=[ 0.05292341  0.5532545  -0.02430047 -0.76707758], action=0, reward=1.0, next_state=[ 0.0639885   0.35847538 -0.03964202 -0.4821388 ]\n",
      "[ episode 53 ][ timestamp 15 ] state=[ 0.0639885   0.35847538 -0.03964202 -0.4821388 ], action=1, reward=1.0, next_state=[ 0.071158    0.55413379 -0.0492848  -0.78704753]\n",
      "[ episode 53 ][ timestamp 16 ] state=[ 0.071158    0.55413379 -0.0492848  -0.78704753], action=1, reward=1.0, next_state=[ 0.08224068  0.74989687 -0.06502575 -1.09481947]\n",
      "[ episode 53 ][ timestamp 17 ] state=[ 0.08224068  0.74989687 -0.06502575 -1.09481947], action=0, reward=1.0, next_state=[ 0.09723862  0.55568888 -0.08692214 -0.82322724]\n",
      "[ episode 53 ][ timestamp 18 ] state=[ 0.09723862  0.55568888 -0.08692214 -0.82322724], action=0, reward=1.0, next_state=[ 0.1083524   0.36185674 -0.10338668 -0.55909965]\n",
      "[ episode 53 ][ timestamp 19 ] state=[ 0.1083524   0.36185674 -0.10338668 -0.55909965], action=0, reward=1.0, next_state=[ 0.11558953  0.16832631 -0.11456867 -0.30069564]\n",
      "[ episode 53 ][ timestamp 20 ] state=[ 0.11558953  0.16832631 -0.11456867 -0.30069564], action=0, reward=1.0, next_state=[ 0.11895606 -0.02499213 -0.12058259 -0.04622858]\n",
      "[ episode 53 ][ timestamp 21 ] state=[ 0.11895606 -0.02499213 -0.12058259 -0.04622858], action=1, reward=1.0, next_state=[ 0.11845621  0.17163401 -0.12150716 -0.37439158]\n",
      "[ episode 53 ][ timestamp 22 ] state=[ 0.11845621  0.17163401 -0.12150716 -0.37439158], action=0, reward=1.0, next_state=[ 0.12188889 -0.02157134 -0.12899499 -0.12235556]\n",
      "[ episode 53 ][ timestamp 23 ] state=[ 0.12188889 -0.02157134 -0.12899499 -0.12235556], action=1, reward=1.0, next_state=[ 0.12145747  0.17514008 -0.1314421  -0.45279061]\n",
      "[ episode 53 ][ timestamp 24 ] state=[ 0.12145747  0.17514008 -0.1314421  -0.45279061], action=0, reward=1.0, next_state=[ 0.12496027 -0.01790201 -0.14049791 -0.20425806]\n",
      "[ episode 53 ][ timestamp 25 ] state=[ 0.12496027 -0.01790201 -0.14049791 -0.20425806], action=0, reward=1.0, next_state=[ 0.12460223 -0.2107644  -0.14458307  0.0410143 ]\n",
      "[ episode 53 ][ timestamp 26 ] state=[ 0.12460223 -0.2107644  -0.14458307  0.0410143 ], action=0, reward=1.0, next_state=[ 0.12038694 -0.403549   -0.14376279  0.28481447]\n",
      "[ episode 53 ][ timestamp 27 ] state=[ 0.12038694 -0.403549   -0.14376279  0.28481447], action=1, reward=1.0, next_state=[ 0.11231596 -0.20670055 -0.1380665  -0.04953297]\n",
      "[ episode 53 ][ timestamp 28 ] state=[ 0.11231596 -0.20670055 -0.1380665  -0.04953297], action=0, reward=1.0, next_state=[ 0.10818195 -0.39960047 -0.13905716  0.19660073]\n",
      "[ episode 53 ][ timestamp 29 ] state=[ 0.10818195 -0.39960047 -0.13905716  0.19660073], action=0, reward=1.0, next_state=[ 0.10018994 -0.59248781 -0.13512514  0.4423877 ]\n",
      "[ episode 53 ][ timestamp 30 ] state=[ 0.10018994 -0.59248781 -0.13512514  0.4423877 ], action=0, reward=1.0, next_state=[ 0.08834018 -0.78546484 -0.12627739  0.68960861]\n",
      "[ episode 53 ][ timestamp 31 ] state=[ 0.08834018 -0.78546484 -0.12627739  0.68960861], action=1, reward=1.0, next_state=[ 0.07263089 -0.58883772 -0.11248522  0.35998939]\n",
      "[ episode 53 ][ timestamp 32 ] state=[ 0.07263089 -0.58883772 -0.11248522  0.35998939], action=0, reward=1.0, next_state=[ 0.06085413 -0.78219597 -0.10528543  0.61519283]\n",
      "[ episode 53 ][ timestamp 33 ] state=[ 0.06085413 -0.78219597 -0.10528543  0.61519283], action=0, reward=1.0, next_state=[ 0.04521021 -0.97570153 -0.09298157  0.87294715]\n",
      "[ episode 53 ][ timestamp 34 ] state=[ 0.04521021 -0.97570153 -0.09298157  0.87294715], action=0, reward=1.0, next_state=[ 0.02569618 -1.16944448 -0.07552263  1.135009  ]\n",
      "[ episode 53 ][ timestamp 35 ] state=[ 0.02569618 -1.16944448 -0.07552263  1.135009  ], action=1, reward=1.0, next_state=[ 0.00230729 -0.97342002 -0.05282245  0.8196279 ]\n",
      "[ episode 53 ][ timestamp 36 ] state=[ 0.00230729 -0.97342002 -0.05282245  0.8196279 ], action=1, reward=1.0, next_state=[-0.01716111 -0.77761647 -0.03642989  0.51080965]\n",
      "[ episode 53 ][ timestamp 37 ] state=[-0.01716111 -0.77761647 -0.03642989  0.51080965], action=0, reward=1.0, next_state=[-0.03271344 -0.97220681 -0.0262137   0.79179348]\n",
      "[ episode 53 ][ timestamp 38 ] state=[-0.03271344 -0.97220681 -0.0262137   0.79179348], action=0, reward=1.0, next_state=[-0.05215757 -1.16695923 -0.01037783  1.0761158 ]\n",
      "[ episode 53 ][ timestamp 39 ] state=[-0.05215757 -1.16695923 -0.01037783  1.0761158 ], action=0, reward=1.0, next_state=[-0.07549676 -1.36194255  0.01114449  1.365524  ]\n",
      "[ episode 53 ][ timestamp 40 ] state=[-0.07549676 -1.36194255  0.01114449  1.365524  ], action=0, reward=1.0, next_state=[-0.10273561 -1.55720227  0.03845497  1.6616718 ]\n",
      "[ episode 53 ][ timestamp 41 ] state=[-0.10273561 -1.55720227  0.03845497  1.6616718 ], action=0, reward=1.0, next_state=[-0.13387965 -1.75275046  0.0716884   1.96608021]\n",
      "[ episode 53 ][ timestamp 42 ] state=[-0.13387965 -1.75275046  0.0716884   1.96608021], action=1, reward=1.0, next_state=[-0.16893466 -1.55845593  0.11101001  1.69644533]\n",
      "[ episode 53 ][ timestamp 43 ] state=[-0.16893466 -1.55845593  0.11101001  1.69644533], action=0, reward=1.0, next_state=[-0.20010378 -1.75466963  0.14493891  2.0215242 ]\n",
      "[ episode 53 ][ timestamp 44 ] state=[-0.20010378 -1.75466963  0.14493891  2.0215242 ], action=0, reward=1.0, next_state=[-0.23519717 -1.95096577  0.1853694   2.35534409]\n",
      "[ episode 53 ][ timestamp 45 ] state=[-0.23519717 -1.95096577  0.1853694   2.35534409], action=0, reward=-1.0, next_state=[-0.27421649 -2.1472004   0.23247628  2.69884029]\n",
      "[ Ended! ] Episode 53: Exploration_rate=0.7705488893118823. Score=45.\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ] state=[-1.74675948e-02 -2.42826825e-03  1.02452170e-06  6.13554275e-03]\n",
      "[ episode 54 ][ timestamp 1 ] state=[-1.74675948e-02 -2.42826825e-03  1.02452170e-06  6.13554275e-03], action=0, reward=1.0, next_state=[-1.75161601e-02 -1.97550234e-01  1.23735377e-04  2.98818793e-01]\n",
      "[ episode 54 ][ timestamp 2 ] state=[-1.75161601e-02 -1.97550234e-01  1.23735377e-04  2.98818793e-01], action=0, reward=1.0, next_state=[-0.02146716 -0.39267395  0.00610011  0.59154074]\n",
      "[ episode 54 ][ timestamp 3 ] state=[-0.02146716 -0.39267395  0.00610011  0.59154074], action=0, reward=1.0, next_state=[-0.02932064 -0.58788077  0.01793093  0.88613894]\n",
      "[ episode 54 ][ timestamp 4 ] state=[-0.02932064 -0.58788077  0.01793093  0.88613894], action=1, reward=1.0, next_state=[-0.04107826 -0.39300677  0.0356537   0.59914634]\n",
      "[ episode 54 ][ timestamp 5 ] state=[-0.04107826 -0.39300677  0.0356537   0.59914634], action=1, reward=1.0, next_state=[-0.04893839 -0.19840132  0.04763663  0.31790365]\n",
      "[ episode 54 ][ timestamp 6 ] state=[-0.04893839 -0.19840132  0.04763663  0.31790365], action=0, reward=1.0, next_state=[-0.05290642 -0.39416824  0.0539947   0.62522079]\n",
      "[ episode 54 ][ timestamp 7 ] state=[-0.05290642 -0.39416824  0.0539947   0.62522079], action=0, reward=1.0, next_state=[-0.06078979 -0.59000073  0.06649912  0.93440816]\n",
      "[ episode 54 ][ timestamp 8 ] state=[-0.06078979 -0.59000073  0.06649912  0.93440816], action=0, reward=1.0, next_state=[-0.0725898  -0.78595365  0.08518728  1.24722422]\n",
      "[ episode 54 ][ timestamp 9 ] state=[-0.0725898  -0.78595365  0.08518728  1.24722422], action=0, reward=1.0, next_state=[-0.08830887 -0.98205841  0.11013177  1.56532945]\n",
      "[ episode 54 ][ timestamp 10 ] state=[-0.08830887 -0.98205841  0.11013177  1.56532945], action=0, reward=1.0, next_state=[-0.10795004 -1.17831086  0.14143836  1.890238  ]\n",
      "[ episode 54 ][ timestamp 11 ] state=[-0.10795004 -1.17831086  0.14143836  1.890238  ], action=0, reward=1.0, next_state=[-0.13151626 -1.37465742  0.17924312  2.22326121]\n",
      "[ episode 54 ][ timestamp 12 ] state=[-0.13151626 -1.37465742  0.17924312  2.22326121], action=0, reward=-1.0, next_state=[-0.15900941 -1.57097892  0.22370834  2.56544129]\n",
      "[ Ended! ] Episode 54: Exploration_rate=0.7666961448653229. Score=12.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 55 ] state=[-0.01243509 -0.01131522 -0.04438431  0.00068926]\n",
      "[ episode 55 ][ timestamp 1 ] state=[-0.01243509 -0.01131522 -0.04438431  0.00068926], action=1, reward=1.0, next_state=[-0.01266139  0.18441424 -0.04437052 -0.30566049]\n",
      "[ episode 55 ][ timestamp 2 ] state=[-0.01266139  0.18441424 -0.04437052 -0.30566049], action=1, reward=1.0, next_state=[-0.00897311  0.38013947 -0.05048373 -0.61200004]\n",
      "[ episode 55 ][ timestamp 3 ] state=[-0.00897311  0.38013947 -0.05048373 -0.61200004], action=1, reward=1.0, next_state=[-0.00137032  0.57592928 -0.06272373 -0.92014651]\n",
      "[ episode 55 ][ timestamp 4 ] state=[-0.00137032  0.57592928 -0.06272373 -0.92014651], action=1, reward=1.0, next_state=[ 0.01014827  0.77184033 -0.08112666 -1.23186388]\n",
      "[ episode 55 ][ timestamp 5 ] state=[ 0.01014827  0.77184033 -0.08112666 -1.23186388], action=0, reward=1.0, next_state=[ 0.02558508  0.57784999 -0.10576394 -0.9656605 ]\n",
      "[ episode 55 ][ timestamp 6 ] state=[ 0.02558508  0.57784999 -0.10576394 -0.9656605 ], action=0, reward=1.0, next_state=[ 0.03714208  0.38429536 -0.12507715 -0.70798753]\n",
      "[ episode 55 ][ timestamp 7 ] state=[ 0.03714208  0.38429536 -0.12507715 -0.70798753], action=0, reward=1.0, next_state=[ 0.04482798  0.19110753 -0.1392369  -0.45714643]\n",
      "[ episode 55 ][ timestamp 8 ] state=[ 0.04482798  0.19110753 -0.1392369  -0.45714643], action=0, reward=1.0, next_state=[ 0.04865013 -0.00179968 -0.14837983 -0.2113895 ]\n",
      "[ episode 55 ][ timestamp 9 ] state=[ 0.04865013 -0.00179968 -0.14837983 -0.2113895 ], action=0, reward=1.0, next_state=[ 0.04861414 -0.19452299 -0.15260762  0.0310552 ]\n",
      "[ episode 55 ][ timestamp 10 ] state=[ 0.04861414 -0.19452299 -0.15260762  0.0310552 ], action=1, reward=1.0, next_state=[ 0.04472368  0.0024206  -0.15198652 -0.30561957]\n",
      "[ episode 55 ][ timestamp 11 ] state=[ 0.04472368  0.0024206  -0.15198652 -0.30561957], action=0, reward=1.0, next_state=[ 0.04477209 -0.19024569 -0.15809891 -0.06446383]\n",
      "[ episode 55 ][ timestamp 12 ] state=[ 0.04477209 -0.19024569 -0.15809891 -0.06446383], action=0, reward=1.0, next_state=[ 0.04096718 -0.38278929 -0.15938818  0.17446187]\n",
      "[ episode 55 ][ timestamp 13 ] state=[ 0.04096718 -0.38278929 -0.15938818  0.17446187], action=1, reward=1.0, next_state=[ 0.03331139 -0.18578782 -0.15589895 -0.16395669]\n",
      "[ episode 55 ][ timestamp 14 ] state=[ 0.03331139 -0.18578782 -0.15589895 -0.16395669], action=1, reward=1.0, next_state=[ 0.02959564  0.01118237 -0.15917808 -0.50147764]\n",
      "[ episode 55 ][ timestamp 15 ] state=[ 0.02959564  0.01118237 -0.15917808 -0.50147764], action=0, reward=1.0, next_state=[ 0.02981928 -0.18137999 -0.16920763 -0.26288667]\n",
      "[ episode 55 ][ timestamp 16 ] state=[ 0.02981928 -0.18137999 -0.16920763 -0.26288667], action=0, reward=1.0, next_state=[ 0.02619168 -0.37373352 -0.17446537 -0.02798702]\n",
      "[ episode 55 ][ timestamp 17 ] state=[ 0.02619168 -0.37373352 -0.17446537 -0.02798702], action=1, reward=1.0, next_state=[ 0.01871701 -0.1765947  -0.17502511 -0.37023924]\n",
      "[ episode 55 ][ timestamp 18 ] state=[ 0.01871701 -0.1765947  -0.17502511 -0.37023924], action=1, reward=1.0, next_state=[ 0.01518512  0.02052578 -0.18242989 -0.71259766]\n",
      "[ episode 55 ][ timestamp 19 ] state=[ 0.01518512  0.02052578 -0.18242989 -0.71259766], action=0, reward=1.0, next_state=[ 0.01559563 -0.17166456 -0.19668184 -0.48243342]\n",
      "[ episode 55 ][ timestamp 20 ] state=[ 0.01559563 -0.17166456 -0.19668184 -0.48243342], action=0, reward=1.0, next_state=[ 0.01216234 -0.3635462  -0.20633051 -0.25761243]\n",
      "[ episode 55 ][ timestamp 21 ] state=[ 0.01216234 -0.3635462  -0.20633051 -0.25761243], action=0, reward=-1.0, next_state=[ 0.00489142 -0.55521697 -0.21148276 -0.03643617]\n",
      "[ Ended! ] Episode 55: Exploration_rate=0.7628626641409962. Score=21.\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ] state=[-0.0447301   0.04940131 -0.00751866 -0.02033787]\n",
      "[ episode 56 ][ timestamp 1 ] state=[-0.0447301   0.04940131 -0.00751866 -0.02033787], action=0, reward=1.0, next_state=[-0.04374207 -0.14561201 -0.00792542  0.26996338]\n",
      "[ episode 56 ][ timestamp 2 ] state=[-0.04374207 -0.14561201 -0.00792542  0.26996338], action=1, reward=1.0, next_state=[-0.04665431  0.04962213 -0.00252615 -0.0252087 ]\n",
      "[ episode 56 ][ timestamp 3 ] state=[-0.04665431  0.04962213 -0.00252615 -0.0252087 ], action=1, reward=1.0, next_state=[-0.04566187  0.24478022 -0.00303033 -0.31868758]\n",
      "[ episode 56 ][ timestamp 4 ] state=[-0.04566187  0.24478022 -0.00303033 -0.31868758], action=1, reward=1.0, next_state=[-0.04076627  0.4399452  -0.00940408 -0.61232462]\n",
      "[ episode 56 ][ timestamp 5 ] state=[-0.04076627  0.4399452  -0.00940408 -0.61232462], action=1, reward=1.0, next_state=[-0.03196736  0.63519731 -0.02165057 -0.90795459]\n",
      "[ episode 56 ][ timestamp 6 ] state=[-0.03196736  0.63519731 -0.02165057 -0.90795459], action=0, reward=1.0, next_state=[-0.01926342  0.44037503 -0.03980966 -0.62215443]\n",
      "[ episode 56 ][ timestamp 7 ] state=[-0.01926342  0.44037503 -0.03980966 -0.62215443], action=1, reward=1.0, next_state=[-0.01045592  0.6360296  -0.05225275 -0.92710471]\n",
      "[ episode 56 ][ timestamp 8 ] state=[-0.01045592  0.6360296  -0.05225275 -0.92710471], action=1, reward=1.0, next_state=[ 0.00226468  0.83181669 -0.07079484 -1.23573983]\n",
      "[ episode 56 ][ timestamp 9 ] state=[ 0.00226468  0.83181669 -0.07079484 -1.23573983], action=0, reward=1.0, next_state=[ 0.01890101  0.63767235 -0.09550964 -0.96604909]\n",
      "[ episode 56 ][ timestamp 10 ] state=[ 0.01890101  0.63767235 -0.09550964 -0.96604909], action=0, reward=1.0, next_state=[ 0.03165446  0.44395391 -0.11483062 -0.70483293]\n",
      "[ episode 56 ][ timestamp 11 ] state=[ 0.03165446  0.44395391 -0.11483062 -0.70483293], action=0, reward=1.0, next_state=[ 0.04053353  0.25059457 -0.12892728 -0.45039011]\n",
      "[ episode 56 ][ timestamp 12 ] state=[ 0.04053353  0.25059457 -0.12892728 -0.45039011], action=1, reward=1.0, next_state=[ 0.04554543  0.44728176 -0.13793508 -0.78077195]\n",
      "[ episode 56 ][ timestamp 13 ] state=[ 0.04554543  0.44728176 -0.13793508 -0.78077195], action=1, reward=1.0, next_state=[ 0.05449106  0.644003   -0.15355052 -1.11347557]\n",
      "[ episode 56 ][ timestamp 14 ] state=[ 0.05449106  0.644003   -0.15355052 -1.11347557], action=0, reward=1.0, next_state=[ 0.06737112  0.45119376 -0.17582003 -0.87263119]\n",
      "[ episode 56 ][ timestamp 15 ] state=[ 0.06737112  0.45119376 -0.17582003 -0.87263119], action=1, reward=1.0, next_state=[ 0.076395    0.64821457 -0.19327266 -1.21503151]\n",
      "[ episode 56 ][ timestamp 16 ] state=[ 0.076395    0.64821457 -0.19327266 -1.21503151], action=1, reward=-1.0, next_state=[ 0.08935929  0.84523136 -0.21757329 -1.56152335]\n",
      "[ Ended! ] Episode 56: Exploration_rate=0.7590483508202912. Score=16.\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ] state=[0.03637915 0.04089196 0.01805071 0.0214861 ]\n",
      "[ episode 57 ][ timestamp 1 ] state=[0.03637915 0.04089196 0.01805071 0.0214861 ], action=1, reward=1.0, next_state=[ 0.03719699  0.23575045  0.01848043 -0.26544741]\n",
      "[ episode 57 ][ timestamp 2 ] state=[ 0.03719699  0.23575045  0.01848043 -0.26544741], action=1, reward=1.0, next_state=[ 0.041912    0.43060383  0.01317148 -0.55224463]\n",
      "[ episode 57 ][ timestamp 3 ] state=[ 0.041912    0.43060383  0.01317148 -0.55224463], action=0, reward=1.0, next_state=[ 0.05052407  0.2352994   0.00212659 -0.25544109]\n",
      "[ episode 57 ][ timestamp 4 ] state=[ 0.05052407  0.2352994   0.00212659 -0.25544109], action=1, reward=1.0, next_state=[ 0.05523006  0.43039092 -0.00298223 -0.5474525 ]\n",
      "[ episode 57 ][ timestamp 5 ] state=[ 0.05523006  0.43039092 -0.00298223 -0.5474525 ], action=1, reward=1.0, next_state=[ 0.06383788  0.62555465 -0.01393128 -0.84107355]\n",
      "[ episode 57 ][ timestamp 6 ] state=[ 0.06383788  0.62555465 -0.01393128 -0.84107355], action=1, reward=1.0, next_state=[ 0.07634897  0.82086398 -0.03075275 -1.13810479]\n",
      "[ episode 57 ][ timestamp 7 ] state=[ 0.07634897  0.82086398 -0.03075275 -1.13810479], action=0, reward=1.0, next_state=[ 0.09276625  0.6261574  -0.05351485 -0.8552229 ]\n",
      "[ episode 57 ][ timestamp 8 ] state=[ 0.09276625  0.6261574  -0.05351485 -0.8552229 ], action=1, reward=1.0, next_state=[ 0.1052894   0.82196621 -0.07061931 -1.1642415 ]\n",
      "[ episode 57 ][ timestamp 9 ] state=[ 0.1052894   0.82196621 -0.07061931 -1.1642415 ], action=1, reward=1.0, next_state=[ 0.12172873  1.01793288 -0.09390414 -1.47820365]\n",
      "[ episode 57 ][ timestamp 10 ] state=[ 0.12172873  1.01793288 -0.09390414 -1.47820365], action=0, reward=1.0, next_state=[ 0.14208738  0.82407457 -0.12346821 -1.21626459]\n",
      "[ episode 57 ][ timestamp 11 ] state=[ 0.14208738  0.82407457 -0.12346821 -1.21626459], action=1, reward=1.0, next_state=[ 0.15856888  1.02055351 -0.1477935  -1.54494694]\n",
      "[ episode 57 ][ timestamp 12 ] state=[ 0.15856888  1.02055351 -0.1477935  -1.54494694], action=1, reward=1.0, next_state=[ 0.17897995  1.21710953 -0.17869244 -1.87986009]\n",
      "[ episode 57 ][ timestamp 13 ] state=[ 0.17897995  1.21710953 -0.17869244 -1.87986009], action=1, reward=-1.0, next_state=[ 0.20332214  1.4136728  -0.21628964 -2.2222666 ]\n",
      "[ Ended! ] Episode 57: Exploration_rate=0.7552531090661897. Score=13.\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ] state=[ 0.00290774  0.02724752 -0.01130739 -0.03319322]\n",
      "[ episode 58 ][ timestamp 1 ] state=[ 0.00290774  0.02724752 -0.01130739 -0.03319322], action=1, reward=1.0, next_state=[ 0.00345269  0.22252979 -0.01197125 -0.32942219]\n",
      "[ episode 58 ][ timestamp 2 ] state=[ 0.00345269  0.22252979 -0.01197125 -0.32942219], action=0, reward=1.0, next_state=[ 0.00790329  0.02758028 -0.01855969 -0.04053835]\n",
      "[ episode 58 ][ timestamp 3 ] state=[ 0.00790329  0.02758028 -0.01855969 -0.04053835], action=0, reward=1.0, next_state=[ 0.00845489 -0.16727068 -0.01937046  0.24623152]\n",
      "[ episode 58 ][ timestamp 4 ] state=[ 0.00845489 -0.16727068 -0.01937046  0.24623152], action=1, reward=1.0, next_state=[ 0.00510948  0.0281225  -0.01444583 -0.05249782]\n",
      "[ episode 58 ][ timestamp 5 ] state=[ 0.00510948  0.0281225  -0.01444583 -0.05249782], action=1, reward=1.0, next_state=[ 0.00567193  0.22344857 -0.01549579 -0.34970329]\n",
      "[ episode 58 ][ timestamp 6 ] state=[ 0.00567193  0.22344857 -0.01549579 -0.34970329], action=0, reward=1.0, next_state=[ 0.0101409   0.02855039 -0.02248985 -0.0619467 ]\n",
      "[ episode 58 ][ timestamp 7 ] state=[ 0.0101409   0.02855039 -0.02248985 -0.0619467 ], action=1, reward=1.0, next_state=[ 0.01071191  0.22398746 -0.02372879 -0.36163962]\n",
      "[ episode 58 ][ timestamp 8 ] state=[ 0.01071191  0.22398746 -0.02372879 -0.36163962], action=0, reward=1.0, next_state=[ 0.01519166  0.02921068 -0.03096158 -0.07653231]\n",
      "[ episode 58 ][ timestamp 9 ] state=[ 0.01519166  0.02921068 -0.03096158 -0.07653231], action=1, reward=1.0, next_state=[ 0.01577587  0.22476249 -0.03249223 -0.3788207 ]\n",
      "[ episode 58 ][ timestamp 10 ] state=[ 0.01577587  0.22476249 -0.03249223 -0.3788207 ], action=1, reward=1.0, next_state=[ 0.02027112  0.42033045 -0.04006864 -0.68156883]\n",
      "[ episode 58 ][ timestamp 11 ] state=[ 0.02027112  0.42033045 -0.04006864 -0.68156883], action=1, reward=1.0, next_state=[ 0.02867773  0.6159853  -0.05370002 -0.98659257]\n",
      "[ episode 58 ][ timestamp 12 ] state=[ 0.02867773  0.6159853  -0.05370002 -0.98659257], action=1, reward=1.0, next_state=[ 0.04099743  0.81178367 -0.07343187 -1.29564697]\n",
      "[ episode 58 ][ timestamp 13 ] state=[ 0.04099743  0.81178367 -0.07343187 -1.29564697], action=0, reward=1.0, next_state=[ 0.05723311  0.61766735 -0.09934481 -1.02682676]\n",
      "[ episode 58 ][ timestamp 14 ] state=[ 0.05723311  0.61766735 -0.09934481 -1.02682676], action=1, reward=1.0, next_state=[ 0.06958645  0.81396138 -0.11988134 -1.34897538]\n",
      "[ episode 58 ][ timestamp 15 ] state=[ 0.06958645  0.81396138 -0.11988134 -1.34897538], action=0, reward=1.0, next_state=[ 0.08586568  0.62053239 -0.14686085 -1.09607506]\n",
      "[ episode 58 ][ timestamp 16 ] state=[ 0.08586568  0.62053239 -0.14686085 -1.09607506], action=1, reward=1.0, next_state=[ 0.09827633  0.81725065 -0.16878235 -1.43099807]\n",
      "[ episode 58 ][ timestamp 17 ] state=[ 0.09827633  0.81725065 -0.16878235 -1.43099807], action=1, reward=1.0, next_state=[ 0.11462134  1.01400563 -0.19740231 -1.77132348]\n",
      "[ episode 58 ][ timestamp 18 ] state=[ 0.11462134  1.01400563 -0.19740231 -1.77132348], action=0, reward=-1.0, next_state=[ 0.13490146  0.82158297 -0.23282878 -1.54595502]\n",
      "[ Ended! ] Episode 58: Exploration_rate=0.7514768435208588. Score=18.\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ] state=[ 0.00533227 -0.04442931 -0.02703673 -0.03291908]\n",
      "[ episode 59 ][ timestamp 1 ] state=[ 0.00533227 -0.04442931 -0.02703673 -0.03291908], action=1, reward=1.0, next_state=[ 0.00444369  0.15106971 -0.02769511 -0.33400827]\n",
      "[ episode 59 ][ timestamp 2 ] state=[ 0.00444369  0.15106971 -0.02769511 -0.33400827], action=0, reward=1.0, next_state=[ 0.00746508 -0.04364734 -0.03437527 -0.05018602]\n",
      "[ episode 59 ][ timestamp 3 ] state=[ 0.00746508 -0.04364734 -0.03437527 -0.05018602], action=1, reward=1.0, next_state=[ 0.00659213  0.15195022 -0.03537899 -0.35351337]\n",
      "[ episode 59 ][ timestamp 4 ] state=[ 0.00659213  0.15195022 -0.03537899 -0.35351337], action=1, reward=1.0, next_state=[ 0.00963114  0.34755692 -0.04244926 -0.65713906]\n",
      "[ episode 59 ][ timestamp 5 ] state=[ 0.00963114  0.34755692 -0.04244926 -0.65713906], action=0, reward=1.0, next_state=[ 0.01658228  0.15305077 -0.05559204 -0.37811901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 59 ][ timestamp 6 ] state=[ 0.01658228  0.15305077 -0.05559204 -0.37811901], action=0, reward=1.0, next_state=[ 0.01964329 -0.04123941 -0.06315442 -0.1034696 ]\n",
      "[ episode 59 ][ timestamp 7 ] state=[ 0.01964329 -0.04123941 -0.06315442 -0.1034696 ], action=1, reward=1.0, next_state=[ 0.0188185   0.15472809 -0.06522381 -0.41538989]\n",
      "[ episode 59 ][ timestamp 8 ] state=[ 0.0188185   0.15472809 -0.06522381 -0.41538989], action=1, reward=1.0, next_state=[ 0.02191306  0.3507109  -0.07353161 -0.72790124]\n",
      "[ episode 59 ][ timestamp 9 ] state=[ 0.02191306  0.3507109  -0.07353161 -0.72790124], action=0, reward=1.0, next_state=[ 0.02892728  0.15667835 -0.08808964 -0.45923771]\n",
      "[ episode 59 ][ timestamp 10 ] state=[ 0.02892728  0.15667835 -0.08808964 -0.45923771], action=1, reward=1.0, next_state=[ 0.03206085  0.35292787 -0.09727439 -0.77833546]\n",
      "[ episode 59 ][ timestamp 11 ] state=[ 0.03206085  0.35292787 -0.09727439 -0.77833546], action=1, reward=1.0, next_state=[ 0.03911941  0.5492432  -0.1128411  -1.09996995]\n",
      "[ episode 59 ][ timestamp 12 ] state=[ 0.03911941  0.5492432  -0.1128411  -1.09996995], action=0, reward=1.0, next_state=[ 0.05010427  0.35577243 -0.1348405  -0.84471437]\n",
      "[ episode 59 ][ timestamp 13 ] state=[ 0.05010427  0.35577243 -0.1348405  -0.84471437], action=0, reward=1.0, next_state=[ 0.05721972  0.16272263 -0.15173479 -0.59729128]\n",
      "[ episode 59 ][ timestamp 14 ] state=[ 0.05721972  0.16272263 -0.15173479 -0.59729128], action=1, reward=1.0, next_state=[ 0.06047417  0.35960568 -0.16368061 -0.93366173]\n",
      "[ episode 59 ][ timestamp 15 ] state=[ 0.06047417  0.35960568 -0.16368061 -0.93366173], action=0, reward=1.0, next_state=[ 0.06766629  0.16702503 -0.18235385 -0.69655925]\n",
      "[ episode 59 ][ timestamp 16 ] state=[ 0.06766629  0.16702503 -0.18235385 -0.69655925], action=1, reward=1.0, next_state=[ 0.07100679  0.36414444 -0.19628503 -1.04065128]\n",
      "[ episode 59 ][ timestamp 17 ] state=[ 0.07100679  0.36414444 -0.19628503 -1.04065128], action=1, reward=-1.0, next_state=[ 0.07828968  0.56125502 -0.21709806 -1.38797769]\n",
      "[ Ended! ] Episode 59: Exploration_rate=0.7477194593032545. Score=17.\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ] state=[-0.02636951  0.00219667 -0.00438661  0.02289945]\n",
      "[ episode 60 ][ timestamp 1 ] state=[-0.02636951  0.00219667 -0.00438661  0.02289945], action=0, reward=1.0, next_state=[-0.02632557 -0.1928621  -0.00392862  0.31419513]\n",
      "[ episode 60 ][ timestamp 2 ] state=[-0.02632557 -0.1928621  -0.00392862  0.31419513], action=1, reward=1.0, next_state=[-0.03018282  0.00231559  0.00235528  0.02027584]\n",
      "[ episode 60 ][ timestamp 3 ] state=[-0.03018282  0.00231559  0.00235528  0.02027584], action=0, reward=1.0, next_state=[-0.0301365  -0.19284006  0.0027608   0.31370095]\n",
      "[ episode 60 ][ timestamp 4 ] state=[-0.0301365  -0.19284006  0.0027608   0.31370095], action=0, reward=1.0, next_state=[-0.03399331 -0.38800123  0.00903482  0.60725327]\n",
      "[ episode 60 ][ timestamp 5 ] state=[-0.03399331 -0.38800123  0.00903482  0.60725327], action=1, reward=1.0, next_state=[-0.04175333 -0.19300676  0.02117989  0.3174297 ]\n",
      "[ episode 60 ][ timestamp 6 ] state=[-0.04175333 -0.19300676  0.02117989  0.3174297 ], action=1, reward=1.0, next_state=[-0.04561347  0.00180722  0.02752848  0.03150069]\n",
      "[ episode 60 ][ timestamp 7 ] state=[-0.04561347  0.00180722  0.02752848  0.03150069], action=1, reward=1.0, next_state=[-0.04557732  0.19652381  0.02815849 -0.25237117]\n",
      "[ episode 60 ][ timestamp 8 ] state=[-0.04557732  0.19652381  0.02815849 -0.25237117], action=0, reward=1.0, next_state=[-0.04164684  0.00101132  0.02311107  0.0490588 ]\n",
      "[ episode 60 ][ timestamp 9 ] state=[-0.04164684  0.00101132  0.02311107  0.0490588 ], action=1, reward=1.0, next_state=[-0.04162662  0.19579439  0.02409225 -0.23624373]\n",
      "[ episode 60 ][ timestamp 10 ] state=[-0.04162662  0.19579439  0.02409225 -0.23624373], action=1, reward=1.0, next_state=[-0.03771073  0.390564    0.01936737 -0.52123092]\n",
      "[ episode 60 ][ timestamp 11 ] state=[-0.03771073  0.390564    0.01936737 -0.52123092], action=0, reward=1.0, next_state=[-0.02989945  0.19517485  0.00894275 -0.22250852]\n",
      "[ episode 60 ][ timestamp 12 ] state=[-0.02989945  0.19517485  0.00894275 -0.22250852], action=1, reward=1.0, next_state=[-0.02599595  0.39016785  0.00449258 -0.51235718]\n",
      "[ episode 60 ][ timestamp 13 ] state=[-0.02599595  0.39016785  0.00449258 -0.51235718], action=1, reward=1.0, next_state=[-0.0181926   0.58522624 -0.00575456 -0.80362099]\n",
      "[ episode 60 ][ timestamp 14 ] state=[-0.0181926   0.58522624 -0.00575456 -0.80362099], action=0, reward=1.0, next_state=[-0.00648807  0.39018366 -0.02182698 -0.5127538 ]\n",
      "[ episode 60 ][ timestamp 15 ] state=[-0.00648807  0.39018366 -0.02182698 -0.5127538 ], action=1, reward=1.0, next_state=[ 0.0013156   0.58560613 -0.03208206 -0.81223431]\n",
      "[ episode 60 ][ timestamp 16 ] state=[ 0.0013156   0.58560613 -0.03208206 -0.81223431], action=0, reward=1.0, next_state=[ 0.01302772  0.39093798 -0.04832674 -0.52981285]\n",
      "[ episode 60 ][ timestamp 17 ] state=[ 0.01302772  0.39093798 -0.04832674 -0.52981285], action=1, reward=1.0, next_state=[ 0.02084648  0.58670527 -0.058923   -0.83732348]\n",
      "[ episode 60 ][ timestamp 18 ] state=[ 0.02084648  0.58670527 -0.058923   -0.83732348], action=1, reward=1.0, next_state=[ 0.03258059  0.78258032 -0.07566947 -1.1479395 ]\n",
      "[ episode 60 ][ timestamp 19 ] state=[ 0.03258059  0.78258032 -0.07566947 -1.1479395 ], action=1, reward=1.0, next_state=[ 0.0482322   0.97860419 -0.09862826 -1.46335949]\n",
      "[ episode 60 ][ timestamp 20 ] state=[ 0.0482322   0.97860419 -0.09862826 -1.46335949], action=0, reward=1.0, next_state=[ 0.06780428  0.78481932 -0.12789545 -1.20304455]\n",
      "[ episode 60 ][ timestamp 21 ] state=[ 0.06780428  0.78481932 -0.12789545 -1.20304455], action=0, reward=1.0, next_state=[ 0.08350067  0.59156159 -0.15195634 -0.95302443]\n",
      "[ episode 60 ][ timestamp 22 ] state=[ 0.08350067  0.59156159 -0.15195634 -0.95302443], action=0, reward=1.0, next_state=[ 0.0953319   0.39877462 -0.17101683 -0.71167968]\n",
      "[ episode 60 ][ timestamp 23 ] state=[ 0.0953319   0.39877462 -0.17101683 -0.71167968], action=0, reward=1.0, next_state=[ 0.10330739  0.20638138 -0.18525042 -0.47733394]\n",
      "[ episode 60 ][ timestamp 24 ] state=[ 0.10330739  0.20638138 -0.18525042 -0.47733394], action=1, reward=1.0, next_state=[ 0.10743502  0.40356928 -0.1947971  -0.82220764]\n",
      "[ episode 60 ][ timestamp 25 ] state=[ 0.10743502  0.40356928 -0.1947971  -0.82220764], action=0, reward=-1.0, next_state=[ 0.1155064   0.21156958 -0.21124125 -0.59656388]\n",
      "[ Ended! ] Episode 60: Exploration_rate=0.7439808620067382. Score=25.\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ] state=[ 0.01692514 -0.01240243  0.03264803  0.00164978]\n",
      "[ episode 61 ][ timestamp 1 ] state=[ 0.01692514 -0.01240243  0.03264803  0.00164978], action=1, reward=1.0, next_state=[ 0.01667709  0.18223646  0.03268103 -0.28055615]\n",
      "[ episode 61 ][ timestamp 2 ] state=[ 0.01667709  0.18223646  0.03268103 -0.28055615], action=1, reward=1.0, next_state=[ 0.02032182  0.37687735  0.0270699  -0.56275508]\n",
      "[ episode 61 ][ timestamp 3 ] state=[ 0.02032182  0.37687735  0.0270699  -0.56275508], action=0, reward=1.0, next_state=[ 0.02785937  0.18138621  0.0158148  -0.26166822]\n",
      "[ episode 61 ][ timestamp 4 ] state=[ 0.02785937  0.18138621  0.0158148  -0.26166822], action=0, reward=1.0, next_state=[ 0.03148709 -0.01395788  0.01058144  0.03596063]\n",
      "[ episode 61 ][ timestamp 5 ] state=[ 0.03148709 -0.01395788  0.01058144  0.03596063], action=1, reward=1.0, next_state=[ 0.03120794  0.18101075  0.01130065 -0.25336505]\n",
      "[ episode 61 ][ timestamp 6 ] state=[ 0.03120794  0.18101075  0.01130065 -0.25336505], action=1, reward=1.0, next_state=[ 0.03482815  0.37596953  0.00623335 -0.54246224]\n",
      "[ episode 61 ][ timestamp 7 ] state=[ 0.03482815  0.37596953  0.00623335 -0.54246224], action=0, reward=1.0, next_state=[ 0.04234754  0.18076053 -0.0046159  -0.24782183]\n",
      "[ episode 61 ][ timestamp 8 ] state=[ 0.04234754  0.18076053 -0.0046159  -0.24782183], action=1, reward=1.0, next_state=[ 0.04596275  0.3759481  -0.00957233 -0.54195713]\n",
      "[ episode 61 ][ timestamp 9 ] state=[ 0.04596275  0.3759481  -0.00957233 -0.54195713], action=1, reward=1.0, next_state=[ 0.05348171  0.57120327 -0.02041148 -0.8376407 ]\n",
      "[ episode 61 ][ timestamp 10 ] state=[ 0.05348171  0.57120327 -0.02041148 -0.8376407 ], action=0, reward=1.0, next_state=[ 0.06490578  0.37636594 -0.03716429 -0.55144613]\n",
      "[ episode 61 ][ timestamp 11 ] state=[ 0.06490578  0.37636594 -0.03716429 -0.55144613], action=0, reward=1.0, next_state=[ 0.0724331   0.18178512 -0.04819321 -0.27070023]\n",
      "[ episode 61 ][ timestamp 12 ] state=[ 0.0724331   0.18178512 -0.04819321 -0.27070023], action=1, reward=1.0, next_state=[ 0.0760688   0.37756047 -0.05360722 -0.57818562]\n",
      "[ episode 61 ][ timestamp 13 ] state=[ 0.0760688   0.37756047 -0.05360722 -0.57818562], action=0, reward=1.0, next_state=[ 0.08362001  0.1832292  -0.06517093 -0.30286043]\n",
      "[ episode 61 ][ timestamp 14 ] state=[ 0.08362001  0.1832292  -0.06517093 -0.30286043], action=1, reward=1.0, next_state=[ 0.08728459  0.37921651 -0.07122814 -0.615364  ]\n",
      "[ episode 61 ][ timestamp 15 ] state=[ 0.08728459  0.37921651 -0.07122814 -0.615364  ], action=0, reward=1.0, next_state=[ 0.09486893  0.18515825 -0.08353542 -0.34593807]\n",
      "[ episode 61 ][ timestamp 16 ] state=[ 0.09486893  0.18515825 -0.08353542 -0.34593807], action=1, reward=1.0, next_state=[ 0.09857209  0.38136296 -0.09045418 -0.66374974]\n",
      "[ episode 61 ][ timestamp 17 ] state=[ 0.09857209  0.38136296 -0.09045418 -0.66374974], action=1, reward=1.0, next_state=[ 0.10619935  0.57761909 -0.10372917 -0.98348772]\n",
      "[ episode 61 ][ timestamp 18 ] state=[ 0.10619935  0.57761909 -0.10372917 -0.98348772], action=0, reward=1.0, next_state=[ 0.11775173  0.38402828 -0.12339893 -0.72510406]\n",
      "[ episode 61 ][ timestamp 19 ] state=[ 0.11775173  0.38402828 -0.12339893 -0.72510406], action=0, reward=1.0, next_state=[ 0.1254323   0.19080919 -0.13790101 -0.47366656]\n",
      "[ episode 61 ][ timestamp 20 ] state=[ 0.1254323   0.19080919 -0.13790101 -0.47366656], action=1, reward=1.0, next_state=[ 0.12924848  0.38758171 -0.14737434 -0.80643783]\n",
      "[ episode 61 ][ timestamp 21 ] state=[ 0.12924848  0.38758171 -0.14737434 -0.80643783], action=0, reward=1.0, next_state=[ 0.13700011  0.19475392 -0.1635031  -0.56350289]\n",
      "[ episode 61 ][ timestamp 22 ] state=[ 0.13700011  0.19475392 -0.1635031  -0.56350289], action=1, reward=1.0, next_state=[ 0.14089519  0.39174683 -0.17477316 -0.90290738]\n",
      "[ episode 61 ][ timestamp 23 ] state=[ 0.14089519  0.39174683 -0.17477316 -0.90290738], action=0, reward=1.0, next_state=[ 0.14873013  0.19936797 -0.1928313  -0.66985725]\n",
      "[ episode 61 ][ timestamp 24 ] state=[ 0.14873013  0.19936797 -0.1928313  -0.66985725], action=0, reward=1.0, next_state=[ 0.15271749  0.00737549 -0.20622845 -0.44354794]\n",
      "[ episode 61 ][ timestamp 25 ] state=[ 0.15271749  0.00737549 -0.20622845 -0.44354794], action=1, reward=-1.0, next_state=[ 0.152865    0.20472711 -0.21509941 -0.79350491]\n",
      "[ Ended! ] Episode 61: Exploration_rate=0.7402609576967045. Score=25.\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ] state=[ 0.03334722  0.03549241 -0.04297203  0.04451563]\n",
      "[ episode 62 ][ timestamp 1 ] state=[ 0.03334722  0.03549241 -0.04297203  0.04451563], action=1, reward=1.0, next_state=[ 0.03405707  0.23120337 -0.04208172 -0.2614097 ]\n",
      "[ episode 62 ][ timestamp 2 ] state=[ 0.03405707  0.23120337 -0.04208172 -0.2614097 ], action=1, reward=1.0, next_state=[ 0.03868114  0.42689998 -0.04730991 -0.5670631 ]\n",
      "[ episode 62 ][ timestamp 3 ] state=[ 0.03868114  0.42689998 -0.04730991 -0.5670631 ], action=1, reward=1.0, next_state=[ 0.04721914  0.62265254 -0.05865117 -0.87426733]\n",
      "[ episode 62 ][ timestamp 4 ] state=[ 0.04721914  0.62265254 -0.05865117 -0.87426733], action=0, reward=1.0, next_state=[ 0.05967219  0.42837494 -0.07613652 -0.60058557]\n",
      "[ episode 62 ][ timestamp 5 ] state=[ 0.05967219  0.42837494 -0.07613652 -0.60058557], action=1, reward=1.0, next_state=[ 0.06823969  0.6244748  -0.08814823 -0.91624574]\n",
      "[ episode 62 ][ timestamp 6 ] state=[ 0.06823969  0.6244748  -0.08814823 -0.91624574], action=0, reward=1.0, next_state=[ 0.08072918  0.43064829 -0.10647315 -0.65251682]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 62 ][ timestamp 7 ] state=[ 0.08072918  0.43064829 -0.10647315 -0.65251682], action=1, reward=1.0, next_state=[ 0.08934215  0.62707924 -0.11952348 -0.97673868]\n",
      "[ episode 62 ][ timestamp 8 ] state=[ 0.08934215  0.62707924 -0.11952348 -0.97673868], action=1, reward=1.0, next_state=[ 0.10188373  0.82358364 -0.13905826 -1.30444866]\n",
      "[ episode 62 ][ timestamp 9 ] state=[ 0.10188373  0.82358364 -0.13905826 -1.30444866], action=1, reward=1.0, next_state=[ 0.11835541  1.02016782 -0.16514723 -1.63722996]\n",
      "[ episode 62 ][ timestamp 10 ] state=[ 0.11835541  1.02016782 -0.16514723 -1.63722996], action=0, reward=1.0, next_state=[ 0.13875876  0.82732293 -0.19789183 -1.40023123]\n",
      "[ episode 62 ][ timestamp 11 ] state=[ 0.13875876  0.82732293 -0.19789183 -1.40023123], action=0, reward=-1.0, next_state=[ 0.15530522  0.63513301 -0.22589645 -1.17537395]\n",
      "[ Ended! ] Episode 62: Exploration_rate=0.736559652908221. Score=11.\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ] state=[ 0.04985646  0.02878747  0.02877438 -0.00208835]\n",
      "[ episode 63 ][ timestamp 1 ] state=[ 0.04985646  0.02878747  0.02877438 -0.00208835], action=0, reward=1.0, next_state=[ 0.05043221 -0.16673508  0.02873261  0.29953257]\n",
      "[ episode 63 ][ timestamp 2 ] state=[ 0.05043221 -0.16673508  0.02873261  0.29953257], action=1, reward=1.0, next_state=[0.04709751 0.02796579 0.03472326 0.01604804]\n",
      "[ episode 63 ][ timestamp 3 ] state=[0.04709751 0.02796579 0.03472326 0.01604804], action=1, reward=1.0, next_state=[ 0.04765683  0.22257301  0.03504422 -0.26548024]\n",
      "[ episode 63 ][ timestamp 4 ] state=[ 0.04765683  0.22257301  0.03504422 -0.26548024], action=0, reward=1.0, next_state=[0.05210829 0.02696886 0.02973462 0.03804672]\n",
      "[ episode 63 ][ timestamp 5 ] state=[0.05210829 0.02696886 0.02973462 0.03804672], action=0, reward=1.0, next_state=[ 0.05264767 -0.16856659  0.03049555  0.33996094]\n",
      "[ episode 63 ][ timestamp 6 ] state=[ 0.05264767 -0.16856659  0.03049555  0.33996094], action=0, reward=1.0, next_state=[ 0.04927633 -0.36410888  0.03729477  0.6421023 ]\n",
      "[ episode 63 ][ timestamp 7 ] state=[ 0.04927633 -0.36410888  0.03729477  0.6421023 ], action=0, reward=1.0, next_state=[ 0.04199416 -0.5597303   0.05013681  0.9462925 ]\n",
      "[ episode 63 ][ timestamp 8 ] state=[ 0.04199416 -0.5597303   0.05013681  0.9462925 ], action=1, reward=1.0, next_state=[ 0.03079955 -0.36531812  0.06906266  0.66977473]\n",
      "[ episode 63 ][ timestamp 9 ] state=[ 0.03079955 -0.36531812  0.06906266  0.66977473], action=1, reward=1.0, next_state=[ 0.02349319 -0.17122092  0.08245816  0.39961126]\n",
      "[ episode 63 ][ timestamp 10 ] state=[ 0.02349319 -0.17122092  0.08245816  0.39961126], action=0, reward=1.0, next_state=[ 0.02006877 -0.36740987  0.09045038  0.71711003]\n",
      "[ episode 63 ][ timestamp 11 ] state=[ 0.02006877 -0.36740987  0.09045038  0.71711003], action=1, reward=1.0, next_state=[ 0.01272057 -0.1736484   0.10479259  0.4542121 ]\n",
      "[ episode 63 ][ timestamp 12 ] state=[ 0.01272057 -0.1736484   0.10479259  0.4542121 ], action=1, reward=1.0, next_state=[0.0092476  0.01984779 0.11387683 0.19631267]\n",
      "[ episode 63 ][ timestamp 13 ] state=[0.0092476  0.01984779 0.11387683 0.19631267], action=1, reward=1.0, next_state=[ 0.00964456  0.21317229  0.11780308 -0.05838837]\n",
      "[ episode 63 ][ timestamp 14 ] state=[ 0.00964456  0.21317229  0.11780308 -0.05838837], action=0, reward=1.0, next_state=[0.01390801 0.01657556 0.11663531 0.26901693]\n",
      "[ episode 63 ][ timestamp 15 ] state=[0.01390801 0.01657556 0.11663531 0.26901693], action=0, reward=1.0, next_state=[ 0.01423952 -0.18000098  0.12201565  0.59609146]\n",
      "[ episode 63 ][ timestamp 16 ] state=[ 0.01423952 -0.18000098  0.12201565  0.59609146], action=0, reward=1.0, next_state=[ 0.0106395  -0.3766002   0.13393748  0.92458147]\n",
      "[ episode 63 ][ timestamp 17 ] state=[ 0.0106395  -0.3766002   0.13393748  0.92458147], action=0, reward=1.0, next_state=[ 0.00310749 -0.57325216  0.15242911  1.25617751]\n",
      "[ episode 63 ][ timestamp 18 ] state=[ 0.00310749 -0.57325216  0.15242911  1.25617751], action=1, reward=1.0, next_state=[-0.00835755 -0.38037419  0.17755266  1.01485595]\n",
      "[ episode 63 ][ timestamp 19 ] state=[-0.00835755 -0.38037419  0.17755266  1.01485595], action=0, reward=1.0, next_state=[-0.01596503 -0.57736221  0.19784978  1.35761933]\n",
      "[ episode 63 ][ timestamp 20 ] state=[-0.01596503 -0.57736221  0.19784978  1.35761933], action=0, reward=-1.0, next_state=[-0.02751228 -0.77433784  0.22500217  1.7051078 ]\n",
      "[ Ended! ] Episode 63: Exploration_rate=0.7328768546436799. Score=20.\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ] state=[ 0.04899117  0.04704899 -0.03408528  0.03670891]\n",
      "[ episode 64 ][ timestamp 1 ] state=[ 0.04899117  0.04704899 -0.03408528  0.03670891], action=1, reward=1.0, next_state=[ 0.04993215  0.24264273 -0.0333511  -0.26653042]\n",
      "[ episode 64 ][ timestamp 2 ] state=[ 0.04993215  0.24264273 -0.0333511  -0.26653042], action=0, reward=1.0, next_state=[ 0.05478501  0.04801225 -0.03868171  0.01544954]\n",
      "[ episode 64 ][ timestamp 3 ] state=[ 0.05478501  0.04801225 -0.03868171  0.01544954], action=1, reward=1.0, next_state=[ 0.05574525  0.24366698 -0.03837272 -0.2891826 ]\n",
      "[ episode 64 ][ timestamp 4 ] state=[ 0.05574525  0.24366698 -0.03837272 -0.2891826 ], action=1, reward=1.0, next_state=[ 0.06061859  0.43931451 -0.04415637 -0.59371668]\n",
      "[ episode 64 ][ timestamp 5 ] state=[ 0.06061859  0.43931451 -0.04415637 -0.59371668], action=1, reward=1.0, next_state=[ 0.06940488  0.63502582 -0.0560307  -0.89997525]\n",
      "[ episode 64 ][ timestamp 6 ] state=[ 0.06940488  0.63502582 -0.0560307  -0.89997525], action=1, reward=1.0, next_state=[ 0.0821054   0.83086047 -0.07403021 -1.20973063]\n",
      "[ episode 64 ][ timestamp 7 ] state=[ 0.0821054   0.83086047 -0.07403021 -1.20973063], action=0, reward=1.0, next_state=[ 0.09872261  0.63676843 -0.09822482 -0.94113501]\n",
      "[ episode 64 ][ timestamp 8 ] state=[ 0.09872261  0.63676843 -0.09822482 -0.94113501], action=0, reward=1.0, next_state=[ 0.11145798  0.44309767 -0.11704752 -0.68086084]\n",
      "[ episode 64 ][ timestamp 9 ] state=[ 0.11145798  0.44309767 -0.11704752 -0.68086084], action=1, reward=1.0, next_state=[ 0.12031993  0.63963403 -0.13066474 -1.0079817 ]\n",
      "[ episode 64 ][ timestamp 10 ] state=[ 0.12031993  0.63963403 -0.13066474 -1.0079817 ], action=1, reward=1.0, next_state=[ 0.13311261  0.83623528 -0.15082437 -1.33867591]\n",
      "[ episode 64 ][ timestamp 11 ] state=[ 0.13311261  0.83623528 -0.15082437 -1.33867591], action=1, reward=1.0, next_state=[ 0.14983732  1.03290005 -0.17759789 -1.67449857]\n",
      "[ episode 64 ][ timestamp 12 ] state=[ 0.14983732  1.03290005 -0.17759789 -1.67449857], action=1, reward=-1.0, next_state=[ 0.17049532  1.22958335 -0.21108786 -2.01682277]\n",
      "[ Ended! ] Episode 64: Exploration_rate=0.7292124703704616. Score=12.\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ] state=[-0.01780628  0.02553079 -0.0044031   0.00808813]\n",
      "[ episode 65 ][ timestamp 1 ] state=[-0.01780628  0.02553079 -0.0044031   0.00808813], action=1, reward=1.0, next_state=[-0.01729566  0.22071561 -0.00424134 -0.28598077]\n",
      "[ episode 65 ][ timestamp 2 ] state=[-0.01729566  0.22071561 -0.00424134 -0.28598077], action=1, reward=1.0, next_state=[-0.01288135  0.4158978  -0.00996096 -0.57999836]\n",
      "[ episode 65 ][ timestamp 3 ] state=[-0.01288135  0.4158978  -0.00996096 -0.57999836], action=1, reward=1.0, next_state=[-0.0045634   0.61115791 -0.02156092 -0.87580247]\n",
      "[ episode 65 ][ timestamp 4 ] state=[-0.0045634   0.61115791 -0.02156092 -0.87580247], action=1, reward=1.0, next_state=[ 0.00765976  0.8065662  -0.03907697 -1.1751852 ]\n",
      "[ episode 65 ][ timestamp 5 ] state=[ 0.00765976  0.8065662  -0.03907697 -1.1751852 ], action=1, reward=1.0, next_state=[ 0.02379109  1.00217352 -0.06258068 -1.47985789]\n",
      "[ episode 65 ][ timestamp 6 ] state=[ 0.02379109  1.00217352 -0.06258068 -1.47985789], action=0, reward=1.0, next_state=[ 0.04383456  0.80786871 -0.09217783 -1.20735793]\n",
      "[ episode 65 ][ timestamp 7 ] state=[ 0.04383456  0.80786871 -0.09217783 -1.20735793], action=1, reward=1.0, next_state=[ 0.05999193  1.00405262 -0.11632499 -1.52744641]\n",
      "[ episode 65 ][ timestamp 8 ] state=[ 0.05999193  1.00405262 -0.11632499 -1.52744641], action=1, reward=1.0, next_state=[ 0.08007298  1.20037015 -0.14687392 -1.85405507]\n",
      "[ episode 65 ][ timestamp 9 ] state=[ 0.08007298  1.20037015 -0.14687392 -1.85405507], action=1, reward=1.0, next_state=[ 0.10408039  1.39676975 -0.18395502 -2.18850848]\n",
      "[ episode 65 ][ timestamp 10 ] state=[ 0.10408039  1.39676975 -0.18395502 -2.18850848], action=0, reward=-1.0, next_state=[ 0.13201578  1.20384446 -0.22772519 -1.95778139]\n",
      "[ Ended! ] Episode 65: Exploration_rate=0.7255664080186093. Score=10.\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ] state=[ 0.01081877 -0.03793902  0.00575893 -0.03352795]\n",
      "[ episode 66 ][ timestamp 1 ] state=[ 0.01081877 -0.03793902  0.00575893 -0.03352795], action=0, reward=1.0, next_state=[ 0.01005999 -0.23314308  0.00508838  0.26096641]\n",
      "[ episode 66 ][ timestamp 2 ] state=[ 0.01005999 -0.23314308  0.00508838  0.26096641], action=0, reward=1.0, next_state=[ 0.00539713 -0.42833729  0.0103077   0.55524992]\n",
      "[ episode 66 ][ timestamp 3 ] state=[ 0.00539713 -0.42833729  0.0103077   0.55524992], action=0, reward=1.0, next_state=[-0.00316962 -0.62360244  0.0214127   0.85116249]\n",
      "[ episode 66 ][ timestamp 4 ] state=[-0.00316962 -0.62360244  0.0214127   0.85116249], action=0, reward=1.0, next_state=[-0.01564167 -0.8190097   0.03843595  1.15050104]\n",
      "[ episode 66 ][ timestamp 5 ] state=[-0.01564167 -0.8190097   0.03843595  1.15050104], action=1, reward=1.0, next_state=[-0.03202186 -0.62440984  0.06144597  0.87011422]\n",
      "[ episode 66 ][ timestamp 6 ] state=[-0.03202186 -0.62440984  0.06144597  0.87011422], action=0, reward=1.0, next_state=[-0.04451006 -0.82031139  0.07884826  1.18146574]\n",
      "[ episode 66 ][ timestamp 7 ] state=[-0.04451006 -0.82031139  0.07884826  1.18146574], action=1, reward=1.0, next_state=[-0.06091629 -0.62629637  0.10247757  0.91450476]\n",
      "[ episode 66 ][ timestamp 8 ] state=[-0.06091629 -0.62629637  0.10247757  0.91450476], action=1, reward=1.0, next_state=[-0.07344221 -0.4326986   0.12076767  0.6557073 ]\n",
      "[ episode 66 ][ timestamp 9 ] state=[-0.07344221 -0.4326986   0.12076767  0.6557073 ], action=1, reward=1.0, next_state=[-0.08209618 -0.23944654  0.13388181  0.40336001]\n",
      "[ episode 66 ][ timestamp 10 ] state=[-0.08209618 -0.23944654  0.13388181  0.40336001], action=1, reward=1.0, next_state=[-0.08688512 -0.04645221  0.14194901  0.15570288]\n",
      "[ episode 66 ][ timestamp 11 ] state=[-0.08688512 -0.04645221  0.14194901  0.15570288], action=0, reward=1.0, next_state=[-0.08781416 -0.24329107  0.14506307  0.48958451]\n",
      "[ episode 66 ][ timestamp 12 ] state=[-0.08781416 -0.24329107  0.14506307  0.48958451], action=0, reward=1.0, next_state=[-0.09267998 -0.44012961  0.15485476  0.82424028]\n",
      "[ episode 66 ][ timestamp 13 ] state=[-0.09267998 -0.44012961  0.15485476  0.82424028], action=1, reward=1.0, next_state=[-0.10148257 -0.24742637  0.17133957  0.58398983]\n",
      "[ episode 66 ][ timestamp 14 ] state=[-0.10148257 -0.24742637  0.17133957  0.58398983], action=0, reward=1.0, next_state=[-0.1064311  -0.44448187  0.18301936  0.92537265]\n",
      "[ episode 66 ][ timestamp 15 ] state=[-0.1064311  -0.44448187  0.18301936  0.92537265], action=0, reward=1.0, next_state=[-0.11532074 -0.64154071  0.20152682  1.26953199]\n",
      "[ episode 66 ][ timestamp 16 ] state=[-0.11532074 -0.64154071  0.20152682  1.26953199], action=1, reward=-1.0, next_state=[-0.12815155 -0.44947965  0.22691746  1.04611942]\n",
      "[ Ended! ] Episode 66: Exploration_rate=0.7219385759785162. Score=16.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 67 ] state=[-0.04848494  0.03431363  0.01201304  0.02812302]\n",
      "[ episode 67 ][ timestamp 1 ] state=[-0.04848494  0.03431363  0.01201304  0.02812302], action=0, reward=1.0, next_state=[-0.04779867 -0.16097852  0.0125755   0.32457185]\n",
      "[ episode 67 ][ timestamp 2 ] state=[-0.04779867 -0.16097852  0.0125755   0.32457185], action=0, reward=1.0, next_state=[-0.05101824 -0.35627725  0.01906694  0.62119388]\n",
      "[ episode 67 ][ timestamp 3 ] state=[-0.05101824 -0.35627725  0.01906694  0.62119388], action=1, reward=1.0, next_state=[-0.05814378 -0.16142668  0.03149082  0.3345765 ]\n",
      "[ episode 67 ][ timestamp 4 ] state=[-0.05814378 -0.16142668  0.03149082  0.3345765 ], action=1, reward=1.0, next_state=[-0.06137231  0.03323326  0.03818235  0.05198813]\n",
      "[ episode 67 ][ timestamp 5 ] state=[-0.06137231  0.03323326  0.03818235  0.05198813], action=1, reward=1.0, next_state=[-0.06070765  0.22778751  0.03922211 -0.22840766]\n",
      "[ episode 67 ][ timestamp 6 ] state=[-0.06070765  0.22778751  0.03922211 -0.22840766], action=1, reward=1.0, next_state=[-0.0561519   0.42232764  0.03465396 -0.50846509]\n",
      "[ episode 67 ][ timestamp 7 ] state=[-0.0561519   0.42232764  0.03465396 -0.50846509], action=1, reward=1.0, next_state=[-0.04770535  0.61694465  0.02448465 -0.79002911]\n",
      "[ episode 67 ][ timestamp 8 ] state=[-0.04770535  0.61694465  0.02448465 -0.79002911], action=1, reward=1.0, next_state=[-0.03536645  0.81172196  0.00868407 -1.07490973]\n",
      "[ episode 67 ][ timestamp 9 ] state=[-0.03536645  0.81172196  0.00868407 -1.07490973], action=1, reward=1.0, next_state=[-0.01913201  1.00672809 -0.01281412 -1.36485481]\n",
      "[ episode 67 ][ timestamp 10 ] state=[-0.01913201  1.00672809 -0.01281412 -1.36485481], action=1, reward=1.0, next_state=[ 1.00254720e-03  1.20200816e+00 -4.01112190e-02 -1.66151812e+00]\n",
      "[ episode 67 ][ timestamp 11 ] state=[ 1.00254720e-03  1.20200816e+00 -4.01112190e-02 -1.66151812e+00], action=1, reward=1.0, next_state=[ 0.02504271  1.39757373 -0.07334158 -1.96642006]\n",
      "[ episode 67 ][ timestamp 12 ] state=[ 0.02504271  1.39757373 -0.07334158 -1.96642006], action=0, reward=1.0, next_state=[ 0.05299419  1.20329973 -0.11266998 -1.69733556]\n",
      "[ episode 67 ][ timestamp 13 ] state=[ 0.05299419  1.20329973 -0.11266998 -1.69733556], action=0, reward=1.0, next_state=[ 0.07706018  1.00964311 -0.14661669 -1.44174739]\n",
      "[ episode 67 ][ timestamp 14 ] state=[ 0.07706018  1.00964311 -0.14661669 -1.44174739], action=1, reward=1.0, next_state=[ 0.09725304  1.20623447 -0.17545164 -1.77642164]\n",
      "[ episode 67 ][ timestamp 15 ] state=[ 0.09725304  1.20623447 -0.17545164 -1.77642164], action=0, reward=-1.0, next_state=[ 0.12137773  1.01346979 -0.21098007 -1.5430322 ]\n",
      "[ Ended! ] Episode 67: Exploration_rate=0.7183288830986236. Score=15.\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ] state=[ 0.01233761 -0.04146722 -0.01014843 -0.01164619]\n",
      "[ episode 68 ][ timestamp 1 ] state=[ 0.01233761 -0.04146722 -0.01014843 -0.01164619], action=1, reward=1.0, next_state=[ 0.01150827  0.15379879 -0.01038136 -0.30751371]\n",
      "[ episode 68 ][ timestamp 2 ] state=[ 0.01150827  0.15379879 -0.01038136 -0.30751371], action=1, reward=1.0, next_state=[ 0.01458425  0.34906712 -0.01653163 -0.60345248]\n",
      "[ episode 68 ][ timestamp 3 ] state=[ 0.01458425  0.34906712 -0.01653163 -0.60345248], action=1, reward=1.0, next_state=[ 0.02156559  0.54441634 -0.02860068 -0.90129634]\n",
      "[ episode 68 ][ timestamp 4 ] state=[ 0.02156559  0.54441634 -0.02860068 -0.90129634], action=1, reward=1.0, next_state=[ 0.03245392  0.73991388 -0.04662661 -1.20283018]\n",
      "[ episode 68 ][ timestamp 5 ] state=[ 0.03245392  0.73991388 -0.04662661 -1.20283018], action=0, reward=1.0, next_state=[ 0.04725219  0.54542478 -0.07068321 -0.92511685]\n",
      "[ episode 68 ][ timestamp 6 ] state=[ 0.04725219  0.54542478 -0.07068321 -0.92511685], action=0, reward=1.0, next_state=[ 0.05816069  0.35132502 -0.08918555 -0.65545779]\n",
      "[ episode 68 ][ timestamp 7 ] state=[ 0.05816069  0.35132502 -0.08918555 -0.65545779], action=0, reward=1.0, next_state=[ 0.06518719  0.1575505  -0.1022947  -0.39213701]\n",
      "[ episode 68 ][ timestamp 8 ] state=[ 0.06518719  0.1575505  -0.1022947  -0.39213701], action=1, reward=1.0, next_state=[ 0.0683382   0.35396411 -0.11013744 -0.7152395 ]\n",
      "[ episode 68 ][ timestamp 9 ] state=[ 0.0683382   0.35396411 -0.11013744 -0.7152395 ], action=1, reward=1.0, next_state=[ 0.07541748  0.55042432 -0.12444223 -1.04045928]\n",
      "[ episode 68 ][ timestamp 10 ] state=[ 0.07541748  0.55042432 -0.12444223 -1.04045928], action=0, reward=1.0, next_state=[ 0.08642597  0.35715546 -0.14525142 -0.78928946]\n",
      "[ episode 68 ][ timestamp 11 ] state=[ 0.08642597  0.35715546 -0.14525142 -0.78928946], action=1, reward=1.0, next_state=[ 0.09356908  0.55394173 -0.16103721 -1.12391439]\n",
      "[ episode 68 ][ timestamp 12 ] state=[ 0.09356908  0.55394173 -0.16103721 -1.12391439], action=0, reward=1.0, next_state=[ 0.10464791  0.36125444 -0.1835155  -0.88576366]\n",
      "[ episode 68 ][ timestamp 13 ] state=[ 0.10464791  0.36125444 -0.1835155  -0.88576366], action=1, reward=1.0, next_state=[ 0.111873    0.55832968 -0.20123077 -1.23006389]\n",
      "[ episode 68 ][ timestamp 14 ] state=[ 0.111873    0.55832968 -0.20123077 -1.23006389], action=1, reward=-1.0, next_state=[ 0.12303959  0.75538921 -0.22583205 -1.57845195]\n",
      "[ Ended! ] Episode 68: Exploration_rate=0.7147372386831305. Score=14.\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ] state=[-0.00460194 -0.04715056 -0.01054589  0.04444543]\n",
      "[ episode 69 ][ timestamp 1 ] state=[-0.00460194 -0.04715056 -0.01054589  0.04444543], action=0, reward=1.0, next_state=[-0.00554495 -0.24211971 -0.00965698  0.33378246]\n",
      "[ episode 69 ][ timestamp 2 ] state=[-0.00554495 -0.24211971 -0.00965698  0.33378246], action=1, reward=1.0, next_state=[-0.01038734 -0.04686165 -0.00298133  0.03806992]\n",
      "[ episode 69 ][ timestamp 3 ] state=[-0.01038734 -0.04686165 -0.00298133  0.03806992], action=1, reward=1.0, next_state=[-0.01132457  0.14830292 -0.00221994 -0.25555215]\n",
      "[ episode 69 ][ timestamp 4 ] state=[-0.01132457  0.14830292 -0.00221994 -0.25555215], action=0, reward=1.0, next_state=[-0.00835852 -0.04678726 -0.00733098  0.03642974]\n",
      "[ episode 69 ][ timestamp 5 ] state=[-0.00835852 -0.04678726 -0.00733098  0.03642974], action=0, reward=1.0, next_state=[-0.00929426 -0.24180332 -0.00660238  0.32679069]\n",
      "[ episode 69 ][ timestamp 6 ] state=[-0.00929426 -0.24180332 -0.00660238  0.32679069], action=1, reward=1.0, next_state=[-1.41303278e-02 -4.65879966e-02 -6.65702106e-05  3.20329932e-02]\n",
      "[ episode 69 ][ timestamp 7 ] state=[-1.41303278e-02 -4.65879966e-02 -6.65702106e-05  3.20329932e-02], action=1, reward=1.0, next_state=[-0.01506209  0.14853491  0.00057409 -0.26067094]\n",
      "[ episode 69 ][ timestamp 8 ] state=[-0.01506209  0.14853491  0.00057409 -0.26067094], action=1, reward=1.0, next_state=[-0.01209139  0.34364866 -0.00463933 -0.55317273]\n",
      "[ episode 69 ][ timestamp 9 ] state=[-0.01209139  0.34364866 -0.00463933 -0.55317273], action=1, reward=1.0, next_state=[-0.00521842  0.53883545 -0.01570278 -0.84731373]\n",
      "[ episode 69 ][ timestamp 10 ] state=[-0.00521842  0.53883545 -0.01570278 -0.84731373], action=0, reward=1.0, next_state=[ 0.00555829  0.34393118 -0.03264906 -0.5596098 ]\n",
      "[ episode 69 ][ timestamp 11 ] state=[ 0.00555829  0.34393118 -0.03264906 -0.5596098 ], action=1, reward=1.0, next_state=[ 0.01243692  0.53949582 -0.04384125 -0.86239753]\n",
      "[ episode 69 ][ timestamp 12 ] state=[ 0.01243692  0.53949582 -0.04384125 -0.86239753], action=1, reward=1.0, next_state=[ 0.02322683  0.73518641 -0.0610892  -1.16853656]\n",
      "[ episode 69 ][ timestamp 13 ] state=[ 0.02322683  0.73518641 -0.0610892  -1.16853656], action=1, reward=1.0, next_state=[ 0.03793056  0.93104754 -0.08445994 -1.47972929]\n",
      "[ episode 69 ][ timestamp 14 ] state=[ 0.03793056  0.93104754 -0.08445994 -1.47972929], action=1, reward=1.0, next_state=[ 0.05655151  1.12709272 -0.11405452 -1.79755054]\n",
      "[ episode 69 ][ timestamp 15 ] state=[ 0.05655151  1.12709272 -0.11405452 -1.79755054], action=0, reward=1.0, next_state=[ 0.07909337  0.93341712 -0.15000553 -1.54238402]\n",
      "[ episode 69 ][ timestamp 16 ] state=[ 0.07909337  0.93341712 -0.15000553 -1.54238402], action=0, reward=1.0, next_state=[ 0.09776171  0.74038294 -0.18085321 -1.30002076]\n",
      "[ episode 69 ][ timestamp 17 ] state=[ 0.09776171  0.74038294 -0.18085321 -1.30002076], action=1, reward=1.0, next_state=[ 0.11256937  0.93727972 -0.20685363 -1.64343049]\n",
      "[ episode 69 ][ timestamp 18 ] state=[ 0.11256937  0.93727972 -0.20685363 -1.64343049], action=1, reward=-1.0, next_state=[ 0.13131496  1.13413559 -0.23972224 -1.99280162]\n",
      "[ Ended! ] Episode 69: Exploration_rate=0.7111635524897149. Score=18.\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ] state=[ 0.01618143 -0.04584759 -0.00849073 -0.00562426]\n",
      "[ episode 70 ][ timestamp 1 ] state=[ 0.01618143 -0.04584759 -0.00849073 -0.00562426], action=1, reward=1.0, next_state=[ 0.01526448  0.14939509 -0.00860322 -0.30097398]\n",
      "[ episode 70 ][ timestamp 2 ] state=[ 0.01526448  0.14939509 -0.00860322 -0.30097398], action=1, reward=1.0, next_state=[ 0.01825239  0.3446386  -0.01462269 -0.59635772]\n",
      "[ episode 70 ][ timestamp 3 ] state=[ 0.01825239  0.3446386  -0.01462269 -0.59635772], action=1, reward=1.0, next_state=[ 0.02514516  0.53996211 -0.02654985 -0.89361057]\n",
      "[ episode 70 ][ timestamp 4 ] state=[ 0.02514516  0.53996211 -0.02654985 -0.89361057], action=0, reward=1.0, next_state=[ 0.0359444   0.3452101  -0.04442206 -0.60939025]\n",
      "[ episode 70 ][ timestamp 5 ] state=[ 0.0359444   0.3452101  -0.04442206 -0.60939025], action=0, reward=1.0, next_state=[ 0.0428486   0.15073636 -0.05660987 -0.33102321]\n",
      "[ episode 70 ][ timestamp 6 ] state=[ 0.0428486   0.15073636 -0.05660987 -0.33102321], action=1, reward=1.0, next_state=[ 0.04586333  0.34661652 -0.06323033 -0.64100718]\n",
      "[ episode 70 ][ timestamp 7 ] state=[ 0.04586333  0.34661652 -0.06323033 -0.64100718], action=0, reward=1.0, next_state=[ 0.05279566  0.15243037 -0.07605047 -0.36888737]\n",
      "[ episode 70 ][ timestamp 8 ] state=[ 0.05279566  0.15243037 -0.07605047 -0.36888737], action=1, reward=1.0, next_state=[ 0.05584427  0.34854587 -0.08342822 -0.68454763]\n",
      "[ episode 70 ][ timestamp 9 ] state=[ 0.05584427  0.34854587 -0.08342822 -0.68454763], action=0, reward=1.0, next_state=[ 0.06281518  0.15467527 -0.09711917 -0.41925263]\n",
      "[ episode 70 ][ timestamp 10 ] state=[ 0.06281518  0.15467527 -0.09711917 -0.41925263], action=1, reward=1.0, next_state=[ 0.06590869  0.35102959 -0.10550423 -0.74090435]\n",
      "[ episode 70 ][ timestamp 11 ] state=[ 0.06590869  0.35102959 -0.10550423 -0.74090435], action=0, reward=1.0, next_state=[ 0.07292928  0.15751016 -0.12032231 -0.4832    ]\n",
      "[ episode 70 ][ timestamp 12 ] state=[ 0.07292928  0.15751016 -0.12032231 -0.4832    ], action=0, reward=1.0, next_state=[ 0.07607948 -0.03572643 -0.12998631 -0.23073023]\n",
      "[ episode 70 ][ timestamp 13 ] state=[ 0.07607948 -0.03572643 -0.12998631 -0.23073023], action=0, reward=1.0, next_state=[ 0.07536496 -0.22877452 -0.13460092  0.01829052]\n",
      "[ episode 70 ][ timestamp 14 ] state=[ 0.07536496 -0.22877452 -0.13460092  0.01829052], action=0, reward=1.0, next_state=[ 0.07078947 -0.42173511 -0.13423511  0.26566012]\n",
      "[ episode 70 ][ timestamp 15 ] state=[ 0.07078947 -0.42173511 -0.13423511  0.26566012], action=1, reward=1.0, next_state=[ 0.06235476 -0.22497815 -0.1289219  -0.06616698]\n",
      "[ episode 70 ][ timestamp 16 ] state=[ 0.06235476 -0.22497815 -0.1289219  -0.06616698], action=1, reward=1.0, next_state=[ 0.0578552  -0.02826615 -0.13024524 -0.39658436]\n",
      "[ episode 70 ][ timestamp 17 ] state=[ 0.0578552  -0.02826615 -0.13024524 -0.39658436], action=0, reward=1.0, next_state=[ 0.05728988 -0.2213229  -0.13817693 -0.14763594]\n",
      "[ episode 70 ][ timestamp 18 ] state=[ 0.05728988 -0.2213229  -0.13817693 -0.14763594], action=0, reward=1.0, next_state=[ 0.05286342 -0.41422347 -0.14112965  0.09846217]\n",
      "[ episode 70 ][ timestamp 19 ] state=[ 0.05286342 -0.41422347 -0.14112965  0.09846217], action=0, reward=1.0, next_state=[ 0.04457895 -0.60707032 -0.13916041  0.34350193]\n",
      "[ episode 70 ][ timestamp 20 ] state=[ 0.04457895 -0.60707032 -0.13916041  0.34350193], action=1, reward=1.0, next_state=[ 0.03243754 -0.41027131 -0.13229037  0.01037591]\n",
      "[ episode 70 ][ timestamp 21 ] state=[ 0.03243754 -0.41027131 -0.13229037  0.01037591], action=0, reward=1.0, next_state=[ 0.02423212 -0.60327238 -0.13208285  0.25856795]\n",
      "[ episode 70 ][ timestamp 22 ] state=[ 0.02423212 -0.60327238 -0.13208285  0.25856795], action=1, reward=1.0, next_state=[ 0.01216667 -0.4065364  -0.12691149 -0.07268514]\n",
      "[ episode 70 ][ timestamp 23 ] state=[ 0.01216667 -0.4065364  -0.12691149 -0.07268514], action=0, reward=1.0, next_state=[ 0.00403594 -0.59963204 -0.12836519  0.17741698]\n",
      "[ episode 70 ][ timestamp 24 ] state=[ 0.00403594 -0.59963204 -0.12836519  0.17741698], action=1, reward=1.0, next_state=[-0.0079567  -0.40292909 -0.12481685 -0.15284569]\n",
      "[ episode 70 ][ timestamp 25 ] state=[-0.0079567  -0.40292909 -0.12481685 -0.15284569], action=1, reward=1.0, next_state=[-0.01601528 -0.20626148 -0.12787377 -0.48215308]\n",
      "[ episode 70 ][ timestamp 26 ] state=[-0.01601528 -0.20626148 -0.12787377 -0.48215308], action=1, reward=1.0, next_state=[-0.02014051 -0.00958852 -0.13751683 -0.81224638]\n",
      "[ episode 70 ][ timestamp 27 ] state=[-0.02014051 -0.00958852 -0.13751683 -0.81224638], action=0, reward=1.0, next_state=[-0.02033228 -0.2025859  -0.15376176 -0.56578594]\n",
      "[ episode 70 ][ timestamp 28 ] state=[-0.02033228 -0.2025859  -0.15376176 -0.56578594], action=1, reward=1.0, next_state=[-0.024384   -0.00567917 -0.16507748 -0.90268939]\n",
      "[ episode 70 ][ timestamp 29 ] state=[-0.024384   -0.00567917 -0.16507748 -0.90268939], action=1, reward=1.0, next_state=[-0.02449758  0.19124771 -0.18313126 -1.24237672]\n",
      "[ episode 70 ][ timestamp 30 ] state=[-0.02449758  0.19124771 -0.18313126 -1.24237672], action=0, reward=1.0, next_state=[-0.02067263 -0.00111364 -0.2079788  -1.01219977]\n",
      "[ episode 70 ][ timestamp 31 ] state=[-0.02067263 -0.00111364 -0.2079788  -1.01219977], action=0, reward=-1.0, next_state=[-0.0206949  -0.19294608 -0.22822279 -0.79135792]\n",
      "[ Ended! ] Episode 70: Exploration_rate=0.7076077347272662. Score=31.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 71 ] state=[-0.01748718 -0.04908238  0.03700404 -0.04420474]\n",
      "[ episode 71 ][ timestamp 1 ] state=[-0.01748718 -0.04908238  0.03700404 -0.04420474], action=0, reward=1.0, next_state=[-0.01846883 -0.24471487  0.03611995  0.25991982]\n",
      "[ episode 71 ][ timestamp 2 ] state=[-0.01846883 -0.24471487  0.03611995  0.25991982], action=0, reward=1.0, next_state=[-0.02336313 -0.44033334  0.04131835  0.56377309]\n",
      "[ episode 71 ][ timestamp 3 ] state=[-0.02336313 -0.44033334  0.04131835  0.56377309], action=1, reward=1.0, next_state=[-0.03216979 -0.24581476  0.05259381  0.28438838]\n",
      "[ episode 71 ][ timestamp 4 ] state=[-0.03216979 -0.24581476  0.05259381  0.28438838], action=0, reward=1.0, next_state=[-0.03708609 -0.44164584  0.05828157  0.59318429]\n",
      "[ episode 71 ][ timestamp 5 ] state=[-0.03708609 -0.44164584  0.05828157  0.59318429], action=0, reward=1.0, next_state=[-0.045919   -0.63753312  0.07014526  0.90364139]\n",
      "[ episode 71 ][ timestamp 6 ] state=[-0.045919   -0.63753312  0.07014526  0.90364139], action=0, reward=1.0, next_state=[-0.05866967 -0.83353148  0.08821809  1.21752174]\n",
      "[ episode 71 ][ timestamp 7 ] state=[-0.05866967 -0.83353148  0.08821809  1.21752174], action=1, reward=1.0, next_state=[-0.0753403  -0.63965085  0.11256852  0.9537342 ]\n",
      "[ episode 71 ][ timestamp 8 ] state=[-0.0753403  -0.63965085  0.11256852  0.9537342 ], action=0, reward=1.0, next_state=[-0.08813331 -0.83609254  0.13164321  1.27955706]\n",
      "[ episode 71 ][ timestamp 9 ] state=[-0.08813331 -0.83609254  0.13164321  1.27955706], action=0, reward=1.0, next_state=[-0.10485516 -1.03262326  0.15723435  1.61039385]\n",
      "[ episode 71 ][ timestamp 10 ] state=[-0.10485516 -1.03262326  0.15723435  1.61039385], action=1, reward=1.0, next_state=[-0.12550763 -0.8396693   0.18944223  1.37056995]\n",
      "[ episode 71 ][ timestamp 11 ] state=[-0.12550763 -0.8396693   0.18944223  1.37056995], action=1, reward=-1.0, next_state=[-0.14230102 -0.64735358  0.21685362  1.14262081]\n",
      "[ Ended! ] Episode 71: Exploration_rate=0.7040696960536299. Score=11.\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ] state=[ 0.02549114  0.00114762 -0.01028129 -0.00561367]\n",
      "[ episode 72 ][ timestamp 1 ] state=[ 0.02549114  0.00114762 -0.01028129 -0.00561367], action=1, reward=1.0, next_state=[ 0.02551409  0.19641549 -0.01039356 -0.30152265]\n",
      "[ episode 72 ][ timestamp 2 ] state=[ 0.02551409  0.19641549 -0.01039356 -0.30152265], action=1, reward=1.0, next_state=[ 0.0294424   0.39168403 -0.01642401 -0.59746529]\n",
      "[ episode 72 ][ timestamp 3 ] state=[ 0.0294424   0.39168403 -0.01642401 -0.59746529], action=1, reward=1.0, next_state=[ 0.03727608  0.58703191 -0.02837332 -0.89527603]\n",
      "[ episode 72 ][ timestamp 4 ] state=[ 0.03727608  0.58703191 -0.02837332 -0.89527603], action=1, reward=1.0, next_state=[ 0.04901672  0.78252686 -0.04627884 -1.19674106]\n",
      "[ episode 72 ][ timestamp 5 ] state=[ 0.04901672  0.78252686 -0.04627884 -1.19674106], action=1, reward=1.0, next_state=[ 0.06466725  0.97821628 -0.07021366 -1.50356204]\n",
      "[ episode 72 ][ timestamp 6 ] state=[ 0.06466725  0.97821628 -0.07021366 -1.50356204], action=0, reward=1.0, next_state=[ 0.08423158  0.7840132  -0.1002849  -1.23360104]\n",
      "[ episode 72 ][ timestamp 7 ] state=[ 0.08423158  0.7840132  -0.1002849  -1.23360104], action=0, reward=1.0, next_state=[ 0.09991184  0.59031328 -0.12495692 -0.97394534]\n",
      "[ episode 72 ][ timestamp 8 ] state=[ 0.09991184  0.59031328 -0.12495692 -0.97394534], action=0, reward=1.0, next_state=[ 0.11171811  0.39706909 -0.14443583 -0.72298095]\n",
      "[ episode 72 ][ timestamp 9 ] state=[ 0.11171811  0.39706909 -0.14443583 -0.72298095], action=0, reward=1.0, next_state=[ 0.11965949  0.20420883 -0.15889545 -0.47901949]\n",
      "[ episode 72 ][ timestamp 10 ] state=[ 0.11965949  0.20420883 -0.15889545 -0.47901949], action=0, reward=1.0, next_state=[ 0.12374367  0.01164485 -0.16847584 -0.24033114]\n",
      "[ episode 72 ][ timestamp 11 ] state=[ 0.12374367  0.01164485 -0.16847584 -0.24033114], action=1, reward=1.0, next_state=[ 0.12397656  0.20872263 -0.17328246 -0.58106023]\n",
      "[ episode 72 ][ timestamp 12 ] state=[ 0.12397656  0.20872263 -0.17328246 -0.58106023], action=0, reward=1.0, next_state=[ 0.12815102  0.0163979  -0.18490367 -0.34758396]\n",
      "[ episode 72 ][ timestamp 13 ] state=[ 0.12815102  0.0163979  -0.18490367 -0.34758396], action=1, reward=1.0, next_state=[ 0.12847897  0.2136022  -0.19185534 -0.69240052]\n",
      "[ episode 72 ][ timestamp 14 ] state=[ 0.12847897  0.2136022  -0.19185534 -0.69240052], action=1, reward=1.0, next_state=[ 0.13275102  0.41079481 -0.20570335 -1.03882242]\n",
      "[ episode 72 ][ timestamp 15 ] state=[ 0.13275102  0.41079481 -0.20570335 -1.03882242], action=1, reward=-1.0, next_state=[ 0.14096691  0.60796737 -0.2264798  -1.38839713]\n",
      "[ Ended! ] Episode 72: Exploration_rate=0.7005493475733617. Score=15.\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ] state=[-0.01042265 -0.01743934 -0.01469836 -0.00998034]\n",
      "[ episode 73 ][ timestamp 1 ] state=[-0.01042265 -0.01743934 -0.01469836 -0.00998034], action=1, reward=1.0, next_state=[-0.01077143  0.17789029 -0.01489796 -0.30726429]\n",
      "[ episode 73 ][ timestamp 2 ] state=[-0.01077143  0.17789029 -0.01489796 -0.30726429], action=0, reward=1.0, next_state=[-0.00721363 -0.01701624 -0.02104325 -0.01931678]\n",
      "[ episode 73 ][ timestamp 3 ] state=[-0.00721363 -0.01701624 -0.02104325 -0.01931678], action=0, reward=1.0, next_state=[-0.00755395 -0.21183019 -0.02142959  0.26665318]\n",
      "[ episode 73 ][ timestamp 4 ] state=[-0.00755395 -0.21183019 -0.02142959  0.26665318], action=0, reward=1.0, next_state=[-0.01179056 -0.40663984 -0.01609652  0.55250075]\n",
      "[ episode 73 ][ timestamp 5 ] state=[-0.01179056 -0.40663984 -0.01609652  0.55250075], action=0, reward=1.0, next_state=[-0.01992335 -0.60153208 -0.00504651  0.84006907]\n",
      "[ episode 73 ][ timestamp 6 ] state=[-0.01992335 -0.60153208 -0.00504651  0.84006907], action=1, reward=1.0, next_state=[-0.031954   -0.4063416   0.01175487  0.54580341]\n",
      "[ episode 73 ][ timestamp 7 ] state=[-0.031954   -0.4063416   0.01175487  0.54580341], action=0, reward=1.0, next_state=[-0.04008083 -0.60162672  0.02267094  0.84216671]\n",
      "[ episode 73 ][ timestamp 8 ] state=[-0.04008083 -0.60162672  0.02267094  0.84216671], action=0, reward=1.0, next_state=[-0.05211336 -0.79705066  0.03951428  1.14189198]\n",
      "[ episode 73 ][ timestamp 9 ] state=[-0.05211336 -0.79705066  0.03951428  1.14189198], action=1, reward=1.0, next_state=[-0.06805438 -0.60246678  0.06235212  0.86185817]\n",
      "[ episode 73 ][ timestamp 10 ] state=[-0.06805438 -0.60246678  0.06235212  0.86185817], action=1, reward=1.0, next_state=[-0.08010371 -0.40824676  0.07958928  0.58941391]\n",
      "[ episode 73 ][ timestamp 11 ] state=[-0.08010371 -0.40824676  0.07958928  0.58941391], action=0, reward=1.0, next_state=[-0.08826865 -0.60438766  0.09137756  0.90606846]\n",
      "[ episode 73 ][ timestamp 12 ] state=[-0.08826865 -0.60438766  0.09137756  0.90606846], action=0, reward=1.0, next_state=[-0.1003564  -0.80062017  0.10949893  1.22601683]\n",
      "[ episode 73 ][ timestamp 13 ] state=[-0.1003564  -0.80062017  0.10949893  1.22601683], action=1, reward=1.0, next_state=[-0.1163688  -0.60706495  0.13401926  0.9695512 ]\n",
      "[ episode 73 ][ timestamp 14 ] state=[-0.1163688  -0.60706495  0.13401926  0.9695512 ], action=0, reward=1.0, next_state=[-0.1285101  -0.80370657  0.15341029  1.30115248]\n",
      "[ episode 73 ][ timestamp 15 ] state=[-0.1285101  -0.80370657  0.15341029  1.30115248], action=0, reward=1.0, next_state=[-0.14458423 -1.00040582  0.17943334  1.63766213]\n",
      "[ episode 73 ][ timestamp 16 ] state=[-0.14458423 -1.00040582  0.17943334  1.63766213], action=0, reward=-1.0, next_state=[-0.16459235 -1.19712077  0.21218658  1.98046795]\n",
      "[ Ended! ] Episode 73: Exploration_rate=0.697046600835495. Score=16.\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ] state=[ 0.01684989 -0.03914854  0.0286399   0.03665196]\n",
      "[ episode 74 ][ timestamp 1 ] state=[ 0.01684989 -0.03914854  0.0286399   0.03665196], action=1, reward=1.0, next_state=[ 0.01606692  0.15555126  0.02937294 -0.24685898]\n",
      "[ episode 74 ][ timestamp 2 ] state=[ 0.01606692  0.15555126  0.02937294 -0.24685898], action=0, reward=1.0, next_state=[ 0.01917795 -0.03997762  0.02443576  0.05494222]\n",
      "[ episode 74 ][ timestamp 3 ] state=[ 0.01917795 -0.03997762  0.02443576  0.05494222], action=1, reward=1.0, next_state=[ 0.01837839  0.15478559  0.02553461 -0.22993198]\n",
      "[ episode 74 ][ timestamp 4 ] state=[ 0.01837839  0.15478559  0.02553461 -0.22993198], action=1, reward=1.0, next_state=[ 0.0214741   0.34953353  0.02093597 -0.51445229]\n",
      "[ episode 74 ][ timestamp 5 ] state=[ 0.0214741   0.34953353  0.02093597 -0.51445229], action=0, reward=1.0, next_state=[ 0.02846478  0.15412308  0.01064692 -0.21524614]\n",
      "[ episode 74 ][ timestamp 6 ] state=[ 0.02846478  0.15412308  0.01064692 -0.21524614], action=0, reward=1.0, next_state=[ 0.03154724 -0.04114945  0.006342    0.0807762 ]\n",
      "[ episode 74 ][ timestamp 7 ] state=[ 0.03154724 -0.04114945  0.006342    0.0807762 ], action=0, reward=1.0, next_state=[ 0.03072425 -0.23636174  0.00795752  0.37545327]\n",
      "[ episode 74 ][ timestamp 8 ] state=[ 0.03072425 -0.23636174  0.00795752  0.37545327], action=0, reward=1.0, next_state=[ 0.02599701 -0.43159581  0.01546659  0.67063459]\n",
      "[ episode 74 ][ timestamp 9 ] state=[ 0.02599701 -0.43159581  0.01546659  0.67063459], action=1, reward=1.0, next_state=[ 0.0173651  -0.23669226  0.02887928  0.38286124]\n",
      "[ episode 74 ][ timestamp 10 ] state=[ 0.0173651  -0.23669226  0.02887928  0.38286124], action=0, reward=1.0, next_state=[ 0.01263125 -0.43221209  0.03653651  0.68450802]\n",
      "[ episode 74 ][ timestamp 11 ] state=[ 0.01263125 -0.43221209  0.03653651  0.68450802], action=0, reward=1.0, next_state=[ 0.00398701 -0.62782177  0.05022667  0.98846606]\n",
      "[ episode 74 ][ timestamp 12 ] state=[ 0.00398701 -0.62782177  0.05022667  0.98846606], action=1, reward=1.0, next_state=[-0.00856942 -0.43340693  0.06999599  0.711972  ]\n",
      "[ episode 74 ][ timestamp 13 ] state=[-0.00856942 -0.43340693  0.06999599  0.711972  ], action=0, reward=1.0, next_state=[-0.01723756 -0.62942471  0.08423543  1.02584069]\n",
      "[ episode 74 ][ timestamp 14 ] state=[-0.01723756 -0.62942471  0.08423543  1.02584069], action=0, reward=1.0, next_state=[-0.02982606 -0.82556104  0.10475224  1.34373797]\n",
      "[ episode 74 ][ timestamp 15 ] state=[-0.02982606 -0.82556104  0.10475224  1.34373797], action=0, reward=1.0, next_state=[-0.04633728 -1.02183312  0.131627    1.66727316]\n",
      "[ episode 74 ][ timestamp 16 ] state=[-0.04633728 -1.02183312  0.131627    1.66727316], action=0, reward=1.0, next_state=[-0.06677394 -1.21821759  0.16497246  1.99788837]\n",
      "[ episode 74 ][ timestamp 17 ] state=[-0.06677394 -1.21821759  0.16497246  1.99788837], action=0, reward=1.0, next_state=[-0.09113829 -1.41463566  0.20493023  2.33679748]\n",
      "[ episode 74 ][ timestamp 18 ] state=[-0.09113829 -1.41463566  0.20493023  2.33679748], action=0, reward=-1.0, next_state=[-0.11943101 -1.61093607  0.25166618  2.68491543]\n",
      "[ Ended! ] Episode 74: Exploration_rate=0.6935613678313175. Score=18.\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ] state=[-0.02256048  0.02000099  0.01666236 -0.03602144]\n",
      "[ episode 75 ][ timestamp 1 ] state=[-0.02256048  0.02000099  0.01666236 -0.03602144], action=0, reward=1.0, next_state=[-0.02216046 -0.17535589  0.01594194  0.26187172]\n",
      "[ episode 75 ][ timestamp 2 ] state=[-0.02216046 -0.17535589  0.01594194  0.26187172], action=0, reward=1.0, next_state=[-0.02566758 -0.37070174  0.02117937  0.55953998]\n",
      "[ episode 75 ][ timestamp 3 ] state=[-0.02566758 -0.37070174  0.02117937  0.55953998], action=0, reward=1.0, next_state=[-0.03308162 -0.56611446  0.03237017  0.85881959]\n",
      "[ episode 75 ][ timestamp 4 ] state=[-0.03308162 -0.56611446  0.03237017  0.85881959], action=0, reward=1.0, next_state=[-0.04440391 -0.76166205  0.04954656  1.16150248]\n",
      "[ episode 75 ][ timestamp 5 ] state=[-0.04440391 -0.76166205  0.04954656  1.16150248], action=0, reward=1.0, next_state=[-0.05963715 -0.95739309  0.07277661  1.46929948]\n",
      "[ episode 75 ][ timestamp 6 ] state=[-0.05963715 -0.95739309  0.07277661  1.46929948], action=1, reward=1.0, next_state=[-0.07878501 -0.76323315  0.1021626   1.20020793]\n",
      "[ episode 75 ][ timestamp 7 ] state=[-0.07878501 -0.76323315  0.1021626   1.20020793], action=1, reward=1.0, next_state=[-0.09404967 -0.56957024  0.12616676  0.94121182]\n",
      "[ episode 75 ][ timestamp 8 ] state=[-0.09404967 -0.56957024  0.12616676  0.94121182], action=1, reward=1.0, next_state=[-0.10544108 -0.37635367  0.144991    0.69068532]\n",
      "[ episode 75 ][ timestamp 9 ] state=[-0.10544108 -0.37635367  0.144991    0.69068532], action=0, reward=1.0, next_state=[-0.11296815 -0.57315812  0.1588047   1.02527261]\n",
      "[ episode 75 ][ timestamp 10 ] state=[-0.11296815 -0.57315812  0.1588047   1.02527261], action=1, reward=1.0, next_state=[-0.12443131 -0.38046598  0.17931015  0.78636396]\n",
      "[ episode 75 ][ timestamp 11 ] state=[-0.12443131 -0.38046598  0.17931015  0.78636396], action=0, reward=1.0, next_state=[-0.13204063 -0.57753846  0.19503743  1.1296683 ]\n",
      "[ episode 75 ][ timestamp 12 ] state=[-0.13204063 -0.57753846  0.19503743  1.1296683 ], action=1, reward=-1.0, next_state=[-0.1435914  -0.38542991  0.2176308   0.90394709]\n",
      "[ Ended! ] Episode 75: Exploration_rate=0.6900935609921609. Score=12.\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ] state=[-0.00824003 -0.03882844  0.01938159  0.02631033]\n",
      "[ episode 76 ][ timestamp 1 ] state=[-0.00824003 -0.03882844  0.01938159  0.02631033], action=0, reward=1.0, next_state=[-0.0090166 -0.2342229  0.0199078  0.3250448]\n",
      "[ episode 76 ][ timestamp 2 ] state=[-0.0090166 -0.2342229  0.0199078  0.3250448], action=1, reward=1.0, next_state=[-0.01370106 -0.03938998  0.02640869  0.03870583]\n",
      "[ episode 76 ][ timestamp 3 ] state=[-0.01370106 -0.03938998  0.02640869  0.03870583], action=0, reward=1.0, next_state=[-0.01448886 -0.23488048  0.02718281  0.33960259]\n",
      "[ episode 76 ][ timestamp 4 ] state=[-0.01448886 -0.23488048  0.02718281  0.33960259], action=1, reward=1.0, next_state=[-0.01918647 -0.04015565  0.03397486  0.05561401]\n",
      "[ episode 76 ][ timestamp 5 ] state=[-0.01918647 -0.04015565  0.03397486  0.05561401], action=0, reward=1.0, next_state=[-0.01998958 -0.23574786  0.03508714  0.3588197 ]\n",
      "[ episode 76 ][ timestamp 6 ] state=[-0.01998958 -0.23574786  0.03508714  0.3588197 ], action=1, reward=1.0, next_state=[-0.02470454 -0.04114181  0.04226353  0.0774038 ]\n",
      "[ episode 76 ][ timestamp 7 ] state=[-0.02470454 -0.04114181  0.04226353  0.0774038 ], action=1, reward=1.0, next_state=[-0.02552737  0.15334959  0.04381161 -0.201651  ]\n",
      "[ episode 76 ][ timestamp 8 ] state=[-0.02552737  0.15334959  0.04381161 -0.201651  ], action=0, reward=1.0, next_state=[-0.02246038 -0.04237068  0.03977859  0.10452417]\n",
      "[ episode 76 ][ timestamp 9 ] state=[-0.02246038 -0.04237068  0.03977859  0.10452417], action=0, reward=1.0, next_state=[-0.0233078  -0.23803944  0.04186907  0.40948696]\n",
      "[ episode 76 ][ timestamp 10 ] state=[-0.0233078  -0.23803944  0.04186907  0.40948696], action=1, reward=1.0, next_state=[-0.02806859 -0.04353534  0.05005881  0.13029241]\n",
      "[ episode 76 ][ timestamp 11 ] state=[-0.02806859 -0.04353534  0.05005881  0.13029241], action=0, reward=1.0, next_state=[-0.02893929 -0.23933731  0.05266466  0.4383386 ]\n",
      "[ episode 76 ][ timestamp 12 ] state=[-0.02893929 -0.23933731  0.05266466  0.4383386 ], action=1, reward=1.0, next_state=[-0.03372604 -0.04499879  0.06143143  0.16271123]\n",
      "[ episode 76 ][ timestamp 13 ] state=[-0.03372604 -0.04499879  0.06143143  0.16271123], action=1, reward=1.0, next_state=[-0.03462601  0.14919239  0.06468566 -0.1099766 ]\n",
      "[ episode 76 ][ timestamp 14 ] state=[-0.03462601  0.14919239  0.06468566 -0.1099766 ], action=1, reward=1.0, next_state=[-0.03164217  0.34333065  0.06248613 -0.38157063]\n",
      "[ episode 76 ][ timestamp 15 ] state=[-0.03164217  0.34333065  0.06248613 -0.38157063], action=0, reward=1.0, next_state=[-0.02477555  0.14737967  0.05485471 -0.06985882]\n",
      "[ episode 76 ][ timestamp 16 ] state=[-0.02477555  0.14737967  0.05485471 -0.06985882], action=0, reward=1.0, next_state=[-0.02182796 -0.04848406  0.05345754  0.23961406]\n",
      "[ episode 76 ][ timestamp 17 ] state=[-0.02182796 -0.04848406  0.05345754  0.23961406], action=1, reward=1.0, next_state=[-0.02279764  0.14583509  0.05824982 -0.03573925]\n",
      "[ episode 76 ][ timestamp 18 ] state=[-0.02279764  0.14583509  0.05824982 -0.03573925], action=1, reward=1.0, next_state=[-0.01988094  0.34007544  0.05753503 -0.30948986]\n",
      "[ episode 76 ][ timestamp 19 ] state=[-0.01988094  0.34007544  0.05753503 -0.30948986], action=0, reward=1.0, next_state=[-0.01307943  0.14418295  0.05134524  0.00076864]\n",
      "[ episode 76 ][ timestamp 20 ] state=[-0.01307943  0.14418295  0.05134524  0.00076864], action=1, reward=1.0, next_state=[-0.01019577  0.33853237  0.05136061 -0.27528243]\n",
      "[ episode 76 ][ timestamp 21 ] state=[-0.01019577  0.33853237  0.05136061 -0.27528243], action=0, reward=1.0, next_state=[-0.00342512  0.14271668  0.04585496  0.03314716]\n",
      "[ episode 76 ][ timestamp 22 ] state=[-0.00342512  0.14271668  0.04585496  0.03314716], action=0, reward=1.0, next_state=[-0.00057079 -0.05303183  0.0465179   0.33993792]\n",
      "[ episode 76 ][ timestamp 23 ] state=[-0.00057079 -0.05303183  0.0465179   0.33993792], action=1, reward=1.0, next_state=[-0.00163143  0.14139843  0.05331666  0.06227935]\n",
      "[ episode 76 ][ timestamp 24 ] state=[-0.00163143  0.14139843  0.05331666  0.06227935], action=1, reward=1.0, next_state=[ 0.00119654  0.33571701  0.05456225 -0.21311666]\n",
      "[ episode 76 ][ timestamp 25 ] state=[ 0.00119654  0.33571701  0.05456225 -0.21311666], action=1, reward=1.0, next_state=[ 0.00791088  0.53001814  0.05029992 -0.48810129]\n",
      "[ episode 76 ][ timestamp 26 ] state=[ 0.00791088  0.53001814  0.05029992 -0.48810129], action=1, reward=1.0, next_state=[ 0.01851124  0.72439567  0.04053789 -0.76451688]\n",
      "[ episode 76 ][ timestamp 27 ] state=[ 0.01851124  0.72439567  0.04053789 -0.76451688], action=0, reward=1.0, next_state=[ 0.03299916  0.52873961  0.02524755 -0.45935901]\n",
      "[ episode 76 ][ timestamp 28 ] state=[ 0.03299916  0.52873961  0.02524755 -0.45935901], action=0, reward=1.0, next_state=[ 0.04357395  0.33327003  0.01606037 -0.15882611]\n",
      "[ episode 76 ][ timestamp 29 ] state=[ 0.04357395  0.33327003  0.01606037 -0.15882611], action=0, reward=1.0, next_state=[0.05023935 0.13792187 0.01288385 0.13887989]\n",
      "[ episode 76 ][ timestamp 30 ] state=[0.05023935 0.13792187 0.01288385 0.13887989], action=1, reward=1.0, next_state=[ 0.05299779  0.33285695  0.01566145 -0.14971071]\n",
      "[ episode 76 ][ timestamp 31 ] state=[ 0.05299779  0.33285695  0.01566145 -0.14971071], action=1, reward=1.0, next_state=[ 0.05965493  0.52775117  0.01266723 -0.43741192]\n",
      "[ episode 76 ][ timestamp 32 ] state=[ 0.05965493  0.52775117  0.01266723 -0.43741192], action=0, reward=1.0, next_state=[ 0.07020995  0.33245223  0.00391899 -0.14076294]\n",
      "[ episode 76 ][ timestamp 33 ] state=[ 0.07020995  0.33245223  0.00391899 -0.14076294], action=0, reward=1.0, next_state=[0.076859   0.13727437 0.00110374 0.15315378]\n",
      "[ episode 76 ][ timestamp 34 ] state=[0.076859   0.13727437 0.00110374 0.15315378], action=1, reward=1.0, next_state=[ 0.07960448  0.3323805   0.00416681 -0.13918074]\n",
      "[ episode 76 ][ timestamp 35 ] state=[ 0.07960448  0.3323805   0.00416681 -0.13918074], action=1, reward=1.0, next_state=[ 0.08625209  0.52744253  0.0013832  -0.43054619]\n",
      "[ episode 76 ][ timestamp 36 ] state=[ 0.08625209  0.52744253  0.0013832  -0.43054619], action=1, reward=1.0, next_state=[ 0.09680094  0.72254486 -0.00722773 -0.72279276]\n",
      "[ episode 76 ][ timestamp 37 ] state=[ 0.09680094  0.72254486 -0.00722773 -0.72279276], action=0, reward=1.0, next_state=[ 0.11125184  0.52752363 -0.02168358 -0.43239348]\n",
      "[ episode 76 ][ timestamp 38 ] state=[ 0.11125184  0.52752363 -0.02168358 -0.43239348], action=0, reward=1.0, next_state=[ 0.12180231  0.3327153  -0.03033145 -0.14662415]\n",
      "[ episode 76 ][ timestamp 39 ] state=[ 0.12180231  0.3327153  -0.03033145 -0.14662415], action=0, reward=1.0, next_state=[ 0.12845662  0.13804054 -0.03326393  0.13633759]\n",
      "[ episode 76 ][ timestamp 40 ] state=[ 0.12845662  0.13804054 -0.03326393  0.13633759], action=0, reward=1.0, next_state=[ 0.13121743 -0.05658956 -0.03053718  0.41834344]\n",
      "[ episode 76 ][ timestamp 41 ] state=[ 0.13121743 -0.05658956 -0.03053718  0.41834344], action=0, reward=1.0, next_state=[ 0.13008564 -0.25126577 -0.02217031  0.70124507]\n",
      "[ episode 76 ][ timestamp 42 ] state=[ 0.13008564 -0.25126577 -0.02217031  0.70124507], action=1, reward=1.0, next_state=[ 0.12506032 -0.05584363 -0.00814541  0.40166637]\n",
      "[ episode 76 ][ timestamp 43 ] state=[ 0.12506032 -0.05584363 -0.00814541  0.40166637], action=0, reward=1.0, next_state=[ 1.23943451e-01 -2.50849103e-01 -1.12085571e-04  6.91770149e-01]\n",
      "[ episode 76 ][ timestamp 44 ] state=[ 1.23943451e-01 -2.50849103e-01 -1.12085571e-04  6.91770149e-01], action=1, reward=1.0, next_state=[ 0.11892647 -0.0557256   0.01372332  0.39905194]\n",
      "[ episode 76 ][ timestamp 45 ] state=[ 0.11892647 -0.0557256   0.01372332  0.39905194], action=0, reward=1.0, next_state=[ 0.11781196 -0.25103951  0.02170436  0.69602975]\n",
      "[ episode 76 ][ timestamp 46 ] state=[ 0.11781196 -0.25103951  0.02170436  0.69602975], action=0, reward=1.0, next_state=[ 0.11279117 -0.44645565  0.03562495  0.99546549]\n",
      "[ episode 76 ][ timestamp 47 ] state=[ 0.11279117 -0.44645565  0.03562495  0.99546549], action=1, reward=1.0, next_state=[ 0.10386205 -0.25182781  0.05553426  0.7141805 ]\n",
      "[ episode 76 ][ timestamp 48 ] state=[ 0.10386205 -0.25182781  0.05553426  0.7141805 ], action=1, reward=1.0, next_state=[ 0.0988255  -0.05751685  0.06981787  0.43948207]\n",
      "[ episode 76 ][ timestamp 49 ] state=[ 0.0988255  -0.05751685  0.06981787  0.43948207], action=0, reward=1.0, next_state=[ 0.09767516 -0.25355388  0.07860751  0.75333101]\n",
      "[ episode 76 ][ timestamp 50 ] state=[ 0.09767516 -0.25355388  0.07860751  0.75333101], action=1, reward=1.0, next_state=[ 0.09260408 -0.05959868  0.09367413  0.4863834 ]\n",
      "[ episode 76 ][ timestamp 51 ] state=[ 0.09260408 -0.05959868  0.09367413  0.4863834 ], action=1, reward=1.0, next_state=[0.09141211 0.13408529 0.1034018  0.22463112]\n",
      "[ episode 76 ][ timestamp 52 ] state=[0.09141211 0.13408529 0.1034018  0.22463112], action=1, reward=1.0, next_state=[ 0.09409382  0.32758909  0.10789442 -0.03372828]\n",
      "[ episode 76 ][ timestamp 53 ] state=[ 0.09409382  0.32758909  0.10789442 -0.03372828], action=0, reward=1.0, next_state=[0.1006456  0.1310986  0.10721986 0.29095303]\n",
      "[ episode 76 ][ timestamp 54 ] state=[0.1006456  0.1310986  0.10721986 0.29095303], action=0, reward=1.0, next_state=[ 0.10326757 -0.06537581  0.11303892  0.61543452]\n",
      "[ episode 76 ][ timestamp 55 ] state=[ 0.10326757 -0.06537581  0.11303892  0.61543452], action=0, reward=1.0, next_state=[ 0.10196005 -0.26188051  0.12534761  0.94147311]\n",
      "[ episode 76 ][ timestamp 56 ] state=[ 0.10196005 -0.26188051  0.12534761  0.94147311], action=1, reward=1.0, next_state=[ 0.09672244 -0.06865033  0.14417707  0.69065766]\n",
      "[ episode 76 ][ timestamp 57 ] state=[ 0.09672244 -0.06865033  0.14417707  0.69065766], action=0, reward=1.0, next_state=[ 0.09534944 -0.26544733  0.15799022  1.02503172]\n",
      "[ episode 76 ][ timestamp 58 ] state=[ 0.09534944 -0.26544733  0.15799022  1.02503172], action=0, reward=1.0, next_state=[ 0.09004049 -0.46227978  0.17849086  1.36285935]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 76 ][ timestamp 59 ] state=[ 0.09004049 -0.46227978  0.17849086  1.36285935], action=0, reward=1.0, next_state=[ 0.08079489 -0.65913177  0.20574805  1.70564429]\n",
      "[ episode 76 ][ timestamp 60 ] state=[ 0.08079489 -0.65913177  0.20574805  1.70564429], action=1, reward=-1.0, next_state=[ 0.06761226 -0.46688524  0.23986093  1.48342071]\n",
      "[ Ended! ] Episode 76: Exploration_rate=0.6866430931872001. Score=60.\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ] state=[-0.04399404  0.0191766  -0.03907563  0.01551216]\n",
      "[ episode 77 ][ timestamp 1 ] state=[-0.04399404  0.0191766  -0.03907563  0.01551216], action=1, reward=1.0, next_state=[-0.0436105   0.21483652 -0.03876538 -0.289239  ]\n",
      "[ episode 77 ][ timestamp 2 ] state=[-0.0436105   0.21483652 -0.03876538 -0.289239  ], action=0, reward=1.0, next_state=[-0.03931377  0.02028818 -0.04455016 -0.0090299 ]\n",
      "[ episode 77 ][ timestamp 3 ] state=[-0.03931377  0.02028818 -0.04455016 -0.0090299 ], action=1, reward=1.0, next_state=[-0.03890801  0.21601979 -0.04473076 -0.31542943]\n",
      "[ episode 77 ][ timestamp 4 ] state=[-0.03890801  0.21601979 -0.04473076 -0.31542943], action=0, reward=1.0, next_state=[-0.03458761  0.0215626  -0.05103935 -0.03718186]\n",
      "[ episode 77 ][ timestamp 5 ] state=[-0.03458761  0.0215626  -0.05103935 -0.03718186], action=0, reward=1.0, next_state=[-0.03415636 -0.1727917  -0.05178299  0.2389709 ]\n",
      "[ episode 77 ][ timestamp 6 ] state=[-0.03415636 -0.1727917  -0.05178299  0.2389709 ], action=1, reward=1.0, next_state=[-0.0376122   0.0230303  -0.04700357 -0.06958576]\n",
      "[ episode 77 ][ timestamp 7 ] state=[-0.0376122   0.0230303  -0.04700357 -0.06958576], action=0, reward=1.0, next_state=[-0.03715159 -0.17138736 -0.04839528  0.20790467]\n",
      "[ episode 77 ][ timestamp 8 ] state=[-0.03715159 -0.17138736 -0.04839528  0.20790467], action=1, reward=1.0, next_state=[-0.04057934  0.024392   -0.04423719 -0.09964319]\n",
      "[ episode 77 ][ timestamp 9 ] state=[-0.04057934  0.024392   -0.04423719 -0.09964319], action=0, reward=1.0, next_state=[-0.0400915  -0.17006895 -0.04623005  0.17876138]\n",
      "[ episode 77 ][ timestamp 10 ] state=[-0.0400915  -0.17006895 -0.04623005  0.17876138], action=1, reward=1.0, next_state=[-0.04349288  0.02568303 -0.04265483 -0.12813967]\n",
      "[ episode 77 ][ timestamp 11 ] state=[-0.04349288  0.02568303 -0.04265483 -0.12813967], action=1, reward=1.0, next_state=[-0.04297922  0.22138925 -0.04521762 -0.4339687 ]\n",
      "[ episode 77 ][ timestamp 12 ] state=[-0.04297922  0.22138925 -0.04521762 -0.4339687 ], action=0, reward=1.0, next_state=[-0.03855143  0.02693567 -0.05389699 -0.15587592]\n",
      "[ episode 77 ][ timestamp 13 ] state=[-0.03855143  0.02693567 -0.05389699 -0.15587592], action=0, reward=1.0, next_state=[-0.03801272 -0.16737483 -0.05701451  0.11932855]\n",
      "[ episode 77 ][ timestamp 14 ] state=[-0.03801272 -0.16737483 -0.05701451  0.11932855], action=1, reward=1.0, next_state=[-0.04136021  0.02851569 -0.05462794 -0.19078296]\n",
      "[ episode 77 ][ timestamp 15 ] state=[-0.04136021  0.02851569 -0.05462794 -0.19078296], action=1, reward=1.0, next_state=[-0.0407899   0.22437486 -0.0584436  -0.5001861 ]\n",
      "[ episode 77 ][ timestamp 16 ] state=[-0.0407899   0.22437486 -0.0584436  -0.5001861 ], action=0, reward=1.0, next_state=[-0.0363024   0.03012341 -0.06844732 -0.22647903]\n",
      "[ episode 77 ][ timestamp 17 ] state=[-0.0363024   0.03012341 -0.06844732 -0.22647903], action=1, reward=1.0, next_state=[-0.03569993  0.22615341 -0.0729769  -0.53994331]\n",
      "[ episode 77 ][ timestamp 18 ] state=[-0.03569993  0.22615341 -0.0729769  -0.53994331], action=0, reward=1.0, next_state=[-0.03117687  0.03212908 -0.08377577 -0.27111761]\n",
      "[ episode 77 ][ timestamp 19 ] state=[-0.03117687  0.03212908 -0.08377577 -0.27111761], action=0, reward=1.0, next_state=[-0.03053428 -0.16170371 -0.08919812 -0.0059894 ]\n",
      "[ episode 77 ][ timestamp 20 ] state=[-0.03053428 -0.16170371 -0.08919812 -0.0059894 ], action=0, reward=1.0, next_state=[-0.03376836 -0.35544072 -0.08931791  0.25727132]\n",
      "[ episode 77 ][ timestamp 21 ] state=[-0.03376836 -0.35544072 -0.08931791  0.25727132], action=0, reward=1.0, next_state=[-0.04087717 -0.54918149 -0.08417248  0.52049948]\n",
      "[ episode 77 ][ timestamp 22 ] state=[-0.04087717 -0.54918149 -0.08417248  0.52049948], action=0, reward=1.0, next_state=[-0.0518608  -0.74302395 -0.07376249  0.78551624]\n",
      "[ episode 77 ][ timestamp 23 ] state=[-0.0518608  -0.74302395 -0.07376249  0.78551624], action=1, reward=1.0, next_state=[-0.06672128 -0.54697024 -0.05805217  0.47056884]\n",
      "[ episode 77 ][ timestamp 24 ] state=[-0.06672128 -0.54697024 -0.05805217  0.47056884], action=1, reward=1.0, next_state=[-0.07766069 -0.35107839 -0.04864079  0.1601683 ]\n",
      "[ episode 77 ][ timestamp 25 ] state=[-0.07766069 -0.35107839 -0.04864079  0.1601683 ], action=0, reward=1.0, next_state=[-0.08468225 -0.54547145 -0.04543743  0.43711826]\n",
      "[ episode 77 ][ timestamp 26 ] state=[-0.08468225 -0.54547145 -0.04543743  0.43711826], action=0, reward=1.0, next_state=[-0.09559168 -0.73992177 -0.03669506  0.7151387 ]\n",
      "[ episode 77 ][ timestamp 27 ] state=[-0.09559168 -0.73992177 -0.03669506  0.7151387 ], action=1, reward=1.0, next_state=[-0.11039012 -0.5443116  -0.02239229  0.41113504]\n",
      "[ episode 77 ][ timestamp 28 ] state=[-0.11039012 -0.5443116  -0.02239229  0.41113504], action=0, reward=1.0, next_state=[-0.12127635 -0.73910907 -0.01416959  0.69667521]\n",
      "[ episode 77 ][ timestamp 29 ] state=[-0.12127635 -0.73910907 -0.01416959  0.69667521], action=1, reward=1.0, next_state=[-1.36058533e-01 -5.43793512e-01 -2.36081529e-04  3.99565561e-01]\n",
      "[ episode 77 ][ timestamp 30 ] state=[-1.36058533e-01 -5.43793512e-01 -2.36081529e-04  3.99565561e-01], action=1, reward=1.0, next_state=[-0.1469344  -0.34866821  0.00775523  0.10680821]\n",
      "[ episode 77 ][ timestamp 31 ] state=[-0.1469344  -0.34866821  0.00775523  0.10680821], action=1, reward=1.0, next_state=[-0.15390777 -0.15365825  0.00989139 -0.18341792]\n",
      "[ episode 77 ][ timestamp 32 ] state=[-0.15390777 -0.15365825  0.00989139 -0.18341792], action=1, reward=1.0, next_state=[-0.15698093  0.04132078  0.00622304 -0.47296414]\n",
      "[ episode 77 ][ timestamp 33 ] state=[-0.15698093  0.04132078  0.00622304 -0.47296414], action=0, reward=1.0, next_state=[-0.15615452 -0.1538885  -0.00323625 -0.17832632]\n",
      "[ episode 77 ][ timestamp 34 ] state=[-0.15615452 -0.1538885  -0.00323625 -0.17832632], action=1, reward=1.0, next_state=[-0.15923229  0.04127961 -0.00680277 -0.47202841]\n",
      "[ episode 77 ][ timestamp 35 ] state=[-0.15923229  0.04127961 -0.00680277 -0.47202841], action=0, reward=1.0, next_state=[-0.15840669 -0.1537456  -0.01624334 -0.18149736]\n",
      "[ episode 77 ][ timestamp 36 ] state=[-0.15840669 -0.1537456  -0.01624334 -0.18149736], action=1, reward=1.0, next_state=[-0.16148161  0.04160497 -0.01987329 -0.4792599 ]\n",
      "[ episode 77 ][ timestamp 37 ] state=[-0.16148161  0.04160497 -0.01987329 -0.4792599 ], action=0, reward=1.0, next_state=[-0.16064951 -0.15323087 -0.02945849 -0.19290621]\n",
      "[ episode 77 ][ timestamp 38 ] state=[-0.16064951 -0.15323087 -0.02945849 -0.19290621], action=1, reward=1.0, next_state=[-0.16371412  0.04229984 -0.03331661 -0.49473456]\n",
      "[ episode 77 ][ timestamp 39 ] state=[-0.16371412  0.04229984 -0.03331661 -0.49473456], action=0, reward=1.0, next_state=[-0.16286813 -0.15233681 -0.0432113  -0.21273488]\n",
      "[ episode 77 ][ timestamp 40 ] state=[-0.16286813 -0.15233681 -0.0432113  -0.21273488], action=0, reward=1.0, next_state=[-0.16591486 -0.34681517 -0.047466    0.06601019]\n",
      "[ episode 77 ][ timestamp 41 ] state=[-0.16591486 -0.34681517 -0.047466    0.06601019], action=0, reward=1.0, next_state=[-0.17285117 -0.54122559 -0.0461458   0.3433476 ]\n",
      "[ episode 77 ][ timestamp 42 ] state=[-0.17285117 -0.54122559 -0.0461458   0.3433476 ], action=1, reward=1.0, next_state=[-0.18367568 -0.34547856 -0.03927884  0.03647758]\n",
      "[ episode 77 ][ timestamp 43 ] state=[-0.18367568 -0.34547856 -0.03927884  0.03647758], action=0, reward=1.0, next_state=[-0.19058525 -0.54001588 -0.03854929  0.31651347]\n",
      "[ episode 77 ][ timestamp 44 ] state=[-0.19058525 -0.54001588 -0.03854929  0.31651347], action=1, reward=1.0, next_state=[-0.20138557 -0.34436665 -0.03221902  0.01192697]\n",
      "[ episode 77 ][ timestamp 45 ] state=[-0.20138557 -0.34436665 -0.03221902  0.01192697], action=1, reward=1.0, next_state=[-0.2082729  -0.14879781 -0.03198048 -0.2907448 ]\n",
      "[ episode 77 ][ timestamp 46 ] state=[-0.2082729  -0.14879781 -0.03198048 -0.2907448 ], action=1, reward=1.0, next_state=[-0.21124886  0.04676521 -0.03779538 -0.59333999]\n",
      "[ episode 77 ][ timestamp 47 ] state=[-0.21124886  0.04676521 -0.03779538 -0.59333999], action=0, reward=1.0, next_state=[-0.21031355 -0.14780787 -0.04966218 -0.31279801]\n",
      "[ episode 77 ][ timestamp 48 ] state=[-0.21031355 -0.14780787 -0.04966218 -0.31279801], action=1, reward=1.0, next_state=[-0.21326971  0.0479851  -0.05591814 -0.62072004]\n",
      "[ episode 77 ][ timestamp 49 ] state=[-0.21326971  0.0479851  -0.05591814 -0.62072004], action=0, reward=1.0, next_state=[-0.21231001 -0.14631317 -0.06833254 -0.34615954]\n",
      "[ episode 77 ][ timestamp 50 ] state=[-0.21231001 -0.14631317 -0.06833254 -0.34615954], action=0, reward=1.0, next_state=[-0.21523627 -0.34039996 -0.07525573 -0.07578292]\n",
      "[ episode 77 ][ timestamp 51 ] state=[-0.21523627 -0.34039996 -0.07525573 -0.07578292], action=1, reward=1.0, next_state=[-0.22204427 -0.14428434 -0.07677139 -0.39122804]\n",
      "[ episode 77 ][ timestamp 52 ] state=[-0.22204427 -0.14428434 -0.07677139 -0.39122804], action=1, reward=1.0, next_state=[-0.22492996  0.05183844 -0.08459595 -0.70709432]\n",
      "[ episode 77 ][ timestamp 53 ] state=[-0.22492996  0.05183844 -0.08459595 -0.70709432], action=0, reward=1.0, next_state=[-0.22389319 -0.14201601 -0.09873784 -0.44219407]\n",
      "[ episode 77 ][ timestamp 54 ] state=[-0.22389319 -0.14201601 -0.09873784 -0.44219407], action=1, reward=1.0, next_state=[-0.22673351  0.05435437 -0.10758172 -0.76429674]\n",
      "[ episode 77 ][ timestamp 55 ] state=[-0.22673351  0.05435437 -0.10758172 -0.76429674], action=1, reward=1.0, next_state=[-0.22564642  0.25078043 -0.12286765 -1.08880047]\n",
      "[ episode 77 ][ timestamp 56 ] state=[-0.22564642  0.25078043 -0.12286765 -1.08880047], action=1, reward=1.0, next_state=[-0.22063081  0.44728907 -0.14464366 -1.41737357]\n",
      "[ episode 77 ][ timestamp 57 ] state=[-0.22063081  0.44728907 -0.14464366 -1.41737357], action=0, reward=1.0, next_state=[-0.21168503  0.2542237  -0.17299113 -1.1731768 ]\n",
      "[ episode 77 ][ timestamp 58 ] state=[-0.21168503  0.2542237  -0.17299113 -1.1731768 ], action=0, reward=1.0, next_state=[-0.20660056  0.06171971 -0.19645467 -0.93933679]\n",
      "[ episode 77 ][ timestamp 59 ] state=[-0.20660056  0.06171971 -0.19645467 -0.93933679], action=0, reward=-1.0, next_state=[-0.20536616 -0.13028947 -0.21524141 -0.71424991]\n",
      "[ Ended! ] Episode 77: Exploration_rate=0.6832098777212641. Score=59.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 78 ] state=[-0.02550602 -0.01763589 -0.02416274  0.03924912]\n",
      "[ episode 78 ][ timestamp 1 ] state=[-0.02550602 -0.01763589 -0.02416274  0.03924912], action=0, reward=1.0, next_state=[-0.02585874 -0.21240317 -0.02337776  0.3242116 ]\n",
      "[ episode 78 ][ timestamp 2 ] state=[-0.02585874 -0.21240317 -0.02337776  0.3242116 ], action=1, reward=1.0, next_state=[-0.0301068  -0.01695628 -0.01689352  0.02424894]\n",
      "[ episode 78 ][ timestamp 3 ] state=[-0.0301068  -0.01695628 -0.01689352  0.02424894], action=1, reward=1.0, next_state=[-0.03044593  0.17840382 -0.01640855 -0.27371585]\n",
      "[ episode 78 ][ timestamp 4 ] state=[-0.03044593  0.17840382 -0.01640855 -0.27371585], action=1, reward=1.0, next_state=[-0.02687785  0.373756   -0.02188286 -0.57152858]\n",
      "[ episode 78 ][ timestamp 5 ] state=[-0.02687785  0.373756   -0.02188286 -0.57152858], action=0, reward=1.0, next_state=[-0.01940273  0.17894763 -0.03331343 -0.28581903]\n",
      "[ episode 78 ][ timestamp 6 ] state=[-0.01940273  0.17894763 -0.03331343 -0.28581903], action=1, reward=1.0, next_state=[-0.01582378  0.37452846 -0.03902982 -0.58881984]\n",
      "[ episode 78 ][ timestamp 7 ] state=[-0.01582378  0.37452846 -0.03902982 -0.58881984], action=1, reward=1.0, next_state=[-0.00833321  0.5701746  -0.05080621 -0.8935374 ]\n",
      "[ episode 78 ][ timestamp 8 ] state=[-0.00833321  0.5701746  -0.05080621 -0.8935374 ], action=1, reward=1.0, next_state=[ 0.00307028  0.76594743 -0.06867696 -1.20174832]\n",
      "[ episode 78 ][ timestamp 9 ] state=[ 0.00307028  0.76594743 -0.06867696 -1.20174832], action=1, reward=1.0, next_state=[ 0.01838923  0.96188701 -0.09271193 -1.51514001]\n",
      "[ episode 78 ][ timestamp 10 ] state=[ 0.01838923  0.96188701 -0.09271193 -1.51514001], action=0, reward=1.0, next_state=[ 0.03762697  0.76800133 -0.12301473 -1.25277877]\n",
      "[ episode 78 ][ timestamp 11 ] state=[ 0.03762697  0.76800133 -0.12301473 -1.25277877], action=0, reward=1.0, next_state=[ 0.052987    0.57465097 -0.1480703  -1.00102008]\n",
      "[ episode 78 ][ timestamp 12 ] state=[ 0.052987    0.57465097 -0.1480703  -1.00102008], action=0, reward=1.0, next_state=[ 0.06448001  0.38178459 -0.1680907  -0.7582599 ]\n",
      "[ episode 78 ][ timestamp 13 ] state=[ 0.06448001  0.38178459 -0.1680907  -0.7582599 ], action=0, reward=1.0, next_state=[ 0.07211571  0.18932846 -0.1832559  -0.52283071]\n",
      "[ episode 78 ][ timestamp 14 ] state=[ 0.07211571  0.18932846 -0.1832559  -0.52283071], action=0, reward=1.0, next_state=[ 0.07590228 -0.00280552 -0.19371252 -0.29303169]\n",
      "[ episode 78 ][ timestamp 15 ] state=[ 0.07590228 -0.00280552 -0.19371252 -0.29303169], action=1, reward=1.0, next_state=[ 0.07584617  0.19447452 -0.19957315 -0.64001292]\n",
      "[ episode 78 ][ timestamp 16 ] state=[ 0.07584617  0.19447452 -0.19957315 -0.64001292], action=0, reward=-1.0, next_state=[ 0.07973566  0.00261189 -0.21237341 -0.41621709]\n",
      "[ Ended! ] Episode 78: Exploration_rate=0.6797938283326578. Score=16.\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ] state=[-0.04532565  0.00163806  0.03301772 -0.01301467]\n",
      "[ episode 79 ][ timestamp 1 ] state=[-0.04532565  0.00163806  0.03301772 -0.01301467], action=1, reward=1.0, next_state=[-0.04529289  0.19627132  0.03275743 -0.29509999]\n",
      "[ episode 79 ][ timestamp 2 ] state=[-0.04529289  0.19627132  0.03275743 -0.29509999], action=1, reward=1.0, next_state=[-0.04136746  0.39091132  0.02685543 -0.57727441]\n",
      "[ episode 79 ][ timestamp 3 ] state=[-0.04136746  0.39091132  0.02685543 -0.57727441], action=1, reward=1.0, next_state=[-0.03354924  0.58564677  0.01530994 -0.8613777 ]\n",
      "[ episode 79 ][ timestamp 4 ] state=[-0.03354924  0.58564677  0.01530994 -0.8613777 ], action=0, reward=1.0, next_state=[-0.0218363   0.39031971 -0.00191762 -0.56392051]\n",
      "[ episode 79 ][ timestamp 5 ] state=[-0.0218363   0.39031971 -0.00191762 -0.56392051], action=0, reward=1.0, next_state=[-0.01402991  0.19522472 -0.01319603 -0.27184234]\n",
      "[ episode 79 ][ timestamp 6 ] state=[-0.01402991  0.19522472 -0.01319603 -0.27184234], action=0, reward=1.0, next_state=[-0.01012541  0.00029353 -0.01863287  0.01664947]\n",
      "[ episode 79 ][ timestamp 7 ] state=[-0.01012541  0.00029353 -0.01863287  0.01664947], action=1, reward=1.0, next_state=[-0.01011954  0.19567767 -0.01829988 -0.28185362]\n",
      "[ episode 79 ][ timestamp 8 ] state=[-0.01011954  0.19567767 -0.01829988 -0.28185362], action=0, reward=1.0, next_state=[-0.00620599  0.00082147 -0.02393696  0.00500188]\n",
      "[ episode 79 ][ timestamp 9 ] state=[-0.00620599  0.00082147 -0.02393696  0.00500188], action=1, reward=1.0, next_state=[-0.00618956  0.19627839 -0.02383692 -0.2951363 ]\n",
      "[ episode 79 ][ timestamp 10 ] state=[-0.00618956  0.19627839 -0.02383692 -0.2951363 ], action=1, reward=1.0, next_state=[-0.00226399  0.39173191 -0.02973965 -0.59524069]\n",
      "[ episode 79 ][ timestamp 11 ] state=[-0.00226399  0.39173191 -0.02973965 -0.59524069], action=0, reward=1.0, next_state=[ 0.00557064  0.19703854 -0.04164446 -0.31207193]\n",
      "[ episode 79 ][ timestamp 12 ] state=[ 0.00557064  0.19703854 -0.04164446 -0.31207193], action=1, reward=1.0, next_state=[ 0.00951142  0.39272827 -0.0478859  -0.61759197]\n",
      "[ episode 79 ][ timestamp 13 ] state=[ 0.00951142  0.39272827 -0.0478859  -0.61759197], action=1, reward=1.0, next_state=[ 0.01736598  0.58848529 -0.06023774 -0.92496398]\n",
      "[ episode 79 ][ timestamp 14 ] state=[ 0.01736598  0.58848529 -0.06023774 -0.92496398], action=1, reward=1.0, next_state=[ 0.02913569  0.78436686 -0.07873702 -1.23595259]\n",
      "[ episode 79 ][ timestamp 15 ] state=[ 0.02913569  0.78436686 -0.07873702 -1.23595259], action=0, reward=1.0, next_state=[ 0.04482302  0.59034006 -0.10345607 -0.96893886]\n",
      "[ episode 79 ][ timestamp 16 ] state=[ 0.04482302  0.59034006 -0.10345607 -0.96893886], action=1, reward=1.0, next_state=[ 0.05662983  0.78668734 -0.12283485 -1.29224689]\n",
      "[ episode 79 ][ timestamp 17 ] state=[ 0.05662983  0.78668734 -0.12283485 -1.29224689], action=1, reward=1.0, next_state=[ 0.07236357  0.98313784 -0.14867978 -1.62072504]\n",
      "[ episode 79 ][ timestamp 18 ] state=[ 0.07236357  0.98313784 -0.14867978 -1.62072504], action=1, reward=1.0, next_state=[ 0.09202633  1.17966563 -0.18109428 -1.95581543]\n",
      "[ episode 79 ][ timestamp 19 ] state=[ 0.09202633  1.17966563 -0.18109428 -1.95581543], action=1, reward=-1.0, next_state=[ 0.11561964  1.37618973 -0.22021059 -2.29873218]\n",
      "[ Ended! ] Episode 79: Exploration_rate=0.6763948591909945. Score=19.\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ] state=[ 0.04893084  0.04597589 -0.00307883 -0.01730199]\n",
      "[ episode 80 ][ timestamp 1 ] state=[ 0.04893084  0.04597589 -0.00307883 -0.01730199], action=1, reward=1.0, next_state=[ 0.04985036  0.24114186 -0.00342487 -0.31095473]\n",
      "[ episode 80 ][ timestamp 2 ] state=[ 0.04985036  0.24114186 -0.00342487 -0.31095473], action=1, reward=1.0, next_state=[ 0.05467319  0.43631243 -0.00964397 -0.60471579]\n",
      "[ episode 80 ][ timestamp 3 ] state=[ 0.05467319  0.43631243 -0.00964397 -0.60471579], action=0, reward=1.0, next_state=[ 0.06339944  0.24132667 -0.02173828 -0.31508603]\n",
      "[ episode 80 ][ timestamp 4 ] state=[ 0.06339944  0.24132667 -0.02173828 -0.31508603], action=0, reward=1.0, next_state=[ 0.06822598  0.04652101 -0.02804001 -0.02933713]\n",
      "[ episode 80 ][ timestamp 5 ] state=[ 0.06822598  0.04652101 -0.02804001 -0.02933713], action=1, reward=1.0, next_state=[ 0.0691564   0.24203361 -0.02862675 -0.33073344]\n",
      "[ episode 80 ][ timestamp 6 ] state=[ 0.0691564   0.24203361 -0.02862675 -0.33073344], action=1, reward=1.0, next_state=[ 0.07399707  0.43755112 -0.03524142 -0.63230465]\n",
      "[ episode 80 ][ timestamp 7 ] state=[ 0.07399707  0.43755112 -0.03524142 -0.63230465], action=1, reward=1.0, next_state=[ 0.08274809  0.63314655 -0.04788751 -0.93587446]\n",
      "[ episode 80 ][ timestamp 8 ] state=[ 0.08274809  0.63314655 -0.04788751 -0.93587446], action=0, reward=1.0, next_state=[ 0.09541102  0.43870203 -0.066605   -0.65861558]\n",
      "[ episode 80 ][ timestamp 9 ] state=[ 0.09541102  0.43870203 -0.066605   -0.65861558], action=0, reward=1.0, next_state=[ 0.10418506  0.24456722 -0.07977731 -0.38762645]\n",
      "[ episode 80 ][ timestamp 10 ] state=[ 0.10418506  0.24456722 -0.07977731 -0.38762645], action=0, reward=1.0, next_state=[ 0.10907641  0.05066297 -0.08752984 -0.12112481]\n",
      "[ episode 80 ][ timestamp 11 ] state=[ 0.10907641  0.05066297 -0.08752984 -0.12112481], action=1, reward=1.0, next_state=[ 0.11008967  0.24692283 -0.08995234 -0.44008852]\n",
      "[ episode 80 ][ timestamp 12 ] state=[ 0.11008967  0.24692283 -0.08995234 -0.44008852], action=1, reward=1.0, next_state=[ 0.11502812  0.44319502 -0.09875411 -0.75971685]\n",
      "[ episode 80 ][ timestamp 13 ] state=[ 0.11502812  0.44319502 -0.09875411 -0.75971685], action=1, reward=1.0, next_state=[ 0.12389202  0.6395289  -0.11394844 -1.08176933]\n",
      "[ episode 80 ][ timestamp 14 ] state=[ 0.12389202  0.6395289  -0.11394844 -1.08176933], action=1, reward=1.0, next_state=[ 0.1366826   0.8359553  -0.13558383 -1.40792656]\n",
      "[ episode 80 ][ timestamp 15 ] state=[ 0.1366826   0.8359553  -0.13558383 -1.40792656], action=0, reward=1.0, next_state=[ 0.15340171  0.64275096 -0.16374236 -1.16051936]\n",
      "[ episode 80 ][ timestamp 16 ] state=[ 0.15340171  0.64275096 -0.16374236 -1.16051936], action=1, reward=1.0, next_state=[ 0.16625673  0.83958257 -0.18695275 -1.499743  ]\n",
      "[ episode 80 ][ timestamp 17 ] state=[ 0.16625673  0.83958257 -0.18695275 -1.499743  ], action=1, reward=-1.0, next_state=[ 0.18304838  1.03641825 -0.21694761 -1.84449627]\n",
      "[ Ended! ] Episode 80: Exploration_rate=0.6730128848950395. Score=17.\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ] state=[-0.01291827  0.04731144 -0.04755432  0.02834216]\n",
      "[ episode 81 ][ timestamp 1 ] state=[-0.01291827  0.04731144 -0.04755432  0.02834216], action=1, reward=1.0, next_state=[-0.01197204  0.24308196 -0.04698748 -0.27895734]\n",
      "[ episode 81 ][ timestamp 2 ] state=[-0.01197204  0.24308196 -0.04698748 -0.27895734], action=1, reward=1.0, next_state=[-0.0071104   0.43884162 -0.05256663 -0.58608197]\n",
      "[ episode 81 ][ timestamp 3 ] state=[-0.0071104   0.43884162 -0.05256663 -0.58608197], action=0, reward=1.0, next_state=[ 0.00166643  0.24449381 -0.06428826 -0.31041042]\n",
      "[ episode 81 ][ timestamp 4 ] state=[ 0.00166643  0.24449381 -0.06428826 -0.31041042], action=0, reward=1.0, next_state=[ 0.00655631  0.0503439  -0.07049647 -0.03867489]\n",
      "[ episode 81 ][ timestamp 5 ] state=[ 0.00655631  0.0503439  -0.07049647 -0.03867489], action=0, reward=1.0, next_state=[ 0.00756319 -0.14370003 -0.07126997  0.23095924]\n",
      "[ episode 81 ][ timestamp 6 ] state=[ 0.00756319 -0.14370003 -0.07126997  0.23095924], action=0, reward=1.0, next_state=[ 0.00468919 -0.33773503 -0.06665079  0.50033724]\n",
      "[ episode 81 ][ timestamp 7 ] state=[ 0.00468919 -0.33773503 -0.06665079  0.50033724], action=0, reward=1.0, next_state=[-0.00206551 -0.5318572  -0.05664404  0.77129314]\n",
      "[ episode 81 ][ timestamp 8 ] state=[-0.00206551 -0.5318572  -0.05664404  0.77129314], action=1, reward=1.0, next_state=[-0.01270266 -0.33600342 -0.04121818  0.4613392 ]\n",
      "[ episode 81 ][ timestamp 9 ] state=[-0.01270266 -0.33600342 -0.04121818  0.4613392 ], action=0, reward=1.0, next_state=[-0.01942273 -0.5305193  -0.03199139  0.74075049]\n",
      "[ episode 81 ][ timestamp 10 ] state=[-0.01942273 -0.5305193  -0.03199139  0.74075049], action=1, reward=1.0, next_state=[-0.03003311 -0.33497062 -0.01717638  0.43817369]\n",
      "[ episode 81 ][ timestamp 11 ] state=[-0.03003311 -0.33497062 -0.01717638  0.43817369], action=0, reward=1.0, next_state=[-0.03673252 -0.5298453  -0.00841291  0.72539298]\n",
      "[ episode 81 ][ timestamp 12 ] state=[-0.03673252 -0.5298453  -0.00841291  0.72539298], action=0, reward=1.0, next_state=[-0.04732943 -0.72484991  0.00609495  1.01541618]\n",
      "[ episode 81 ][ timestamp 13 ] state=[-0.04732943 -0.72484991  0.00609495  1.01541618], action=0, reward=1.0, next_state=[-0.06182643 -0.92005261  0.02640327  1.31000669]\n",
      "[ episode 81 ][ timestamp 14 ] state=[-0.06182643 -0.92005261  0.02640327  1.31000669], action=1, reward=1.0, next_state=[-0.08022748 -0.72527487  0.05260341  1.02570359]\n",
      "[ episode 81 ][ timestamp 15 ] state=[-0.08022748 -0.72527487  0.05260341  1.02570359], action=0, reward=1.0, next_state=[-0.09473298 -0.92105627  0.07311748  1.33442773]\n",
      "[ episode 81 ][ timestamp 16 ] state=[-0.09473298 -0.92105627  0.07311748  1.33442773], action=0, reward=1.0, next_state=[-0.1131541  -1.11701967  0.09980603  1.64906483]\n",
      "[ episode 81 ][ timestamp 17 ] state=[-0.1131541  -1.11701967  0.09980603  1.64906483], action=1, reward=1.0, next_state=[-0.1354945  -0.92319605  0.13278733  1.38907052]\n",
      "[ episode 81 ][ timestamp 18 ] state=[-0.1354945  -0.92319605  0.13278733  1.38907052], action=1, reward=1.0, next_state=[-0.15395842 -0.72995477  0.16056874  1.1406852 ]\n",
      "[ episode 81 ][ timestamp 19 ] state=[-0.15395842 -0.72995477  0.16056874  1.1406852 ], action=1, reward=1.0, next_state=[-0.16855751 -0.5372537   0.18338244  0.90235642]\n",
      "[ episode 81 ][ timestamp 20 ] state=[-0.16855751 -0.5372537   0.18338244  0.90235642], action=1, reward=1.0, next_state=[-0.17930259 -0.34502602  0.20142957  0.67246243]\n",
      "[ episode 81 ][ timestamp 21 ] state=[-0.17930259 -0.34502602  0.20142957  0.67246243], action=1, reward=-1.0, next_state=[-0.18620311 -0.15318896  0.21487882  0.44934544]\n",
      "[ Ended! ] Episode 81: Exploration_rate=0.6696478204705644. Score=21.\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ] state=[ 0.01354198  0.04655105 -0.00500404  0.00519953]\n",
      "[ episode 82 ][ timestamp 1 ] state=[ 0.01354198  0.04655105 -0.00500404  0.00519953], action=1, reward=1.0, next_state=[ 0.014473    0.24174441 -0.00490005 -0.28905802]\n",
      "[ episode 82 ][ timestamp 2 ] state=[ 0.014473    0.24174441 -0.00490005 -0.28905802], action=0, reward=1.0, next_state=[ 0.01930788  0.04669267 -0.01068121  0.00207546]\n",
      "[ episode 82 ][ timestamp 3 ] state=[ 0.01930788  0.04669267 -0.01068121  0.00207546], action=1, reward=1.0, next_state=[ 0.02024174  0.24196616 -0.0106397  -0.29395829]\n",
      "[ episode 82 ][ timestamp 4 ] state=[ 0.02024174  0.24196616 -0.0106397  -0.29395829], action=0, reward=1.0, next_state=[ 0.02508106  0.04699751 -0.01651887 -0.00464988]\n",
      "[ episode 82 ][ timestamp 5 ] state=[ 0.02508106  0.04699751 -0.01651887 -0.00464988], action=0, reward=1.0, next_state=[ 0.02602101 -0.14788369 -0.01661187  0.28277571]\n",
      "[ episode 82 ][ timestamp 6 ] state=[ 0.02602101 -0.14788369 -0.01661187  0.28277571], action=1, reward=1.0, next_state=[ 0.02306334  0.04747121 -0.01095635 -0.01509988]\n",
      "[ episode 82 ][ timestamp 7 ] state=[ 0.02306334  0.04747121 -0.01095635 -0.01509988], action=0, reward=1.0, next_state=[ 0.02401276 -0.14749191 -0.01125835  0.27410615]\n",
      "[ episode 82 ][ timestamp 8 ] state=[ 0.02401276 -0.14749191 -0.01125835  0.27410615], action=1, reward=1.0, next_state=[ 0.02106292  0.04778885 -0.00577623 -0.02210631]\n",
      "[ episode 82 ][ timestamp 9 ] state=[ 0.02106292  0.04778885 -0.00577623 -0.02210631], action=0, reward=1.0, next_state=[ 0.0220187  -0.14724979 -0.00621835  0.26874856]\n",
      "[ episode 82 ][ timestamp 10 ] state=[ 0.0220187  -0.14724979 -0.00621835  0.26874856], action=0, reward=1.0, next_state=[ 0.0190737  -0.34228245 -0.00084338  0.55946371]\n",
      "[ episode 82 ][ timestamp 11 ] state=[ 0.0190737  -0.34228245 -0.00084338  0.55946371], action=0, reward=1.0, next_state=[ 0.01222806 -0.53739255  0.01034589  0.85188081]\n",
      "[ episode 82 ][ timestamp 12 ] state=[ 0.01222806 -0.53739255  0.01034589  0.85188081], action=0, reward=1.0, next_state=[ 0.0014802  -0.73265401  0.02738351  1.14779896]\n",
      "[ episode 82 ][ timestamp 13 ] state=[ 0.0014802  -0.73265401  0.02738351  1.14779896], action=1, reward=1.0, next_state=[-0.01317288 -0.53790007  0.05033949  0.86382732]\n",
      "[ episode 82 ][ timestamp 14 ] state=[-0.01317288 -0.53790007  0.05033949  0.86382732], action=0, reward=1.0, next_state=[-0.02393088 -0.73366985  0.06761603  1.17190355]\n",
      "[ episode 82 ][ timestamp 15 ] state=[-0.02393088 -0.73366985  0.06761603  1.17190355], action=1, reward=1.0, next_state=[-0.03860427 -0.53948899  0.0910541   0.90116182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 82 ][ timestamp 16 ] state=[-0.03860427 -0.53948899  0.0910541   0.90116182], action=0, reward=1.0, next_state=[-0.04939405 -0.73571883  0.10907734  1.22102017]\n",
      "[ episode 82 ][ timestamp 17 ] state=[-0.04939405 -0.73571883  0.10907734  1.22102017], action=1, reward=1.0, next_state=[-0.06410843 -0.54215833  0.13349774  0.96441011]\n",
      "[ episode 82 ][ timestamp 18 ] state=[-0.06410843 -0.54215833  0.13349774  0.96441011], action=0, reward=1.0, next_state=[-0.0749516  -0.73879649  0.15278595  1.2958748 ]\n",
      "[ episode 82 ][ timestamp 19 ] state=[-0.0749516  -0.73879649  0.15278595  1.2958748 ], action=1, reward=1.0, next_state=[-0.08972753 -0.54590934  0.17870344  1.05465902]\n",
      "[ episode 82 ][ timestamp 20 ] state=[-0.08972753 -0.54590934  0.17870344  1.05465902], action=1, reward=1.0, next_state=[-0.10064571 -0.35354787  0.19979662  0.82297148]\n",
      "[ episode 82 ][ timestamp 21 ] state=[-0.10064571 -0.35354787  0.19979662  0.82297148], action=0, reward=-1.0, next_state=[-0.10771667 -0.55076     0.21625605  1.17125513]\n",
      "[ Ended! ] Episode 82: Exploration_rate=0.6662995813682115. Score=21.\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ] state=[-0.00134041 -0.02637524  0.02580936  0.03886123]\n",
      "[ episode 83 ][ timestamp 1 ] state=[-0.00134041 -0.02637524  0.02580936  0.03886123], action=1, reward=1.0, next_state=[-0.00186791  0.16836728  0.02658658 -0.24556815]\n",
      "[ episode 83 ][ timestamp 2 ] state=[-0.00186791  0.16836728  0.02658658 -0.24556815], action=0, reward=1.0, next_state=[ 0.00149943 -0.02712411  0.02167522  0.05538083]\n",
      "[ episode 83 ][ timestamp 3 ] state=[ 0.00149943 -0.02712411  0.02167522  0.05538083], action=1, reward=1.0, next_state=[ 0.00095695  0.16768045  0.02278284 -0.23038536]\n",
      "[ episode 83 ][ timestamp 4 ] state=[ 0.00095695  0.16768045  0.02278284 -0.23038536], action=1, reward=1.0, next_state=[ 0.00431056  0.36246956  0.01817513 -0.51579562]\n",
      "[ episode 83 ][ timestamp 5 ] state=[ 0.00431056  0.36246956  0.01817513 -0.51579562], action=1, reward=1.0, next_state=[ 0.01155995  0.55733092  0.00785922 -0.80269618]\n",
      "[ episode 83 ][ timestamp 6 ] state=[ 0.01155995  0.55733092  0.00785922 -0.80269618], action=0, reward=1.0, next_state=[ 0.02270657  0.36210208 -0.0081947  -0.50755139]\n",
      "[ episode 83 ][ timestamp 7 ] state=[ 0.02270657  0.36210208 -0.0081947  -0.50755139], action=1, reward=1.0, next_state=[ 0.02994861  0.55733853 -0.01834573 -0.80280545]\n",
      "[ episode 83 ][ timestamp 8 ] state=[ 0.02994861  0.55733853 -0.01834573 -0.80280545], action=0, reward=1.0, next_state=[ 0.04109538  0.36247289 -0.03440184 -0.51594951]\n",
      "[ episode 83 ][ timestamp 9 ] state=[ 0.04109538  0.36247289 -0.03440184 -0.51594951], action=0, reward=1.0, next_state=[ 0.04834484  0.16785184 -0.04472083 -0.23430281]\n",
      "[ episode 83 ][ timestamp 10 ] state=[ 0.04834484  0.16785184 -0.04472083 -0.23430281], action=0, reward=1.0, next_state=[ 0.05170188 -0.02660356 -0.04940689  0.04394512]\n",
      "[ episode 83 ][ timestamp 11 ] state=[ 0.05170188 -0.02660356 -0.04940689  0.04394512], action=0, reward=1.0, next_state=[ 0.05116981 -0.2209835  -0.04852799  0.32063952]\n",
      "[ episode 83 ][ timestamp 12 ] state=[ 0.05116981 -0.2209835  -0.04852799  0.32063952], action=0, reward=1.0, next_state=[ 0.04675014 -0.41538198 -0.04211519  0.59763232]\n",
      "[ episode 83 ][ timestamp 13 ] state=[ 0.04675014 -0.41538198 -0.04211519  0.59763232], action=1, reward=1.0, next_state=[ 0.0384425  -0.2196968  -0.03016255  0.29198662]\n",
      "[ episode 83 ][ timestamp 14 ] state=[ 0.0384425  -0.2196968  -0.03016255  0.29198662], action=0, reward=1.0, next_state=[ 0.03404856 -0.41437599 -0.02432282  0.57500614]\n",
      "[ episode 83 ][ timestamp 15 ] state=[ 0.03404856 -0.41437599 -0.02432282  0.57500614], action=0, reward=1.0, next_state=[ 0.02576104 -0.60914867 -0.01282269  0.85992854]\n",
      "[ episode 83 ][ timestamp 16 ] state=[ 0.02576104 -0.60914867 -0.01282269  0.85992854], action=0, reward=1.0, next_state=[ 0.01357807 -0.80409365  0.00437588  1.1485522 ]\n",
      "[ episode 83 ][ timestamp 17 ] state=[ 0.01357807 -0.80409365  0.00437588  1.1485522 ], action=1, reward=1.0, next_state=[-0.0025038  -0.6090291   0.02734692  0.85724467]\n",
      "[ episode 83 ][ timestamp 18 ] state=[-0.0025038  -0.6090291   0.02734692  0.85724467], action=1, reward=1.0, next_state=[-0.01468439 -0.41429019  0.04449181  0.57328453]\n",
      "[ episode 83 ][ timestamp 19 ] state=[-0.01468439 -0.41429019  0.04449181  0.57328453], action=0, reward=1.0, next_state=[-0.02297019 -0.61000679  0.05595751  0.87964518]\n",
      "[ episode 83 ][ timestamp 20 ] state=[-0.02297019 -0.61000679  0.05595751  0.87964518], action=0, reward=1.0, next_state=[-0.03517033 -0.80584254  0.07355041  1.18938194]\n",
      "[ episode 83 ][ timestamp 21 ] state=[-0.03517033 -0.80584254  0.07355041  1.18938194], action=1, reward=1.0, next_state=[-0.05128718 -0.61174688  0.09733805  0.92062992]\n",
      "[ episode 83 ][ timestamp 22 ] state=[-0.05128718 -0.61174688  0.09733805  0.92062992], action=0, reward=1.0, next_state=[-0.06352211 -0.80803998  0.11575065  1.24224803]\n",
      "[ episode 83 ][ timestamp 23 ] state=[-0.06352211 -0.80803998  0.11575065  1.24224803], action=0, reward=1.0, next_state=[-0.07968291 -1.00444162  0.14059561  1.56883386]\n",
      "[ episode 83 ][ timestamp 24 ] state=[-0.07968291 -1.00444162  0.14059561  1.56883386], action=1, reward=1.0, next_state=[-0.09977175 -0.81125065  0.17197228  1.32310588]\n",
      "[ episode 83 ][ timestamp 25 ] state=[-0.09977175 -0.81125065  0.17197228  1.32310588], action=0, reward=1.0, next_state=[-0.11599676 -1.00807658  0.1984344   1.66430077]\n",
      "[ episode 83 ][ timestamp 26 ] state=[-0.11599676 -1.00807658  0.1984344   1.66430077], action=0, reward=-1.0, next_state=[-0.13615829 -1.20487791  0.23172042  2.01166744]\n",
      "[ Ended! ] Episode 83: Exploration_rate=0.6629680834613705. Score=26.\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ] state=[-0.02852417 -0.03008178 -0.04984408  0.00440854]\n",
      "[ episode 84 ][ timestamp 1 ] state=[-0.02852417 -0.03008178 -0.04984408  0.00440854], action=1, reward=1.0, next_state=[-0.02912581  0.16571826 -0.04975591 -0.30357485]\n",
      "[ episode 84 ][ timestamp 2 ] state=[-0.02912581  0.16571826 -0.04975591 -0.30357485], action=1, reward=1.0, next_state=[-0.02581144  0.3615127  -0.05582741 -0.61152525]\n",
      "[ episode 84 ][ timestamp 3 ] state=[-0.02581144  0.3615127  -0.05582741 -0.61152525], action=0, reward=1.0, next_state=[-0.01858119  0.16721364 -0.06805791 -0.33693545]\n",
      "[ episode 84 ][ timestamp 4 ] state=[-0.01858119  0.16721364 -0.06805791 -0.33693545], action=1, reward=1.0, next_state=[-0.01523691  0.36323476 -0.07479662 -0.65028003]\n",
      "[ episode 84 ][ timestamp 5 ] state=[-0.01523691  0.36323476 -0.07479662 -0.65028003], action=1, reward=1.0, next_state=[-0.00797222  0.55931447 -0.08780222 -0.96554694]\n",
      "[ episode 84 ][ timestamp 6 ] state=[-0.00797222  0.55931447 -0.08780222 -0.96554694], action=1, reward=1.0, next_state=[ 0.00321407  0.75549903 -0.10711316 -1.28447089]\n",
      "[ episode 84 ][ timestamp 7 ] state=[ 0.00321407  0.75549903 -0.10711316 -1.28447089], action=1, reward=1.0, next_state=[ 0.01832405  0.95180916 -0.13280258 -1.60867955]\n",
      "[ episode 84 ][ timestamp 8 ] state=[ 0.01832405  0.95180916 -0.13280258 -1.60867955], action=0, reward=1.0, next_state=[ 0.03736023  0.75848319 -0.16497617 -1.36017334]\n",
      "[ episode 84 ][ timestamp 9 ] state=[ 0.03736023  0.75848319 -0.16497617 -1.36017334], action=0, reward=1.0, next_state=[ 0.0525299   0.56576844 -0.19217964 -1.12330943]\n",
      "[ episode 84 ][ timestamp 10 ] state=[ 0.0525299   0.56576844 -0.19217964 -1.12330943], action=0, reward=-1.0, next_state=[ 0.06384527  0.37361306 -0.21464583 -0.89653631]\n",
      "[ Ended! ] Episode 84: Exploration_rate=0.6596532430440636. Score=10.\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ] state=[ 0.00339141  0.02911781  0.00592788 -0.033841  ]\n",
      "[ episode 85 ][ timestamp 1 ] state=[ 0.00339141  0.02911781  0.00592788 -0.033841  ], action=1, reward=1.0, next_state=[ 0.00397377  0.22415425  0.00525106 -0.32464773]\n",
      "[ episode 85 ][ timestamp 2 ] state=[ 0.00397377  0.22415425  0.00525106 -0.32464773], action=1, reward=1.0, next_state=[ 0.00845685  0.41920104 -0.00124189 -0.61567008]\n",
      "[ episode 85 ][ timestamp 3 ] state=[ 0.00845685  0.41920104 -0.00124189 -0.61567008], action=0, reward=1.0, next_state=[ 0.01684087  0.22409646 -0.01355529 -0.32337856]\n",
      "[ episode 85 ][ timestamp 4 ] state=[ 0.01684087  0.22409646 -0.01355529 -0.32337856], action=1, reward=1.0, next_state=[ 0.0213228   0.41940878 -0.02002286 -0.62030526]\n",
      "[ episode 85 ][ timestamp 5 ] state=[ 0.0213228   0.41940878 -0.02002286 -0.62030526], action=0, reward=1.0, next_state=[ 0.02971098  0.22457211 -0.03242897 -0.33399516]\n",
      "[ episode 85 ][ timestamp 6 ] state=[ 0.02971098  0.22457211 -0.03242897 -0.33399516], action=1, reward=1.0, next_state=[ 0.03420242  0.42014024 -0.03910887 -0.63672557]\n",
      "[ episode 85 ][ timestamp 7 ] state=[ 0.03420242  0.42014024 -0.03910887 -0.63672557], action=0, reward=1.0, next_state=[ 0.04260523  0.2255849  -0.05184338 -0.35661078]\n",
      "[ episode 85 ][ timestamp 8 ] state=[ 0.04260523  0.2255849  -0.05184338 -0.35661078], action=0, reward=1.0, next_state=[ 0.04711692  0.03123688 -0.0589756  -0.08071556]\n",
      "[ episode 85 ][ timestamp 9 ] state=[ 0.04711692  0.03123688 -0.0589756  -0.08071556], action=0, reward=1.0, next_state=[ 0.04774166 -0.16299223 -0.06058991  0.19279282]\n",
      "[ episode 85 ][ timestamp 10 ] state=[ 0.04774166 -0.16299223 -0.06058991  0.19279282], action=1, reward=1.0, next_state=[ 0.04448182  0.03294178 -0.05673405 -0.11837142]\n",
      "[ episode 85 ][ timestamp 11 ] state=[ 0.04448182  0.03294178 -0.05673405 -0.11837142], action=0, reward=1.0, next_state=[ 0.04514065 -0.16132334 -0.05910148  0.15588655]\n",
      "[ episode 85 ][ timestamp 12 ] state=[ 0.04514065 -0.16132334 -0.05910148  0.15588655], action=0, reward=1.0, next_state=[ 0.04191419 -0.35555148 -0.05598375  0.42935435]\n",
      "[ episode 85 ][ timestamp 13 ] state=[ 0.04191419 -0.35555148 -0.05598375  0.42935435], action=0, reward=1.0, next_state=[ 0.03480316 -0.54983777 -0.04739666  0.70387659]\n",
      "[ episode 85 ][ timestamp 14 ] state=[ 0.03480316 -0.54983777 -0.04739666  0.70387659], action=1, reward=1.0, next_state=[ 0.0238064  -0.35409215 -0.03331913  0.3966585 ]\n",
      "[ episode 85 ][ timestamp 15 ] state=[ 0.0238064  -0.35409215 -0.03331913  0.3966585 ], action=1, reward=1.0, next_state=[ 0.01672456 -0.15851371 -0.02538596  0.09365964]\n",
      "[ episode 85 ][ timestamp 16 ] state=[ 0.01672456 -0.15851371 -0.02538596  0.09365964], action=0, reward=1.0, next_state=[ 0.01355428 -0.35326278 -0.02351277  0.37822645]\n",
      "[ episode 85 ][ timestamp 17 ] state=[ 0.01355428 -0.35326278 -0.02351277  0.37822645], action=0, reward=1.0, next_state=[ 0.00648903 -0.54804305 -0.01594824  0.66340398]\n",
      "[ episode 85 ][ timestamp 18 ] state=[ 0.00648903 -0.54804305 -0.01594824  0.66340398], action=1, reward=1.0, next_state=[-0.00447183 -0.3527029  -0.00268016  0.36574244]\n",
      "[ episode 85 ][ timestamp 19 ] state=[-0.00447183 -0.3527029  -0.00268016  0.36574244], action=1, reward=1.0, next_state=[-0.01152589 -0.15754296  0.00463469  0.07221562]\n",
      "[ episode 85 ][ timestamp 20 ] state=[-0.01152589 -0.15754296  0.00463469  0.07221562], action=1, reward=1.0, next_state=[-0.01467675  0.03751224  0.006079   -0.21900145]\n",
      "[ episode 85 ][ timestamp 21 ] state=[-0.01467675  0.03751224  0.006079   -0.21900145], action=0, reward=1.0, next_state=[-0.0139265  -0.15769608  0.00169897  0.07559283]\n",
      "[ episode 85 ][ timestamp 22 ] state=[-0.0139265  -0.15769608  0.00169897  0.07559283], action=1, reward=1.0, next_state=[-0.01708043  0.03740148  0.00321083 -0.21655358]\n",
      "[ episode 85 ][ timestamp 23 ] state=[-0.01708043  0.03740148  0.00321083 -0.21655358], action=1, reward=1.0, next_state=[-0.0163324   0.23247738 -0.00112024 -0.50822194]\n",
      "[ episode 85 ][ timestamp 24 ] state=[-0.0163324   0.23247738 -0.00112024 -0.50822194], action=1, reward=1.0, next_state=[-0.01168285  0.4276151  -0.01128468 -0.80125768]\n",
      "[ episode 85 ][ timestamp 25 ] state=[-0.01168285  0.4276151  -0.01128468 -0.80125768], action=0, reward=1.0, next_state=[-0.00313055  0.23264972 -0.02730984 -0.51214586]\n",
      "[ episode 85 ][ timestamp 26 ] state=[-0.00313055  0.23264972 -0.02730984 -0.51214586], action=1, reward=1.0, next_state=[ 0.00152245  0.42814548 -0.03755275 -0.81330826]\n",
      "[ episode 85 ][ timestamp 27 ] state=[ 0.00152245  0.42814548 -0.03755275 -0.81330826], action=1, reward=1.0, next_state=[ 0.01008536  0.62376109 -0.05381892 -1.11756271]\n",
      "[ episode 85 ][ timestamp 28 ] state=[ 0.01008536  0.62376109 -0.05381892 -1.11756271], action=0, reward=1.0, next_state=[ 0.02256058  0.4293851  -0.07617017 -0.842236  ]\n",
      "[ episode 85 ][ timestamp 29 ] state=[ 0.02256058  0.4293851  -0.07617017 -0.842236  ], action=0, reward=1.0, next_state=[ 0.03114828  0.23538086 -0.09301489 -0.57444581]\n",
      "[ episode 85 ][ timestamp 30 ] state=[ 0.03114828  0.23538086 -0.09301489 -0.57444581], action=1, reward=1.0, next_state=[ 0.0358559   0.43167529 -0.10450381 -0.89492161]\n",
      "[ episode 85 ][ timestamp 31 ] state=[ 0.0358559   0.43167529 -0.10450381 -0.89492161], action=1, reward=1.0, next_state=[ 0.0444894   0.62804723 -0.12240224 -1.21854078]\n",
      "[ episode 85 ][ timestamp 32 ] state=[ 0.0444894   0.62804723 -0.12240224 -1.21854078], action=1, reward=1.0, next_state=[ 0.05705035  0.82451595 -0.14677306 -1.54693541]\n",
      "[ episode 85 ][ timestamp 33 ] state=[ 0.05705035  0.82451595 -0.14677306 -1.54693541], action=1, reward=1.0, next_state=[ 0.07354067  1.02106371 -0.17771176 -1.88158369]\n",
      "[ episode 85 ][ timestamp 34 ] state=[ 0.07354067  1.02106371 -0.17771176 -1.88158369], action=0, reward=-1.0, next_state=[ 0.09396194  0.82826734 -0.21534344 -1.64891642]\n",
      "[ Ended! ] Episode 85: Exploration_rate=0.6563549768288433. Score=34.\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ] state=[ 0.01864208 -0.03335847 -0.00171924 -0.04238407]\n",
      "[ episode 86 ][ timestamp 1 ] state=[ 0.01864208 -0.03335847 -0.00171924 -0.04238407], action=1, reward=1.0, next_state=[ 0.01797491  0.16178809 -0.00256692 -0.33560894]\n",
      "[ episode 86 ][ timestamp 2 ] state=[ 0.01797491  0.16178809 -0.00256692 -0.33560894], action=0, reward=1.0, next_state=[ 0.02121067 -0.03329724 -0.0092791  -0.04373659]\n",
      "[ episode 86 ][ timestamp 3 ] state=[ 0.02121067 -0.03329724 -0.0092791  -0.04373659], action=1, reward=1.0, next_state=[ 0.02054472  0.16195653 -0.01015383 -0.33933266]\n",
      "[ episode 86 ][ timestamp 4 ] state=[ 0.02054472  0.16195653 -0.01015383 -0.33933266], action=1, reward=1.0, next_state=[ 0.02378385  0.35722148 -0.01694049 -0.63520016]\n",
      "[ episode 86 ][ timestamp 5 ] state=[ 0.02378385  0.35722148 -0.01694049 -0.63520016], action=0, reward=1.0, next_state=[ 0.03092828  0.16233986 -0.02964449 -0.34789993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 86 ][ timestamp 6 ] state=[ 0.03092828  0.16233986 -0.02964449 -0.34789993], action=1, reward=1.0, next_state=[ 0.03417508  0.35787064 -0.03660249 -0.64978144]\n",
      "[ episode 86 ][ timestamp 7 ] state=[ 0.03417508  0.35787064 -0.03660249 -0.64978144], action=0, reward=1.0, next_state=[ 0.04133249  0.16327715 -0.04959812 -0.36884543]\n",
      "[ episode 86 ][ timestamp 8 ] state=[ 0.04133249  0.16327715 -0.04959812 -0.36884543], action=0, reward=1.0, next_state=[ 0.04459804 -0.03110628 -0.05697502 -0.09220472]\n",
      "[ episode 86 ][ timestamp 9 ] state=[ 0.04459804 -0.03110628 -0.05697502 -0.09220472], action=1, reward=1.0, next_state=[ 0.04397591  0.16478406 -0.05881912 -0.40230504]\n",
      "[ episode 86 ][ timestamp 10 ] state=[ 0.04397591  0.16478406 -0.05881912 -0.40230504], action=1, reward=1.0, next_state=[ 0.04727159  0.3606888  -0.06686522 -0.71293682]\n",
      "[ episode 86 ][ timestamp 11 ] state=[ 0.04727159  0.3606888  -0.06686522 -0.71293682], action=1, reward=1.0, next_state=[ 0.05448537  0.55666969 -0.08112396 -1.02589496]\n",
      "[ episode 86 ][ timestamp 12 ] state=[ 0.05448537  0.55666969 -0.08112396 -1.02589496], action=1, reward=1.0, next_state=[ 0.06561876  0.75277255 -0.10164186 -1.34290614]\n",
      "[ episode 86 ][ timestamp 13 ] state=[ 0.06561876  0.75277255 -0.10164186 -1.34290614], action=1, reward=1.0, next_state=[ 0.08067421  0.94901581 -0.12849998 -1.66558308]\n",
      "[ episode 86 ][ timestamp 14 ] state=[ 0.08067421  0.94901581 -0.12849998 -1.66558308], action=0, reward=1.0, next_state=[ 0.09965453  0.75560209 -0.16181164 -1.41552958]\n",
      "[ episode 86 ][ timestamp 15 ] state=[ 0.09965453  0.75560209 -0.16181164 -1.41552958], action=0, reward=1.0, next_state=[ 0.11476657  0.56281166 -0.19012223 -1.17748685]\n",
      "[ episode 86 ][ timestamp 16 ] state=[ 0.11476657  0.56281166 -0.19012223 -1.17748685], action=0, reward=-1.0, next_state=[ 0.1260228   0.37059777 -0.21367197 -0.94992103]\n",
      "[ Ended! ] Episode 86: Exploration_rate=0.653073201944699. Score=16.\n",
      "[ Experience replay ] starts\n",
      "[ episode 87 ] state=[ 0.00266921  0.03011514  0.02598831 -0.01127348]\n",
      "[ episode 87 ][ timestamp 1 ] state=[ 0.00266921  0.03011514  0.02598831 -0.01127348], action=0, reward=1.0, next_state=[ 0.00327151 -0.16536969  0.02576284  0.28949445]\n",
      "[ episode 87 ][ timestamp 2 ] state=[ 0.00327151 -0.16536969  0.02576284  0.28949445], action=0, reward=1.0, next_state=[-3.58804552e-05 -3.60849361e-01  3.15527251e-02  5.90190087e-01]\n",
      "[ episode 87 ][ timestamp 3 ] state=[-3.58804552e-05 -3.60849361e-01  3.15527251e-02  5.90190087e-01], action=1, reward=1.0, next_state=[-0.00725287 -0.16618308  0.04335653  0.30761096]\n",
      "[ episode 87 ][ timestamp 4 ] state=[-0.00725287 -0.16618308  0.04335653  0.30761096], action=1, reward=1.0, next_state=[-0.01057653  0.02829512  0.04950875  0.02891063]\n",
      "[ episode 87 ][ timestamp 5 ] state=[-0.01057653  0.02829512  0.04950875  0.02891063], action=1, reward=1.0, next_state=[-0.01001063  0.22267342  0.05008696 -0.24774992]\n",
      "[ episode 87 ][ timestamp 6 ] state=[-0.01001063  0.22267342  0.05008696 -0.24774992], action=1, reward=1.0, next_state=[-0.00555716  0.4170456   0.04513196 -0.52422314]\n",
      "[ episode 87 ][ timestamp 7 ] state=[-0.00555716  0.4170456   0.04513196 -0.52422314], action=1, reward=1.0, next_state=[ 0.00278375  0.6115043   0.0346475  -0.80234989]\n",
      "[ episode 87 ][ timestamp 8 ] state=[ 0.00278375  0.6115043   0.0346475  -0.80234989], action=0, reward=1.0, next_state=[ 0.01501384  0.41592478  0.0186005  -0.49897235]\n",
      "[ episode 87 ][ timestamp 9 ] state=[ 0.01501384  0.41592478  0.0186005  -0.49897235], action=0, reward=1.0, next_state=[ 0.02333234  0.22054559  0.00862105 -0.20048604]\n",
      "[ episode 87 ][ timestamp 10 ] state=[ 0.02333234  0.22054559  0.00862105 -0.20048604], action=1, reward=1.0, next_state=[ 0.02774325  0.41554319  0.00461133 -0.49043701]\n",
      "[ episode 87 ][ timestamp 11 ] state=[ 0.02774325  0.41554319  0.00461133 -0.49043701], action=1, reward=1.0, next_state=[ 0.03605411  0.61059979 -0.00519741 -0.78166307]\n",
      "[ episode 87 ][ timestamp 12 ] state=[ 0.03605411  0.61059979 -0.00519741 -0.78166307], action=1, reward=1.0, next_state=[ 0.04826611  0.80579279 -0.02083067 -1.07597665]\n",
      "[ episode 87 ][ timestamp 13 ] state=[ 0.04826611  0.80579279 -0.02083067 -1.07597665], action=0, reward=1.0, next_state=[ 0.06438196  0.61095216 -0.0423502  -0.78990287]\n",
      "[ episode 87 ][ timestamp 14 ] state=[ 0.06438196  0.61095216 -0.0423502  -0.78990287], action=1, reward=1.0, next_state=[ 0.07660101  0.80662931 -0.05814826 -1.09560267]\n",
      "[ episode 87 ][ timestamp 15 ] state=[ 0.07660101  0.80662931 -0.05814826 -1.09560267], action=0, reward=1.0, next_state=[ 0.09273359  0.61231937 -0.08006031 -0.82171633]\n",
      "[ episode 87 ][ timestamp 16 ] state=[ 0.09273359  0.61231937 -0.08006031 -0.82171633], action=0, reward=1.0, next_state=[ 0.10497998  0.41837878 -0.09649464 -0.55524986]\n",
      "[ episode 87 ][ timestamp 17 ] state=[ 0.10497998  0.41837878 -0.09649464 -0.55524986], action=1, reward=1.0, next_state=[ 0.11334755  0.61471371 -0.10759964 -0.87670764]\n",
      "[ episode 87 ][ timestamp 18 ] state=[ 0.11334755  0.61471371 -0.10759964 -0.87670764], action=1, reward=1.0, next_state=[ 0.12564183  0.81112064 -0.12513379 -1.20118752]\n",
      "[ episode 87 ][ timestamp 19 ] state=[ 0.12564183  0.81112064 -0.12513379 -1.20118752], action=1, reward=1.0, next_state=[ 0.14186424  1.00761886 -0.14915754 -1.53032362]\n",
      "[ episode 87 ][ timestamp 20 ] state=[ 0.14186424  1.00761886 -0.14915754 -1.53032362], action=1, reward=1.0, next_state=[ 0.16201662  1.2041912  -0.17976401 -1.8655981 ]\n",
      "[ episode 87 ][ timestamp 21 ] state=[ 0.16201662  1.2041912  -0.17976401 -1.8655981 ], action=0, reward=-1.0, next_state=[ 0.18610044  1.01143592 -0.21707597 -1.63369071]\n",
      "[ Ended! ] Episode 87: Exploration_rate=0.6498078359349755. Score=21.\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ] state=[-0.01097904 -0.00337663  0.01189213  0.01249944]\n",
      "[ episode 88 ][ timestamp 1 ] state=[-0.01097904 -0.00337663  0.01189213  0.01249944], action=0, reward=1.0, next_state=[-0.01104657 -0.19866709  0.01214212  0.30891063]\n",
      "[ episode 88 ][ timestamp 2 ] state=[-0.01104657 -0.19866709  0.01214212  0.30891063], action=1, reward=1.0, next_state=[-0.01501991 -0.00372023  0.01832033  0.02008159]\n",
      "[ episode 88 ][ timestamp 3 ] state=[-0.01501991 -0.00372023  0.01832033  0.02008159], action=1, reward=1.0, next_state=[-0.01509432  0.19113426  0.01872197 -0.26676522]\n",
      "[ episode 88 ][ timestamp 4 ] state=[-0.01509432  0.19113426  0.01872197 -0.26676522], action=0, reward=1.0, next_state=[-0.01127163 -0.00424982  0.01338666  0.03176347]\n",
      "[ episode 88 ][ timestamp 5 ] state=[-0.01127163 -0.00424982  0.01338666  0.03176347], action=0, reward=1.0, next_state=[-0.01135663 -0.19956116  0.01402193  0.32863979]\n",
      "[ episode 88 ][ timestamp 6 ] state=[-0.01135663 -0.19956116  0.01402193  0.32863979], action=1, reward=1.0, next_state=[-0.01534785 -0.0046416   0.02059473  0.04041151]\n",
      "[ episode 88 ][ timestamp 7 ] state=[-0.01534785 -0.0046416   0.02059473  0.04041151], action=1, reward=1.0, next_state=[-0.01544068  0.19017906  0.02140296 -0.24570309]\n",
      "[ episode 88 ][ timestamp 8 ] state=[-0.01544068  0.19017906  0.02140296 -0.24570309], action=1, reward=1.0, next_state=[-0.0116371   0.38498889  0.01648889 -0.53155892]\n",
      "[ episode 88 ][ timestamp 9 ] state=[-0.0116371   0.38498889  0.01648889 -0.53155892], action=1, reward=1.0, next_state=[-0.00393732  0.57987508  0.00585772 -0.81900094]\n",
      "[ episode 88 ][ timestamp 10 ] state=[-0.00393732  0.57987508  0.00585772 -0.81900094], action=0, reward=1.0, next_state=[ 0.00766018  0.38467344 -0.0105223  -0.52448136]\n",
      "[ episode 88 ][ timestamp 11 ] state=[ 0.00766018  0.38467344 -0.0105223  -0.52448136], action=1, reward=1.0, next_state=[ 0.01535365  0.57994188 -0.02101193 -0.8204613 ]\n",
      "[ episode 88 ][ timestamp 12 ] state=[ 0.01535365  0.57994188 -0.02101193 -0.8204613 ], action=1, reward=1.0, next_state=[ 0.02695249  0.77534498 -0.03742116 -1.11967829]\n",
      "[ episode 88 ][ timestamp 13 ] state=[ 0.02695249  0.77534498 -0.03742116 -1.11967829], action=0, reward=1.0, next_state=[ 0.04245939  0.58073337 -0.05981472 -0.83896451]\n",
      "[ episode 88 ][ timestamp 14 ] state=[ 0.04245939  0.58073337 -0.05981472 -0.83896451], action=1, reward=1.0, next_state=[ 0.05407405  0.77661884 -0.07659401 -1.14984228]\n",
      "[ episode 88 ][ timestamp 15 ] state=[ 0.05407405  0.77661884 -0.07659401 -1.14984228], action=0, reward=1.0, next_state=[ 0.06960643  0.5825755  -0.09959086 -0.88212727]\n",
      "[ episode 88 ][ timestamp 16 ] state=[ 0.06960643  0.5825755  -0.09959086 -0.88212727], action=0, reward=1.0, next_state=[ 0.08125794  0.38893699 -0.1172334  -0.62234007]\n",
      "[ episode 88 ][ timestamp 17 ] state=[ 0.08125794  0.38893699 -0.1172334  -0.62234007], action=0, reward=1.0, next_state=[ 0.08903668  0.19563028 -0.1296802  -0.36875801]\n",
      "[ episode 88 ][ timestamp 18 ] state=[ 0.08903668  0.19563028 -0.1296802  -0.36875801], action=1, reward=1.0, next_state=[ 0.09294928  0.39233341 -0.13705536 -0.69935443]\n",
      "[ episode 88 ][ timestamp 19 ] state=[ 0.09294928  0.39233341 -0.13705536 -0.69935443], action=0, reward=1.0, next_state=[ 0.10079595  0.1993509  -0.15104245 -0.45276343]\n",
      "[ episode 88 ][ timestamp 20 ] state=[ 0.10079595  0.1993509  -0.15104245 -0.45276343], action=1, reward=1.0, next_state=[ 0.10478297  0.39624994 -0.16009772 -0.7889872 ]\n",
      "[ episode 88 ][ timestamp 21 ] state=[ 0.10478297  0.39624994 -0.16009772 -0.7889872 ], action=1, reward=1.0, next_state=[ 0.11270797  0.59316592 -0.17587747 -1.12745175]\n",
      "[ episode 88 ][ timestamp 22 ] state=[ 0.11270797  0.59316592 -0.17587747 -1.12745175], action=0, reward=1.0, next_state=[ 0.12457129  0.40072871 -0.1984265  -0.89469072]\n",
      "[ episode 88 ][ timestamp 23 ] state=[ 0.12457129  0.40072871 -0.1984265  -0.89469072], action=1, reward=-1.0, next_state=[ 0.13258586  0.59790768 -0.21632031 -1.24261093]\n",
      "[ Ended! ] Episode 88: Exploration_rate=0.6465587967553006. Score=23.\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ] state=[-0.04053788  0.00023133  0.00093916  0.03366997]\n",
      "[ episode 89 ][ timestamp 1 ] state=[-0.04053788  0.00023133  0.00093916  0.03366997], action=0, reward=1.0, next_state=[-0.04053325 -0.19490407  0.00161256  0.32664906]\n",
      "[ episode 89 ][ timestamp 2 ] state=[-0.04053325 -0.19490407  0.00161256  0.32664906], action=1, reward=1.0, next_state=[-0.04443134  0.00019488  0.00814554  0.0344751 ]\n",
      "[ episode 89 ][ timestamp 3 ] state=[-0.04443134  0.00019488  0.00814554  0.0344751 ], action=1, reward=1.0, next_state=[-0.04442744  0.19519908  0.00883504 -0.25562674]\n",
      "[ episode 89 ][ timestamp 4 ] state=[-0.04442744  0.19519908  0.00883504 -0.25562674], action=1, reward=1.0, next_state=[-0.04052346  0.39019378  0.0037225  -0.54550991]\n",
      "[ episode 89 ][ timestamp 5 ] state=[-0.04052346  0.39019378  0.0037225  -0.54550991], action=0, reward=1.0, next_state=[-0.03271958  0.19501973 -0.00718769 -0.25165643]\n",
      "[ episode 89 ][ timestamp 6 ] state=[-0.03271958  0.19501973 -0.00718769 -0.25165643], action=0, reward=1.0, next_state=[-2.88191867e-02  1.14637956e-06 -1.22208221e-02  3.87507170e-02]\n",
      "[ episode 89 ][ timestamp 7 ] state=[-2.88191867e-02  1.14637956e-06 -1.22208221e-02  3.87507170e-02], action=1, reward=1.0, next_state=[-0.02881916  0.19529619 -0.01144581 -0.25776281]\n",
      "[ episode 89 ][ timestamp 8 ] state=[-0.02881916  0.19529619 -0.01144581 -0.25776281], action=0, reward=1.0, next_state=[-0.02491324  0.0003395  -0.01660106  0.03128807]\n",
      "[ episode 89 ][ timestamp 9 ] state=[-0.02491324  0.0003395  -0.01660106  0.03128807], action=0, reward=1.0, next_state=[-0.02490645 -0.19454049 -0.0159753   0.3186873 ]\n",
      "[ episode 89 ][ timestamp 10 ] state=[-0.02490645 -0.19454049 -0.0159753   0.3186873 ], action=1, reward=1.0, next_state=[-0.02879726  0.0008053  -0.00960156  0.02100947]\n",
      "[ episode 89 ][ timestamp 11 ] state=[-0.02879726  0.0008053  -0.00960156  0.02100947], action=0, reward=1.0, next_state=[-0.02878115 -0.19417765 -0.00918137  0.31064759]\n",
      "[ episode 89 ][ timestamp 12 ] state=[-0.02878115 -0.19417765 -0.00918137  0.31064759], action=1, reward=1.0, next_state=[-0.03266471  0.0010739  -0.00296842  0.01508333]\n",
      "[ episode 89 ][ timestamp 13 ] state=[-0.03266471  0.0010739  -0.00296842  0.01508333], action=0, reward=1.0, next_state=[-0.03264323 -0.19400536 -0.00266675  0.30682821]\n",
      "[ episode 89 ][ timestamp 14 ] state=[-0.03264323 -0.19400536 -0.00266675  0.30682821], action=1, reward=1.0, next_state=[-0.03652334  0.00115449  0.00346982  0.01330545]\n",
      "[ episode 89 ][ timestamp 15 ] state=[-0.03652334  0.00115449  0.00346982  0.01330545], action=0, reward=1.0, next_state=[-0.03650025 -0.19401705  0.00373592  0.30708113]\n",
      "[ episode 89 ][ timestamp 16 ] state=[-0.03650025 -0.19401705  0.00373592  0.30708113], action=1, reward=1.0, next_state=[-0.04038059  0.00105147  0.00987755  0.01557875]\n",
      "[ episode 89 ][ timestamp 17 ] state=[-0.04038059  0.00105147  0.00987755  0.01557875], action=1, reward=1.0, next_state=[-0.04035956  0.19603038  0.01018912 -0.2739714 ]\n",
      "[ episode 89 ][ timestamp 18 ] state=[-0.04035956  0.19603038  0.01018912 -0.2739714 ], action=0, reward=1.0, next_state=[-0.03643895  0.00076454  0.00470969  0.02190771]\n",
      "[ episode 89 ][ timestamp 19 ] state=[-0.03643895  0.00076454  0.00470969  0.02190771], action=0, reward=1.0, next_state=[-0.03642366 -0.19442463  0.00514785  0.31607287]\n",
      "[ episode 89 ][ timestamp 20 ] state=[-0.03642366 -0.19442463  0.00514785  0.31607287], action=0, reward=1.0, next_state=[-0.04031215 -0.38961953  0.01146931  0.61037479]\n",
      "[ episode 89 ][ timestamp 21 ] state=[-0.04031215 -0.38961953  0.01146931  0.61037479], action=1, reward=1.0, next_state=[-0.04810454 -0.19465976  0.0236768   0.32132627]\n",
      "[ episode 89 ][ timestamp 22 ] state=[-0.04810454 -0.19465976  0.0236768   0.32132627], action=1, reward=1.0, next_state=[-0.05199774  0.00011716  0.03010333  0.03620312]\n",
      "[ episode 89 ][ timestamp 23 ] state=[-0.05199774  0.00011716  0.03010333  0.03620312], action=0, reward=1.0, next_state=[-0.05199539 -0.19542326  0.03082739  0.33822989]\n",
      "[ episode 89 ][ timestamp 24 ] state=[-0.05199539 -0.19542326  0.03082739  0.33822989], action=0, reward=1.0, next_state=[-0.05590386 -0.39097001  0.03759199  0.64047247]\n",
      "[ episode 89 ][ timestamp 25 ] state=[-0.05590386 -0.39097001  0.03759199  0.64047247], action=0, reward=1.0, next_state=[-0.06372326 -0.58659531  0.05040144  0.94475256]\n",
      "[ episode 89 ][ timestamp 26 ] state=[-0.06372326 -0.58659531  0.05040144  0.94475256], action=1, reward=1.0, next_state=[-0.07545517 -0.39218721  0.06929649  0.66832246]\n",
      "[ episode 89 ][ timestamp 27 ] state=[-0.07545517 -0.39218721  0.06929649  0.66832246], action=1, reward=1.0, next_state=[-0.08329891 -0.1980938   0.08266294  0.39823797]\n",
      "[ episode 89 ][ timestamp 28 ] state=[-0.08329891 -0.1980938   0.08266294  0.39823797], action=0, reward=1.0, next_state=[-0.08726079 -0.39428522  0.0906277   0.71579545]\n",
      "[ episode 89 ][ timestamp 29 ] state=[-0.08726079 -0.39428522  0.0906277   0.71579545], action=0, reward=1.0, next_state=[-0.09514649 -0.59053694  0.10494361  1.03557303]\n",
      "[ episode 89 ][ timestamp 30 ] state=[-0.09514649 -0.59053694  0.10494361  1.03557303], action=0, reward=1.0, next_state=[-0.10695723 -0.78688567  0.12565507  1.35927261]\n",
      "[ episode 89 ][ timestamp 31 ] state=[-0.10695723 -0.78688567  0.12565507  1.35927261], action=1, reward=1.0, next_state=[-0.12269494 -0.59354318  0.15284052  1.10839085]\n",
      "[ episode 89 ][ timestamp 32 ] state=[-0.12269494 -0.59354318  0.15284052  1.10839085], action=1, reward=1.0, next_state=[-0.13456581 -0.40072376  0.17500833  0.86729374]\n",
      "[ episode 89 ][ timestamp 33 ] state=[-0.13456581 -0.40072376  0.17500833  0.86729374], action=1, reward=1.0, next_state=[-0.14258028 -0.20835969  0.19235421  0.63434536]\n",
      "[ episode 89 ][ timestamp 34 ] state=[-0.14258028 -0.20835969  0.19235421  0.63434536], action=0, reward=1.0, next_state=[-0.14674748 -0.40557032  0.20504112  0.9809096 ]\n",
      "[ episode 89 ][ timestamp 35 ] state=[-0.14674748 -0.40557032  0.20504112  0.9809096 ], action=1, reward=-1.0, next_state=[-0.15485888 -0.21369812  0.22465931  0.75899071]\n",
      "[ Ended! ] Episode 89: Exploration_rate=0.6433260027715241. Score=35.\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ] state=[ 0.01860113 -0.02003506 -0.01781635  0.03360138]\n",
      "[ episode 90 ][ timestamp 1 ] state=[ 0.01860113 -0.02003506 -0.01781635  0.03360138], action=0, reward=1.0, next_state=[ 0.01820043 -0.21489705 -0.01714432  0.32061024]\n",
      "[ episode 90 ][ timestamp 2 ] state=[ 0.01820043 -0.21489705 -0.01714432  0.32061024], action=0, reward=1.0, next_state=[ 0.01390249 -0.4097707  -0.01073211  0.60783758]\n",
      "[ episode 90 ][ timestamp 3 ] state=[ 0.01390249 -0.4097707  -0.01073211  0.60783758], action=0, reward=1.0, next_state=[ 0.00570708 -0.60474097  0.00142464  0.89712097]\n",
      "[ episode 90 ][ timestamp 4 ] state=[ 0.00570708 -0.60474097  0.00142464  0.89712097], action=0, reward=1.0, next_state=[-0.00638774 -0.79988221  0.01936706  1.19025137]\n",
      "[ episode 90 ][ timestamp 5 ] state=[-0.00638774 -0.79988221  0.01936706  1.19025137], action=1, reward=1.0, next_state=[-0.02238539 -0.60501652  0.04317208  0.90370121]\n",
      "[ episode 90 ][ timestamp 6 ] state=[-0.02238539 -0.60501652  0.04317208  0.90370121], action=0, reward=1.0, next_state=[-0.03448572 -0.8006958   0.06124611  1.20963528]\n",
      "[ episode 90 ][ timestamp 7 ] state=[-0.03448572 -0.8006958   0.06124611  1.20963528], action=0, reward=1.0, next_state=[-0.05049963 -0.99655285  0.08543881  1.52086512]\n",
      "[ episode 90 ][ timestamp 8 ] state=[-0.05049963 -0.99655285  0.08543881  1.52086512], action=1, reward=1.0, next_state=[-0.07043069 -0.80256106  0.11585612  1.25602733]\n",
      "[ episode 90 ][ timestamp 9 ] state=[-0.07043069 -0.80256106  0.11585612  1.25602733], action=1, reward=1.0, next_state=[-0.08648191 -0.60909707  0.14097666  1.00176231]\n",
      "[ episode 90 ][ timestamp 10 ] state=[-0.08648191 -0.60909707  0.14097666  1.00176231], action=1, reward=1.0, next_state=[-0.09866385 -0.41611147  0.16101191  0.75646575]\n",
      "[ episode 90 ][ timestamp 11 ] state=[-0.09866385 -0.41611147  0.16101191  0.75646575], action=0, reward=1.0, next_state=[-0.10698608 -0.61304301  0.17614122  1.09517549]\n",
      "[ episode 90 ][ timestamp 12 ] state=[-0.10698608 -0.61304301  0.17614122  1.09517549], action=0, reward=1.0, next_state=[-0.11924694 -0.80999161  0.19804473  1.43754552]\n",
      "[ episode 90 ][ timestamp 13 ] state=[-0.11924694 -0.80999161  0.19804473  1.43754552], action=1, reward=-1.0, next_state=[-0.13544678 -0.61778398  0.22679564  1.21271494]\n",
      "[ Ended! ] Episode 90: Exploration_rate=0.6401093727576664. Score=13.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 91 ] state=[ 0.01318458 -0.0299232  -0.00424924 -0.02295416]\n",
      "[ episode 91 ][ timestamp 1 ] state=[ 0.01318458 -0.0299232  -0.00424924 -0.02295416], action=0, reward=1.0, next_state=[ 0.01258611 -0.22498396 -0.00470832  0.26838506]\n",
      "[ episode 91 ][ timestamp 2 ] state=[ 0.01258611 -0.22498396 -0.00470832  0.26838506], action=1, reward=1.0, next_state=[ 0.00808643 -0.02979513  0.00065938 -0.02577918]\n",
      "[ episode 91 ][ timestamp 3 ] state=[ 0.00808643 -0.02979513  0.00065938 -0.02577918], action=0, reward=1.0, next_state=[ 7.49052920e-03 -2.24926535e-01  1.43795534e-04  2.67111720e-01]\n",
      "[ episode 91 ][ timestamp 4 ] state=[ 7.49052920e-03 -2.24926535e-01  1.43795534e-04  2.67111720e-01], action=0, reward=1.0, next_state=[ 0.002992   -0.42005054  0.00548603  0.55984   ]\n",
      "[ episode 91 ][ timestamp 5 ] state=[ 0.002992   -0.42005054  0.00548603  0.55984   ], action=0, reward=1.0, next_state=[-0.00540901 -0.61524906  0.01668283  0.85424626]\n",
      "[ episode 91 ][ timestamp 6 ] state=[-0.00540901 -0.61524906  0.01668283  0.85424626], action=1, reward=1.0, next_state=[-0.01771399 -0.42035841  0.03376776  0.56685549]\n",
      "[ episode 91 ][ timestamp 7 ] state=[-0.01771399 -0.42035841  0.03376776  0.56685549], action=0, reward=1.0, next_state=[-0.02612116 -0.61593738  0.04510486  0.86998253]\n",
      "[ episode 91 ][ timestamp 8 ] state=[-0.02612116 -0.61593738  0.04510486  0.86998253], action=0, reward=1.0, next_state=[-0.03843991 -0.81164291  0.06250452  1.17649861]\n",
      "[ episode 91 ][ timestamp 9 ] state=[-0.03843991 -0.81164291  0.06250452  1.17649861], action=0, reward=1.0, next_state=[-0.05467277 -1.00751867  0.08603449  1.48810285]\n",
      "[ episode 91 ][ timestamp 10 ] state=[-0.05467277 -1.00751867  0.08603449  1.48810285], action=1, reward=1.0, next_state=[-0.07482314 -0.81354365  0.11579654  1.22347945]\n",
      "[ episode 91 ][ timestamp 11 ] state=[-0.07482314 -0.81354365  0.11579654  1.22347945], action=0, reward=1.0, next_state=[-0.09109401 -1.00995092  0.14026613  1.55008551]\n",
      "[ episode 91 ][ timestamp 12 ] state=[-0.09109401 -1.00995092  0.14026613  1.55008551], action=1, reward=1.0, next_state=[-0.11129303 -0.81676287  0.17126784  1.3042526 ]\n",
      "[ episode 91 ][ timestamp 13 ] state=[-0.11129303 -0.81676287  0.17126784  1.3042526 ], action=0, reward=1.0, next_state=[-0.12762829 -1.01359212  0.1973529   1.64528384]\n",
      "[ episode 91 ][ timestamp 14 ] state=[-0.12762829 -1.01359212  0.1973529   1.64528384], action=0, reward=-1.0, next_state=[-0.14790013 -1.21040003  0.23025857  1.9924112 ]\n",
      "[ Ended! ] Episode 91: Exploration_rate=0.6369088258938781. Score=14.\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ] state=[ 0.04217235  0.00942058 -0.00576082  0.019292  ]\n",
      "[ episode 92 ][ timestamp 1 ] state=[ 0.04217235  0.00942058 -0.00576082  0.019292  ], action=0, reward=1.0, next_state=[ 0.04236076 -0.18561829 -0.00537498  0.31015177]\n",
      "[ episode 92 ][ timestamp 2 ] state=[ 0.04236076 -0.18561829 -0.00537498  0.31015177], action=0, reward=1.0, next_state=[ 0.03864839 -0.38066325  0.00082806  0.60113475]\n",
      "[ episode 92 ][ timestamp 3 ] state=[ 0.03864839 -0.38066325  0.00082806  0.60113475], action=1, reward=1.0, next_state=[ 0.03103513 -0.18555289  0.01285075  0.30871276]\n",
      "[ episode 92 ][ timestamp 4 ] state=[ 0.03103513 -0.18555289  0.01285075  0.30871276], action=1, reward=1.0, next_state=[0.02732407 0.00938363 0.01902501 0.02011015]\n",
      "[ episode 92 ][ timestamp 5 ] state=[0.02732407 0.00938363 0.01902501 0.02011015], action=0, reward=1.0, next_state=[ 0.02751174 -0.18600593  0.01942721  0.31873445]\n",
      "[ episode 92 ][ timestamp 6 ] state=[ 0.02751174 -0.18600593  0.01942721  0.31873445], action=0, reward=1.0, next_state=[ 0.02379162 -0.3813991   0.0258019   0.61748015]\n",
      "[ episode 92 ][ timestamp 7 ] state=[ 0.02379162 -0.3813991   0.0258019   0.61748015], action=0, reward=1.0, next_state=[ 0.01616364 -0.57687181  0.0381515   0.91817653]\n",
      "[ episode 92 ][ timestamp 8 ] state=[ 0.01616364 -0.57687181  0.0381515   0.91817653], action=0, reward=1.0, next_state=[ 0.00462621 -0.77248818  0.05651503  1.22260139]\n",
      "[ episode 92 ][ timestamp 9 ] state=[ 0.00462621 -0.77248818  0.05651503  1.22260139], action=1, reward=1.0, next_state=[-0.01082356 -0.57813801  0.08096706  0.94814815]\n",
      "[ episode 92 ][ timestamp 10 ] state=[-0.01082356 -0.57813801  0.08096706  0.94814815], action=1, reward=1.0, next_state=[-0.02238632 -0.38419407  0.09993002  0.68196361]\n",
      "[ episode 92 ][ timestamp 11 ] state=[-0.02238632 -0.38419407  0.09993002  0.68196361], action=1, reward=1.0, next_state=[-0.0300702  -0.19059147  0.1135693   0.42233904]\n",
      "[ episode 92 ][ timestamp 12 ] state=[-0.0300702  -0.19059147  0.1135693   0.42233904], action=1, reward=1.0, next_state=[-0.03388203  0.00275373  0.12201608  0.16750719]\n",
      "[ episode 92 ][ timestamp 13 ] state=[-0.03388203  0.00275373  0.12201608  0.16750719], action=0, reward=1.0, next_state=[-0.03382695 -0.19388431  0.12536622  0.49605511]\n",
      "[ episode 92 ][ timestamp 14 ] state=[-0.03382695 -0.19388431  0.12536622  0.49605511], action=1, reward=1.0, next_state=[-0.03770464 -0.00073246  0.13528732  0.24536233]\n",
      "[ episode 92 ][ timestamp 15 ] state=[-0.03770464 -0.00073246  0.13528732  0.24536233], action=0, reward=1.0, next_state=[-0.03771929 -0.19750121  0.14019457  0.57747178]\n",
      "[ episode 92 ][ timestamp 16 ] state=[-0.03771929 -0.19750121  0.14019457  0.57747178], action=1, reward=1.0, next_state=[-0.04166931 -0.00459376  0.151744    0.33203191]\n",
      "[ episode 92 ][ timestamp 17 ] state=[-0.04166931 -0.00459376  0.151744    0.33203191], action=1, reward=1.0, next_state=[-0.04176119  0.18807933  0.15838464  0.09078502]\n",
      "[ episode 92 ][ timestamp 18 ] state=[-0.04176119  0.18807933  0.15838464  0.09078502], action=0, reward=1.0, next_state=[-0.0379996  -0.00891644  0.16020034  0.42895073]\n",
      "[ episode 92 ][ timestamp 19 ] state=[-0.0379996  -0.00891644  0.16020034  0.42895073], action=0, reward=1.0, next_state=[-0.03817793 -0.2059014   0.16877936  0.76754238]\n",
      "[ episode 92 ][ timestamp 20 ] state=[-0.03817793 -0.2059014   0.16877936  0.76754238], action=0, reward=1.0, next_state=[-0.04229596 -0.40289492  0.1841302   1.10821977]\n",
      "[ episode 92 ][ timestamp 21 ] state=[-0.04229596 -0.40289492  0.1841302   1.10821977], action=0, reward=1.0, next_state=[-0.05035386 -0.59989569  0.2062946   1.45255464]\n",
      "[ episode 92 ][ timestamp 22 ] state=[-0.05035386 -0.59989569  0.2062946   1.45255464], action=1, reward=-1.0, next_state=[-0.06235177 -0.40781712  0.23534569  1.2307672 ]\n",
      "[ Ended! ] Episode 92: Exploration_rate=0.6337242817644086. Score=22.\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ] state=[-0.04258865  0.00430705 -0.01717628 -0.00979547]\n",
      "[ episode 93 ][ timestamp 1 ] state=[-0.04258865  0.00430705 -0.01717628 -0.00979547], action=1, reward=1.0, next_state=[-0.04250251  0.19967107 -0.01737219 -0.30784785]\n",
      "[ episode 93 ][ timestamp 2 ] state=[-0.04250251  0.19967107 -0.01737219 -0.30784785], action=0, reward=1.0, next_state=[-0.03850909  0.00480091 -0.02352915 -0.02069388]\n",
      "[ episode 93 ][ timestamp 3 ] state=[-0.03850909  0.00480091 -0.02352915 -0.02069388], action=1, reward=1.0, next_state=[-0.03841307  0.20025225 -0.02394302 -0.32070667]\n",
      "[ episode 93 ][ timestamp 4 ] state=[-0.03841307  0.20025225 -0.02394302 -0.32070667], action=0, reward=1.0, next_state=[-0.03440802  0.00547931 -0.03035716 -0.03566958]\n",
      "[ episode 93 ][ timestamp 5 ] state=[-0.03440802  0.00547931 -0.03035716 -0.03566958], action=1, reward=1.0, next_state=[-0.03429844  0.20102314 -0.03107055 -0.33777382]\n",
      "[ episode 93 ][ timestamp 6 ] state=[-0.03429844  0.20102314 -0.03107055 -0.33777382], action=0, reward=1.0, next_state=[-0.03027798  0.00635679 -0.03782602 -0.05504849]\n",
      "[ episode 93 ][ timestamp 7 ] state=[-0.03027798  0.00635679 -0.03782602 -0.05504849], action=0, reward=1.0, next_state=[-0.03015084 -0.18820295 -0.03892699  0.22546416]\n",
      "[ episode 93 ][ timestamp 8 ] state=[-0.03015084 -0.18820295 -0.03892699  0.22546416], action=1, reward=1.0, next_state=[-0.0339149   0.00745309 -0.03441771 -0.07923922]\n",
      "[ episode 93 ][ timestamp 9 ] state=[-0.0339149   0.00745309 -0.03441771 -0.07923922], action=1, reward=1.0, next_state=[-0.03376584  0.2030511  -0.0360025  -0.38257928]\n",
      "[ episode 93 ][ timestamp 10 ] state=[-0.03376584  0.2030511  -0.0360025  -0.38257928], action=0, reward=1.0, next_state=[-0.02970481  0.00845834 -0.04365408 -0.10146174]\n",
      "[ episode 93 ][ timestamp 11 ] state=[-0.02970481  0.00845834 -0.04365408 -0.10146174], action=1, reward=1.0, next_state=[-0.02953565  0.20417785 -0.04568332 -0.40759153]\n",
      "[ episode 93 ][ timestamp 12 ] state=[-0.02953565  0.20417785 -0.04568332 -0.40759153], action=1, reward=1.0, next_state=[-0.02545209  0.39991678 -0.05383515 -0.71431983]\n",
      "[ episode 93 ][ timestamp 13 ] state=[-0.02545209  0.39991678 -0.05383515 -0.71431983], action=0, reward=1.0, next_state=[-0.01745376  0.2055798  -0.06812154 -0.43905657]\n",
      "[ episode 93 ][ timestamp 14 ] state=[-0.01745376  0.2055798  -0.06812154 -0.43905657], action=0, reward=1.0, next_state=[-0.01334216  0.01148479 -0.07690268 -0.16860158]\n",
      "[ episode 93 ][ timestamp 15 ] state=[-0.01334216  0.01148479 -0.07690268 -0.16860158], action=1, reward=1.0, next_state=[-0.01311246  0.20761846 -0.08027471 -0.48451966]\n",
      "[ episode 93 ][ timestamp 16 ] state=[-0.01311246  0.20761846 -0.08027471 -0.48451966], action=0, reward=1.0, next_state=[-0.00896009  0.01371569 -0.0899651  -0.21817757]\n",
      "[ episode 93 ][ timestamp 17 ] state=[-0.00896009  0.01371569 -0.0899651  -0.21817757], action=1, reward=1.0, next_state=[-0.00868578  0.21000082 -0.09432865 -0.53782863]\n",
      "[ episode 93 ][ timestamp 18 ] state=[-0.00868578  0.21000082 -0.09432865 -0.53782863], action=1, reward=1.0, next_state=[-0.00448576  0.40631356 -0.10508522 -0.85868015]\n",
      "[ episode 93 ][ timestamp 19 ] state=[-0.00448576  0.40631356 -0.10508522 -0.85868015], action=1, reward=1.0, next_state=[ 0.00364051  0.60269796 -0.12225883 -1.18246998]\n",
      "[ episode 93 ][ timestamp 20 ] state=[ 0.00364051  0.60269796 -0.12225883 -1.18246998], action=1, reward=1.0, next_state=[ 0.01569447  0.79917568 -0.14590823 -1.51084133]\n",
      "[ episode 93 ][ timestamp 21 ] state=[ 0.01569447  0.79917568 -0.14590823 -1.51084133], action=0, reward=1.0, next_state=[ 0.03167798  0.60609156 -0.17612505 -1.26703761]\n",
      "[ episode 93 ][ timestamp 22 ] state=[ 0.03167798  0.60609156 -0.17612505 -1.26703761], action=0, reward=1.0, next_state=[ 0.04379981  0.41360157 -0.20146581 -1.03428282]\n",
      "[ episode 93 ][ timestamp 23 ] state=[ 0.04379981  0.41360157 -0.20146581 -1.03428282], action=1, reward=-1.0, next_state=[ 0.05207184  0.61074882 -0.22215146 -1.38285361]\n",
      "[ Ended! ] Episode 93: Exploration_rate=0.6305556603555866. Score=23.\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ] state=[ 0.00413333 -0.03544184 -0.04756482  0.01108778]\n",
      "[ episode 94 ][ timestamp 1 ] state=[ 0.00413333 -0.03544184 -0.04756482  0.01108778], action=1, reward=1.0, next_state=[ 0.0034245   0.16032884 -0.04734307 -0.2962149 ]\n",
      "[ episode 94 ][ timestamp 2 ] state=[ 0.0034245   0.16032884 -0.04734307 -0.2962149 ], action=0, reward=1.0, next_state=[ 0.00663107 -0.03408734 -0.05326737 -0.01883104]\n",
      "[ episode 94 ][ timestamp 3 ] state=[ 0.00663107 -0.03408734 -0.05326737 -0.01883104], action=0, reward=1.0, next_state=[ 0.00594933 -0.22840652 -0.05364399  0.2565811 ]\n",
      "[ episode 94 ][ timestamp 4 ] state=[ 0.00594933 -0.22840652 -0.05364399  0.2565811 ], action=0, reward=1.0, next_state=[ 0.0013812  -0.42272318 -0.04851236  0.53187303]\n",
      "[ episode 94 ][ timestamp 5 ] state=[ 0.0013812  -0.42272318 -0.04851236  0.53187303], action=1, reward=1.0, next_state=[-0.00707327 -0.22695365 -0.0378749   0.22430718]\n",
      "[ episode 94 ][ timestamp 6 ] state=[-0.00707327 -0.22695365 -0.0378749   0.22430718], action=1, reward=1.0, next_state=[-0.01161234 -0.03131142 -0.03338876 -0.08007826]\n",
      "[ episode 94 ][ timestamp 7 ] state=[-0.01161234 -0.03131142 -0.03338876 -0.08007826], action=0, reward=1.0, next_state=[-0.01223857 -0.22593922 -0.03499033  0.20188626]\n",
      "[ episode 94 ][ timestamp 8 ] state=[-0.01223857 -0.22593922 -0.03499033  0.20188626], action=1, reward=1.0, next_state=[-0.01675735 -0.03033478 -0.0309526  -0.10162588]\n",
      "[ episode 94 ][ timestamp 9 ] state=[-0.01675735 -0.03033478 -0.0309526  -0.10162588], action=1, reward=1.0, next_state=[-0.01736405  0.16521678 -0.03298512 -0.40391132]\n",
      "[ episode 94 ][ timestamp 10 ] state=[-0.01736405  0.16521678 -0.03298512 -0.40391132], action=0, reward=1.0, next_state=[-0.01405971 -0.02942222 -0.04106334 -0.1218075 ]\n",
      "[ episode 94 ][ timestamp 11 ] state=[-0.01405971 -0.02942222 -0.04106334 -0.1218075 ], action=1, reward=1.0, next_state=[-0.01464816  0.16626326 -0.04349949 -0.42715751]\n",
      "[ episode 94 ][ timestamp 12 ] state=[-0.01464816  0.16626326 -0.04349949 -0.42715751], action=1, reward=1.0, next_state=[-0.01132289  0.36197345 -0.05204264 -0.73322991]\n",
      "[ episode 94 ][ timestamp 13 ] state=[-0.01132289  0.36197345 -0.05204264 -0.73322991], action=0, reward=1.0, next_state=[-0.00408342  0.16760771 -0.06670724 -0.45736968]\n",
      "[ episode 94 ][ timestamp 14 ] state=[-0.00408342  0.16760771 -0.06670724 -0.45736968], action=0, reward=1.0, next_state=[-0.00073127 -0.02651089 -0.07585464 -0.18643677]\n",
      "[ episode 94 ][ timestamp 15 ] state=[-0.00073127 -0.02651089 -0.07585464 -0.18643677], action=0, reward=1.0, next_state=[-0.00126149 -0.2204702  -0.07958337  0.0813857 ]\n",
      "[ episode 94 ][ timestamp 16 ] state=[-0.00126149 -0.2204702  -0.07958337  0.0813857 ], action=0, reward=1.0, next_state=[-0.00567089 -0.41436647 -0.07795566  0.34793674]\n",
      "[ episode 94 ][ timestamp 17 ] state=[-0.00567089 -0.41436647 -0.07795566  0.34793674], action=0, reward=1.0, next_state=[-0.01395822 -0.60829809 -0.07099692  0.61505495]\n",
      "[ episode 94 ][ timestamp 18 ] state=[-0.01395822 -0.60829809 -0.07099692  0.61505495], action=1, reward=1.0, next_state=[-0.02612418 -0.41225972 -0.05869582  0.30088262]\n",
      "[ episode 94 ][ timestamp 19 ] state=[-0.02612418 -0.41225972 -0.05869582  0.30088262], action=1, reward=1.0, next_state=[-0.03436938 -0.21635242 -0.05267817 -0.00971892]\n",
      "[ episode 94 ][ timestamp 20 ] state=[-0.03436938 -0.21635242 -0.05267817 -0.00971892], action=1, reward=1.0, next_state=[-0.03869643 -0.02051612 -0.05287255 -0.31854611]\n",
      "[ episode 94 ][ timestamp 21 ] state=[-0.03869643 -0.02051612 -0.05287255 -0.31854611], action=0, reward=1.0, next_state=[-0.03910675 -0.21484673 -0.05924347 -0.04299483]\n",
      "[ episode 94 ][ timestamp 22 ] state=[-0.03910675 -0.21484673 -0.05924347 -0.04299483], action=0, reward=1.0, next_state=[-0.04340368 -0.40907132 -0.06010337  0.23042354]\n",
      "[ episode 94 ][ timestamp 23 ] state=[-0.04340368 -0.40907132 -0.06010337  0.23042354], action=1, reward=1.0, next_state=[-0.05158511 -0.21314431 -0.0554949  -0.08059606]\n",
      "[ episode 94 ][ timestamp 24 ] state=[-0.05158511 -0.21314431 -0.0554949  -0.08059606], action=1, reward=1.0, next_state=[-0.055848   -0.01727256 -0.05710682 -0.39025851]\n",
      "[ episode 94 ][ timestamp 25 ] state=[-0.055848   -0.01727256 -0.05710682 -0.39025851], action=0, reward=1.0, next_state=[-0.05619345 -0.21153947 -0.06491199 -0.11611345]\n",
      "[ episode 94 ][ timestamp 26 ] state=[-0.05619345 -0.21153947 -0.06491199 -0.11611345], action=0, reward=1.0, next_state=[-0.06042424 -0.40567418 -0.06723426  0.15540461]\n",
      "[ episode 94 ][ timestamp 27 ] state=[-0.06042424 -0.40567418 -0.06723426  0.15540461], action=0, reward=1.0, next_state=[-0.06853772 -0.59977228 -0.06412617  0.42614196]\n",
      "[ episode 94 ][ timestamp 28 ] state=[-0.06853772 -0.59977228 -0.06412617  0.42614196], action=1, reward=1.0, next_state=[-0.08053316 -0.40380342 -0.05560333  0.11395268]\n",
      "[ episode 94 ][ timestamp 29 ] state=[-0.08053316 -0.40380342 -0.05560333  0.11395268], action=1, reward=1.0, next_state=[-0.08860923 -0.20793065 -0.05332427 -0.19574136]\n",
      "[ episode 94 ][ timestamp 30 ] state=[-0.08860923 -0.20793065 -0.05332427 -0.19574136], action=1, reward=1.0, next_state=[-0.09276785 -0.0120881  -0.0572391  -0.50475753]\n",
      "[ episode 94 ][ timestamp 31 ] state=[-0.09276785 -0.0120881  -0.0572391  -0.50475753], action=1, reward=1.0, next_state=[-0.09300961  0.18379183 -0.06733425 -0.81491534]\n",
      "[ episode 94 ][ timestamp 32 ] state=[-0.09300961  0.18379183 -0.06733425 -0.81491534], action=0, reward=1.0, next_state=[-0.08933377 -0.01034665 -0.08363256 -0.54414883]\n",
      "[ episode 94 ][ timestamp 33 ] state=[-0.08933377 -0.01034665 -0.08363256 -0.54414883], action=1, reward=1.0, next_state=[-0.0895407   0.18584482 -0.09451553 -0.86196678]\n",
      "[ episode 94 ][ timestamp 34 ] state=[-0.0895407   0.18584482 -0.09451553 -0.86196678], action=1, reward=1.0, next_state=[-0.08582381  0.38211789 -0.11175487 -1.18280856]\n",
      "[ episode 94 ][ timestamp 35 ] state=[-0.08582381  0.38211789 -0.11175487 -1.18280856], action=0, reward=1.0, next_state=[-0.07818145  0.18860923 -0.13541104 -0.92714385]\n",
      "[ episode 94 ][ timestamp 36 ] state=[-0.07818145  0.18860923 -0.13541104 -0.92714385], action=1, reward=1.0, next_state=[-0.07440927  0.38527406 -0.15395392 -1.25912996]\n",
      "[ episode 94 ][ timestamp 37 ] state=[-0.07440927  0.38527406 -0.15395392 -1.25912996], action=1, reward=1.0, next_state=[-0.06670378  0.58199355 -0.17913652 -1.595803  ]\n",
      "[ episode 94 ][ timestamp 38 ] state=[-0.06670378  0.58199355 -0.17913652 -1.595803  ], action=0, reward=-1.0, next_state=[-0.05506391  0.38939081 -0.21105258 -1.36390687]\n",
      "[ Ended! ] Episode 94: Exploration_rate=0.6274028820538087. Score=38.\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ] state=[ 0.02665077 -0.02753811 -0.01615269 -0.02103089]\n",
      "[ episode 95 ][ timestamp 1 ] state=[ 0.02665077 -0.02753811 -0.01615269 -0.02103089], action=1, reward=1.0, next_state=[ 0.02610001  0.16781172 -0.0165733  -0.31876609]\n",
      "[ episode 95 ][ timestamp 2 ] state=[ 0.02610001  0.16781172 -0.0165733  -0.31876609], action=0, reward=1.0, next_state=[ 0.02945624 -0.02707032 -0.02294863 -0.03135551]\n",
      "[ episode 95 ][ timestamp 3 ] state=[ 0.02945624 -0.02707032 -0.02294863 -0.03135551], action=1, reward=1.0, next_state=[ 0.02891483  0.16837308 -0.02357574 -0.33118973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 95 ][ timestamp 4 ] state=[ 0.02891483  0.16837308 -0.02357574 -0.33118973], action=0, reward=1.0, next_state=[ 0.0322823  -0.02640549 -0.03019953 -0.04603369]\n",
      "[ episode 95 ][ timestamp 5 ] state=[ 0.0322823  -0.02640549 -0.03019953 -0.04603369], action=0, reward=1.0, next_state=[ 0.03175419 -0.22108167 -0.0311202   0.23697012]\n",
      "[ episode 95 ][ timestamp 6 ] state=[ 0.03175419 -0.22108167 -0.0311202   0.23697012], action=1, reward=1.0, next_state=[ 0.02733255 -0.02552926 -0.0263808  -0.06536433]\n",
      "[ episode 95 ][ timestamp 7 ] state=[ 0.02733255 -0.02552926 -0.0263808  -0.06536433], action=0, reward=1.0, next_state=[ 0.02682197 -0.22026324 -0.02768809  0.21887995]\n",
      "[ episode 95 ][ timestamp 8 ] state=[ 0.02682197 -0.22026324 -0.02768809  0.21887995], action=1, reward=1.0, next_state=[ 0.0224167  -0.02475666 -0.02331049 -0.08240677]\n",
      "[ episode 95 ][ timestamp 9 ] state=[ 0.0224167  -0.02475666 -0.02331049 -0.08240677], action=1, reward=1.0, next_state=[ 0.02192157  0.17069155 -0.02495862 -0.38235211]\n",
      "[ episode 95 ][ timestamp 10 ] state=[ 0.02192157  0.17069155 -0.02495862 -0.38235211], action=0, reward=1.0, next_state=[ 0.0253354  -0.02406729 -0.03260567 -0.09764191]\n",
      "[ episode 95 ][ timestamp 11 ] state=[ 0.0253354  -0.02406729 -0.03260567 -0.09764191], action=0, reward=1.0, next_state=[ 0.02485406 -0.21870713 -0.0345585   0.1845783 ]\n",
      "[ episode 95 ][ timestamp 12 ] state=[ 0.02485406 -0.21870713 -0.0345585   0.1845783 ], action=1, reward=1.0, next_state=[ 0.02047991 -0.02310819 -0.03086694 -0.11880311]\n",
      "[ episode 95 ][ timestamp 13 ] state=[ 0.02047991 -0.02310819 -0.03086694 -0.11880311], action=0, reward=1.0, next_state=[ 0.02001775 -0.2177746  -0.033243    0.16398398]\n",
      "[ episode 95 ][ timestamp 14 ] state=[ 0.02001775 -0.2177746  -0.033243    0.16398398], action=1, reward=1.0, next_state=[ 0.01566226 -0.02219293 -0.02996332 -0.13899808]\n",
      "[ episode 95 ][ timestamp 15 ] state=[ 0.01566226 -0.02219293 -0.02996332 -0.13899808], action=1, reward=1.0, next_state=[ 0.0152184   0.17334508 -0.03274328 -0.44098133]\n",
      "[ episode 95 ][ timestamp 16 ] state=[ 0.0152184   0.17334508 -0.03274328 -0.44098133], action=0, reward=1.0, next_state=[ 0.0186853  -0.02129857 -0.04156291 -0.15879716]\n",
      "[ episode 95 ][ timestamp 17 ] state=[ 0.0186853  -0.02129857 -0.04156291 -0.15879716], action=1, reward=1.0, next_state=[ 0.01825933  0.17439303 -0.04473885 -0.46429703]\n",
      "[ episode 95 ][ timestamp 18 ] state=[ 0.01825933  0.17439303 -0.04473885 -0.46429703], action=1, reward=1.0, next_state=[ 0.02174719  0.37011769 -0.05402479 -0.77073909]\n",
      "[ episode 95 ][ timestamp 19 ] state=[ 0.02174719  0.37011769 -0.05402479 -0.77073909], action=0, reward=1.0, next_state=[ 0.02914954  0.17577919 -0.06943958 -0.49553222]\n",
      "[ episode 95 ][ timestamp 20 ] state=[ 0.02914954  0.17577919 -0.06943958 -0.49553222], action=0, reward=1.0, next_state=[ 0.03266513 -0.01829835 -0.07935022 -0.22551632]\n",
      "[ episode 95 ][ timestamp 21 ] state=[ 0.03266513 -0.01829835 -0.07935022 -0.22551632], action=0, reward=1.0, next_state=[ 0.03229916 -0.21220186 -0.08386055  0.04111926]\n",
      "[ episode 95 ][ timestamp 22 ] state=[ 0.03229916 -0.21220186 -0.08386055  0.04111926], action=0, reward=1.0, next_state=[ 0.02805512 -0.40602739 -0.08303816  0.30620972]\n",
      "[ episode 95 ][ timestamp 23 ] state=[ 0.02805512 -0.40602739 -0.08303816  0.30620972], action=0, reward=1.0, next_state=[ 0.01993457 -0.59987393 -0.07691397  0.57159245]\n",
      "[ episode 95 ][ timestamp 24 ] state=[ 0.01993457 -0.59987393 -0.07691397  0.57159245], action=0, reward=1.0, next_state=[ 0.0079371  -0.79383786 -0.06548212  0.83908778]\n",
      "[ episode 95 ][ timestamp 25 ] state=[ 0.0079371  -0.79383786 -0.06548212  0.83908778], action=1, reward=1.0, next_state=[-0.00793966 -0.59788582 -0.04870036  0.52655168]\n",
      "[ episode 95 ][ timestamp 26 ] state=[-0.00793966 -0.59788582 -0.04870036  0.52655168], action=1, reward=1.0, next_state=[-0.01989738 -0.40211366 -0.03816933  0.21892935]\n",
      "[ episode 95 ][ timestamp 27 ] state=[-0.01989738 -0.40211366 -0.03816933  0.21892935], action=1, reward=1.0, next_state=[-0.02793965 -0.20646747 -0.03379074 -0.08554524]\n",
      "[ episode 95 ][ timestamp 28 ] state=[-0.02793965 -0.20646747 -0.03379074 -0.08554524], action=0, reward=1.0, next_state=[-0.032069   -0.40108916 -0.03550165  0.19628806]\n",
      "[ episode 95 ][ timestamp 29 ] state=[-0.032069   -0.40108916 -0.03550165  0.19628806], action=1, reward=1.0, next_state=[-0.04009078 -0.20547786 -0.03157588 -0.10737929]\n",
      "[ episode 95 ][ timestamp 30 ] state=[-0.04009078 -0.20547786 -0.03157588 -0.10737929], action=0, reward=1.0, next_state=[-0.04420034 -0.40013343 -0.03372347  0.17517674]\n",
      "[ episode 95 ][ timestamp 31 ] state=[-0.04420034 -0.40013343 -0.03372347  0.17517674], action=0, reward=1.0, next_state=[-0.05220301 -0.59475692 -0.03021994  0.45703317]\n",
      "[ episode 95 ][ timestamp 32 ] state=[-0.05220301 -0.59475692 -0.03021994  0.45703317], action=1, reward=1.0, next_state=[-0.06409815 -0.39922106 -0.02107927  0.15497998]\n",
      "[ episode 95 ][ timestamp 33 ] state=[-0.06409815 -0.39922106 -0.02107927  0.15497998], action=1, reward=1.0, next_state=[-0.07208257 -0.20380373 -0.01797967 -0.14427773]\n",
      "[ episode 95 ][ timestamp 34 ] state=[-0.07208257 -0.20380373 -0.01797967 -0.14427773], action=1, reward=1.0, next_state=[-0.07615864 -0.00842897 -0.02086523 -0.44257824]\n",
      "[ episode 95 ][ timestamp 35 ] state=[-0.07615864 -0.00842897 -0.02086523 -0.44257824], action=0, reward=1.0, next_state=[-0.07632722 -0.20324955 -0.02971679 -0.15654491]\n",
      "[ episode 95 ][ timestamp 36 ] state=[-0.07632722 -0.20324955 -0.02971679 -0.15654491], action=1, reward=1.0, next_state=[-0.08039221 -0.00771501 -0.03284769 -0.45845268]\n",
      "[ episode 95 ][ timestamp 37 ] state=[-0.08039221 -0.00771501 -0.03284769 -0.45845268], action=0, reward=1.0, next_state=[-0.08054651 -0.20235759 -0.04201674 -0.17630179]\n",
      "[ episode 95 ][ timestamp 38 ] state=[-0.08054651 -0.20235759 -0.04201674 -0.17630179], action=0, reward=1.0, next_state=[-0.08459367 -0.39685383 -0.04554278  0.1028358 ]\n",
      "[ episode 95 ][ timestamp 39 ] state=[-0.08459367 -0.39685383 -0.04554278  0.1028358 ], action=0, reward=1.0, next_state=[-0.09253074 -0.59129452 -0.04348606  0.38080945]\n",
      "[ episode 95 ][ timestamp 40 ] state=[-0.09253074 -0.59129452 -0.04348606  0.38080945], action=1, reward=1.0, next_state=[-0.10435663 -0.39558291 -0.03586987  0.0747387 ]\n",
      "[ episode 95 ][ timestamp 41 ] state=[-0.10435663 -0.39558291 -0.03586987  0.0747387 ], action=1, reward=1.0, next_state=[-0.11226829 -0.19996558 -0.0343751  -0.22904203]\n",
      "[ episode 95 ][ timestamp 42 ] state=[-0.11226829 -0.19996558 -0.0343751  -0.22904203], action=1, reward=1.0, next_state=[-0.1162676  -0.00436969 -0.03895594 -0.53236683]\n",
      "[ episode 95 ][ timestamp 43 ] state=[-0.1162676  -0.00436969 -0.03895594 -0.53236683], action=0, reward=1.0, next_state=[-0.116355   -0.1989227  -0.04960328 -0.25220887]\n",
      "[ episode 95 ][ timestamp 44 ] state=[-0.116355   -0.1989227  -0.04960328 -0.25220887], action=0, reward=1.0, next_state=[-0.12033345 -0.39330254 -0.05464746  0.02442488]\n",
      "[ episode 95 ][ timestamp 45 ] state=[-0.12033345 -0.39330254 -0.05464746  0.02442488], action=1, reward=1.0, next_state=[-0.1281995  -0.19744121 -0.05415896 -0.2849869 ]\n",
      "[ episode 95 ][ timestamp 46 ] state=[-0.1281995  -0.19744121 -0.05415896 -0.2849869 ], action=0, reward=1.0, next_state=[-0.13214833 -0.39175059 -0.0598587  -0.00986514]\n",
      "[ episode 95 ][ timestamp 47 ] state=[-0.13214833 -0.39175059 -0.0598587  -0.00986514], action=1, reward=1.0, next_state=[-0.13998334 -0.19582354 -0.060056   -0.32081731]\n",
      "[ episode 95 ][ timestamp 48 ] state=[-0.13998334 -0.19582354 -0.060056   -0.32081731], action=1, reward=1.0, next_state=[-1.43899808e-01  9.99650079e-05 -6.64723445e-02 -6.31818589e-01]\n",
      "[ episode 95 ][ timestamp 49 ] state=[-1.43899808e-01  9.99650079e-05 -6.64723445e-02 -6.31818589e-01], action=0, reward=1.0, next_state=[-0.14389781 -0.19403468 -0.07910872 -0.36078822]\n",
      "[ episode 95 ][ timestamp 50 ] state=[-0.14389781 -0.19403468 -0.07910872 -0.36078822], action=0, reward=1.0, next_state=[-0.1477785  -0.38794825 -0.08632448 -0.09406126]\n",
      "[ episode 95 ][ timestamp 51 ] state=[-0.1477785  -0.38794825 -0.08632448 -0.09406126], action=1, reward=1.0, next_state=[-0.15553747 -0.1917019  -0.08820571 -0.41268254]\n",
      "[ episode 95 ][ timestamp 52 ] state=[-0.15553747 -0.1917019  -0.08820571 -0.41268254], action=0, reward=1.0, next_state=[-0.1593715  -0.38547002 -0.09645936 -0.14905917]\n",
      "[ episode 95 ][ timestamp 53 ] state=[-0.1593715  -0.38547002 -0.09645936 -0.14905917], action=1, reward=1.0, next_state=[-0.16708091 -0.18910862 -0.09944054 -0.47054715]\n",
      "[ episode 95 ][ timestamp 54 ] state=[-0.16708091 -0.18910862 -0.09944054 -0.47054715], action=0, reward=1.0, next_state=[-0.17086308 -0.38269567 -0.10885148 -0.21078846]\n",
      "[ episode 95 ][ timestamp 55 ] state=[-0.17086308 -0.38269567 -0.10885148 -0.21078846], action=0, reward=1.0, next_state=[-0.17851699 -0.57610652 -0.11306725  0.0456716 ]\n",
      "[ episode 95 ][ timestamp 56 ] state=[-0.17851699 -0.57610652 -0.11306725  0.0456716 ], action=0, reward=1.0, next_state=[-0.19003912 -0.76944086 -0.11215382  0.30065037]\n",
      "[ episode 95 ][ timestamp 57 ] state=[-0.19003912 -0.76944086 -0.11215382  0.30065037], action=0, reward=1.0, next_state=[-0.20542794 -0.96280046 -0.10614081  0.55596342]\n",
      "[ episode 95 ][ timestamp 58 ] state=[-0.20542794 -0.96280046 -0.10614081  0.55596342], action=1, reward=1.0, next_state=[-0.22468395 -0.76636098 -0.09502154  0.23181559]\n",
      "[ episode 95 ][ timestamp 59 ] state=[-0.22468395 -0.76636098 -0.09502154  0.23181559], action=1, reward=1.0, next_state=[-0.24001117 -0.57001877 -0.09038523 -0.08926345]\n",
      "[ episode 95 ][ timestamp 60 ] state=[-0.24001117 -0.57001877 -0.09038523 -0.08926345], action=1, reward=1.0, next_state=[-0.25141154 -0.37372533 -0.0921705  -0.4090388 ]\n",
      "[ episode 95 ][ timestamp 61 ] state=[-0.25141154 -0.37372533 -0.0921705  -0.4090388 ], action=0, reward=1.0, next_state=[-0.25888605 -0.56742785 -0.10035128 -0.14677811]\n",
      "[ episode 95 ][ timestamp 62 ] state=[-0.25888605 -0.56742785 -0.10035128 -0.14677811], action=1, reward=1.0, next_state=[-0.27023461 -0.3710227  -0.10328684 -0.46935746]\n",
      "[ episode 95 ][ timestamp 63 ] state=[-0.27023461 -0.3710227  -0.10328684 -0.46935746], action=0, reward=1.0, next_state=[-0.27765506 -0.56454551 -0.11267399 -0.21093263]\n",
      "[ episode 95 ][ timestamp 64 ] state=[-0.27765506 -0.56454551 -0.11267399 -0.21093263], action=1, reward=1.0, next_state=[-0.28894597 -0.36800797 -0.11689264 -0.53692568]\n",
      "[ episode 95 ][ timestamp 65 ] state=[-0.28894597 -0.36800797 -0.11689264 -0.53692568], action=0, reward=1.0, next_state=[-0.29630613 -0.56130916 -0.12763116 -0.2832408 ]\n",
      "[ episode 95 ][ timestamp 66 ] state=[-0.29630613 -0.56130916 -0.12763116 -0.2832408 ], action=1, reward=1.0, next_state=[-0.30753231 -0.36461973 -0.13329597 -0.61329696]\n",
      "[ episode 95 ][ timestamp 67 ] state=[-0.30753231 -0.36461973 -0.13329597 -0.61329696], action=0, reward=1.0, next_state=[-0.31482471 -0.55765188 -0.14556191 -0.36539031]\n",
      "[ episode 95 ][ timestamp 68 ] state=[-0.31482471 -0.55765188 -0.14556191 -0.36539031], action=0, reward=1.0, next_state=[-0.32597775 -0.75043774 -0.15286972 -0.12191396]\n",
      "[ episode 95 ][ timestamp 69 ] state=[-0.32597775 -0.75043774 -0.15286972 -0.12191396], action=1, reward=1.0, next_state=[-0.3409865  -0.55349376 -0.155308   -0.45865368]\n",
      "[ episode 95 ][ timestamp 70 ] state=[-0.3409865  -0.55349376 -0.155308   -0.45865368], action=1, reward=1.0, next_state=[-0.35205638 -0.35655665 -0.16448107 -0.79598104]\n",
      "[ episode 95 ][ timestamp 71 ] state=[-0.35205638 -0.35655665 -0.16448107 -0.79598104], action=0, reward=1.0, next_state=[-0.35918751 -0.54908553 -0.18040069 -0.55922511]\n",
      "[ episode 95 ][ timestamp 72 ] state=[-0.35918751 -0.54908553 -0.18040069 -0.55922511], action=1, reward=1.0, next_state=[-0.37016922 -0.35195125 -0.19158519 -0.90287845]\n",
      "[ episode 95 ][ timestamp 73 ] state=[-0.37016922 -0.35195125 -0.19158519 -0.90287845], action=0, reward=-1.0, next_state=[-0.37720824 -0.54403417 -0.20964276 -0.67600781]\n",
      "[ Ended! ] Episode 95: Exploration_rate=0.6242658676435396. Score=73.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 96 ] state=[-0.0026789  -0.04852906 -0.02199837 -0.03008976]\n",
      "[ episode 96 ][ timestamp 1 ] state=[-0.0026789  -0.04852906 -0.02199837 -0.03008976], action=0, reward=1.0, next_state=[-0.00364948 -0.24332875 -0.02260017  0.25557208]\n",
      "[ episode 96 ][ timestamp 2 ] state=[-0.00364948 -0.24332875 -0.02260017  0.25557208], action=1, reward=1.0, next_state=[-0.00851605 -0.04789153 -0.01748872 -0.04415277]\n",
      "[ episode 96 ][ timestamp 3 ] state=[-0.00851605 -0.04789153 -0.01748872 -0.04415277], action=0, reward=1.0, next_state=[-0.00947388 -0.24275839 -0.01837178  0.2429614 ]\n",
      "[ episode 96 ][ timestamp 4 ] state=[-0.00947388 -0.24275839 -0.01837178  0.2429614 ], action=0, reward=1.0, next_state=[-0.01432905 -0.43761317 -0.01351255  0.52979324]\n",
      "[ episode 96 ][ timestamp 5 ] state=[-0.01432905 -0.43761317 -0.01351255  0.52979324], action=0, reward=1.0, next_state=[-0.02308132 -0.63254245 -0.00291669  0.8181879 ]\n",
      "[ episode 96 ][ timestamp 6 ] state=[-0.02308132 -0.63254245 -0.00291669  0.8181879 ], action=1, reward=1.0, next_state=[-0.03573216 -0.4373807   0.01344707  0.52458901]\n",
      "[ episode 96 ][ timestamp 7 ] state=[-0.03573216 -0.4373807   0.01344707  0.52458901], action=1, reward=1.0, next_state=[-0.04447978 -0.24245054  0.02393885  0.23617353]\n",
      "[ episode 96 ][ timestamp 8 ] state=[-0.04447978 -0.24245054  0.02393885  0.23617353], action=1, reward=1.0, next_state=[-0.04932879 -0.04767864  0.02866232 -0.04886326]\n",
      "[ episode 96 ][ timestamp 9 ] state=[-0.04932879 -0.04767864  0.02866232 -0.04886326], action=0, reward=1.0, next_state=[-0.05028236 -0.24319961  0.02768506  0.2527233 ]\n",
      "[ episode 96 ][ timestamp 10 ] state=[-0.05028236 -0.24319961  0.02768506  0.2527233 ], action=1, reward=1.0, next_state=[-0.05514635 -0.04848369  0.03273952 -0.03110028]\n",
      "[ episode 96 ][ timestamp 11 ] state=[-0.05514635 -0.04848369  0.03273952 -0.03110028], action=0, reward=1.0, next_state=[-0.05611603 -0.24405948  0.03211752  0.27172989]\n",
      "[ episode 96 ][ timestamp 12 ] state=[-0.05611603 -0.24405948  0.03211752  0.27172989], action=0, reward=1.0, next_state=[-0.06099722 -0.43962466  0.03755212  0.57436729]\n",
      "[ episode 96 ][ timestamp 13 ] state=[-0.06099722 -0.43962466  0.03755212  0.57436729], action=1, reward=1.0, next_state=[-0.06978971 -0.24504874  0.04903946  0.2937469 ]\n",
      "[ episode 96 ][ timestamp 14 ] state=[-0.06978971 -0.24504874  0.04903946  0.2937469 ], action=1, reward=1.0, next_state=[-0.07469069 -0.05065901  0.0549144   0.01692467]\n",
      "[ episode 96 ][ timestamp 15 ] state=[-0.07469069 -0.05065901  0.0549144   0.01692467], action=0, reward=1.0, next_state=[-0.07570387 -0.24652374  0.05525289  0.32641562]\n",
      "[ episode 96 ][ timestamp 16 ] state=[-0.07570387 -0.24652374  0.05525289  0.32641562], action=0, reward=1.0, next_state=[-0.08063434 -0.44238704  0.06178121  0.6359983 ]\n",
      "[ episode 96 ][ timestamp 17 ] state=[-0.08063434 -0.44238704  0.06178121  0.6359983 ], action=1, reward=1.0, next_state=[-0.08948208 -0.24817867  0.07450117  0.36339366]\n",
      "[ episode 96 ][ timestamp 18 ] state=[-0.08948208 -0.24817867  0.07450117  0.36339366], action=0, reward=1.0, next_state=[-0.09444566 -0.44427605  0.08176904  0.67860687]\n",
      "[ episode 96 ][ timestamp 19 ] state=[-0.09444566 -0.44427605  0.08176904  0.67860687], action=0, reward=1.0, next_state=[-0.10333118 -0.64043302  0.09534118  0.99587254]\n",
      "[ episode 96 ][ timestamp 20 ] state=[-0.10333118 -0.64043302  0.09534118  0.99587254], action=1, reward=1.0, next_state=[-0.11613984 -0.44670648  0.11525863  0.73459031]\n",
      "[ episode 96 ][ timestamp 21 ] state=[-0.11613984 -0.44670648  0.11525863  0.73459031], action=1, reward=1.0, next_state=[-0.12507397 -0.25334948  0.12995044  0.48029024]\n",
      "[ episode 96 ][ timestamp 22 ] state=[-0.12507397 -0.25334948  0.12995044  0.48029024], action=1, reward=1.0, next_state=[-0.13014096 -0.06027836  0.13955624  0.23122342]\n",
      "[ episode 96 ][ timestamp 23 ] state=[-0.13014096 -0.06027836  0.13955624  0.23122342], action=1, reward=1.0, next_state=[-0.13134652  0.1326022   0.14418071 -0.01438811]\n",
      "[ episode 96 ][ timestamp 24 ] state=[-0.13134652  0.1326022   0.14418071 -0.01438811], action=1, reward=1.0, next_state=[-0.12869448  0.32539373  0.14389295 -0.25833237]\n",
      "[ episode 96 ][ timestamp 25 ] state=[-0.12869448  0.32539373  0.14389295 -0.25833237], action=1, reward=1.0, next_state=[-0.1221866   0.51819966  0.1387263  -0.50239367]\n",
      "[ episode 96 ][ timestamp 26 ] state=[-0.1221866   0.51819966  0.1387263  -0.50239367], action=1, reward=1.0, next_state=[-0.11182261  0.71112162  0.12867843 -0.74834165]\n",
      "[ episode 96 ][ timestamp 27 ] state=[-0.11182261  0.71112162  0.12867843 -0.74834165], action=0, reward=1.0, next_state=[-0.09760018  0.51448158  0.1137116  -0.41809308]\n",
      "[ episode 96 ][ timestamp 28 ] state=[-0.09760018  0.51448158  0.1137116  -0.41809308], action=0, reward=1.0, next_state=[-0.08731055  0.31794736  0.10534973 -0.09183641]\n",
      "[ episode 96 ][ timestamp 29 ] state=[-0.08731055  0.31794736  0.10534973 -0.09183641], action=0, reward=1.0, next_state=[-0.0809516   0.12148551  0.10351301  0.23213809]\n",
      "[ episode 96 ][ timestamp 30 ] state=[-0.0809516   0.12148551  0.10351301  0.23213809], action=0, reward=1.0, next_state=[-0.07852189 -0.07495149  0.10815577  0.55559491]\n",
      "[ episode 96 ][ timestamp 31 ] state=[-0.07852189 -0.07495149  0.10815577  0.55559491], action=0, reward=1.0, next_state=[-0.08002092 -0.27141248  0.11926767  0.8803003 ]\n",
      "[ episode 96 ][ timestamp 32 ] state=[-0.08002092 -0.27141248  0.11926767  0.8803003 ], action=0, reward=1.0, next_state=[-0.08544917 -0.46793519  0.13687367  1.20797186]\n",
      "[ episode 96 ][ timestamp 33 ] state=[-0.08544917 -0.46793519  0.13687367  1.20797186], action=1, reward=1.0, next_state=[-0.09480787 -0.27482061  0.16103311  0.9611245 ]\n",
      "[ episode 96 ][ timestamp 34 ] state=[-0.09480787 -0.27482061  0.16103311  0.9611245 ], action=1, reward=1.0, next_state=[-0.10030428 -0.08218625  0.1802556   0.72305075]\n",
      "[ episode 96 ][ timestamp 35 ] state=[-0.10030428 -0.08218625  0.1802556   0.72305075], action=1, reward=1.0, next_state=[-0.10194801  0.11004555  0.19471661  0.49208351]\n",
      "[ episode 96 ][ timestamp 36 ] state=[-0.10194801  0.11004555  0.19471661  0.49208351], action=0, reward=1.0, next_state=[-0.0997471  -0.0872128   0.20455828  0.83926514]\n",
      "[ episode 96 ][ timestamp 37 ] state=[-0.0997471  -0.0872128   0.20455828  0.83926514], action=1, reward=-1.0, next_state=[-0.10149135  0.10461704  0.22134359  0.61724121]\n",
      "[ Ended! ] Episode 96: Exploration_rate=0.6211445383053219. Score=37.\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ] state=[ 0.02008229 -0.01007599  0.02765294 -0.01163051]\n",
      "[ episode 97 ][ timestamp 1 ] state=[ 0.02008229 -0.01007599  0.02765294 -0.01163051], action=1, reward=1.0, next_state=[ 0.01988077  0.1846387   0.02742033 -0.29546194]\n",
      "[ episode 97 ][ timestamp 2 ] state=[ 0.01988077  0.1846387   0.02742033 -0.29546194], action=0, reward=1.0, next_state=[ 0.02357354 -0.01086322  0.0215111   0.00574126]\n",
      "[ episode 97 ][ timestamp 3 ] state=[ 0.02357354 -0.01086322  0.0215111   0.00574126], action=1, reward=1.0, next_state=[ 0.02335628  0.18394373  0.02162592 -0.28007778]\n",
      "[ episode 97 ][ timestamp 4 ] state=[ 0.02335628  0.18394373  0.02162592 -0.28007778], action=0, reward=1.0, next_state=[ 0.02703515 -0.01147993  0.01602436  0.01934669]\n",
      "[ episode 97 ][ timestamp 5 ] state=[ 0.02703515 -0.01147993  0.01602436  0.01934669], action=1, reward=1.0, next_state=[ 0.02680555  0.1834086   0.0164113  -0.2682376 ]\n",
      "[ episode 97 ][ timestamp 6 ] state=[ 0.02680555  0.1834086   0.0164113  -0.2682376 ], action=0, reward=1.0, next_state=[ 0.03047373 -0.01194367  0.01104655  0.02957605]\n",
      "[ episode 97 ][ timestamp 7 ] state=[ 0.03047373 -0.01194367  0.01104655  0.02957605], action=1, reward=1.0, next_state=[ 0.03023485  0.18301813  0.01163807 -0.25960121]\n",
      "[ episode 97 ][ timestamp 8 ] state=[ 0.03023485  0.18301813  0.01163807 -0.25960121], action=0, reward=1.0, next_state=[ 0.03389522 -0.01226801  0.00644604  0.03672969]\n",
      "[ episode 97 ][ timestamp 9 ] state=[ 0.03389522 -0.01226801  0.00644604  0.03672969], action=1, reward=1.0, next_state=[ 0.03364986  0.18276092  0.00718064 -0.2539125 ]\n",
      "[ episode 97 ][ timestamp 10 ] state=[ 0.03364986  0.18276092  0.00718064 -0.2539125 ], action=0, reward=1.0, next_state=[ 0.03730507 -0.01246282  0.00210239  0.04102665]\n",
      "[ episode 97 ][ timestamp 11 ] state=[ 0.03730507 -0.01246282  0.00210239  0.04102665], action=0, reward=1.0, next_state=[ 0.03705582 -0.20761486  0.00292292  0.33437216]\n",
      "[ episode 97 ][ timestamp 12 ] state=[ 0.03705582 -0.20761486  0.00292292  0.33437216], action=1, reward=1.0, next_state=[ 0.03290352 -0.01253463  0.00961036  0.0426124 ]\n",
      "[ episode 97 ][ timestamp 13 ] state=[ 0.03290352 -0.01253463  0.00961036  0.0426124 ], action=1, reward=1.0, next_state=[ 0.03265283  0.1824482   0.01046261 -0.24702294]\n",
      "[ episode 97 ][ timestamp 14 ] state=[ 0.03265283  0.1824482   0.01046261 -0.24702294], action=0, reward=1.0, next_state=[ 0.03630179 -0.0128216   0.00552215  0.04894169]\n",
      "[ episode 97 ][ timestamp 15 ] state=[ 0.03630179 -0.0128216   0.00552215  0.04894169], action=1, reward=1.0, next_state=[ 0.03604536  0.18222073  0.00650099 -0.24199385]\n",
      "[ episode 97 ][ timestamp 16 ] state=[ 0.03604536  0.18222073  0.00650099 -0.24199385], action=0, reward=1.0, next_state=[ 0.03968977 -0.01299347  0.00166111  0.05273255]\n",
      "[ episode 97 ][ timestamp 17 ] state=[ 0.03968977 -0.01299347  0.00166111  0.05273255], action=1, reward=1.0, next_state=[ 0.03942991  0.18210462  0.00271576 -0.23942582]\n",
      "[ episode 97 ][ timestamp 18 ] state=[ 0.03942991  0.18210462  0.00271576 -0.23942582], action=0, reward=1.0, next_state=[ 0.043072   -0.01305602 -0.00207276  0.0541125 ]\n",
      "[ episode 97 ][ timestamp 19 ] state=[ 0.043072   -0.01305602 -0.00207276  0.0541125 ], action=1, reward=1.0, next_state=[ 0.04281088  0.18209559 -0.00099051 -0.23922368]\n",
      "[ episode 97 ][ timestamp 20 ] state=[ 0.04281088  0.18209559 -0.00099051 -0.23922368], action=1, reward=1.0, next_state=[ 0.04645279  0.37723168 -0.00577498 -0.53221888]\n",
      "[ episode 97 ][ timestamp 21 ] state=[ 0.04645279  0.37723168 -0.00577498 -0.53221888], action=1, reward=1.0, next_state=[ 0.05399742  0.57243437 -0.01641936 -0.82671588]\n",
      "[ episode 97 ][ timestamp 22 ] state=[ 0.05399742  0.57243437 -0.01641936 -0.82671588], action=0, reward=1.0, next_state=[ 0.06544611  0.37754076 -0.03295367 -0.53924193]\n",
      "[ episode 97 ][ timestamp 23 ] state=[ 0.06544611  0.37754076 -0.03295367 -0.53924193], action=0, reward=1.0, next_state=[ 0.07299692  0.18289718 -0.04373851 -0.2571217 ]\n",
      "[ episode 97 ][ timestamp 24 ] state=[ 0.07299692  0.18289718 -0.04373851 -0.2571217 ], action=0, reward=1.0, next_state=[ 0.07665487 -0.01157391 -0.04888095  0.02145093]\n",
      "[ episode 97 ][ timestamp 25 ] state=[ 0.07665487 -0.01157391 -0.04888095  0.02145093], action=1, reward=1.0, next_state=[ 0.07642339  0.18421372 -0.04845193 -0.28624502]\n",
      "[ episode 97 ][ timestamp 26 ] state=[ 0.07642339  0.18421372 -0.04845193 -0.28624502], action=0, reward=1.0, next_state=[ 0.08010766 -0.01018495 -0.05417683 -0.00922851]\n",
      "[ episode 97 ][ timestamp 27 ] state=[ 0.08010766 -0.01018495 -0.05417683 -0.00922851], action=0, reward=1.0, next_state=[ 0.07990397 -0.20448976 -0.0543614   0.26588088]\n",
      "[ episode 97 ][ timestamp 28 ] state=[ 0.07990397 -0.20448976 -0.0543614   0.26588088], action=0, reward=1.0, next_state=[ 0.07581417 -0.3987954  -0.04904378  0.54093441]\n",
      "[ episode 97 ][ timestamp 29 ] state=[ 0.07581417 -0.3987954  -0.04904378  0.54093441], action=1, reward=1.0, next_state=[ 0.06783826 -0.20301964 -0.03822509  0.23321078]\n",
      "[ episode 97 ][ timestamp 30 ] state=[ 0.06783826 -0.20301964 -0.03822509  0.23321078], action=0, reward=1.0, next_state=[ 0.06377787 -0.39757516 -0.03356088  0.51359543]\n",
      "[ episode 97 ][ timestamp 31 ] state=[ 0.06377787 -0.39757516 -0.03356088  0.51359543], action=1, reward=1.0, next_state=[ 0.05582637 -0.201997   -0.02328897  0.21052836]\n",
      "[ episode 97 ][ timestamp 32 ] state=[ 0.05582637 -0.201997   -0.02328897  0.21052836], action=1, reward=1.0, next_state=[ 0.05178643 -0.00654994 -0.0190784  -0.08940908]\n",
      "[ episode 97 ][ timestamp 33 ] state=[ 0.05178643 -0.00654994 -0.0190784  -0.08940908], action=1, reward=1.0, next_state=[ 0.05165543  0.18884021 -0.02086658 -0.38804967]\n",
      "[ episode 97 ][ timestamp 34 ] state=[ 0.05165543  0.18884021 -0.02086658 -0.38804967], action=0, reward=1.0, next_state=[ 0.05543223 -0.00597943 -0.02862758 -0.10201816]\n",
      "[ episode 97 ][ timestamp 35 ] state=[ 0.05543223 -0.00597943 -0.02862758 -0.10201816], action=0, reward=1.0, next_state=[ 0.05531264 -0.20067966 -0.03066794  0.18149717]\n",
      "[ episode 97 ][ timestamp 36 ] state=[ 0.05531264 -0.20067966 -0.03066794  0.18149717], action=1, reward=1.0, next_state=[ 0.05129905 -0.0051326  -0.027038   -0.12070046]\n",
      "[ episode 97 ][ timestamp 37 ] state=[ 0.05129905 -0.0051326  -0.027038   -0.12070046], action=1, reward=1.0, next_state=[ 0.0511964   0.19036608 -0.02945201 -0.42178951]\n",
      "[ episode 97 ][ timestamp 38 ] state=[ 0.0511964   0.19036608 -0.02945201 -0.42178951], action=1, reward=1.0, next_state=[ 0.05500372  0.38589266 -0.0378878  -0.72360981]\n",
      "[ episode 97 ][ timestamp 39 ] state=[ 0.05500372  0.38589266 -0.0378878  -0.72360981], action=1, reward=1.0, next_state=[ 0.06272157  0.58151757 -0.05235999 -1.02797294]\n",
      "[ episode 97 ][ timestamp 40 ] state=[ 0.06272157  0.58151757 -0.05235999 -1.02797294], action=0, reward=1.0, next_state=[ 0.07435192  0.38713017 -0.07291945 -0.75217825]\n",
      "[ episode 97 ][ timestamp 41 ] state=[ 0.07435192  0.38713017 -0.07291945 -0.75217825], action=0, reward=1.0, next_state=[ 0.08209453  0.19308543 -0.08796302 -0.48330396]\n",
      "[ episode 97 ][ timestamp 42 ] state=[ 0.08209453  0.19308543 -0.08796302 -0.48330396], action=0, reward=1.0, next_state=[ 0.08595624 -0.00069209 -0.0976291  -0.21958926]\n",
      "[ episode 97 ][ timestamp 43 ] state=[ 0.08595624 -0.00069209 -0.0976291  -0.21958926], action=1, reward=1.0, next_state=[ 0.08594239  0.19568002 -0.10202088 -0.54140213]\n",
      "[ episode 97 ][ timestamp 44 ] state=[ 0.08594239  0.19568002 -0.10202088 -0.54140213], action=0, reward=1.0, next_state=[ 0.08985599  0.00212884 -0.11284892 -0.28252708]\n",
      "[ episode 97 ][ timestamp 45 ] state=[ 0.08985599  0.00212884 -0.11284892 -0.28252708], action=1, reward=1.0, next_state=[ 0.08989857  0.19866437 -0.11849946 -0.60856244]\n",
      "[ episode 97 ][ timestamp 46 ] state=[ 0.08989857  0.19866437 -0.11849946 -0.60856244], action=0, reward=1.0, next_state=[ 0.09387186  0.00538096 -0.13067071 -0.35542789]\n",
      "[ episode 97 ][ timestamp 47 ] state=[ 0.09387186  0.00538096 -0.13067071 -0.35542789], action=1, reward=1.0, next_state=[ 0.09397948  0.20209523 -0.13777927 -0.68629169]\n",
      "[ episode 97 ][ timestamp 48 ] state=[ 0.09397948  0.20209523 -0.13777927 -0.68629169], action=0, reward=1.0, next_state=[ 0.09802138  0.00912755 -0.15150511 -0.43996225]\n",
      "[ episode 97 ][ timestamp 49 ] state=[ 0.09802138  0.00912755 -0.15150511 -0.43996225], action=1, reward=1.0, next_state=[ 0.09820393  0.20603252 -0.16030435 -0.77630868]\n",
      "[ episode 97 ][ timestamp 50 ] state=[ 0.09820393  0.20603252 -0.16030435 -0.77630868], action=1, reward=1.0, next_state=[ 0.10232458  0.40295333 -0.17583052 -1.11483062]\n",
      "[ episode 97 ][ timestamp 51 ] state=[ 0.10232458  0.40295333 -0.17583052 -1.11483062], action=0, reward=1.0, next_state=[ 0.11038365  0.21052014 -0.19812714 -0.88205958]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 97 ][ timestamp 52 ] state=[ 0.11038365  0.21052014 -0.19812714 -0.88205958], action=0, reward=-1.0, next_state=[ 0.11459405  0.01856062 -0.21576833 -0.6576223 ]\n",
      "[ Ended! ] Episode 97: Exploration_rate=0.6180388156137953. Score=52.\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ] state=[0.04041957 0.03237287 0.03930133 0.00073584]\n",
      "[ episode 98 ][ timestamp 1 ] state=[0.04041957 0.03237287 0.03930133 0.00073584], action=0, reward=1.0, next_state=[ 0.04106703 -0.16329003  0.03931605  0.30555518]\n",
      "[ episode 98 ][ timestamp 2 ] state=[ 0.04106703 -0.16329003  0.03931605  0.30555518], action=0, reward=1.0, next_state=[ 0.03780123 -0.35894955  0.04542715  0.6103736 ]\n",
      "[ episode 98 ][ timestamp 3 ] state=[ 0.03780123 -0.35894955  0.04542715  0.6103736 ], action=0, reward=1.0, next_state=[ 0.03062224 -0.55467606  0.05763462  0.91701147]\n",
      "[ episode 98 ][ timestamp 4 ] state=[ 0.03062224 -0.55467606  0.05763462  0.91701147], action=0, reward=1.0, next_state=[ 0.01952871 -0.75052794  0.07597485  1.2272367 ]\n",
      "[ episode 98 ][ timestamp 5 ] state=[ 0.01952871 -0.75052794  0.07597485  1.2272367 ], action=0, reward=1.0, next_state=[ 0.00451816 -0.94654114  0.10051959  1.54272346]\n",
      "[ episode 98 ][ timestamp 6 ] state=[ 0.00451816 -0.94654114  0.10051959  1.54272346], action=1, reward=1.0, next_state=[-0.01441267 -0.75276087  0.13137405  1.28302333]\n",
      "[ episode 98 ][ timestamp 7 ] state=[-0.01441267 -0.75276087  0.13137405  1.28302333], action=0, reward=1.0, next_state=[-0.02946788 -0.94928817  0.15703452  1.61378699]\n",
      "[ episode 98 ][ timestamp 8 ] state=[-0.02946788 -0.94928817  0.15703452  1.61378699], action=1, reward=1.0, next_state=[-0.04845365 -0.75632945  0.18931026  1.37388895]\n",
      "[ episode 98 ][ timestamp 9 ] state=[-0.04845365 -0.75632945  0.18931026  1.37388895], action=0, reward=-1.0, next_state=[-0.06358024 -0.9532452   0.21678804  1.71931089]\n",
      "[ Ended! ] Episode 98: Exploration_rate=0.6149486215357263. Score=9.\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ] state=[-0.00680511 -0.01631518  0.0374257   0.02566912]\n",
      "[ episode 99 ][ timestamp 1 ] state=[-0.00680511 -0.01631518  0.0374257   0.02566912], action=1, reward=1.0, next_state=[-0.00713141  0.17825063  0.03793908 -0.25497463]\n",
      "[ episode 99 ][ timestamp 2 ] state=[-0.00713141  0.17825063  0.03793908 -0.25497463], action=1, reward=1.0, next_state=[-0.0035664   0.37281093  0.03283959 -0.53545365]\n",
      "[ episode 99 ][ timestamp 3 ] state=[-0.0035664   0.37281093  0.03283959 -0.53545365], action=1, reward=1.0, next_state=[ 0.00388982  0.56745608  0.02213052 -0.81761085]\n",
      "[ episode 99 ][ timestamp 4 ] state=[ 0.00388982  0.56745608  0.02213052 -0.81761085], action=1, reward=1.0, next_state=[ 0.01523894  0.7622682   0.0057783  -1.10325164]\n",
      "[ episode 99 ][ timestamp 5 ] state=[ 0.01523894  0.7622682   0.0057783  -1.10325164], action=1, reward=1.0, next_state=[ 0.0304843   0.95731367 -0.01628673 -1.39411615]\n",
      "[ episode 99 ][ timestamp 6 ] state=[ 0.0304843   0.95731367 -0.01628673 -1.39411615], action=1, reward=1.0, next_state=[ 0.04963058  1.15263448 -0.04416905 -1.6918466 ]\n",
      "[ episode 99 ][ timestamp 7 ] state=[ 0.04963058  1.15263448 -0.04416905 -1.6918466 ], action=1, reward=1.0, next_state=[ 0.07268327  1.34823785 -0.07800599 -1.99794698]\n",
      "[ episode 99 ][ timestamp 8 ] state=[ 0.07268327  1.34823785 -0.07800599 -1.99794698], action=1, reward=1.0, next_state=[ 0.09964802  1.54408349 -0.11796493 -2.31373261]\n",
      "[ episode 99 ][ timestamp 9 ] state=[ 0.09964802  1.54408349 -0.11796493 -2.31373261], action=1, reward=1.0, next_state=[ 0.13052969  1.74006827 -0.16423958 -2.64026801]\n",
      "[ episode 99 ][ timestamp 10 ] state=[ 0.13052969  1.74006827 -0.16423958 -2.64026801], action=0, reward=-1.0, next_state=[ 0.16533106  1.54652623 -0.21704494 -2.40193134]\n",
      "[ Ended! ] Episode 99: Exploration_rate=0.6118738784280476. Score=10.\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ] state=[ 0.02512704 -0.04404248 -0.03578977 -0.00153519]\n",
      "[ episode 100 ][ timestamp 1 ] state=[ 0.02512704 -0.04404248 -0.03578977 -0.00153519], action=1, reward=1.0, next_state=[ 0.02424619  0.15157399 -0.03582048 -0.30529193]\n",
      "[ episode 100 ][ timestamp 2 ] state=[ 0.02424619  0.15157399 -0.03582048 -0.30529193], action=0, reward=1.0, next_state=[ 0.02727767 -0.04301968 -0.04192632 -0.02411764]\n",
      "[ episode 100 ][ timestamp 3 ] state=[ 0.02727767 -0.04301968 -0.04192632 -0.02411764], action=1, reward=1.0, next_state=[ 0.02641727  0.15267767 -0.04240867 -0.32972843]\n",
      "[ episode 100 ][ timestamp 4 ] state=[ 0.02641727  0.15267767 -0.04240867 -0.32972843], action=1, reward=1.0, next_state=[ 0.02947083  0.34837686 -0.04900324 -0.6354777 ]\n",
      "[ episode 100 ][ timestamp 5 ] state=[ 0.02947083  0.34837686 -0.04900324 -0.6354777 ], action=0, reward=1.0, next_state=[ 0.03643836  0.1539714  -0.06171279 -0.35862074]\n",
      "[ episode 100 ][ timestamp 6 ] state=[ 0.03643836  0.1539714  -0.06171279 -0.35862074], action=1, reward=1.0, next_state=[ 0.03951779  0.34991388 -0.06888521 -0.67010701]\n",
      "[ episode 100 ][ timestamp 7 ] state=[ 0.03951779  0.34991388 -0.06888521 -0.67010701], action=1, reward=1.0, next_state=[ 0.04651607  0.54592252 -0.08228735 -0.98365891]\n",
      "[ episode 100 ][ timestamp 8 ] state=[ 0.04651607  0.54592252 -0.08228735 -0.98365891], action=0, reward=1.0, next_state=[ 0.05743452  0.35199365 -0.10196052 -0.71791508]\n",
      "[ episode 100 ][ timestamp 9 ] state=[ 0.05743452  0.35199365 -0.10196052 -0.71791508], action=1, reward=1.0, next_state=[ 0.06447439  0.54836771 -0.11631883 -1.04087086]\n",
      "[ episode 100 ][ timestamp 10 ] state=[ 0.06447439  0.54836771 -0.11631883 -1.04087086], action=0, reward=1.0, next_state=[ 0.07544175  0.35496687 -0.13713624 -0.7868506 ]\n",
      "[ episode 100 ][ timestamp 11 ] state=[ 0.07544175  0.35496687 -0.13713624 -0.7868506 ], action=1, reward=1.0, next_state=[ 0.08254109  0.55167942 -0.15287326 -1.11934099]\n",
      "[ episode 100 ][ timestamp 12 ] state=[ 0.08254109  0.55167942 -0.15287326 -1.11934099], action=1, reward=1.0, next_state=[ 0.09357467  0.7484398  -0.17526007 -1.45580941]\n",
      "[ episode 100 ][ timestamp 13 ] state=[ 0.09357467  0.7484398  -0.17526007 -1.45580941], action=0, reward=1.0, next_state=[ 0.10854347  0.55584809 -0.20437626 -1.22261033]\n",
      "[ episode 100 ][ timestamp 14 ] state=[ 0.10854347  0.55584809 -0.20437626 -1.22261033], action=0, reward=-1.0, next_state=[ 0.11966043  0.36385894 -0.22882847 -1.00028937]\n",
      "[ Ended! ] Episode 100: Exploration_rate=0.6088145090359074. Score=14.\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ] state=[-0.04983246  0.00198069 -0.01596287  0.02663866]\n",
      "[ episode 101 ][ timestamp 1 ] state=[-0.04983246  0.00198069 -0.01596287  0.02663866], action=1, reward=1.0, next_state=[-0.04979284  0.19732788 -0.01543009 -0.27103768]\n",
      "[ episode 101 ][ timestamp 2 ] state=[-0.04979284  0.19732788 -0.01543009 -0.27103768], action=1, reward=1.0, next_state=[-0.04584629  0.39266658 -0.02085085 -0.56854711]\n",
      "[ episode 101 ][ timestamp 3 ] state=[-0.04584629  0.39266658 -0.02085085 -0.56854711], action=1, reward=1.0, next_state=[-0.03799295  0.58807469 -0.03222179 -0.86772526]\n",
      "[ episode 101 ][ timestamp 4 ] state=[-0.03799295  0.58807469 -0.03222179 -0.86772526], action=1, reward=1.0, next_state=[-0.02623146  0.78361991 -0.0495763  -1.17036241]\n",
      "[ episode 101 ][ timestamp 5 ] state=[-0.02623146  0.78361991 -0.0495763  -1.17036241], action=1, reward=1.0, next_state=[-0.01055906  0.9793503  -0.07298354 -1.47816673]\n",
      "[ episode 101 ][ timestamp 6 ] state=[-0.01055906  0.9793503  -0.07298354 -1.47816673], action=1, reward=1.0, next_state=[ 0.00902794  1.17528356 -0.10254688 -1.79272234]\n",
      "[ episode 101 ][ timestamp 7 ] state=[ 0.00902794  1.17528356 -0.10254688 -1.79272234], action=1, reward=1.0, next_state=[ 0.03253361  1.37139455 -0.13840133 -2.11543945]\n",
      "[ episode 101 ][ timestamp 8 ] state=[ 0.03253361  1.37139455 -0.13840133 -2.11543945], action=1, reward=1.0, next_state=[ 0.05996151  1.56760056 -0.18071011 -2.44749444]\n",
      "[ episode 101 ][ timestamp 9 ] state=[ 0.05996151  1.56760056 -0.18071011 -2.44749444], action=0, reward=-1.0, next_state=[ 0.09131352  1.37442045 -0.22966    -2.21528287]\n",
      "[ Ended! ] Episode 101: Exploration_rate=0.6057704364907278. Score=9.\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ] state=[-0.03926217  0.0174265   0.01290433 -0.0249585 ]\n",
      "[ episode 102 ][ timestamp 1 ] state=[-0.03926217  0.0174265   0.01290433 -0.0249585 ], action=1, reward=1.0, next_state=[-0.03891364  0.21236104  0.01240516 -0.3135422 ]\n",
      "[ episode 102 ][ timestamp 2 ] state=[-0.03891364  0.21236104  0.01240516 -0.3135422 ], action=0, reward=1.0, next_state=[-0.03466642  0.01706459  0.00613431 -0.01697304]\n",
      "[ episode 102 ][ timestamp 3 ] state=[-0.03466642  0.01706459  0.00613431 -0.01697304], action=1, reward=1.0, next_state=[-0.03432513  0.21209803  0.00579485 -0.30771422]\n",
      "[ episode 102 ][ timestamp 4 ] state=[-0.03432513  0.21209803  0.00579485 -0.30771422], action=0, reward=1.0, next_state=[-0.03008317  0.01689399 -0.00035943 -0.0132094 ]\n",
      "[ episode 102 ][ timestamp 5 ] state=[-0.03008317  0.01689399 -0.00035943 -0.0132094 ], action=1, reward=1.0, next_state=[-0.02974529  0.21202109 -0.00062362 -0.30600571]\n",
      "[ episode 102 ][ timestamp 6 ] state=[-0.02974529  0.21202109 -0.00062362 -0.30600571], action=1, reward=1.0, next_state=[-0.02550487  0.40715193 -0.00674373 -0.59888524]\n",
      "[ episode 102 ][ timestamp 7 ] state=[-0.02550487  0.40715193 -0.00674373 -0.59888524], action=1, reward=1.0, next_state=[-0.01736183  0.60236758 -0.01872144 -0.89368471]\n",
      "[ episode 102 ][ timestamp 8 ] state=[-0.01736183  0.60236758 -0.01872144 -0.89368471], action=0, reward=1.0, next_state=[-0.00531448  0.40750447 -0.03659513 -0.60694504]\n",
      "[ episode 102 ][ timestamp 9 ] state=[-0.00531448  0.40750447 -0.03659513 -0.60694504], action=1, reward=1.0, next_state=[ 0.00283561  0.60311847 -0.04873403 -0.91092616]\n",
      "[ episode 102 ][ timestamp 10 ] state=[ 0.00283561  0.60311847 -0.04873403 -0.91092616], action=1, reward=1.0, next_state=[ 0.01489798  0.79886481 -0.06695256 -1.21851919]\n",
      "[ episode 102 ][ timestamp 11 ] state=[ 0.01489798  0.79886481 -0.06695256 -1.21851919], action=1, reward=1.0, next_state=[ 0.03087528  0.99478301 -0.09132294 -1.53140742]\n",
      "[ episode 102 ][ timestamp 12 ] state=[ 0.03087528  0.99478301 -0.09132294 -1.53140742], action=1, reward=1.0, next_state=[ 0.05077094  1.19087941 -0.12195109 -1.85113794]\n",
      "[ episode 102 ][ timestamp 13 ] state=[ 0.05077094  1.19087941 -0.12195109 -1.85113794], action=1, reward=1.0, next_state=[ 0.07458853  1.38711385 -0.15897385 -2.17906833]\n",
      "[ episode 102 ][ timestamp 14 ] state=[ 0.07458853  1.38711385 -0.15897385 -2.17906833], action=0, reward=1.0, next_state=[ 0.1023308   1.19385462 -0.20255521 -1.9393766 ]\n",
      "[ episode 102 ][ timestamp 15 ] state=[ 0.1023308   1.19385462 -0.20255521 -1.9393766 ], action=0, reward=-1.0, next_state=[ 0.12620789  1.00139055 -0.24134275 -1.71572752]\n",
      "[ Ended! ] Episode 102: Exploration_rate=0.6027415843082742. Score=15.\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ] state=[0.00402546 0.03698163 0.00632199 0.03507671]\n",
      "[ episode 103 ][ timestamp 1 ] state=[0.00402546 0.03698163 0.00632199 0.03507671], action=0, reward=1.0, next_state=[ 0.0047651  -0.15823041  0.00702352  0.32974757]\n",
      "[ episode 103 ][ timestamp 2 ] state=[ 0.0047651  -0.15823041  0.00702352  0.32974757], action=1, reward=1.0, next_state=[0.00160049 0.03679086 0.01361848 0.03928778]\n",
      "[ episode 103 ][ timestamp 3 ] state=[0.00160049 0.03679086 0.01361848 0.03928778], action=1, reward=1.0, next_state=[ 0.00233631  0.2317149   0.01440423 -0.24906746]\n",
      "[ episode 103 ][ timestamp 4 ] state=[ 0.00233631  0.2317149   0.01440423 -0.24906746], action=1, reward=1.0, next_state=[ 0.0069706   0.42662822  0.00942288 -0.53717241]\n",
      "[ episode 103 ][ timestamp 5 ] state=[ 0.0069706   0.42662822  0.00942288 -0.53717241], action=1, reward=1.0, next_state=[ 0.01550317  0.62161643 -0.00132057 -0.82687145]\n",
      "[ episode 103 ][ timestamp 6 ] state=[ 0.01550317  0.62161643 -0.00132057 -0.82687145], action=1, reward=1.0, next_state=[ 0.0279355   0.81675641 -0.017858   -1.11996942]\n",
      "[ episode 103 ][ timestamp 7 ] state=[ 0.0279355   0.81675641 -0.017858   -1.11996942], action=1, reward=1.0, next_state=[ 0.04427063  1.01210801 -0.04025738 -1.41820006]\n",
      "[ episode 103 ][ timestamp 8 ] state=[ 0.04427063  1.01210801 -0.04025738 -1.41820006], action=1, reward=1.0, next_state=[ 0.06451279  1.20770453 -0.06862139 -1.7231896 ]\n",
      "[ episode 103 ][ timestamp 9 ] state=[ 0.06451279  1.20770453 -0.06862139 -1.7231896 ], action=1, reward=1.0, next_state=[ 0.08866688  1.40354152 -0.10308518 -2.03641258]\n",
      "[ episode 103 ][ timestamp 10 ] state=[ 0.08866688  1.40354152 -0.10308518 -2.03641258], action=0, reward=1.0, next_state=[ 0.11673771  1.20962143 -0.14381343 -1.77733   ]\n",
      "[ episode 103 ][ timestamp 11 ] state=[ 0.11673771  1.20962143 -0.14381343 -1.77733   ], action=0, reward=1.0, next_state=[ 0.14093014  1.01638239 -0.17936003 -1.53259931]\n",
      "[ episode 103 ][ timestamp 12 ] state=[ 0.14093014  1.01638239 -0.17936003 -1.53259931], action=0, reward=-1.0, next_state=[ 0.16125778  0.82381754 -0.21001202 -1.30083527]\n",
      "[ Ended! ] Episode 103: Exploration_rate=0.5997278763867329. Score=12.\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ] state=[ 0.04166393  0.01334501 -0.01966819  0.0349914 ]\n",
      "[ episode 104 ][ timestamp 1 ] state=[ 0.04166393  0.01334501 -0.01966819  0.0349914 ], action=1, reward=1.0, next_state=[ 0.04193083  0.2087434  -0.01896836 -0.26383158]\n",
      "[ episode 104 ][ timestamp 2 ] state=[ 0.04193083  0.2087434  -0.01896836 -0.26383158], action=0, reward=1.0, next_state=[ 0.0461057   0.01389726 -0.02424499  0.02280869]\n",
      "[ episode 104 ][ timestamp 3 ] state=[ 0.0461057   0.01389726 -0.02424499  0.02280869], action=0, reward=1.0, next_state=[ 0.04638365 -0.18086875 -0.02378882  0.30774452]\n",
      "[ episode 104 ][ timestamp 4 ] state=[ 0.04638365 -0.18086875 -0.02378882  0.30774452], action=1, reward=1.0, next_state=[ 0.04276627  0.01458394 -0.01763392  0.00765518]\n",
      "[ episode 104 ][ timestamp 5 ] state=[ 0.04276627  0.01458394 -0.01763392  0.00765518], action=0, reward=1.0, next_state=[ 0.04305795 -0.18028073 -0.01748082  0.29472264]\n",
      "[ episode 104 ][ timestamp 6 ] state=[ 0.04305795 -0.18028073 -0.01748082  0.29472264], action=0, reward=1.0, next_state=[ 0.03945233 -0.37514916 -0.01158637  0.58184152]\n",
      "[ episode 104 ][ timestamp 7 ] state=[ 0.03945233 -0.37514916 -0.01158637  0.58184152], action=0, reward=1.0, next_state=[ 3.19493518e-02 -5.70106869e-01  5.04620033e-05  8.70852142e-01]\n",
      "[ episode 104 ][ timestamp 8 ] state=[ 3.19493518e-02 -5.70106869e-01  5.04620033e-05  8.70852142e-01], action=1, reward=1.0, next_state=[ 0.02054721 -0.3749856   0.0174675   0.57818508]\n",
      "[ episode 104 ][ timestamp 9 ] state=[ 0.02054721 -0.3749856   0.0174675   0.57818508], action=1, reward=1.0, next_state=[ 0.0130475  -0.18011277  0.02903121  0.2910556 ]\n",
      "[ episode 104 ][ timestamp 10 ] state=[ 0.0130475  -0.18011277  0.02903121  0.2910556 ], action=1, reward=1.0, next_state=[0.00944525 0.01458347 0.03485232 0.00766829]\n",
      "[ episode 104 ][ timestamp 11 ] state=[0.00944525 0.01458347 0.03485232 0.00766829], action=0, reward=1.0, next_state=[ 0.00973692 -0.18102053  0.03500568  0.31114062]\n",
      "[ episode 104 ][ timestamp 12 ] state=[ 0.00973692 -0.18102053  0.03500568  0.31114062], action=1, reward=1.0, next_state=[0.00611651 0.01358566 0.0412285  0.02969973]\n",
      "[ episode 104 ][ timestamp 13 ] state=[0.00611651 0.01358566 0.0412285  0.02969973], action=0, reward=1.0, next_state=[ 0.00638822 -0.18210254  0.04182249  0.33510034]\n",
      "[ episode 104 ][ timestamp 14 ] state=[ 0.00638822 -0.18210254  0.04182249  0.33510034], action=1, reward=1.0, next_state=[0.00274617 0.01240002 0.0485245  0.05589385]\n",
      "[ episode 104 ][ timestamp 15 ] state=[0.00274617 0.01240002 0.0485245  0.05589385], action=1, reward=1.0, next_state=[ 0.00299417  0.20679383  0.04964238 -0.22109304]\n",
      "[ episode 104 ][ timestamp 16 ] state=[ 0.00299417  0.20679383  0.04964238 -0.22109304], action=0, reward=1.0, next_state=[0.00713004 0.01099875 0.04522051 0.08682664]\n",
      "[ episode 104 ][ timestamp 17 ] state=[0.00713004 0.01099875 0.04522051 0.08682664], action=0, reward=1.0, next_state=[ 0.00735002 -0.18474125  0.04695705  0.39342679]\n",
      "[ episode 104 ][ timestamp 18 ] state=[ 0.00735002 -0.18474125  0.04695705  0.39342679], action=1, reward=1.0, next_state=[0.00365519 0.009684   0.05482558 0.11591068]\n",
      "[ episode 104 ][ timestamp 19 ] state=[0.00365519 0.009684   0.05482558 0.11591068], action=0, reward=1.0, next_state=[ 0.00384887 -0.1861789   0.0571438   0.42537424]\n",
      "[ episode 104 ][ timestamp 20 ] state=[ 0.00384887 -0.1861789   0.0571438   0.42537424], action=1, reward=1.0, next_state=[1.25296881e-04 8.08902316e-03 6.56512814e-02 1.51239132e-01]\n",
      "[ episode 104 ][ timestamp 21 ] state=[1.25296881e-04 8.08902316e-03 6.56512814e-02 1.51239132e-01], action=1, reward=1.0, next_state=[ 0.00028708  0.20221247  0.06867606 -0.12003114]\n",
      "[ episode 104 ][ timestamp 22 ] state=[ 0.00028708  0.20221247  0.06867606 -0.12003114], action=1, reward=1.0, next_state=[ 0.00433133  0.3962867   0.06627544 -0.39028135]\n",
      "[ episode 104 ][ timestamp 23 ] state=[ 0.00433133  0.3962867   0.06627544 -0.39028135], action=1, reward=1.0, next_state=[ 0.01225706  0.59040848  0.05846981 -0.66135404]\n",
      "[ episode 104 ][ timestamp 24 ] state=[ 0.01225706  0.59040848  0.05846981 -0.66135404], action=1, reward=1.0, next_state=[ 0.02406523  0.7846702   0.04524273 -0.93506833]\n",
      "[ episode 104 ][ timestamp 25 ] state=[ 0.02406523  0.7846702   0.04524273 -0.93506833], action=1, reward=1.0, next_state=[ 0.03975863  0.97915366  0.02654137 -1.21319818]\n",
      "[ episode 104 ][ timestamp 26 ] state=[ 0.03975863  0.97915366  0.02654137 -1.21319818], action=1, reward=1.0, next_state=[ 0.05934171  1.17392322  0.0022774  -1.49744738]\n",
      "[ episode 104 ][ timestamp 27 ] state=[ 0.05934171  1.17392322  0.0022774  -1.49744738], action=1, reward=1.0, next_state=[ 0.08282017  1.36901742 -0.02767154 -1.78941836]\n",
      "[ episode 104 ][ timestamp 28 ] state=[ 0.08282017  1.36901742 -0.02767154 -1.78941836], action=1, reward=1.0, next_state=[ 0.11020052  1.56443864 -0.06345991 -2.09057236]\n",
      "[ episode 104 ][ timestamp 29 ] state=[ 0.11020052  1.56443864 -0.06345991 -2.09057236], action=0, reward=1.0, next_state=[ 0.14148929  1.37001116 -0.10527136 -1.81816289]\n",
      "[ episode 104 ][ timestamp 30 ] state=[ 0.14148929  1.37001116 -0.10527136 -1.81816289], action=0, reward=1.0, next_state=[ 0.16888952  1.17620552 -0.14163462 -1.55995641]\n",
      "[ episode 104 ][ timestamp 31 ] state=[ 0.16888952  1.17620552 -0.14163462 -1.55995641], action=0, reward=1.0, next_state=[ 0.19241363  0.98303428 -0.17283374 -1.3146025 ]\n",
      "[ episode 104 ][ timestamp 32 ] state=[ 0.19241363  0.98303428 -0.17283374 -1.3146025 ], action=0, reward=1.0, next_state=[ 0.21207431  0.79046871 -0.19912579 -1.08061812]\n",
      "[ episode 104 ][ timestamp 33 ] state=[ 0.21207431  0.79046871 -0.19912579 -1.08061812], action=0, reward=-1.0, next_state=[ 0.22788369  0.59845204 -0.22073816 -0.85644138]\n",
      "[ Ended! ] Episode 104: Exploration_rate=0.5967292370047992. Score=33.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 105 ] state=[-0.04683196 -0.00377198  0.04594766 -0.04679963]\n",
      "[ episode 105 ][ timestamp 1 ] state=[-0.04683196 -0.00377198  0.04594766 -0.04679963], action=0, reward=1.0, next_state=[-0.0469074  -0.19952165  0.04501167  0.26001884]\n",
      "[ episode 105 ][ timestamp 2 ] state=[-0.0469074  -0.19952165  0.04501167  0.26001884], action=1, reward=1.0, next_state=[-0.05089783 -0.0050702   0.05021205 -0.01813395]\n",
      "[ episode 105 ][ timestamp 3 ] state=[-0.05089783 -0.0050702   0.05021205 -0.01813395], action=0, reward=1.0, next_state=[-0.05099924 -0.20087494  0.04984937  0.28995913]\n",
      "[ episode 105 ][ timestamp 4 ] state=[-0.05099924 -0.20087494  0.04984937  0.28995913], action=1, reward=1.0, next_state=[-0.05501674 -0.00649795  0.05564855  0.01340548]\n",
      "[ episode 105 ][ timestamp 5 ] state=[-0.05501674 -0.00649795  0.05564855  0.01340548], action=0, reward=1.0, next_state=[-0.05514669 -0.20237199  0.05591666  0.32311395]\n",
      "[ episode 105 ][ timestamp 6 ] state=[-0.05514669 -0.20237199  0.05591666  0.32311395], action=1, reward=1.0, next_state=[-0.05919413 -0.008089    0.06237894  0.04857588]\n",
      "[ episode 105 ][ timestamp 7 ] state=[-0.05919413 -0.008089    0.06237894  0.04857588], action=0, reward=1.0, next_state=[-0.05935591 -0.20404737  0.06335046  0.36026927]\n",
      "[ episode 105 ][ timestamp 8 ] state=[-0.05935591 -0.20404737  0.06335046  0.36026927], action=1, reward=1.0, next_state=[-0.06343686 -0.00988046  0.07055584  0.08821571]\n",
      "[ episode 105 ][ timestamp 9 ] state=[-0.06343686 -0.00988046  0.07055584  0.08821571], action=1, reward=1.0, next_state=[-0.06363447  0.18416295  0.07232016 -0.18139901]\n",
      "[ episode 105 ][ timestamp 10 ] state=[-0.06363447  0.18416295  0.07232016 -0.18139901], action=0, reward=1.0, next_state=[-0.05995121 -0.01191534  0.06869218  0.13319321]\n",
      "[ episode 105 ][ timestamp 11 ] state=[-0.05995121 -0.01191534  0.06869218  0.13319321], action=0, reward=1.0, next_state=[-0.06018952 -0.20795058  0.07135604  0.4467322 ]\n",
      "[ episode 105 ][ timestamp 12 ] state=[-0.06018952 -0.20795058  0.07135604  0.4467322 ], action=1, reward=1.0, next_state=[-0.06434853 -0.0139068   0.08029068  0.17736809]\n",
      "[ episode 105 ][ timestamp 13 ] state=[-0.06434853 -0.0139068   0.08029068  0.17736809], action=1, reward=1.0, next_state=[-0.06462667  0.17997981  0.08383805 -0.08894479]\n",
      "[ episode 105 ][ timestamp 14 ] state=[-0.06462667  0.17997981  0.08383805 -0.08894479], action=0, reward=1.0, next_state=[-0.06102707 -0.01623755  0.08205915  0.228967  ]\n",
      "[ episode 105 ][ timestamp 15 ] state=[-0.06102707 -0.01623755  0.08205915  0.228967  ], action=0, reward=1.0, next_state=[-0.06135182 -0.21243043  0.08663849  0.54636437]\n",
      "[ episode 105 ][ timestamp 16 ] state=[-0.06135182 -0.21243043  0.08663849  0.54636437], action=1, reward=1.0, next_state=[-0.06560043 -0.01862577  0.09756578  0.28218762]\n",
      "[ episode 105 ][ timestamp 17 ] state=[-0.06560043 -0.01862577  0.09756578  0.28218762], action=1, reward=1.0, next_state=[-0.06597294  0.17497895  0.10320953  0.02180049]\n",
      "[ episode 105 ][ timestamp 18 ] state=[-0.06597294  0.17497895  0.10320953  0.02180049], action=1, reward=1.0, next_state=[-0.06247337  0.36848098  0.10364554 -0.23661824]\n",
      "[ episode 105 ][ timestamp 19 ] state=[-0.06247337  0.36848098  0.10364554 -0.23661824], action=0, reward=1.0, next_state=[-0.05510375  0.17204273  0.09891317  0.08687515]\n",
      "[ episode 105 ][ timestamp 20 ] state=[-0.05510375  0.17204273  0.09891317  0.08687515], action=0, reward=1.0, next_state=[-0.05166289 -0.02434769  0.10065068  0.40905394]\n",
      "[ episode 105 ][ timestamp 21 ] state=[-0.05166289 -0.02434769  0.10065068  0.40905394], action=1, reward=1.0, next_state=[-0.05214984  0.16921392  0.10883176  0.14972231]\n",
      "[ episode 105 ][ timestamp 22 ] state=[-0.05214984  0.16921392  0.10883176  0.14972231], action=0, reward=1.0, next_state=[-0.04876557 -0.02728454  0.1118262   0.47465959]\n",
      "[ episode 105 ][ timestamp 23 ] state=[-0.04876557 -0.02728454  0.1118262   0.47465959], action=1, reward=1.0, next_state=[-0.04931126  0.16609532  0.12131939  0.21921001]\n",
      "[ episode 105 ][ timestamp 24 ] state=[-0.04931126  0.16609532  0.12131939  0.21921001], action=0, reward=1.0, next_state=[-0.04598935 -0.03053311  0.12570359  0.54756525]\n",
      "[ episode 105 ][ timestamp 25 ] state=[-0.04598935 -0.03053311  0.12570359  0.54756525], action=1, reward=1.0, next_state=[-0.04660001  0.16261952  0.1366549   0.29698195]\n",
      "[ episode 105 ][ timestamp 26 ] state=[-0.04660001  0.16261952  0.1366549   0.29698195], action=0, reward=1.0, next_state=[-0.04334762 -0.03415901  0.14259454  0.62944959]\n",
      "[ episode 105 ][ timestamp 27 ] state=[-0.04334762 -0.03415901  0.14259454  0.62944959], action=1, reward=1.0, next_state=[-0.0440308   0.15871545  0.15518353  0.3848551 ]\n",
      "[ episode 105 ][ timestamp 28 ] state=[-0.0440308   0.15871545  0.15518353  0.3848551 ], action=1, reward=1.0, next_state=[-0.04085649  0.35133304  0.16288063  0.14484175]\n",
      "[ episode 105 ][ timestamp 29 ] state=[-0.04085649  0.35133304  0.16288063  0.14484175], action=1, reward=1.0, next_state=[-0.03382983  0.54379314  0.16577747 -0.09235193]\n",
      "[ episode 105 ][ timestamp 30 ] state=[-0.03382983  0.54379314  0.16577747 -0.09235193], action=0, reward=1.0, next_state=[-0.02295397  0.34673105  0.16393043  0.24770438]\n",
      "[ episode 105 ][ timestamp 31 ] state=[-0.02295397  0.34673105  0.16393043  0.24770438], action=0, reward=1.0, next_state=[-0.01601935  0.14969374  0.16888452  0.58727794]\n",
      "[ episode 105 ][ timestamp 32 ] state=[-0.01601935  0.14969374  0.16888452  0.58727794], action=1, reward=1.0, next_state=[-0.01302547  0.34209825  0.18063008  0.35219357]\n",
      "[ episode 105 ][ timestamp 33 ] state=[-0.01302547  0.34209825  0.18063008  0.35219357], action=0, reward=1.0, next_state=[-0.00618351  0.14492913  0.18767395  0.69595247]\n",
      "[ episode 105 ][ timestamp 34 ] state=[-0.00618351  0.14492913  0.18767395  0.69595247], action=1, reward=1.0, next_state=[-0.00328493  0.33702121  0.201593    0.46772662]\n",
      "[ episode 105 ][ timestamp 35 ] state=[-0.00328493  0.33702121  0.201593    0.46772662], action=0, reward=-1.0, next_state=[0.0034555  0.13970739 0.21094753 0.81657132]\n",
      "[ Ended! ] Episode 105: Exploration_rate=0.5937455908197752. Score=35.\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ] state=[ 0.00723614 -0.00069807 -0.00064443  0.02730625]\n",
      "[ episode 106 ][ timestamp 1 ] state=[ 0.00723614 -0.00069807 -0.00064443  0.02730625], action=0, reward=1.0, next_state=[ 7.22218017e-03 -1.95810776e-01 -9.83058179e-05  3.19785786e-01]\n",
      "[ episode 106 ][ timestamp 2 ] state=[ 7.22218017e-03 -1.95810776e-01 -9.83058179e-05  3.19785786e-01], action=1, reward=1.0, next_state=[ 0.00330596 -0.00068742  0.00629741  0.02707186]\n",
      "[ episode 106 ][ timestamp 3 ] state=[ 0.00330596 -0.00068742  0.00629741  0.02707186], action=1, reward=1.0, next_state=[ 0.00329222  0.19434365  0.00683885 -0.26361753]\n",
      "[ episode 106 ][ timestamp 4 ] state=[ 0.00329222  0.19434365  0.00683885 -0.26361753], action=0, reward=1.0, next_state=[ 0.00717909 -0.00087524  0.0015665   0.03121457]\n",
      "[ episode 106 ][ timestamp 5 ] state=[ 0.00717909 -0.00087524  0.0015665   0.03121457], action=0, reward=1.0, next_state=[ 0.00716158 -0.19601962  0.00219079  0.32439133]\n",
      "[ episode 106 ][ timestamp 6 ] state=[ 0.00716158 -0.19601962  0.00219079  0.32439133], action=0, reward=1.0, next_state=[ 0.00324119 -0.3911727   0.00867861  0.61776433]\n",
      "[ episode 106 ][ timestamp 7 ] state=[ 0.00324119 -0.3911727   0.00867861  0.61776433], action=1, reward=1.0, next_state=[-0.00458226 -0.19617305  0.0210339   0.32782736]\n",
      "[ episode 106 ][ timestamp 8 ] state=[-0.00458226 -0.19617305  0.0210339   0.32782736], action=1, reward=1.0, next_state=[-0.00850572 -0.00135677  0.02759045  0.04185108]\n",
      "[ episode 106 ][ timestamp 9 ] state=[-0.00850572 -0.00135677  0.02759045  0.04185108], action=1, reward=1.0, next_state=[-0.00853286  0.1933589   0.02842747 -0.2420007 ]\n",
      "[ episode 106 ][ timestamp 10 ] state=[-0.00853286  0.1933589   0.02842747 -0.2420007 ], action=0, reward=1.0, next_state=[-0.00466568 -0.00215734  0.02358746  0.05951172]\n",
      "[ episode 106 ][ timestamp 11 ] state=[-0.00466568 -0.00215734  0.02358746  0.05951172], action=0, reward=1.0, next_state=[-0.00470883 -0.19760941  0.02477769  0.35954234]\n",
      "[ episode 106 ][ timestamp 12 ] state=[-0.00470883 -0.19760941  0.02477769  0.35954234], action=1, reward=1.0, next_state=[-0.00866102 -0.00284828  0.03196854  0.07477422]\n",
      "[ episode 106 ][ timestamp 13 ] state=[-0.00866102 -0.00284828  0.03196854  0.07477422], action=0, reward=1.0, next_state=[-0.00871798 -0.1984136   0.03346402  0.37736947]\n",
      "[ episode 106 ][ timestamp 14 ] state=[-0.00871798 -0.1984136   0.03346402  0.37736947], action=0, reward=1.0, next_state=[-0.01268625 -0.39399445  0.04101141  0.68041307]\n",
      "[ episode 106 ][ timestamp 15 ] state=[-0.01268625 -0.39399445  0.04101141  0.68041307], action=0, reward=1.0, next_state=[-0.02056614 -0.58966132  0.05461967  0.98572056]\n",
      "[ episode 106 ][ timestamp 16 ] state=[-0.02056614 -0.58966132  0.05461967  0.98572056], action=0, reward=1.0, next_state=[-0.03235937 -0.78547059  0.07433408  1.29504666]\n",
      "[ episode 106 ][ timestamp 17 ] state=[-0.03235937 -0.78547059  0.07433408  1.29504666], action=1, reward=1.0, next_state=[-0.04806878 -0.59136759  0.10023502  1.02653029]\n",
      "[ episode 106 ][ timestamp 18 ] state=[-0.04806878 -0.59136759  0.10023502  1.02653029], action=1, reward=1.0, next_state=[-0.05989613 -0.39771256  0.12076562  0.76692554]\n",
      "[ episode 106 ][ timestamp 19 ] state=[-0.05989613 -0.39771256  0.12076562  0.76692554], action=1, reward=1.0, next_state=[-0.06785038 -0.20444189  0.13610413  0.51454987]\n",
      "[ episode 106 ][ timestamp 20 ] state=[-0.06785038 -0.20444189  0.13610413  0.51454987], action=0, reward=1.0, next_state=[-0.07193922 -0.40119167  0.14639513  0.84683645]\n",
      "[ episode 106 ][ timestamp 21 ] state=[-0.07193922 -0.40119167  0.14639513  0.84683645], action=0, reward=1.0, next_state=[-0.07996305 -0.59797483  0.16333186  1.1817404 ]\n",
      "[ episode 106 ][ timestamp 22 ] state=[-0.07996305 -0.59797483  0.16333186  1.1817404 ], action=1, reward=1.0, next_state=[-0.09192255 -0.40530507  0.18696667  0.94438848]\n",
      "[ episode 106 ][ timestamp 23 ] state=[-0.09192255 -0.40530507  0.18696667  0.94438848], action=1, reward=1.0, next_state=[-0.10002865 -0.21312656  0.20585444  0.71579298]\n",
      "[ episode 106 ][ timestamp 24 ] state=[-0.10002865 -0.21312656  0.20585444  0.71579298], action=1, reward=-1.0, next_state=[-0.10429118 -0.02135825  0.2201703   0.49430848]\n",
      "[ Ended! ] Episode 106: Exploration_rate=0.5907768628656763. Score=24.\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ] state=[ 0.02620085 -0.03297713 -0.03391855 -0.00128624]\n",
      "[ episode 107 ][ timestamp 1 ] state=[ 0.02620085 -0.03297713 -0.03391855 -0.00128624], action=1, reward=1.0, next_state=[ 0.02554131  0.16261443 -0.03394427 -0.30447498]\n",
      "[ episode 107 ][ timestamp 2 ] state=[ 0.02554131  0.16261443 -0.03394427 -0.30447498], action=1, reward=1.0, next_state=[ 0.0287936   0.35820326 -0.04003377 -0.60766693]\n",
      "[ episode 107 ][ timestamp 3 ] state=[ 0.0287936   0.35820326 -0.04003377 -0.60766693], action=0, reward=1.0, next_state=[ 0.03595766  0.16366323 -0.05218711 -0.32785746]\n",
      "[ episode 107 ][ timestamp 4 ] state=[ 0.03595766  0.16366323 -0.05218711 -0.32785746], action=1, reward=1.0, next_state=[ 0.03923093  0.3594878  -0.05874426 -0.63653047]\n",
      "[ episode 107 ][ timestamp 5 ] state=[ 0.03923093  0.3594878  -0.05874426 -0.63653047], action=1, reward=1.0, next_state=[ 0.04642068  0.55537768 -0.07147487 -0.94711932]\n",
      "[ episode 107 ][ timestamp 6 ] state=[ 0.04642068  0.55537768 -0.07147487 -0.94711932], action=0, reward=1.0, next_state=[ 0.05752824  0.36128722 -0.09041726 -0.6777227 ]\n",
      "[ episode 107 ][ timestamp 7 ] state=[ 0.05752824  0.36128722 -0.09041726 -0.6777227 ], action=0, reward=1.0, next_state=[ 0.06475398  0.16753006 -0.10397171 -0.41482063]\n",
      "[ episode 107 ][ timestamp 8 ] state=[ 0.06475398  0.16753006 -0.10397171 -0.41482063], action=1, reward=1.0, next_state=[ 0.06810458  0.36396016 -0.11226812 -0.73838727]\n",
      "[ episode 107 ][ timestamp 9 ] state=[ 0.06810458  0.36396016 -0.11226812 -0.73838727], action=0, reward=1.0, next_state=[ 0.07538379  0.17055283 -0.12703587 -0.48304019]\n",
      "[ episode 107 ][ timestamp 10 ] state=[ 0.07538379  0.17055283 -0.12703587 -0.48304019], action=0, reward=1.0, next_state=[ 0.07879484 -0.0225688  -0.13669667 -0.23294024]\n",
      "[ episode 107 ][ timestamp 11 ] state=[ 0.07879484 -0.0225688  -0.13669667 -0.23294024], action=1, reward=1.0, next_state=[ 0.07834347  0.17421466 -0.14135548 -0.56542567]\n",
      "[ episode 107 ][ timestamp 12 ] state=[ 0.07834347  0.17421466 -0.14135548 -0.56542567], action=1, reward=1.0, next_state=[ 0.08182776  0.3710072  -0.15266399 -0.89909051]\n",
      "[ episode 107 ][ timestamp 13 ] state=[ 0.08182776  0.3710072  -0.15266399 -0.89909051], action=0, reward=1.0, next_state=[ 0.0892479   0.17824706 -0.1706458  -0.65802222]\n",
      "[ episode 107 ][ timestamp 14 ] state=[ 0.0892479   0.17824706 -0.1706458  -0.65802222], action=1, reward=1.0, next_state=[ 0.09281285  0.37528158 -0.18380624 -0.99920793]\n",
      "[ episode 107 ][ timestamp 15 ] state=[ 0.09281285  0.37528158 -0.18380624 -0.99920793], action=1, reward=1.0, next_state=[ 0.10031848  0.57232095 -0.2037904  -1.3435236 ]\n",
      "[ episode 107 ][ timestamp 16 ] state=[ 0.10031848  0.57232095 -0.2037904  -1.3435236 ], action=1, reward=-1.0, next_state=[ 0.1117649   0.76933844 -0.23066087 -1.69243489]\n",
      "[ Ended! ] Episode 107: Exploration_rate=0.5878229785513479. Score=16.\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ] state=[-0.00260107 -0.0261397  -0.01105928 -0.03555457]\n",
      "[ episode 108 ][ timestamp 1 ] state=[-0.00260107 -0.0261397  -0.01105928 -0.03555457], action=0, reward=1.0, next_state=[-0.00312386 -0.22110133 -0.01177038  0.25361862]\n",
      "[ episode 108 ][ timestamp 2 ] state=[-0.00312386 -0.22110133 -0.01177038  0.25361862], action=0, reward=1.0, next_state=[-0.00754589 -0.41605325 -0.006698    0.54256584]\n",
      "[ episode 108 ][ timestamp 3 ] state=[-0.00754589 -0.41605325 -0.006698    0.54256584], action=1, reward=1.0, next_state=[-0.01586695 -0.22083781  0.00415331  0.24778005]\n",
      "[ episode 108 ][ timestamp 4 ] state=[-0.01586695 -0.22083781  0.00415331  0.24778005], action=0, reward=1.0, next_state=[-0.02028371 -0.41601883  0.00910891  0.54177012]\n",
      "[ episode 108 ][ timestamp 5 ] state=[-0.02028371 -0.41601883  0.00910891  0.54177012], action=0, reward=1.0, next_state=[-0.02860409 -0.61126761  0.01994432  0.83730914]\n",
      "[ episode 108 ][ timestamp 6 ] state=[-0.02860409 -0.61126761  0.01994432  0.83730914], action=0, reward=1.0, next_state=[-0.04082944 -0.80665619  0.0366905   1.13619696]\n",
      "[ episode 108 ][ timestamp 7 ] state=[-0.04082944 -0.80665619  0.0366905   1.13619696], action=1, reward=1.0, next_state=[-0.05696256 -0.61203293  0.05941444  0.85524312]\n",
      "[ episode 108 ][ timestamp 8 ] state=[-0.05696256 -0.61203293  0.05941444  0.85524312], action=0, reward=1.0, next_state=[-0.06920322 -0.80791206  0.0765193   1.16600094]\n",
      "[ episode 108 ][ timestamp 9 ] state=[-0.06920322 -0.80791206  0.0765193   1.16600094], action=0, reward=1.0, next_state=[-0.08536146 -1.00394191  0.09983932  1.48166002]\n",
      "[ episode 108 ][ timestamp 10 ] state=[-0.08536146 -1.00394191  0.09983932  1.48166002], action=0, reward=1.0, next_state=[-0.1054403  -1.20013011  0.12947252  1.80378088]\n",
      "[ episode 108 ][ timestamp 11 ] state=[-0.1054403  -1.20013011  0.12947252  1.80378088], action=1, reward=1.0, next_state=[-0.1294429  -1.00667044  0.16554814  1.55397888]\n",
      "[ episode 108 ][ timestamp 12 ] state=[-0.1294429  -1.00667044  0.16554814  1.55397888], action=1, reward=1.0, next_state=[-0.14957631 -0.81387441  0.19662772  1.31718778]\n",
      "[ episode 108 ][ timestamp 13 ] state=[-0.14957631 -0.81387441  0.19662772  1.31718778], action=0, reward=-1.0, next_state=[-0.1658538  -1.01086335  0.22297147  1.66441428]\n",
      "[ Ended! ] Episode 108: Exploration_rate=0.5848838636585911. Score=13.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 109 ] state=[0.04280532 0.02265422 0.00315982 0.03869043]\n",
      "[ episode 109 ][ timestamp 1 ] state=[0.04280532 0.02265422 0.00315982 0.03869043], action=0, reward=1.0, next_state=[ 0.0432584  -0.1725129   0.00393363  0.33236863]\n",
      "[ episode 109 ][ timestamp 2 ] state=[ 0.0432584  -0.1725129   0.00393363  0.33236863], action=0, reward=1.0, next_state=[ 0.03980814 -0.36769062  0.01058101  0.62628943]\n",
      "[ episode 109 ][ timestamp 3 ] state=[ 0.03980814 -0.36769062  0.01058101  0.62628943], action=1, reward=1.0, next_state=[ 0.03245433 -0.17271795  0.02310679  0.33695756]\n",
      "[ episode 109 ][ timestamp 4 ] state=[ 0.03245433 -0.17271795  0.02310679  0.33695756], action=0, reward=1.0, next_state=[ 0.02899997 -0.36816098  0.02984595  0.63683663]\n",
      "[ episode 109 ][ timestamp 5 ] state=[ 0.02899997 -0.36816098  0.02984595  0.63683663], action=0, reward=1.0, next_state=[ 0.02163675 -0.56368616  0.04258268  0.93876719]\n",
      "[ episode 109 ][ timestamp 6 ] state=[ 0.02163675 -0.56368616  0.04258268  0.93876719], action=1, reward=1.0, next_state=[ 0.01036303 -0.36916337  0.06135802  0.65976302]\n",
      "[ episode 109 ][ timestamp 7 ] state=[ 0.01036303 -0.36916337  0.06135802  0.65976302], action=0, reward=1.0, next_state=[ 0.00297976 -0.56508313  0.07455328  0.97111758]\n",
      "[ episode 109 ][ timestamp 8 ] state=[ 0.00297976 -0.56508313  0.07455328  0.97111758], action=1, reward=1.0, next_state=[-0.0083219  -0.37103667  0.09397563  0.70275479]\n",
      "[ episode 109 ][ timestamp 9 ] state=[-0.0083219  -0.37103667  0.09397563  0.70275479], action=1, reward=1.0, next_state=[-0.01574264 -0.17733413  0.10803073  0.44107123]\n",
      "[ episode 109 ][ timestamp 10 ] state=[-0.01574264 -0.17733413  0.10803073  0.44107123], action=1, reward=1.0, next_state=[-0.01928932  0.01610644  0.11685215  0.1843012 ]\n",
      "[ episode 109 ][ timestamp 11 ] state=[-0.01928932  0.01610644  0.11685215  0.1843012 ], action=0, reward=1.0, next_state=[-0.01896719 -0.18047675  0.12053818  0.5114415 ]\n",
      "[ episode 109 ][ timestamp 12 ] state=[-0.01896719 -0.18047675  0.12053818  0.5114415 ], action=0, reward=1.0, next_state=[-0.02257672 -0.37707201  0.13076701  0.83954715]\n",
      "[ episode 109 ][ timestamp 13 ] state=[-0.02257672 -0.37707201  0.13076701  0.83954715], action=0, reward=1.0, next_state=[-0.03011816 -0.57371374  0.14755795  1.17032742]\n",
      "[ episode 109 ][ timestamp 14 ] state=[-0.03011816 -0.57371374  0.14755795  1.17032742], action=1, reward=1.0, next_state=[-0.04159244 -0.38078608  0.1709645   0.92730552]\n",
      "[ episode 109 ][ timestamp 15 ] state=[-0.04159244 -0.38078608  0.1709645   0.92730552], action=0, reward=1.0, next_state=[-0.04920816 -0.57775252  0.18951061  1.26846692]\n",
      "[ episode 109 ][ timestamp 16 ] state=[-0.04920816 -0.57775252  0.18951061  1.26846692], action=1, reward=-1.0, next_state=[-0.06076321 -0.38548734  0.21487995  1.04061569]\n",
      "[ Ended! ] Episode 109: Exploration_rate=0.5819594443402982. Score=16.\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ] state=[-0.04090479 -0.04054065 -0.02417352 -0.0214664 ]\n",
      "[ episode 110 ][ timestamp 1 ] state=[-0.04090479 -0.04054065 -0.02417352 -0.0214664 ], action=1, reward=1.0, next_state=[-0.0417156   0.15491948 -0.02460285 -0.32167726]\n",
      "[ episode 110 ][ timestamp 2 ] state=[-0.0417156   0.15491948 -0.02460285 -0.32167726], action=0, reward=1.0, next_state=[-0.03861722 -0.03984363 -0.03103639 -0.03685352]\n",
      "[ episode 110 ][ timestamp 3 ] state=[-0.03861722 -0.03984363 -0.03103639 -0.03685352], action=0, reward=1.0, next_state=[-0.03941409 -0.23450709 -0.03177346  0.24587781]\n",
      "[ episode 110 ][ timestamp 4 ] state=[-0.03941409 -0.23450709 -0.03177346  0.24587781], action=0, reward=1.0, next_state=[-0.04410423 -0.42916117 -0.02685591  0.52837173]\n",
      "[ episode 110 ][ timestamp 5 ] state=[-0.04410423 -0.42916117 -0.02685591  0.52837173], action=1, reward=1.0, next_state=[-0.05268745 -0.23367187 -0.01628847  0.22734884]\n",
      "[ episode 110 ][ timestamp 6 ] state=[-0.05268745 -0.23367187 -0.01628847  0.22734884], action=0, reward=1.0, next_state=[-0.05736089 -0.4285573  -0.0117415   0.51484961]\n",
      "[ episode 110 ][ timestamp 7 ] state=[-0.05736089 -0.4285573  -0.0117415   0.51484961], action=1, reward=1.0, next_state=[-0.06593204 -0.23327199 -0.0014445   0.2184899 ]\n",
      "[ episode 110 ][ timestamp 8 ] state=[-0.06593204 -0.23327199 -0.0014445   0.2184899 ], action=0, reward=1.0, next_state=[-0.07059748 -0.42837326  0.00292529  0.51071682]\n",
      "[ episode 110 ][ timestamp 9 ] state=[-0.07059748 -0.42837326  0.00292529  0.51071682], action=1, reward=1.0, next_state=[-0.07916494 -0.23329264  0.01313963  0.21895718]\n",
      "[ episode 110 ][ timestamp 10 ] state=[-0.07916494 -0.23329264  0.01313963  0.21895718], action=0, reward=1.0, next_state=[-0.08383079 -0.42859993  0.01751878  0.51575576]\n",
      "[ episode 110 ][ timestamp 11 ] state=[-0.08383079 -0.42859993  0.01751878  0.51575576], action=1, reward=1.0, next_state=[-0.09240279 -0.233729    0.02783389  0.22864448]\n",
      "[ episode 110 ][ timestamp 12 ] state=[-0.09240279 -0.233729    0.02783389  0.22864448], action=0, reward=1.0, next_state=[-0.09707737 -0.42923742  0.03240678  0.52997564]\n",
      "[ episode 110 ][ timestamp 13 ] state=[-0.09707737 -0.42923742  0.03240678  0.52997564], action=1, reward=1.0, next_state=[-0.10566212 -0.23458598  0.04300629  0.2476777 ]\n",
      "[ episode 110 ][ timestamp 14 ] state=[-0.10566212 -0.23458598  0.04300629  0.2476777 ], action=1, reward=1.0, next_state=[-0.11035384 -0.04010377  0.04795985 -0.03113592]\n",
      "[ episode 110 ][ timestamp 15 ] state=[-0.11035384 -0.04010377  0.04795985 -0.03113592], action=1, reward=1.0, next_state=[-0.11115592  0.15429877  0.04733713 -0.30830964]\n",
      "[ episode 110 ][ timestamp 16 ] state=[-0.11115592  0.15429877  0.04733713 -0.30830964], action=1, reward=1.0, next_state=[-0.10806994  0.34871539  0.04117094 -0.58569597]\n",
      "[ episode 110 ][ timestamp 17 ] state=[-0.10806994  0.34871539  0.04117094 -0.58569597], action=0, reward=1.0, next_state=[-0.10109563  0.15304168  0.02945702 -0.2803333 ]\n",
      "[ episode 110 ][ timestamp 18 ] state=[-0.10109563  0.15304168  0.02945702 -0.2803333 ], action=0, reward=1.0, next_state=[-0.0980348  -0.04248781  0.02385035  0.02149281]\n",
      "[ episode 110 ][ timestamp 19 ] state=[-0.0980348  -0.04248781  0.02385035  0.02149281], action=1, reward=1.0, next_state=[-0.09888456  0.15228412  0.02428021 -0.26357067]\n",
      "[ episode 110 ][ timestamp 20 ] state=[-0.09888456  0.15228412  0.02428021 -0.26357067], action=1, reward=1.0, next_state=[-0.09583887  0.34705124  0.01900879 -0.54849755]\n",
      "[ episode 110 ][ timestamp 21 ] state=[-0.09583887  0.34705124  0.01900879 -0.54849755], action=1, reward=1.0, next_state=[-0.08889785  0.54190107  0.00803884 -0.83513125]\n",
      "[ episode 110 ][ timestamp 22 ] state=[-0.08889785  0.54190107  0.00803884 -0.83513125], action=1, reward=1.0, next_state=[-0.07805983  0.73691229 -0.00866378 -1.12527523]\n",
      "[ episode 110 ][ timestamp 23 ] state=[-0.07805983  0.73691229 -0.00866378 -1.12527523], action=1, reward=1.0, next_state=[-0.06332158  0.93214671 -0.03116929 -1.42066299]\n",
      "[ episode 110 ][ timestamp 24 ] state=[-0.06332158  0.93214671 -0.03116929 -1.42066299], action=0, reward=1.0, next_state=[-0.04467865  0.73742396 -0.05958255 -1.13788302]\n",
      "[ episode 110 ][ timestamp 25 ] state=[-0.04467865  0.73742396 -0.05958255 -1.13788302], action=1, reward=1.0, next_state=[-0.02993017  0.93327235 -0.08234021 -1.44864122]\n",
      "[ episode 110 ][ timestamp 26 ] state=[-0.02993017  0.93327235 -0.08234021 -1.44864122], action=0, reward=1.0, next_state=[-0.01126472  0.7392536  -0.11131303 -1.18277977]\n",
      "[ episode 110 ][ timestamp 27 ] state=[-0.01126472  0.7392536  -0.11131303 -1.18277977], action=0, reward=1.0, next_state=[ 0.00352035  0.54573799 -0.13496863 -0.92696133]\n",
      "[ episode 110 ][ timestamp 28 ] state=[ 0.00352035  0.54573799 -0.13496863 -0.92696133], action=0, reward=1.0, next_state=[ 0.01443511  0.35267125 -0.15350785 -0.67955538]\n",
      "[ episode 110 ][ timestamp 29 ] state=[ 0.01443511  0.35267125 -0.15350785 -0.67955538], action=0, reward=1.0, next_state=[ 0.02148854  0.15997716 -0.16709896 -0.43886741]\n",
      "[ episode 110 ][ timestamp 30 ] state=[ 0.02148854  0.15997716 -0.16709896 -0.43886741], action=0, reward=1.0, next_state=[ 0.02468808 -0.03243449 -0.17587631 -0.20316875]\n",
      "[ episode 110 ][ timestamp 31 ] state=[ 0.02468808 -0.03243449 -0.17587631 -0.20316875], action=0, reward=1.0, next_state=[ 0.02403939 -0.22466225 -0.17993968  0.02928333]\n",
      "[ episode 110 ][ timestamp 32 ] state=[ 0.02403939 -0.22466225 -0.17993968  0.02928333], action=0, reward=1.0, next_state=[ 0.01954615 -0.41680876 -0.17935402  0.2602324 ]\n",
      "[ episode 110 ][ timestamp 33 ] state=[ 0.01954615 -0.41680876 -0.17935402  0.2602324 ], action=1, reward=1.0, next_state=[ 0.01120997 -0.21964034 -0.17414937 -0.08322394]\n",
      "[ episode 110 ][ timestamp 34 ] state=[ 0.01120997 -0.21964034 -0.17414937 -0.08322394], action=0, reward=1.0, next_state=[ 0.00681716 -0.41189375 -0.17581385  0.14985272]\n",
      "[ episode 110 ][ timestamp 35 ] state=[ 0.00681716 -0.41189375 -0.17581385  0.14985272], action=0, reward=1.0, next_state=[-0.00142071 -0.60411944 -0.17281679  0.38232298]\n",
      "[ episode 110 ][ timestamp 36 ] state=[-0.00142071 -0.60411944 -0.17281679  0.38232298], action=1, reward=1.0, next_state=[-0.0135031  -0.40701907 -0.16517034  0.04052074]\n",
      "[ episode 110 ][ timestamp 37 ] state=[-0.0135031  -0.40701907 -0.16517034  0.04052074], action=1, reward=1.0, next_state=[-0.02164348 -0.20996125 -0.16435992 -0.29938276]\n",
      "[ episode 110 ][ timestamp 38 ] state=[-0.02164348 -0.20996125 -0.16435992 -0.29938276], action=1, reward=1.0, next_state=[-0.02584271 -0.01292464 -0.17034758 -0.6390591 ]\n",
      "[ episode 110 ][ timestamp 39 ] state=[-0.02584271 -0.01292464 -0.17034758 -0.6390591 ], action=0, reward=1.0, next_state=[-0.0261012  -0.20531353 -0.18312876 -0.40449307]\n",
      "[ episode 110 ][ timestamp 40 ] state=[-0.0261012  -0.20531353 -0.18312876 -0.40449307], action=0, reward=1.0, next_state=[-0.03020747 -0.39743036 -0.19121862 -0.17467591]\n",
      "[ episode 110 ][ timestamp 41 ] state=[-0.03020747 -0.39743036 -0.19121862 -0.17467591], action=0, reward=1.0, next_state=[-0.03815608 -0.58937467 -0.19471214  0.05211649]\n",
      "[ episode 110 ][ timestamp 42 ] state=[-0.03815608 -0.58937467 -0.19471214  0.05211649], action=1, reward=1.0, next_state=[-0.04994357 -0.39207128 -0.19366981 -0.29513038]\n",
      "[ episode 110 ][ timestamp 43 ] state=[-0.04994357 -0.39207128 -0.19366981 -0.29513038], action=0, reward=1.0, next_state=[-0.057785   -0.58398087 -0.19957242 -0.06923139]\n",
      "[ episode 110 ][ timestamp 44 ] state=[-0.057785   -0.58398087 -0.19957242 -0.06923139], action=0, reward=1.0, next_state=[-0.06946461 -0.77576544 -0.20095704  0.15444992]\n",
      "[ episode 110 ][ timestamp 45 ] state=[-0.06946461 -0.77576544 -0.20095704  0.15444992], action=1, reward=1.0, next_state=[-0.08497992 -0.57841842 -0.19786804 -0.19429802]\n",
      "[ episode 110 ][ timestamp 46 ] state=[-0.08497992 -0.57841842 -0.19786804 -0.19429802], action=1, reward=1.0, next_state=[-0.09654829 -0.38109742 -0.201754   -0.54229865]\n",
      "[ episode 110 ][ timestamp 47 ] state=[-0.09654829 -0.38109742 -0.201754   -0.54229865], action=0, reward=-1.0, next_state=[-0.10417024 -0.57289775 -0.21259998 -0.31934778]\n",
      "[ Ended! ] Episode 110: Exploration_rate=0.5790496471185967. Score=47.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 111 ] state=[-0.03722399 -0.00268046  0.04671278 -0.00991563]\n",
      "[ episode 111 ][ timestamp 1 ] state=[-0.03722399 -0.00268046  0.04671278 -0.00991563], action=0, reward=1.0, next_state=[-0.0372776  -0.19844013  0.04651446  0.29713212]\n",
      "[ episode 111 ][ timestamp 2 ] state=[-0.0372776  -0.19844013  0.04651446  0.29713212], action=1, reward=1.0, next_state=[-0.0412464  -0.00401106  0.0524571   0.01947427]\n",
      "[ episode 111 ][ timestamp 3 ] state=[-0.0412464  -0.00401106  0.0524571   0.01947427], action=1, reward=1.0, next_state=[-0.04132662  0.19032089  0.05284659 -0.25620736]\n",
      "[ episode 111 ][ timestamp 4 ] state=[-0.04132662  0.19032089  0.05284659 -0.25620736], action=0, reward=1.0, next_state=[-0.0375202  -0.00551419  0.04772244  0.05266482]\n",
      "[ episode 111 ][ timestamp 5 ] state=[-0.0375202  -0.00551419  0.04772244  0.05266482], action=0, reward=1.0, next_state=[-0.03763049 -0.20128678  0.04877574  0.36001446]\n",
      "[ episode 111 ][ timestamp 6 ] state=[-0.03763049 -0.20128678  0.04877574  0.36001446], action=0, reward=1.0, next_state=[-0.04165622 -0.39706692  0.05597603  0.66766979]\n",
      "[ episode 111 ][ timestamp 7 ] state=[-0.04165622 -0.39706692  0.05597603  0.66766979], action=0, reward=1.0, next_state=[-0.04959756 -0.59292079  0.06932942  0.97743881]\n",
      "[ episode 111 ][ timestamp 8 ] state=[-0.04959756 -0.59292079  0.06932942  0.97743881], action=0, reward=1.0, next_state=[-0.06145598 -0.78890045  0.0888782   1.29106863]\n",
      "[ episode 111 ][ timestamp 9 ] state=[-0.06145598 -0.78890045  0.0888782   1.29106863], action=1, reward=1.0, next_state=[-0.07723399 -0.59501387  0.11469957  1.02748249]\n",
      "[ episode 111 ][ timestamp 10 ] state=[-0.07723399 -0.59501387  0.11469957  1.02748249], action=1, reward=1.0, next_state=[-0.08913426 -0.40158999  0.13524922  0.77290087]\n",
      "[ episode 111 ][ timestamp 11 ] state=[-0.08913426 -0.40158999  0.13524922  0.77290087], action=0, reward=1.0, next_state=[-0.09716606 -0.59828779  0.15070724  1.10489529]\n",
      "[ episode 111 ][ timestamp 12 ] state=[-0.09716606 -0.59828779  0.15070724  1.10489529], action=1, reward=1.0, next_state=[-0.10913182 -0.4054339   0.17280515  0.86303379]\n",
      "[ episode 111 ][ timestamp 13 ] state=[-0.10913182 -0.4054339   0.17280515  0.86303379], action=0, reward=1.0, next_state=[-0.1172405  -0.60243396  0.19006582  1.20468504]\n",
      "[ episode 111 ][ timestamp 14 ] state=[-0.1172405  -0.60243396  0.19006582  1.20468504], action=1, reward=-1.0, next_state=[-0.12928918 -0.41020719  0.21415952  0.97708088]\n",
      "[ Ended! ] Episode 111: Exploration_rate=0.5761543988830038. Score=14.\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ] state=[-0.03772223  0.03039005  0.00177477  0.02138015]\n",
      "[ episode 112 ][ timestamp 1 ] state=[-0.03772223  0.03039005  0.00177477  0.02138015], action=1, reward=1.0, next_state=[-0.03711443  0.22548651  0.00220237 -0.27074228]\n",
      "[ episode 112 ][ timestamp 2 ] state=[-0.03711443  0.22548651  0.00220237 -0.27074228], action=1, reward=1.0, next_state=[-0.0326047   0.42057696 -0.00321247 -0.56272976]\n",
      "[ episode 112 ][ timestamp 3 ] state=[-0.0326047   0.42057696 -0.00321247 -0.56272976], action=1, reward=1.0, next_state=[-0.02419316  0.61574384 -0.01446707 -0.85642304]\n",
      "[ episode 112 ][ timestamp 4 ] state=[-0.02419316  0.61574384 -0.01446707 -0.85642304], action=0, reward=1.0, next_state=[-0.01187828  0.42082198 -0.03159553 -0.568324  ]\n",
      "[ episode 112 ][ timestamp 5 ] state=[-0.01187828  0.42082198 -0.03159553 -0.568324  ], action=1, reward=1.0, next_state=[-0.00346184  0.61637252 -0.04296201 -0.87079096]\n",
      "[ episode 112 ][ timestamp 6 ] state=[-0.00346184  0.61637252 -0.04296201 -0.87079096], action=0, reward=1.0, next_state=[ 0.00886561  0.42186043 -0.06037783 -0.59191899]\n",
      "[ episode 112 ][ timestamp 7 ] state=[ 0.00886561  0.42186043 -0.06037783 -0.59191899], action=1, reward=1.0, next_state=[ 0.01730281  0.61777336 -0.07221621 -0.9029932 ]\n",
      "[ episode 112 ][ timestamp 8 ] state=[ 0.01730281  0.61777336 -0.07221621 -0.9029932 ], action=0, reward=1.0, next_state=[ 0.02965828  0.42370003 -0.09027607 -0.63385509]\n",
      "[ episode 112 ][ timestamp 9 ] state=[ 0.02965828  0.42370003 -0.09027607 -0.63385509], action=1, reward=1.0, next_state=[ 0.03813228  0.6199576  -0.10295317 -0.95354779]\n",
      "[ episode 112 ][ timestamp 10 ] state=[ 0.03813228  0.6199576  -0.10295317 -0.95354779], action=1, reward=1.0, next_state=[ 0.05053143  0.81630273 -0.12202413 -1.27672081]\n",
      "[ episode 112 ][ timestamp 11 ] state=[ 0.05053143  0.81630273 -0.12202413 -1.27672081], action=0, reward=1.0, next_state=[ 0.06685749  0.62292953 -0.14755855 -1.02460394]\n",
      "[ episode 112 ][ timestamp 12 ] state=[ 0.06685749  0.62292953 -0.14755855 -1.02460394], action=0, reward=1.0, next_state=[ 0.07931608  0.43004769 -0.16805062 -0.78165021]\n",
      "[ episode 112 ][ timestamp 13 ] state=[ 0.07931608  0.43004769 -0.16805062 -0.78165021], action=1, reward=1.0, next_state=[ 0.08791703  0.62703168 -0.18368363 -1.1221384 ]\n",
      "[ episode 112 ][ timestamp 14 ] state=[ 0.08791703  0.62703168 -0.18368363 -1.1221384 ], action=0, reward=1.0, next_state=[ 0.10045767  0.43473021 -0.2061264  -0.8922385 ]\n",
      "[ episode 112 ][ timestamp 15 ] state=[ 0.10045767  0.43473021 -0.2061264  -0.8922385 ], action=0, reward=-1.0, next_state=[ 0.10915227  0.24291037 -0.22397117 -0.67077261]\n",
      "[ Ended! ] Episode 112: Exploration_rate=0.5732736268885887. Score=15.\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ] state=[ 0.00143792 -0.01668451 -0.01368553 -0.01063847]\n",
      "[ episode 113 ][ timestamp 1 ] state=[ 0.00143792 -0.01668451 -0.01368553 -0.01063847], action=1, reward=1.0, next_state=[ 0.00110423  0.17863101 -0.0138983  -0.30760773]\n",
      "[ episode 113 ][ timestamp 2 ] state=[ 0.00110423  0.17863101 -0.0138983  -0.30760773], action=1, reward=1.0, next_state=[ 0.00467685  0.37394822 -0.02005046 -0.60464121]\n",
      "[ episode 113 ][ timestamp 3 ] state=[ 0.00467685  0.37394822 -0.02005046 -0.60464121], action=0, reward=1.0, next_state=[ 0.01215582  0.17911232 -0.03214328 -0.31834055]\n",
      "[ episode 113 ][ timestamp 4 ] state=[ 0.01215582  0.17911232 -0.03214328 -0.31834055], action=1, reward=1.0, next_state=[ 0.01573807  0.37467698 -0.03851009 -0.62098451]\n",
      "[ episode 113 ][ timestamp 5 ] state=[ 0.01573807  0.37467698 -0.03851009 -0.62098451], action=1, reward=1.0, next_state=[ 0.0232316   0.57031497 -0.05092978 -0.92554309]\n",
      "[ episode 113 ][ timestamp 6 ] state=[ 0.0232316   0.57031497 -0.05092978 -0.92554309], action=0, reward=1.0, next_state=[ 0.0346379   0.37591648 -0.06944064 -0.64929034]\n",
      "[ episode 113 ][ timestamp 7 ] state=[ 0.0346379   0.37591648 -0.06944064 -0.64929034], action=1, reward=1.0, next_state=[ 0.04215623  0.57193353 -0.08242645 -0.96300645]\n",
      "[ episode 113 ][ timestamp 8 ] state=[ 0.04215623  0.57193353 -0.08242645 -0.96300645], action=1, reward=1.0, next_state=[ 0.0535949   0.76806048 -0.10168658 -1.280404  ]\n",
      "[ episode 113 ][ timestamp 9 ] state=[ 0.0535949   0.76806048 -0.10168658 -1.280404  ], action=0, reward=1.0, next_state=[ 0.06895611  0.57437051 -0.12729466 -1.0212142 ]\n",
      "[ episode 113 ][ timestamp 10 ] state=[ 0.06895611  0.57437051 -0.12729466 -1.0212142 ], action=1, reward=1.0, next_state=[ 0.08044352  0.77093742 -0.14771894 -1.35100257]\n",
      "[ episode 113 ][ timestamp 11 ] state=[ 0.08044352  0.77093742 -0.14771894 -1.35100257], action=1, reward=1.0, next_state=[ 0.09586227  0.96757343 -0.17473899 -1.68601594]\n",
      "[ episode 113 ][ timestamp 12 ] state=[ 0.09586227  0.96757343 -0.17473899 -1.68601594], action=0, reward=1.0, next_state=[ 0.11521374  0.77485086 -0.20845931 -1.45244649]\n",
      "[ episode 113 ][ timestamp 13 ] state=[ 0.11521374  0.77485086 -0.20845931 -1.45244649], action=0, reward=-1.0, next_state=[ 0.13071076  0.58280855 -0.23750824 -1.23146347]\n",
      "[ Ended! ] Episode 113: Exploration_rate=0.5704072587541458. Score=13.\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ] state=[-0.00087059 -0.01127649  0.01616434 -0.01784859]\n",
      "[ episode 114 ][ timestamp 1 ] state=[-0.00087059 -0.01127649  0.01616434 -0.01784859], action=0, reward=1.0, next_state=[-0.00109612 -0.20662649  0.01580736  0.27989022]\n",
      "[ episode 114 ][ timestamp 2 ] state=[-0.00109612 -0.20662649  0.01580736  0.27989022], action=1, reward=1.0, next_state=[-0.00522865 -0.01173355  0.02140517 -0.00776548]\n",
      "[ episode 114 ][ timestamp 3 ] state=[-0.00522865 -0.01173355  0.02140517 -0.00776548], action=1, reward=1.0, next_state=[-0.00546332  0.18307498  0.02124986 -0.29361871]\n",
      "[ episode 114 ][ timestamp 4 ] state=[-0.00546332  0.18307498  0.02124986 -0.29361871], action=1, reward=1.0, next_state=[-0.00180182  0.37788762  0.01537749 -0.5795247 ]\n",
      "[ episode 114 ][ timestamp 5 ] state=[-0.00180182  0.37788762  0.01537749 -0.5795247 ], action=1, reward=1.0, next_state=[ 0.00575594  0.57279074  0.00378699 -0.86732401]\n",
      "[ episode 114 ][ timestamp 6 ] state=[ 0.00575594  0.57279074  0.00378699 -0.86732401], action=0, reward=1.0, next_state=[ 0.01721175  0.37761746 -0.01355949 -0.57345282]\n",
      "[ episode 114 ][ timestamp 7 ] state=[ 0.01721175  0.37761746 -0.01355949 -0.57345282], action=0, reward=1.0, next_state=[ 0.0247641   0.18268822 -0.02502855 -0.28507221]\n",
      "[ episode 114 ][ timestamp 8 ] state=[ 0.0247641   0.18268822 -0.02502855 -0.28507221], action=1, reward=1.0, next_state=[ 0.02841786  0.37815803 -0.03072999 -0.58554271]\n",
      "[ episode 114 ][ timestamp 9 ] state=[ 0.02841786  0.37815803 -0.03072999 -0.58554271], action=0, reward=1.0, next_state=[ 0.03598102  0.18347968 -0.04244084 -0.30269626]\n",
      "[ episode 114 ][ timestamp 10 ] state=[ 0.03598102  0.18347968 -0.04244084 -0.30269626], action=1, reward=1.0, next_state=[ 0.03965062  0.37918    -0.04849477 -0.60845626]\n",
      "[ episode 114 ][ timestamp 11 ] state=[ 0.03965062  0.37918    -0.04849477 -0.60845626], action=0, reward=1.0, next_state=[ 0.04723422  0.18476836 -0.06066389 -0.33143351]\n",
      "[ episode 114 ][ timestamp 12 ] state=[ 0.04723422  0.18476836 -0.06066389 -0.33143351], action=1, reward=1.0, next_state=[ 0.05092959  0.38069899 -0.06729256 -0.64261309]\n",
      "[ episode 114 ][ timestamp 13 ] state=[ 0.05092959  0.38069899 -0.06729256 -0.64261309], action=0, reward=1.0, next_state=[ 0.05854357  0.18657634 -0.08014483 -0.37185723]\n",
      "[ episode 114 ][ timestamp 14 ] state=[ 0.05854357  0.18657634 -0.08014483 -0.37185723], action=1, reward=1.0, next_state=[ 0.06227509  0.38273997 -0.08758197 -0.68869555]\n",
      "[ episode 114 ][ timestamp 15 ] state=[ 0.06227509  0.38273997 -0.08758197 -0.68869555], action=0, reward=1.0, next_state=[ 0.06992989  0.18893568 -0.10135588 -0.42481954]\n",
      "[ episode 114 ][ timestamp 16 ] state=[ 0.06992989  0.18893568 -0.10135588 -0.42481954], action=0, reward=1.0, next_state=[ 0.07370861 -0.00461544 -0.10985227 -0.16573049]\n",
      "[ episode 114 ][ timestamp 17 ] state=[ 0.07370861 -0.00461544 -0.10985227 -0.16573049], action=1, reward=1.0, next_state=[ 0.0736163   0.19189356 -0.11316688 -0.49094889]\n",
      "[ episode 114 ][ timestamp 18 ] state=[ 0.0736163   0.19189356 -0.11316688 -0.49094889], action=0, reward=1.0, next_state=[ 0.07745417 -0.00146537 -0.12298586 -0.23596583]\n",
      "[ episode 114 ][ timestamp 19 ] state=[ 0.07745417 -0.00146537 -0.12298586 -0.23596583], action=0, reward=1.0, next_state=[ 0.07742486 -0.19463521 -0.12770518  0.01553358]\n",
      "[ episode 114 ][ timestamp 20 ] state=[ 0.07742486 -0.19463521 -0.12770518  0.01553358], action=0, reward=1.0, next_state=[ 0.07353216 -0.3877164  -0.12739451  0.26535356]\n",
      "[ episode 114 ][ timestamp 21 ] state=[ 0.07353216 -0.3877164  -0.12739451  0.26535356], action=1, reward=1.0, next_state=[ 0.06577783 -0.19102816 -0.12208743 -0.0646407 ]\n",
      "[ episode 114 ][ timestamp 22 ] state=[ 0.06577783 -0.19102816 -0.12208743 -0.0646407 ], action=1, reward=1.0, next_state=[ 0.06195726  0.00561345 -0.12338025 -0.3932122 ]\n",
      "[ episode 114 ][ timestamp 23 ] state=[ 0.06195726  0.00561345 -0.12338025 -0.3932122 ], action=0, reward=1.0, next_state=[ 0.06206953 -0.18756143 -0.13124449 -0.14183439]\n",
      "[ episode 114 ][ timestamp 24 ] state=[ 0.06206953 -0.18756143 -0.13124449 -0.14183439], action=0, reward=1.0, next_state=[ 0.05831831 -0.38058337 -0.13408118  0.10673327]\n",
      "[ episode 114 ][ timestamp 25 ] state=[ 0.05831831 -0.38058337 -0.13408118  0.10673327], action=1, reward=1.0, next_state=[ 0.05070664 -0.18382023 -0.13194651 -0.22506425]\n",
      "[ episode 114 ][ timestamp 26 ] state=[ 0.05070664 -0.18382023 -0.13194651 -0.22506425], action=1, reward=1.0, next_state=[ 0.04703023  0.01291647 -0.1364478  -0.55628396]\n",
      "[ episode 114 ][ timestamp 27 ] state=[ 0.04703023  0.01291647 -0.1364478  -0.55628396], action=0, reward=1.0, next_state=[ 0.04728856 -0.18005263 -0.14757348 -0.30951194]\n",
      "[ episode 114 ][ timestamp 28 ] state=[ 0.04728856 -0.18005263 -0.14757348 -0.30951194], action=0, reward=1.0, next_state=[ 0.04368751 -0.37279758 -0.15376372 -0.06676629]\n",
      "[ episode 114 ][ timestamp 29 ] state=[ 0.04368751 -0.37279758 -0.15376372 -0.06676629], action=1, reward=1.0, next_state=[ 0.03623156 -0.17584375 -0.15509904 -0.40374005]\n",
      "[ episode 114 ][ timestamp 30 ] state=[ 0.03623156 -0.17584375 -0.15509904 -0.40374005], action=1, reward=1.0, next_state=[ 0.03271468  0.0210986  -0.16317384 -0.74102402]\n",
      "[ episode 114 ][ timestamp 31 ] state=[ 0.03271468  0.0210986  -0.16317384 -0.74102402], action=1, reward=1.0, next_state=[ 0.03313666  0.21805204 -0.17799433 -1.08029039]\n",
      "[ episode 114 ][ timestamp 32 ] state=[ 0.03313666  0.21805204 -0.17799433 -1.08029039], action=0, reward=1.0, next_state=[ 0.0374977   0.02566892 -0.19960013 -0.8483294 ]\n",
      "[ episode 114 ][ timestamp 33 ] state=[ 0.0374977   0.02566892 -0.19960013 -0.8483294 ], action=0, reward=-1.0, next_state=[ 0.03801107 -0.16625303 -0.21656672 -0.62445569]\n",
      "[ Ended! ] Episode 114: Exploration_rate=0.567555222460375. Score=33.\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ] state=[-0.0224107   0.02202674  0.02470541  0.02561601]\n",
      "[ episode 115 ][ timestamp 1 ] state=[-0.0224107   0.02202674  0.02470541  0.02561601], action=0, reward=1.0, next_state=[-0.02197017 -0.17344063  0.02521773  0.32599025]\n",
      "[ episode 115 ][ timestamp 2 ] state=[-0.02197017 -0.17344063  0.02521773  0.32599025], action=0, reward=1.0, next_state=[-0.02543898 -0.36891238  0.03173753  0.62651788]\n",
      "[ episode 115 ][ timestamp 3 ] state=[-0.02543898 -0.36891238  0.03173753  0.62651788], action=1, reward=1.0, next_state=[-0.03281723 -0.17424748  0.04426789  0.34399684]\n",
      "[ episode 115 ][ timestamp 4 ] state=[-0.03281723 -0.17424748  0.04426789  0.34399684], action=1, reward=1.0, next_state=[-0.03630218  0.02021768  0.05114782  0.06559538]\n",
      "[ episode 115 ][ timestamp 5 ] state=[-0.03630218  0.02021768  0.05114782  0.06559538], action=1, reward=1.0, next_state=[-0.03589782  0.21457042  0.05245973 -0.21052158]\n",
      "[ episode 115 ][ timestamp 6 ] state=[-0.03589782  0.21457042  0.05245973 -0.21052158], action=1, reward=1.0, next_state=[-0.03160642  0.40890457  0.0482493  -0.4862057 ]\n",
      "[ episode 115 ][ timestamp 7 ] state=[-0.03160642  0.40890457  0.0482493  -0.4862057 ], action=0, reward=1.0, next_state=[-0.02342832  0.21313617  0.03852519 -0.17871506]\n",
      "[ episode 115 ][ timestamp 8 ] state=[-0.02342832  0.21313617  0.03852519 -0.17871506], action=1, reward=1.0, next_state=[-0.0191656   0.40768624  0.03495089 -0.45900003]\n",
      "[ episode 115 ][ timestamp 9 ] state=[-0.0191656   0.40768624  0.03495089 -0.45900003], action=0, reward=1.0, next_state=[-0.01101188  0.21208811  0.02577089 -0.15550854]\n",
      "[ episode 115 ][ timestamp 10 ] state=[-0.01101188  0.21208811  0.02577089 -0.15550854], action=0, reward=1.0, next_state=[-0.00677011  0.01660684  0.02266071  0.14519181]\n",
      "[ episode 115 ][ timestamp 11 ] state=[-0.00677011  0.01660684  0.02266071  0.14519181], action=0, reward=1.0, next_state=[-0.00643798 -0.17883218  0.02556455  0.44493675]\n",
      "[ episode 115 ][ timestamp 12 ] state=[-0.00643798 -0.17883218  0.02556455  0.44493675], action=1, reward=1.0, next_state=[-0.01001462  0.01591892  0.03446329  0.16042071]\n",
      "[ episode 115 ][ timestamp 13 ] state=[-0.01001462  0.01591892  0.03446329  0.16042071], action=1, reward=1.0, next_state=[-0.00969624  0.21053097  0.0376717  -0.12119382]\n",
      "[ episode 115 ][ timestamp 14 ] state=[-0.00969624  0.21053097  0.0376717  -0.12119382], action=0, reward=1.0, next_state=[-0.00548562  0.01489011  0.03524782  0.18313212]\n",
      "[ episode 115 ][ timestamp 15 ] state=[-0.00548562  0.01489011  0.03524782  0.18313212], action=0, reward=1.0, next_state=[-0.00518782 -0.180718    0.03891047  0.48672275]\n",
      "[ episode 115 ][ timestamp 16 ] state=[-0.00518782 -0.180718    0.03891047  0.48672275], action=1, reward=1.0, next_state=[-0.00880218  0.01383393  0.04864492  0.20655253]\n",
      "[ episode 115 ][ timestamp 17 ] state=[-0.00880218  0.01383393  0.04864492  0.20655253], action=1, reward=1.0, next_state=[-0.0085255   0.20822773  0.05277597 -0.07039727]\n",
      "[ episode 115 ][ timestamp 18 ] state=[-0.0085255   0.20822773  0.05277597 -0.07039727], action=0, reward=1.0, next_state=[-0.00436095  0.01239043  0.05136803  0.23845861]\n",
      "[ episode 115 ][ timestamp 19 ] state=[-0.00436095  0.01239043  0.05136803  0.23845861], action=0, reward=1.0, next_state=[-0.00411314 -0.1834263   0.0561372   0.54689182]\n",
      "[ episode 115 ][ timestamp 20 ] state=[-0.00411314 -0.1834263   0.0561372   0.54689182], action=1, reward=1.0, next_state=[-0.00778166  0.01086387  0.06707503  0.27241132]\n",
      "[ episode 115 ][ timestamp 21 ] state=[-0.00778166  0.01086387  0.06707503  0.27241132], action=0, reward=1.0, next_state=[-0.00756439 -0.18514787  0.07252326  0.58547306]\n",
      "[ episode 115 ][ timestamp 22 ] state=[-0.00756439 -0.18514787  0.07252326  0.58547306], action=1, reward=1.0, next_state=[-0.01126734  0.00888733  0.08423272  0.3164885 ]\n",
      "[ episode 115 ][ timestamp 23 ] state=[-0.01126734  0.00888733  0.08423272  0.3164885 ], action=0, reward=1.0, next_state=[-0.0110896  -0.18732709  0.09056249  0.63450176]\n",
      "[ episode 115 ][ timestamp 24 ] state=[-0.0110896  -0.18732709  0.09056249  0.63450176], action=1, reward=1.0, next_state=[-0.01483614  0.00642272  0.10325253  0.37165701]\n",
      "[ episode 115 ][ timestamp 25 ] state=[-0.01483614  0.00642272  0.10325253  0.37165701], action=0, reward=1.0, next_state=[-0.01470769 -0.19000295  0.11068567  0.69502866]\n",
      "[ episode 115 ][ timestamp 26 ] state=[-0.01470769 -0.19000295  0.11068567  0.69502866], action=1, reward=1.0, next_state=[-0.01850774  0.00342391  0.12458624  0.43913904]\n",
      "[ episode 115 ][ timestamp 27 ] state=[-0.01850774  0.00342391  0.12458624  0.43913904], action=1, reward=1.0, next_state=[-0.01843927  0.19658268  0.13336902  0.18818027]\n",
      "[ episode 115 ][ timestamp 28 ] state=[-0.01843927  0.19658268  0.13336902  0.18818027], action=0, reward=1.0, next_state=[-1.45076127e-02 -1.70224616e-04  1.37132627e-01  5.19783090e-01]\n",
      "[ episode 115 ][ timestamp 29 ] state=[-1.45076127e-02 -1.70224616e-04  1.37132627e-01  5.19783090e-01], action=0, reward=1.0, next_state=[-0.01451102 -0.19692921  0.14752829  0.85234158]\n",
      "[ episode 115 ][ timestamp 30 ] state=[-0.01451102 -0.19692921  0.14752829  0.85234158], action=1, reward=1.0, next_state=[-0.0184496  -0.00409316  0.16457512  0.60944569]\n",
      "[ episode 115 ][ timestamp 31 ] state=[-0.0184496  -0.00409316  0.16457512  0.60944569], action=1, reward=1.0, next_state=[-0.01853146  0.18839226  0.17676403  0.37278581]\n",
      "[ episode 115 ][ timestamp 32 ] state=[-0.01853146  0.18839226  0.17676403  0.37278581], action=0, reward=1.0, next_state=[-0.01476362 -0.0087424   0.18421975  0.71557855]\n",
      "[ episode 115 ][ timestamp 33 ] state=[-0.01476362 -0.0087424   0.18421975  0.71557855], action=1, reward=1.0, next_state=[-0.01493847  0.18341665  0.19853132  0.4860719 ]\n",
      "[ episode 115 ][ timestamp 34 ] state=[-0.01493847  0.18341665  0.19853132  0.4860719 ], action=0, reward=1.0, next_state=[-0.01127013 -0.0138713   0.20825276  0.83417646]\n",
      "[ episode 115 ][ timestamp 35 ] state=[-0.01127013 -0.0138713   0.20825276  0.83417646], action=0, reward=-1.0, next_state=[-0.01154756 -0.21113694  0.22493629  1.18446635]\n",
      "[ Ended! ] Episode 115: Exploration_rate=0.5647174463480732. Score=35.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 116 ] state=[ 0.02401809 -0.02863897  0.00505457 -0.04299058]\n",
      "[ episode 116 ][ timestamp 1 ] state=[ 0.02401809 -0.02863897  0.00505457 -0.04299058], action=1, reward=1.0, next_state=[ 0.02344531  0.16641014  0.00419476 -0.33407447]\n",
      "[ episode 116 ][ timestamp 2 ] state=[ 0.02344531  0.16641014  0.00419476 -0.33407447], action=1, reward=1.0, next_state=[ 0.02677351  0.36147214 -0.00248673 -0.62543164]\n",
      "[ episode 116 ][ timestamp 3 ] state=[ 0.02677351  0.36147214 -0.00248673 -0.62543164], action=1, reward=1.0, next_state=[ 0.03400296  0.55662872 -0.01499536 -0.91889669]\n",
      "[ episode 116 ][ timestamp 4 ] state=[ 0.03400296  0.55662872 -0.01499536 -0.91889669], action=0, reward=1.0, next_state=[ 0.04513553  0.36171264 -0.0333733  -0.63096393]\n",
      "[ episode 116 ][ timestamp 5 ] state=[ 0.04513553  0.36171264 -0.0333733  -0.63096393], action=1, reward=1.0, next_state=[ 0.05236978  0.55728397 -0.04599257 -0.93396749]\n",
      "[ episode 116 ][ timestamp 6 ] state=[ 0.05236978  0.55728397 -0.04599257 -0.93396749], action=0, reward=1.0, next_state=[ 0.06351546  0.36281164 -0.06467192 -0.65608452]\n",
      "[ episode 116 ][ timestamp 7 ] state=[ 0.06351546  0.36281164 -0.06467192 -0.65608452], action=0, reward=1.0, next_state=[ 0.07077169  0.16864679 -0.07779362 -0.3844464 ]\n",
      "[ episode 116 ][ timestamp 8 ] state=[ 0.07077169  0.16864679 -0.07779362 -0.3844464 ], action=1, reward=1.0, next_state=[ 0.07414463  0.36478204 -0.08548254 -0.70060774]\n",
      "[ episode 116 ][ timestamp 9 ] state=[ 0.07414463  0.36478204 -0.08548254 -0.70060774], action=1, reward=1.0, next_state=[ 0.08144027  0.56097844 -0.0994947  -1.01892903]\n",
      "[ episode 116 ][ timestamp 10 ] state=[ 0.08144027  0.56097844 -0.0994947  -1.01892903], action=0, reward=1.0, next_state=[ 0.09265984  0.36731319 -0.11987328 -0.75907102]\n",
      "[ episode 116 ][ timestamp 11 ] state=[ 0.09265984  0.36731319 -0.11987328 -0.75907102], action=0, reward=1.0, next_state=[ 0.1000061   0.17402901 -0.1350547  -0.50638372]\n",
      "[ episode 116 ][ timestamp 12 ] state=[ 0.1000061   0.17402901 -0.1350547  -0.50638372], action=1, reward=1.0, next_state=[ 0.10348668  0.37076971 -0.14518237 -0.83839296]\n",
      "[ episode 116 ][ timestamp 13 ] state=[ 0.10348668  0.37076971 -0.14518237 -0.83839296], action=1, reward=1.0, next_state=[ 0.11090208  0.56754409 -0.16195023 -1.17298312]\n",
      "[ episode 116 ][ timestamp 14 ] state=[ 0.11090208  0.56754409 -0.16195023 -1.17298312], action=1, reward=1.0, next_state=[ 0.12225296  0.76435739 -0.18540989 -1.51174553]\n",
      "[ episode 116 ][ timestamp 15 ] state=[ 0.12225296  0.76435739 -0.18540989 -1.51174553], action=0, reward=-1.0, next_state=[ 0.13754011  0.57190151 -0.21564481 -1.28220826]\n",
      "[ Ended! ] Episode 116: Exploration_rate=0.5618938591163328. Score=15.\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ] state=[-0.00371274  0.04786056 -0.02399354  0.00857516]\n",
      "[ episode 117 ][ timestamp 1 ] state=[-0.00371274  0.04786056 -0.02399354  0.00857516], action=1, reward=1.0, next_state=[-0.00275553  0.24331825 -0.02382204 -0.29158041]\n",
      "[ episode 117 ][ timestamp 2 ] state=[-0.00275553  0.24331825 -0.02382204 -0.29158041], action=1, reward=1.0, next_state=[ 0.00211083  0.43877162 -0.02965365 -0.5916803 ]\n",
      "[ episode 117 ][ timestamp 3 ] state=[ 0.00211083  0.43877162 -0.02965365 -0.5916803 ], action=0, reward=1.0, next_state=[ 0.01088627  0.24407709 -0.04148726 -0.3084838 ]\n",
      "[ episode 117 ][ timestamp 4 ] state=[ 0.01088627  0.24407709 -0.04148726 -0.3084838 ], action=1, reward=1.0, next_state=[ 0.01576781  0.43976487 -0.04765693 -0.61395664]\n",
      "[ episode 117 ][ timestamp 5 ] state=[ 0.01576781  0.43976487 -0.04765693 -0.61395664], action=1, reward=1.0, next_state=[ 0.02456311  0.63551923 -0.05993606 -0.92126064]\n",
      "[ episode 117 ][ timestamp 6 ] state=[ 0.02456311  0.63551923 -0.05993606 -0.92126064], action=1, reward=1.0, next_state=[ 0.03727349  0.83139767 -0.07836128 -1.23216137]\n",
      "[ episode 117 ][ timestamp 7 ] state=[ 0.03727349  0.83139767 -0.07836128 -1.23216137], action=1, reward=1.0, next_state=[ 0.05390144  1.02743499 -0.1030045  -1.54832963]\n",
      "[ episode 117 ][ timestamp 8 ] state=[ 0.05390144  1.02743499 -0.1030045  -1.54832963], action=0, reward=1.0, next_state=[ 0.07445014  0.83368924 -0.1339711  -1.28948117]\n",
      "[ episode 117 ][ timestamp 9 ] state=[ 0.07445014  0.83368924 -0.1339711  -1.28948117], action=1, reward=1.0, next_state=[ 0.09112393  1.03023637 -0.15976072 -1.62092984]\n",
      "[ episode 117 ][ timestamp 10 ] state=[ 0.09112393  1.03023637 -0.15976072 -1.62092984], action=1, reward=1.0, next_state=[ 0.11172866  1.2268389  -0.19217932 -1.95884825]\n",
      "[ episode 117 ][ timestamp 11 ] state=[ 0.11172866  1.2268389  -0.19217932 -1.95884825], action=1, reward=-1.0, next_state=[ 0.13626543  1.42340991 -0.23135628 -2.30443012]\n",
      "[ Ended! ] Episode 117: Exploration_rate=0.5590843898207511. Score=11.\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ] state=[ 0.00116269 -0.02272633  0.04180216  0.03742394]\n",
      "[ episode 118 ][ timestamp 1 ] state=[ 0.00116269 -0.02272633  0.04180216  0.03742394], action=1, reward=1.0, next_state=[ 0.00070816  0.17177202  0.04255064 -0.24178246]\n",
      "[ episode 118 ][ timestamp 2 ] state=[ 0.00070816  0.17177202  0.04255064 -0.24178246], action=1, reward=1.0, next_state=[ 0.0041436   0.36626114  0.03771499 -0.52074597]\n",
      "[ episode 118 ][ timestamp 3 ] state=[ 0.0041436   0.36626114  0.03771499 -0.52074597], action=0, reward=1.0, next_state=[ 0.01146882  0.17062914  0.02730007 -0.21642107]\n",
      "[ episode 118 ][ timestamp 4 ] state=[ 0.01146882  0.17062914  0.02730007 -0.21642107], action=1, reward=1.0, next_state=[ 0.01488141  0.3653504   0.02297165 -0.5003689 ]\n",
      "[ episode 118 ][ timestamp 5 ] state=[ 0.01488141  0.3653504   0.02297165 -0.5003689 ], action=1, reward=1.0, next_state=[ 0.02218841  0.56014111  0.01296427 -0.7857248 ]\n",
      "[ episode 118 ][ timestamp 6 ] state=[ 0.02218841  0.56014111  0.01296427 -0.7857248 ], action=0, reward=1.0, next_state=[ 0.03339124  0.36484346 -0.00275023 -0.48899156]\n",
      "[ episode 118 ][ timestamp 7 ] state=[ 0.03339124  0.36484346 -0.00275023 -0.48899156], action=0, reward=1.0, next_state=[ 0.0406881   0.16976042 -0.01253006 -0.19717667]\n",
      "[ episode 118 ][ timestamp 8 ] state=[ 0.0406881   0.16976042 -0.01253006 -0.19717667], action=0, reward=1.0, next_state=[ 0.04408331 -0.02518009 -0.01647359  0.09152739]\n",
      "[ episode 118 ][ timestamp 9 ] state=[ 0.04408331 -0.02518009 -0.01647359  0.09152739], action=1, reward=1.0, next_state=[ 0.04357971  0.17017406 -0.01464304 -0.20630709]\n",
      "[ episode 118 ][ timestamp 10 ] state=[ 0.04357971  0.17017406 -0.01464304 -0.20630709], action=1, reward=1.0, next_state=[ 0.04698319  0.36550231 -0.01876919 -0.50357296]\n",
      "[ episode 118 ][ timestamp 11 ] state=[ 0.04698319  0.36550231 -0.01876919 -0.50357296], action=1, reward=1.0, next_state=[ 0.05429324  0.5608837  -0.02884065 -0.80211123]\n",
      "[ episode 118 ][ timestamp 12 ] state=[ 0.05429324  0.5608837  -0.02884065 -0.80211123], action=0, reward=1.0, next_state=[ 0.06551091  0.36616888 -0.04488287 -0.51863844]\n",
      "[ episode 118 ][ timestamp 13 ] state=[ 0.06551091  0.36616888 -0.04488287 -0.51863844], action=1, reward=1.0, next_state=[ 0.07283429  0.56189305 -0.05525564 -0.82512017]\n",
      "[ episode 118 ][ timestamp 14 ] state=[ 0.07283429  0.56189305 -0.05525564 -0.82512017], action=0, reward=1.0, next_state=[ 0.08407215  0.36756862 -0.07175804 -0.55031528]\n",
      "[ episode 118 ][ timestamp 15 ] state=[ 0.08407215  0.36756862 -0.07175804 -0.55031528], action=1, reward=1.0, next_state=[ 0.09142352  0.56362124 -0.08276435 -0.86471616]\n",
      "[ episode 118 ][ timestamp 16 ] state=[ 0.09142352  0.56362124 -0.08276435 -0.86471616], action=1, reward=1.0, next_state=[ 0.10269595  0.75976634 -0.10005867 -1.18223165]\n",
      "[ episode 118 ][ timestamp 17 ] state=[ 0.10269595  0.75976634 -0.10005867 -1.18223165], action=0, reward=1.0, next_state=[ 0.11789128  0.56607503 -0.1237033  -0.92251605]\n",
      "[ episode 118 ][ timestamp 18 ] state=[ 0.11789128  0.56607503 -0.1237033  -0.92251605], action=1, reward=1.0, next_state=[ 0.12921278  0.76263161 -0.14215363 -1.25137403]\n",
      "[ episode 118 ][ timestamp 19 ] state=[ 0.12921278  0.76263161 -0.14215363 -1.25137403], action=0, reward=1.0, next_state=[ 0.14446541  0.56958807 -0.16718111 -1.00638206]\n",
      "[ episode 118 ][ timestamp 20 ] state=[ 0.14446541  0.56958807 -0.16718111 -1.00638206], action=0, reward=1.0, next_state=[ 0.15585717  0.37704501 -0.18730875 -0.77051679]\n",
      "[ episode 118 ][ timestamp 21 ] state=[ 0.15585717  0.37704501 -0.18730875 -0.77051679], action=0, reward=1.0, next_state=[ 0.16339807  0.18492654 -0.20271908 -0.54212696]\n",
      "[ episode 118 ][ timestamp 22 ] state=[ 0.16339807  0.18492654 -0.20271908 -0.54212696], action=1, reward=-1.0, next_state=[ 0.1670966   0.38223385 -0.21356162 -0.89121949]\n",
      "[ Ended! ] Episode 118: Exploration_rate=0.5562889678716474. Score=22.\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ] state=[ 0.04896425 -0.01667025 -0.02636561 -0.02444538]\n",
      "[ episode 119 ][ timestamp 1 ] state=[ 0.04896425 -0.01667025 -0.02636561 -0.02444538], action=0, reward=1.0, next_state=[ 0.04863085 -0.21140437 -0.02685452  0.25980369]\n",
      "[ episode 119 ][ timestamp 2 ] state=[ 0.04863085 -0.21140437 -0.02685452  0.25980369], action=1, reward=1.0, next_state=[ 0.04440276 -0.01590955 -0.02165844 -0.04122709]\n",
      "[ episode 119 ][ timestamp 3 ] state=[ 0.04440276 -0.01590955 -0.02165844 -0.04122709], action=0, reward=1.0, next_state=[ 0.04408457 -0.21071433 -0.02248299  0.24454447]\n",
      "[ episode 119 ][ timestamp 4 ] state=[ 0.04408457 -0.21071433 -0.02248299  0.24454447], action=1, reward=1.0, next_state=[ 0.03987028 -0.01527859 -0.0175921  -0.05514449]\n",
      "[ episode 119 ][ timestamp 5 ] state=[ 0.03987028 -0.01527859 -0.0175921  -0.05514449], action=0, reward=1.0, next_state=[ 0.03956471 -0.21014394 -0.01869499  0.23193649]\n",
      "[ episode 119 ][ timestamp 6 ] state=[ 0.03956471 -0.21014394 -0.01869499  0.23193649], action=1, reward=1.0, next_state=[ 0.03536183 -0.01475991 -0.01405626 -0.06658434]\n",
      "[ episode 119 ][ timestamp 7 ] state=[ 0.03536183 -0.01475991 -0.01405626 -0.06658434], action=0, reward=1.0, next_state=[ 0.03506663 -0.20967755 -0.01538794  0.22163082]\n",
      "[ episode 119 ][ timestamp 8 ] state=[ 0.03506663 -0.20967755 -0.01538794  0.22163082], action=1, reward=1.0, next_state=[ 0.03087308 -0.01433907 -0.01095533 -0.07586608]\n",
      "[ episode 119 ][ timestamp 9 ] state=[ 0.03087308 -0.01433907 -0.01095533 -0.07586608], action=1, reward=1.0, next_state=[ 0.0305863   0.18093821 -0.01247265 -0.37198522]\n",
      "[ episode 119 ][ timestamp 10 ] state=[ 0.0305863   0.18093821 -0.01247265 -0.37198522], action=0, reward=1.0, next_state=[ 0.03420506 -0.01400435 -0.01991235 -0.08326099]\n",
      "[ episode 119 ][ timestamp 11 ] state=[ 0.03420506 -0.01400435 -0.01991235 -0.08326099], action=1, reward=1.0, next_state=[ 0.03392498  0.1813973  -0.02157757 -0.3821592 ]\n",
      "[ episode 119 ][ timestamp 12 ] state=[ 0.03392498  0.1813973  -0.02157757 -0.3821592 ], action=0, reward=1.0, next_state=[ 0.03755292 -0.01341173 -0.02922076 -0.09635699]\n",
      "[ episode 119 ][ timestamp 13 ] state=[ 0.03755292 -0.01341173 -0.02922076 -0.09635699], action=1, reward=1.0, next_state=[ 0.03728469  0.18211657 -0.0311479  -0.39811392]\n",
      "[ episode 119 ][ timestamp 14 ] state=[ 0.03728469  0.18211657 -0.0311479  -0.39811392], action=0, reward=1.0, next_state=[ 0.04092702 -0.01254997 -0.03911017 -0.11541176]\n",
      "[ episode 119 ][ timestamp 15 ] state=[ 0.04092702 -0.01254997 -0.03911017 -0.11541176], action=1, reward=1.0, next_state=[ 0.04067602  0.18310992 -0.04141841 -0.42017261]\n",
      "[ episode 119 ][ timestamp 16 ] state=[ 0.04067602  0.18310992 -0.04141841 -0.42017261], action=0, reward=1.0, next_state=[ 0.04433822 -0.01140144 -0.04982186 -0.14082932]\n",
      "[ episode 119 ][ timestamp 17 ] state=[ 0.04433822 -0.01140144 -0.04982186 -0.14082932], action=1, reward=1.0, next_state=[ 0.04411019  0.18439735 -0.05263845 -0.44880464]\n",
      "[ episode 119 ][ timestamp 18 ] state=[ 0.04411019  0.18439735 -0.05263845 -0.44880464], action=0, reward=1.0, next_state=[ 0.04779814 -0.00994206 -0.06161454 -0.17316785]\n",
      "[ episode 119 ][ timestamp 19 ] state=[ 0.04779814 -0.00994206 -0.06161454 -0.17316785], action=0, reward=1.0, next_state=[ 0.04759929 -0.20413053 -0.0650779   0.09945891]\n",
      "[ episode 119 ][ timestamp 20 ] state=[ 0.04759929 -0.20413053 -0.0650779   0.09945891], action=0, reward=1.0, next_state=[ 0.04351668 -0.39826236 -0.06308872  0.37092083]\n",
      "[ episode 119 ][ timestamp 21 ] state=[ 0.04351668 -0.39826236 -0.06308872  0.37092083], action=1, reward=1.0, next_state=[ 0.03555144 -0.20230348 -0.0556703   0.0590315 ]\n",
      "[ episode 119 ][ timestamp 22 ] state=[ 0.03555144 -0.20230348 -0.0556703   0.0590315 ], action=0, reward=1.0, next_state=[ 0.03150537 -0.39658486 -0.05448967  0.33364349]\n",
      "[ episode 119 ][ timestamp 23 ] state=[ 0.03150537 -0.39658486 -0.05448967  0.33364349], action=1, reward=1.0, next_state=[ 0.02357367 -0.20073142 -0.0478168   0.02428731]\n",
      "[ episode 119 ][ timestamp 24 ] state=[ 0.02357367 -0.20073142 -0.0478168   0.02428731], action=1, reward=1.0, next_state=[ 0.01955904 -0.0049575  -0.04733106 -0.28309069]\n",
      "[ episode 119 ][ timestamp 25 ] state=[ 0.01955904 -0.0049575  -0.04733106 -0.28309069], action=0, reward=1.0, next_state=[ 0.01945989 -0.19937352 -0.05299287 -0.00570338]\n",
      "[ episode 119 ][ timestamp 26 ] state=[ 0.01945989 -0.19937352 -0.05299287 -0.00570338], action=0, reward=1.0, next_state=[ 0.01547242 -0.39369701 -0.05310694  0.26980005]\n",
      "[ episode 119 ][ timestamp 27 ] state=[ 0.01547242 -0.39369701 -0.05310694  0.26980005], action=1, reward=1.0, next_state=[ 0.00759848 -0.19785901 -0.04771094 -0.0391489 ]\n",
      "[ episode 119 ][ timestamp 28 ] state=[ 0.00759848 -0.19785901 -0.04771094 -0.0391489 ], action=1, reward=1.0, next_state=[ 0.0036413  -0.0020865  -0.04849392 -0.34649519]\n",
      "[ episode 119 ][ timestamp 29 ] state=[ 0.0036413  -0.0020865  -0.04849392 -0.34649519], action=1, reward=1.0, next_state=[ 0.00359957  0.19369049 -0.05542382 -0.65406707]\n",
      "[ episode 119 ][ timestamp 30 ] state=[ 0.00359957  0.19369049 -0.05542382 -0.65406707], action=0, reward=1.0, next_state=[ 0.00747338 -0.00061772 -0.06850516 -0.37933856]\n",
      "[ episode 119 ][ timestamp 31 ] state=[ 0.00747338 -0.00061772 -0.06850516 -0.37933856], action=0, reward=1.0, next_state=[ 0.00746103 -0.19470334 -0.07609193 -0.10901775]\n",
      "[ episode 119 ][ timestamp 32 ] state=[ 0.00746103 -0.19470334 -0.07609193 -0.10901775], action=0, reward=1.0, next_state=[ 0.00356696 -0.3886571  -0.07827229  0.15872161]\n",
      "[ episode 119 ][ timestamp 33 ] state=[ 0.00356696 -0.3886571  -0.07827229  0.15872161], action=0, reward=1.0, next_state=[-0.00420618 -0.58257626 -0.07509786  0.4257212 ]\n",
      "[ episode 119 ][ timestamp 34 ] state=[-0.00420618 -0.58257626 -0.07509786  0.4257212 ], action=0, reward=1.0, next_state=[-0.01585771 -0.77655861 -0.06658343  0.69381659]\n",
      "[ episode 119 ][ timestamp 35 ] state=[-0.01585771 -0.77655861 -0.06658343  0.69381659], action=1, reward=1.0, next_state=[-0.03138888 -0.5805793  -0.0527071   0.38093795]\n",
      "[ episode 119 ][ timestamp 36 ] state=[-0.03138888 -0.5805793  -0.0527071   0.38093795], action=1, reward=1.0, next_state=[-0.04300047 -0.38475008 -0.04508834  0.07211333]\n",
      "[ episode 119 ][ timestamp 37 ] state=[-0.04300047 -0.38475008 -0.04508834  0.07211333], action=1, reward=1.0, next_state=[-0.05069547 -0.1890117  -0.04364607 -0.23444733]\n",
      "[ episode 119 ][ timestamp 38 ] state=[-0.05069547 -0.1890117  -0.04364607 -0.23444733], action=1, reward=1.0, next_state=[-0.0544757   0.0067058  -0.04833502 -0.54057187]\n",
      "[ episode 119 ][ timestamp 39 ] state=[-0.0544757   0.0067058  -0.04833502 -0.54057187], action=0, reward=1.0, next_state=[-0.05434159 -0.1877046  -0.05914646 -0.26350181]\n",
      "[ episode 119 ][ timestamp 40 ] state=[-0.05434159 -0.1877046  -0.05914646 -0.26350181], action=1, reward=1.0, next_state=[-0.05809568  0.00820953 -0.06441649 -0.57423805]\n",
      "[ episode 119 ][ timestamp 41 ] state=[-0.05809568  0.00820953 -0.06441649 -0.57423805], action=0, reward=1.0, next_state=[-0.05793149 -0.18595299 -0.07590125 -0.30252367]\n",
      "[ episode 119 ][ timestamp 42 ] state=[-0.05793149 -0.18595299 -0.07590125 -0.30252367], action=0, reward=1.0, next_state=[-0.06165055 -0.37991575 -0.08195173 -0.03471075]\n",
      "[ episode 119 ][ timestamp 43 ] state=[-0.06165055 -0.37991575 -0.08195173 -0.03471075], action=1, reward=1.0, next_state=[-0.06924886 -0.18372004 -0.08264594 -0.35208346]\n",
      "[ episode 119 ][ timestamp 44 ] state=[-0.06924886 -0.18372004 -0.08264594 -0.35208346], action=1, reward=1.0, next_state=[-0.07292326  0.01247397 -0.08968761 -0.66964025]\n",
      "[ episode 119 ][ timestamp 45 ] state=[-0.07292326  0.01247397 -0.08968761 -0.66964025], action=0, reward=1.0, next_state=[-0.07267378 -0.18129406 -0.10308042 -0.40648923]\n",
      "[ episode 119 ][ timestamp 46 ] state=[-0.07267378 -0.18129406 -0.10308042 -0.40648923], action=0, reward=1.0, next_state=[-0.07629966 -0.3748148  -0.1112102  -0.14800096]\n",
      "[ episode 119 ][ timestamp 47 ] state=[-0.07629966 -0.3748148  -0.1112102  -0.14800096], action=0, reward=1.0, next_state=[-0.08379596 -0.56818305 -0.11417022  0.10763118]\n",
      "[ episode 119 ][ timestamp 48 ] state=[-0.08379596 -0.56818305 -0.11417022  0.10763118], action=1, reward=1.0, next_state=[-0.09515962 -0.37162587 -0.1120176  -0.21877828]\n",
      "[ episode 119 ][ timestamp 49 ] state=[-0.09515962 -0.37162587 -0.1120176  -0.21877828], action=1, reward=1.0, next_state=[-0.10259214 -0.17509573 -0.11639316 -0.54459022]\n",
      "[ episode 119 ][ timestamp 50 ] state=[-0.10259214 -0.17509573 -0.11639316 -0.54459022], action=0, reward=1.0, next_state=[-0.10609405 -0.36840631 -0.12728497 -0.29072865]\n",
      "[ episode 119 ][ timestamp 51 ] state=[-0.10609405 -0.36840631 -0.12728497 -0.29072865], action=0, reward=1.0, next_state=[-0.11346218 -0.56150525 -0.13309954 -0.04074426]\n",
      "[ episode 119 ][ timestamp 52 ] state=[-0.11346218 -0.56150525 -0.13309954 -0.04074426], action=1, reward=1.0, next_state=[-0.12469228 -0.36475068 -0.13391443 -0.3722816 ]\n",
      "[ episode 119 ][ timestamp 53 ] state=[-0.12469228 -0.36475068 -0.13391443 -0.3722816 ], action=0, reward=1.0, next_state=[-0.1319873  -0.55774131 -0.14136006 -0.12464074]\n",
      "[ episode 119 ][ timestamp 54 ] state=[-0.1319873  -0.55774131 -0.14136006 -0.12464074], action=1, reward=1.0, next_state=[-0.14314212 -0.36090697 -0.14385287 -0.45836878]\n",
      "[ episode 119 ][ timestamp 55 ] state=[-0.14314212 -0.36090697 -0.14385287 -0.45836878], action=0, reward=1.0, next_state=[-0.15036026 -0.55373363 -0.15302025 -0.21426339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 119 ][ timestamp 56 ] state=[-0.15036026 -0.55373363 -0.15302025 -0.21426339], action=1, reward=1.0, next_state=[-0.16143494 -0.35679286 -0.15730552 -0.55103532]\n",
      "[ episode 119 ][ timestamp 57 ] state=[-0.16143494 -0.35679286 -0.15730552 -0.55103532], action=0, reward=1.0, next_state=[-0.16857079 -0.54939639 -0.16832622 -0.31175446]\n",
      "[ episode 119 ][ timestamp 58 ] state=[-0.16857079 -0.54939639 -0.16832622 -0.31175446], action=0, reward=1.0, next_state=[-0.17955872 -0.74177045 -0.17456131 -0.07652628]\n",
      "[ episode 119 ][ timestamp 59 ] state=[-0.17955872 -0.74177045 -0.17456131 -0.07652628], action=1, reward=1.0, next_state=[-0.19439413 -0.54463168 -0.17609184 -0.4188013 ]\n",
      "[ episode 119 ][ timestamp 60 ] state=[-0.19439413 -0.54463168 -0.17609184 -0.4188013 ], action=1, reward=1.0, next_state=[-0.20528676 -0.34750882 -0.18446786 -0.76141694]\n",
      "[ episode 119 ][ timestamp 61 ] state=[-0.20528676 -0.34750882 -0.18446786 -0.76141694], action=1, reward=1.0, next_state=[-0.21223694 -0.15038994 -0.1996962  -1.10600526]\n",
      "[ episode 119 ][ timestamp 62 ] state=[-0.21223694 -0.15038994 -0.1996962  -1.10600526], action=0, reward=-1.0, next_state=[-0.21524474 -0.34240735 -0.22181631 -0.88202438]\n",
      "[ Ended! ] Episode 119: Exploration_rate=0.5535075230322891. Score=62.\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ] state=[-0.0305309   0.00329186  0.04484428 -0.04593805]\n",
      "[ episode 120 ][ timestamp 1 ] state=[-0.0305309   0.00329186  0.04484428 -0.04593805], action=0, reward=1.0, next_state=[-0.03046507 -0.19244348  0.04392552  0.2605496 ]\n",
      "[ episode 120 ][ timestamp 2 ] state=[-0.03046507 -0.19244348  0.04392552  0.2605496 ], action=1, reward=1.0, next_state=[-0.03431394  0.00202479  0.04913651 -0.01796149]\n",
      "[ episode 120 ][ timestamp 3 ] state=[-0.03431394  0.00202479  0.04913651 -0.01796149], action=1, reward=1.0, next_state=[-0.03427344  0.19640889  0.04877728 -0.2947454 ]\n",
      "[ episode 120 ][ timestamp 4 ] state=[-0.03427344  0.19640889  0.04877728 -0.2947454 ], action=0, reward=1.0, next_state=[-0.03034526  0.0006267   0.04288237  0.01291343]\n",
      "[ episode 120 ][ timestamp 5 ] state=[-0.03034526  0.0006267   0.04288237  0.01291343], action=0, reward=1.0, next_state=[-0.03033273 -0.19508317  0.04314064  0.31881191]\n",
      "[ episode 120 ][ timestamp 6 ] state=[-0.03033273 -0.19508317  0.04314064  0.31881191], action=0, reward=1.0, next_state=[-0.03423439 -0.39079214  0.04951688  0.62478165]\n",
      "[ episode 120 ][ timestamp 7 ] state=[-0.03423439 -0.39079214  0.04951688  0.62478165], action=1, reward=1.0, next_state=[-0.04205024 -0.19639517  0.06201251  0.34809562]\n",
      "[ episode 120 ][ timestamp 8 ] state=[-0.04205024 -0.19639517  0.06201251  0.34809562], action=1, reward=1.0, next_state=[-0.04597814 -0.00220754  0.06897442  0.07559406]\n",
      "[ episode 120 ][ timestamp 9 ] state=[-0.04597814 -0.00220754  0.06897442  0.07559406], action=0, reward=1.0, next_state=[-0.04602229 -0.19824703  0.07048631  0.38921649]\n",
      "[ episode 120 ][ timestamp 10 ] state=[-0.04602229 -0.19824703  0.07048631  0.38921649], action=1, reward=1.0, next_state=[-0.04998723 -0.00419263  0.07827064  0.1195635 ]\n",
      "[ episode 120 ][ timestamp 11 ] state=[-0.04998723 -0.00419263  0.07827064  0.1195635 ], action=0, reward=1.0, next_state=[-0.05007108 -0.20034368  0.08066191  0.43587736]\n",
      "[ episode 120 ][ timestamp 12 ] state=[-0.05007108 -0.20034368  0.08066191  0.43587736], action=0, reward=1.0, next_state=[-0.05407796 -0.39650931  0.08937945  0.75285797]\n",
      "[ episode 120 ][ timestamp 13 ] state=[-0.05407796 -0.39650931  0.08937945  0.75285797], action=1, reward=1.0, next_state=[-0.06200814 -0.20272599  0.10443661  0.48958586]\n",
      "[ episode 120 ][ timestamp 14 ] state=[-0.06200814 -0.20272599  0.10443661  0.48958586], action=1, reward=1.0, next_state=[-0.06606266 -0.00922043  0.11422833  0.23155759]\n",
      "[ episode 120 ][ timestamp 15 ] state=[-0.06606266 -0.00922043  0.11422833  0.23155759], action=0, reward=1.0, next_state=[-0.06624707 -0.20577357  0.11885948  0.55797604]\n",
      "[ episode 120 ][ timestamp 16 ] state=[-0.06624707 -0.20577357  0.11885948  0.55797604], action=1, reward=1.0, next_state=[-0.07036254 -0.01250306  0.130019    0.30497817]\n",
      "[ episode 120 ][ timestamp 17 ] state=[-0.07036254 -0.01250306  0.130019    0.30497817], action=0, reward=1.0, next_state=[-0.0706126  -0.20921501  0.13611857  0.63567353]\n",
      "[ episode 120 ][ timestamp 18 ] state=[-0.0706126  -0.20921501  0.13611857  0.63567353], action=1, reward=1.0, next_state=[-0.0747969  -0.0162277   0.14883204  0.38876562]\n",
      "[ episode 120 ][ timestamp 19 ] state=[-0.0747969  -0.0162277   0.14883204  0.38876562], action=1, reward=1.0, next_state=[-0.07512146  0.17650294  0.15660735  0.14646087]\n",
      "[ episode 120 ][ timestamp 20 ] state=[-0.07512146  0.17650294  0.15660735  0.14646087], action=1, reward=1.0, next_state=[-0.0715914   0.36907595  0.15953657 -0.09300903]\n",
      "[ episode 120 ][ timestamp 21 ] state=[-0.0715914   0.36907595  0.15953657 -0.09300903], action=0, reward=1.0, next_state=[-0.06420988  0.17206977  0.15767639  0.24545262]\n",
      "[ episode 120 ][ timestamp 22 ] state=[-0.06420988  0.17206977  0.15767639  0.24545262], action=0, reward=1.0, next_state=[-0.06076848 -0.02491159  0.16258544  0.58342428]\n",
      "[ episode 120 ][ timestamp 23 ] state=[-0.06076848 -0.02491159  0.16258544  0.58342428], action=1, reward=1.0, next_state=[-0.06126672  0.16760419  0.17425392  0.34604875]\n",
      "[ episode 120 ][ timestamp 24 ] state=[-0.06126672  0.16760419  0.17425392  0.34604875], action=0, reward=1.0, next_state=[-0.05791463 -0.02951275  0.1811749   0.68821827]\n",
      "[ episode 120 ][ timestamp 25 ] state=[-0.05791463 -0.02951275  0.1811749   0.68821827], action=1, reward=1.0, next_state=[-0.05850489  0.16269403  0.19493926  0.45760147]\n",
      "[ episode 120 ][ timestamp 26 ] state=[-0.05850489  0.16269403  0.19493926  0.45760147], action=0, reward=1.0, next_state=[-0.05525101 -0.03457217  0.20409129  0.80484611]\n",
      "[ episode 120 ][ timestamp 27 ] state=[-0.05525101 -0.03457217  0.20409129  0.80484611], action=1, reward=-1.0, next_state=[-0.05594245  0.15725493  0.22018821  0.58266451]\n",
      "[ Ended! ] Episode 120: Exploration_rate=0.5507399854171277. Score=27.\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ] state=[-0.04014437 -0.02598301 -0.02450625  0.04399546]\n",
      "[ episode 121 ][ timestamp 1 ] state=[-0.04014437 -0.02598301 -0.02450625  0.04399546], action=1, reward=1.0, next_state=[-0.04066403  0.16948162 -0.02362634 -0.25631757]\n",
      "[ episode 121 ][ timestamp 2 ] state=[-0.04066403  0.16948162 -0.02362634 -0.25631757], action=1, reward=1.0, next_state=[-0.0372744   0.36493279 -0.02875269 -0.556358  ]\n",
      "[ episode 121 ][ timestamp 3 ] state=[-0.0372744   0.36493279 -0.02875269 -0.556358  ], action=0, reward=1.0, next_state=[-0.02997574  0.17022606 -0.03987985 -0.27287075]\n",
      "[ episode 121 ][ timestamp 4 ] state=[-0.02997574  0.17022606 -0.03987985 -0.27287075], action=1, reward=1.0, next_state=[-0.02657122  0.36589368 -0.04533727 -0.57786039]\n",
      "[ episode 121 ][ timestamp 5 ] state=[-0.02657122  0.36589368 -0.04533727 -0.57786039], action=0, reward=1.0, next_state=[-0.01925335  0.17143551 -0.05689448 -0.29979744]\n",
      "[ episode 121 ][ timestamp 6 ] state=[-0.01925335  0.17143551 -0.05689448 -0.29979744], action=0, reward=1.0, next_state=[-0.01582464 -0.02283128 -0.06289042 -0.02558672]\n",
      "[ episode 121 ][ timestamp 7 ] state=[-0.01582464 -0.02283128 -0.06289042 -0.02558672], action=1, reward=1.0, next_state=[-0.01628126  0.17313356 -0.06340216 -0.33743046]\n",
      "[ episode 121 ][ timestamp 8 ] state=[-0.01628126  0.17313356 -0.06340216 -0.33743046], action=0, reward=1.0, next_state=[-0.01281859 -0.02103155 -0.07015077 -0.06539573]\n",
      "[ episode 121 ][ timestamp 9 ] state=[-0.01281859 -0.02103155 -0.07015077 -0.06539573], action=1, reward=1.0, next_state=[-0.01323922  0.17502239 -0.07145868 -0.37936075]\n",
      "[ episode 121 ][ timestamp 10 ] state=[-0.01323922  0.17502239 -0.07145868 -0.37936075], action=0, reward=1.0, next_state=[-0.00973878 -0.01901587 -0.0790459  -0.11003714]\n",
      "[ episode 121 ][ timestamp 11 ] state=[-0.00973878 -0.01901587 -0.0790459  -0.11003714], action=0, reward=1.0, next_state=[-0.01011909 -0.21292137 -0.08124664  0.1566976 ]\n",
      "[ episode 121 ][ timestamp 12 ] state=[-0.01011909 -0.21292137 -0.08124664  0.1566976 ], action=1, reward=1.0, next_state=[-0.01437752 -0.01673584 -0.07811269 -0.1604702 ]\n",
      "[ episode 121 ][ timestamp 13 ] state=[-0.01437752 -0.01673584 -0.07811269 -0.1604702 ], action=1, reward=1.0, next_state=[-0.01471224  0.17941246 -0.08132209 -0.47673727]\n",
      "[ episode 121 ][ timestamp 14 ] state=[-0.01471224  0.17941246 -0.08132209 -0.47673727], action=0, reward=1.0, next_state=[-0.01112399 -0.01447275 -0.09085684 -0.21075295]\n",
      "[ episode 121 ][ timestamp 15 ] state=[-0.01112399 -0.01447275 -0.09085684 -0.21075295], action=0, reward=1.0, next_state=[-0.01141344 -0.20818608 -0.0950719   0.05194338]\n",
      "[ episode 121 ][ timestamp 16 ] state=[-0.01141344 -0.20818608 -0.0950719   0.05194338], action=0, reward=1.0, next_state=[-0.01557716 -0.40182533 -0.09403303  0.31318152]\n",
      "[ episode 121 ][ timestamp 17 ] state=[-0.01557716 -0.40182533 -0.09403303  0.31318152], action=0, reward=1.0, next_state=[-0.02361367 -0.5954907  -0.0877694   0.57479121]\n",
      "[ episode 121 ][ timestamp 18 ] state=[-0.02361367 -0.5954907  -0.0877694   0.57479121], action=1, reward=1.0, next_state=[-0.03552348 -0.39925505 -0.07627358  0.25579968]\n",
      "[ episode 121 ][ timestamp 19 ] state=[-0.03552348 -0.39925505 -0.07627358  0.25579968], action=1, reward=1.0, next_state=[-0.04350859 -0.20313165 -0.07115758 -0.05993279]\n",
      "[ episode 121 ][ timestamp 20 ] state=[-0.04350859 -0.20313165 -0.07115758 -0.05993279], action=0, reward=1.0, next_state=[-0.04757122 -0.39716502 -0.07235624  0.20947804]\n",
      "[ episode 121 ][ timestamp 21 ] state=[-0.04757122 -0.39716502 -0.07235624  0.20947804], action=1, reward=1.0, next_state=[-0.05551452 -0.20108707 -0.06816668 -0.10512348]\n",
      "[ episode 121 ][ timestamp 22 ] state=[-0.05551452 -0.20108707 -0.06816668 -0.10512348], action=0, reward=1.0, next_state=[-0.05953626 -0.39516928 -0.07026915  0.16529824]\n",
      "[ episode 121 ][ timestamp 23 ] state=[-0.05953626 -0.39516928 -0.07026915  0.16529824], action=0, reward=1.0, next_state=[-0.06743965 -0.58921866 -0.06696318  0.43501185]\n",
      "[ episode 121 ][ timestamp 24 ] state=[-0.06743965 -0.58921866 -0.06696318  0.43501185], action=1, reward=1.0, next_state=[-0.07922402 -0.3932158  -0.05826294  0.121994  ]\n",
      "[ episode 121 ][ timestamp 25 ] state=[-0.07922402 -0.3932158  -0.05826294  0.121994  ], action=1, reward=1.0, next_state=[-0.08708834 -0.19730961 -0.05582306 -0.18848627]\n",
      "[ episode 121 ][ timestamp 26 ] state=[-0.08708834 -0.19730961 -0.05582306 -0.18848627], action=0, reward=1.0, next_state=[-0.09103453 -0.39159032 -0.05959279  0.08607739]\n",
      "[ episode 121 ][ timestamp 27 ] state=[-0.09103453 -0.39159032 -0.05959279  0.08607739], action=0, reward=1.0, next_state=[-0.09886633 -0.58580967 -0.05787124  0.35937935]\n",
      "[ episode 121 ][ timestamp 28 ] state=[-0.09886633 -0.58580967 -0.05787124  0.35937935], action=1, reward=1.0, next_state=[-0.11058253 -0.38991485 -0.05068366  0.04902438]\n",
      "[ episode 121 ][ timestamp 29 ] state=[-0.11058253 -0.38991485 -0.05068366  0.04902438], action=0, reward=1.0, next_state=[-0.11838082 -0.58427478 -0.04970317  0.32529529]\n",
      "[ episode 121 ][ timestamp 30 ] state=[-0.11838082 -0.58427478 -0.04970317  0.32529529], action=1, reward=1.0, next_state=[-0.13006632 -0.38848168 -0.04319726  0.01736162]\n",
      "[ episode 121 ][ timestamp 31 ] state=[-0.13006632 -0.38848168 -0.04319726  0.01736162], action=0, reward=1.0, next_state=[-0.13783595 -0.58295837 -0.04285003  0.29610847]\n",
      "[ episode 121 ][ timestamp 32 ] state=[-0.13783595 -0.58295837 -0.04285003  0.29610847], action=1, reward=1.0, next_state=[-0.14949512 -0.38725258 -0.03692786 -0.0097748 ]\n",
      "[ episode 121 ][ timestamp 33 ] state=[-0.14949512 -0.38725258 -0.03692786 -0.0097748 ], action=1, reward=1.0, next_state=[-0.15724017 -0.19162102 -0.03712336 -0.3138764 ]\n",
      "[ episode 121 ][ timestamp 34 ] state=[-0.15724017 -0.19162102 -0.03712336 -0.3138764 ], action=1, reward=1.0, next_state=[-0.16107259  0.00400956 -0.04340088 -0.61803185]\n",
      "[ episode 121 ][ timestamp 35 ] state=[-0.16107259  0.00400956 -0.04340088 -0.61803185], action=1, reward=1.0, next_state=[-0.1609924   0.19971004 -0.05576152 -0.92406199]\n",
      "[ episode 121 ][ timestamp 36 ] state=[-0.1609924   0.19971004 -0.05576152 -0.92406199], action=1, reward=1.0, next_state=[-0.1569982   0.3955391  -0.07424276 -1.23373442]\n",
      "[ episode 121 ][ timestamp 37 ] state=[-0.1569982   0.3955391  -0.07424276 -1.23373442], action=0, reward=1.0, next_state=[-0.14908742  0.20144598 -0.09891745 -0.96520407]\n",
      "[ episode 121 ][ timestamp 38 ] state=[-0.14908742  0.20144598 -0.09891745 -0.96520407], action=0, reward=1.0, next_state=[-0.1450585   0.00778185 -0.11822153 -0.70516226]\n",
      "[ episode 121 ][ timestamp 39 ] state=[-0.1450585   0.00778185 -0.11822153 -0.70516226], action=0, reward=1.0, next_state=[-0.14490286 -0.18552084 -0.13232478 -0.45190834]\n",
      "[ episode 121 ][ timestamp 40 ] state=[-0.14490286 -0.18552084 -0.13232478 -0.45190834], action=1, reward=1.0, next_state=[-0.14861328  0.01119991 -0.14136294 -0.78319986]\n",
      "[ episode 121 ][ timestamp 41 ] state=[-0.14861328  0.01119991 -0.14136294 -0.78319986], action=1, reward=1.0, next_state=[-0.14838928  0.20795221 -0.15702694 -1.1168068 ]\n",
      "[ episode 121 ][ timestamp 42 ] state=[-0.14838928  0.20795221 -0.15702694 -1.1168068 ], action=1, reward=1.0, next_state=[-0.14423024  0.40474688 -0.17936308 -1.45434337]\n",
      "[ episode 121 ][ timestamp 43 ] state=[-0.14423024  0.40474688 -0.17936308 -1.45434337], action=1, reward=1.0, next_state=[-0.1361353   0.60155986 -0.20844994 -1.79727724]\n",
      "[ episode 121 ][ timestamp 44 ] state=[-0.1361353   0.60155986 -0.20844994 -1.79727724], action=0, reward=-1.0, next_state=[-0.1241041   0.40929186 -0.24439549 -1.57595975]\n",
      "[ Ended! ] Episode 121: Exploration_rate=0.547986285490042. Score=44.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 122 ] state=[ 0.00185129 -0.04566346  0.04989536  0.00481088]\n",
      "[ episode 122 ][ timestamp 1 ] state=[ 0.00185129 -0.04566346  0.04989536  0.00481088], action=0, reward=1.0, next_state=[ 0.00093802 -0.24146416  0.04999157  0.31280956]\n",
      "[ episode 122 ][ timestamp 2 ] state=[ 0.00093802 -0.24146416  0.04999157  0.31280956], action=0, reward=1.0, next_state=[-0.00389126 -0.43726132  0.05624777  0.62082978]\n",
      "[ episode 122 ][ timestamp 3 ] state=[-0.00389126 -0.43726132  0.05624777  0.62082978], action=1, reward=1.0, next_state=[-0.01263649 -0.24296814  0.06866436  0.34637904]\n",
      "[ episode 122 ][ timestamp 4 ] state=[-0.01263649 -0.24296814  0.06866436  0.34637904], action=1, reward=1.0, next_state=[-0.01749585 -0.04888666  0.07559194  0.07611431]\n",
      "[ episode 122 ][ timestamp 5 ] state=[-0.01749585 -0.04888666  0.07559194  0.07611431], action=0, reward=1.0, next_state=[-0.01847358 -0.24500632  0.07711423  0.39165657]\n",
      "[ episode 122 ][ timestamp 6 ] state=[-0.01847358 -0.24500632  0.07711423  0.39165657], action=1, reward=1.0, next_state=[-0.02337371 -0.05105861  0.08494736  0.1242487 ]\n",
      "[ episode 122 ][ timestamp 7 ] state=[-0.02337371 -0.05105861  0.08494736  0.1242487 ], action=1, reward=1.0, next_state=[-0.02439488  0.14275011  0.08743233 -0.14047162]\n",
      "[ episode 122 ][ timestamp 8 ] state=[-0.02439488  0.14275011  0.08743233 -0.14047162], action=0, reward=1.0, next_state=[-0.02153988 -0.05350819  0.0846229   0.17846371]\n",
      "[ episode 122 ][ timestamp 9 ] state=[-0.02153988 -0.05350819  0.0846229   0.17846371], action=1, reward=1.0, next_state=[-0.02261004  0.14030725  0.08819217 -0.08636968]\n",
      "[ episode 122 ][ timestamp 10 ] state=[-0.02261004  0.14030725  0.08819217 -0.08636968], action=1, reward=1.0, next_state=[-0.0198039   0.3340616   0.08646478 -0.34997679]\n",
      "[ episode 122 ][ timestamp 11 ] state=[-0.0198039   0.3340616   0.08646478 -0.34997679], action=1, reward=1.0, next_state=[-0.01312267  0.52785426  0.07946525 -0.61419086]\n",
      "[ episode 122 ][ timestamp 12 ] state=[-0.01312267  0.52785426  0.07946525 -0.61419086], action=1, reward=1.0, next_state=[-0.00256558  0.72178116  0.06718143 -0.88082504]\n",
      "[ episode 122 ][ timestamp 13 ] state=[-0.00256558  0.72178116  0.06718143 -0.88082504], action=0, reward=1.0, next_state=[ 0.01187004  0.52581406  0.04956493 -0.567801  ]\n",
      "[ episode 122 ][ timestamp 14 ] state=[ 0.01187004  0.52581406  0.04956493 -0.567801  ], action=0, reward=1.0, next_state=[ 0.02238632  0.33003318  0.03820891 -0.25992422]\n",
      "[ episode 122 ][ timestamp 15 ] state=[ 0.02238632  0.33003318  0.03820891 -0.25992422], action=0, reward=1.0, next_state=[0.02898699 0.1343872  0.03301042 0.04456124]\n",
      "[ episode 122 ][ timestamp 16 ] state=[0.02898699 0.1343872  0.03301042 0.04456124], action=0, reward=1.0, next_state=[ 0.03167473 -0.06119217  0.03390165  0.34747378]\n",
      "[ episode 122 ][ timestamp 17 ] state=[ 0.03167473 -0.06119217  0.03390165  0.34747378], action=1, reward=1.0, next_state=[0.03045089 0.13343158 0.04085112 0.06567107]\n",
      "[ episode 122 ][ timestamp 18 ] state=[0.03045089 0.13343158 0.04085112 0.06567107], action=0, reward=1.0, next_state=[ 0.03311952 -0.06225153  0.04216455  0.37095774]\n",
      "[ episode 122 ][ timestamp 19 ] state=[ 0.03311952 -0.06225153  0.04216455  0.37095774], action=0, reward=1.0, next_state=[ 0.03187449 -0.25794636  0.0495837   0.6766318 ]\n",
      "[ episode 122 ][ timestamp 20 ] state=[ 0.03187449 -0.25794636  0.0495837   0.6766318 ], action=1, reward=1.0, next_state=[ 0.02671556 -0.06354715  0.06311634  0.399963  ]\n",
      "[ episode 122 ][ timestamp 21 ] state=[ 0.02671556 -0.06354715  0.06311634  0.399963  ], action=0, reward=1.0, next_state=[ 0.02544462 -0.259505    0.0711156   0.71185837]\n",
      "[ episode 122 ][ timestamp 22 ] state=[ 0.02544462 -0.259505    0.0711156   0.71185837], action=1, reward=1.0, next_state=[ 0.02025452 -0.06543608  0.08535276  0.44238117]\n",
      "[ episode 122 ][ timestamp 23 ] state=[ 0.02025452 -0.06543608  0.08535276  0.44238117], action=1, reward=1.0, next_state=[0.0189458  0.12838094 0.09420039 0.17777724]\n",
      "[ episode 122 ][ timestamp 24 ] state=[0.0189458  0.12838094 0.09420039 0.17777724], action=1, reward=1.0, next_state=[ 0.02151342  0.32203743  0.09775593 -0.08376564]\n",
      "[ episode 122 ][ timestamp 25 ] state=[ 0.02151342  0.32203743  0.09775593 -0.08376564], action=0, reward=1.0, next_state=[0.02795416 0.12565997 0.09608062 0.23808868]\n",
      "[ episode 122 ][ timestamp 26 ] state=[0.02795416 0.12565997 0.09608062 0.23808868], action=1, reward=1.0, next_state=[ 0.03046736  0.31928737  0.10084239 -0.02280858]\n",
      "[ episode 122 ][ timestamp 27 ] state=[ 0.03046736  0.31928737  0.10084239 -0.02280858], action=0, reward=1.0, next_state=[0.03685311 0.12287468 0.10038622 0.29991115]\n",
      "[ episode 122 ][ timestamp 28 ] state=[0.03685311 0.12287468 0.10038622 0.29991115], action=0, reward=1.0, next_state=[ 0.0393106  -0.07352418  0.10638444  0.62249029]\n",
      "[ episode 122 ][ timestamp 29 ] state=[ 0.0393106  -0.07352418  0.10638444  0.62249029], action=0, reward=1.0, next_state=[ 0.03784012 -0.26995815  0.11883425  0.94669351]\n",
      "[ episode 122 ][ timestamp 30 ] state=[ 0.03784012 -0.26995815  0.11883425  0.94669351], action=0, reward=1.0, next_state=[ 0.03244096 -0.46646261  0.13776812  1.27422652]\n",
      "[ episode 122 ][ timestamp 31 ] state=[ 0.03244096 -0.46646261  0.13776812  1.27422652], action=0, reward=1.0, next_state=[ 0.02311171 -0.66304662  0.16325265  1.6066844 ]\n",
      "[ episode 122 ][ timestamp 32 ] state=[ 0.02311171 -0.66304662  0.16325265  1.6066844 ], action=0, reward=1.0, next_state=[ 0.00985077 -0.85967909  0.19538634  1.9454948 ]\n",
      "[ episode 122 ][ timestamp 33 ] state=[ 0.00985077 -0.85967909  0.19538634  1.9454948 ], action=1, reward=-1.0, next_state=[-0.00734281 -0.66710271  0.23429623  1.71920531]\n",
      "[ Ended! ] Episode 122: Exploration_rate=0.5452463540625918. Score=33.\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ] state=[ 0.03382162  0.0281966   0.02138898 -0.04810449]\n",
      "[ episode 123 ][ timestamp 1 ] state=[ 0.03382162  0.0281966   0.02138898 -0.04810449], action=1, reward=1.0, next_state=[ 0.03438555  0.22300543  0.02042689 -0.33396301]\n",
      "[ episode 123 ][ timestamp 2 ] state=[ 0.03438555  0.22300543  0.02042689 -0.33396301], action=0, reward=1.0, next_state=[ 0.03884566  0.02759879  0.01374763 -0.03490912]\n",
      "[ episode 123 ][ timestamp 3 ] state=[ 0.03884566  0.02759879  0.01374763 -0.03490912], action=0, reward=1.0, next_state=[ 0.03939763 -0.16771758  0.01304945  0.26207943]\n",
      "[ episode 123 ][ timestamp 4 ] state=[ 0.03939763 -0.16771758  0.01304945  0.26207943], action=1, reward=1.0, next_state=[ 0.03604328  0.02721569  0.01829104 -0.02645915]\n",
      "[ episode 123 ][ timestamp 5 ] state=[ 0.03604328  0.02721569  0.01829104 -0.02645915], action=0, reward=1.0, next_state=[ 0.0365876  -0.16816373  0.01776186  0.27193822]\n",
      "[ episode 123 ][ timestamp 6 ] state=[ 0.0365876  -0.16816373  0.01776186  0.27193822], action=0, reward=1.0, next_state=[ 0.03322432 -0.36353456  0.02320062  0.57016996]\n",
      "[ episode 123 ][ timestamp 7 ] state=[ 0.03322432 -0.36353456  0.02320062  0.57016996], action=1, reward=1.0, next_state=[ 0.02595363 -0.16874554  0.03460402  0.28488543]\n",
      "[ episode 123 ][ timestamp 8 ] state=[ 0.02595363 -0.16874554  0.03460402  0.28488543], action=0, reward=1.0, next_state=[ 0.02257872 -0.36434349  0.04030173  0.58827827]\n",
      "[ episode 123 ][ timestamp 9 ] state=[ 0.02257872 -0.36434349  0.04030173  0.58827827], action=1, reward=1.0, next_state=[ 0.01529185 -0.16980841  0.05206729  0.30855808]\n",
      "[ episode 123 ][ timestamp 10 ] state=[ 0.01529185 -0.16980841  0.05206729  0.30855808], action=0, reward=1.0, next_state=[ 0.01189568 -0.36563209  0.05823846  0.6171964 ]\n",
      "[ episode 123 ][ timestamp 11 ] state=[ 0.01189568 -0.36563209  0.05823846  0.6171964 ], action=1, reward=1.0, next_state=[ 0.00458304 -0.17137     0.07058238  0.34340972]\n",
      "[ episode 123 ][ timestamp 12 ] state=[ 0.00458304 -0.17137     0.07058238  0.34340972], action=0, reward=1.0, next_state=[ 0.00115564 -0.36742138  0.07745058  0.65748857]\n",
      "[ episode 123 ][ timestamp 13 ] state=[ 0.00115564 -0.36742138  0.07745058  0.65748857], action=0, reward=1.0, next_state=[-0.00619279 -0.56353112  0.09060035  0.97351903]\n",
      "[ episode 123 ][ timestamp 14 ] state=[-0.00619279 -0.56353112  0.09060035  0.97351903], action=0, reward=1.0, next_state=[-0.01746341 -0.75974413  0.11007073  1.2932315 ]\n",
      "[ episode 123 ][ timestamp 15 ] state=[-0.01746341 -0.75974413  0.11007073  1.2932315 ], action=0, reward=1.0, next_state=[-0.03265829 -0.95607935  0.13593536  1.61824759]\n",
      "[ episode 123 ][ timestamp 16 ] state=[-0.03265829 -0.95607935  0.13593536  1.61824759], action=1, reward=1.0, next_state=[-0.05177988 -0.76279648  0.16830031  1.37083987]\n",
      "[ episode 123 ][ timestamp 17 ] state=[-0.05177988 -0.76279648  0.16830031  1.37083987], action=1, reward=1.0, next_state=[-0.06703581 -0.57013134  0.19571711  1.13517247]\n",
      "[ episode 123 ][ timestamp 18 ] state=[-0.06703581 -0.57013134  0.19571711  1.13517247], action=0, reward=-1.0, next_state=[-0.07843844 -0.76719921  0.21842056  1.48230497]\n",
      "[ Ended! ] Episode 123: Exploration_rate=0.5425201222922789. Score=18.\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ] state=[-0.04948881 -0.01997585  0.00866993  0.02198243]\n",
      "[ episode 124 ][ timestamp 1 ] state=[-0.04948881 -0.01997585  0.00866993  0.02198243], action=0, reward=1.0, next_state=[-0.04988832 -0.21522106  0.00910958  0.31738816]\n",
      "[ episode 124 ][ timestamp 2 ] state=[-0.04988832 -0.21522106  0.00910958  0.31738816], action=0, reward=1.0, next_state=[-0.05419275 -0.41047157  0.01545735  0.61292995]\n",
      "[ episode 124 ][ timestamp 3 ] state=[-0.05419275 -0.41047157  0.01545735  0.61292995], action=1, reward=1.0, next_state=[-0.06240218 -0.215569    0.02771594  0.32515531]\n",
      "[ episode 124 ][ timestamp 4 ] state=[-0.06240218 -0.215569    0.02771594  0.32515531], action=1, reward=1.0, next_state=[-0.06671356 -0.02085242  0.03421905  0.04134005]\n",
      "[ episode 124 ][ timestamp 5 ] state=[-0.06671356 -0.02085242  0.03421905  0.04134005], action=0, reward=1.0, next_state=[-0.06713061 -0.21644793  0.03504585  0.34462   ]\n",
      "[ episode 124 ][ timestamp 6 ] state=[-0.06713061 -0.21644793  0.03504585  0.34462   ], action=0, reward=1.0, next_state=[-0.07145956 -0.41205045  0.04193825  0.64814498]\n",
      "[ episode 124 ][ timestamp 7 ] state=[-0.07145956 -0.41205045  0.04193825  0.64814498], action=0, reward=1.0, next_state=[-0.07970057 -0.6077308   0.05490115  0.95373365]\n",
      "[ episode 124 ][ timestamp 8 ] state=[-0.07970057 -0.6077308   0.05490115  0.95373365], action=0, reward=1.0, next_state=[-0.09185519 -0.80354669  0.07397582  1.26314777]\n",
      "[ episode 124 ][ timestamp 9 ] state=[-0.09185519 -0.80354669  0.07397582  1.26314777], action=0, reward=1.0, next_state=[-0.10792612 -0.99953232  0.09923878  1.57805125]\n",
      "[ episode 124 ][ timestamp 10 ] state=[-0.10792612 -0.99953232  0.09923878  1.57805125], action=1, reward=1.0, next_state=[-0.12791677 -0.80572277  0.1307998   1.31789561]\n",
      "[ episode 124 ][ timestamp 11 ] state=[-0.12791677 -0.80572277  0.1307998   1.31789561], action=0, reward=1.0, next_state=[-0.14403122 -1.00223365  0.15715772  1.64848959]\n",
      "[ episode 124 ][ timestamp 12 ] state=[-0.14403122 -1.00223365  0.15715772  1.64848959], action=1, reward=1.0, next_state=[-0.1640759  -0.80925957  0.19012751  1.40861017]\n",
      "[ episode 124 ][ timestamp 13 ] state=[-0.1640759  -0.80925957  0.19012751  1.40861017], action=0, reward=-1.0, next_state=[-0.18026109 -1.00616259  0.21829971  1.75420379]\n",
      "[ Ended! ] Episode 124: Exploration_rate=0.5398075216808175. Score=13.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 125 ] state=[-0.01410906 -0.04826137  0.03261837  0.02333651]\n",
      "[ episode 125 ][ timestamp 1 ] state=[-0.01410906 -0.04826137  0.03261837  0.02333651], action=0, reward=1.0, next_state=[-0.01507429 -0.24383555  0.0330851   0.32612983]\n",
      "[ episode 125 ][ timestamp 2 ] state=[-0.01507429 -0.24383555  0.0330851   0.32612983], action=1, reward=1.0, next_state=[-0.019951   -0.04919989  0.03960769  0.04406136]\n",
      "[ episode 125 ][ timestamp 3 ] state=[-0.019951   -0.04919989  0.03960769  0.04406136], action=0, reward=1.0, next_state=[-0.020935   -0.24486676  0.04048892  0.34897309]\n",
      "[ episode 125 ][ timestamp 4 ] state=[-0.020935   -0.24486676  0.04048892  0.34897309], action=0, reward=1.0, next_state=[-0.02583234 -0.44054048  0.04746838  0.65414361]\n",
      "[ episode 125 ][ timestamp 5 ] state=[-0.02583234 -0.44054048  0.04746838  0.65414361], action=1, reward=1.0, next_state=[-0.03464314 -0.24611049  0.06055126  0.37677761]\n",
      "[ episode 125 ][ timestamp 6 ] state=[-0.03464314 -0.24611049  0.06055126  0.37677761], action=1, reward=1.0, next_state=[-0.03956535 -0.05189847  0.06808681  0.10378466]\n",
      "[ episode 125 ][ timestamp 7 ] state=[-0.03956535 -0.05189847  0.06808681  0.10378466], action=1, reward=1.0, next_state=[-0.04060332  0.14218501  0.0701625  -0.16666396]\n",
      "[ episode 125 ][ timestamp 8 ] state=[-0.04060332  0.14218501  0.0701625  -0.16666396], action=1, reward=1.0, next_state=[-0.03775962  0.33623615  0.06682922 -0.43641365]\n",
      "[ episode 125 ][ timestamp 9 ] state=[-0.03775962  0.33623615  0.06682922 -0.43641365], action=0, reward=1.0, next_state=[-0.0310349   0.14023498  0.05810095 -0.12343502]\n",
      "[ episode 125 ][ timestamp 10 ] state=[-0.0310349   0.14023498  0.05810095 -0.12343502], action=0, reward=1.0, next_state=[-0.0282302  -0.05566915  0.05563225  0.1869974 ]\n",
      "[ episode 125 ][ timestamp 11 ] state=[-0.0282302  -0.05566915  0.05563225  0.1869974 ], action=1, reward=1.0, next_state=[-0.02934358  0.13861454  0.0593722  -0.08762984]\n",
      "[ episode 125 ][ timestamp 12 ] state=[-0.02934358  0.13861454  0.0593722  -0.08762984], action=1, reward=1.0, next_state=[-0.02657129  0.33283742  0.0576196  -0.36100564]\n",
      "[ episode 125 ][ timestamp 13 ] state=[-0.02657129  0.33283742  0.0576196  -0.36100564], action=0, reward=1.0, next_state=[-0.01991455  0.1369458   0.05039949 -0.05072507]\n",
      "[ episode 125 ][ timestamp 14 ] state=[-0.01991455  0.1369458   0.05039949 -0.05072507], action=0, reward=1.0, next_state=[-0.01717563 -0.05886124  0.04938498  0.25742372]\n",
      "[ episode 125 ][ timestamp 15 ] state=[-0.01717563 -0.05886124  0.04938498  0.25742372], action=1, reward=1.0, next_state=[-0.01835285  0.13552214  0.05453346 -0.01928258]\n",
      "[ episode 125 ][ timestamp 16 ] state=[-0.01835285  0.13552214  0.05453346 -0.01928258], action=0, reward=1.0, next_state=[-0.01564241 -0.06033775  0.05414781  0.29009541]\n",
      "[ episode 125 ][ timestamp 17 ] state=[-0.01564241 -0.06033775  0.05414781  0.29009541], action=1, reward=1.0, next_state=[-0.01684917  0.13397195  0.05994972  0.01496971]\n",
      "[ episode 125 ][ timestamp 18 ] state=[-0.01684917  0.13397195  0.05994972  0.01496971], action=1, reward=1.0, next_state=[-0.01416973  0.3281852   0.06024911 -0.25821215]\n",
      "[ episode 125 ][ timestamp 19 ] state=[-0.01416973  0.3281852   0.06024911 -0.25821215], action=1, reward=1.0, next_state=[-0.00760602  0.52239757  0.05508487 -0.53129962]\n",
      "[ episode 125 ][ timestamp 20 ] state=[-0.00760602  0.52239757  0.05508487 -0.53129962], action=1, reward=1.0, next_state=[ 0.00284193  0.71670319  0.04445887 -0.8061292 ]\n",
      "[ episode 125 ][ timestamp 21 ] state=[ 0.00284193  0.71670319  0.04445887 -0.8061292 ], action=0, reward=1.0, next_state=[ 0.01717599  0.52100093  0.02833629 -0.49979928]\n",
      "[ episode 125 ][ timestamp 22 ] state=[ 0.01717599  0.52100093  0.02833629 -0.49979928], action=0, reward=1.0, next_state=[ 0.02759601  0.32549121  0.01834031 -0.19832266]\n",
      "[ episode 125 ][ timestamp 23 ] state=[ 0.02759601  0.32549121  0.01834031 -0.19832266], action=1, reward=1.0, next_state=[ 0.03410583  0.5203461   0.01437385 -0.4851641 ]\n",
      "[ episode 125 ][ timestamp 24 ] state=[ 0.03410583  0.5203461   0.01437385 -0.4851641 ], action=0, reward=1.0, next_state=[ 0.04451276  0.32502429  0.00467057 -0.18798588]\n",
      "[ episode 125 ][ timestamp 25 ] state=[ 0.04451276  0.32502429  0.00467057 -0.18798588], action=0, reward=1.0, next_state=[0.05101324 0.12983582 0.00091085 0.10616676]\n",
      "[ episode 125 ][ timestamp 26 ] state=[0.05101324 0.12983582 0.00091085 0.10616676], action=0, reward=1.0, next_state=[ 0.05360996 -0.06529917  0.00303419  0.39913692]\n",
      "[ episode 125 ][ timestamp 27 ] state=[ 0.05360996 -0.06529917  0.00303419  0.39913692], action=1, reward=1.0, next_state=[0.05230398 0.12977961 0.01101693 0.10741215]\n",
      "[ episode 125 ][ timestamp 28 ] state=[0.05230398 0.12977961 0.01101693 0.10741215], action=1, reward=1.0, next_state=[ 0.05489957  0.32474197  0.01316517 -0.18177473]\n",
      "[ episode 125 ][ timestamp 29 ] state=[ 0.05489957  0.32474197  0.01316517 -0.18177473], action=0, reward=1.0, next_state=[0.06139441 0.12943413 0.00952967 0.11503208]\n",
      "[ episode 125 ][ timestamp 30 ] state=[0.06139441 0.12943413 0.00952967 0.11503208], action=1, reward=1.0, next_state=[ 0.06398309  0.32441825  0.01183032 -0.17462913]\n",
      "[ episode 125 ][ timestamp 31 ] state=[ 0.06398309  0.32441825  0.01183032 -0.17462913], action=1, reward=1.0, next_state=[ 0.07047145  0.51936891  0.00833773 -0.46355662]\n",
      "[ episode 125 ][ timestamp 32 ] state=[ 0.07047145  0.51936891  0.00833773 -0.46355662], action=0, reward=1.0, next_state=[ 0.08085883  0.32413013 -0.0009334  -0.16825737]\n",
      "[ episode 125 ][ timestamp 33 ] state=[ 0.08085883  0.32413013 -0.0009334  -0.16825737], action=0, reward=1.0, next_state=[ 0.08734144  0.12902155 -0.00429855  0.12413095]\n",
      "[ episode 125 ][ timestamp 34 ] state=[ 0.08734144  0.12902155 -0.00429855  0.12413095], action=1, reward=1.0, next_state=[ 0.08992187  0.32420482 -0.00181593 -0.16990502]\n",
      "[ episode 125 ][ timestamp 35 ] state=[ 0.08992187  0.32420482 -0.00181593 -0.16990502], action=1, reward=1.0, next_state=[ 0.09640596  0.51935271 -0.00521403 -0.46316026]\n",
      "[ episode 125 ][ timestamp 36 ] state=[ 0.09640596  0.51935271 -0.00521403 -0.46316026], action=0, reward=1.0, next_state=[ 0.10679302  0.32430484 -0.01447723 -0.17212533]\n",
      "[ episode 125 ][ timestamp 37 ] state=[ 0.10679302  0.32430484 -0.01447723 -0.17212533], action=0, reward=1.0, next_state=[ 0.11327911  0.12939305 -0.01791974  0.11595555]\n",
      "[ episode 125 ][ timestamp 38 ] state=[ 0.11327911  0.12939305 -0.01791974  0.11595555], action=0, reward=1.0, next_state=[ 0.11586697 -0.06546762 -0.01560063  0.4029315 ]\n",
      "[ episode 125 ][ timestamp 39 ] state=[ 0.11586697 -0.06546762 -0.01560063  0.4029315 ], action=1, reward=1.0, next_state=[ 0.11455762  0.12987208 -0.007542    0.1053712 ]\n",
      "[ episode 125 ][ timestamp 40 ] state=[ 0.11455762  0.12987208 -0.007542    0.1053712 ], action=1, reward=1.0, next_state=[ 0.11715506  0.3251013  -0.00543457 -0.18968162]\n",
      "[ episode 125 ][ timestamp 41 ] state=[ 0.11715506  0.3251013  -0.00543457 -0.18968162], action=1, reward=1.0, next_state=[ 0.12365709  0.52030058 -0.00922821 -0.48407397]\n",
      "[ episode 125 ][ timestamp 42 ] state=[ 0.12365709  0.52030058 -0.00922821 -0.48407397], action=0, reward=1.0, next_state=[ 0.1340631   0.32531007 -0.01890969 -0.19431371]\n",
      "[ episode 125 ][ timestamp 43 ] state=[ 0.1340631   0.32531007 -0.01890969 -0.19431371], action=1, reward=1.0, next_state=[ 0.1405693   0.52069734 -0.02279596 -0.49290134]\n",
      "[ episode 125 ][ timestamp 44 ] state=[ 0.1405693   0.52069734 -0.02279596 -0.49290134], action=0, reward=1.0, next_state=[ 0.15098325  0.32590421 -0.03265399 -0.20748899]\n",
      "[ episode 125 ][ timestamp 45 ] state=[ 0.15098325  0.32590421 -0.03265399 -0.20748899], action=1, reward=1.0, next_state=[ 0.15750133  0.52147751 -0.03680377 -0.51029111]\n",
      "[ episode 125 ][ timestamp 46 ] state=[ 0.15750133  0.52147751 -0.03680377 -0.51029111], action=0, reward=1.0, next_state=[ 0.16793088  0.32689283 -0.04700959 -0.22942961]\n",
      "[ episode 125 ][ timestamp 47 ] state=[ 0.16793088  0.32689283 -0.04700959 -0.22942961], action=1, reward=1.0, next_state=[ 0.17446874  0.52265393 -0.05159818 -0.53656259]\n",
      "[ episode 125 ][ timestamp 48 ] state=[ 0.17446874  0.52265393 -0.05159818 -0.53656259], action=0, reward=1.0, next_state=[ 0.18492182  0.328294   -0.06232943 -0.26057385]\n",
      "[ episode 125 ][ timestamp 49 ] state=[ 0.18492182  0.328294   -0.06232943 -0.26057385], action=1, reward=1.0, next_state=[ 0.1914877   0.52424778 -0.06754091 -0.57224673]\n",
      "[ episode 125 ][ timestamp 50 ] state=[ 0.1914877   0.52424778 -0.06754091 -0.57224673], action=0, reward=1.0, next_state=[ 0.20197265  0.33013465 -0.07898585 -0.30158285]\n",
      "[ episode 125 ][ timestamp 51 ] state=[ 0.20197265  0.33013465 -0.07898585 -0.30158285], action=1, reward=1.0, next_state=[ 0.20857535  0.52628831 -0.0850175  -0.6180937 ]\n",
      "[ episode 125 ][ timestamp 52 ] state=[ 0.20857535  0.52628831 -0.0850175  -0.6180937 ], action=1, reward=1.0, next_state=[ 0.21910111  0.7224885  -0.09737938 -0.93629608]\n",
      "[ episode 125 ][ timestamp 53 ] state=[ 0.21910111  0.7224885  -0.09737938 -0.93629608], action=0, reward=1.0, next_state=[ 0.23355088  0.52880511 -0.1161053  -0.6757317 ]\n",
      "[ episode 125 ][ timestamp 54 ] state=[ 0.23355088  0.52880511 -0.1161053  -0.6757317 ], action=1, reward=1.0, next_state=[ 0.24412699  0.72533268 -0.12961993 -1.00259663]\n",
      "[ episode 125 ][ timestamp 55 ] state=[ 0.24412699  0.72533268 -0.12961993 -1.00259663], action=0, reward=1.0, next_state=[ 0.25863364  0.53215831 -0.14967186 -0.7532675 ]\n",
      "[ episode 125 ][ timestamp 56 ] state=[ 0.25863364  0.53215831 -0.14967186 -0.7532675 ], action=0, reward=1.0, next_state=[ 0.26927681  0.33938205 -0.16473721 -0.51117537]\n",
      "[ episode 125 ][ timestamp 57 ] state=[ 0.26927681  0.33938205 -0.16473721 -0.51117537], action=1, reward=1.0, next_state=[ 0.27606445  0.53639456 -0.17496072 -0.85090724]\n",
      "[ episode 125 ][ timestamp 58 ] state=[ 0.27606445  0.53639456 -0.17496072 -0.85090724], action=0, reward=1.0, next_state=[ 0.28679234  0.34403443 -0.19197887 -0.61794849]\n",
      "[ episode 125 ][ timestamp 59 ] state=[ 0.28679234  0.34403443 -0.19197887 -0.61794849], action=0, reward=1.0, next_state=[ 0.29367303  0.15203894 -0.20433784 -0.39134182]\n",
      "[ episode 125 ][ timestamp 60 ] state=[ 0.29367303  0.15203894 -0.20433784 -0.39134182], action=1, reward=-1.0, next_state=[ 0.29671381  0.3493856  -0.21216467 -0.74086141]\n",
      "[ Ended! ] Episode 125: Exploration_rate=0.5371084840724134. Score=60.\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ] state=[ 0.04590618 -0.01381282 -0.03683213  0.01557535]\n",
      "[ episode 126 ][ timestamp 1 ] state=[ 0.04590618 -0.01381282 -0.03683213  0.01557535], action=0, reward=1.0, next_state=[ 0.04562992 -0.20838773 -0.03652063  0.29641356]\n",
      "[ episode 126 ][ timestamp 2 ] state=[ 0.04562992 -0.20838773 -0.03652063  0.29641356], action=0, reward=1.0, next_state=[ 0.04146217 -0.40297053 -0.03059236  0.57735847]\n",
      "[ episode 126 ][ timestamp 3 ] state=[ 0.04146217 -0.40297053 -0.03059236  0.57735847], action=1, reward=1.0, next_state=[ 0.03340276 -0.20743345 -0.01904519  0.27519733]\n",
      "[ episode 126 ][ timestamp 4 ] state=[ 0.03340276 -0.20743345 -0.01904519  0.27519733], action=0, reward=1.0, next_state=[ 0.02925409 -0.40227857 -0.01354124  0.56181306]\n",
      "[ episode 126 ][ timestamp 5 ] state=[ 0.02925409 -0.40227857 -0.01354124  0.56181306], action=1, reward=1.0, next_state=[ 0.02120852 -0.20696923 -0.00230498  0.26489491]\n",
      "[ episode 126 ][ timestamp 6 ] state=[ 0.02120852 -0.20696923 -0.00230498  0.26489491], action=0, reward=1.0, next_state=[ 0.01706913 -0.40205821  0.00299292  0.55684993]\n",
      "[ episode 126 ][ timestamp 7 ] state=[ 0.01706913 -0.40205821  0.00299292  0.55684993], action=1, reward=1.0, next_state=[ 0.00902797 -0.2069784   0.01412992  0.26511145]\n",
      "[ episode 126 ][ timestamp 8 ] state=[ 0.00902797 -0.2069784   0.01412992  0.26511145], action=0, reward=1.0, next_state=[ 0.0048884  -0.40229915  0.01943215  0.56221738]\n",
      "[ episode 126 ][ timestamp 9 ] state=[ 0.0048884  -0.40229915  0.01943215  0.56221738], action=1, reward=1.0, next_state=[-0.00315758 -0.2074552   0.03067649  0.27571933]\n",
      "[ episode 126 ][ timestamp 10 ] state=[-0.00315758 -0.2074552   0.03067649  0.27571933], action=0, reward=1.0, next_state=[-0.00730669 -0.40300109  0.03619088  0.57791763]\n",
      "[ episode 126 ][ timestamp 11 ] state=[-0.00730669 -0.40300109  0.03619088  0.57791763], action=1, reward=1.0, next_state=[-0.01536671 -0.20840456  0.04774923  0.29685178]\n",
      "[ episode 126 ][ timestamp 12 ] state=[-0.01536671 -0.20840456  0.04774923  0.29685178], action=1, reward=1.0, next_state=[-0.0195348  -0.01399467  0.05368627  0.01960226]\n",
      "[ episode 126 ][ timestamp 13 ] state=[-0.0195348  -0.01399467  0.05368627  0.01960226], action=1, reward=1.0, next_state=[-0.01981469  0.1803179   0.05407831 -0.25567047]\n",
      "[ episode 126 ][ timestamp 14 ] state=[-0.01981469  0.1803179   0.05407831 -0.25567047], action=0, reward=1.0, next_state=[-0.01620833 -0.01553279  0.04896491  0.05356737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 126 ][ timestamp 15 ] state=[-0.01620833 -0.01553279  0.04896491  0.05356737], action=0, reward=1.0, next_state=[-0.01651899 -0.21132139  0.05003625  0.36128821]\n",
      "[ episode 126 ][ timestamp 16 ] state=[-0.01651899 -0.21132139  0.05003625  0.36128821], action=1, reward=1.0, next_state=[-0.02074542 -0.01694504  0.05726202  0.08479312]\n",
      "[ episode 126 ][ timestamp 17 ] state=[-0.02074542 -0.01694504  0.05726202  0.08479312], action=0, reward=1.0, next_state=[-0.02108432 -0.21283907  0.05895788  0.39497839]\n",
      "[ episode 126 ][ timestamp 18 ] state=[-0.02108432 -0.21283907  0.05895788  0.39497839], action=1, reward=1.0, next_state=[-0.0253411  -0.01860108  0.06685745  0.12145121]\n",
      "[ episode 126 ][ timestamp 19 ] state=[-0.0253411  -0.01860108  0.06685745  0.12145121], action=0, reward=1.0, next_state=[-0.02571312 -0.21461404  0.06928647  0.43445522]\n",
      "[ episode 126 ][ timestamp 20 ] state=[-0.02571312 -0.21461404  0.06928647  0.43445522], action=0, reward=1.0, next_state=[-0.0300054  -0.41064498  0.07797558  0.74815005]\n",
      "[ episode 126 ][ timestamp 21 ] state=[-0.0300054  -0.41064498  0.07797558  0.74815005], action=1, reward=1.0, next_state=[-0.0382183  -0.21668037  0.09293858  0.48098878]\n",
      "[ episode 126 ][ timestamp 22 ] state=[-0.0382183  -0.21668037  0.09293858  0.48098878], action=1, reward=1.0, next_state=[-0.04255191 -0.02298475  0.10255835  0.21898387]\n",
      "[ episode 126 ][ timestamp 23 ] state=[-0.04255191 -0.02298475  0.10255835  0.21898387], action=0, reward=1.0, next_state=[-0.04301161 -0.21941181  0.10693803  0.5421756 ]\n",
      "[ episode 126 ][ timestamp 24 ] state=[-0.04301161 -0.21941181  0.10693803  0.5421756 ], action=1, reward=1.0, next_state=[-0.04739984 -0.02594251  0.11778154  0.2850093 ]\n",
      "[ episode 126 ][ timestamp 25 ] state=[-0.04739984 -0.02594251  0.11778154  0.2850093 ], action=0, reward=1.0, next_state=[-0.04791869 -0.2225301   0.12348173  0.61239545]\n",
      "[ episode 126 ][ timestamp 26 ] state=[-0.04791869 -0.2225301   0.12348173  0.61239545], action=1, reward=1.0, next_state=[-0.05236929 -0.0293305   0.13572964  0.36101408]\n",
      "[ episode 126 ][ timestamp 27 ] state=[-0.05236929 -0.0293305   0.13572964  0.36101408], action=0, reward=1.0, next_state=[-0.0529559  -0.22609438  0.14294992  0.6932275 ]\n",
      "[ episode 126 ][ timestamp 28 ] state=[-0.0529559  -0.22609438  0.14294992  0.6932275 ], action=0, reward=1.0, next_state=[-0.05747779 -0.42287959  0.15681447  1.02727881]\n",
      "[ episode 126 ][ timestamp 29 ] state=[-0.05747779 -0.42287959  0.15681447  1.02727881], action=1, reward=1.0, next_state=[-0.06593538 -0.23015306  0.17736004  0.78765093]\n",
      "[ episode 126 ][ timestamp 30 ] state=[-0.06593538 -0.23015306  0.17736004  0.78765093], action=1, reward=1.0, next_state=[-0.07053844 -0.03785291  0.19311306  0.55559656]\n",
      "[ episode 126 ][ timestamp 31 ] state=[-0.07053844 -0.03785291  0.19311306  0.55559656], action=0, reward=1.0, next_state=[-0.0712955  -0.2350867   0.20422499  0.90237086]\n",
      "[ episode 126 ][ timestamp 32 ] state=[-0.0712955  -0.2350867   0.20422499  0.90237086], action=1, reward=-1.0, next_state=[-0.07599724 -0.04322918  0.22227241  0.68019086]\n",
      "[ Ended! ] Episode 126: Exploration_rate=0.5344229416520513. Score=32.\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ] state=[ 0.01269337  0.04416733  0.02495607 -0.04305077]\n",
      "[ episode 127 ][ timestamp 1 ] state=[ 0.01269337  0.04416733  0.02495607 -0.04305077], action=1, reward=1.0, next_state=[ 0.01357672  0.23892269  0.02409505 -0.32775653]\n",
      "[ episode 127 ][ timestamp 2 ] state=[ 0.01357672  0.23892269  0.02409505 -0.32775653], action=1, reward=1.0, next_state=[ 0.01835517  0.43369347  0.01753992 -0.61274463]\n",
      "[ episode 127 ][ timestamp 3 ] state=[ 0.01835517  0.43369347  0.01753992 -0.61274463], action=1, reward=1.0, next_state=[ 0.02702904  0.62856596  0.00528503 -0.89985194]\n",
      "[ episode 127 ][ timestamp 4 ] state=[ 0.02702904  0.62856596  0.00528503 -0.89985194], action=0, reward=1.0, next_state=[ 0.03960036  0.43337279 -0.01271201 -0.60551248]\n",
      "[ episode 127 ][ timestamp 5 ] state=[ 0.03960036  0.43337279 -0.01271201 -0.60551248], action=1, reward=1.0, next_state=[ 0.04826781  0.62867018 -0.02482226 -0.90217212]\n",
      "[ episode 127 ][ timestamp 6 ] state=[ 0.04826781  0.62867018 -0.02482226 -0.90217212], action=0, reward=1.0, next_state=[ 0.06084122  0.43389314 -0.0428657  -0.61739356]\n",
      "[ episode 127 ][ timestamp 7 ] state=[ 0.06084122  0.43389314 -0.0428657  -0.61739356], action=1, reward=1.0, next_state=[ 0.06951908  0.62958686 -0.05521357 -0.92326316]\n",
      "[ episode 127 ][ timestamp 8 ] state=[ 0.06951908  0.62958686 -0.05521357 -0.92326316], action=0, reward=1.0, next_state=[ 0.08211082  0.43525256 -0.07367884 -0.64843046]\n",
      "[ episode 127 ][ timestamp 9 ] state=[ 0.08211082  0.43525256 -0.07367884 -0.64843046], action=1, reward=1.0, next_state=[ 0.09081587  0.63131941 -0.08664745 -0.96337482]\n",
      "[ episode 127 ][ timestamp 10 ] state=[ 0.09081587  0.63131941 -0.08664745 -0.96337482], action=0, reward=1.0, next_state=[ 0.10344226  0.43746178 -0.10591494 -0.69912175]\n",
      "[ episode 127 ][ timestamp 11 ] state=[ 0.10344226  0.43746178 -0.10591494 -0.69912175], action=1, reward=1.0, next_state=[ 0.11219149  0.63388033 -0.11989738 -1.02317937]\n",
      "[ episode 127 ][ timestamp 12 ] state=[ 0.11219149  0.63388033 -0.11989738 -1.02317937], action=1, reward=1.0, next_state=[ 0.1248691   0.83037749 -0.14036096 -1.35097454]\n",
      "[ episode 127 ][ timestamp 13 ] state=[ 0.1248691   0.83037749 -0.14036096 -1.35097454], action=1, reward=1.0, next_state=[ 0.14147665  1.02695546 -0.16738046 -1.6840724 ]\n",
      "[ episode 127 ][ timestamp 14 ] state=[ 0.14147665  1.02695546 -0.16738046 -1.6840724 ], action=0, reward=1.0, next_state=[ 0.16201576  0.83412006 -0.2010619  -1.44784214]\n",
      "[ episode 127 ][ timestamp 15 ] state=[ 0.16201576  0.83412006 -0.2010619  -1.44784214], action=0, reward=-1.0, next_state=[ 0.17869816  0.64195692 -0.23001875 -1.22411881]\n",
      "[ Ended! ] Episode 127: Exploration_rate=0.531750826943791. Score=15.\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ] state=[ 0.03417165  0.02890025 -0.01499403  0.00572418]\n",
      "[ episode 128 ][ timestamp 1 ] state=[ 0.03417165  0.02890025 -0.01499403  0.00572418], action=0, reward=1.0, next_state=[ 0.03474966 -0.16600349 -0.01487955  0.29363885]\n",
      "[ episode 128 ][ timestamp 2 ] state=[ 0.03474966 -0.16600349 -0.01487955  0.29363885], action=1, reward=1.0, next_state=[ 0.03142959  0.0293274  -0.00900677 -0.00369949]\n",
      "[ episode 128 ][ timestamp 3 ] state=[ 0.03142959  0.0293274  -0.00900677 -0.00369949], action=1, reward=1.0, next_state=[ 0.03201613  0.22457736 -0.00908076 -0.2992105 ]\n",
      "[ episode 128 ][ timestamp 4 ] state=[ 0.03201613  0.22457736 -0.00908076 -0.2992105 ], action=1, reward=1.0, next_state=[ 0.03650768  0.41982756 -0.01506497 -0.59474344]\n",
      "[ episode 128 ][ timestamp 5 ] state=[ 0.03650768  0.41982756 -0.01506497 -0.59474344], action=0, reward=1.0, next_state=[ 0.04490423  0.22491967 -0.02695984 -0.30684371]\n",
      "[ episode 128 ][ timestamp 6 ] state=[ 0.04490423  0.22491967 -0.02695984 -0.30684371], action=1, reward=1.0, next_state=[ 0.04940263  0.42041521 -0.03309671 -0.60790568]\n",
      "[ episode 128 ][ timestamp 7 ] state=[ 0.04940263  0.42041521 -0.03309671 -0.60790568], action=0, reward=1.0, next_state=[ 0.05781093  0.22577123 -0.04525483 -0.32582827]\n",
      "[ episode 128 ][ timestamp 8 ] state=[ 0.05781093  0.22577123 -0.04525483 -0.32582827], action=1, reward=1.0, next_state=[ 0.06232635  0.42150732 -0.05177139 -0.63243218]\n",
      "[ episode 128 ][ timestamp 9 ] state=[ 0.06232635  0.42150732 -0.05177139 -0.63243218], action=0, reward=1.0, next_state=[ 0.0707565   0.22714441 -0.06442003 -0.35649243]\n",
      "[ episode 128 ][ timestamp 10 ] state=[ 0.0707565   0.22714441 -0.06442003 -0.35649243], action=1, reward=1.0, next_state=[ 0.07529939  0.42312028 -0.07154988 -0.66877287]\n",
      "[ episode 128 ][ timestamp 11 ] state=[ 0.07529939  0.42312028 -0.07154988 -0.66877287], action=0, reward=1.0, next_state=[ 0.08376179  0.22906233 -0.08492534 -0.39944844]\n",
      "[ episode 128 ][ timestamp 12 ] state=[ 0.08376179  0.22906233 -0.08492534 -0.39944844], action=1, reward=1.0, next_state=[ 0.08834304  0.42527988 -0.09291431 -0.71765207]\n",
      "[ episode 128 ][ timestamp 13 ] state=[ 0.08834304  0.42527988 -0.09291431 -0.71765207], action=0, reward=1.0, next_state=[ 0.09684864  0.2315582  -0.10726735 -0.45560048]\n",
      "[ episode 128 ][ timestamp 14 ] state=[ 0.09684864  0.2315582  -0.10726735 -0.45560048], action=1, reward=1.0, next_state=[ 0.1014798   0.4280203  -0.11637936 -0.78007599]\n",
      "[ episode 128 ][ timestamp 15 ] state=[ 0.1014798   0.4280203  -0.11637936 -0.78007599], action=0, reward=1.0, next_state=[ 0.11004021  0.23467418 -0.13198088 -0.52615698]\n",
      "[ episode 128 ][ timestamp 16 ] state=[ 0.11004021  0.23467418 -0.13198088 -0.52615698], action=0, reward=1.0, next_state=[ 0.11473369  0.0416322  -0.14250402 -0.27780212]\n",
      "[ episode 128 ][ timestamp 17 ] state=[ 0.11473369  0.0416322  -0.14250402 -0.27780212], action=1, reward=1.0, next_state=[ 0.11556634  0.23846911 -0.14806006 -0.61181915]\n",
      "[ episode 128 ][ timestamp 18 ] state=[ 0.11556634  0.23846911 -0.14806006 -0.61181915], action=0, reward=1.0, next_state=[ 0.12033572  0.04569275 -0.16029644 -0.36918912]\n",
      "[ episode 128 ][ timestamp 19 ] state=[ 0.12033572  0.04569275 -0.16029644 -0.36918912], action=1, reward=1.0, next_state=[ 0.12124957  0.24268598 -0.16768023 -0.70781637]\n",
      "[ episode 128 ][ timestamp 20 ] state=[ 0.12124957  0.24268598 -0.16768023 -0.70781637], action=1, reward=1.0, next_state=[ 0.12610329  0.43968481 -0.18183655 -1.04823742]\n",
      "[ episode 128 ][ timestamp 21 ] state=[ 0.12610329  0.43968481 -0.18183655 -1.04823742], action=0, reward=1.0, next_state=[ 0.13489699  0.24737984 -0.2028013  -0.81770154]\n",
      "[ episode 128 ][ timestamp 22 ] state=[ 0.13489699  0.24737984 -0.2028013  -0.81770154], action=0, reward=-1.0, next_state=[ 0.13984459  0.05552537 -0.21915533 -0.59503329]\n",
      "[ Ended! ] Episode 128: Exploration_rate=0.5290920728090721. Score=22.\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ] state=[-0.02702943  0.01478727 -0.03010024 -0.00744901]\n",
      "[ episode 129 ][ timestamp 1 ] state=[-0.02702943  0.01478727 -0.03010024 -0.00744901], action=1, reward=1.0, next_state=[-0.02673368  0.21032768 -0.03024922 -0.3094749 ]\n",
      "[ episode 129 ][ timestamp 2 ] state=[-0.02673368  0.21032768 -0.03024922 -0.3094749 ], action=0, reward=1.0, next_state=[-0.02252713  0.01564948 -0.03643871 -0.02648311]\n",
      "[ episode 129 ][ timestamp 3 ] state=[-0.02252713  0.01564948 -0.03643871 -0.02648311], action=1, reward=1.0, next_state=[-0.02221414  0.21127453 -0.03696838 -0.33043651]\n",
      "[ episode 129 ][ timestamp 4 ] state=[-0.02221414  0.21127453 -0.03696838 -0.33043651], action=1, reward=1.0, next_state=[-0.01798865  0.40690269 -0.04357711 -0.63454448]\n",
      "[ episode 129 ][ timestamp 5 ] state=[-0.01798865  0.40690269 -0.04357711 -0.63454448], action=0, reward=1.0, next_state=[-0.00985059  0.2124148  -0.056268   -0.35589721]\n",
      "[ episode 129 ][ timestamp 6 ] state=[-0.00985059  0.2124148  -0.056268   -0.35589721], action=0, reward=1.0, next_state=[-0.0056023   0.01813613 -0.06338594 -0.08147446]\n",
      "[ episode 129 ][ timestamp 7 ] state=[-0.0056023   0.01813613 -0.06338594 -0.08147446], action=1, reward=1.0, next_state=[-0.00523958  0.21410675 -0.06501543 -0.39346306]\n",
      "[ episode 129 ][ timestamp 8 ] state=[-0.00523958  0.21410675 -0.06501543 -0.39346306], action=1, reward=1.0, next_state=[-0.00095744  0.41008815 -0.07288469 -0.70591514]\n",
      "[ episode 129 ][ timestamp 9 ] state=[-0.00095744  0.41008815 -0.07288469 -0.70591514], action=0, reward=1.0, next_state=[ 0.00724432  0.21604766 -0.08700299 -0.43703628]\n",
      "[ episode 129 ][ timestamp 10 ] state=[ 0.00724432  0.21604766 -0.08700299 -0.43703628], action=0, reward=1.0, next_state=[ 0.01156528  0.02225804 -0.09574372 -0.17299795]\n",
      "[ episode 129 ][ timestamp 11 ] state=[ 0.01156528  0.02225804 -0.09574372 -0.17299795], action=1, reward=1.0, next_state=[ 0.01201044  0.21861063 -0.09920368 -0.49428358]\n",
      "[ episode 129 ][ timestamp 12 ] state=[ 0.01201044  0.21861063 -0.09920368 -0.49428358], action=1, reward=1.0, next_state=[ 0.01638265  0.41498144 -0.10908935 -0.81650963]\n",
      "[ episode 129 ][ timestamp 13 ] state=[ 0.01638265  0.41498144 -0.10908935 -0.81650963], action=0, reward=1.0, next_state=[ 0.02468228  0.22150859 -0.12541954 -0.56003416]\n",
      "[ episode 129 ][ timestamp 14 ] state=[ 0.02468228  0.22150859 -0.12541954 -0.56003416], action=0, reward=1.0, next_state=[ 0.02911245  0.02834942 -0.13662023 -0.30934797]\n",
      "[ episode 129 ][ timestamp 15 ] state=[ 0.02911245  0.02834942 -0.13662023 -0.30934797], action=1, reward=1.0, next_state=[ 0.02967944  0.22512662 -0.14280718 -0.64180492]\n",
      "[ episode 129 ][ timestamp 16 ] state=[ 0.02967944  0.22512662 -0.14280718 -0.64180492], action=1, reward=1.0, next_state=[ 0.03418197  0.42192004 -0.15564328 -0.97583287]\n",
      "[ episode 129 ][ timestamp 17 ] state=[ 0.03418197  0.42192004 -0.15564328 -0.97583287], action=0, reward=1.0, next_state=[ 0.04262037  0.22918923 -0.17515994 -0.73580584]\n",
      "[ episode 129 ][ timestamp 18 ] state=[ 0.04262037  0.22918923 -0.17515994 -0.73580584], action=0, reward=1.0, next_state=[ 0.04720416  0.03686358 -0.18987606 -0.50296572]\n",
      "[ episode 129 ][ timestamp 19 ] state=[ 0.04720416  0.03686358 -0.18987606 -0.50296572], action=1, reward=1.0, next_state=[ 0.04794143  0.23408302 -0.19993537 -0.84896686]\n",
      "[ episode 129 ][ timestamp 20 ] state=[ 0.04794143  0.23408302 -0.19993537 -0.84896686], action=0, reward=-1.0, next_state=[ 0.05262309  0.04216688 -0.21691471 -0.62521744]\n",
      "[ Ended! ] Episode 129: Exploration_rate=0.5264466124450268. Score=20.\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ] state=[ 0.03811173 -0.04206317 -0.00359147 -0.01091699]\n",
      "[ episode 130 ][ timestamp 1 ] state=[ 0.03811173 -0.04206317 -0.00359147 -0.01091699], action=0, reward=1.0, next_state=[ 0.03727046 -0.23713343 -0.00380981  0.28063063]\n",
      "[ episode 130 ][ timestamp 2 ] state=[ 0.03727046 -0.23713343 -0.00380981  0.28063063], action=1, reward=1.0, next_state=[ 0.03252779 -0.04195734  0.00180281 -0.01325146]\n",
      "[ episode 130 ][ timestamp 3 ] state=[ 0.03252779 -0.04195734  0.00180281 -0.01325146], action=1, reward=1.0, next_state=[ 0.03168865  0.15313871  0.00153778 -0.30536504]\n",
      "[ episode 130 ][ timestamp 4 ] state=[ 0.03168865  0.15313871  0.00153778 -0.30536504], action=0, reward=1.0, next_state=[ 0.03475142 -0.04200513 -0.00456952 -0.01219753]\n",
      "[ episode 130 ][ timestamp 5 ] state=[ 0.03475142 -0.04200513 -0.00456952 -0.01219753], action=0, reward=1.0, next_state=[ 0.03391132 -0.23706125 -0.00481347  0.27904016]\n",
      "[ episode 130 ][ timestamp 6 ] state=[ 0.03391132 -0.23706125 -0.00481347  0.27904016], action=1, reward=1.0, next_state=[ 0.02917009 -0.04187096  0.00076733 -0.01515703]\n",
      "[ episode 130 ][ timestamp 7 ] state=[ 0.02917009 -0.04187096  0.00076733 -0.01515703], action=0, reward=1.0, next_state=[ 0.02833267 -0.23700391  0.00046419  0.2777679 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 130 ][ timestamp 8 ] state=[ 0.02833267 -0.23700391  0.00046419  0.2777679 ], action=1, reward=1.0, next_state=[ 0.0235926  -0.04188858  0.00601955 -0.01476859]\n",
      "[ episode 130 ][ timestamp 9 ] state=[ 0.0235926  -0.04188858  0.00601955 -0.01476859], action=0, reward=1.0, next_state=[ 0.02275482 -0.23709634  0.00572417  0.27980748]\n",
      "[ episode 130 ][ timestamp 10 ] state=[ 0.02275482 -0.23709634  0.00572417  0.27980748], action=0, reward=1.0, next_state=[ 0.0180129  -0.43229948  0.01132032  0.57429029]\n",
      "[ episode 130 ][ timestamp 11 ] state=[ 0.0180129  -0.43229948  0.01132032  0.57429029], action=1, reward=1.0, next_state=[ 0.00936691 -0.23733805  0.02280613  0.28519499]\n",
      "[ episode 130 ][ timestamp 12 ] state=[ 0.00936691 -0.23733805  0.02280613  0.28519499], action=1, reward=1.0, next_state=[ 0.00462015 -0.04254866  0.02851003 -0.00020869]\n",
      "[ episode 130 ][ timestamp 13 ] state=[ 0.00462015 -0.04254866  0.02851003 -0.00020869], action=0, reward=1.0, next_state=[ 0.00376917 -0.23806764  0.02850586  0.30133141]\n",
      "[ episode 130 ][ timestamp 14 ] state=[ 0.00376917 -0.23806764  0.02850586  0.30133141], action=1, reward=1.0, next_state=[-0.00099218 -0.04336333  0.03453248  0.01777318]\n",
      "[ episode 130 ][ timestamp 15 ] state=[-0.00099218 -0.04336333  0.03453248  0.01777318], action=0, reward=1.0, next_state=[-0.00185945 -0.23896306  0.03488795  0.32114839]\n",
      "[ episode 130 ][ timestamp 16 ] state=[-0.00185945 -0.23896306  0.03488795  0.32114839], action=1, reward=1.0, next_state=[-0.00663871 -0.04435487  0.04131092  0.03966871]\n",
      "[ episode 130 ][ timestamp 17 ] state=[-0.00663871 -0.04435487  0.04131092  0.03966871], action=1, reward=1.0, next_state=[-0.0075258   0.15015109  0.04210429 -0.23969935]\n",
      "[ episode 130 ][ timestamp 18 ] state=[-0.0075258   0.15015109  0.04210429 -0.23969935], action=1, reward=1.0, next_state=[-0.00452278  0.34464706  0.0373103  -0.51880974]\n",
      "[ episode 130 ][ timestamp 19 ] state=[-0.00452278  0.34464706  0.0373103  -0.51880974], action=0, reward=1.0, next_state=[ 0.00237016  0.14902023  0.02693411 -0.21460704]\n",
      "[ episode 130 ][ timestamp 20 ] state=[ 0.00237016  0.14902023  0.02693411 -0.21460704], action=1, reward=1.0, next_state=[ 0.00535056  0.34374697  0.02264197 -0.49867354]\n",
      "[ episode 130 ][ timestamp 21 ] state=[ 0.00535056  0.34374697  0.02264197 -0.49867354], action=1, reward=1.0, next_state=[ 0.0122255   0.5385425   0.0126685  -0.78413577]\n",
      "[ episode 130 ][ timestamp 22 ] state=[ 0.0122255   0.5385425   0.0126685  -0.78413577], action=0, reward=1.0, next_state=[ 0.02299635  0.34324878 -0.00301422 -0.48749425]\n",
      "[ episode 130 ][ timestamp 23 ] state=[ 0.02299635  0.34324878 -0.00301422 -0.48749425], action=0, reward=1.0, next_state=[ 0.02986133  0.14816949 -0.0127641  -0.19576282]\n",
      "[ episode 130 ][ timestamp 24 ] state=[ 0.02986133  0.14816949 -0.0127641  -0.19576282], action=0, reward=1.0, next_state=[ 0.03282472 -0.04676758 -0.01667936  0.09286642]\n",
      "[ episode 130 ][ timestamp 25 ] state=[ 0.03282472 -0.04676758 -0.01667936  0.09286642], action=1, reward=1.0, next_state=[ 0.03188937  0.14858942 -0.01482203 -0.20503182]\n",
      "[ episode 130 ][ timestamp 26 ] state=[ 0.03188937  0.14858942 -0.01482203 -0.20503182], action=0, reward=1.0, next_state=[ 0.03486115 -0.04631747 -0.01892267  0.08293888]\n",
      "[ episode 130 ][ timestamp 27 ] state=[ 0.03486115 -0.04631747 -0.01892267  0.08293888], action=1, reward=1.0, next_state=[ 0.03393481  0.14907055 -0.01726389 -0.21565361]\n",
      "[ episode 130 ][ timestamp 28 ] state=[ 0.03393481  0.14907055 -0.01726389 -0.21565361], action=0, reward=1.0, next_state=[ 0.03691622 -0.0458004  -0.02157696  0.07153391]\n",
      "[ episode 130 ][ timestamp 29 ] state=[ 0.03691622 -0.0458004  -0.02157696  0.07153391], action=0, reward=1.0, next_state=[ 0.03600021 -0.24060647 -0.02014629  0.35733187]\n",
      "[ episode 130 ][ timestamp 30 ] state=[ 0.03600021 -0.24060647 -0.02014629  0.35733187], action=1, reward=1.0, next_state=[ 0.03118808 -0.04520398 -0.01299965  0.05836501]\n",
      "[ episode 130 ][ timestamp 31 ] state=[ 0.03118808 -0.04520398 -0.01299965  0.05836501], action=0, reward=1.0, next_state=[ 0.030284   -0.24013715 -0.01183235  0.34691827]\n",
      "[ episode 130 ][ timestamp 32 ] state=[ 0.030284   -0.24013715 -0.01183235  0.34691827], action=0, reward=1.0, next_state=[ 0.02548126 -0.43508882 -0.00489398  0.63584667]\n",
      "[ episode 130 ][ timestamp 33 ] state=[ 0.02548126 -0.43508882 -0.00489398  0.63584667], action=1, reward=1.0, next_state=[ 0.01677948 -0.23989896  0.00782295  0.34162656]\n",
      "[ episode 130 ][ timestamp 34 ] state=[ 0.01677948 -0.23989896  0.00782295  0.34162656], action=1, reward=1.0, next_state=[ 0.0119815  -0.04488918  0.01465548  0.05142076]\n",
      "[ episode 130 ][ timestamp 35 ] state=[ 0.0119815  -0.04488918  0.01465548  0.05142076], action=0, reward=1.0, next_state=[ 0.01108372 -0.24021817  0.0156839   0.34869135]\n",
      "[ episode 130 ][ timestamp 36 ] state=[ 0.01108372 -0.24021817  0.0156839   0.34869135], action=0, reward=1.0, next_state=[ 0.00627935 -0.43555964  0.02265772  0.64627839]\n",
      "[ episode 130 ][ timestamp 37 ] state=[ 0.00627935 -0.43555964  0.02265772  0.64627839], action=1, reward=1.0, next_state=[-0.00243184 -0.24076061  0.03558329  0.36081564]\n",
      "[ episode 130 ][ timestamp 38 ] state=[-0.00243184 -0.24076061  0.03558329  0.36081564], action=1, reward=1.0, next_state=[-0.00724705 -0.04616204  0.04279961  0.07956185]\n",
      "[ episode 130 ][ timestamp 39 ] state=[-0.00724705 -0.04616204  0.04279961  0.07956185], action=1, reward=1.0, next_state=[-0.00817029  0.14832106  0.04439084 -0.19931641]\n",
      "[ episode 130 ][ timestamp 40 ] state=[-0.00817029  0.14832106  0.04439084 -0.19931641], action=0, reward=1.0, next_state=[-0.00520387 -0.04740676  0.04040451  0.10703272]\n",
      "[ episode 130 ][ timestamp 41 ] state=[-0.00520387 -0.04740676  0.04040451  0.10703272], action=0, reward=1.0, next_state=[-0.00615201 -0.24308373  0.04254517  0.41218432]\n",
      "[ episode 130 ][ timestamp 42 ] state=[-0.00615201 -0.24308373  0.04254517  0.41218432], action=0, reward=1.0, next_state=[-0.01101368 -0.43878215  0.05078885  0.71797082]\n",
      "[ episode 130 ][ timestamp 43 ] state=[-0.01101368 -0.43878215  0.05078885  0.71797082], action=1, reward=1.0, next_state=[-0.01978932 -0.24439846  0.06514827  0.44169677]\n",
      "[ episode 130 ][ timestamp 44 ] state=[-0.01978932 -0.24439846  0.06514827  0.44169677], action=0, reward=1.0, next_state=[-0.02467729 -0.44037892  0.07398221  0.75418388]\n",
      "[ episode 130 ][ timestamp 45 ] state=[-0.02467729 -0.44037892  0.07398221  0.75418388], action=0, reward=1.0, next_state=[-0.03348487 -0.63643863  0.08906588  1.06919991]\n",
      "[ episode 130 ][ timestamp 46 ] state=[-0.03348487 -0.63643863  0.08906588  1.06919991], action=1, reward=1.0, next_state=[-0.04621364 -0.44260028  0.11044988  0.80574563]\n",
      "[ episode 130 ][ timestamp 47 ] state=[-0.04621364 -0.44260028  0.11044988  0.80574563], action=1, reward=1.0, next_state=[-0.05506565 -0.24915165  0.1265648   0.54974711]\n",
      "[ episode 130 ][ timestamp 48 ] state=[-0.05506565 -0.24915165  0.1265648   0.54974711], action=1, reward=1.0, next_state=[-0.06004868 -0.05601345  0.13755974  0.29946787]\n",
      "[ episode 130 ][ timestamp 49 ] state=[-0.06004868 -0.05601345  0.13755974  0.29946787], action=0, reward=1.0, next_state=[-0.06116895 -0.25280066  0.14354909  0.63217542]\n",
      "[ episode 130 ][ timestamp 50 ] state=[-0.06116895 -0.25280066  0.14354909  0.63217542], action=0, reward=1.0, next_state=[-0.06622496 -0.44960267  0.1561926   0.96640077]\n",
      "[ episode 130 ][ timestamp 51 ] state=[-0.06622496 -0.44960267  0.1561926   0.96640077], action=0, reward=1.0, next_state=[-0.07521702 -0.64643814  0.17552062  1.30379391]\n",
      "[ episode 130 ][ timestamp 52 ] state=[-0.07521702 -0.64643814  0.17552062  1.30379391], action=1, reward=1.0, next_state=[-0.08814578 -0.45392197  0.2015965   1.07079496]\n",
      "[ episode 130 ][ timestamp 53 ] state=[-0.08814578 -0.45392197  0.2015965   1.07079496], action=1, reward=-1.0, next_state=[-0.09722422 -0.2619527   0.2230124   0.84754136]\n",
      "[ Ended! ] Episode 130: Exploration_rate=0.5238143793828016. Score=53.\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ] state=[ 0.01095847 -0.01497615 -0.03708367 -0.04765981]\n",
      "[ episode 131 ][ timestamp 1 ] state=[ 0.01095847 -0.01497615 -0.03708367 -0.04765981], action=1, reward=1.0, next_state=[ 0.01065895  0.18065739 -0.03803686 -0.35180846]\n",
      "[ episode 131 ][ timestamp 2 ] state=[ 0.01065895  0.18065739 -0.03803686 -0.35180846], action=0, reward=1.0, next_state=[ 0.0142721  -0.01390359 -0.04507303 -0.07135823]\n",
      "[ episode 131 ][ timestamp 3 ] state=[ 0.0142721  -0.01390359 -0.04507303 -0.07135823], action=0, reward=1.0, next_state=[ 0.01399402 -0.20835134 -0.0465002   0.20677018]\n",
      "[ episode 131 ][ timestamp 4 ] state=[ 0.01399402 -0.20835134 -0.0465002   0.20677018], action=0, reward=1.0, next_state=[ 0.009827   -0.40277857 -0.04236479  0.48442965]\n",
      "[ episode 131 ][ timestamp 5 ] state=[ 0.009827   -0.40277857 -0.04236479  0.48442965], action=1, reward=1.0, next_state=[ 0.00177143 -0.20708515 -0.0326762   0.17870138]\n",
      "[ episode 131 ][ timestamp 6 ] state=[ 0.00177143 -0.20708515 -0.0326762   0.17870138], action=1, reward=1.0, next_state=[-0.00237028 -0.0115112  -0.02910217 -0.12410804]\n",
      "[ episode 131 ][ timestamp 7 ] state=[-0.00237028 -0.0115112  -0.02910217 -0.12410804], action=0, reward=1.0, next_state=[-0.0026005  -0.2062044  -0.03158433  0.15925326]\n",
      "[ episode 131 ][ timestamp 8 ] state=[-0.0026005  -0.2062044  -0.03158433  0.15925326], action=1, reward=1.0, next_state=[-0.00672459 -0.01064483 -0.02839927 -0.14322403]\n",
      "[ episode 131 ][ timestamp 9 ] state=[-0.00672459 -0.01064483 -0.02839927 -0.14322403], action=0, reward=1.0, next_state=[-0.00693749 -0.20534879 -0.03126375  0.14036588]\n",
      "[ episode 131 ][ timestamp 10 ] state=[-0.00693749 -0.20534879 -0.03126375  0.14036588], action=1, reward=1.0, next_state=[-0.01104446 -0.00979335 -0.02845643 -0.16201398]\n",
      "[ episode 131 ][ timestamp 11 ] state=[-0.01104446 -0.00979335 -0.02845643 -0.16201398], action=0, reward=1.0, next_state=[-0.01124033 -0.20449661 -0.03169671  0.1215576 ]\n",
      "[ episode 131 ][ timestamp 12 ] state=[-0.01124033 -0.20449661 -0.03169671  0.1215576 ], action=1, reward=1.0, next_state=[-0.01533026 -0.00893521 -0.02926556 -0.18095442]\n",
      "[ episode 131 ][ timestamp 13 ] state=[-0.01533026 -0.00893521 -0.02926556 -0.18095442], action=0, reward=1.0, next_state=[-0.01550897 -0.20362643 -0.03288465  0.10235451]\n",
      "[ episode 131 ][ timestamp 14 ] state=[-0.01550897 -0.20362643 -0.03288465  0.10235451], action=1, reward=1.0, next_state=[-0.01958149 -0.00804901 -0.03083756 -0.20051936]\n",
      "[ episode 131 ][ timestamp 15 ] state=[-0.01958149 -0.00804901 -0.03083756 -0.20051936], action=0, reward=1.0, next_state=[-0.01974247 -0.20271666 -0.03484795  0.08227848]\n",
      "[ episode 131 ][ timestamp 16 ] state=[-0.01974247 -0.20271666 -0.03484795  0.08227848], action=1, reward=1.0, next_state=[-0.02379681 -0.00711294 -0.03320238 -0.22119218]\n",
      "[ episode 131 ][ timestamp 17 ] state=[-0.02379681 -0.00711294 -0.03320238 -0.22119218], action=0, reward=1.0, next_state=[-0.02393907 -0.20174496 -0.03762622  0.06083524]\n",
      "[ episode 131 ][ timestamp 18 ] state=[-0.02393907 -0.20174496 -0.03762622  0.06083524], action=1, reward=1.0, next_state=[-0.02797397 -0.0061043  -0.03640951 -0.24347755]\n",
      "[ episode 131 ][ timestamp 19 ] state=[-0.02797397 -0.0061043  -0.03640951 -0.24347755], action=0, reward=1.0, next_state=[-0.02809605 -0.20068779 -0.04127907  0.03750221]\n",
      "[ episode 131 ][ timestamp 20 ] state=[-0.02809605 -0.20068779 -0.04127907  0.03750221], action=1, reward=1.0, next_state=[-0.03210981 -0.00499895 -0.04052902 -0.2679136 ]\n",
      "[ episode 131 ][ timestamp 21 ] state=[-0.03210981 -0.00499895 -0.04052902 -0.2679136 ], action=1, reward=1.0, next_state=[-0.03220979  0.19067727 -0.04588729 -0.57309917]\n",
      "[ episode 131 ][ timestamp 22 ] state=[-0.03220979  0.19067727 -0.04588729 -0.57309917], action=0, reward=1.0, next_state=[-0.02839624 -0.00377227 -0.05734928 -0.29521801]\n",
      "[ episode 131 ][ timestamp 23 ] state=[-0.02839624 -0.00377227 -0.05734928 -0.29521801], action=1, reward=1.0, next_state=[-0.02847169  0.19211839 -0.06325364 -0.60542237]\n",
      "[ episode 131 ][ timestamp 24 ] state=[-0.02847169  0.19211839 -0.06325364 -0.60542237], action=1, reward=1.0, next_state=[-0.02462932  0.38806518 -0.07536208 -0.91733893]\n",
      "[ episode 131 ][ timestamp 25 ] state=[-0.02462932  0.38806518 -0.07536208 -0.91733893], action=1, reward=1.0, next_state=[-0.01686801  0.58412071 -0.09370886 -1.232723  ]\n",
      "[ episode 131 ][ timestamp 26 ] state=[-0.01686801  0.58412071 -0.09370886 -1.232723  ], action=0, reward=1.0, next_state=[-0.0051856   0.39032026 -0.11836332 -0.97080787]\n",
      "[ episode 131 ][ timestamp 27 ] state=[-0.0051856   0.39032026 -0.11836332 -0.97080787], action=0, reward=1.0, next_state=[ 0.0026208   0.19696871 -0.13777948 -0.71752741]\n",
      "[ episode 131 ][ timestamp 28 ] state=[ 0.0026208   0.19696871 -0.13777948 -0.71752741], action=1, reward=1.0, next_state=[ 0.00656018  0.39370118 -0.15213003 -1.05020872]\n",
      "[ episode 131 ][ timestamp 29 ] state=[ 0.00656018  0.39370118 -0.15213003 -1.05020872], action=1, reward=1.0, next_state=[ 0.0144342   0.59047766 -0.1731342  -1.38651835]\n",
      "[ episode 131 ][ timestamp 30 ] state=[ 0.0144342   0.59047766 -0.1731342  -1.38651835], action=1, reward=1.0, next_state=[ 0.02624376  0.78728282 -0.20086457 -1.72796018]\n",
      "[ episode 131 ][ timestamp 31 ] state=[ 0.02624376  0.78728282 -0.20086457 -1.72796018], action=0, reward=-1.0, next_state=[ 0.04198941  0.59494373 -0.23542377 -1.50391005]\n",
      "[ Ended! ] Episode 131: Exploration_rate=0.5211953074858876. Score=31.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 132 ] state=[ 0.00565933 -0.03695251 -0.03243902 -0.02869907]\n",
      "[ episode 132 ][ timestamp 1 ] state=[ 0.00565933 -0.03695251 -0.03243902 -0.02869907], action=1, reward=1.0, next_state=[ 0.00492028  0.15861926 -0.03301301 -0.33143779]\n",
      "[ episode 132 ][ timestamp 2 ] state=[ 0.00492028  0.15861926 -0.03301301 -0.33143779], action=0, reward=1.0, next_state=[ 0.00809267 -0.03601761 -0.03964176 -0.04934564]\n",
      "[ episode 132 ][ timestamp 3 ] state=[ 0.00809267 -0.03601761 -0.03964176 -0.04934564], action=0, reward=1.0, next_state=[ 0.00737232 -0.23054937 -0.04062867  0.23057114]\n",
      "[ episode 132 ][ timestamp 4 ] state=[ 0.00737232 -0.23054937 -0.04062867  0.23057114], action=1, reward=1.0, next_state=[ 0.00276133 -0.03487111 -0.03601725 -0.07464557]\n",
      "[ episode 132 ][ timestamp 5 ] state=[ 0.00276133 -0.03487111 -0.03601725 -0.07464557], action=0, reward=1.0, next_state=[ 0.00206391 -0.2294587  -0.03751016  0.20645973]\n",
      "[ episode 132 ][ timestamp 6 ] state=[ 0.00206391 -0.2294587  -0.03751016  0.20645973], action=1, reward=1.0, next_state=[-0.00252527 -0.033821   -0.03338097 -0.0978158 ]\n",
      "[ episode 132 ][ timestamp 7 ] state=[-0.00252527 -0.033821   -0.03338097 -0.0978158 ], action=1, reward=1.0, next_state=[-0.00320169  0.16176308 -0.03533728 -0.40084066]\n",
      "[ episode 132 ][ timestamp 8 ] state=[-0.00320169  0.16176308 -0.03533728 -0.40084066], action=0, reward=1.0, next_state=[ 3.35730848e-05 -3.28402709e-02 -4.33540968e-02 -1.19504871e-01]\n",
      "[ episode 132 ][ timestamp 9 ] state=[ 3.35730848e-05 -3.28402709e-02 -4.33540968e-02 -1.19504871e-01], action=1, reward=1.0, next_state=[-0.00062323  0.16287516 -0.04574419 -0.42554427]\n",
      "[ episode 132 ][ timestamp 10 ] state=[-0.00062323  0.16287516 -0.04574419 -0.42554427], action=0, reward=1.0, next_state=[ 0.00263427 -0.03157    -0.05425508 -0.14762575]\n",
      "[ episode 132 ][ timestamp 11 ] state=[ 0.00263427 -0.03157    -0.05425508 -0.14762575], action=0, reward=1.0, next_state=[ 0.00200287 -0.22587472 -0.05720759  0.1274593 ]\n",
      "[ episode 132 ][ timestamp 12 ] state=[ 0.00200287 -0.22587472 -0.05720759  0.1274593 ], action=0, reward=1.0, next_state=[-0.00251462 -0.42013246 -0.05465841  0.40155936]\n",
      "[ episode 132 ][ timestamp 13 ] state=[-0.00251462 -0.42013246 -0.05465841  0.40155936], action=1, reward=1.0, next_state=[-0.01091727 -0.22427955 -0.04662722  0.09215716]\n",
      "[ episode 132 ][ timestamp 14 ] state=[-0.01091727 -0.22427955 -0.04662722  0.09215716], action=0, reward=1.0, next_state=[-0.01540286 -0.41870325 -0.04478408  0.36977231]\n",
      "[ episode 132 ][ timestamp 15 ] state=[-0.01540286 -0.41870325 -0.04478408  0.36977231], action=1, reward=1.0, next_state=[-0.02377693 -0.22297456 -0.03738863  0.06331153]\n",
      "[ episode 132 ][ timestamp 16 ] state=[-0.02377693 -0.22297456 -0.03738863  0.06331153], action=0, reward=1.0, next_state=[-0.02823642 -0.41754106 -0.0361224   0.34396761]\n",
      "[ episode 132 ][ timestamp 17 ] state=[-0.02823642 -0.41754106 -0.0361224   0.34396761], action=1, reward=1.0, next_state=[-0.03658724 -0.22192435 -0.02924305  0.04011628]\n",
      "[ episode 132 ][ timestamp 18 ] state=[-0.03658724 -0.22192435 -0.02924305  0.04011628], action=0, reward=1.0, next_state=[-0.04102573 -0.41661502 -0.02844072  0.32343119]\n",
      "[ episode 132 ][ timestamp 19 ] state=[-0.04102573 -0.41661502 -0.02844072  0.32343119], action=1, reward=1.0, next_state=[-0.04935803 -0.22109987 -0.0219721   0.02191664]\n",
      "[ episode 132 ][ timestamp 20 ] state=[-0.04935803 -0.22109987 -0.0219721   0.02191664], action=0, reward=1.0, next_state=[-0.05378003 -0.41589995 -0.02153377  0.30758693]\n",
      "[ episode 132 ][ timestamp 21 ] state=[-0.05378003 -0.41589995 -0.02153377  0.30758693], action=1, reward=1.0, next_state=[-0.06209802 -0.22047788 -0.01538203  0.00819136]\n",
      "[ episode 132 ][ timestamp 22 ] state=[-0.06209802 -0.22047788 -0.01538203  0.00819136], action=0, reward=1.0, next_state=[-0.06650758 -0.41537589 -0.0152182   0.29598166]\n",
      "[ episode 132 ][ timestamp 23 ] state=[-0.06650758 -0.41537589 -0.0152182   0.29598166], action=1, reward=1.0, next_state=[-0.0748151  -0.22004033 -0.00929857 -0.00146173]\n",
      "[ episode 132 ][ timestamp 24 ] state=[-0.0748151  -0.22004033 -0.00929857 -0.00146173], action=0, reward=1.0, next_state=[-0.07921591 -0.41502771 -0.0093278   0.28827294]\n",
      "[ episode 132 ][ timestamp 25 ] state=[-0.07921591 -0.41502771 -0.0093278   0.28827294], action=1, reward=1.0, next_state=[-0.08751646 -0.21977399 -0.00356234 -0.00733723]\n",
      "[ episode 132 ][ timestamp 26 ] state=[-0.08751646 -0.21977399 -0.00356234 -0.00733723], action=1, reward=1.0, next_state=[-0.09191194 -0.02460113 -0.00370909 -0.30114199]\n",
      "[ episode 132 ][ timestamp 27 ] state=[-0.09191194 -0.02460113 -0.00370909 -0.30114199], action=0, reward=1.0, next_state=[-0.09240396 -0.21967002 -0.00973193 -0.00963114]\n",
      "[ episode 132 ][ timestamp 28 ] state=[-0.09240396 -0.21967002 -0.00973193 -0.00963114], action=0, reward=1.0, next_state=[-0.09679736 -0.41465106 -0.00992455  0.27996543]\n",
      "[ episode 132 ][ timestamp 29 ] state=[-0.09679736 -0.41465106 -0.00992455  0.27996543], action=1, reward=1.0, next_state=[-0.10509038 -0.21938895 -0.00432524 -0.01583108]\n",
      "[ episode 132 ][ timestamp 30 ] state=[-0.10509038 -0.21938895 -0.00432524 -0.01583108], action=0, reward=1.0, next_state=[-0.10947816 -0.4144486  -0.00464186  0.27548405]\n",
      "[ episode 132 ][ timestamp 31 ] state=[-0.10947816 -0.4144486  -0.00464186  0.27548405], action=1, reward=1.0, next_state=[-0.11776714 -0.21926073  0.00086782 -0.0186593 ]\n",
      "[ episode 132 ][ timestamp 32 ] state=[-0.11776714 -0.21926073  0.00086782 -0.0186593 ], action=0, reward=1.0, next_state=[-0.12215235 -0.41439512  0.00049463  0.27429731]\n",
      "[ episode 132 ][ timestamp 33 ] state=[-0.12215235 -0.41439512  0.00049463  0.27429731], action=0, reward=1.0, next_state=[-0.13044025 -0.60952413  0.00598058  0.5671362 ]\n",
      "[ episode 132 ][ timestamp 34 ] state=[-0.13044025 -0.60952413  0.00598058  0.5671362 ], action=1, reward=1.0, next_state=[-0.14263074 -0.41448658  0.0173233   0.27634339]\n",
      "[ episode 132 ][ timestamp 35 ] state=[-0.14263074 -0.41448658  0.0173233   0.27634339], action=1, reward=1.0, next_state=[-0.15092047 -0.21961601  0.02285017 -0.01082582]\n",
      "[ episode 132 ][ timestamp 36 ] state=[-0.15092047 -0.21961601  0.02285017 -0.01082582], action=1, reward=1.0, next_state=[-0.15531279 -0.02482908  0.02263365 -0.29621256]\n",
      "[ episode 132 ][ timestamp 37 ] state=[-0.15531279 -0.02482908  0.02263365 -0.29621256], action=0, reward=1.0, next_state=[-0.15580937 -0.22026626  0.0167094   0.00352185]\n",
      "[ episode 132 ][ timestamp 38 ] state=[-0.15580937 -0.22026626  0.0167094   0.00352185], action=0, reward=1.0, next_state=[-0.16021469 -0.41562381  0.01677984  0.30142961]\n",
      "[ episode 132 ][ timestamp 39 ] state=[-0.16021469 -0.41562381  0.01677984  0.30142961], action=1, reward=1.0, next_state=[-0.16852717 -0.22074499  0.02280843  0.01408557]\n",
      "[ episode 132 ][ timestamp 40 ] state=[-0.16852717 -0.22074499  0.02280843  0.01408557], action=0, reward=1.0, next_state=[-0.17294207 -0.41618649  0.02309014  0.31387666]\n",
      "[ episode 132 ][ timestamp 41 ] state=[-0.17294207 -0.41618649  0.02309014  0.31387666], action=1, reward=1.0, next_state=[-0.1812658  -0.22140095  0.02936768  0.02856413]\n",
      "[ episode 132 ][ timestamp 42 ] state=[-0.1812658  -0.22140095  0.02936768  0.02856413], action=0, reward=1.0, next_state=[-0.18569382 -0.41693147  0.02993896  0.33036631]\n",
      "[ episode 132 ][ timestamp 43 ] state=[-0.18569382 -0.41693147  0.02993896  0.33036631], action=1, reward=1.0, next_state=[-0.19403245 -0.22224821  0.03654628  0.04727302]\n",
      "[ episode 132 ][ timestamp 44 ] state=[-0.19403245 -0.22224821  0.03654628  0.04727302], action=0, reward=1.0, next_state=[-0.19847741 -0.41787464  0.03749174  0.35125893]\n",
      "[ episode 132 ][ timestamp 45 ] state=[-0.19847741 -0.41787464  0.03749174  0.35125893], action=1, reward=1.0, next_state=[-0.20683491 -0.22330536  0.04451692  0.07063009]\n",
      "[ episode 132 ][ timestamp 46 ] state=[-0.20683491 -0.22330536  0.04451692  0.07063009], action=1, reward=1.0, next_state=[-0.21130101 -0.02884896  0.04592952 -0.20768187]\n",
      "[ episode 132 ][ timestamp 47 ] state=[-0.21130101 -0.02884896  0.04592952 -0.20768187], action=0, reward=1.0, next_state=[-0.21187799 -0.22459656  0.04177589  0.09912841]\n",
      "[ episode 132 ][ timestamp 48 ] state=[-0.21187799 -0.22459656  0.04177589  0.09912841], action=0, reward=1.0, next_state=[-0.21636992 -0.42029156  0.04375846  0.40469335]\n",
      "[ episode 132 ][ timestamp 49 ] state=[-0.21636992 -0.42029156  0.04375846  0.40469335], action=0, reward=1.0, next_state=[-0.22477575 -0.61600588  0.05185232  0.71084469]\n",
      "[ episode 132 ][ timestamp 50 ] state=[-0.22477575 -0.61600588  0.05185232  0.71084469], action=1, reward=1.0, next_state=[-0.23709587 -0.42163889  0.06606922  0.4349238 ]\n",
      "[ episode 132 ][ timestamp 51 ] state=[-0.23709587 -0.42163889  0.06606922  0.4349238 ], action=0, reward=1.0, next_state=[-0.24552865 -0.61763092  0.07476769  0.74768066]\n",
      "[ episode 132 ][ timestamp 52 ] state=[-0.24552865 -0.61763092  0.07476769  0.74768066], action=0, reward=1.0, next_state=[-0.25788127 -0.81370037  0.08972131  1.06292439]\n",
      "[ episode 132 ][ timestamp 53 ] state=[-0.25788127 -0.81370037  0.08972131  1.06292439], action=1, reward=1.0, next_state=[-0.27415528 -0.61987336  0.11097979  0.79969599]\n",
      "[ episode 132 ][ timestamp 54 ] state=[-0.27415528 -0.61987336  0.11097979  0.79969599], action=0, reward=1.0, next_state=[-0.28655274 -0.81632846  0.12697371  1.1251269 ]\n",
      "[ episode 132 ][ timestamp 55 ] state=[-0.28655274 -0.81632846  0.12697371  1.1251269 ], action=1, reward=1.0, next_state=[-0.30287931 -0.62307835  0.14947625  0.87481536]\n",
      "[ episode 132 ][ timestamp 56 ] state=[-0.30287931 -0.62307835  0.14947625  0.87481536], action=1, reward=1.0, next_state=[-0.31534088 -0.43027     0.16697256  0.63261034]\n",
      "[ episode 132 ][ timestamp 57 ] state=[-0.31534088 -0.43027     0.16697256  0.63261034], action=0, reward=1.0, next_state=[-0.32394628 -0.62727926  0.17962477  0.9728765 ]\n",
      "[ episode 132 ][ timestamp 58 ] state=[-0.32394628 -0.62727926  0.17962477  0.9728765 ], action=1, reward=1.0, next_state=[-0.33649186 -0.43496253  0.1990823   0.74156889]\n",
      "[ episode 132 ][ timestamp 59 ] state=[-0.33649186 -0.43496253  0.1990823   0.74156889], action=1, reward=-1.0, next_state=[-0.34519111 -0.24306396  0.21391367  0.5175508 ]\n",
      "[ Ended! ] Episode 132: Exploration_rate=0.5185893309484582. Score=59.\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ] state=[-0.03961441  0.04094627  0.00965788  0.01862216]\n",
      "[ episode 133 ][ timestamp 1 ] state=[-0.03961441  0.04094627  0.00965788  0.01862216], action=0, reward=1.0, next_state=[-0.03879549 -0.15431284  0.01003033  0.31433655]\n",
      "[ episode 133 ][ timestamp 2 ] state=[-0.03879549 -0.15431284  0.01003033  0.31433655], action=0, reward=1.0, next_state=[-0.04188174 -0.34957623  0.01631706  0.61016576]\n",
      "[ episode 133 ][ timestamp 3 ] state=[-0.04188174 -0.34957623  0.01631706  0.61016576], action=1, reward=1.0, next_state=[-0.04887327 -0.15468612  0.02852037  0.32266651]\n",
      "[ episode 133 ][ timestamp 4 ] state=[-0.04887327 -0.15468612  0.02852037  0.32266651], action=0, reward=1.0, next_state=[-0.05196699 -0.35020234  0.0349737   0.62420543]\n",
      "[ episode 133 ][ timestamp 5 ] state=[-0.05196699 -0.35020234  0.0349737   0.62420543], action=1, reward=1.0, next_state=[-0.05897104 -0.15558567  0.04745781  0.34273912]\n",
      "[ episode 133 ][ timestamp 6 ] state=[-0.05897104 -0.15558567  0.04745781  0.34273912], action=1, reward=1.0, next_state=[-0.06208275  0.03883011  0.05431259  0.06539115]\n",
      "[ episode 133 ][ timestamp 7 ] state=[-0.06208275  0.03883011  0.05431259  0.06539115], action=0, reward=1.0, next_state=[-0.06130615 -0.15702678  0.05562042  0.37470332]\n",
      "[ episode 133 ][ timestamp 8 ] state=[-0.06130615 -0.15702678  0.05562042  0.37470332], action=1, reward=1.0, next_state=[-0.06444669  0.03726282  0.06311448  0.10006358]\n",
      "[ episode 133 ][ timestamp 9 ] state=[-0.06444669  0.03726282  0.06311448  0.10006358], action=0, reward=1.0, next_state=[-0.06370143 -0.15870422  0.06511576  0.41197222]\n",
      "[ episode 133 ][ timestamp 10 ] state=[-0.06370143 -0.15870422  0.06511576  0.41197222], action=0, reward=1.0, next_state=[-0.06687551 -0.3546859   0.0733552   0.72445223]\n",
      "[ episode 133 ][ timestamp 11 ] state=[-0.06687551 -0.3546859   0.0733552   0.72445223], action=0, reward=1.0, next_state=[-0.07396923 -0.55074152  0.08784424  1.03929188]\n",
      "[ episode 133 ][ timestamp 12 ] state=[-0.07396923 -0.55074152  0.08784424  1.03929188], action=0, reward=1.0, next_state=[-0.08498406 -0.74691389  0.10863008  1.35820882]\n",
      "[ episode 133 ][ timestamp 13 ] state=[-0.08498406 -0.74691389  0.10863008  1.35820882], action=1, reward=1.0, next_state=[-0.09992234 -0.55330901  0.13579426  1.10138777]\n",
      "[ episode 133 ][ timestamp 14 ] state=[-0.09992234 -0.55330901  0.13579426  1.10138777], action=1, reward=1.0, next_state=[-0.11098852 -0.36020943  0.15782201  0.8542058 ]\n",
      "[ episode 133 ][ timestamp 15 ] state=[-0.11098852 -0.36020943  0.15782201  0.8542058 ], action=0, reward=1.0, next_state=[-0.11819271 -0.55708965  0.17490613  1.19206316]\n",
      "[ episode 133 ][ timestamp 16 ] state=[-0.11819271 -0.55708965  0.17490613  1.19206316], action=1, reward=1.0, next_state=[-0.1293345  -0.36461049  0.19874739  0.95891006]\n",
      "[ episode 133 ][ timestamp 17 ] state=[-0.1293345  -0.36461049  0.19874739  0.95891006], action=0, reward=-1.0, next_state=[-0.13662671 -0.5617688   0.21792559  1.30687364]\n",
      "[ Ended! ] Episode 133: Exploration_rate=0.5159963842937159. Score=17.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 134 ] state=[-0.02015259  0.04698064 -0.0248349   0.03637913]\n",
      "[ episode 134 ][ timestamp 1 ] state=[-0.02015259  0.04698064 -0.0248349   0.03637913], action=0, reward=1.0, next_state=[-0.01921298 -0.14777653 -0.02410731  0.32112409]\n",
      "[ episode 134 ][ timestamp 2 ] state=[-0.01921298 -0.14777653 -0.02410731  0.32112409], action=0, reward=1.0, next_state=[-0.02216851 -0.34254703 -0.01768483  0.60610808]\n",
      "[ episode 134 ][ timestamp 3 ] state=[-0.02216851 -0.34254703 -0.01768483  0.60610808], action=1, reward=1.0, next_state=[-0.02901945 -0.14718231 -0.00556267  0.30790776]\n",
      "[ episode 134 ][ timestamp 4 ] state=[-0.02901945 -0.14718231 -0.00556267  0.30790776], action=1, reward=1.0, next_state=[-0.0319631   0.04801846  0.00059548  0.01347572]\n",
      "[ episode 134 ][ timestamp 5 ] state=[-0.0319631   0.04801846  0.00059548  0.01347572], action=0, reward=1.0, next_state=[-0.03100273 -0.14711203  0.000865    0.30634647]\n",
      "[ episode 134 ][ timestamp 6 ] state=[-0.03100273 -0.14711203  0.000865    0.30634647], action=1, reward=1.0, next_state=[-0.03394497  0.04799759  0.00699193  0.01393647]\n",
      "[ episode 134 ][ timestamp 7 ] state=[-0.03394497  0.04799759  0.00699193  0.01393647], action=0, reward=1.0, next_state=[-0.03298502 -0.14722394  0.00727066  0.30881721]\n",
      "[ episode 134 ][ timestamp 8 ] state=[-0.03298502 -0.14722394  0.00727066  0.30881721], action=1, reward=1.0, next_state=[-0.0359295   0.04779367  0.013447    0.01843608]\n",
      "[ episode 134 ][ timestamp 9 ] state=[-0.0359295   0.04779367  0.013447    0.01843608], action=1, reward=1.0, next_state=[-0.03497362  0.24272022  0.01381572 -0.26997401]\n",
      "[ episode 134 ][ timestamp 10 ] state=[-0.03497362  0.24272022  0.01381572 -0.26997401], action=0, reward=1.0, next_state=[-0.03011922  0.04740387  0.00841624  0.02703426]\n",
      "[ episode 134 ][ timestamp 11 ] state=[-0.03011922  0.04740387  0.00841624  0.02703426], action=0, reward=1.0, next_state=[-0.02917114 -0.14783776  0.00895693  0.32236067]\n",
      "[ episode 134 ][ timestamp 12 ] state=[-0.02917114 -0.14783776  0.00895693  0.32236067], action=1, reward=1.0, next_state=[-0.0321279   0.0471555   0.01540414  0.03251581]\n",
      "[ episode 134 ][ timestamp 13 ] state=[-0.0321279   0.0471555   0.01540414  0.03251581], action=0, reward=1.0, next_state=[-0.03118479 -0.14818392  0.01605446  0.33001882]\n",
      "[ episode 134 ][ timestamp 14 ] state=[-0.03118479 -0.14818392  0.01605446  0.33001882], action=0, reward=1.0, next_state=[-0.03414847 -0.34353069  0.02265483  0.62772102]\n",
      "[ episode 134 ][ timestamp 15 ] state=[-0.03414847 -0.34353069  0.02265483  0.62772102], action=1, reward=1.0, next_state=[-0.04101908 -0.14873214  0.03520925  0.34225812]\n",
      "[ episode 134 ][ timestamp 16 ] state=[-0.04101908 -0.14873214  0.03520925  0.34225812], action=1, reward=1.0, next_state=[-0.04399372  0.04587166  0.04205442  0.06088273]\n",
      "[ episode 134 ][ timestamp 17 ] state=[-0.04399372  0.04587166  0.04205442  0.06088273], action=0, reward=1.0, next_state=[-0.04307629 -0.14982724  0.04327207  0.3665319 ]\n",
      "[ episode 134 ][ timestamp 18 ] state=[-0.04307629 -0.14982724  0.04327207  0.3665319 ], action=0, reward=1.0, next_state=[-0.04607283 -0.34553654  0.05060271  0.67253905]\n",
      "[ episode 134 ][ timestamp 19 ] state=[-0.04607283 -0.34553654  0.05060271  0.67253905], action=0, reward=1.0, next_state=[-0.05298357 -0.541324    0.06405349  0.98071516]\n",
      "[ episode 134 ][ timestamp 20 ] state=[-0.05298357 -0.541324    0.06405349  0.98071516], action=1, reward=1.0, next_state=[-0.06381005 -0.34711632  0.08366779  0.70881989]\n",
      "[ episode 134 ][ timestamp 21 ] state=[-0.06381005 -0.34711632  0.08366779  0.70881989], action=0, reward=1.0, next_state=[-0.07075237 -0.54329138  0.09784419  1.02662277]\n",
      "[ episode 134 ][ timestamp 22 ] state=[-0.07075237 -0.54329138  0.09784419  1.02662277], action=1, reward=1.0, next_state=[-0.0816182  -0.34959852  0.11837665  0.76619342]\n",
      "[ episode 134 ][ timestamp 23 ] state=[-0.0816182  -0.34959852  0.11837665  0.76619342], action=1, reward=1.0, next_state=[-0.08861017 -0.15628809  0.13370052  0.51297857]\n",
      "[ episode 134 ][ timestamp 24 ] state=[-0.08861017 -0.15628809  0.13370052  0.51297857], action=1, reward=1.0, next_state=[-0.09173593  0.03672243  0.14396009  0.26523754]\n",
      "[ episode 134 ][ timestamp 25 ] state=[-0.09173593  0.03672243  0.14396009  0.26523754], action=1, reward=1.0, next_state=[-0.09100148  0.22952768  0.14926484  0.02119957]\n",
      "[ episode 134 ][ timestamp 26 ] state=[-0.09100148  0.22952768  0.14926484  0.02119957], action=1, reward=1.0, next_state=[-0.08641093  0.42222885  0.14968883 -0.22091704]\n",
      "[ episode 134 ][ timestamp 27 ] state=[-0.08641093  0.42222885  0.14968883 -0.22091704], action=0, reward=1.0, next_state=[-0.07796635  0.2253196   0.14527049  0.11498829]\n",
      "[ episode 134 ][ timestamp 28 ] state=[-0.07796635  0.2253196   0.14527049  0.11498829], action=1, reward=1.0, next_state=[-0.07345996  0.41809363  0.14757025 -0.12856749]\n",
      "[ episode 134 ][ timestamp 29 ] state=[-0.07345996  0.41809363  0.14757025 -0.12856749], action=0, reward=1.0, next_state=[-0.06509809  0.22119974  0.1449989   0.20679172]\n",
      "[ episode 134 ][ timestamp 30 ] state=[-0.06509809  0.22119974  0.1449989   0.20679172], action=0, reward=1.0, next_state=[-0.06067409  0.02433408  0.14913474  0.54147182]\n",
      "[ episode 134 ][ timestamp 31 ] state=[-0.06067409  0.02433408  0.14913474  0.54147182], action=1, reward=1.0, next_state=[-0.06018741  0.21707994  0.15996418  0.29924551]\n",
      "[ episode 134 ][ timestamp 32 ] state=[-0.06018741  0.21707994  0.15996418  0.29924551], action=0, reward=1.0, next_state=[-0.05584581  0.02008243  0.16594909  0.63779833]\n",
      "[ episode 134 ][ timestamp 33 ] state=[-0.05584581  0.02008243  0.16594909  0.63779833], action=0, reward=1.0, next_state=[-0.05544416 -0.17691713  0.17870505  0.9778035 ]\n",
      "[ episode 134 ][ timestamp 34 ] state=[-0.05544416 -0.17691713  0.17870505  0.9778035 ], action=1, reward=1.0, next_state=[-0.05898251  0.01541728  0.19826112  0.74615645]\n",
      "[ episode 134 ][ timestamp 35 ] state=[-0.05898251  0.01541728  0.19826112  0.74615645], action=1, reward=-1.0, next_state=[-0.05867416  0.20733201  0.21318425  0.52183127]\n",
      "[ Ended! ] Episode 134: Exploration_rate=0.5134164023722473. Score=35.\n",
      "[ Experience replay ] starts\n",
      "[ episode 135 ] state=[ 0.04586912 -0.02857769 -0.03946805 -0.03769281]\n",
      "[ episode 135 ][ timestamp 1 ] state=[ 0.04586912 -0.02857769 -0.03946805 -0.03769281], action=0, reward=1.0, next_state=[ 0.04529757 -0.22311209 -0.0402219   0.24228095]\n",
      "[ episode 135 ][ timestamp 2 ] state=[ 0.04529757 -0.22311209 -0.0402219   0.24228095], action=1, reward=1.0, next_state=[ 0.04083533 -0.02743938 -0.03537628 -0.06281279]\n",
      "[ episode 135 ][ timestamp 3 ] state=[ 0.04083533 -0.02743938 -0.03537628 -0.06281279], action=1, reward=1.0, next_state=[ 0.04028654  0.16817146 -0.03663254 -0.36644391]\n",
      "[ episode 135 ][ timestamp 4 ] state=[ 0.04028654  0.16817146 -0.03663254 -0.36644391], action=1, reward=1.0, next_state=[ 0.04364997  0.36379431 -0.04396142 -0.67044888]\n",
      "[ episode 135 ][ timestamp 5 ] state=[ 0.04364997  0.36379431 -0.04396142 -0.67044888], action=1, reward=1.0, next_state=[ 0.05092585  0.55949899 -0.0573704  -0.97664278]\n",
      "[ episode 135 ][ timestamp 6 ] state=[ 0.05092585  0.55949899 -0.0573704  -0.97664278], action=0, reward=1.0, next_state=[ 0.06211583  0.36519139 -0.07690325 -0.70251855]\n",
      "[ episode 135 ][ timestamp 7 ] state=[ 0.06211583  0.36519139 -0.07690325 -0.70251855], action=1, reward=1.0, next_state=[ 0.06941966  0.56129022 -0.09095362 -1.01838469]\n",
      "[ episode 135 ][ timestamp 8 ] state=[ 0.06941966  0.56129022 -0.09095362 -1.01838469], action=1, reward=1.0, next_state=[ 0.08064547  0.75749906 -0.11132132 -1.33818493]\n",
      "[ episode 135 ][ timestamp 9 ] state=[ 0.08064547  0.75749906 -0.11132132 -1.33818493], action=0, reward=1.0, next_state=[ 0.09579545  0.56394117 -0.13808502 -1.08230613]\n",
      "[ episode 135 ][ timestamp 10 ] state=[ 0.09579545  0.56394117 -0.13808502 -1.08230613], action=1, reward=1.0, next_state=[ 0.10707427  0.76058846 -0.15973114 -1.41493746]\n",
      "[ episode 135 ][ timestamp 11 ] state=[ 0.10707427  0.76058846 -0.15973114 -1.41493746], action=0, reward=1.0, next_state=[ 0.12228604  0.56776478 -0.18802989 -1.17614541]\n",
      "[ episode 135 ][ timestamp 12 ] state=[ 0.12228604  0.56776478 -0.18802989 -1.17614541], action=1, reward=-1.0, next_state=[ 0.13364134  0.76476432 -0.2115528  -1.52139199]\n",
      "[ Ended! ] Episode 135: Exploration_rate=0.510849320360386. Score=12.\n",
      "[ Experience replay ] starts\n",
      "[ episode 136 ] state=[-0.00462421  0.03435132 -0.03151275  0.00561057]\n",
      "[ episode 136 ][ timestamp 1 ] state=[-0.00462421  0.03435132 -0.03151275  0.00561057], action=0, reward=1.0, next_state=[-0.00393719 -0.16030485 -0.03140054  0.28818665]\n",
      "[ episode 136 ][ timestamp 2 ] state=[-0.00393719 -0.16030485 -0.03140054  0.28818665], action=0, reward=1.0, next_state=[-0.00714328 -0.35496527 -0.02563681  0.5708031 ]\n",
      "[ episode 136 ][ timestamp 3 ] state=[-0.00714328 -0.35496527 -0.02563681  0.5708031 ], action=0, reward=1.0, next_state=[-0.01424259 -0.5497185  -0.01422074  0.85530055]\n",
      "[ episode 136 ][ timestamp 4 ] state=[-0.01424259 -0.5497185  -0.01422074  0.85530055], action=0, reward=1.0, next_state=[-0.02523696 -0.7446438   0.00288527  1.14347817]\n",
      "[ episode 136 ][ timestamp 5 ] state=[-0.02523696 -0.7446438   0.00288527  1.14347817], action=1, reward=1.0, next_state=[-0.04012983 -0.54955966  0.02575483  0.85170146]\n",
      "[ episode 136 ][ timestamp 6 ] state=[-0.04012983 -0.54955966  0.02575483  0.85170146], action=1, reward=1.0, next_state=[-0.05112103 -0.35479814  0.04278886  0.56722713]\n",
      "[ episode 136 ][ timestamp 7 ] state=[-0.05112103 -0.35479814  0.04278886  0.56722713], action=1, reward=1.0, next_state=[-0.05821699 -0.1603017   0.0541334   0.28832561]\n",
      "[ episode 136 ][ timestamp 8 ] state=[-0.05821699 -0.1603017   0.0541334   0.28832561], action=1, reward=1.0, next_state=[-0.06142302  0.03400818  0.05989991  0.01319519]\n",
      "[ episode 136 ][ timestamp 9 ] state=[-0.06142302  0.03400818  0.05989991  0.01319519], action=0, reward=1.0, next_state=[-0.06074286 -0.16191939  0.06016382  0.3241595 ]\n",
      "[ episode 136 ][ timestamp 10 ] state=[-0.06074286 -0.16191939  0.06016382  0.3241595 ], action=0, reward=1.0, next_state=[-0.06398125 -0.3578441   0.06664701  0.63519233]\n",
      "[ episode 136 ][ timestamp 11 ] state=[-0.06398125 -0.3578441   0.06664701  0.63519233], action=0, reward=1.0, next_state=[-0.07113813 -0.55382922  0.07935085  0.94809708]\n",
      "[ episode 136 ][ timestamp 12 ] state=[-0.07113813 -0.55382922  0.07935085  0.94809708], action=1, reward=1.0, next_state=[-0.08221471 -0.35986017  0.0983128   0.6813637 ]\n",
      "[ episode 136 ][ timestamp 13 ] state=[-0.08221471 -0.35986017  0.0983128   0.6813637 ], action=1, reward=1.0, next_state=[-0.08941192 -0.16623114  0.11194007  0.42118008]\n",
      "[ episode 136 ][ timestamp 14 ] state=[-0.08941192 -0.16623114  0.11194007  0.42118008], action=0, reward=1.0, next_state=[-0.09273654 -0.36274634  0.12036367  0.74694966]\n",
      "[ episode 136 ][ timestamp 15 ] state=[-0.09273654 -0.36274634  0.12036367  0.74694966], action=0, reward=1.0, next_state=[-0.09999147 -0.55930514  0.13530266  1.07495625]\n",
      "[ episode 136 ][ timestamp 16 ] state=[-0.09999147 -0.55930514  0.13530266  1.07495625], action=0, reward=1.0, next_state=[-0.11117757 -0.75593009  0.15680179  1.40685583]\n",
      "[ episode 136 ][ timestamp 17 ] state=[-0.11117757 -0.75593009  0.15680179  1.40685583], action=1, reward=1.0, next_state=[-0.12629617 -0.56306283  0.18493891  1.16701519]\n",
      "[ episode 136 ][ timestamp 18 ] state=[-0.12629617 -0.56306283  0.18493891  1.16701519], action=1, reward=1.0, next_state=[-0.13755743 -0.37076456  0.20827921  0.93754917]\n",
      "[ episode 136 ][ timestamp 19 ] state=[-0.13755743 -0.37076456  0.20827921  0.93754917], action=1, reward=-1.0, next_state=[-0.14497272 -0.17896672  0.22703019  0.71686239]\n",
      "[ Ended! ] Episode 136: Exploration_rate=0.5082950737585841. Score=19.\n",
      "[ Experience replay ] starts\n",
      "[ episode 137 ] state=[-0.04011504  0.00272797  0.02955492 -0.00266256]\n",
      "[ episode 137 ][ timestamp 1 ] state=[-0.04011504  0.00272797  0.02955492 -0.00266256], action=0, reward=1.0, next_state=[-0.04006048 -0.1928051   0.02950167  0.29919684]\n",
      "[ episode 137 ][ timestamp 2 ] state=[-0.04006048 -0.1928051   0.02950167  0.29919684], action=0, reward=1.0, next_state=[-0.04391658 -0.38833488  0.03548561  0.60103611]\n",
      "[ episode 137 ][ timestamp 3 ] state=[-0.04391658 -0.38833488  0.03548561  0.60103611], action=0, reward=1.0, next_state=[-0.05168328 -0.5839348   0.04750633  0.90468187]\n",
      "[ episode 137 ][ timestamp 4 ] state=[-0.05168328 -0.5839348   0.04750633  0.90468187], action=1, reward=1.0, next_state=[-0.06336197 -0.38948731  0.06559997  0.62730129]\n",
      "[ episode 137 ][ timestamp 5 ] state=[-0.06336197 -0.38948731  0.06559997  0.62730129], action=1, reward=1.0, next_state=[-0.07115172 -0.19533934  0.078146    0.35597829]\n",
      "[ episode 137 ][ timestamp 6 ] state=[-0.07115172 -0.19533934  0.078146    0.35597829], action=0, reward=1.0, next_state=[-0.07505851 -0.39148034  0.08526556  0.67224346]\n",
      "[ episode 137 ][ timestamp 7 ] state=[-0.07505851 -0.39148034  0.08526556  0.67224346], action=1, reward=1.0, next_state=[-0.08288811 -0.19764063  0.09871043  0.4075779 ]\n",
      "[ episode 137 ][ timestamp 8 ] state=[-0.08288811 -0.19764063  0.09871043  0.4075779 ], action=1, reward=1.0, next_state=[-0.08684093 -0.00404675  0.10686199  0.14757445]\n",
      "[ episode 137 ][ timestamp 9 ] state=[-0.08684093 -0.00404675  0.10686199  0.14757445], action=1, reward=1.0, next_state=[-0.08692186  0.18939553  0.10981348 -0.10957613]\n",
      "[ episode 137 ][ timestamp 10 ] state=[-0.08692186  0.18939553  0.10981348 -0.10957613], action=0, reward=1.0, next_state=[-0.08313395 -0.0071147   0.10762196  0.21563402]\n",
      "[ episode 137 ][ timestamp 11 ] state=[-0.08313395 -0.0071147   0.10762196  0.21563402], action=0, reward=1.0, next_state=[-0.08327624 -0.20359746  0.11193464  0.54023281]\n",
      "[ episode 137 ][ timestamp 12 ] state=[-0.08327624 -0.20359746  0.11193464  0.54023281], action=1, reward=1.0, next_state=[-0.08734819 -0.01021219  0.12273929  0.28481036]\n",
      "[ episode 137 ][ timestamp 13 ] state=[-0.08734819 -0.01021219  0.12273929  0.28481036], action=0, reward=1.0, next_state=[-0.08755244 -0.20685142  0.1284355   0.61354503]\n",
      "[ episode 137 ][ timestamp 14 ] state=[-0.08755244 -0.20685142  0.1284355   0.61354503], action=1, reward=1.0, next_state=[-0.09168947 -0.013736    0.1407064   0.36391411]\n",
      "[ episode 137 ][ timestamp 15 ] state=[-0.09168947 -0.013736    0.1407064   0.36391411], action=0, reward=1.0, next_state=[-0.09196419 -0.21054801  0.14798468  0.69744584]\n",
      "[ episode 137 ][ timestamp 16 ] state=[-0.09196419 -0.21054801  0.14798468  0.69744584], action=1, reward=1.0, next_state=[-0.09617515 -0.01775423  0.1619336   0.45476484]\n",
      "[ episode 137 ][ timestamp 17 ] state=[-0.09617515 -0.01775423  0.1619336   0.45476484], action=0, reward=1.0, next_state=[-0.09653023 -0.21475094  0.1710289   0.79379474]\n",
      "[ episode 137 ][ timestamp 18 ] state=[-0.09653023 -0.21475094  0.1710289   0.79379474], action=1, reward=1.0, next_state=[-0.10082525 -0.02233744  0.18690479  0.55942313]\n",
      "[ episode 137 ][ timestamp 19 ] state=[-0.10082525 -0.02233744  0.18690479  0.55942313], action=1, reward=1.0, next_state=[-0.101272    0.16973747  0.19809325  0.33095913]\n",
      "[ episode 137 ][ timestamp 20 ] state=[-0.101272    0.16973747  0.19809325  0.33095913], action=0, reward=1.0, next_state=[-0.09787725 -0.0275715   0.20471244  0.67899387]\n",
      "[ episode 137 ][ timestamp 21 ] state=[-0.09787725 -0.0275715   0.20471244  0.67899387], action=1, reward=-1.0, next_state=[-0.09842868  0.16420746  0.21829231  0.45709808]\n",
      "[ Ended! ] Episode 137: Exploration_rate=0.5057535983897912. Score=21.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 138 ] state=[ 0.00704646 -0.02661459 -0.03318057 -0.01436639]\n",
      "[ episode 138 ][ timestamp 1 ] state=[ 0.00704646 -0.02661459 -0.03318057 -0.01436639], action=0, reward=1.0, next_state=[ 0.00651417 -0.22124536 -0.0334679   0.26766578]\n",
      "[ episode 138 ][ timestamp 2 ] state=[ 0.00651417 -0.22124536 -0.0334679   0.26766578], action=1, reward=1.0, next_state=[ 0.00208926 -0.02566216 -0.02811459 -0.03538247]\n",
      "[ episode 138 ][ timestamp 3 ] state=[ 0.00208926 -0.02566216 -0.02811459 -0.03538247], action=1, reward=1.0, next_state=[ 0.00157602  0.16985145 -0.02882223 -0.33680158]\n",
      "[ episode 138 ][ timestamp 4 ] state=[ 0.00157602  0.16985145 -0.02882223 -0.33680158], action=1, reward=1.0, next_state=[ 0.00497305  0.36537145 -0.03555827 -0.63843234]\n",
      "[ episode 138 ][ timestamp 5 ] state=[ 0.00497305  0.36537145 -0.03555827 -0.63843234], action=1, reward=1.0, next_state=[ 0.01228047  0.56097071 -0.04832691 -0.94209768]\n",
      "[ episode 138 ][ timestamp 6 ] state=[ 0.01228047  0.56097071 -0.04832691 -0.94209768], action=0, reward=1.0, next_state=[ 0.02349989  0.36653213 -0.06716887 -0.66498291]\n",
      "[ episode 138 ][ timestamp 7 ] state=[ 0.02349989  0.36653213 -0.06716887 -0.66498291], action=0, reward=1.0, next_state=[ 0.03083053  0.17240561 -0.08046852 -0.39418257]\n",
      "[ episode 138 ][ timestamp 8 ] state=[ 0.03083053  0.17240561 -0.08046852 -0.39418257], action=0, reward=1.0, next_state=[ 0.03427864 -0.02148779 -0.08835218 -0.12791579]\n",
      "[ episode 138 ][ timestamp 9 ] state=[ 0.03427864 -0.02148779 -0.08835218 -0.12791579], action=1, reward=1.0, next_state=[ 0.03384889  0.17478145 -0.09091049 -0.44711309]\n",
      "[ episode 138 ][ timestamp 10 ] state=[ 0.03384889  0.17478145 -0.09091049 -0.44711309], action=0, reward=1.0, next_state=[ 0.03734452 -0.01894476 -0.09985275 -0.18441465]\n",
      "[ episode 138 ][ timestamp 11 ] state=[ 0.03734452 -0.01894476 -0.09985275 -0.18441465], action=1, reward=1.0, next_state=[ 0.03696562  0.17745361 -0.10354105 -0.50685273]\n",
      "[ episode 138 ][ timestamp 12 ] state=[ 0.03696562  0.17745361 -0.10354105 -0.50685273], action=0, reward=1.0, next_state=[ 0.04051469 -0.01606864 -0.1136781  -0.2485107 ]\n",
      "[ episode 138 ][ timestamp 13 ] state=[ 0.04051469 -0.01606864 -0.1136781  -0.2485107 ], action=1, reward=1.0, next_state=[ 0.04019332  0.18047773 -0.11864832 -0.57477679]\n",
      "[ episode 138 ][ timestamp 14 ] state=[ 0.04019332  0.18047773 -0.11864832 -0.57477679], action=0, reward=1.0, next_state=[ 0.04380288 -0.01279855 -0.13014385 -0.32170142]\n",
      "[ episode 138 ][ timestamp 15 ] state=[ 0.04380288 -0.01279855 -0.13014385 -0.32170142], action=1, reward=1.0, next_state=[ 0.0435469   0.18391333 -0.13657788 -0.65242829]\n",
      "[ episode 138 ][ timestamp 16 ] state=[ 0.0435469   0.18391333 -0.13657788 -0.65242829], action=0, reward=1.0, next_state=[ 0.04722517 -0.00906894 -0.14962645 -0.40567972]\n",
      "[ episode 138 ][ timestamp 17 ] state=[ 0.04722517 -0.00906894 -0.14962645 -0.40567972], action=1, reward=1.0, next_state=[ 0.04704379  0.18782292 -0.15774004 -0.74154386]\n",
      "[ episode 138 ][ timestamp 18 ] state=[ 0.04704379  0.18782292 -0.15774004 -0.74154386], action=0, reward=1.0, next_state=[ 0.05080025 -0.00481061 -0.17257092 -0.50236444]\n",
      "[ episode 138 ][ timestamp 19 ] state=[ 0.05080025 -0.00481061 -0.17257092 -0.50236444], action=1, reward=1.0, next_state=[ 0.05070404  0.19226993 -0.18261821 -0.84407866]\n",
      "[ episode 138 ][ timestamp 20 ] state=[ 0.05070404  0.19226993 -0.18261821 -0.84407866], action=0, reward=1.0, next_state=[ 5.45494370e-02  4.67898976e-05 -1.99499779e-01 -6.13930327e-01]\n",
      "[ episode 138 ][ timestamp 21 ] state=[ 5.45494370e-02  4.67898976e-05 -1.99499779e-01 -6.13930327e-01], action=1, reward=-1.0, next_state=[ 0.05455037  0.19731477 -0.21177839 -0.96222797]\n",
      "[ Ended! ] Episode 138: Exploration_rate=0.5032248303978422. Score=21.\n",
      "[ Experience replay ] starts\n",
      "[ episode 139 ] state=[ 0.00947007 -0.04333272  0.04734578 -0.03950568]\n",
      "[ episode 139 ][ timestamp 1 ] state=[ 0.00947007 -0.04333272  0.04734578 -0.03950568], action=0, reward=1.0, next_state=[ 0.00860341 -0.23910051  0.04655567  0.26773139]\n",
      "[ episode 139 ][ timestamp 2 ] state=[ 0.00860341 -0.23910051  0.04655567  0.26773139], action=0, reward=1.0, next_state=[ 0.0038214  -0.43485489  0.0519103   0.57472724]\n",
      "[ episode 139 ][ timestamp 3 ] state=[ 0.0038214  -0.43485489  0.0519103   0.57472724], action=1, reward=1.0, next_state=[-0.00487569 -0.24049764  0.06340484  0.29883884]\n",
      "[ episode 139 ][ timestamp 4 ] state=[-0.00487569 -0.24049764  0.06340484  0.29883884], action=0, reward=1.0, next_state=[-0.00968565 -0.43646339  0.06938162  0.61082533]\n",
      "[ episode 139 ][ timestamp 5 ] state=[-0.00968565 -0.43646339  0.06938162  0.61082533], action=1, reward=1.0, next_state=[-0.01841492 -0.2423763   0.08159812  0.34077697]\n",
      "[ episode 139 ][ timestamp 6 ] state=[-0.01841492 -0.2423763   0.08159812  0.34077697], action=1, reward=1.0, next_state=[-0.02326244 -0.0485044   0.08841366  0.07489997]\n",
      "[ episode 139 ][ timestamp 7 ] state=[-0.02326244 -0.0485044   0.08841366  0.07489997], action=0, reward=1.0, next_state=[-0.02423253 -0.24477529  0.08991166  0.39411612]\n",
      "[ episode 139 ][ timestamp 8 ] state=[-0.02423253 -0.24477529  0.08991166  0.39411612], action=1, reward=1.0, next_state=[-0.02912803 -0.05103655  0.09779398  0.1310803 ]\n",
      "[ episode 139 ][ timestamp 9 ] state=[-0.02912803 -0.05103655  0.09779398  0.1310803 ], action=0, reward=1.0, next_state=[-0.03014877 -0.24741346  0.10041559  0.45294386]\n",
      "[ episode 139 ][ timestamp 10 ] state=[-0.03014877 -0.24741346  0.10041559  0.45294386], action=1, reward=1.0, next_state=[-0.03509703 -0.05384423  0.10947447  0.19352524]\n",
      "[ episode 139 ][ timestamp 11 ] state=[-0.03509703 -0.05384423  0.10947447  0.19352524], action=0, reward=1.0, next_state=[-0.03617392 -0.25034806  0.11334497  0.51863772]\n",
      "[ episode 139 ][ timestamp 12 ] state=[-0.03617392 -0.25034806  0.11334497  0.51863772], action=0, reward=1.0, next_state=[-0.04118088 -0.44686804  0.12371773  0.84477829]\n",
      "[ episode 139 ][ timestamp 13 ] state=[-0.04118088 -0.44686804  0.12371773  0.84477829], action=1, reward=1.0, next_state=[-0.05011824 -0.25363172  0.14061329  0.59341956]\n",
      "[ episode 139 ][ timestamp 14 ] state=[-0.05011824 -0.25363172  0.14061329  0.59341956], action=1, reward=1.0, next_state=[-0.05519088 -0.06072899  0.15248168  0.34812553]\n",
      "[ episode 139 ][ timestamp 15 ] state=[-0.05519088 -0.06072899  0.15248168  0.34812553], action=0, reward=1.0, next_state=[-0.05640546 -0.25765362  0.15944419  0.68474125]\n",
      "[ episode 139 ][ timestamp 16 ] state=[-0.05640546 -0.25765362  0.15944419  0.68474125], action=1, reward=1.0, next_state=[-0.06155853 -0.06506238  0.17313902  0.44619695]\n",
      "[ episode 139 ][ timestamp 17 ] state=[-0.06155853 -0.06506238  0.17313902  0.44619695], action=1, reward=1.0, next_state=[-0.06285978  0.12724168  0.18206296  0.21270254]\n",
      "[ episode 139 ][ timestamp 18 ] state=[-0.06285978  0.12724168  0.18206296  0.21270254], action=0, reward=1.0, next_state=[-0.06031494 -0.069953    0.18631701  0.55683708]\n",
      "[ episode 139 ][ timestamp 19 ] state=[-0.06031494 -0.069953    0.18631701  0.55683708], action=1, reward=1.0, next_state=[-0.061714    0.12213206  0.19745375  0.32815691]\n",
      "[ episode 139 ][ timestamp 20 ] state=[-0.061714    0.12213206  0.19745375  0.32815691], action=0, reward=1.0, next_state=[-0.05927136 -0.07517244  0.20401689  0.67603791]\n",
      "[ episode 139 ][ timestamp 21 ] state=[-0.05927136 -0.07517244  0.20401689  0.67603791], action=1, reward=-1.0, next_state=[-0.06077481  0.11661841  0.21753765  0.4538838 ]\n",
      "[ Ended! ] Episode 139: Exploration_rate=0.500708706245853. Score=21.\n",
      "[ Experience replay ] starts\n",
      "[ episode 140 ] state=[-0.04265227 -0.04900602 -0.02977544 -0.02468383]\n",
      "[ episode 140 ][ timestamp 1 ] state=[-0.04265227 -0.04900602 -0.02977544 -0.02468383], action=0, reward=1.0, next_state=[-0.04363239 -0.24368859 -0.03026911  0.2584579 ]\n",
      "[ episode 140 ][ timestamp 2 ] state=[-0.04363239 -0.24368859 -0.03026911  0.2584579 ], action=1, reward=1.0, next_state=[-0.04850616 -0.04814788 -0.02509996 -0.04361657]\n",
      "[ episode 140 ][ timestamp 3 ] state=[-0.04850616 -0.04814788 -0.02509996 -0.04361657], action=0, reward=1.0, next_state=[-0.04946912 -0.24290108 -0.02597229  0.2410426 ]\n",
      "[ episode 140 ][ timestamp 4 ] state=[-0.04946912 -0.24290108 -0.02597229  0.2410426 ], action=1, reward=1.0, next_state=[-0.05432714 -0.04741793 -0.02115144 -0.05971822]\n",
      "[ episode 140 ][ timestamp 5 ] state=[-0.05432714 -0.04741793 -0.02115144 -0.05971822], action=0, reward=1.0, next_state=[-0.0552755  -0.24223033 -0.0223458   0.22621696]\n",
      "[ episode 140 ][ timestamp 6 ] state=[-0.0552755  -0.24223033 -0.0223458   0.22621696], action=1, reward=1.0, next_state=[-0.06012011 -0.04679627 -0.01782146 -0.07343007]\n",
      "[ episode 140 ][ timestamp 7 ] state=[-0.06012011 -0.04679627 -0.01782146 -0.07343007], action=0, reward=1.0, next_state=[-0.06105603 -0.24165826 -0.01929006  0.21357726]\n",
      "[ episode 140 ][ timestamp 8 ] state=[-0.06105603 -0.24165826 -0.01929006  0.21357726], action=0, reward=1.0, next_state=[-0.0658892  -0.43649918 -0.01501852  0.50011335]\n",
      "[ episode 140 ][ timestamp 9 ] state=[-0.0658892  -0.43649918 -0.01501852  0.50011335], action=0, reward=1.0, next_state=[-0.07461918 -0.63140623 -0.00501625  0.78802567]\n",
      "[ episode 140 ][ timestamp 10 ] state=[-0.07461918 -0.63140623 -0.00501625  0.78802567], action=1, reward=1.0, next_state=[-0.08724731 -0.43621573  0.01074426  0.49376884]\n",
      "[ episode 140 ][ timestamp 11 ] state=[-0.08724731 -0.43621573  0.01074426  0.49376884], action=1, reward=1.0, next_state=[-0.09597162 -0.24124695  0.02061964  0.2044913 ]\n",
      "[ episode 140 ][ timestamp 12 ] state=[-0.09597162 -0.24124695  0.02061964  0.2044913 ], action=0, reward=1.0, next_state=[-0.10079656 -0.43665761  0.02470947  0.50360673]\n",
      "[ episode 140 ][ timestamp 13 ] state=[-0.10079656 -0.43665761  0.02470947  0.50360673], action=1, reward=1.0, next_state=[-0.10952971 -0.24189247  0.0347816   0.21881205]\n",
      "[ episode 140 ][ timestamp 14 ] state=[-0.10952971 -0.24189247  0.0347816   0.21881205], action=1, reward=1.0, next_state=[-0.11436756 -0.04728454  0.03915784 -0.06269958]\n",
      "[ episode 140 ][ timestamp 15 ] state=[-0.11436756 -0.04728454  0.03915784 -0.06269958], action=0, reward=1.0, next_state=[-0.11531325 -0.2429454   0.03790385  0.24207621]\n",
      "[ episode 140 ][ timestamp 16 ] state=[-0.11531325 -0.2429454   0.03790385  0.24207621], action=0, reward=1.0, next_state=[-0.12017216 -0.43858771  0.04274537  0.54646994]\n",
      "[ episode 140 ][ timestamp 17 ] state=[-0.12017216 -0.43858771  0.04274537  0.54646994], action=1, reward=1.0, next_state=[-0.12894391 -0.24409158  0.05367477  0.26755555]\n",
      "[ episode 140 ][ timestamp 18 ] state=[-0.12894391 -0.24409158  0.05367477  0.26755555], action=1, reward=1.0, next_state=[-0.13382575 -0.0497751   0.05902588 -0.00772659]\n",
      "[ episode 140 ][ timestamp 19 ] state=[-0.13382575 -0.0497751   0.05902588 -0.00772659], action=0, reward=1.0, next_state=[-0.13482125 -0.24569171  0.05887135  0.30298008]\n",
      "[ episode 140 ][ timestamp 20 ] state=[-0.13482125 -0.24569171  0.05887135  0.30298008], action=0, reward=1.0, next_state=[-0.13973508 -0.44160112  0.06493095  0.61363327]\n",
      "[ episode 140 ][ timestamp 21 ] state=[-0.13973508 -0.44160112  0.06493095  0.61363327], action=0, reward=1.0, next_state=[-0.1485671  -0.63756746  0.07720362  0.92603964]\n",
      "[ episode 140 ][ timestamp 22 ] state=[-0.1485671  -0.63756746  0.07720362  0.92603964], action=0, reward=1.0, next_state=[-0.16131845 -0.83364236  0.09572441  1.24195123]\n",
      "[ episode 140 ][ timestamp 23 ] state=[-0.16131845 -0.83364236  0.09572441  1.24195123], action=0, reward=1.0, next_state=[-0.1779913  -1.02985378  0.12056344  1.56302098]\n",
      "[ episode 140 ][ timestamp 24 ] state=[-0.1779913  -1.02985378  0.12056344  1.56302098], action=0, reward=1.0, next_state=[-0.19858838 -1.22619361  0.15182386  1.89075272]\n",
      "[ episode 140 ][ timestamp 25 ] state=[-0.19858838 -1.22619361  0.15182386  1.89075272], action=1, reward=1.0, next_state=[-0.22311225 -1.03301157  0.18963891  1.64877788]\n",
      "[ episode 140 ][ timestamp 26 ] state=[-0.22311225 -1.03301157  0.18963891  1.64877788], action=1, reward=-1.0, next_state=[-0.24377248 -0.84054505  0.22261447  1.42067408]\n",
      "[ Ended! ] Episode 140: Exploration_rate=0.4982051627146237. Score=26.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 141 ] state=[-0.04081025  0.04661038  0.00893141  0.03985192]\n",
      "[ episode 141 ][ timestamp 1 ] state=[-0.04081025  0.04661038  0.00893141  0.03985192], action=1, reward=1.0, next_state=[-0.03987804  0.24160312  0.00972845 -0.24999973]\n",
      "[ episode 141 ][ timestamp 2 ] state=[-0.03987804  0.24160312  0.00972845 -0.24999973], action=0, reward=1.0, next_state=[-0.03504598  0.04634361  0.00472845  0.0457358 ]\n",
      "[ episode 141 ][ timestamp 3 ] state=[-0.03504598  0.04634361  0.00472845  0.0457358 ], action=0, reward=1.0, next_state=[-0.03411911 -0.14884583  0.00564317  0.33990684]\n",
      "[ episode 141 ][ timestamp 4 ] state=[-0.03411911 -0.14884583  0.00564317  0.33990684], action=0, reward=1.0, next_state=[-0.03709602 -0.34404762  0.01244131  0.63436395]\n",
      "[ episode 141 ][ timestamp 5 ] state=[-0.03709602 -0.34404762  0.01244131  0.63436395], action=1, reward=1.0, next_state=[-0.04397698 -0.1491014   0.02512859  0.3456249 ]\n",
      "[ episode 141 ][ timestamp 6 ] state=[-0.04397698 -0.1491014   0.02512859  0.3456249 ], action=1, reward=1.0, next_state=[-0.046959    0.04565426  0.03204108  0.06097067]\n",
      "[ episode 141 ][ timestamp 7 ] state=[-0.046959    0.04565426  0.03204108  0.06097067], action=0, reward=1.0, next_state=[-0.04604592 -0.14991209  0.0332605   0.3635881 ]\n",
      "[ episode 141 ][ timestamp 8 ] state=[-0.04604592 -0.14991209  0.0332605   0.3635881 ], action=1, reward=1.0, next_state=[-0.04904416  0.04472174  0.04053226  0.0815756 ]\n",
      "[ episode 141 ][ timestamp 9 ] state=[-0.04904416  0.04472174  0.04053226  0.0815756 ], action=1, reward=1.0, next_state=[-0.04814973  0.23923993  0.04216377 -0.19804882]\n",
      "[ episode 141 ][ timestamp 10 ] state=[-0.04814973  0.23923993  0.04216377 -0.19804882], action=1, reward=1.0, next_state=[-0.04336493  0.43373424  0.03820279 -0.47713852]\n",
      "[ episode 141 ][ timestamp 11 ] state=[-0.04336493  0.43373424  0.03820279 -0.47713852], action=0, reward=1.0, next_state=[-0.03469024  0.2380943   0.02866002 -0.17266384]\n",
      "[ episode 141 ][ timestamp 12 ] state=[-0.03469024  0.2380943   0.02866002 -0.17266384], action=1, reward=1.0, next_state=[-0.02992836  0.43279459  0.02520675 -0.45616944]\n",
      "[ episode 141 ][ timestamp 13 ] state=[-0.02992836  0.43279459  0.02520675 -0.45616944], action=1, reward=1.0, next_state=[-0.02127246  0.62755125  0.01608336 -0.74080164]\n",
      "[ episode 141 ][ timestamp 14 ] state=[-0.02127246  0.62755125  0.01608336 -0.74080164], action=0, reward=1.0, next_state=[-0.00872144  0.43221099  0.00126733 -0.44310083]\n",
      "[ episode 141 ][ timestamp 15 ] state=[-0.00872144  0.43221099  0.00126733 -0.44310083], action=0, reward=1.0, next_state=[-7.72196558e-05  2.37071129e-01 -7.59469066e-03 -1.50018684e-01]\n",
      "[ episode 141 ][ timestamp 16 ] state=[-7.72196558e-05  2.37071129e-01 -7.59469066e-03 -1.50018684e-01], action=1, reward=1.0, next_state=[ 0.0046642   0.432301   -0.01059506 -0.44508787]\n",
      "[ episode 141 ][ timestamp 17 ] state=[ 0.0046642   0.432301   -0.01059506 -0.44508787], action=0, reward=1.0, next_state=[ 0.01331022  0.23733054 -0.01949682 -0.15576348]\n",
      "[ episode 141 ][ timestamp 18 ] state=[ 0.01331022  0.23733054 -0.01949682 -0.15576348], action=1, reward=1.0, next_state=[ 0.01805683  0.43272614 -0.02261209 -0.45453288]\n",
      "[ episode 141 ][ timestamp 19 ] state=[ 0.01805683  0.43272614 -0.02261209 -0.45453288], action=0, reward=1.0, next_state=[ 0.02671136  0.2379311  -0.03170275 -0.1690624 ]\n",
      "[ episode 141 ][ timestamp 20 ] state=[ 0.02671136  0.2379311  -0.03170275 -0.1690624 ], action=1, reward=1.0, next_state=[ 0.03146998  0.43349215 -0.035084   -0.47157562]\n",
      "[ episode 141 ][ timestamp 21 ] state=[ 0.03146998  0.43349215 -0.035084   -0.47157562], action=0, reward=1.0, next_state=[ 0.04013982  0.23888285 -0.04451551 -0.19015389]\n",
      "[ episode 141 ][ timestamp 22 ] state=[ 0.04013982  0.23888285 -0.04451551 -0.19015389], action=0, reward=1.0, next_state=[ 0.04491748  0.04442508 -0.04831859  0.08816056]\n",
      "[ episode 141 ][ timestamp 23 ] state=[ 0.04491748  0.04442508 -0.04831859  0.08816056], action=0, reward=1.0, next_state=[ 0.04580598 -0.14997217 -0.04655538  0.36521598]\n",
      "[ episode 141 ][ timestamp 24 ] state=[ 0.04580598 -0.14997217 -0.04655538  0.36521598], action=1, reward=1.0, next_state=[ 0.04280654  0.04577941 -0.03925106  0.05822442]\n",
      "[ episode 141 ][ timestamp 25 ] state=[ 0.04280654  0.04577941 -0.03925106  0.05822442], action=0, reward=1.0, next_state=[ 0.04372213 -0.14875841 -0.03808657  0.33826955]\n",
      "[ episode 141 ][ timestamp 26 ] state=[ 0.04372213 -0.14875841 -0.03808657  0.33826955], action=1, reward=1.0, next_state=[ 0.04074696  0.04688422 -0.03132118  0.03382368]\n",
      "[ episode 141 ][ timestamp 27 ] state=[ 0.04074696  0.04688422 -0.03132118  0.03382368], action=1, reward=1.0, next_state=[ 0.04168464  0.242441   -0.0306447  -0.26857455]\n",
      "[ episode 141 ][ timestamp 28 ] state=[ 0.04168464  0.242441   -0.0306447  -0.26857455], action=0, reward=1.0, next_state=[ 0.04653346  0.04776948 -0.03601619  0.0142875 ]\n",
      "[ episode 141 ][ timestamp 29 ] state=[ 0.04653346  0.04776948 -0.03601619  0.0142875 ], action=1, reward=1.0, next_state=[ 0.04748885  0.24338895 -0.03573044 -0.28953787]\n",
      "[ episode 141 ][ timestamp 30 ] state=[ 0.04748885  0.24338895 -0.03573044 -0.28953787], action=1, reward=1.0, next_state=[ 0.05235663  0.4390017  -0.0415212  -0.59327224]\n",
      "[ episode 141 ][ timestamp 31 ] state=[ 0.05235663  0.4390017  -0.0415212  -0.59327224], action=0, reward=1.0, next_state=[ 0.06113666  0.24448481 -0.05338665 -0.31395211]\n",
      "[ episode 141 ][ timestamp 32 ] state=[ 0.06113666  0.24448481 -0.05338665 -0.31395211], action=1, reward=1.0, next_state=[ 0.06602636  0.44032502 -0.05966569 -0.62298211]\n",
      "[ episode 141 ][ timestamp 33 ] state=[ 0.06602636  0.44032502 -0.05966569 -0.62298211], action=0, reward=1.0, next_state=[ 0.07483286  0.24608469 -0.07212533 -0.34967139]\n",
      "[ episode 141 ][ timestamp 34 ] state=[ 0.07483286  0.24608469 -0.07212533 -0.34967139], action=0, reward=1.0, next_state=[ 0.07975455  0.05205866 -0.07911876 -0.0805755 ]\n",
      "[ episode 141 ][ timestamp 35 ] state=[ 0.07975455  0.05205866 -0.07911876 -0.0805755 ], action=0, reward=1.0, next_state=[ 0.08079573 -0.14184521 -0.08073027  0.18613379]\n",
      "[ episode 141 ][ timestamp 36 ] state=[ 0.08079573 -0.14184521 -0.08073027  0.18613379], action=1, reward=1.0, next_state=[ 0.07795882  0.05433343 -0.07700759 -0.13088469]\n",
      "[ episode 141 ][ timestamp 37 ] state=[ 0.07795882  0.05433343 -0.07700759 -0.13088469], action=1, reward=1.0, next_state=[ 0.07904549  0.2504692  -0.07962529 -0.4468343 ]\n",
      "[ episode 141 ][ timestamp 38 ] state=[ 0.07904549  0.2504692  -0.07962529 -0.4468343 ], action=1, reward=1.0, next_state=[ 0.08405488  0.44662197 -0.08856197 -0.76351632]\n",
      "[ episode 141 ][ timestamp 39 ] state=[ 0.08405488  0.44662197 -0.08856197 -0.76351632], action=0, reward=1.0, next_state=[ 0.09298732  0.25282413 -0.1038323  -0.499962  ]\n",
      "[ episode 141 ][ timestamp 40 ] state=[ 0.09298732  0.25282413 -0.1038323  -0.499962  ], action=0, reward=1.0, next_state=[ 0.0980438   0.05930744 -0.11383154 -0.24172219]\n",
      "[ episode 141 ][ timestamp 41 ] state=[ 0.0980438   0.05930744 -0.11383154 -0.24172219], action=0, reward=1.0, next_state=[ 0.09922995 -0.13402004 -0.11866598  0.01299801]\n",
      "[ episode 141 ][ timestamp 42 ] state=[ 0.09922995 -0.13402004 -0.11866598  0.01299801], action=0, reward=1.0, next_state=[ 0.09654955 -0.32725792 -0.11840602  0.26601043]\n",
      "[ episode 141 ][ timestamp 43 ] state=[ 0.09654955 -0.32725792 -0.11840602  0.26601043], action=1, reward=1.0, next_state=[ 0.09000439 -0.13066258 -0.11308581 -0.06154789]\n",
      "[ episode 141 ][ timestamp 44 ] state=[ 0.09000439 -0.13066258 -0.11308581 -0.06154789], action=1, reward=1.0, next_state=[ 0.08739114  0.06588385 -0.11431677 -0.38766082]\n",
      "[ episode 141 ][ timestamp 45 ] state=[ 0.08739114  0.06588385 -0.11431677 -0.38766082], action=1, reward=1.0, next_state=[ 0.08870881  0.26242718 -0.12206999 -0.71408752]\n",
      "[ episode 141 ][ timestamp 46 ] state=[ 0.08870881  0.26242718 -0.12206999 -0.71408752], action=0, reward=1.0, next_state=[ 0.09395736  0.06918762 -0.13635174 -0.46218462]\n",
      "[ episode 141 ][ timestamp 47 ] state=[ 0.09395736  0.06918762 -0.13635174 -0.46218462], action=0, reward=1.0, next_state=[ 0.09534111 -0.12377045 -0.14559543 -0.21539724]\n",
      "[ episode 141 ][ timestamp 48 ] state=[ 0.09534111 -0.12377045 -0.14559543 -0.21539724], action=1, reward=1.0, next_state=[ 0.0928657   0.07310039 -0.14990338 -0.55023306]\n",
      "[ episode 141 ][ timestamp 49 ] state=[ 0.0928657   0.07310039 -0.14990338 -0.55023306], action=0, reward=1.0, next_state=[ 0.09432771 -0.11963337 -0.16090804 -0.30828126]\n",
      "[ episode 141 ][ timestamp 50 ] state=[ 0.09432771 -0.11963337 -0.16090804 -0.30828126], action=0, reward=1.0, next_state=[ 0.09193504 -0.31214057 -0.16707366 -0.07035368]\n",
      "[ episode 141 ][ timestamp 51 ] state=[ 0.09193504 -0.31214057 -0.16707366 -0.07035368], action=1, reward=1.0, next_state=[ 0.08569223 -0.11506647 -0.16848074 -0.4107401 ]\n",
      "[ episode 141 ][ timestamp 52 ] state=[ 0.08569223 -0.11506647 -0.16848074 -0.4107401 ], action=0, reward=1.0, next_state=[ 0.0833909  -0.30744941 -0.17669554 -0.17555103]\n",
      "[ episode 141 ][ timestamp 53 ] state=[ 0.0833909  -0.30744941 -0.17669554 -0.17555103], action=1, reward=1.0, next_state=[ 0.07724191 -0.11029682 -0.18020656 -0.518354  ]\n",
      "[ episode 141 ][ timestamp 54 ] state=[ 0.07724191 -0.11029682 -0.18020656 -0.518354  ], action=0, reward=1.0, next_state=[ 0.07503598 -0.30248516 -0.19057364 -0.28743417]\n",
      "[ episode 141 ][ timestamp 55 ] state=[ 0.07503598 -0.30248516 -0.19057364 -0.28743417], action=1, reward=1.0, next_state=[ 0.06898627 -0.10522895 -0.19632232 -0.63365184]\n",
      "[ episode 141 ][ timestamp 56 ] state=[ 0.06898627 -0.10522895 -0.19632232 -0.63365184], action=1, reward=1.0, next_state=[ 0.06688169  0.09201121 -0.20899536 -0.98117748]\n",
      "[ episode 141 ][ timestamp 57 ] state=[ 0.06688169  0.09201121 -0.20899536 -0.98117748], action=0, reward=-1.0, next_state=[ 0.06872192 -0.0997907  -0.22861891 -0.76073338]\n",
      "[ Ended! ] Episode 141: Exploration_rate=0.49571413690105054. Score=57.\n",
      "[ Experience replay ] starts\n",
      "[ episode 142 ] state=[0.03021777 0.03114787 0.02552391 0.00792217]\n",
      "[ episode 142 ][ timestamp 1 ] state=[0.03021777 0.03114787 0.02552391 0.00792217], action=0, reward=1.0, next_state=[ 0.03084072 -0.16433065  0.02568236  0.30854766]\n",
      "[ episode 142 ][ timestamp 2 ] state=[ 0.03084072 -0.16433065  0.02568236  0.30854766], action=0, reward=1.0, next_state=[ 0.02755411 -0.35980894  0.03185331  0.60921819]\n",
      "[ episode 142 ][ timestamp 3 ] state=[ 0.02755411 -0.35980894  0.03185331  0.60921819], action=1, reward=1.0, next_state=[ 0.02035793 -0.16514642  0.04403767  0.32673582]\n",
      "[ episode 142 ][ timestamp 4 ] state=[ 0.02035793 -0.16514642  0.04403767  0.32673582], action=0, reward=1.0, next_state=[ 0.017055   -0.36086678  0.05057239  0.63297462]\n",
      "[ episode 142 ][ timestamp 5 ] state=[ 0.017055   -0.36086678  0.05057239  0.63297462], action=1, reward=1.0, next_state=[ 0.00983767 -0.16648546  0.06323188  0.35663737]\n",
      "[ episode 142 ][ timestamp 6 ] state=[ 0.00983767 -0.16648546  0.06323188  0.35663737], action=1, reward=1.0, next_state=[0.00650796 0.02768318 0.07036463 0.08454426]\n",
      "[ episode 142 ][ timestamp 7 ] state=[0.00650796 0.02768318 0.07036463 0.08454426], action=0, reward=1.0, next_state=[ 0.00706162 -0.16837318  0.07205552  0.39857119]\n",
      "[ episode 142 ][ timestamp 8 ] state=[ 0.00706162 -0.16837318  0.07205552  0.39857119], action=1, reward=1.0, next_state=[0.00369416 0.02565654 0.08002694 0.12944784]\n",
      "[ episode 142 ][ timestamp 9 ] state=[0.00369416 0.02565654 0.08002694 0.12944784], action=0, reward=1.0, next_state=[ 0.00420729 -0.17051518  0.0826159   0.44626648]\n",
      "[ episode 142 ][ timestamp 10 ] state=[ 0.00420729 -0.17051518  0.0826159   0.44626648], action=1, reward=1.0, next_state=[0.00079699 0.02334675 0.09154123 0.18072685]\n",
      "[ episode 142 ][ timestamp 11 ] state=[0.00079699 0.02334675 0.09154123 0.18072685], action=0, reward=1.0, next_state=[ 0.00126392 -0.17295777  0.09515576  0.5008263 ]\n",
      "[ episode 142 ][ timestamp 12 ] state=[ 0.00126392 -0.17295777  0.09515576  0.5008263 ], action=1, reward=1.0, next_state=[-0.00219524  0.02070306  0.10517229  0.23958279]\n",
      "[ episode 142 ][ timestamp 13 ] state=[-0.00219524  0.02070306  0.10517229  0.23958279], action=0, reward=1.0, next_state=[-0.00178117 -0.17575182  0.10996394  0.56350053]\n",
      "[ episode 142 ][ timestamp 14 ] state=[-0.00178117 -0.17575182  0.10996394  0.56350053], action=1, reward=1.0, next_state=[-0.00529621  0.01766932  0.12123395  0.30738548]\n",
      "[ episode 142 ][ timestamp 15 ] state=[-0.00529621  0.01766932  0.12123395  0.30738548], action=0, reward=1.0, next_state=[-0.00494282 -0.17895275  0.12738166  0.63570936]\n",
      "[ episode 142 ][ timestamp 16 ] state=[-0.00494282 -0.17895275  0.12738166  0.63570936], action=1, reward=1.0, next_state=[-0.00852188  0.01418408  0.14009585  0.38570035]\n",
      "[ episode 142 ][ timestamp 17 ] state=[-0.00852188  0.01418408  0.14009585  0.38570035], action=0, reward=1.0, next_state=[-0.0082382  -0.18261981  0.14780986  0.71906752]\n",
      "[ episode 142 ][ timestamp 18 ] state=[-0.0082382  -0.18261981  0.14780986  0.71906752], action=1, reward=1.0, next_state=[-0.01189059  0.01018139  0.16219121  0.47631722]\n",
      "[ episode 142 ][ timestamp 19 ] state=[-0.01189059  0.01018139  0.16219121  0.47631722], action=0, reward=1.0, next_state=[-0.01168697 -0.18681443  0.17171755  0.81540826]\n",
      "[ episode 142 ][ timestamp 20 ] state=[-0.01168697 -0.18681443  0.17171755  0.81540826], action=1, reward=1.0, next_state=[-0.01542325  0.00559274  0.18802572  0.5812794 ]\n",
      "[ episode 142 ][ timestamp 21 ] state=[-0.01542325  0.00559274  0.18802572  0.5812794 ], action=1, reward=1.0, next_state=[-0.0153114   0.19765185  0.19965131  0.35322265]\n",
      "[ episode 142 ][ timestamp 22 ] state=[-0.0153114   0.19765185  0.19965131  0.35322265], action=0, reward=1.0, next_state=[-1.13583627e-02  3.34126469e-04  2.06715759e-01  7.01628202e-01]\n",
      "[ episode 142 ][ timestamp 23 ] state=[-1.13583627e-02  3.34126469e-04  2.06715759e-01  7.01628202e-01], action=0, reward=-1.0, next_state=[-0.01135168 -0.19696197  0.22074832  1.05161431]\n",
      "[ Ended! ] Episode 142: Exploration_rate=0.4932355662165453. Score=23.\n",
      "[ Experience replay ] starts\n",
      "[ episode 143 ] state=[-0.02479487  0.00178177  0.04925159 -0.02043435]\n",
      "[ episode 143 ][ timestamp 1 ] state=[-0.02479487  0.00178177  0.04925159 -0.02043435], action=1, reward=1.0, next_state=[-0.02475923  0.19616407  0.04884291 -0.29718011]\n",
      "[ episode 143 ][ timestamp 2 ] state=[-0.02475923  0.19616407  0.04884291 -0.29718011], action=1, reward=1.0, next_state=[-0.02083595  0.39055696  0.04289931 -0.57406759]\n",
      "[ episode 143 ][ timestamp 3 ] state=[-0.02083595  0.39055696  0.04289931 -0.57406759], action=0, reward=1.0, next_state=[-0.01302481  0.19486065  0.03141795 -0.26818467]\n",
      "[ episode 143 ][ timestamp 4 ] state=[-0.01302481  0.19486065  0.03141795 -0.26818467], action=1, reward=1.0, next_state=[-0.0091276   0.38952046  0.02605426 -0.55079494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 143 ][ timestamp 5 ] state=[-0.0091276   0.38952046  0.02605426 -0.55079494], action=1, reward=1.0, next_state=[-0.00133719  0.58426697  0.01503836 -0.83515647]\n",
      "[ episode 143 ][ timestamp 6 ] state=[-0.00133719  0.58426697  0.01503836 -0.83515647], action=1, reward=1.0, next_state=[ 0.01034815  0.77918029 -0.00166477 -1.12307228]\n",
      "[ episode 143 ][ timestamp 7 ] state=[ 0.01034815  0.77918029 -0.00166477 -1.12307228], action=0, reward=1.0, next_state=[ 0.02593176  0.5840802  -0.02412621 -0.830912  ]\n",
      "[ episode 143 ][ timestamp 8 ] state=[ 0.02593176  0.5840802  -0.02412621 -0.830912  ], action=0, reward=1.0, next_state=[ 0.03761336  0.38929617 -0.04074445 -0.54591339]\n",
      "[ episode 143 ][ timestamp 9 ] state=[ 0.03761336  0.38929617 -0.04074445 -0.54591339], action=1, reward=1.0, next_state=[ 0.04539928  0.58496621 -0.05166272 -0.85115042]\n",
      "[ episode 143 ][ timestamp 10 ] state=[ 0.04539928  0.58496621 -0.05166272 -0.85115042], action=1, reward=1.0, next_state=[ 0.05709861  0.78075306 -0.06868573 -1.15962094]\n",
      "[ episode 143 ][ timestamp 11 ] state=[ 0.05709861  0.78075306 -0.06868573 -1.15962094], action=0, reward=1.0, next_state=[ 0.07271367  0.58658995 -0.09187815 -0.88924074]\n",
      "[ episode 143 ][ timestamp 12 ] state=[ 0.07271367  0.58658995 -0.09187815 -0.88924074], action=1, reward=1.0, next_state=[ 0.08444547  0.78283051 -0.10966296 -1.20933421]\n",
      "[ episode 143 ][ timestamp 13 ] state=[ 0.08444547  0.78283051 -0.10966296 -1.20933421], action=0, reward=1.0, next_state=[ 0.10010208  0.58928218 -0.13384965 -0.952932  ]\n",
      "[ episode 143 ][ timestamp 14 ] state=[ 0.10010208  0.58928218 -0.13384965 -0.952932  ], action=1, reward=1.0, next_state=[ 0.11188772  0.78592641 -0.15290829 -1.28449442]\n",
      "[ episode 143 ][ timestamp 15 ] state=[ 0.11188772  0.78592641 -0.15290829 -1.28449442], action=1, reward=1.0, next_state=[ 0.12760625  0.98262819 -0.17859818 -1.62088456]\n",
      "[ episode 143 ][ timestamp 16 ] state=[ 0.12760625  0.98262819 -0.17859818 -1.62088456], action=0, reward=-1.0, next_state=[ 0.14725881  0.79000295 -0.21101587 -1.38877179]\n",
      "[ Ended! ] Episode 143: Exploration_rate=0.4907693883854626. Score=16.\n",
      "[ Experience replay ] starts\n",
      "[ episode 144 ] state=[-0.00317782  0.00122219 -0.00782214  0.02878663]\n",
      "[ episode 144 ][ timestamp 1 ] state=[-0.00317782  0.00122219 -0.00782214  0.02878663], action=0, reward=1.0, next_state=[-0.00315337 -0.19378672 -0.00724641  0.31899135]\n",
      "[ episode 144 ][ timestamp 2 ] state=[-0.00315337 -0.19378672 -0.00724641  0.31899135], action=1, reward=1.0, next_state=[-0.00702911  0.00143768 -0.00086658  0.02403201]\n",
      "[ episode 144 ][ timestamp 3 ] state=[-0.00702911  0.00143768 -0.00086658  0.02403201], action=1, reward=1.0, next_state=[-0.00700035  0.19657205 -0.00038594 -0.2689242 ]\n",
      "[ episode 144 ][ timestamp 4 ] state=[-0.00700035  0.19657205 -0.00038594 -0.2689242 ], action=1, reward=1.0, next_state=[-0.00306891  0.3916995  -0.00576442 -0.56172883]\n",
      "[ episode 144 ][ timestamp 5 ] state=[-0.00306891  0.3916995  -0.00576442 -0.56172883], action=0, reward=1.0, next_state=[ 0.00476508  0.19665892 -0.016999   -0.27086755]\n",
      "[ episode 144 ][ timestamp 6 ] state=[ 0.00476508  0.19665892 -0.016999   -0.27086755], action=0, reward=1.0, next_state=[ 0.00869826  0.00178362 -0.02241635  0.01640571]\n",
      "[ episode 144 ][ timestamp 7 ] state=[ 0.00869826  0.00178362 -0.02241635  0.01640571], action=1, reward=1.0, next_state=[ 0.00873393  0.19721975 -0.02208824 -0.2832647 ]\n",
      "[ episode 144 ][ timestamp 8 ] state=[ 0.00873393  0.19721975 -0.02208824 -0.2832647 ], action=1, reward=1.0, next_state=[ 0.01267832  0.39264968 -0.02775353 -0.58283149]\n",
      "[ episode 144 ][ timestamp 9 ] state=[ 0.01267832  0.39264968 -0.02775353 -0.58283149], action=0, reward=1.0, next_state=[ 0.02053132  0.19792732 -0.03941016 -0.29901893]\n",
      "[ episode 144 ][ timestamp 10 ] state=[ 0.02053132  0.19792732 -0.03941016 -0.29901893], action=1, reward=1.0, next_state=[ 0.02448986  0.39358823 -0.04539054 -0.60386599]\n",
      "[ episode 144 ][ timestamp 11 ] state=[ 0.02448986  0.39358823 -0.04539054 -0.60386599], action=1, reward=1.0, next_state=[ 0.03236163  0.58931462 -0.05746786 -0.91049343]\n",
      "[ episode 144 ][ timestamp 12 ] state=[ 0.03236163  0.58931462 -0.05746786 -0.91049343], action=1, reward=1.0, next_state=[ 0.04414792  0.7851652  -0.07567773 -1.22067058]\n",
      "[ episode 144 ][ timestamp 13 ] state=[ 0.04414792  0.7851652  -0.07567773 -1.22067058], action=0, reward=1.0, next_state=[ 0.05985122  0.59109571 -0.10009114 -0.95262755]\n",
      "[ episode 144 ][ timestamp 14 ] state=[ 0.05985122  0.59109571 -0.10009114 -0.95262755], action=1, reward=1.0, next_state=[ 0.07167314  0.78741165 -0.11914369 -1.27500532]\n",
      "[ episode 144 ][ timestamp 15 ] state=[ 0.07167314  0.78741165 -0.11914369 -1.27500532], action=0, reward=1.0, next_state=[ 0.08742137  0.5939937  -0.1446438  -1.02188061]\n",
      "[ episode 144 ][ timestamp 16 ] state=[ 0.08742137  0.5939937  -0.1446438  -1.02188061], action=0, reward=1.0, next_state=[ 0.09930124  0.40106379 -0.16508141 -0.77788495]\n",
      "[ episode 144 ][ timestamp 17 ] state=[ 0.09930124  0.40106379 -0.16508141 -0.77788495], action=0, reward=1.0, next_state=[ 0.10732252  0.20854996 -0.18063911 -0.54135385]\n",
      "[ episode 144 ][ timestamp 18 ] state=[ 0.10732252  0.20854996 -0.18063911 -0.54135385], action=1, reward=1.0, next_state=[ 0.11149352  0.40568959 -0.19146618 -0.88507137]\n",
      "[ episode 144 ][ timestamp 19 ] state=[ 0.11149352  0.40568959 -0.19146618 -0.88507137], action=0, reward=1.0, next_state=[ 0.11960731  0.21361046 -0.20916761 -0.65816543]\n",
      "[ episode 144 ][ timestamp 20 ] state=[ 0.11960731  0.21361046 -0.20916761 -0.65816543], action=0, reward=-1.0, next_state=[ 0.12387952  0.02191856 -0.22233092 -0.43794257]\n",
      "[ Ended! ] Episode 144: Exploration_rate=0.4883155414435353. Score=20.\n",
      "[ Experience replay ] starts\n",
      "[ episode 145 ] state=[-0.02055042 -0.03767578 -0.02357646  0.03811718]\n",
      "[ episode 145 ][ timestamp 1 ] state=[-0.02055042 -0.03767578 -0.02357646  0.03811718], action=0, reward=1.0, next_state=[-0.02130393 -0.23245185 -0.02281412  0.32326925]\n",
      "[ episode 145 ][ timestamp 2 ] state=[-0.02130393 -0.23245185 -0.02281412  0.32326925], action=0, reward=1.0, next_state=[-0.02595297 -0.42724164 -0.01634874  0.60867112]\n",
      "[ episode 145 ][ timestamp 3 ] state=[-0.02595297 -0.42724164 -0.01634874  0.60867112], action=1, reward=1.0, next_state=[-0.0344978  -0.23189499 -0.00417531  0.310884  ]\n",
      "[ episode 145 ][ timestamp 4 ] state=[-0.0344978  -0.23189499 -0.00417531  0.310884  ], action=1, reward=1.0, next_state=[-0.0391357  -0.0367138   0.00204237  0.01688723]\n",
      "[ episode 145 ][ timestamp 5 ] state=[-0.0391357  -0.0367138   0.00204237  0.01688723], action=1, reward=1.0, next_state=[-0.03986998  0.1583788   0.00238011 -0.27515061]\n",
      "[ episode 145 ][ timestamp 6 ] state=[-0.03986998  0.1583788   0.00238011 -0.27515061], action=0, reward=1.0, next_state=[-0.0367024  -0.03677703 -0.0031229   0.01828206]\n",
      "[ episode 145 ][ timestamp 7 ] state=[-0.0367024  -0.03677703 -0.0031229   0.01828206], action=0, reward=1.0, next_state=[-0.03743794 -0.23185406 -0.00275726  0.30997804]\n",
      "[ episode 145 ][ timestamp 8 ] state=[-0.03743794 -0.23185406 -0.00275726  0.30997804], action=0, reward=1.0, next_state=[-0.04207502 -0.42693661  0.0034423   0.60179013]\n",
      "[ episode 145 ][ timestamp 9 ] state=[-0.04207502 -0.42693661  0.0034423   0.60179013], action=0, reward=1.0, next_state=[-0.05061375 -0.62210655  0.0154781   0.89555533]\n",
      "[ episode 145 ][ timestamp 10 ] state=[-0.05061375 -0.62210655  0.0154781   0.89555533], action=0, reward=1.0, next_state=[-0.06305589 -0.81743491  0.03338921  1.19306316]\n",
      "[ episode 145 ][ timestamp 11 ] state=[-0.06305589 -0.81743491  0.03338921  1.19306316], action=1, reward=1.0, next_state=[-0.07940458 -0.62276097  0.05725047  0.91102961]\n",
      "[ episode 145 ][ timestamp 12 ] state=[-0.07940458 -0.62276097  0.05725047  0.91102961], action=1, reward=1.0, next_state=[-0.0918598  -0.42845849  0.07547107  0.63687585]\n",
      "[ episode 145 ][ timestamp 13 ] state=[-0.0918598  -0.42845849  0.07547107  0.63687585], action=1, reward=1.0, next_state=[-0.10042897 -0.23446566  0.08820858  0.36888237]\n",
      "[ episode 145 ][ timestamp 14 ] state=[-0.10042897 -0.23446566  0.08820858  0.36888237], action=0, reward=1.0, next_state=[-0.10511829 -0.43072296  0.09558623  0.6880235 ]\n",
      "[ episode 145 ][ timestamp 15 ] state=[-0.10511829 -0.43072296  0.09558623  0.6880235 ], action=1, reward=1.0, next_state=[-0.11373275 -0.23704853  0.1093467   0.42689758]\n",
      "[ episode 145 ][ timestamp 16 ] state=[-0.11373275 -0.23704853  0.1093467   0.42689758], action=1, reward=1.0, next_state=[-0.11847372 -0.04363144  0.11788465  0.17058859]\n",
      "[ episode 145 ][ timestamp 17 ] state=[-0.11847372 -0.04363144  0.11788465  0.17058859], action=0, reward=1.0, next_state=[-0.11934635 -0.24022609  0.12129642  0.49801179]\n",
      "[ episode 145 ][ timestamp 18 ] state=[-0.11934635 -0.24022609  0.12129642  0.49801179], action=1, reward=1.0, next_state=[-0.12415087 -0.04700441  0.13125666  0.24588255]\n",
      "[ episode 145 ][ timestamp 19 ] state=[-0.12415087 -0.04700441  0.13125666  0.24588255], action=0, reward=1.0, next_state=[-0.12509096 -0.24373294  0.13617431  0.57691575]\n",
      "[ episode 145 ][ timestamp 20 ] state=[-0.12509096 -0.24373294  0.13617431  0.57691575], action=1, reward=1.0, next_state=[-0.12996561 -0.05075601  0.14771263  0.33004169]\n",
      "[ episode 145 ][ timestamp 21 ] state=[-0.12996561 -0.05075601  0.14771263  0.33004169], action=0, reward=1.0, next_state=[-0.13098073 -0.24763801  0.15431346  0.66541848]\n",
      "[ episode 145 ][ timestamp 22 ] state=[-0.13098073 -0.24763801  0.15431346  0.66541848], action=0, reward=1.0, next_state=[-0.13593349 -0.44453133  0.16762183  1.00243733]\n",
      "[ episode 145 ][ timestamp 23 ] state=[-0.13593349 -0.44453133  0.16762183  1.00243733], action=1, reward=1.0, next_state=[-0.14482412 -0.25199712  0.18767058  0.76673412]\n",
      "[ episode 145 ][ timestamp 24 ] state=[-0.14482412 -0.25199712  0.18767058  0.76673412], action=0, reward=1.0, next_state=[-0.14986406 -0.44913867  0.20300526  1.11210602]\n",
      "[ episode 145 ][ timestamp 25 ] state=[-0.14986406 -0.44913867  0.20300526  1.11210602], action=1, reward=-1.0, next_state=[-0.15884684 -0.25717645  0.22524738  0.88935004]\n",
      "[ Ended! ] Episode 145: Exploration_rate=0.4858739637363176. Score=25.\n",
      "[ Experience replay ] starts\n",
      "[ episode 146 ] state=[-0.03656316  0.02316451  0.01615401 -0.0059303 ]\n",
      "[ episode 146 ][ timestamp 1 ] state=[-0.03656316  0.02316451  0.01615401 -0.0059303 ], action=0, reward=1.0, next_state=[-0.03609987 -0.17218534  0.0160354   0.29180532]\n",
      "[ episode 146 ][ timestamp 2 ] state=[-0.03609987 -0.17218534  0.0160354   0.29180532], action=1, reward=1.0, next_state=[-0.03954357  0.02270434  0.02187151  0.00422258]\n",
      "[ episode 146 ][ timestamp 3 ] state=[-0.03954357  0.02270434  0.02187151  0.00422258], action=0, reward=1.0, next_state=[-0.03908949 -0.17272434  0.02195596  0.3037252 ]\n",
      "[ episode 146 ][ timestamp 4 ] state=[-0.03908949 -0.17272434  0.02195596  0.3037252 ], action=1, reward=1.0, next_state=[-0.04254397  0.02207794  0.02803046  0.01804675]\n",
      "[ episode 146 ][ timestamp 5 ] state=[-0.04254397  0.02207794  0.02803046  0.01804675], action=0, reward=1.0, next_state=[-0.04210241 -0.17343455  0.0283914   0.31944016]\n",
      "[ episode 146 ][ timestamp 6 ] state=[-0.04210241 -0.17343455  0.0283914   0.31944016], action=1, reward=1.0, next_state=[-0.04557111  0.02127179  0.0347802   0.0358443 ]\n",
      "[ episode 146 ][ timestamp 7 ] state=[-0.04557111  0.02127179  0.0347802   0.0358443 ], action=1, reward=1.0, next_state=[-0.04514567  0.21587817  0.03549709 -0.24566542]\n",
      "[ episode 146 ][ timestamp 8 ] state=[-0.04514567  0.21587817  0.03549709 -0.24566542], action=0, reward=1.0, next_state=[-0.04082811  0.02026768  0.03058378  0.05799943]\n",
      "[ episode 146 ][ timestamp 9 ] state=[-0.04082811  0.02026768  0.03058378  0.05799943], action=0, reward=1.0, next_state=[-0.04042275 -0.17527913  0.03174377  0.36017271]\n",
      "[ episode 146 ][ timestamp 10 ] state=[-0.04042275 -0.17527913  0.03174377  0.36017271], action=1, reward=1.0, next_state=[-0.04392834  0.01937754  0.03894722  0.0776659 ]\n",
      "[ episode 146 ][ timestamp 11 ] state=[-0.04392834  0.01937754  0.03894722  0.0776659 ], action=0, reward=1.0, next_state=[-0.04354078 -0.17628047  0.04050054  0.38237794]\n",
      "[ episode 146 ][ timestamp 12 ] state=[-0.04354078 -0.17628047  0.04050054  0.38237794], action=0, reward=1.0, next_state=[-0.04706639 -0.37195337  0.0481481   0.68755051]\n",
      "[ episode 146 ][ timestamp 13 ] state=[-0.04706639 -0.37195337  0.0481481   0.68755051], action=1, reward=1.0, next_state=[-0.05450546 -0.17753163  0.06189911  0.41040593]\n",
      "[ episode 146 ][ timestamp 14 ] state=[-0.05450546 -0.17753163  0.06189911  0.41040593], action=1, reward=1.0, next_state=[-0.05805609  0.01666066  0.07010723  0.13786208]\n",
      "[ episode 146 ][ timestamp 15 ] state=[-0.05805609  0.01666066  0.07010723  0.13786208], action=0, reward=1.0, next_state=[-0.05772288 -0.17939175  0.07286447  0.45181294]\n",
      "[ episode 146 ][ timestamp 16 ] state=[-0.05772288 -0.17939175  0.07286447  0.45181294], action=1, reward=1.0, next_state=[-0.06131072  0.01462817  0.08190073  0.18295849]\n",
      "[ episode 146 ][ timestamp 17 ] state=[-0.06131072  0.01462817  0.08190073  0.18295849], action=1, reward=1.0, next_state=[-0.06101815  0.20848853  0.0855599  -0.08280542]\n",
      "[ episode 146 ][ timestamp 18 ] state=[-0.06101815  0.20848853  0.0855599  -0.08280542], action=0, reward=1.0, next_state=[-0.05684838  0.01225093  0.08390379  0.23559815]\n",
      "[ episode 146 ][ timestamp 19 ] state=[-0.05684838  0.01225093  0.08390379  0.23559815], action=1, reward=1.0, next_state=[-0.05660336  0.20608015  0.08861575 -0.0294841 ]\n",
      "[ episode 146 ][ timestamp 20 ] state=[-0.05660336  0.20608015  0.08861575 -0.0294841 ], action=0, reward=1.0, next_state=[-0.05248176  0.00980652  0.08802607  0.28979008]\n",
      "[ episode 146 ][ timestamp 21 ] state=[-0.05248176  0.00980652  0.08802607  0.28979008], action=0, reward=1.0, next_state=[-0.05228563 -0.18645316  0.09382187  0.60888603]\n",
      "[ episode 146 ][ timestamp 22 ] state=[-0.05228563 -0.18645316  0.09382187  0.60888603], action=0, reward=1.0, next_state=[-0.05601469 -0.38275282  0.10599959  0.92958371]\n",
      "[ episode 146 ][ timestamp 23 ] state=[-0.05601469 -0.38275282  0.10599959  0.92958371], action=1, reward=1.0, next_state=[-0.06366975 -0.18920903  0.12459127  0.67200302]\n",
      "[ episode 146 ][ timestamp 24 ] state=[-0.06366975 -0.18920903  0.12459127  0.67200302], action=1, reward=1.0, next_state=[-0.06745393  0.00398099  0.13803133  0.42099939]\n",
      "[ episode 146 ][ timestamp 25 ] state=[-0.06745393  0.00398099  0.13803133  0.42099939], action=0, reward=1.0, next_state=[-0.06737431 -0.1927991   0.14645131  0.75381457]\n",
      "[ episode 146 ][ timestamp 26 ] state=[-0.06737431 -0.1927991   0.14645131  0.75381457], action=1, reward=1.0, next_state=[-7.12302922e-02  3.28620839e-05  1.61527606e-01  5.10565922e-01]\n",
      "[ episode 146 ][ timestamp 27 ] state=[-7.12302922e-02  3.28620839e-05  1.61527606e-01  5.10565922e-01], action=1, reward=1.0, next_state=[-0.07122963  0.1925549   0.17173892  0.27282491]\n",
      "[ episode 146 ][ timestamp 28 ] state=[-0.07122963  0.1925549   0.17173892  0.27282491], action=0, reward=1.0, next_state=[-0.06737854 -0.00454828  0.17719542  0.61437373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 146 ][ timestamp 29 ] state=[-0.06737854 -0.00454828  0.17719542  0.61437373], action=1, reward=1.0, next_state=[-0.0674695   0.1877131   0.1894829   0.38232059]\n",
      "[ episode 146 ][ timestamp 30 ] state=[-0.0674695   0.1877131   0.1894829   0.38232059], action=0, reward=1.0, next_state=[-0.06371524 -0.00952289  0.19712931  0.72825452]\n",
      "[ episode 146 ][ timestamp 31 ] state=[-0.06371524 -0.00952289  0.19712931  0.72825452], action=1, reward=-1.0, next_state=[-0.0639057   0.18240725  0.2116944   0.50351641]\n",
      "[ Ended! ] Episode 146: Exploration_rate=0.483444593917636. Score=31.\n",
      "[ Experience replay ] starts\n",
      "[ episode 147 ] state=[ 0.00292609 -0.02939866  0.02254782  0.0265651 ]\n",
      "[ episode 147 ][ timestamp 1 ] state=[ 0.00292609 -0.02939866  0.02254782  0.0265651 ], action=1, reward=1.0, next_state=[ 0.00233811  0.1653928   0.02307912 -0.25891933]\n",
      "[ episode 147 ][ timestamp 2 ] state=[ 0.00233811  0.1653928   0.02307912 -0.25891933], action=0, reward=1.0, next_state=[ 0.00564597 -0.0300509   0.01790074  0.0409528 ]\n",
      "[ episode 147 ][ timestamp 3 ] state=[ 0.00564597 -0.0300509   0.01790074  0.0409528 ], action=1, reward=1.0, next_state=[ 0.00504495  0.16480985  0.01871979 -0.24602895]\n",
      "[ episode 147 ][ timestamp 4 ] state=[ 0.00504495  0.16480985  0.01871979 -0.24602895], action=1, reward=1.0, next_state=[ 0.00834115  0.3596595   0.01379921 -0.53274892]\n",
      "[ episode 147 ][ timestamp 5 ] state=[ 0.00834115  0.3596595   0.01379921 -0.53274892], action=0, reward=1.0, next_state=[ 0.01553434  0.16434622  0.00314423 -0.23575005]\n",
      "[ episode 147 ][ timestamp 6 ] state=[ 0.01553434  0.16434622  0.00314423 -0.23575005], action=0, reward=1.0, next_state=[ 0.01882126 -0.03082052 -0.00157077  0.05792301]\n",
      "[ episode 147 ][ timestamp 7 ] state=[ 0.01882126 -0.03082052 -0.00157077  0.05792301], action=1, reward=1.0, next_state=[ 0.01820485  0.16432392 -0.00041231 -0.2352551 ]\n",
      "[ episode 147 ][ timestamp 8 ] state=[ 0.01820485  0.16432392 -0.00041231 -0.2352551 ], action=0, reward=1.0, next_state=[ 0.02149133 -0.03079214 -0.00511741  0.05729775]\n",
      "[ episode 147 ][ timestamp 9 ] state=[ 0.02149133 -0.03079214 -0.00511741  0.05729775], action=1, reward=1.0, next_state=[ 0.02087549  0.16440282 -0.00397145 -0.23699536]\n",
      "[ episode 147 ][ timestamp 10 ] state=[ 0.02087549  0.16440282 -0.00397145 -0.23699536], action=0, reward=1.0, next_state=[ 0.02416354 -0.03066217 -0.00871136  0.05443221]\n",
      "[ episode 147 ][ timestamp 11 ] state=[ 0.02416354 -0.03066217 -0.00871136  0.05443221], action=1, reward=1.0, next_state=[ 0.0235503   0.1645836  -0.00762272 -0.24098644]\n",
      "[ episode 147 ][ timestamp 12 ] state=[ 0.0235503   0.1645836  -0.00762272 -0.24098644], action=1, reward=1.0, next_state=[ 0.02684197  0.3598136  -0.01244245 -0.53606399]\n",
      "[ episode 147 ][ timestamp 13 ] state=[ 0.02684197  0.3598136  -0.01244245 -0.53606399], action=1, reward=1.0, next_state=[ 0.03403824  0.55510828 -0.02316373 -0.83264132]\n",
      "[ episode 147 ][ timestamp 14 ] state=[ 0.03403824  0.55510828 -0.02316373 -0.83264132], action=1, reward=1.0, next_state=[ 0.04514041  0.75053897 -0.03981655 -1.13251825]\n",
      "[ episode 147 ][ timestamp 15 ] state=[ 0.04514041  0.75053897 -0.03981655 -1.13251825], action=0, reward=1.0, next_state=[ 0.06015119  0.5559602  -0.06246692 -0.85258438]\n",
      "[ episode 147 ][ timestamp 16 ] state=[ 0.06015119  0.5559602  -0.06246692 -0.85258438], action=0, reward=1.0, next_state=[ 0.07127039  0.36174289 -0.0795186  -0.58017997]\n",
      "[ episode 147 ][ timestamp 17 ] state=[ 0.07127039  0.36174289 -0.0795186  -0.58017997], action=0, reward=1.0, next_state=[ 0.07850525  0.16782    -0.0911222  -0.31356865]\n",
      "[ episode 147 ][ timestamp 18 ] state=[ 0.07850525  0.16782    -0.0911222  -0.31356865], action=1, reward=1.0, next_state=[ 0.08186165  0.3641139  -0.09739358 -0.6335408 ]\n",
      "[ episode 147 ][ timestamp 19 ] state=[ 0.08186165  0.3641139  -0.09739358 -0.6335408 ], action=0, reward=1.0, next_state=[ 0.08914393  0.17047579 -0.11006439 -0.37304858]\n",
      "[ episode 147 ][ timestamp 20 ] state=[ 0.08914393  0.17047579 -0.11006439 -0.37304858], action=1, reward=1.0, next_state=[ 0.09255345  0.36697512 -0.11752536 -0.6983077 ]\n",
      "[ episode 147 ][ timestamp 21 ] state=[ 0.09255345  0.36697512 -0.11752536 -0.6983077 ], action=0, reward=1.0, next_state=[ 0.09989295  0.17366187 -0.13149152 -0.44481105]\n",
      "[ episode 147 ][ timestamp 22 ] state=[ 0.09989295  0.17366187 -0.13149152 -0.44481105], action=1, reward=1.0, next_state=[ 0.10336618  0.3703752  -0.14038774 -0.77588103]\n",
      "[ episode 147 ][ timestamp 23 ] state=[ 0.10336618  0.3703752  -0.14038774 -0.77588103], action=0, reward=1.0, next_state=[ 0.11077369  0.1774345  -0.15590536 -0.53045581]\n",
      "[ episode 147 ][ timestamp 24 ] state=[ 0.11077369  0.1774345  -0.15590536 -0.53045581], action=1, reward=1.0, next_state=[ 0.11432238  0.37436626 -0.16651448 -0.8679214 ]\n",
      "[ episode 147 ][ timestamp 25 ] state=[ 0.11432238  0.37436626 -0.16651448 -0.8679214 ], action=0, reward=1.0, next_state=[ 0.1218097   0.18185364 -0.1838729  -0.63187592]\n",
      "[ episode 147 ][ timestamp 26 ] state=[ 0.1218097   0.18185364 -0.1838729  -0.63187592], action=0, reward=1.0, next_state=[ 0.12544678 -0.01029156 -0.19651042 -0.40227116]\n",
      "[ episode 147 ][ timestamp 27 ] state=[ 0.12544678 -0.01029156 -0.19651042 -0.40227116], action=1, reward=1.0, next_state=[ 0.12524095  0.18699551 -0.20455585 -0.74990918]\n",
      "[ episode 147 ][ timestamp 28 ] state=[ 0.12524095  0.18699551 -0.20455585 -0.74990918], action=0, reward=-1.0, next_state=[ 0.12898086 -0.00480632 -0.21955403 -0.52792556]\n",
      "[ Ended! ] Episode 147: Exploration_rate=0.4810273709480478. Score=28.\n",
      "[ Experience replay ] starts\n",
      "[ episode 148 ] state=[ 0.02102205 -0.02751876 -0.03897069 -0.03222045]\n",
      "[ episode 148 ][ timestamp 1 ] state=[ 0.02102205 -0.02751876 -0.03897069 -0.03222045], action=0, reward=1.0, next_state=[ 0.02047168 -0.22206081 -0.0396151   0.24791658]\n",
      "[ episode 148 ][ timestamp 2 ] state=[ 0.02047168 -0.22206081 -0.0396151   0.24791658], action=0, reward=1.0, next_state=[ 0.01603046 -0.41659527 -0.03465677  0.52784553]\n",
      "[ episode 148 ][ timestamp 3 ] state=[ 0.01603046 -0.41659527 -0.03465677  0.52784553], action=0, reward=1.0, next_state=[ 0.00769856 -0.61121291 -0.02409986  0.80940964]\n",
      "[ episode 148 ][ timestamp 4 ] state=[ 0.00769856 -0.61121291 -0.02409986  0.80940964], action=0, reward=1.0, next_state=[-0.0045257  -0.80599649 -0.00791166  1.0944155 ]\n",
      "[ episode 148 ][ timestamp 5 ] state=[-0.0045257  -0.80599649 -0.00791166  1.0944155 ], action=0, reward=1.0, next_state=[-0.02064563 -1.00101334  0.01397665  1.38460561]\n",
      "[ episode 148 ][ timestamp 6 ] state=[-0.02064563 -1.00101334  0.01397665  1.38460561], action=0, reward=1.0, next_state=[-0.0406659  -1.19630678  0.04166876  1.68162615]\n",
      "[ episode 148 ][ timestamp 7 ] state=[-0.0406659  -1.19630678  0.04166876  1.68162615], action=0, reward=1.0, next_state=[-0.06459204 -1.39188587  0.07530128  1.98698721]\n",
      "[ episode 148 ][ timestamp 8 ] state=[-0.06459204 -1.39188587  0.07530128  1.98698721], action=0, reward=1.0, next_state=[-0.09242975 -1.5877128   0.11504103  2.30201288]\n",
      "[ episode 148 ][ timestamp 9 ] state=[-0.09242975 -1.5877128   0.11504103  2.30201288], action=1, reward=1.0, next_state=[-0.12418401 -1.39381967  0.16108128  2.04684312]\n",
      "[ episode 148 ][ timestamp 10 ] state=[-0.12418401 -1.39381967  0.16108128  2.04684312], action=1, reward=1.0, next_state=[-0.1520604  -1.20067609  0.20201815  1.80803163]\n",
      "[ episode 148 ][ timestamp 11 ] state=[-0.1520604  -1.20067609  0.20201815  1.80803163], action=1, reward=-1.0, next_state=[-0.17607392 -1.00829997  0.23817878  1.58432598]\n",
      "[ Ended! ] Episode 148: Exploration_rate=0.47862223409330756. Score=11.\n",
      "[ Experience replay ] starts\n",
      "[ episode 149 ] state=[ 0.00411082 -0.00211621  0.01991598  0.00631159]\n",
      "[ episode 149 ][ timestamp 1 ] state=[ 0.00411082 -0.00211621  0.01991598  0.00631159], action=1, reward=1.0, next_state=[ 0.0040685   0.19271454  0.02004221 -0.28002167]\n",
      "[ episode 149 ][ timestamp 2 ] state=[ 0.0040685   0.19271454  0.02004221 -0.28002167], action=0, reward=1.0, next_state=[ 0.00792279 -0.00268749  0.01444177  0.01891453]\n",
      "[ episode 149 ][ timestamp 3 ] state=[ 0.00792279 -0.00268749  0.01444177  0.01891453], action=0, reward=1.0, next_state=[ 0.00786904 -0.19801354  0.01482006  0.31611879]\n",
      "[ episode 149 ][ timestamp 4 ] state=[ 0.00786904 -0.19801354  0.01482006  0.31611879], action=1, reward=1.0, next_state=[ 0.00390877 -0.00310579  0.02114244  0.0281462 ]\n",
      "[ episode 149 ][ timestamp 5 ] state=[ 0.00390877 -0.00310579  0.02114244  0.0281462 ], action=0, reward=1.0, next_state=[ 0.00384665 -0.19852446  0.02170536  0.32742411]\n",
      "[ episode 149 ][ timestamp 6 ] state=[ 0.00384665 -0.19852446  0.02170536  0.32742411], action=1, reward=1.0, next_state=[-0.00012384 -0.00371814  0.02825385  0.04166434]\n",
      "[ episode 149 ][ timestamp 7 ] state=[-0.00012384 -0.00371814  0.02825385  0.04166434], action=0, reward=1.0, next_state=[-1.98201628e-04 -1.99233610e-01  2.90871328e-02  3.43126021e-01]\n",
      "[ episode 149 ][ timestamp 8 ] state=[-1.98201628e-04 -1.99233610e-01  2.90871328e-02  3.43126021e-01], action=0, reward=1.0, next_state=[-0.00418287 -0.39475704  0.03594965  0.64483751]\n",
      "[ episode 149 ][ timestamp 9 ] state=[-0.00418287 -0.39475704  0.03594965  0.64483751], action=1, reward=1.0, next_state=[-0.01207801 -0.20015403  0.0488464   0.36368852]\n",
      "[ episode 149 ][ timestamp 10 ] state=[-0.01207801 -0.20015403  0.0488464   0.36368852], action=0, reward=1.0, next_state=[-0.0160811  -0.39593494  0.05612017  0.67136475]\n",
      "[ episode 149 ][ timestamp 11 ] state=[-0.0160811  -0.39593494  0.05612017  0.67136475], action=1, reward=1.0, next_state=[-0.02399979 -0.20163621  0.06954747  0.39686616]\n",
      "[ episode 149 ][ timestamp 12 ] state=[-0.02399979 -0.20163621  0.06954747  0.39686616], action=1, reward=1.0, next_state=[-0.02803252 -0.00756635  0.07748479  0.12689558]\n",
      "[ episode 149 ][ timestamp 13 ] state=[-0.02803252 -0.00756635  0.07748479  0.12689558], action=1, reward=1.0, next_state=[-0.02818385  0.18636499  0.0800227  -0.14037087]\n",
      "[ episode 149 ][ timestamp 14 ] state=[-0.02818385  0.18636499  0.0800227  -0.14037087], action=0, reward=1.0, next_state=[-0.02445655 -0.00980645  0.07721529  0.17644622]\n",
      "[ episode 149 ][ timestamp 15 ] state=[-0.02445655 -0.00980645  0.07721529  0.17644622], action=0, reward=1.0, next_state=[-0.02465267 -0.20594365  0.08074421  0.49245413]\n",
      "[ episode 149 ][ timestamp 16 ] state=[-0.02465267 -0.20594365  0.08074421  0.49245413], action=1, reward=1.0, next_state=[-0.02877155 -0.01204788  0.09059329  0.22627106]\n",
      "[ episode 149 ][ timestamp 17 ] state=[-0.02877155 -0.01204788  0.09059329  0.22627106], action=0, reward=1.0, next_state=[-0.0290125  -0.2083399   0.09511871  0.54609968]\n",
      "[ episode 149 ][ timestamp 18 ] state=[-0.0290125  -0.2083399   0.09511871  0.54609968], action=1, reward=1.0, next_state=[-0.0331793  -0.01467406  0.10604071  0.28483685]\n",
      "[ episode 149 ][ timestamp 19 ] state=[-0.0331793  -0.01467406  0.10604071  0.28483685], action=1, reward=1.0, next_state=[-0.03347278  0.17878828  0.11173745  0.02739094]\n",
      "[ episode 149 ][ timestamp 20 ] state=[-0.03347278  0.17878828  0.11173745  0.02739094], action=0, reward=1.0, next_state=[-0.02989702 -0.01774397  0.11228526  0.3531334 ]\n",
      "[ episode 149 ][ timestamp 21 ] state=[-0.02989702 -0.01774397  0.11228526  0.3531334 ], action=0, reward=1.0, next_state=[-0.0302519  -0.21426858  0.11934793  0.67900648]\n",
      "[ episode 149 ][ timestamp 22 ] state=[-0.0302519  -0.21426858  0.11934793  0.67900648], action=1, reward=1.0, next_state=[-0.03453727 -0.02098898  0.13292806  0.42615447]\n",
      "[ episode 149 ][ timestamp 23 ] state=[-0.03453727 -0.02098898  0.13292806  0.42615447], action=1, reward=1.0, next_state=[-0.03495705  0.17202437  0.14145115  0.17815444]\n",
      "[ episode 149 ][ timestamp 24 ] state=[-0.03495705  0.17202437  0.14145115  0.17815444], action=1, reward=1.0, next_state=[-0.03151656  0.36486857  0.14501424 -0.0667747 ]\n",
      "[ episode 149 ][ timestamp 25 ] state=[-0.03151656  0.36486857  0.14501424 -0.0667747 ], action=0, reward=1.0, next_state=[-0.02421919  0.16799737  0.14367875  0.26791742]\n",
      "[ episode 149 ][ timestamp 26 ] state=[-0.02421919  0.16799737  0.14367875  0.26791742], action=0, reward=1.0, next_state=[-0.02085924 -0.02885157  0.14903709  0.60224469]\n",
      "[ episode 149 ][ timestamp 27 ] state=[-0.02085924 -0.02885157  0.14903709  0.60224469], action=1, reward=1.0, next_state=[-0.02143627  0.16390605  0.16108199  0.35996835]\n",
      "[ episode 149 ][ timestamp 28 ] state=[-0.02143627  0.16390605  0.16108199  0.35996835], action=1, reward=1.0, next_state=[-0.01815815  0.35641554  0.16828135  0.12209594]\n",
      "[ episode 149 ][ timestamp 29 ] state=[-0.01815815  0.35641554  0.16828135  0.12209594], action=0, reward=1.0, next_state=[-0.01102984  0.15933246  0.17072327  0.46278612]\n",
      "[ episode 149 ][ timestamp 30 ] state=[-0.01102984  0.15933246  0.17072327  0.46278612], action=1, reward=1.0, next_state=[-0.00784319  0.35168254  0.179979    0.22840472]\n",
      "[ episode 149 ][ timestamp 31 ] state=[-0.00784319  0.35168254  0.179979    0.22840472], action=0, reward=1.0, next_state=[-0.00080954  0.15450623  0.18454709  0.57202044]\n",
      "[ episode 149 ][ timestamp 32 ] state=[-0.00080954  0.15450623  0.18454709  0.57202044], action=0, reward=1.0, next_state=[ 0.00228058 -0.04265834  0.1959875   0.91669476]\n",
      "[ episode 149 ][ timestamp 33 ] state=[ 0.00228058 -0.04265834  0.1959875   0.91669476], action=1, reward=-1.0, next_state=[0.00142742 0.14935115 0.21432139 0.69144646]\n",
      "[ Ended! ] Episode 149: Exploration_rate=0.47622912292284103. Score=33.\n",
      "[ Experience replay ] starts\n",
      "[ episode 150 ] state=[-0.00022005  0.0277343   0.00207141  0.00244835]\n",
      "[ episode 150 ][ timestamp 1 ] state=[-0.00022005  0.0277343   0.00207141  0.00244835], action=1, reward=1.0, next_state=[ 0.00033464  0.22282648  0.00212038 -0.28958031]\n",
      "[ episode 150 ][ timestamp 2 ] state=[ 0.00033464  0.22282648  0.00212038 -0.28958031], action=1, reward=1.0, next_state=[ 0.00479117  0.41791814 -0.00367123 -0.58159374]\n",
      "[ episode 150 ][ timestamp 3 ] state=[ 0.00479117  0.41791814 -0.00367123 -0.58159374], action=1, reward=1.0, next_state=[ 0.01314953  0.61309133 -0.0153031  -0.8754309 ]\n",
      "[ episode 150 ][ timestamp 4 ] state=[ 0.01314953  0.61309133 -0.0153031  -0.8754309 ], action=0, reward=1.0, next_state=[ 0.02541135  0.41818071 -0.03281172 -0.58759814]\n",
      "[ episode 150 ][ timestamp 5 ] state=[ 0.02541135  0.41818071 -0.03281172 -0.58759814], action=1, reward=1.0, next_state=[ 0.03377497  0.61374645 -0.04456368 -0.89043375]\n",
      "[ episode 150 ][ timestamp 6 ] state=[ 0.03377497  0.61374645 -0.04456368 -0.89043375], action=1, reward=1.0, next_state=[ 0.0460499   0.80944378 -0.06237236 -1.19678571]\n",
      "[ episode 150 ][ timestamp 7 ] state=[ 0.0460499   0.80944378 -0.06237236 -1.19678571], action=0, reward=1.0, next_state=[ 0.06223877  0.61518215 -0.08630807 -0.92428547]\n",
      "[ episode 150 ][ timestamp 8 ] state=[ 0.06223877  0.61518215 -0.08630807 -0.92428547], action=0, reward=1.0, next_state=[ 0.07454242  0.4213254  -0.10479378 -0.6599258 ]\n",
      "[ episode 150 ][ timestamp 9 ] state=[ 0.07454242  0.4213254  -0.10479378 -0.6599258 ], action=0, reward=1.0, next_state=[ 0.08296892  0.22780585 -0.1179923  -0.40199192]\n",
      "[ episode 150 ][ timestamp 10 ] state=[ 0.08296892  0.22780585 -0.1179923  -0.40199192], action=1, reward=1.0, next_state=[ 0.08752504  0.42438644 -0.12603214 -0.72942185]\n",
      "[ episode 150 ][ timestamp 11 ] state=[ 0.08752504  0.42438644 -0.12603214 -0.72942185], action=1, reward=1.0, next_state=[ 0.09601277  0.62100424 -0.14062057 -1.05896476]\n",
      "[ episode 150 ][ timestamp 12 ] state=[ 0.09601277  0.62100424 -0.14062057 -1.05896476], action=1, reward=1.0, next_state=[ 0.10843285  0.81768037 -0.16179987 -1.39227327]\n",
      "[ episode 150 ][ timestamp 13 ] state=[ 0.10843285  0.81768037 -0.16179987 -1.39227327], action=0, reward=1.0, next_state=[ 0.12478646  0.6249     -0.18964533 -1.15424146]\n",
      "[ episode 150 ][ timestamp 14 ] state=[ 0.12478646  0.6249     -0.18964533 -1.15424146], action=0, reward=-1.0, next_state=[ 0.13728446  0.43268786 -0.21273016 -0.92651459]\n",
      "[ Ended! ] Episode 150: Exploration_rate=0.4738479773082268. Score=14.\n",
      "[ Experience replay ] starts\n",
      "[ episode 151 ] state=[-0.00339615  0.03737656  0.04057464  0.04518783]\n",
      "[ episode 151 ][ timestamp 1 ] state=[-0.00339615  0.03737656  0.04057464  0.04518783], action=0, reward=1.0, next_state=[-0.00264862 -0.15830301  0.0414784   0.35039128]\n",
      "[ episode 151 ][ timestamp 2 ] state=[-0.00264862 -0.15830301  0.0414784   0.35039128], action=1, reward=1.0, next_state=[-0.00581468  0.03620526  0.04848622  0.07107098]\n",
      "[ episode 151 ][ timestamp 3 ] state=[-0.00581468  0.03620526  0.04848622  0.07107098], action=1, reward=1.0, next_state=[-0.00509057  0.23059976  0.04990764 -0.20592872]\n",
      "[ episode 151 ][ timestamp 4 ] state=[-0.00509057  0.23059976  0.04990764 -0.20592872], action=0, reward=1.0, next_state=[-0.00047858  0.03480097  0.04578907  0.10207053]\n",
      "[ episode 151 ][ timestamp 5 ] state=[-0.00047858  0.03480097  0.04578907  0.10207053], action=0, reward=1.0, next_state=[ 2.17443660e-04 -1.60946276e-01  4.78304765e-02  4.08840920e-01]\n",
      "[ episode 151 ][ timestamp 6 ] state=[ 2.17443660e-04 -1.60946276e-01  4.78304765e-02  4.08840920e-01], action=0, reward=1.0, next_state=[-0.00300148 -0.3567126   0.05600729  0.71621137]\n",
      "[ episode 151 ][ timestamp 7 ] state=[-0.00300148 -0.3567126   0.05600729  0.71621137], action=1, reward=1.0, next_state=[-0.01013573 -0.16240873  0.07033152  0.44167011]\n",
      "[ episode 151 ][ timestamp 8 ] state=[-0.01013573 -0.16240873  0.07033152  0.44167011], action=1, reward=1.0, next_state=[-0.01338391  0.03165114  0.07916492  0.17196037]\n",
      "[ episode 151 ][ timestamp 9 ] state=[-0.01338391  0.03165114  0.07916492  0.17196037], action=1, reward=1.0, next_state=[-0.01275089  0.22555603  0.08260413 -0.09473585]\n",
      "[ episode 151 ][ timestamp 10 ] state=[-0.01275089  0.22555603  0.08260413 -0.09473585], action=0, reward=1.0, next_state=[-0.00823976  0.02935326  0.08070941  0.2228228 ]\n",
      "[ episode 151 ][ timestamp 11 ] state=[-0.00823976  0.02935326  0.08070941  0.2228228 ], action=1, reward=1.0, next_state=[-0.0076527   0.22323444  0.08516587 -0.04334945]\n",
      "[ episode 151 ][ timestamp 12 ] state=[-0.0076527   0.22323444  0.08516587 -0.04334945], action=0, reward=1.0, next_state=[-0.00318801  0.02700103  0.08429888  0.27494232]\n",
      "[ episode 151 ][ timestamp 13 ] state=[-0.00318801  0.02700103  0.08429888  0.27494232], action=0, reward=1.0, next_state=[-0.00264799 -0.16921618  0.08979773  0.59297749]\n",
      "[ episode 151 ][ timestamp 14 ] state=[-0.00264799 -0.16921618  0.08979773  0.59297749], action=1, reward=1.0, next_state=[-0.00603231  0.0245416   0.10165728  0.32987689]\n",
      "[ episode 151 ][ timestamp 15 ] state=[-0.00603231  0.0245416   0.10165728  0.32987689], action=0, reward=1.0, next_state=[-0.00554148 -0.17186946  0.10825482  0.65280827]\n",
      "[ episode 151 ][ timestamp 16 ] state=[-0.00554148 -0.17186946  0.10825482  0.65280827], action=1, reward=1.0, next_state=[-0.00897887  0.02159174  0.12131098  0.39608   ]\n",
      "[ episode 151 ][ timestamp 17 ] state=[-0.00897887  0.02159174  0.12131098  0.39608   ], action=1, reward=1.0, next_state=[-0.00854704  0.21480242  0.12923258  0.14397189]\n",
      "[ episode 151 ][ timestamp 18 ] state=[-0.00854704  0.21480242  0.12923258  0.14397189], action=1, reward=1.0, next_state=[-0.00425099  0.40785955  0.13211202 -0.10531026]\n",
      "[ episode 151 ][ timestamp 19 ] state=[-0.00425099  0.40785955  0.13211202 -0.10531026], action=1, reward=1.0, next_state=[ 0.0039062   0.60086516  0.13000581 -0.35356782]\n",
      "[ episode 151 ][ timestamp 20 ] state=[ 0.0039062   0.60086516  0.13000581 -0.35356782], action=1, reward=1.0, next_state=[ 0.01592351  0.79392198  0.12293446 -0.60259514]\n",
      "[ episode 151 ][ timestamp 21 ] state=[ 0.01592351  0.79392198  0.12293446 -0.60259514], action=1, reward=1.0, next_state=[ 0.03180195  0.98712944  0.11088255 -0.85416739]\n",
      "[ episode 151 ][ timestamp 22 ] state=[ 0.03180195  0.98712944  0.11088255 -0.85416739], action=0, reward=1.0, next_state=[ 0.05154453  0.79068504  0.09379921 -0.52877768]\n",
      "[ episode 151 ][ timestamp 23 ] state=[ 0.05154453  0.79068504  0.09379921 -0.52877768], action=1, reward=1.0, next_state=[ 0.06735824  0.98437088  0.08322365 -0.79049275]\n",
      "[ episode 151 ][ timestamp 24 ] state=[ 0.06735824  0.98437088  0.08322365 -0.79049275], action=0, reward=1.0, next_state=[ 0.08704565  0.78821074  0.0674138  -0.47283142]\n",
      "[ episode 151 ][ timestamp 25 ] state=[ 0.08704565  0.78821074  0.0674138  -0.47283142], action=1, reward=1.0, next_state=[ 0.10280987  0.98231905  0.05795717 -0.74352787]\n",
      "[ episode 151 ][ timestamp 26 ] state=[ 0.10280987  0.98231905  0.05795717 -0.74352787], action=0, reward=1.0, next_state=[ 0.12245625  0.7864471   0.04308661 -0.43318339]\n",
      "[ episode 151 ][ timestamp 27 ] state=[ 0.12245625  0.7864471   0.04308661 -0.43318339], action=0, reward=1.0, next_state=[ 0.13818519  0.59074244  0.03442294 -0.1272353 ]\n",
      "[ episode 151 ][ timestamp 28 ] state=[ 0.13818519  0.59074244  0.03442294 -0.1272353 ], action=0, reward=1.0, next_state=[0.15000004 0.3951447  0.03187824 0.17610584]\n",
      "[ episode 151 ][ timestamp 29 ] state=[0.15000004 0.3951447  0.03187824 0.17610584], action=0, reward=1.0, next_state=[0.15790293 0.19958138 0.03540036 0.4786724 ]\n",
      "[ episode 151 ][ timestamp 30 ] state=[0.15790293 0.19958138 0.03540036 0.4786724 ], action=1, reward=1.0, next_state=[0.16189456 0.39418614 0.0449738  0.19735368]\n",
      "[ episode 151 ][ timestamp 31 ] state=[0.16189456 0.39418614 0.0449738  0.19735368], action=1, reward=1.0, next_state=[ 0.16977828  0.58863692  0.04892088 -0.08080972]\n",
      "[ episode 151 ][ timestamp 32 ] state=[ 0.16977828  0.58863692  0.04892088 -0.08080972], action=0, reward=1.0, next_state=[0.18155102 0.39284906 0.04730468 0.22689772]\n",
      "[ episode 151 ][ timestamp 33 ] state=[0.18155102 0.39284906 0.04730468 0.22689772], action=0, reward=1.0, next_state=[0.189408   0.1970841  0.05184264 0.53411905]\n",
      "[ episode 151 ][ timestamp 34 ] state=[0.189408   0.1970841  0.05184264 0.53411905], action=1, reward=1.0, next_state=[0.19334969 0.39144012 0.06252502 0.25821161]\n",
      "[ episode 151 ][ timestamp 35 ] state=[0.19334969 0.39144012 0.06252502 0.25821161], action=0, reward=1.0, next_state=[0.20117849 0.19548385 0.06768925 0.56994204]\n",
      "[ episode 151 ][ timestamp 36 ] state=[0.20117849 0.19548385 0.06768925 0.56994204], action=1, reward=1.0, next_state=[0.20508817 0.38959446 0.07908809 0.29932835]\n",
      "[ episode 151 ][ timestamp 37 ] state=[0.20508817 0.38959446 0.07908809 0.29932835], action=1, reward=1.0, next_state=[0.21288005 0.58350524 0.08507466 0.03259905]\n",
      "[ episode 151 ][ timestamp 38 ] state=[0.21288005 0.58350524 0.08507466 0.03259905], action=0, reward=1.0, next_state=[0.22455016 0.38727283 0.08572664 0.35086489]\n",
      "[ episode 151 ][ timestamp 39 ] state=[0.22455016 0.38727283 0.08572664 0.35086489], action=1, reward=1.0, next_state=[0.23229562 0.58107767 0.09274394 0.08639796]\n",
      "[ episode 151 ][ timestamp 40 ] state=[0.23229562 0.58107767 0.09274394 0.08639796], action=1, reward=1.0, next_state=[ 0.24391717  0.77475628  0.0944719  -0.17564377]\n",
      "[ episode 151 ][ timestamp 41 ] state=[ 0.24391717  0.77475628  0.0944719  -0.17564377], action=0, reward=1.0, next_state=[0.25941229 0.57841819 0.09095902 0.14528356]\n",
      "[ episode 151 ][ timestamp 42 ] state=[0.25941229 0.57841819 0.09095902 0.14528356], action=0, reward=1.0, next_state=[0.27098066 0.38211931 0.09386469 0.46521974]\n",
      "[ episode 151 ][ timestamp 43 ] state=[0.27098066 0.38211931 0.09386469 0.46521974], action=1, reward=1.0, next_state=[0.27862304 0.5757983  0.10316909 0.20353585]\n",
      "[ episode 151 ][ timestamp 44 ] state=[0.27862304 0.5757983  0.10316909 0.20353585], action=1, reward=1.0, next_state=[ 0.29013901  0.76930512  0.1072398  -0.05490307]\n",
      "[ episode 151 ][ timestamp 45 ] state=[ 0.29013901  0.76930512  0.1072398  -0.05490307], action=0, reward=1.0, next_state=[0.30552511 0.57282198 0.10614174 0.26959665]\n",
      "[ episode 151 ][ timestamp 46 ] state=[0.30552511 0.57282198 0.10614174 0.26959665], action=1, reward=1.0, next_state=[0.31698155 0.76628174 0.11153367 0.01218723]\n",
      "[ episode 151 ][ timestamp 47 ] state=[0.31698155 0.76628174 0.11153367 0.01218723], action=1, reward=1.0, next_state=[ 0.33230719  0.95964211  0.11177742 -0.24332822]\n",
      "[ episode 151 ][ timestamp 48 ] state=[ 0.33230719  0.95964211  0.11177742 -0.24332822], action=0, reward=1.0, next_state=[0.35150003 0.76311579 0.10691085 0.08241577]\n",
      "[ episode 151 ][ timestamp 49 ] state=[0.35150003 0.76311579 0.10691085 0.08241577], action=0, reward=1.0, next_state=[0.36676235 0.56663664 0.10855917 0.40682374]\n",
      "[ episode 151 ][ timestamp 50 ] state=[0.36676235 0.56663664 0.10855917 0.40682374], action=0, reward=1.0, next_state=[0.37809508 0.37015626 0.11669564 0.73166311]\n",
      "[ episode 151 ][ timestamp 51 ] state=[0.37809508 0.37015626 0.11669564 0.73166311], action=1, reward=1.0, next_state=[0.3854982  0.56348881 0.13132891 0.47786732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 151 ][ timestamp 52 ] state=[0.3854982  0.56348881 0.13132891 0.47786732], action=0, reward=1.0, next_state=[0.39676798 0.36678099 0.14088625 0.80888801]\n",
      "[ episode 151 ][ timestamp 53 ] state=[0.39676798 0.36678099 0.14088625 0.80888801], action=1, reward=1.0, next_state=[0.4041036  0.55972032 0.15706401 0.56363016]\n",
      "[ episode 151 ][ timestamp 54 ] state=[0.4041036  0.55972032 0.15706401 0.56363016], action=1, reward=1.0, next_state=[0.41529801 0.75233027 0.16833662 0.32425876]\n",
      "[ episode 151 ][ timestamp 55 ] state=[0.41529801 0.75233027 0.16833662 0.32425876], action=0, reward=1.0, next_state=[0.43034461 0.55526143 0.17482179 0.66494117]\n",
      "[ episode 151 ][ timestamp 56 ] state=[0.43034461 0.55526143 0.17482179 0.66494117], action=1, reward=1.0, next_state=[0.44144984 0.74757627 0.18812062 0.43200213]\n",
      "[ episode 151 ][ timestamp 57 ] state=[0.44144984 0.74757627 0.18812062 0.43200213], action=0, reward=1.0, next_state=[0.45640136 0.55035834 0.19676066 0.7775917 ]\n",
      "[ episode 151 ][ timestamp 58 ] state=[0.45640136 0.55035834 0.19676066 0.7775917 ], action=1, reward=-1.0, next_state=[0.46740853 0.74230927 0.21231249 0.55269594]\n",
      "[ Ended! ] Episode 151: Exploration_rate=0.47147873742168567. Score=58.\n",
      "[ Experience replay ] starts\n",
      "[ episode 152 ] state=[-0.00419706 -0.0307659   0.03599952  0.03711562]\n",
      "[ episode 152 ][ timestamp 1 ] state=[-0.00419706 -0.0307659   0.03599952  0.03711562], action=1, reward=1.0, next_state=[-0.00481238  0.16382182  0.03674184 -0.24399526]\n",
      "[ episode 152 ][ timestamp 2 ] state=[-0.00481238  0.16382182  0.03674184 -0.24399526], action=0, reward=1.0, next_state=[-0.00153594 -0.03180514  0.03186193  0.0600468 ]\n",
      "[ episode 152 ][ timestamp 3 ] state=[-0.00153594 -0.03180514  0.03186193  0.0600468 ], action=1, reward=1.0, next_state=[-0.00217204  0.16284583  0.03306287 -0.22241564]\n",
      "[ episode 152 ][ timestamp 4 ] state=[-0.00217204  0.16284583  0.03306287 -0.22241564], action=1, reward=1.0, next_state=[ 0.00108487  0.35747999  0.02861455 -0.50448861]\n",
      "[ episode 152 ][ timestamp 5 ] state=[ 0.00108487  0.35747999  0.02861455 -0.50448861], action=0, reward=1.0, next_state=[ 0.00823447  0.1619667   0.01852478 -0.2029272 ]\n",
      "[ episode 152 ][ timestamp 6 ] state=[ 0.00823447  0.1619667   0.01852478 -0.2029272 ], action=0, reward=1.0, next_state=[ 0.01147381 -0.03341521  0.01446624  0.09554136]\n",
      "[ episode 152 ][ timestamp 7 ] state=[ 0.01147381 -0.03341521  0.01446624  0.09554136], action=0, reward=1.0, next_state=[ 0.0108055  -0.22874148  0.01637706  0.39275303]\n",
      "[ episode 152 ][ timestamp 8 ] state=[ 0.0108055  -0.22874148  0.01637706  0.39275303], action=0, reward=1.0, next_state=[ 0.00623067 -0.42409196  0.02423212  0.6905541 ]\n",
      "[ episode 152 ][ timestamp 9 ] state=[ 0.00623067 -0.42409196  0.02423212  0.6905541 ], action=1, reward=1.0, next_state=[-0.00225117 -0.2293145   0.03804321  0.40559722]\n",
      "[ episode 152 ][ timestamp 10 ] state=[-0.00225117 -0.2293145   0.03804321  0.40559722], action=1, reward=1.0, next_state=[-0.00683746 -0.0347521   0.04615515  0.1251468 ]\n",
      "[ episode 152 ][ timestamp 11 ] state=[-0.00683746 -0.0347521   0.04615515  0.1251468 ], action=1, reward=1.0, next_state=[-0.0075325   0.15967928  0.04865809 -0.15262488]\n",
      "[ episode 152 ][ timestamp 12 ] state=[-0.0075325   0.15967928  0.04865809 -0.15262488], action=1, reward=1.0, next_state=[-0.00433891  0.35407195  0.04560559 -0.42956894]\n",
      "[ episode 152 ][ timestamp 13 ] state=[-0.00433891  0.35407195  0.04560559 -0.42956894], action=0, reward=1.0, next_state=[ 0.00274253  0.15833483  0.03701421 -0.12286515]\n",
      "[ episode 152 ][ timestamp 14 ] state=[ 0.00274253  0.15833483  0.03701421 -0.12286515], action=1, reward=1.0, next_state=[ 0.00590922  0.35290748  0.03455691 -0.40364451]\n",
      "[ episode 152 ][ timestamp 15 ] state=[ 0.00590922  0.35290748  0.03455691 -0.40364451], action=0, reward=1.0, next_state=[ 0.01296737  0.1573129   0.02648402 -0.1002701 ]\n",
      "[ episode 152 ][ timestamp 16 ] state=[ 0.01296737  0.1573129   0.02648402 -0.1002701 ], action=1, reward=1.0, next_state=[ 0.01611363  0.35204548  0.02447862 -0.38448114]\n",
      "[ episode 152 ][ timestamp 17 ] state=[ 0.01611363  0.35204548  0.02447862 -0.38448114], action=0, reward=1.0, next_state=[ 0.02315454  0.15658471  0.01678899 -0.08418183]\n",
      "[ episode 152 ][ timestamp 18 ] state=[ 0.02315454  0.15658471  0.01678899 -0.08418183], action=1, reward=1.0, next_state=[ 0.02628623  0.35146202  0.01510536 -0.37152087]\n",
      "[ episode 152 ][ timestamp 19 ] state=[ 0.02628623  0.35146202  0.01510536 -0.37152087], action=0, reward=1.0, next_state=[ 0.03331547  0.15612877  0.00767494 -0.07411361]\n",
      "[ episode 152 ][ timestamp 20 ] state=[ 0.03331547  0.15612877  0.00767494 -0.07411361], action=0, reward=1.0, next_state=[ 0.03643805 -0.03910237  0.00619267  0.22098088]\n",
      "[ episode 152 ][ timestamp 21 ] state=[ 0.03643805 -0.03910237  0.00619267  0.22098088], action=1, reward=1.0, next_state=[ 0.035656    0.15593052  0.01061228 -0.06974221]\n",
      "[ episode 152 ][ timestamp 22 ] state=[ 0.035656    0.15593052  0.01061228 -0.06974221], action=1, reward=1.0, next_state=[ 0.03877461  0.35089873  0.00921744 -0.35905811]\n",
      "[ episode 152 ][ timestamp 23 ] state=[ 0.03877461  0.35089873  0.00921744 -0.35905811], action=0, reward=1.0, next_state=[ 0.04579259  0.15564697  0.00203628 -0.06348302]\n",
      "[ episode 152 ][ timestamp 24 ] state=[ 0.04579259  0.15564697  0.00203628 -0.06348302], action=1, reward=1.0, next_state=[ 0.04890553  0.35073967  0.00076662 -0.35552279]\n",
      "[ episode 152 ][ timestamp 25 ] state=[ 0.04890553  0.35073967  0.00076662 -0.35552279], action=1, reward=1.0, next_state=[ 0.05592032  0.54585071 -0.00634384 -0.64796388]\n",
      "[ episode 152 ][ timestamp 26 ] state=[ 0.05592032  0.54585071 -0.00634384 -0.64796388], action=0, reward=1.0, next_state=[ 0.06683733  0.35081771 -0.01930312 -0.35728535]\n",
      "[ episode 152 ][ timestamp 27 ] state=[ 0.06683733  0.35081771 -0.01930312 -0.35728535], action=1, reward=1.0, next_state=[ 0.07385369  0.5462087  -0.02644882 -0.65599199]\n",
      "[ episode 152 ][ timestamp 28 ] state=[ 0.07385369  0.5462087  -0.02644882 -0.65599199], action=0, reward=1.0, next_state=[ 0.08477786  0.35146475 -0.03956866 -0.37175328]\n",
      "[ episode 152 ][ timestamp 29 ] state=[ 0.08477786  0.35146475 -0.03956866 -0.37175328], action=1, reward=1.0, next_state=[ 0.09180716  0.54712584 -0.04700373 -0.67664535]\n",
      "[ episode 152 ][ timestamp 30 ] state=[ 0.09180716  0.54712584 -0.04700373 -0.67664535], action=0, reward=1.0, next_state=[ 0.10274967  0.35268742 -0.06053664 -0.39912385]\n",
      "[ episode 152 ][ timestamp 31 ] state=[ 0.10274967  0.35268742 -0.06053664 -0.39912385], action=1, reward=1.0, next_state=[ 0.10980342  0.54861356 -0.06851911 -0.71026163]\n",
      "[ episode 152 ][ timestamp 32 ] state=[ 0.10980342  0.54861356 -0.06851911 -0.71026163], action=0, reward=1.0, next_state=[ 0.12077569  0.35450409 -0.08272435 -0.43990949]\n",
      "[ episode 152 ][ timestamp 33 ] state=[ 0.12077569  0.35450409 -0.08272435 -0.43990949], action=0, reward=1.0, next_state=[ 0.12786578  0.16064437 -0.09152253 -0.17440755]\n",
      "[ episode 152 ][ timestamp 34 ] state=[ 0.12786578  0.16064437 -0.09152253 -0.17440755], action=0, reward=1.0, next_state=[ 0.13107866 -0.03305666 -0.09501069  0.08805787]\n",
      "[ episode 152 ][ timestamp 35 ] state=[ 0.13107866 -0.03305666 -0.09501069  0.08805787], action=0, reward=1.0, next_state=[ 0.13041753 -0.2266974  -0.09324953  0.34931784]\n",
      "[ episode 152 ][ timestamp 36 ] state=[ 0.13041753 -0.2266974  -0.09324953  0.34931784], action=0, reward=1.0, next_state=[ 0.12588358 -0.42037797 -0.08626317  0.61120085]\n",
      "[ episode 152 ][ timestamp 37 ] state=[ 0.12588358 -0.42037797 -0.08626317  0.61120085], action=1, reward=1.0, next_state=[ 0.11747602 -0.22416298 -0.07403915  0.29264283]\n",
      "[ episode 152 ][ timestamp 38 ] state=[ 0.11747602 -0.22416298 -0.07403915  0.29264283], action=1, reward=1.0, next_state=[ 0.11299276 -0.02806776 -0.0681863  -0.02244177]\n",
      "[ episode 152 ][ timestamp 39 ] state=[ 0.11299276 -0.02806776 -0.0681863  -0.02244177], action=0, reward=1.0, next_state=[ 0.11243141 -0.22214896 -0.06863513  0.24797228]\n",
      "[ episode 152 ][ timestamp 40 ] state=[ 0.11243141 -0.22214896 -0.06863513  0.24797228], action=1, reward=1.0, next_state=[ 0.10798843 -0.02611735 -0.06367569 -0.06554571]\n",
      "[ episode 152 ][ timestamp 41 ] state=[ 0.10798843 -0.02611735 -0.06367569 -0.06554571], action=0, reward=1.0, next_state=[ 0.10746608 -0.22027131 -0.0649866   0.20638701]\n",
      "[ episode 152 ][ timestamp 42 ] state=[ 0.10746608 -0.22027131 -0.0649866   0.20638701], action=1, reward=1.0, next_state=[ 0.10306066 -0.02428315 -0.06085886 -0.10606727]\n",
      "[ episode 152 ][ timestamp 43 ] state=[ 0.10306066 -0.02428315 -0.06085886 -0.10606727], action=1, reward=1.0, next_state=[ 0.10257499  0.17165575 -0.06298021 -0.41731296]\n",
      "[ episode 152 ][ timestamp 44 ] state=[ 0.10257499  0.17165575 -0.06298021 -0.41731296], action=1, reward=1.0, next_state=[ 0.10600811  0.36761105 -0.07132647 -0.72916711]\n",
      "[ episode 152 ][ timestamp 45 ] state=[ 0.10600811  0.36761105 -0.07132647 -0.72916711], action=1, reward=1.0, next_state=[ 0.11336033  0.56364264 -0.08590981 -1.04341904]\n",
      "[ episode 152 ][ timestamp 46 ] state=[ 0.11336033  0.56364264 -0.08590981 -1.04341904], action=0, reward=1.0, next_state=[ 0.12463318  0.36976002 -0.10677819 -0.77889408]\n",
      "[ episode 152 ][ timestamp 47 ] state=[ 0.12463318  0.36976002 -0.10677819 -0.77889408], action=1, reward=1.0, next_state=[ 0.13202838  0.56617539 -0.12235607 -1.10317232]\n",
      "[ episode 152 ][ timestamp 48 ] state=[ 0.13202838  0.56617539 -0.12235607 -1.10317232], action=0, reward=1.0, next_state=[ 0.14335189  0.37285655 -0.14441952 -0.85124498]\n",
      "[ episode 152 ][ timestamp 49 ] state=[ 0.14335189  0.37285655 -0.14441952 -0.85124498], action=1, reward=1.0, next_state=[ 0.15080902  0.56962106 -0.16144442 -1.18563105]\n",
      "[ episode 152 ][ timestamp 50 ] state=[ 0.15080902  0.56962106 -0.16144442 -1.18563105], action=0, reward=1.0, next_state=[ 0.16220144  0.37691831 -0.18515704 -0.94759449]\n",
      "[ episode 152 ][ timestamp 51 ] state=[ 0.16220144  0.37691831 -0.18515704 -0.94759449], action=0, reward=1.0, next_state=[ 0.16973981  0.18470691 -0.20410893 -0.71833115]\n",
      "[ episode 152 ][ timestamp 52 ] state=[ 0.16973981  0.18470691 -0.20410893 -0.71833115], action=1, reward=-1.0, next_state=[ 0.17343395  0.38198037 -0.21847555 -1.06769107]\n",
      "[ Ended! ] Episode 152: Exploration_rate=0.46912134373457726. Score=52.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 153 ] state=[-0.03895744 -0.02057825 -0.00440669  0.02779055]\n",
      "[ episode 153 ][ timestamp 1 ] state=[-0.03895744 -0.02057825 -0.00440669  0.02779055], action=1, reward=1.0, next_state=[-0.039369    0.17460662 -0.00385088 -0.26627948]\n",
      "[ episode 153 ][ timestamp 2 ] state=[-0.039369    0.17460662 -0.00385088 -0.26627948], action=0, reward=1.0, next_state=[-0.03587687 -0.02046016 -0.00917647  0.02518637]\n",
      "[ episode 153 ][ timestamp 3 ] state=[-0.03587687 -0.02046016 -0.00917647  0.02518637], action=0, reward=1.0, next_state=[-0.03628607 -0.21544932 -0.00867274  0.31495995]\n",
      "[ episode 153 ][ timestamp 4 ] state=[-0.03628607 -0.21544932 -0.00867274  0.31495995], action=0, reward=1.0, next_state=[-0.04059506 -0.41044666 -0.00237354  0.60489521]\n",
      "[ episode 153 ][ timestamp 5 ] state=[-0.04059506 -0.41044666 -0.00237354  0.60489521], action=1, reward=1.0, next_state=[-0.04880399 -0.2152916   0.00972436  0.31146562]\n",
      "[ episode 153 ][ timestamp 6 ] state=[-0.04880399 -0.2152916   0.00972436  0.31146562], action=0, reward=1.0, next_state=[-0.05310983 -0.41055073  0.01595367  0.60719938]\n",
      "[ episode 153 ][ timestamp 7 ] state=[-0.05310983 -0.41055073  0.01595367  0.60719938], action=1, reward=1.0, next_state=[-0.06132084 -0.21565543  0.02809766  0.31958382]\n",
      "[ episode 153 ][ timestamp 8 ] state=[-0.06132084 -0.21565543  0.02809766  0.31958382], action=0, reward=1.0, next_state=[-0.06563395 -0.41116604  0.03448934  0.6209936 ]\n",
      "[ episode 153 ][ timestamp 9 ] state=[-0.06563395 -0.41116604  0.03448934  0.6209936 ], action=1, reward=1.0, next_state=[-0.07385727 -0.21654228  0.04690921  0.33936944]\n",
      "[ episode 153 ][ timestamp 10 ] state=[-0.07385727 -0.21654228  0.04690921  0.33936944], action=1, reward=1.0, next_state=[-0.07818812 -0.0221181   0.0536966   0.06184023]\n",
      "[ episode 153 ][ timestamp 11 ] state=[-0.07818812 -0.0221181   0.0536966   0.06184023], action=0, reward=1.0, next_state=[-0.07863048 -0.21796718  0.0549334   0.37096964]\n",
      "[ episode 153 ][ timestamp 12 ] state=[-0.07863048 -0.21796718  0.0549334   0.37096964], action=1, reward=1.0, next_state=[-0.08298982 -0.02366696  0.0623528   0.09610125]\n",
      "[ episode 153 ][ timestamp 13 ] state=[-0.08298982 -0.02366696  0.0623528   0.09610125], action=0, reward=1.0, next_state=[-0.08346316 -0.21962459  0.06427482  0.40778633]\n",
      "[ episode 153 ][ timestamp 14 ] state=[-0.08346316 -0.21962459  0.06427482  0.40778633], action=1, reward=1.0, next_state=[-0.08785565 -0.02547009  0.07243055  0.13603973]\n",
      "[ episode 153 ][ timestamp 15 ] state=[-0.08785565 -0.02547009  0.07243055  0.13603973], action=0, reward=1.0, next_state=[-0.08836505 -0.22155073  0.07515134  0.45066549]\n",
      "[ episode 153 ][ timestamp 16 ] state=[-0.08836505 -0.22155073  0.07515134  0.45066549], action=1, reward=1.0, next_state=[-0.09279607 -0.02756764  0.08416465  0.18258584]\n",
      "[ episode 153 ][ timestamp 17 ] state=[-0.09279607 -0.02756764  0.08416465  0.18258584], action=0, reward=1.0, next_state=[-0.09334742 -0.22378674  0.08781637  0.50058786]\n",
      "[ episode 153 ][ timestamp 18 ] state=[-0.09334742 -0.22378674  0.08781637  0.50058786], action=1, reward=1.0, next_state=[-0.09782316 -0.03000537  0.09782813  0.2368207 ]\n",
      "[ episode 153 ][ timestamp 19 ] state=[-0.09782316 -0.03000537  0.09782813  0.2368207 ], action=0, reward=1.0, next_state=[-0.09842326 -0.22637896  0.10256454  0.55868831]\n",
      "[ episode 153 ][ timestamp 20 ] state=[-0.09842326 -0.22637896  0.10256454  0.55868831], action=1, reward=1.0, next_state=[-0.10295084 -0.03283492  0.11373831  0.29999903]\n",
      "[ episode 153 ][ timestamp 21 ] state=[-0.10295084 -0.03283492  0.11373831  0.29999903], action=1, reward=1.0, next_state=[-0.10360754  0.16049764  0.11973829  0.04524095]\n",
      "[ episode 153 ][ timestamp 22 ] state=[-0.10360754  0.16049764  0.11973829  0.04524095], action=1, reward=1.0, next_state=[-0.10039759  0.35371724  0.12064311 -0.20739426]\n",
      "[ episode 153 ][ timestamp 23 ] state=[-0.10039759  0.35371724  0.12064311 -0.20739426], action=0, reward=1.0, next_state=[-0.09332324  0.15709527  0.11649522  0.12077806]\n",
      "[ episode 153 ][ timestamp 24 ] state=[-0.09332324  0.15709527  0.11649522  0.12077806], action=1, reward=1.0, next_state=[-0.09018134  0.35037218  0.11891078 -0.1330001 ]\n",
      "[ episode 153 ][ timestamp 25 ] state=[-0.09018134  0.35037218  0.11891078 -0.1330001 ], action=0, reward=1.0, next_state=[-0.08317389  0.15376541  0.11625078  0.19470497]\n",
      "[ episode 153 ][ timestamp 26 ] state=[-0.08317389  0.15376541  0.11625078  0.19470497], action=1, reward=1.0, next_state=[-0.08009859  0.34704915  0.12014488 -0.05916299]\n",
      "[ episode 153 ][ timestamp 27 ] state=[-0.08009859  0.34704915  0.12014488 -0.05916299], action=0, reward=1.0, next_state=[-0.0731576   0.15042777  0.11896162  0.26888068]\n",
      "[ episode 153 ][ timestamp 28 ] state=[-0.0731576   0.15042777  0.11896162  0.26888068], action=0, reward=1.0, next_state=[-0.07014905 -0.04617323  0.12433923  0.59659021]\n",
      "[ episode 153 ][ timestamp 29 ] state=[-0.07014905 -0.04617323  0.12433923  0.59659021], action=1, reward=1.0, next_state=[-0.07107251  0.14700949  0.13627104  0.34551485]\n",
      "[ episode 153 ][ timestamp 30 ] state=[-0.07107251  0.14700949  0.13627104  0.34551485], action=0, reward=1.0, next_state=[-0.06813232 -0.04976114  0.14318133  0.67787435]\n",
      "[ episode 153 ][ timestamp 31 ] state=[-0.06813232 -0.04976114  0.14318133  0.67787435], action=0, reward=1.0, next_state=[-0.06912755 -0.24655142  0.15673882  1.01199077]\n",
      "[ episode 153 ][ timestamp 32 ] state=[-0.06912755 -0.24655142  0.15673882  1.01199077], action=0, reward=1.0, next_state=[-0.07405857 -0.44337771  0.17697864  1.34950381]\n",
      "[ episode 153 ][ timestamp 33 ] state=[-0.07405857 -0.44337771  0.17697864  1.34950381], action=1, reward=1.0, next_state=[-0.08292613 -0.25086503  0.20396871  1.11700586]\n",
      "[ episode 153 ][ timestamp 34 ] state=[-0.08292613 -0.25086503  0.20396871  1.11700586], action=1, reward=-1.0, next_state=[-0.08794343 -0.05891752  0.22630883  0.89460495]\n",
      "[ Ended! ] Episode 153: Exploration_rate=0.46677573701590436. Score=34.\n",
      "[ Experience replay ] starts\n",
      "[ episode 154 ] state=[ 0.02924398 -0.02575911  0.03816327 -0.04973862]\n",
      "[ episode 154 ][ timestamp 1 ] state=[ 0.02924398 -0.02575911  0.03816327 -0.04973862], action=0, reward=1.0, next_state=[ 0.0287288  -0.22140692  0.0371685   0.25473668]\n",
      "[ episode 154 ][ timestamp 2 ] state=[ 0.0287288  -0.22140692  0.0371685   0.25473668], action=1, reward=1.0, next_state=[ 0.02430066 -0.02683483  0.04226323 -0.02599484]\n",
      "[ episode 154 ][ timestamp 3 ] state=[ 0.02430066 -0.02683483  0.04226323 -0.02599484], action=1, reward=1.0, next_state=[ 0.02376396  0.16765635  0.04174333 -0.30504941]\n",
      "[ episode 154 ][ timestamp 4 ] state=[ 0.02376396  0.16765635  0.04174333 -0.30504941], action=1, reward=1.0, next_state=[ 0.02711709  0.36215934  0.03564235 -0.58428077]\n",
      "[ episode 154 ][ timestamp 5 ] state=[ 0.02711709  0.36215934  0.03564235 -0.58428077], action=1, reward=1.0, next_state=[ 0.03436027  0.55676434  0.02395673 -0.86552625]\n",
      "[ episode 154 ][ timestamp 6 ] state=[ 0.03436027  0.55676434  0.02395673 -0.86552625], action=0, reward=1.0, next_state=[ 0.04549556  0.36132466  0.00664621 -0.56540825]\n",
      "[ episode 154 ][ timestamp 7 ] state=[ 0.04549556  0.36132466  0.00664621 -0.56540825], action=0, reward=1.0, next_state=[ 0.05272205  0.1661101  -0.00466196 -0.2706389 ]\n",
      "[ episode 154 ][ timestamp 8 ] state=[ 0.05272205  0.1661101  -0.00466196 -0.2706389 ], action=1, reward=1.0, next_state=[ 0.05604426  0.36129827 -0.01007474 -0.56478858]\n",
      "[ episode 154 ][ timestamp 9 ] state=[ 0.05604426  0.36129827 -0.01007474 -0.56478858], action=0, reward=1.0, next_state=[ 0.06327022  0.16631911 -0.02137051 -0.2752966 ]\n",
      "[ episode 154 ][ timestamp 10 ] state=[ 0.06327022  0.16631911 -0.02137051 -0.2752966 ], action=0, reward=1.0, next_state=[ 0.0665966  -0.02849153 -0.02687644  0.01057017]\n",
      "[ episode 154 ][ timestamp 11 ] state=[ 0.0665966  -0.02849153 -0.02687644  0.01057017], action=0, reward=1.0, next_state=[ 0.06602677 -0.22321793 -0.02666504  0.29465356]\n",
      "[ episode 154 ][ timestamp 12 ] state=[ 0.06602677 -0.22321793 -0.02666504  0.29465356], action=1, reward=1.0, next_state=[ 0.06156241 -0.02772617 -0.02077197 -0.00631843]\n",
      "[ episode 154 ][ timestamp 13 ] state=[ 0.06156241 -0.02772617 -0.02077197 -0.00631843], action=1, reward=1.0, next_state=[ 0.06100789  0.16768743 -0.02089834 -0.30548211]\n",
      "[ episode 154 ][ timestamp 14 ] state=[ 0.06100789  0.16768743 -0.02089834 -0.30548211], action=1, reward=1.0, next_state=[ 0.06436164  0.36310086 -0.02700798 -0.60468191]\n",
      "[ episode 154 ][ timestamp 15 ] state=[ 0.06436164  0.36310086 -0.02700798 -0.60468191], action=1, reward=1.0, next_state=[ 0.07162366  0.55858989 -0.03910162 -0.9057479 ]\n",
      "[ episode 154 ][ timestamp 16 ] state=[ 0.07162366  0.55858989 -0.03910162 -0.9057479 ], action=0, reward=1.0, next_state=[ 0.08279545  0.36401861 -0.05721657 -0.62560701]\n",
      "[ episode 154 ][ timestamp 17 ] state=[ 0.08279545  0.36401861 -0.05721657 -0.62560701], action=0, reward=1.0, next_state=[ 0.09007583  0.16974009 -0.06972871 -0.35147861]\n",
      "[ episode 154 ][ timestamp 18 ] state=[ 0.09007583  0.16974009 -0.06972871 -0.35147861], action=0, reward=1.0, next_state=[ 0.09347063 -0.02432455 -0.07675829 -0.08157266]\n",
      "[ episode 154 ][ timestamp 19 ] state=[ 0.09347063 -0.02432455 -0.07675829 -0.08157266], action=1, reward=1.0, next_state=[ 0.09298414  0.17180902 -0.07838974 -0.39745153]\n",
      "[ episode 154 ][ timestamp 20 ] state=[ 0.09298414  0.17180902 -0.07838974 -0.39745153], action=1, reward=1.0, next_state=[ 0.09642032  0.36795051 -0.08633877 -0.71378327]\n",
      "[ episode 154 ][ timestamp 21 ] state=[ 0.09642032  0.36795051 -0.08633877 -0.71378327], action=0, reward=1.0, next_state=[ 0.10377933  0.17412324 -0.10061444 -0.4494774 ]\n",
      "[ episode 154 ][ timestamp 22 ] state=[ 0.10377933  0.17412324 -0.10061444 -0.4494774 ], action=1, reward=1.0, next_state=[ 0.10726179  0.37051365 -0.10960398 -0.77210395]\n",
      "[ episode 154 ][ timestamp 23 ] state=[ 0.10726179  0.37051365 -0.10960398 -0.77210395], action=1, reward=1.0, next_state=[ 0.11467207  0.56695932 -0.12504606 -1.09716339]\n",
      "[ episode 154 ][ timestamp 24 ] state=[ 0.11467207  0.56695932 -0.12504606 -1.09716339], action=0, reward=1.0, next_state=[ 0.12601125  0.37368558 -0.14698933 -0.84618424]\n",
      "[ episode 154 ][ timestamp 25 ] state=[ 0.12601125  0.37368558 -0.14698933 -0.84618424], action=1, reward=1.0, next_state=[ 0.13348496  0.57047416 -0.16391302 -1.18124342]\n",
      "[ episode 154 ][ timestamp 26 ] state=[ 0.13348496  0.57047416 -0.16391302 -1.18124342], action=0, reward=1.0, next_state=[ 0.14489445  0.37781432 -0.18753788 -0.9441021 ]\n",
      "[ episode 154 ][ timestamp 27 ] state=[ 0.14489445  0.37781432 -0.18753788 -0.9441021 ], action=1, reward=1.0, next_state=[ 0.15245073  0.57489982 -0.20641993 -1.2893604 ]\n",
      "[ episode 154 ][ timestamp 28 ] state=[ 0.15245073  0.57489982 -0.20641993 -1.2893604 ], action=0, reward=-1.0, next_state=[ 0.16394873  0.38291254 -0.23220713 -1.06775045]\n",
      "[ Ended! ] Episode 154: Exploration_rate=0.46444185833082485. Score=28.\n",
      "[ Experience replay ] starts\n",
      "[ episode 155 ] state=[ 0.03437358 -0.03533685  0.00352382  0.02884037]\n",
      "[ episode 155 ][ timestamp 1 ] state=[ 0.03437358 -0.03533685  0.00352382  0.02884037], action=0, reward=1.0, next_state=[ 0.03366684 -0.23050915  0.00410063  0.32263302]\n",
      "[ episode 155 ][ timestamp 2 ] state=[ 0.03366684 -0.23050915  0.00410063  0.32263302], action=1, reward=1.0, next_state=[ 0.02905666 -0.03544583  0.01055329  0.03124608]\n",
      "[ episode 155 ][ timestamp 3 ] state=[ 0.02905666 -0.03544583  0.01055329  0.03124608], action=1, reward=1.0, next_state=[ 0.02834774  0.1595232   0.01117821 -0.25808857]\n",
      "[ episode 155 ][ timestamp 4 ] state=[ 0.02834774  0.1595232   0.01117821 -0.25808857], action=1, reward=1.0, next_state=[ 0.03153821  0.3544838   0.00601644 -0.54722487]\n",
      "[ episode 155 ][ timestamp 5 ] state=[ 0.03153821  0.3544838   0.00601644 -0.54722487], action=0, reward=1.0, next_state=[ 0.03862788  0.15927784 -0.00492805 -0.25265241]\n",
      "[ episode 155 ][ timestamp 6 ] state=[ 0.03862788  0.15927784 -0.00492805 -0.25265241], action=0, reward=1.0, next_state=[ 0.04181344 -0.0357734  -0.0099811   0.03847205]\n",
      "[ episode 155 ][ timestamp 7 ] state=[ 0.04181344 -0.0357734  -0.0099811   0.03847205], action=0, reward=1.0, next_state=[ 0.04109797 -0.23075081 -0.00921166  0.3279892 ]\n",
      "[ episode 155 ][ timestamp 8 ] state=[ 0.04109797 -0.23075081 -0.00921166  0.3279892 ], action=1, reward=1.0, next_state=[ 0.03648295 -0.03549894 -0.00265188  0.03241563]\n",
      "[ episode 155 ][ timestamp 9 ] state=[ 0.03648295 -0.03549894 -0.00265188  0.03241563], action=0, reward=1.0, next_state=[ 0.03577298 -0.23058276 -0.00200357  0.32426068]\n",
      "[ episode 155 ][ timestamp 10 ] state=[ 0.03577298 -0.23058276 -0.00200357  0.32426068], action=1, reward=1.0, next_state=[ 0.03116132 -0.03543234  0.00448165  0.03094659]\n",
      "[ episode 155 ][ timestamp 11 ] state=[ 0.03116132 -0.03543234  0.00448165  0.03094659], action=0, reward=1.0, next_state=[ 0.03045267 -0.23061827  0.00510058  0.32504015]\n",
      "[ episode 155 ][ timestamp 12 ] state=[ 0.03045267 -0.23061827  0.00510058  0.32504015], action=1, reward=1.0, next_state=[ 0.02584031 -0.03556931  0.01160138  0.03397008]\n",
      "[ episode 155 ][ timestamp 13 ] state=[ 0.02584031 -0.03556931  0.01160138  0.03397008], action=0, reward=1.0, next_state=[ 0.02512892 -0.23085569  0.01228078  0.33029067]\n",
      "[ episode 155 ][ timestamp 14 ] state=[ 0.02512892 -0.23085569  0.01228078  0.33029067], action=1, reward=1.0, next_state=[ 0.02051181 -0.03591069  0.0188866   0.04150568]\n",
      "[ episode 155 ][ timestamp 15 ] state=[ 0.02051181 -0.03591069  0.0188866   0.04150568], action=0, reward=1.0, next_state=[ 0.01979359 -0.23129831  0.01971671  0.34008717]\n",
      "[ episode 155 ][ timestamp 16 ] state=[ 0.01979359 -0.23129831  0.01971671  0.34008717], action=1, reward=1.0, next_state=[ 0.01516763 -0.03646237  0.02651846  0.0536864 ]\n",
      "[ episode 155 ][ timestamp 17 ] state=[ 0.01516763 -0.03646237  0.02651846  0.0536864 ], action=0, reward=1.0, next_state=[ 0.01443838 -0.23195432  0.02759218  0.35461674]\n",
      "[ episode 155 ][ timestamp 18 ] state=[ 0.01443838 -0.23195432  0.02759218  0.35461674], action=1, reward=1.0, next_state=[ 0.00979929 -0.03723534  0.03468452  0.07076052]\n",
      "[ episode 155 ][ timestamp 19 ] state=[ 0.00979929 -0.03723534  0.03468452  0.07076052], action=0, reward=1.0, next_state=[ 0.00905459 -0.23283694  0.03609973  0.37418166]\n",
      "[ episode 155 ][ timestamp 20 ] state=[ 0.00905459 -0.23283694  0.03609973  0.37418166], action=0, reward=1.0, next_state=[ 0.00439785 -0.42845259  0.04358336  0.67802497]\n",
      "[ episode 155 ][ timestamp 21 ] state=[ 0.00439785 -0.42845259  0.04358336  0.67802497], action=1, reward=1.0, next_state=[-0.0041712  -0.23396237  0.05714386  0.39937613]\n",
      "[ episode 155 ][ timestamp 22 ] state=[-0.0041712  -0.23396237  0.05714386  0.39937613], action=0, reward=1.0, next_state=[-0.00885045 -0.42984643  0.06513138  0.70951376]\n",
      "[ episode 155 ][ timestamp 23 ] state=[-0.00885045 -0.42984643  0.06513138  0.70951376], action=1, reward=1.0, next_state=[-0.01744738 -0.23568413  0.07932166  0.43802293]\n",
      "[ episode 155 ][ timestamp 24 ] state=[-0.01744738 -0.23568413  0.07932166  0.43802293], action=1, reward=1.0, next_state=[-0.02216106 -0.04176926  0.08808212  0.17136134]\n",
      "[ episode 155 ][ timestamp 25 ] state=[-0.02216106 -0.04176926  0.08808212  0.17136134], action=1, reward=1.0, next_state=[-0.02299645  0.1519888   0.09150934 -0.09228636]\n",
      "[ episode 155 ][ timestamp 26 ] state=[-0.02299645  0.1519888   0.09150934 -0.09228636], action=0, reward=1.0, next_state=[-0.01995667 -0.04431751  0.08966362  0.22780728]\n",
      "[ episode 155 ][ timestamp 27 ] state=[-0.01995667 -0.04431751  0.08966362  0.22780728], action=1, reward=1.0, next_state=[-0.02084302  0.14941629  0.09421976 -0.03530025]\n",
      "[ episode 155 ][ timestamp 28 ] state=[-0.02084302  0.14941629  0.09421976 -0.03530025], action=1, reward=1.0, next_state=[-0.0178547   0.34306967  0.09351376 -0.29683228]\n",
      "[ episode 155 ][ timestamp 29 ] state=[-0.0178547   0.34306967  0.09351376 -0.29683228], action=0, reward=1.0, next_state=[-0.0109933   0.14674768  0.08757711  0.02381703]\n",
      "[ episode 155 ][ timestamp 30 ] state=[-0.0109933   0.14674768  0.08757711  0.02381703], action=0, reward=1.0, next_state=[-0.00805835 -0.04951394  0.08805345  0.34279599]\n",
      "[ episode 155 ][ timestamp 31 ] state=[-0.00805835 -0.04951394  0.08805345  0.34279599], action=1, reward=1.0, next_state=[-0.00904863  0.14425216  0.09490937  0.07912716]\n",
      "[ episode 155 ][ timestamp 32 ] state=[-0.00904863  0.14425216  0.09490937  0.07912716], action=0, reward=1.0, next_state=[-0.00616358 -0.05209317  0.09649192  0.40018115]\n",
      "[ episode 155 ][ timestamp 33 ] state=[-0.00616358 -0.05209317  0.09649192  0.40018115], action=1, reward=1.0, next_state=[-0.00720545  0.14153706  0.10449554  0.13941149]\n",
      "[ episode 155 ][ timestamp 34 ] state=[-0.00720545  0.14153706  0.10449554  0.13941149], action=0, reward=1.0, next_state=[-0.00437471 -0.05491425  0.10728377  0.46314691]\n",
      "[ episode 155 ][ timestamp 35 ] state=[-0.00437471 -0.05491425  0.10728377  0.46314691], action=1, reward=1.0, next_state=[-0.00547299  0.13854098  0.11654671  0.20611339]\n",
      "[ episode 155 ][ timestamp 36 ] state=[-0.00547299  0.13854098  0.11654671  0.20611339], action=1, reward=1.0, next_state=[-0.00270217  0.33182016  0.12066897 -0.0476514 ]\n",
      "[ episode 155 ][ timestamp 37 ] state=[-0.00270217  0.33182016  0.12066897 -0.0476514 ], action=0, reward=1.0, next_state=[0.00393423 0.13519314 0.11971595 0.28053507]\n",
      "[ episode 155 ][ timestamp 38 ] state=[0.00393423 0.13519314 0.11971595 0.28053507], action=0, reward=1.0, next_state=[ 0.00663809 -0.06141499  0.12532665  0.60844894]\n",
      "[ episode 155 ][ timestamp 39 ] state=[ 0.00663809 -0.06141499  0.12532665  0.60844894], action=1, reward=1.0, next_state=[0.00540979 0.13175266 0.13749563 0.35771967]\n",
      "[ episode 155 ][ timestamp 40 ] state=[0.00540979 0.13175266 0.13749563 0.35771967], action=1, reward=1.0, next_state=[0.00804485 0.32467939 0.14465002 0.1113572 ]\n",
      "[ episode 155 ][ timestamp 41 ] state=[0.00804485 0.32467939 0.14465002 0.1113572 ], action=0, reward=1.0, next_state=[0.01453844 0.12781282 0.14687716 0.44595203]\n",
      "[ episode 155 ][ timestamp 42 ] state=[0.01453844 0.12781282 0.14687716 0.44595203], action=1, reward=1.0, next_state=[0.01709469 0.32058478 0.1557962  0.20293426]\n",
      "[ episode 155 ][ timestamp 43 ] state=[0.01709469 0.32058478 0.1557962  0.20293426], action=1, reward=1.0, next_state=[ 0.02350639  0.51317531  0.15985489 -0.03683363]\n",
      "[ episode 155 ][ timestamp 44 ] state=[ 0.02350639  0.51317531  0.15985489 -0.03683363], action=0, reward=1.0, next_state=[0.03376989 0.31616511 0.15911822 0.3017114 ]\n",
      "[ episode 155 ][ timestamp 45 ] state=[0.03376989 0.31616511 0.15911822 0.3017114 ], action=0, reward=1.0, next_state=[0.0400932  0.11917544 0.16515245 0.64004677]\n",
      "[ episode 155 ][ timestamp 46 ] state=[0.0400932  0.11917544 0.16515245 0.64004677], action=1, reward=1.0, next_state=[0.04247671 0.31165674 0.17795338 0.40358777]\n",
      "[ episode 155 ][ timestamp 47 ] state=[0.04247671 0.31165674 0.17795338 0.40358777], action=0, reward=1.0, next_state=[0.04870984 0.11451634 0.18602514 0.74667112]\n",
      "[ episode 155 ][ timestamp 48 ] state=[0.04870984 0.11451634 0.18602514 0.74667112], action=1, reward=1.0, next_state=[0.05100017 0.30665122 0.20095856 0.5178176 ]\n",
      "[ episode 155 ][ timestamp 49 ] state=[0.05100017 0.30665122 0.20095856 0.5178176 ], action=0, reward=-1.0, next_state=[0.05713319 0.10935162 0.21131491 0.86649618]\n",
      "[ Ended! ] Episode 155: Exploration_rate=0.46211964903917074. Score=49.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 156 ] state=[-0.01576573  0.0240623  -0.01023444  0.04085849]\n",
      "[ episode 156 ][ timestamp 1 ] state=[-0.01576573  0.0240623  -0.01023444  0.04085849], action=1, reward=1.0, next_state=[-0.01528448  0.21932951 -0.00941727 -0.25503585]\n",
      "[ episode 156 ][ timestamp 2 ] state=[-0.01528448  0.21932951 -0.00941727 -0.25503585], action=1, reward=1.0, next_state=[-0.01089789  0.41458464 -0.01451798 -0.5506742 ]\n",
      "[ episode 156 ][ timestamp 3 ] state=[-0.01089789  0.41458464 -0.01451798 -0.5506742 ], action=0, reward=1.0, next_state=[-0.0026062   0.21966958 -0.02553147 -0.26260056]\n",
      "[ episode 156 ][ timestamp 4 ] state=[-0.0026062   0.21966958 -0.02553147 -0.26260056], action=1, reward=1.0, next_state=[ 0.0017872   0.4151465  -0.03078348 -0.5632258 ]\n",
      "[ episode 156 ][ timestamp 5 ] state=[ 0.0017872   0.4151465  -0.03078348 -0.5632258 ], action=1, reward=1.0, next_state=[ 0.01009013  0.61068657 -0.042048   -0.86544586]\n",
      "[ episode 156 ][ timestamp 6 ] state=[ 0.01009013  0.61068657 -0.042048   -0.86544586], action=1, reward=1.0, next_state=[ 0.02230386  0.80635482 -0.05935691 -1.17104729]\n",
      "[ episode 156 ][ timestamp 7 ] state=[ 0.02230386  0.80635482 -0.05935691 -1.17104729], action=0, reward=1.0, next_state=[ 0.03843095  0.6120528  -0.08277786 -0.89754823]\n",
      "[ episode 156 ][ timestamp 8 ] state=[ 0.03843095  0.6120528  -0.08277786 -0.89754823], action=1, reward=1.0, next_state=[ 0.05067201  0.80819339 -0.10072882 -1.21506061]\n",
      "[ episode 156 ][ timestamp 9 ] state=[ 0.05067201  0.80819339 -0.10072882 -1.21506061], action=0, reward=1.0, next_state=[ 0.06683588  0.61450476 -0.12503004 -0.95556455]\n",
      "[ episode 156 ][ timestamp 10 ] state=[ 0.06683588  0.61450476 -0.12503004 -0.95556455], action=1, reward=1.0, next_state=[ 0.07912597  0.81106644 -0.14414133 -1.28476863]\n",
      "[ episode 156 ][ timestamp 11 ] state=[ 0.07912597  0.81106644 -0.14414133 -1.28476863], action=1, reward=1.0, next_state=[ 0.0953473   1.00769889 -0.1698367  -1.61888954]\n",
      "[ episode 156 ][ timestamp 12 ] state=[ 0.0953473   1.00769889 -0.1698367  -1.61888954], action=1, reward=1.0, next_state=[ 0.11550128  1.20436671 -0.20221449 -1.95933918]\n",
      "[ episode 156 ][ timestamp 13 ] state=[ 0.11550128  1.20436671 -0.20221449 -1.95933918], action=1, reward=-1.0, next_state=[ 0.13958861  1.40097779 -0.24140127 -2.30729337]\n",
      "[ Ended! ] Episode 156: Exploration_rate=0.4598090507939749. Score=13.\n",
      "[ Experience replay ] starts\n",
      "[ episode 157 ] state=[ 0.00981381  0.02992475 -0.00955208  0.01551959]\n",
      "[ episode 157 ][ timestamp 1 ] state=[ 0.00981381  0.02992475 -0.00955208  0.01551959], action=0, reward=1.0, next_state=[ 0.01041231 -0.16505892 -0.00924169  0.30517348]\n",
      "[ episode 157 ][ timestamp 2 ] state=[ 0.01041231 -0.16505892 -0.00924169  0.30517348], action=0, reward=1.0, next_state=[ 0.00711113 -0.36004796 -0.00313822  0.59492753]\n",
      "[ episode 157 ][ timestamp 3 ] state=[ 0.00711113 -0.36004796 -0.00313822  0.59492753], action=1, reward=1.0, next_state=[-8.98282595e-05 -1.64882229e-01  8.76033113e-03  3.01257740e-01]\n",
      "[ episode 157 ][ timestamp 4 ] state=[-8.98282595e-05 -1.64882229e-01  8.76033113e-03  3.01257740e-01], action=0, reward=1.0, next_state=[-0.00338747 -0.36012794  0.01478549  0.59669057]\n",
      "[ episode 157 ][ timestamp 5 ] state=[-0.00338747 -0.36012794  0.01478549  0.59669057], action=1, reward=1.0, next_state=[-0.01059003 -0.16521598  0.0267193   0.30870137]\n",
      "[ episode 157 ][ timestamp 6 ] state=[-0.01059003 -0.16521598  0.0267193   0.30870137], action=1, reward=1.0, next_state=[-0.01389435  0.02951527  0.03289332  0.02456329]\n",
      "[ episode 157 ][ timestamp 7 ] state=[-0.01389435  0.02951527  0.03289332  0.02456329], action=1, reward=1.0, next_state=[-0.01330405  0.22415044  0.03338459 -0.25756264]\n",
      "[ episode 157 ][ timestamp 8 ] state=[-0.01330405  0.22415044  0.03338459 -0.25756264], action=1, reward=1.0, next_state=[-0.00882104  0.41878026  0.02823334 -0.53953145]\n",
      "[ episode 157 ][ timestamp 9 ] state=[-0.00882104  0.41878026  0.02823334 -0.53953145], action=1, reward=1.0, next_state=[-4.45431958e-04  6.13494178e-01  1.74427087e-02 -8.23186431e-01]\n",
      "[ episode 157 ][ timestamp 10 ] state=[-4.45431958e-04  6.13494178e-01  1.74427087e-02 -8.23186431e-01], action=1, reward=1.0, next_state=[ 1.18244516e-02  8.08373219e-01  9.78980037e-04 -1.11033263e+00]\n",
      "[ episode 157 ][ timestamp 11 ] state=[ 1.18244516e-02  8.08373219e-01  9.78980037e-04 -1.11033263e+00], action=0, reward=1.0, next_state=[ 0.02799192  0.61323842 -0.02122767 -0.81734275]\n",
      "[ episode 157 ][ timestamp 12 ] state=[ 0.02799192  0.61323842 -0.02122767 -0.81734275], action=1, reward=1.0, next_state=[ 0.04025668  0.80864444 -0.03757453 -1.11662621]\n",
      "[ episode 157 ][ timestamp 13 ] state=[ 0.04025668  0.80864444 -0.03757453 -1.11662621], action=0, reward=1.0, next_state=[ 0.05642957  0.61403525 -0.05990705 -0.83596278]\n",
      "[ episode 157 ][ timestamp 14 ] state=[ 0.05642957  0.61403525 -0.05990705 -0.83596278], action=0, reward=1.0, next_state=[ 0.06871028  0.41978053 -0.07662631 -0.56270555]\n",
      "[ episode 157 ][ timestamp 15 ] state=[ 0.06871028  0.41978053 -0.07662631 -0.56270555], action=1, reward=1.0, next_state=[ 0.07710589  0.61588938 -0.08788042 -0.87851174]\n",
      "[ episode 157 ][ timestamp 16 ] state=[ 0.07710589  0.61588938 -0.08788042 -0.87851174], action=0, reward=1.0, next_state=[ 0.08942368  0.42206445 -0.10545065 -0.6146999 ]\n",
      "[ episode 157 ][ timestamp 17 ] state=[ 0.08942368  0.42206445 -0.10545065 -0.6146999 ], action=1, reward=1.0, next_state=[ 0.09786497  0.6184895  -0.11774465 -0.9386459 ]\n",
      "[ episode 157 ][ timestamp 18 ] state=[ 0.09786497  0.6184895  -0.11774465 -0.9386459 ], action=0, reward=1.0, next_state=[ 0.11023476  0.42513488 -0.13651757 -0.68515911]\n",
      "[ episode 157 ][ timestamp 19 ] state=[ 0.11023476  0.42513488 -0.13651757 -0.68515911], action=1, reward=1.0, next_state=[ 0.11873745  0.62186152 -0.15022075 -1.01751514]\n",
      "[ episode 157 ][ timestamp 20 ] state=[ 0.11873745  0.62186152 -0.15022075 -1.01751514], action=0, reward=1.0, next_state=[ 0.13117468  0.42902654 -0.17057105 -0.7755192 ]\n",
      "[ episode 157 ][ timestamp 21 ] state=[ 0.13117468  0.42902654 -0.17057105 -0.7755192 ], action=1, reward=1.0, next_state=[ 0.13975521  0.62603261 -0.18608144 -1.11664495]\n",
      "[ episode 157 ][ timestamp 22 ] state=[ 0.13975521  0.62603261 -0.18608144 -1.11664495], action=0, reward=1.0, next_state=[ 0.15227587  0.43377461 -0.20841434 -0.88762922]\n",
      "[ episode 157 ][ timestamp 23 ] state=[ 0.15227587  0.43377461 -0.20841434 -0.88762922], action=1, reward=-1.0, next_state=[ 0.16095136  0.63102279 -0.22616692 -1.23793008]\n",
      "[ Ended! ] Episode 157: Exploration_rate=0.457510005540005. Score=23.\n",
      "[ Experience replay ] starts\n",
      "[ episode 158 ] state=[ 0.02206545 -0.01928646 -0.00057364  0.04152522]\n",
      "[ episode 158 ][ timestamp 1 ] state=[ 0.02206545 -0.01928646 -0.00057364  0.04152522], action=0, reward=1.0, next_state=[ 2.16797247e-02 -2.14400184e-01  2.56865729e-04  3.34027103e-01]\n",
      "[ episode 158 ][ timestamp 2 ] state=[ 2.16797247e-02 -2.14400184e-01  2.56865729e-04  3.34027103e-01], action=1, reward=1.0, next_state=[ 0.01739172 -0.01928189  0.00693741  0.04142519]\n",
      "[ episode 158 ][ timestamp 3 ] state=[ 0.01739172 -0.01928189  0.00693741  0.04142519], action=0, reward=1.0, next_state=[ 0.01700608 -0.21450263  0.00776591  0.33628884]\n",
      "[ episode 158 ][ timestamp 4 ] state=[ 0.01700608 -0.21450263  0.00776591  0.33628884], action=1, reward=1.0, next_state=[ 0.01271603 -0.01949205  0.01449169  0.04606495]\n",
      "[ episode 158 ][ timestamp 5 ] state=[ 0.01271603 -0.01949205  0.01449169  0.04606495], action=0, reward=1.0, next_state=[ 0.01232619 -0.21481878  0.01541299  0.34328467]\n",
      "[ episode 158 ][ timestamp 6 ] state=[ 0.01232619 -0.21481878  0.01541299  0.34328467], action=0, reward=1.0, next_state=[ 0.00802981 -0.41015657  0.02227868  0.6407878 ]\n",
      "[ episode 158 ][ timestamp 7 ] state=[ 0.00802981 -0.41015657  0.02227868  0.6407878 ], action=1, reward=1.0, next_state=[-1.73317410e-04 -2.15352172e-01  3.50944367e-02  3.55203107e-01]\n",
      "[ episode 158 ][ timestamp 8 ] state=[-1.73317410e-04 -2.15352172e-01  3.50944367e-02  3.55203107e-01], action=1, reward=1.0, next_state=[-0.00448036 -0.02074632  0.0421985   0.07378972]\n",
      "[ episode 158 ][ timestamp 9 ] state=[-0.00448036 -0.02074632  0.0421985   0.07378972], action=1, reward=1.0, next_state=[-0.00489529  0.17374605  0.04367429 -0.20528645]\n",
      "[ episode 158 ][ timestamp 10 ] state=[-0.00489529  0.17374605  0.04367429 -0.20528645], action=0, reward=1.0, next_state=[-0.00142037 -0.02197236  0.03956856  0.10084738]\n",
      "[ episode 158 ][ timestamp 11 ] state=[-0.00142037 -0.02197236  0.03956856  0.10084738], action=1, reward=1.0, next_state=[-0.00185981  0.17256083  0.04158551 -0.17909388]\n",
      "[ episode 158 ][ timestamp 12 ] state=[-0.00185981  0.17256083  0.04158551 -0.17909388], action=0, reward=1.0, next_state=[ 0.0015914  -0.02313078  0.03800363  0.12641238]\n",
      "[ episode 158 ][ timestamp 13 ] state=[ 0.0015914  -0.02313078  0.03800363  0.12641238], action=1, reward=1.0, next_state=[ 0.00112879  0.17142671  0.04053188 -0.15404276]\n",
      "[ episode 158 ][ timestamp 14 ] state=[ 0.00112879  0.17142671  0.04053188 -0.15404276], action=0, reward=1.0, next_state=[ 0.00455732 -0.02425145  0.03745103  0.15114652]\n",
      "[ episode 158 ][ timestamp 15 ] state=[ 0.00455732 -0.02425145  0.03745103  0.15114652], action=1, reward=1.0, next_state=[ 0.00407229  0.17031478  0.04047396 -0.12949015]\n",
      "[ episode 158 ][ timestamp 16 ] state=[ 0.00407229  0.17031478  0.04047396 -0.12949015], action=0, reward=1.0, next_state=[ 0.00747859 -0.02536289  0.03788415  0.17568207]\n",
      "[ episode 158 ][ timestamp 17 ] state=[ 0.00747859 -0.02536289  0.03788415  0.17568207], action=1, reward=1.0, next_state=[ 0.00697133  0.16919699  0.0413978  -0.10481306]\n",
      "[ episode 158 ][ timestamp 18 ] state=[ 0.00697133  0.16919699  0.0413978  -0.10481306], action=0, reward=1.0, next_state=[ 0.01035527 -0.02649302  0.03930153  0.20063794]\n",
      "[ episode 158 ][ timestamp 19 ] state=[ 0.01035527 -0.02649302  0.03930153  0.20063794], action=1, reward=1.0, next_state=[ 0.00982541  0.16804543  0.04331429 -0.07939274]\n",
      "[ episode 158 ][ timestamp 20 ] state=[ 0.00982541  0.16804543  0.04331429 -0.07939274], action=0, reward=1.0, next_state=[ 0.01318632 -0.02766981  0.04172644  0.22663521]\n",
      "[ episode 158 ][ timestamp 21 ] state=[ 0.01318632 -0.02766981  0.04172644  0.22663521], action=1, reward=1.0, next_state=[ 0.01263292  0.16683174  0.04625914 -0.05259916]\n",
      "[ episode 158 ][ timestamp 22 ] state=[ 0.01263292  0.16683174  0.04625914 -0.05259916], action=0, reward=1.0, next_state=[ 0.01596956 -0.02892194  0.04520716  0.25431257]\n",
      "[ episode 158 ][ timestamp 23 ] state=[ 0.01596956 -0.02892194  0.04520716  0.25431257], action=1, reward=1.0, next_state=[ 0.01539112  0.16552636  0.05029341 -0.02377549]\n",
      "[ episode 158 ][ timestamp 24 ] state=[ 0.01539112  0.16552636  0.05029341 -0.02377549], action=1, reward=1.0, next_state=[ 0.01870165  0.35989233  0.0498179  -0.30017577]\n",
      "[ episode 158 ][ timestamp 25 ] state=[ 0.01870165  0.35989233  0.0498179  -0.30017577], action=1, reward=1.0, next_state=[ 0.02589949  0.55427011  0.04381439 -0.5767403 ]\n",
      "[ episode 158 ][ timestamp 26 ] state=[ 0.02589949  0.55427011  0.04381439 -0.5767403 ], action=0, reward=1.0, next_state=[ 0.03698489  0.35856228  0.03227958 -0.27058299]\n",
      "[ episode 158 ][ timestamp 27 ] state=[ 0.03698489  0.35856228  0.03227958 -0.27058299], action=1, reward=1.0, next_state=[ 0.04415614  0.55320909  0.02686792 -0.55291255]\n",
      "[ episode 158 ][ timestamp 28 ] state=[ 0.04415614  0.55320909  0.02686792 -0.55291255], action=0, reward=1.0, next_state=[ 0.05522032  0.35772033  0.01580967 -0.25188703]\n",
      "[ episode 158 ][ timestamp 29 ] state=[ 0.05522032  0.35772033  0.01580967 -0.25188703], action=0, reward=1.0, next_state=[0.06237473 0.16237624 0.01077193 0.04574034]\n",
      "[ episode 158 ][ timestamp 30 ] state=[0.06237473 0.16237624 0.01077193 0.04574034], action=0, reward=1.0, next_state=[ 0.06562225 -0.03289851  0.01168673  0.34180235]\n",
      "[ episode 158 ][ timestamp 31 ] state=[ 0.06562225 -0.03289851  0.01168673  0.34180235], action=0, reward=1.0, next_state=[ 0.06496428 -0.22818476  0.01852278  0.63814756]\n",
      "[ episode 158 ][ timestamp 32 ] state=[ 0.06496428 -0.22818476  0.01852278  0.63814756], action=1, reward=1.0, next_state=[ 0.06040059 -0.03332593  0.03128573  0.35135483]\n",
      "[ episode 158 ][ timestamp 33 ] state=[ 0.06040059 -0.03332593  0.03128573  0.35135483], action=1, reward=1.0, next_state=[0.05973407 0.16133746 0.03831283 0.06869914]\n",
      "[ episode 158 ][ timestamp 34 ] state=[0.05973407 0.16133746 0.03831283 0.06869914], action=0, reward=1.0, next_state=[ 0.06296082 -0.03431223  0.03968681  0.37321953]\n",
      "[ episode 158 ][ timestamp 35 ] state=[ 0.06296082 -0.03431223  0.03968681  0.37321953], action=1, reward=1.0, next_state=[0.06227457 0.16022413 0.0471512  0.09330962]\n",
      "[ episode 158 ][ timestamp 36 ] state=[0.06227457 0.16022413 0.0471512  0.09330962], action=1, reward=1.0, next_state=[ 0.06547906  0.35463967  0.0490174  -0.18413225]\n",
      "[ episode 158 ][ timestamp 37 ] state=[ 0.06547906  0.35463967  0.0490174  -0.18413225], action=1, reward=1.0, next_state=[ 0.07257185  0.54902723  0.04533475 -0.46095804]\n",
      "[ episode 158 ][ timestamp 38 ] state=[ 0.07257185  0.54902723  0.04533475 -0.46095804], action=0, reward=1.0, next_state=[ 0.08355239  0.35329482  0.03611559 -0.15433722]\n",
      "[ episode 158 ][ timestamp 39 ] state=[ 0.08355239  0.35329482  0.03611559 -0.15433722], action=0, reward=1.0, next_state=[0.09061829 0.15767487 0.03302885 0.14951704]\n",
      "[ episode 158 ][ timestamp 40 ] state=[0.09061829 0.15767487 0.03302885 0.14951704], action=1, reward=1.0, next_state=[ 0.09377179  0.35230867  0.03601919 -0.13256572]\n",
      "[ episode 158 ][ timestamp 41 ] state=[ 0.09377179  0.35230867  0.03601919 -0.13256572], action=0, reward=1.0, next_state=[0.10081796 0.15668978 0.03336787 0.17125965]\n",
      "[ episode 158 ][ timestamp 42 ] state=[0.10081796 0.15668978 0.03336787 0.17125965], action=0, reward=1.0, next_state=[ 0.10395176 -0.03889348  0.03679306  0.47427956]\n",
      "[ episode 158 ][ timestamp 43 ] state=[ 0.10395176 -0.03889348  0.03679306  0.47427956], action=1, reward=1.0, next_state=[0.10317389 0.15569009 0.04627866 0.19341646]\n",
      "[ episode 158 ][ timestamp 44 ] state=[0.10317389 0.15569009 0.04627866 0.19341646], action=0, reward=1.0, next_state=[ 0.10628769 -0.04006228  0.05014698  0.5003317 ]\n",
      "[ episode 158 ][ timestamp 45 ] state=[ 0.10628769 -0.04006228  0.05014698  0.5003317 ], action=1, reward=1.0, next_state=[0.10548644 0.1543182  0.06015362 0.22386455]\n",
      "[ episode 158 ][ timestamp 46 ] state=[0.10548644 0.1543182  0.06015362 0.22386455], action=1, reward=1.0, next_state=[ 0.10857281  0.34853112  0.06463091 -0.04925343]\n",
      "[ episode 158 ][ timestamp 47 ] state=[ 0.10857281  0.34853112  0.06463091 -0.04925343], action=0, reward=1.0, next_state=[0.11554343 0.15254482 0.06364584 0.2631005 ]\n",
      "[ episode 158 ][ timestamp 48 ] state=[0.11554343 0.15254482 0.06364584 0.2631005 ], action=0, reward=1.0, next_state=[ 0.11859433 -0.04342514  0.06890785  0.57515952]\n",
      "[ episode 158 ][ timestamp 49 ] state=[ 0.11859433 -0.04342514  0.06890785  0.57515952], action=1, reward=1.0, next_state=[0.11772582 0.15066659 0.08041104 0.30495573]\n",
      "[ episode 158 ][ timestamp 50 ] state=[0.11772582 0.15066659 0.08041104 0.30495573], action=1, reward=1.0, next_state=[0.12073916 0.34455605 0.08651016 0.03867667]\n",
      "[ episode 158 ][ timestamp 51 ] state=[0.12073916 0.34455605 0.08651016 0.03867667], action=0, reward=1.0, next_state=[0.12763028 0.14830692 0.08728369 0.35735177]\n",
      "[ episode 158 ][ timestamp 52 ] state=[0.12763028 0.14830692 0.08728369 0.35735177], action=1, reward=1.0, next_state=[0.13059641 0.34208656 0.09443072 0.09341767]\n",
      "[ episode 158 ][ timestamp 53 ] state=[0.13059641 0.34208656 0.09443072 0.09341767], action=1, reward=1.0, next_state=[ 0.13743815  0.5357371   0.09629908 -0.1680426 ]\n",
      "[ episode 158 ][ timestamp 54 ] state=[ 0.13743815  0.5357371   0.09629908 -0.1680426 ], action=0, reward=1.0, next_state=[0.14815289 0.33937808 0.09293823 0.15339948]\n",
      "[ episode 158 ][ timestamp 55 ] state=[0.14815289 0.33937808 0.09293823 0.15339948], action=0, reward=1.0, next_state=[0.15494045 0.14305677 0.09600622 0.47389508]\n",
      "[ episode 158 ][ timestamp 56 ] state=[0.15494045 0.14305677 0.09600622 0.47389508], action=1, reward=1.0, next_state=[0.15780158 0.3367011  0.10548412 0.21294869]\n",
      "[ episode 158 ][ timestamp 57 ] state=[0.15780158 0.3367011  0.10548412 0.21294869], action=0, reward=1.0, next_state=[0.16453561 0.14024156 0.10974309 0.53695487]\n",
      "[ episode 158 ][ timestamp 58 ] state=[0.16453561 0.14024156 0.10974309 0.53695487], action=1, reward=1.0, next_state=[0.16734044 0.33366328 0.12048219 0.28076739]\n",
      "[ episode 158 ][ timestamp 59 ] state=[0.16734044 0.33366328 0.12048219 0.28076739], action=0, reward=1.0, next_state=[0.1740137  0.13704718 0.12609754 0.60888971]\n",
      "[ episode 158 ][ timestamp 60 ] state=[0.1740137  0.13704718 0.12609754 0.60888971], action=0, reward=1.0, next_state=[ 0.17675465 -0.05959102  0.13827533  0.93847963]\n",
      "[ episode 158 ][ timestamp 61 ] state=[ 0.17675465 -0.05959102  0.13827533  0.93847963], action=1, reward=1.0, next_state=[0.17556283 0.13342316 0.15704492 0.69224531]\n",
      "[ episode 158 ][ timestamp 62 ] state=[0.17556283 0.13342316 0.15704492 0.69224531], action=1, reward=1.0, next_state=[0.17823129 0.32605804 0.17088983 0.45283055]\n",
      "[ episode 158 ][ timestamp 63 ] state=[0.17823129 0.32605804 0.17088983 0.45283055], action=0, reward=1.0, next_state=[0.18475245 0.12898377 0.17994644 0.79413347]\n",
      "[ episode 158 ][ timestamp 64 ] state=[0.18475245 0.12898377 0.17994644 0.79413347], action=1, reward=1.0, next_state=[0.18733213 0.32123987 0.19582911 0.56302497]\n",
      "[ episode 158 ][ timestamp 65 ] state=[0.18733213 0.32123987 0.19582911 0.56302497], action=1, reward=1.0, next_state=[0.19375692 0.51315311 0.20708961 0.33786374]\n",
      "[ episode 158 ][ timestamp 66 ] state=[0.19375692 0.51315311 0.20708961 0.33786374], action=0, reward=-1.0, next_state=[0.20401999 0.31577884 0.21384688 0.68804944]\n",
      "[ Ended! ] Episode 158: Exploration_rate=0.45522245551230495. Score=66.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 159 ] state=[-0.03400541 -0.00463489  0.03554818  0.01472772]\n",
      "[ episode 159 ][ timestamp 1 ] state=[-0.03400541 -0.00463489  0.03554818  0.01472772], action=1, reward=1.0, next_state=[-0.03409811  0.1899597   0.03584273 -0.26653079]\n",
      "[ episode 159 ][ timestamp 2 ] state=[-0.03409811  0.1899597   0.03584273 -0.26653079], action=1, reward=1.0, next_state=[-0.03029891  0.38455226  0.03051212 -0.54769664]\n",
      "[ episode 159 ][ timestamp 3 ] state=[-0.03029891  0.38455226  0.03051212 -0.54769664], action=0, reward=1.0, next_state=[-0.02260787  0.18901523  0.01955819 -0.24555846]\n",
      "[ episode 159 ][ timestamp 4 ] state=[-0.02260787  0.18901523  0.01955819 -0.24555846], action=0, reward=1.0, next_state=[-0.01882756 -0.00638052  0.01464702  0.05322886]\n",
      "[ episode 159 ][ timestamp 5 ] state=[-0.01882756 -0.00638052  0.01464702  0.05322886], action=1, reward=1.0, next_state=[-0.01895517  0.18852838  0.01571159 -0.23479707]\n",
      "[ episode 159 ][ timestamp 6 ] state=[-0.01895517  0.18852838  0.01571159 -0.23479707], action=1, reward=1.0, next_state=[-0.01518461  0.38342237  0.01101565 -0.52248294]\n",
      "[ episode 159 ][ timestamp 7 ] state=[-0.01518461  0.38342237  0.01101565 -0.52248294], action=0, reward=1.0, next_state=[-0.00751616  0.18814711  0.00056599 -0.2263493 ]\n",
      "[ episode 159 ][ timestamp 8 ] state=[-0.00751616  0.18814711  0.00056599 -0.2263493 ], action=1, reward=1.0, next_state=[-0.00375322  0.38326097 -0.00396099 -0.51885364]\n",
      "[ episode 159 ][ timestamp 9 ] state=[-0.00375322  0.38326097 -0.00396099 -0.51885364], action=0, reward=1.0, next_state=[ 0.003912    0.18819501 -0.01433807 -0.22742152]\n",
      "[ episode 159 ][ timestamp 10 ] state=[ 0.003912    0.18819501 -0.01433807 -0.22742152], action=1, reward=1.0, next_state=[ 0.0076759   0.3835189  -0.0188865  -0.52459249]\n",
      "[ episode 159 ][ timestamp 11 ] state=[ 0.0076759   0.3835189  -0.0188865  -0.52459249], action=0, reward=1.0, next_state=[ 0.01534628  0.18866776 -0.02937835 -0.2379202 ]\n",
      "[ episode 159 ][ timestamp 12 ] state=[ 0.01534628  0.18866776 -0.02937835 -0.2379202 ], action=1, reward=1.0, next_state=[ 0.01911964  0.38419683 -0.03413675 -0.53972324]\n",
      "[ episode 159 ][ timestamp 13 ] state=[ 0.01911964  0.38419683 -0.03413675 -0.53972324], action=1, reward=1.0, next_state=[ 0.02680357  0.5797816  -0.04493121 -0.84296373]\n",
      "[ episode 159 ][ timestamp 14 ] state=[ 0.02680357  0.5797816  -0.04493121 -0.84296373], action=0, reward=1.0, next_state=[ 0.03839921  0.38530073 -0.06179049 -0.56474218]\n",
      "[ episode 159 ][ timestamp 15 ] state=[ 0.03839921  0.38530073 -0.06179049 -0.56474218], action=0, reward=1.0, next_state=[ 0.04610522  0.19109767 -0.07308533 -0.29214835]\n",
      "[ episode 159 ][ timestamp 16 ] state=[ 0.04610522  0.19109767 -0.07308533 -0.29214835], action=1, reward=1.0, next_state=[ 0.04992717  0.38718146 -0.0789283  -0.60695683]\n",
      "[ episode 159 ][ timestamp 17 ] state=[ 0.04992717  0.38718146 -0.0789283  -0.60695683], action=0, reward=1.0, next_state=[ 0.0576708   0.19324664 -0.09106744 -0.34014107]\n",
      "[ episode 159 ][ timestamp 18 ] state=[ 0.0576708   0.19324664 -0.09106744 -0.34014107], action=1, reward=1.0, next_state=[ 0.06153574  0.38953837 -0.09787026 -0.66009542]\n",
      "[ episode 159 ][ timestamp 19 ] state=[ 0.06153574  0.38953837 -0.09787026 -0.66009542], action=1, reward=1.0, next_state=[ 0.0693265   0.58587627 -0.11107217 -0.98192086]\n",
      "[ episode 159 ][ timestamp 20 ] state=[ 0.0693265   0.58587627 -0.11107217 -0.98192086], action=0, reward=1.0, next_state=[ 0.08104403  0.39240386 -0.13071058 -0.72608867]\n",
      "[ episode 159 ][ timestamp 21 ] state=[ 0.08104403  0.39240386 -0.13071058 -0.72608867], action=1, reward=1.0, next_state=[ 0.08889211  0.58906761 -0.14523236 -1.05688742]\n",
      "[ episode 159 ][ timestamp 22 ] state=[ 0.08889211  0.58906761 -0.14523236 -1.05688742], action=0, reward=1.0, next_state=[ 0.10067346  0.3961373  -0.16637011 -0.81308699]\n",
      "[ episode 159 ][ timestamp 23 ] state=[ 0.10067346  0.3961373  -0.16637011 -0.81308699], action=0, reward=1.0, next_state=[ 0.1085962   0.20363702 -0.18263185 -0.57701102]\n",
      "[ episode 159 ][ timestamp 24 ] state=[ 0.1085962   0.20363702 -0.18263185 -0.57701102], action=1, reward=1.0, next_state=[ 0.11266894  0.40078551 -0.19417207 -0.92121138]\n",
      "[ episode 159 ][ timestamp 25 ] state=[ 0.11266894  0.40078551 -0.19417207 -0.92121138], action=0, reward=-1.0, next_state=[ 0.12068465  0.20874222 -0.21259629 -0.69528838]\n",
      "[ Ended! ] Episode 159: Exploration_rate=0.4529463432347434. Score=25.\n",
      "[ Experience replay ] starts\n",
      "[ episode 160 ] state=[ 0.02788666 -0.01375458  0.0263237  -0.02025658]\n",
      "[ episode 160 ][ timestamp 1 ] state=[ 0.02788666 -0.01375458  0.0263237  -0.02025658], action=0, reward=1.0, next_state=[ 0.02761157 -0.20924396  0.02591857  0.28061417]\n",
      "[ episode 160 ][ timestamp 2 ] state=[ 0.02761157 -0.20924396  0.02591857  0.28061417], action=0, reward=1.0, next_state=[ 0.02342669 -0.40472586  0.03153085  0.58135774]\n",
      "[ episode 160 ][ timestamp 3 ] state=[ 0.02342669 -0.40472586  0.03153085  0.58135774], action=1, reward=1.0, next_state=[ 0.01533217 -0.21005957  0.04315801  0.29877198]\n",
      "[ episode 160 ][ timestamp 4 ] state=[ 0.01533217 -0.21005957  0.04315801  0.29877198], action=1, reward=1.0, next_state=[ 0.01113098 -0.01557853  0.04913345  0.02000657]\n",
      "[ episode 160 ][ timestamp 5 ] state=[ 0.01113098 -0.01557853  0.04913345  0.02000657], action=1, reward=1.0, next_state=[ 0.01081941  0.17880562  0.04953358 -0.25677835]\n",
      "[ episode 160 ][ timestamp 6 ] state=[ 0.01081941  0.17880562  0.04953358 -0.25677835], action=0, reward=1.0, next_state=[ 0.01439552 -0.01698724  0.04439801  0.05110764]\n",
      "[ episode 160 ][ timestamp 7 ] state=[ 0.01439552 -0.01698724  0.04439801  0.05110764], action=1, reward=1.0, next_state=[ 0.01405577  0.1774709   0.04542016 -0.2272434 ]\n",
      "[ episode 160 ][ timestamp 8 ] state=[ 0.01405577  0.1774709   0.04542016 -0.2272434 ], action=0, reward=1.0, next_state=[ 0.01760519 -0.01826974  0.0408753   0.07941368]\n",
      "[ episode 160 ][ timestamp 9 ] state=[ 0.01760519 -0.01826974  0.0408753   0.07941368], action=1, reward=1.0, next_state=[ 0.0172398   0.17624314  0.04246357 -0.20009794]\n",
      "[ episode 160 ][ timestamp 10 ] state=[ 0.0172398   0.17624314  0.04246357 -0.20009794], action=0, reward=1.0, next_state=[ 0.02076466 -0.01945961  0.03846161  0.1056721 ]\n",
      "[ episode 160 ][ timestamp 11 ] state=[ 0.02076466 -0.01945961  0.03846161  0.1056721 ], action=0, reward=1.0, next_state=[ 0.02037547 -0.21511102  0.04057505  0.41023711]\n",
      "[ episode 160 ][ timestamp 12 ] state=[ 0.02037547 -0.21511102  0.04057505  0.41023711], action=0, reward=1.0, next_state=[ 0.01607325 -0.41078402  0.04877979  0.71543082]\n",
      "[ episode 160 ][ timestamp 13 ] state=[ 0.01607325 -0.41078402  0.04877979  0.71543082], action=0, reward=1.0, next_state=[ 0.00785757 -0.60654603  0.06308841  1.02306012]\n",
      "[ episode 160 ][ timestamp 14 ] state=[ 0.00785757 -0.60654603  0.06308841  1.02306012], action=1, reward=1.0, next_state=[-0.00427335 -0.41231857  0.08354961  0.75083423]\n",
      "[ episode 160 ][ timestamp 15 ] state=[-0.00427335 -0.41231857  0.08354961  0.75083423], action=0, reward=1.0, next_state=[-0.01251972 -0.60848732  0.0985663   1.06859593]\n",
      "[ episode 160 ][ timestamp 16 ] state=[-0.01251972 -0.60848732  0.0985663   1.06859593], action=1, reward=1.0, next_state=[-0.02468947 -0.41479743  0.11993822  0.80840287]\n",
      "[ episode 160 ][ timestamp 17 ] state=[-0.02468947 -0.41479743  0.11993822  0.80840287], action=1, reward=1.0, next_state=[-0.03298542 -0.22150531  0.13610627  0.55572496]\n",
      "[ episode 160 ][ timestamp 18 ] state=[-0.03298542 -0.22150531  0.13610627  0.55572496], action=1, reward=1.0, next_state=[-0.03741553 -0.02853039  0.14722077  0.30883138]\n",
      "[ episode 160 ][ timestamp 19 ] state=[-0.03741553 -0.02853039  0.14722077  0.30883138], action=0, reward=1.0, next_state=[-0.03798613 -0.22540969  0.1533974   0.64408247]\n",
      "[ episode 160 ][ timestamp 20 ] state=[-0.03798613 -0.22540969  0.1533974   0.64408247], action=1, reward=1.0, next_state=[-0.04249433 -0.03272067  0.16627905  0.40336503]\n",
      "[ episode 160 ][ timestamp 21 ] state=[-0.04249433 -0.03272067  0.16627905  0.40336503], action=1, reward=1.0, next_state=[-0.04314874  0.15970088  0.17434635  0.16737477]\n",
      "[ episode 160 ][ timestamp 22 ] state=[-0.04314874  0.15970088  0.17434635  0.16737477], action=1, reward=1.0, next_state=[-0.03995472  0.35195426  0.17769385 -0.06563496]\n",
      "[ episode 160 ][ timestamp 23 ] state=[-0.03995472  0.35195426  0.17769385 -0.06563496], action=0, reward=1.0, next_state=[-0.03291564  0.15478872  0.17638115  0.27742398]\n",
      "[ episode 160 ][ timestamp 24 ] state=[-0.03291564  0.15478872  0.17638115  0.27742398], action=1, reward=1.0, next_state=[-0.02981986  0.34701337  0.18192963  0.04514812]\n",
      "[ episode 160 ][ timestamp 25 ] state=[-0.02981986  0.34701337  0.18192963  0.04514812], action=1, reward=1.0, next_state=[-0.0228796   0.53912359  0.18283259 -0.1850687 ]\n",
      "[ episode 160 ][ timestamp 26 ] state=[-0.0228796   0.53912359  0.18283259 -0.1850687 ], action=1, reward=1.0, next_state=[-0.01209712  0.73122292  0.17913121 -0.41496121]\n",
      "[ episode 160 ][ timestamp 27 ] state=[-0.01209712  0.73122292  0.17913121 -0.41496121], action=0, reward=1.0, next_state=[ 0.00252733  0.53407446  0.17083199 -0.07158704]\n",
      "[ episode 160 ][ timestamp 28 ] state=[ 0.00252733  0.53407446  0.17083199 -0.07158704], action=0, reward=1.0, next_state=[0.01320882 0.33696759 0.16940025 0.26975022]\n",
      "[ episode 160 ][ timestamp 29 ] state=[0.01320882 0.33696759 0.16940025 0.26975022], action=1, reward=1.0, next_state=[0.01994817 0.52931823 0.17479525 0.03492001]\n",
      "[ episode 160 ][ timestamp 30 ] state=[0.01994817 0.52931823 0.17479525 0.03492001], action=0, reward=1.0, next_state=[0.03053454 0.33217668 0.17549365 0.37725484]\n",
      "[ episode 160 ][ timestamp 31 ] state=[0.03053454 0.33217668 0.17549365 0.37725484], action=1, reward=1.0, next_state=[0.03717807 0.52442858 0.18303875 0.14463703]\n",
      "[ episode 160 ][ timestamp 32 ] state=[0.03717807 0.52442858 0.18303875 0.14463703], action=1, reward=1.0, next_state=[ 0.04766664  0.71652177  0.18593149 -0.08517601]\n",
      "[ episode 160 ][ timestamp 33 ] state=[ 0.04766664  0.71652177  0.18593149 -0.08517601], action=0, reward=1.0, next_state=[0.06199708 0.51928882 0.18422797 0.25992375]\n",
      "[ episode 160 ][ timestamp 34 ] state=[0.06199708 0.51928882 0.18422797 0.25992375], action=1, reward=1.0, next_state=[0.07238286 0.71136849 0.18942645 0.03053698]\n",
      "[ episode 160 ][ timestamp 35 ] state=[0.07238286 0.71136849 0.18942645 0.03053698], action=0, reward=1.0, next_state=[0.08661023 0.51410633 0.19003719 0.37649631]\n",
      "[ episode 160 ][ timestamp 36 ] state=[0.08661023 0.51410633 0.19003719 0.37649631], action=1, reward=1.0, next_state=[0.09689235 0.70609289 0.19756711 0.14923616]\n",
      "[ episode 160 ][ timestamp 37 ] state=[0.09689235 0.70609289 0.19756711 0.14923616], action=0, reward=1.0, next_state=[0.11101421 0.50877123 0.20055183 0.49716851]\n",
      "[ episode 160 ][ timestamp 38 ] state=[0.11101421 0.50877123 0.20055183 0.49716851], action=1, reward=-1.0, next_state=[0.12118964 0.70058454 0.21049521 0.27378316]\n",
      "[ Ended! ] Episode 160: Exploration_rate=0.4506816115185697. Score=38.\n",
      "[ Experience replay ] starts\n",
      "[ episode 161 ] state=[ 0.01339791 -0.01356778  0.04982747  0.02208875]\n",
      "[ episode 161 ][ timestamp 1 ] state=[ 0.01339791 -0.01356778  0.04982747  0.02208875], action=0, reward=1.0, next_state=[ 0.01312656 -0.20936759  0.05026924  0.33006715]\n",
      "[ episode 161 ][ timestamp 2 ] state=[ 0.01312656 -0.20936759  0.05026924  0.33006715], action=0, reward=1.0, next_state=[ 0.00893921 -0.40516775  0.05687059  0.63816931]\n",
      "[ episode 161 ][ timestamp 3 ] state=[ 0.00893921 -0.40516775  0.05687059  0.63816931], action=0, reward=1.0, next_state=[ 8.35851941e-04 -6.01034660e-01  6.96339740e-02  9.48205634e-01]\n",
      "[ episode 161 ][ timestamp 4 ] state=[ 8.35851941e-04 -6.01034660e-01  6.96339740e-02  9.48205634e-01], action=0, reward=1.0, next_state=[-0.01118484 -0.79702159  0.08859809  1.26192942]\n",
      "[ episode 161 ][ timestamp 5 ] state=[-0.01118484 -0.79702159  0.08859809  1.26192942], action=1, reward=1.0, next_state=[-0.02712527 -0.60313722  0.11383668  0.99825734]\n",
      "[ episode 161 ][ timestamp 6 ] state=[-0.02712527 -0.60313722  0.11383668  0.99825734], action=1, reward=1.0, next_state=[-0.03918802 -0.40970597  0.13380182  0.74338416]\n",
      "[ episode 161 ][ timestamp 7 ] state=[-0.03918802 -0.40970597  0.13380182  0.74338416], action=1, reward=1.0, next_state=[-0.04738214 -0.21665957  0.1486695   0.49562323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 161 ][ timestamp 8 ] state=[-0.04738214 -0.21665957  0.1486695   0.49562323], action=0, reward=1.0, next_state=[-0.05171533 -0.41353083  0.15858197  0.83122059]\n",
      "[ episode 161 ][ timestamp 9 ] state=[-0.05171533 -0.41353083  0.15858197  0.83122059], action=1, reward=1.0, next_state=[-0.05998594 -0.22089031  0.17520638  0.59231356]\n",
      "[ episode 161 ][ timestamp 10 ] state=[-0.05998594 -0.22089031  0.17520638  0.59231356], action=0, reward=1.0, next_state=[-0.06440375 -0.41797604  0.18705265  0.93466379]\n",
      "[ episode 161 ][ timestamp 11 ] state=[-0.06440375 -0.41797604  0.18705265  0.93466379], action=0, reward=1.0, next_state=[-0.07276327 -0.61506125  0.20574593  1.27980821]\n",
      "[ episode 161 ][ timestamp 12 ] state=[-0.07276327 -0.61506125  0.20574593  1.27980821], action=1, reward=-1.0, next_state=[-0.0850645  -0.42306731  0.23134209  1.05795479]\n",
      "[ Ended! ] Episode 161: Exploration_rate=0.4484282034609769. Score=12.\n",
      "[ Experience replay ] starts\n",
      "[ episode 162 ] state=[ 0.04564916  0.00350867 -0.03269501  0.00262439]\n",
      "[ episode 162 ][ timestamp 1 ] state=[ 0.04564916  0.00350867 -0.03269501  0.00262439], action=0, reward=1.0, next_state=[ 0.04571933 -0.1911295  -0.03264252  0.284815  ]\n",
      "[ episode 162 ][ timestamp 2 ] state=[ 0.04571933 -0.1911295  -0.03264252  0.284815  ], action=0, reward=1.0, next_state=[ 0.04189674 -0.38577105 -0.02694622  0.5670266 ]\n",
      "[ episode 162 ][ timestamp 3 ] state=[ 0.04189674 -0.38577105 -0.02694622  0.5670266 ], action=1, reward=1.0, next_state=[ 0.03418132 -0.19028168 -0.01560569  0.26597776]\n",
      "[ episode 162 ][ timestamp 4 ] state=[ 0.03418132 -0.19028168 -0.01560569  0.26597776], action=0, reward=1.0, next_state=[ 0.03037569 -0.38517746 -0.01028613  0.55369795]\n",
      "[ episode 162 ][ timestamp 5 ] state=[ 0.03037569 -0.38517746 -0.01028613  0.55369795], action=1, reward=1.0, next_state=[ 0.02267214 -0.18991259  0.00078783  0.25779207]\n",
      "[ episode 162 ][ timestamp 6 ] state=[ 0.02267214 -0.18991259  0.00078783  0.25779207], action=1, reward=1.0, next_state=[ 0.01887388  0.00519811  0.00594367 -0.03464226]\n",
      "[ episode 162 ][ timestamp 7 ] state=[ 0.01887388  0.00519811  0.00594367 -0.03464226], action=1, reward=1.0, next_state=[ 0.01897785  0.20023432  0.00525082 -0.32544399]\n",
      "[ episode 162 ][ timestamp 8 ] state=[ 0.01897785  0.20023432  0.00525082 -0.32544399], action=0, reward=1.0, next_state=[ 0.02298253  0.005038   -0.00125806 -0.03110981]\n",
      "[ episode 162 ][ timestamp 9 ] state=[ 0.02298253  0.005038   -0.00125806 -0.03110981], action=0, reward=1.0, next_state=[ 0.02308329 -0.19006588 -0.00188025  0.26117592]\n",
      "[ episode 162 ][ timestamp 10 ] state=[ 0.02308329 -0.19006588 -0.00188025  0.26117592], action=0, reward=1.0, next_state=[ 0.01928198 -0.38516095  0.00334327  0.5532652 ]\n",
      "[ episode 162 ][ timestamp 11 ] state=[ 0.01928198 -0.38516095  0.00334327  0.5532652 ], action=1, reward=1.0, next_state=[ 0.01157876 -0.1900861   0.01440857  0.26163749]\n",
      "[ episode 162 ][ timestamp 12 ] state=[ 0.01157876 -0.1900861   0.01440857  0.26163749], action=1, reward=1.0, next_state=[ 0.00777703  0.00482724  0.01964132 -0.0264662 ]\n",
      "[ episode 162 ][ timestamp 13 ] state=[ 0.00777703  0.00482724  0.01964132 -0.0264662 ], action=0, reward=1.0, next_state=[ 0.00787358 -0.1905708   0.019112    0.27234849]\n",
      "[ episode 162 ][ timestamp 14 ] state=[ 0.00787358 -0.1905708   0.019112    0.27234849], action=1, reward=1.0, next_state=[ 0.00406216  0.0042733   0.02455897 -0.0142457 ]\n",
      "[ episode 162 ][ timestamp 15 ] state=[ 0.00406216  0.0042733   0.02455897 -0.0142457 ], action=1, reward=1.0, next_state=[ 0.00414763  0.1990346   0.02427405 -0.29907993]\n",
      "[ episode 162 ][ timestamp 16 ] state=[ 0.00414763  0.1990346   0.02427405 -0.29907993], action=0, reward=1.0, next_state=[0.00812832 0.0035752  0.01829245 0.00115866]\n",
      "[ episode 162 ][ timestamp 17 ] state=[0.00812832 0.0035752  0.01829245 0.00115866], action=0, reward=1.0, next_state=[ 0.00819983 -0.19180425  0.01831563  0.29955649]\n",
      "[ episode 162 ][ timestamp 18 ] state=[ 0.00819983 -0.19180425  0.01831563  0.29955649], action=1, reward=1.0, next_state=[0.00436374 0.00305191 0.02430676 0.01270577]\n",
      "[ episode 162 ][ timestamp 19 ] state=[0.00436374 0.00305191 0.02430676 0.01270577], action=1, reward=1.0, next_state=[ 0.00442478  0.19781699  0.02456087 -0.27221007]\n",
      "[ episode 162 ][ timestamp 20 ] state=[ 0.00442478  0.19781699  0.02456087 -0.27221007], action=0, reward=1.0, next_state=[0.00838112 0.00235334 0.01911667 0.02811714]\n",
      "[ episode 162 ][ timestamp 21 ] state=[0.00838112 0.00235334 0.01911667 0.02811714], action=0, reward=1.0, next_state=[ 0.00842819 -0.19303747  0.01967901  0.32676976]\n",
      "[ episode 162 ][ timestamp 22 ] state=[ 0.00842819 -0.19303747  0.01967901  0.32676976], action=1, reward=1.0, next_state=[0.00456744 0.00179886 0.02621441 0.04035711]\n",
      "[ episode 162 ][ timestamp 23 ] state=[0.00456744 0.00179886 0.02621441 0.04035711], action=0, reward=1.0, next_state=[ 0.00460341 -0.193689    0.02702155  0.34119431]\n",
      "[ episode 162 ][ timestamp 24 ] state=[ 0.00460341 -0.193689    0.02702155  0.34119431], action=1, reward=1.0, next_state=[0.00072963 0.00103828 0.03384544 0.0571534 ]\n",
      "[ episode 162 ][ timestamp 25 ] state=[0.00072963 0.00103828 0.03384544 0.0571534 ], action=0, reward=1.0, next_state=[ 0.0007504  -0.1945522   0.03498851  0.36031975]\n",
      "[ episode 162 ][ timestamp 26 ] state=[ 0.0007504  -0.1945522   0.03498851  0.36031975], action=1, reward=1.0, next_state=[-3.14064537e-03  5.53794777e-05  4.21949007e-02  7.88715619e-02]\n",
      "[ episode 162 ][ timestamp 27 ] state=[-3.14064537e-03  5.53794777e-05  4.21949007e-02  7.88715619e-02], action=1, reward=1.0, next_state=[-0.00313954  0.19454785  0.04377233 -0.20020585]\n",
      "[ episode 162 ][ timestamp 28 ] state=[-0.00313954  0.19454785  0.04377233 -0.20020585], action=0, reward=1.0, next_state=[ 0.00075142 -0.00117193  0.03976821  0.10595756]\n",
      "[ episode 162 ][ timestamp 29 ] state=[ 0.00075142 -0.00117193  0.03976821  0.10595756], action=0, reward=1.0, next_state=[ 0.00072798 -0.19684055  0.04188737  0.41091721]\n",
      "[ episode 162 ][ timestamp 30 ] state=[ 0.00072798 -0.19684055  0.04188737  0.41091721], action=1, reward=1.0, next_state=[-0.00320883 -0.00233668  0.05010571  0.1317286 ]\n",
      "[ episode 162 ][ timestamp 31 ] state=[-0.00320883 -0.00233668  0.05010571  0.1317286 ], action=1, reward=1.0, next_state=[-0.00325556  0.19203306  0.05274028 -0.14473518]\n",
      "[ episode 162 ][ timestamp 32 ] state=[-0.00325556  0.19203306  0.05274028 -0.14473518], action=1, reward=1.0, next_state=[ 0.0005851   0.3863616   0.04984558 -0.42032423]\n",
      "[ episode 162 ][ timestamp 33 ] state=[ 0.0005851   0.3863616   0.04984558 -0.42032423], action=0, reward=1.0, next_state=[ 0.00831233  0.19057013  0.04143909 -0.11235326]\n",
      "[ episode 162 ][ timestamp 34 ] state=[ 0.00831233  0.19057013  0.04143909 -0.11235326], action=0, reward=1.0, next_state=[ 0.01212373 -0.00512036  0.03919203  0.19311008]\n",
      "[ episode 162 ][ timestamp 35 ] state=[ 0.01212373 -0.00512036  0.03919203  0.19311008], action=1, reward=1.0, next_state=[ 0.01202132  0.18941967  0.04305423 -0.08695637]\n",
      "[ episode 162 ][ timestamp 36 ] state=[ 0.01202132  0.18941967  0.04305423 -0.08695637], action=0, reward=1.0, next_state=[ 0.01580972 -0.00629213  0.0413151   0.21899332]\n",
      "[ episode 162 ][ timestamp 37 ] state=[ 0.01580972 -0.00629213  0.0413151   0.21899332], action=0, reward=1.0, next_state=[ 0.01568388 -0.20197957  0.04569497  0.52441718]\n",
      "[ episode 162 ][ timestamp 38 ] state=[ 0.01568388 -0.20197957  0.04569497  0.52441718], action=1, reward=1.0, next_state=[ 0.01164428 -0.00752947  0.05618331  0.24647614]\n",
      "[ episode 162 ][ timestamp 39 ] state=[ 0.01164428 -0.00752947  0.05618331  0.24647614], action=1, reward=1.0, next_state=[ 0.01149369  0.18674693  0.06111284 -0.02796944]\n",
      "[ episode 162 ][ timestamp 40 ] state=[ 0.01149369  0.18674693  0.06111284 -0.02796944], action=0, reward=1.0, next_state=[ 0.01522863 -0.00919576  0.06055345  0.28335191]\n",
      "[ episode 162 ][ timestamp 41 ] state=[ 0.01522863 -0.00919576  0.06055345  0.28335191], action=1, reward=1.0, next_state=[0.01504472 0.18501258 0.06622049 0.01036514]\n",
      "[ episode 162 ][ timestamp 42 ] state=[0.01504472 0.18501258 0.06622049 0.01036514], action=0, reward=1.0, next_state=[ 0.01874497 -0.01099349  0.06642779  0.32318444]\n",
      "[ episode 162 ][ timestamp 43 ] state=[ 0.01874497 -0.01099349  0.06642779  0.32318444], action=0, reward=1.0, next_state=[ 0.0185251  -0.20699535  0.07289148  0.63605422]\n",
      "[ episode 162 ][ timestamp 44 ] state=[ 0.0185251  -0.20699535  0.07289148  0.63605422], action=0, reward=1.0, next_state=[ 0.01438519 -0.40305415  0.08561256  0.95077261]\n",
      "[ episode 162 ][ timestamp 45 ] state=[ 0.01438519 -0.40305415  0.08561256  0.95077261], action=1, reward=1.0, next_state=[ 0.00632411 -0.20918232  0.10462801  0.68616932]\n",
      "[ episode 162 ][ timestamp 46 ] state=[ 0.00632411 -0.20918232  0.10462801  0.68616932], action=1, reward=1.0, next_state=[ 0.00214046 -0.01565644  0.1183514   0.42817249]\n",
      "[ episode 162 ][ timestamp 47 ] state=[ 0.00214046 -0.01565644  0.1183514   0.42817249], action=1, reward=1.0, next_state=[0.00182733 0.17760795 0.12691485 0.17501797]\n",
      "[ episode 162 ][ timestamp 48 ] state=[0.00182733 0.17760795 0.12691485 0.17501797], action=1, reward=1.0, next_state=[ 0.00537949  0.37070665  0.13041521 -0.0750876 ]\n",
      "[ episode 162 ][ timestamp 49 ] state=[ 0.00537949  0.37070665  0.13041521 -0.0750876 ], action=1, reward=1.0, next_state=[ 0.01279363  0.56374124  0.12891346 -0.32394712]\n",
      "[ episode 162 ][ timestamp 50 ] state=[ 0.01279363  0.56374124  0.12891346 -0.32394712], action=0, reward=1.0, next_state=[0.02406845 0.36704192 0.12243451 0.00644925]\n",
      "[ episode 162 ][ timestamp 51 ] state=[0.02406845 0.36704192 0.12243451 0.00644925], action=1, reward=1.0, next_state=[ 0.03140929  0.56021466  0.1225635  -0.24523491]\n",
      "[ episode 162 ][ timestamp 52 ] state=[ 0.03140929  0.56021466  0.1225635  -0.24523491], action=0, reward=1.0, next_state=[0.04261358 0.36357475 0.1176588  0.08345583]\n",
      "[ episode 162 ][ timestamp 53 ] state=[0.04261358 0.36357475 0.1176588  0.08345583], action=0, reward=1.0, next_state=[0.04988508 0.16697995 0.11932792 0.41082113]\n",
      "[ episode 162 ][ timestamp 54 ] state=[0.04988508 0.16697995 0.11932792 0.41082113], action=1, reward=1.0, next_state=[0.05322468 0.36022598 0.12754434 0.1580126 ]\n",
      "[ episode 162 ][ timestamp 55 ] state=[0.05322468 0.36022598 0.12754434 0.1580126 ], action=0, reward=1.0, next_state=[0.0604292  0.16353059 0.13070459 0.48805557]\n",
      "[ episode 162 ][ timestamp 56 ] state=[0.0604292  0.16353059 0.13070459 0.48805557], action=1, reward=1.0, next_state=[0.06369981 0.35658965 0.1404657  0.23925491]\n",
      "[ episode 162 ][ timestamp 57 ] state=[0.06369981 0.35658965 0.1404657  0.23925491], action=1, reward=1.0, next_state=[ 0.0708316   0.54945472  0.1452508  -0.00603212]\n",
      "[ episode 162 ][ timestamp 58 ] state=[ 0.0708316   0.54945472  0.1452508  -0.00603212], action=0, reward=1.0, next_state=[0.0818207  0.35258063 0.14513016 0.32872301]\n",
      "[ episode 162 ][ timestamp 59 ] state=[0.0818207  0.35258063 0.14513016 0.32872301], action=1, reward=1.0, next_state=[0.08887231 0.54537051 0.15170462 0.08509701]\n",
      "[ episode 162 ][ timestamp 60 ] state=[0.08887231 0.54537051 0.15170462 0.08509701], action=0, reward=1.0, next_state=[0.09977972 0.34843628 0.15340656 0.42153591]\n",
      "[ episode 162 ][ timestamp 61 ] state=[0.09977972 0.34843628 0.15340656 0.42153591], action=0, reward=1.0, next_state=[0.10674844 0.15151155 0.16183728 0.75837891]\n",
      "[ episode 162 ][ timestamp 62 ] state=[0.10674844 0.15151155 0.16183728 0.75837891], action=1, reward=1.0, next_state=[0.10977867 0.34407741 0.17700486 0.52067726]\n",
      "[ episode 162 ][ timestamp 63 ] state=[0.10977867 0.34407741 0.17700486 0.52067726], action=1, reward=1.0, next_state=[0.11666022 0.53632398 0.1874184  0.28858116]\n",
      "[ episode 162 ][ timestamp 64 ] state=[0.11666022 0.53632398 0.1874184  0.28858116], action=1, reward=1.0, next_state=[0.1273867  0.72834793 0.19319002 0.06036816]\n",
      "[ episode 162 ][ timestamp 65 ] state=[0.1273867  0.72834793 0.19319002 0.06036816], action=0, reward=1.0, next_state=[0.14195366 0.53105657 0.19439739 0.40724503]\n",
      "[ episode 162 ][ timestamp 66 ] state=[0.14195366 0.53105657 0.19439739 0.40724503], action=0, reward=1.0, next_state=[0.15257479 0.33378629 0.20254229 0.7543704 ]\n",
      "[ episode 162 ][ timestamp 67 ] state=[0.15257479 0.33378629 0.20254229 0.7543704 ], action=1, reward=-1.0, next_state=[0.15925052 0.52562595 0.2176297  0.53163433]\n",
      "[ Ended! ] Episode 162: Exploration_rate=0.446186062443672. Score=67.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 163 ] state=[-0.01775278  0.03640775 -0.02781933  0.00787557]\n",
      "[ episode 163 ][ timestamp 1 ] state=[-0.01775278  0.03640775 -0.02781933  0.00787557], action=0, reward=1.0, next_state=[-0.01702463 -0.15830442 -0.02766182  0.29165298]\n",
      "[ episode 163 ][ timestamp 2 ] state=[-0.01702463 -0.15830442 -0.02766182  0.29165298], action=1, reward=1.0, next_state=[-0.02019072  0.03720081 -0.02182876 -0.0096242 ]\n",
      "[ episode 163 ][ timestamp 3 ] state=[-0.02019072  0.03720081 -0.02182876 -0.0096242 ], action=1, reward=1.0, next_state=[-0.0194467   0.2326289  -0.02202124 -0.30911365]\n",
      "[ episode 163 ][ timestamp 4 ] state=[-0.0194467   0.2326289  -0.02202124 -0.30911365], action=1, reward=1.0, next_state=[-0.01479412  0.42805758 -0.02820351 -0.60865931]\n",
      "[ episode 163 ][ timestamp 5 ] state=[-0.01479412  0.42805758 -0.02820351 -0.60865931], action=0, reward=1.0, next_state=[-0.00623297  0.23334103 -0.0403767  -0.32499138]\n",
      "[ episode 163 ][ timestamp 6 ] state=[-0.00623297  0.23334103 -0.0403767  -0.32499138], action=1, reward=1.0, next_state=[-0.00156615  0.42901393 -0.04687653 -0.63012903]\n",
      "[ episode 163 ][ timestamp 7 ] state=[-0.00156615  0.42901393 -0.04687653 -0.63012903], action=0, reward=1.0, next_state=[ 0.00701413  0.23457636 -0.05947911 -0.35256971]\n",
      "[ episode 163 ][ timestamp 8 ] state=[ 0.00701413  0.23457636 -0.05947911 -0.35256971], action=0, reward=1.0, next_state=[ 0.01170565  0.04034842 -0.0665305  -0.07921956]\n",
      "[ episode 163 ][ timestamp 9 ] state=[ 0.01170565  0.04034842 -0.0665305  -0.07921956], action=1, reward=1.0, next_state=[ 0.01251262  0.23635791 -0.06811489 -0.39212887]\n",
      "[ episode 163 ][ timestamp 10 ] state=[ 0.01251262  0.23635791 -0.06811489 -0.39212887], action=1, reward=1.0, next_state=[ 0.01723978  0.43237705 -0.07595747 -0.70548606]\n",
      "[ episode 163 ][ timestamp 11 ] state=[ 0.01723978  0.43237705 -0.07595747 -0.70548606], action=0, reward=1.0, next_state=[ 0.02588732  0.23838515 -0.09006719 -0.43764726]\n",
      "[ episode 163 ][ timestamp 12 ] state=[ 0.02588732  0.23838515 -0.09006719 -0.43764726], action=1, reward=1.0, next_state=[ 0.03065503  0.43465883 -0.09882014 -0.75730841]\n",
      "[ episode 163 ][ timestamp 13 ] state=[ 0.03065503  0.43465883 -0.09882014 -0.75730841], action=0, reward=1.0, next_state=[ 0.0393482   0.2410276  -0.11396631 -0.49728444]\n",
      "[ episode 163 ][ timestamp 14 ] state=[ 0.0393482   0.2410276  -0.11396631 -0.49728444], action=1, reward=1.0, next_state=[ 0.04416875  0.43755648 -0.12391199 -0.823599  ]\n",
      "[ episode 163 ][ timestamp 15 ] state=[ 0.04416875  0.43755648 -0.12391199 -0.823599  ], action=0, reward=1.0, next_state=[ 0.05291988  0.24432766 -0.14038397 -0.57231505]\n",
      "[ episode 163 ][ timestamp 16 ] state=[ 0.05291988  0.24432766 -0.14038397 -0.57231505], action=1, reward=1.0, next_state=[ 0.05780644  0.44110995 -0.15183028 -0.90572213]\n",
      "[ episode 163 ][ timestamp 17 ] state=[ 0.05780644  0.44110995 -0.15183028 -0.90572213], action=0, reward=1.0, next_state=[ 0.06662864  0.24833379 -0.16994472 -0.66435126]\n",
      "[ episode 163 ][ timestamp 18 ] state=[ 0.06662864  0.24833379 -0.16994472 -0.66435126], action=1, reward=1.0, next_state=[ 0.07159531  0.44536114 -0.18323174 -1.00535834]\n",
      "[ episode 163 ][ timestamp 19 ] state=[ 0.07159531  0.44536114 -0.18323174 -1.00535834], action=1, reward=1.0, next_state=[ 0.08050253  0.64239416 -0.20333891 -1.34952957]\n",
      "[ episode 163 ][ timestamp 20 ] state=[ 0.08050253  0.64239416 -0.20333891 -1.34952957], action=0, reward=-1.0, next_state=[ 0.09335042  0.45032298 -0.2303295  -1.12672896]\n",
      "[ Ended! ] Episode 163: Exploration_rate=0.4439551321314536. Score=20.\n",
      "[ Experience replay ] starts\n",
      "[ episode 164 ] state=[ 0.03794026  0.00385104  0.00030561 -0.00850178]\n",
      "[ episode 164 ][ timestamp 1 ] state=[ 0.03794026  0.00385104  0.00030561 -0.00850178], action=0, reward=1.0, next_state=[ 3.80172822e-02 -1.91275295e-01  1.35570377e-04  2.84277556e-01]\n",
      "[ episode 164 ][ timestamp 2 ] state=[ 3.80172822e-02 -1.91275295e-01  1.35570377e-04  2.84277556e-01], action=1, reward=1.0, next_state=[ 0.03419178  0.00384472  0.00582112 -0.00836261]\n",
      "[ episode 164 ][ timestamp 3 ] state=[ 0.03419178  0.00384472  0.00582112 -0.00836261], action=1, reward=1.0, next_state=[ 0.03426867  0.19888271  0.00565387 -0.29920323]\n",
      "[ episode 164 ][ timestamp 4 ] state=[ 0.03426867  0.19888271  0.00565387 -0.29920323], action=1, reward=1.0, next_state=[ 3.82463249e-02  3.93923615e-01 -3.30195359e-04 -5.90097687e-01]\n",
      "[ episode 164 ][ timestamp 5 ] state=[ 3.82463249e-02  3.93923615e-01 -3.30195359e-04 -5.90097687e-01], action=0, reward=1.0, next_state=[ 0.0461248   0.19880629 -0.01213215 -0.29751879]\n",
      "[ episode 164 ][ timestamp 6 ] state=[ 0.0461248   0.19880629 -0.01213215 -0.29751879], action=0, reward=1.0, next_state=[ 0.05010092  0.00385936 -0.01808252 -0.00868669]\n",
      "[ episode 164 ][ timestamp 7 ] state=[ 0.05010092  0.00385936 -0.01808252 -0.00868669], action=1, reward=1.0, next_state=[ 0.05017811  0.19923591 -0.01825626 -0.30701957]\n",
      "[ episode 164 ][ timestamp 8 ] state=[ 0.05017811  0.19923591 -0.01825626 -0.30701957], action=0, reward=1.0, next_state=[ 0.05416283  0.0043788  -0.02439665 -0.02014965]\n",
      "[ episode 164 ][ timestamp 9 ] state=[ 0.05416283  0.0043788  -0.02439665 -0.02014965], action=0, reward=1.0, next_state=[ 0.0542504  -0.19038494 -0.02479964  0.26473711]\n",
      "[ episode 164 ][ timestamp 10 ] state=[ 0.0542504  -0.19038494 -0.02479964  0.26473711], action=0, reward=1.0, next_state=[ 0.05044271 -0.3851443  -0.0195049   0.54949598]\n",
      "[ episode 164 ][ timestamp 11 ] state=[ 0.05044271 -0.3851443  -0.0195049   0.54949598], action=1, reward=1.0, next_state=[ 0.04273982 -0.18975388 -0.00851498  0.25073202]\n",
      "[ episode 164 ][ timestamp 12 ] state=[ 0.04273982 -0.18975388 -0.00851498  0.25073202], action=1, reward=1.0, next_state=[ 0.03894474  0.00548863 -0.00350034 -0.0446245 ]\n",
      "[ episode 164 ][ timestamp 13 ] state=[ 0.03894474  0.00548863 -0.00350034 -0.0446245 ], action=0, reward=1.0, next_state=[ 0.03905451 -0.18958296 -0.00439283  0.24695199]\n",
      "[ episode 164 ][ timestamp 14 ] state=[ 0.03905451 -0.18958296 -0.00439283  0.24695199], action=0, reward=1.0, next_state=[ 0.03526286 -0.3846419   0.00054621  0.53824608]\n",
      "[ episode 164 ][ timestamp 15 ] state=[ 0.03526286 -0.3846419   0.00054621  0.53824608], action=1, reward=1.0, next_state=[ 0.02757002 -0.18952763  0.01131113  0.24573531]\n",
      "[ episode 164 ][ timestamp 16 ] state=[ 0.02757002 -0.18952763  0.01131113  0.24573531], action=1, reward=1.0, next_state=[ 0.02377947  0.00543096  0.01622584 -0.04335846]\n",
      "[ episode 164 ][ timestamp 17 ] state=[ 0.02377947  0.00543096  0.01622584 -0.04335846], action=0, reward=1.0, next_state=[ 0.02388808 -0.18991986  0.01535867  0.25439938]\n",
      "[ episode 164 ][ timestamp 18 ] state=[ 0.02388808 -0.18991986  0.01535867  0.25439938], action=1, reward=1.0, next_state=[ 0.02008969  0.00497947  0.02044665 -0.03339986]\n",
      "[ episode 164 ][ timestamp 19 ] state=[ 0.02008969  0.00497947  0.02044665 -0.03339986], action=0, reward=1.0, next_state=[ 0.02018928 -0.19042964  0.01977866  0.26566343]\n",
      "[ episode 164 ][ timestamp 20 ] state=[ 0.02018928 -0.19042964  0.01977866  0.26566343], action=1, reward=1.0, next_state=[ 0.01638068  0.00440452  0.02509193 -0.0207161 ]\n",
      "[ episode 164 ][ timestamp 21 ] state=[ 0.01638068  0.00440452  0.02509193 -0.0207161 ], action=0, reward=1.0, next_state=[ 0.01646877 -0.19106812  0.0246776   0.27977682]\n",
      "[ episode 164 ][ timestamp 22 ] state=[ 0.01646877 -0.19106812  0.0246776   0.27977682], action=1, reward=1.0, next_state=[ 0.01264741  0.00369327  0.03027314 -0.00502183]\n",
      "[ episode 164 ][ timestamp 23 ] state=[ 0.01264741  0.00369327  0.03027314 -0.00502183], action=0, reward=1.0, next_state=[ 0.01272128 -0.19184947  0.0301727   0.29705682]\n",
      "[ episode 164 ][ timestamp 24 ] state=[ 0.01272128 -0.19184947  0.0301727   0.29705682], action=1, reward=1.0, next_state=[0.00888429 0.00282965 0.03611384 0.01404048]\n",
      "[ episode 164 ][ timestamp 25 ] state=[0.00888429 0.00282965 0.03611384 0.01404048], action=0, reward=1.0, next_state=[ 0.00894088 -0.19279111  0.03639465  0.31789545]\n",
      "[ episode 164 ][ timestamp 26 ] state=[ 0.00894088 -0.19279111  0.03639465  0.31789545], action=1, reward=1.0, next_state=[0.00508506 0.00179409 0.04275256 0.03690861]\n",
      "[ episode 164 ][ timestamp 27 ] state=[0.00508506 0.00179409 0.04275256 0.03690861], action=0, reward=1.0, next_state=[ 0.00512094 -0.19391403  0.04349073  0.34276797]\n",
      "[ episode 164 ][ timestamp 28 ] state=[ 0.00512094 -0.19391403  0.04349073  0.34276797], action=1, reward=1.0, next_state=[0.00124266 0.00056307 0.05034609 0.0641104 ]\n",
      "[ episode 164 ][ timestamp 29 ] state=[0.00124266 0.00056307 0.05034609 0.0641104 ], action=0, reward=1.0, next_state=[ 0.00125392 -0.19524321  0.0516283   0.37224317]\n",
      "[ episode 164 ][ timestamp 30 ] state=[ 0.00125392 -0.19524321  0.0516283   0.37224317], action=1, reward=1.0, next_state=[-0.00265094 -0.00089127  0.05907316  0.09627567]\n",
      "[ episode 164 ][ timestamp 31 ] state=[-0.00265094 -0.00089127  0.05907316  0.09627567], action=0, reward=1.0, next_state=[-0.00266877 -0.19680794  0.06099868  0.40699549]\n",
      "[ episode 164 ][ timestamp 32 ] state=[-0.00266877 -0.19680794  0.06099868  0.40699549], action=1, reward=1.0, next_state=[-0.00660493 -0.00260159  0.06913859  0.13415024]\n",
      "[ episode 164 ][ timestamp 33 ] state=[-0.00660493 -0.00260159  0.06913859  0.13415024], action=1, reward=1.0, next_state=[-0.00665696  0.19146541  0.07182159 -0.13594424]\n",
      "[ episode 164 ][ timestamp 34 ] state=[-0.00665696  0.19146541  0.07182159 -0.13594424], action=1, reward=1.0, next_state=[-0.00282765  0.38548906  0.06910271 -0.40513201]\n",
      "[ episode 164 ][ timestamp 35 ] state=[-0.00282765  0.38548906  0.06910271 -0.40513201], action=0, reward=1.0, next_state=[ 0.00488213  0.18945867  0.06100007 -0.09148817]\n",
      "[ episode 164 ][ timestamp 36 ] state=[ 0.00488213  0.18945867  0.06100007 -0.09148817], action=0, reward=1.0, next_state=[ 0.0086713  -0.00648216  0.0591703   0.21979932]\n",
      "[ episode 164 ][ timestamp 37 ] state=[ 0.0086713  -0.00648216  0.0591703   0.21979932], action=1, reward=1.0, next_state=[ 0.00854166  0.18774627  0.06356629 -0.05364754]\n",
      "[ episode 164 ][ timestamp 38 ] state=[ 0.00854166  0.18774627  0.06356629 -0.05364754], action=1, reward=1.0, next_state=[ 0.01229659  0.38190189  0.06249334 -0.32561687]\n",
      "[ episode 164 ][ timestamp 39 ] state=[ 0.01229659  0.38190189  0.06249334 -0.32561687], action=0, reward=1.0, next_state=[ 0.01993462  0.18594841  0.055981   -0.01389934]\n",
      "[ episode 164 ][ timestamp 40 ] state=[ 0.01993462  0.18594841  0.055981   -0.01389934], action=1, reward=1.0, next_state=[ 0.02365359  0.38022469  0.05570301 -0.28840744]\n",
      "[ episode 164 ][ timestamp 41 ] state=[ 0.02365359  0.38022469  0.05570301 -0.28840744], action=0, reward=1.0, next_state=[0.03125809 0.18435447 0.04993486 0.02131041]\n",
      "[ episode 164 ][ timestamp 42 ] state=[0.03125809 0.18435447 0.04993486 0.02131041], action=0, reward=1.0, next_state=[ 0.03494517 -0.01144671  0.05036107  0.32932084]\n",
      "[ episode 164 ][ timestamp 43 ] state=[ 0.03494517 -0.01144671  0.05036107  0.32932084], action=1, reward=1.0, next_state=[0.03471624 0.18292349 0.05694749 0.05293508]\n",
      "[ episode 164 ][ timestamp 44 ] state=[0.03471624 0.18292349 0.05694749 0.05293508], action=0, reward=1.0, next_state=[ 0.03837471 -0.01296681  0.05800619  0.36302773]\n",
      "[ episode 164 ][ timestamp 45 ] state=[ 0.03837471 -0.01296681  0.05800619  0.36302773], action=1, reward=1.0, next_state=[0.03811537 0.18128479 0.06526675 0.08918464]\n",
      "[ episode 164 ][ timestamp 46 ] state=[0.03811537 0.18128479 0.06526675 0.08918464], action=0, reward=1.0, next_state=[ 0.04174107 -0.01470902  0.06705044  0.40172421]\n",
      "[ episode 164 ][ timestamp 47 ] state=[ 0.04174107 -0.01470902  0.06705044  0.40172421], action=1, reward=1.0, next_state=[0.04144689 0.179401   0.07508492 0.13091151]\n",
      "[ episode 164 ][ timestamp 48 ] state=[0.04144689 0.179401   0.07508492 0.13091151], action=0, reward=1.0, next_state=[ 0.04503491 -0.01671173  0.07770315  0.446306  ]\n",
      "[ episode 164 ][ timestamp 49 ] state=[ 0.04503491 -0.01671173  0.07770315  0.446306  ], action=1, reward=1.0, next_state=[0.04470067 0.1772299  0.08662927 0.17909309]\n",
      "[ episode 164 ][ timestamp 50 ] state=[0.04470067 0.1772299  0.08662927 0.17909309], action=0, reward=1.0, next_state=[ 0.04824527 -0.01901803  0.09021113  0.49779827]\n",
      "[ episode 164 ][ timestamp 51 ] state=[ 0.04824527 -0.01901803  0.09021113  0.49779827], action=1, reward=1.0, next_state=[0.04786491 0.17472389 0.1001671  0.23485321]\n",
      "[ episode 164 ][ timestamp 52 ] state=[0.04786491 0.17472389 0.1001671  0.23485321], action=0, reward=1.0, next_state=[ 0.05135939 -0.02167592  0.10486416  0.55737615]\n",
      "[ episode 164 ][ timestamp 53 ] state=[ 0.05135939 -0.02167592  0.10486416  0.55737615], action=1, reward=1.0, next_state=[0.05092587 0.17182972 0.11601169 0.29948573]\n",
      "[ episode 164 ][ timestamp 54 ] state=[0.05092587 0.17182972 0.11601169 0.29948573], action=0, reward=1.0, next_state=[ 0.05436247 -0.02473828  0.1220014   0.62638677]\n",
      "[ episode 164 ][ timestamp 55 ] state=[ 0.05436247 -0.02473828  0.1220014   0.62638677], action=0, reward=1.0, next_state=[ 0.0538677  -0.22133297  0.13452914  0.95486638]\n",
      "[ episode 164 ][ timestamp 56 ] state=[ 0.0538677  -0.22133297  0.13452914  0.95486638], action=1, reward=1.0, next_state=[ 0.04944104 -0.02825201  0.15362646  0.70729416]\n",
      "[ episode 164 ][ timestamp 57 ] state=[ 0.04944104 -0.02825201  0.15362646  0.70729416], action=1, reward=1.0, next_state=[0.048876   0.16444574 0.16777235 0.46664047]\n",
      "[ episode 164 ][ timestamp 58 ] state=[0.048876   0.16444574 0.16777235 0.46664047], action=1, reward=1.0, next_state=[0.05216492 0.3568494  0.17710516 0.23118123]\n",
      "[ episode 164 ][ timestamp 59 ] state=[0.05216492 0.3568494  0.17710516 0.23118123], action=1, reward=1.0, next_state=[ 0.0593019   0.54905683  0.18172878 -0.00082296]\n",
      "[ episode 164 ][ timestamp 60 ] state=[ 0.0593019   0.54905683  0.18172878 -0.00082296], action=0, reward=1.0, next_state=[0.07028304 0.35185693 0.18171232 0.34324053]\n",
      "[ episode 164 ][ timestamp 61 ] state=[0.07028304 0.35185693 0.18171232 0.34324053], action=1, reward=1.0, next_state=[0.07732018 0.5439915  0.18857713 0.11291363]\n",
      "[ episode 164 ][ timestamp 62 ] state=[0.07732018 0.5439915  0.18857713 0.11291363], action=0, reward=1.0, next_state=[0.08820001 0.34673828 0.19083541 0.45866176]\n",
      "[ episode 164 ][ timestamp 63 ] state=[0.08820001 0.34673828 0.19083541 0.45866176], action=1, reward=1.0, next_state=[0.09513477 0.53872286 0.20000864 0.23167847]\n",
      "[ episode 164 ][ timestamp 64 ] state=[0.09513477 0.53872286 0.20000864 0.23167847], action=1, reward=1.0, next_state=[0.10590923 0.73050876 0.20464221 0.00814581]\n",
      "[ episode 164 ][ timestamp 65 ] state=[0.10590923 0.73050876 0.20464221 0.00814581], action=1, reward=1.0, next_state=[ 0.12051941  0.92219787  0.20480513 -0.21364236]\n",
      "[ episode 164 ][ timestamp 66 ] state=[ 0.12051941  0.92219787  0.20480513 -0.21364236], action=0, reward=1.0, next_state=[0.13896336 0.72482656 0.20053228 0.13601986]\n",
      "[ episode 164 ][ timestamp 67 ] state=[0.13896336 0.72482656 0.20053228 0.13601986], action=0, reward=1.0, next_state=[0.1534599  0.52748169 0.20325268 0.48466732]\n",
      "[ episode 164 ][ timestamp 68 ] state=[0.1534599  0.52748169 0.20325268 0.48466732], action=0, reward=-1.0, next_state=[0.16400953 0.33015898 0.21294602 0.83390431]\n",
      "[ Ended! ] Episode 164: Exploration_rate=0.4417353564707963. Score=68.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 165 ] state=[ 0.04364593 -0.0403509   0.04354138  0.00671852]\n",
      "[ episode 165 ][ timestamp 1 ] state=[ 0.04364593 -0.0403509   0.04354138  0.00671852], action=1, reward=1.0, next_state=[ 0.04283891  0.15412044  0.04367575 -0.27191489]\n",
      "[ episode 165 ][ timestamp 2 ] state=[ 0.04283891  0.15412044  0.04367575 -0.27191489], action=0, reward=1.0, next_state=[ 0.04592132 -0.04159664  0.03823745  0.03421735]\n",
      "[ episode 165 ][ timestamp 3 ] state=[ 0.04592132 -0.04159664  0.03823745  0.03421735], action=1, reward=1.0, next_state=[ 0.04508939  0.1529567   0.0389218  -0.24616028]\n",
      "[ episode 165 ][ timestamp 4 ] state=[ 0.04508939  0.1529567   0.0389218  -0.24616028], action=0, reward=1.0, next_state=[ 0.04814852 -0.0426989   0.03399859  0.05854098]\n",
      "[ episode 165 ][ timestamp 5 ] state=[ 0.04814852 -0.0426989   0.03399859  0.05854098], action=1, reward=1.0, next_state=[ 0.04729454  0.15191949  0.03516941 -0.22322425]\n",
      "[ episode 165 ][ timestamp 6 ] state=[ 0.04729454  0.15191949  0.03516941 -0.22322425], action=0, reward=1.0, next_state=[ 0.05033293 -0.04368702  0.03070493  0.08034176]\n",
      "[ episode 165 ][ timestamp 7 ] state=[ 0.05033293 -0.04368702  0.03070493  0.08034176], action=1, reward=1.0, next_state=[ 0.04945919  0.15098162  0.03231176 -0.20249774]\n",
      "[ episode 165 ][ timestamp 8 ] state=[ 0.04945919  0.15098162  0.03231176 -0.20249774], action=0, reward=1.0, next_state=[ 0.05247882 -0.04458718  0.02826181  0.10020034]\n",
      "[ episode 165 ][ timestamp 9 ] state=[ 0.05247882 -0.04458718  0.02826181  0.10020034], action=1, reward=1.0, next_state=[ 0.05158708  0.15011858  0.03026582 -0.18343379]\n",
      "[ episode 165 ][ timestamp 10 ] state=[ 0.05158708  0.15011858  0.03026582 -0.18343379], action=1, reward=1.0, next_state=[ 0.05458945  0.34479469  0.02659714 -0.46641743]\n",
      "[ episode 165 ][ timestamp 11 ] state=[ 0.05458945  0.34479469  0.02659714 -0.46641743], action=0, reward=1.0, next_state=[ 0.06148535  0.14930723  0.01726879 -0.16547132]\n",
      "[ episode 165 ][ timestamp 12 ] state=[ 0.06148535  0.14930723  0.01726879 -0.16547132], action=1, reward=1.0, next_state=[ 0.06447149  0.34417778  0.01395937 -0.45265679]\n",
      "[ episode 165 ][ timestamp 13 ] state=[ 0.06447149  0.34417778  0.01395937 -0.45265679], action=0, reward=1.0, next_state=[ 0.07135505  0.14886124  0.00490623 -0.15560659]\n",
      "[ episode 165 ][ timestamp 14 ] state=[ 0.07135505  0.14886124  0.00490623 -0.15560659], action=1, reward=1.0, next_state=[ 0.07433227  0.3439126   0.0017941  -0.44673769]\n",
      "[ episode 165 ][ timestamp 15 ] state=[ 0.07433227  0.3439126   0.0017941  -0.44673769], action=0, reward=1.0, next_state=[ 0.08121052  0.14876531 -0.00714066 -0.15348977]\n",
      "[ episode 165 ][ timestamp 16 ] state=[ 0.08121052  0.14876531 -0.00714066 -0.15348977], action=1, reward=1.0, next_state=[ 0.08418583  0.34398877 -0.01021045 -0.44841683]\n",
      "[ episode 165 ][ timestamp 17 ] state=[ 0.08418583  0.34398877 -0.01021045 -0.44841683], action=1, reward=1.0, next_state=[ 0.0910656   0.53925366 -0.01917879 -0.74430071]\n",
      "[ episode 165 ][ timestamp 18 ] state=[ 0.0910656   0.53925366 -0.01917879 -0.74430071], action=0, reward=1.0, next_state=[ 0.10185068  0.34440157 -0.0340648  -0.45771454]\n",
      "[ episode 165 ][ timestamp 19 ] state=[ 0.10185068  0.34440157 -0.0340648  -0.45771454], action=0, reward=1.0, next_state=[ 0.10873871  0.14977734 -0.04321909 -0.17596068]\n",
      "[ episode 165 ][ timestamp 20 ] state=[ 0.10873871  0.14977734 -0.04321909 -0.17596068], action=1, reward=1.0, next_state=[ 0.11173425  0.3454903  -0.04673831 -0.48195845]\n",
      "[ episode 165 ][ timestamp 21 ] state=[ 0.11173425  0.3454903  -0.04673831 -0.48195845], action=0, reward=1.0, next_state=[ 0.11864406  0.15105814 -0.05637748 -0.20436475]\n",
      "[ episode 165 ][ timestamp 22 ] state=[ 0.11864406  0.15105814 -0.05637748 -0.20436475], action=1, reward=1.0, next_state=[ 0.12166522  0.34693911 -0.06046477 -0.51428559]\n",
      "[ episode 165 ][ timestamp 23 ] state=[ 0.12166522  0.34693911 -0.06046477 -0.51428559], action=0, reward=1.0, next_state=[ 0.12860401  0.1527185  -0.07075048 -0.24125288]\n",
      "[ episode 165 ][ timestamp 24 ] state=[ 0.12860401  0.1527185  -0.07075048 -0.24125288], action=1, reward=1.0, next_state=[ 0.13165838  0.34877603 -0.07557554 -0.55538673]\n",
      "[ episode 165 ][ timestamp 25 ] state=[ 0.13165838  0.34877603 -0.07557554 -0.55538673], action=0, reward=1.0, next_state=[ 0.1386339   0.15479202 -0.08668327 -0.28743935]\n",
      "[ episode 165 ][ timestamp 26 ] state=[ 0.1386339   0.15479202 -0.08668327 -0.28743935], action=0, reward=1.0, next_state=[ 0.14172974 -0.03899369 -0.09243206 -0.02330517]\n",
      "[ episode 165 ][ timestamp 27 ] state=[ 0.14172974 -0.03899369 -0.09243206 -0.02330517], action=0, reward=1.0, next_state=[ 0.14094986 -0.23267688 -0.09289816  0.23884308]\n",
      "[ episode 165 ][ timestamp 28 ] state=[ 0.14094986 -0.23267688 -0.09289816  0.23884308], action=1, reward=1.0, next_state=[ 0.13629633 -0.03635906 -0.0881213  -0.08163668]\n",
      "[ episode 165 ][ timestamp 29 ] state=[ 0.13629633 -0.03635906 -0.0881213  -0.08163668], action=1, reward=1.0, next_state=[ 0.13556914  0.15990835 -0.08975404 -0.40076961]\n",
      "[ episode 165 ][ timestamp 30 ] state=[ 0.13556914  0.15990835 -0.08975404 -0.40076961], action=0, reward=1.0, next_state=[ 0.13876731 -0.03383345 -0.09776943 -0.13767895]\n",
      "[ episode 165 ][ timestamp 31 ] state=[ 0.13876731 -0.03383345 -0.09776943 -0.13767895], action=1, reward=1.0, next_state=[ 0.13809064  0.16254302 -0.10052301 -0.45953537]\n",
      "[ episode 165 ][ timestamp 32 ] state=[ 0.13809064  0.16254302 -0.10052301 -0.45953537], action=0, reward=1.0, next_state=[ 0.1413415  -0.03102502 -0.10971371 -0.20015308]\n",
      "[ episode 165 ][ timestamp 33 ] state=[ 0.1413415  -0.03102502 -0.10971371 -0.20015308], action=1, reward=1.0, next_state=[ 0.140721    0.16548113 -0.11371678 -0.52533122]\n",
      "[ episode 165 ][ timestamp 34 ] state=[ 0.140721    0.16548113 -0.11371678 -0.52533122], action=1, reward=1.0, next_state=[ 0.14403062  0.36200422 -0.1242234  -0.85157263]\n",
      "[ episode 165 ][ timestamp 35 ] state=[ 0.14403062  0.36200422 -0.1242234  -0.85157263], action=0, reward=1.0, next_state=[ 0.15127071  0.16877495 -0.14125485 -0.60039002]\n",
      "[ episode 165 ][ timestamp 36 ] state=[ 0.15127071  0.16877495 -0.14125485 -0.60039002], action=0, reward=1.0, next_state=[ 0.15464621 -0.02411781 -0.15326265 -0.35532364]\n",
      "[ episode 165 ][ timestamp 37 ] state=[ 0.15464621 -0.02411781 -0.15326265 -0.35532364], action=1, reward=1.0, next_state=[ 0.15416385  0.17281326 -0.16036913 -0.6921407 ]\n",
      "[ episode 165 ][ timestamp 38 ] state=[ 0.15416385  0.17281326 -0.16036913 -0.6921407 ], action=0, reward=1.0, next_state=[ 0.15762012 -0.01976332 -0.17421194 -0.4539291 ]\n",
      "[ episode 165 ][ timestamp 39 ] state=[ 0.15762012 -0.01976332 -0.17421194 -0.4539291 ], action=1, reward=1.0, next_state=[ 0.15722485  0.17733869 -0.18329052 -0.79606659]\n",
      "[ episode 165 ][ timestamp 40 ] state=[ 0.15722485  0.17733869 -0.18329052 -0.79606659], action=0, reward=1.0, next_state=[ 0.16077162 -0.01485858 -0.19921185 -0.56618604]\n",
      "[ episode 165 ][ timestamp 41 ] state=[ 0.16077162 -0.01485858 -0.19921185 -0.56618604], action=1, reward=-1.0, next_state=[ 0.16047445  0.18241816 -0.21053558 -0.91443046]\n",
      "[ Ended! ] Episode 165: Exploration_rate=0.43952667968844233. Score=41.\n",
      "[ Experience replay ] starts\n",
      "[ episode 166 ] state=[ 0.03171047  0.03692297  0.00213741 -0.02982244]\n",
      "[ episode 166 ][ timestamp 1 ] state=[ 0.03171047  0.03692297  0.00213741 -0.02982244], action=0, reward=1.0, next_state=[ 0.03244893 -0.15822956  0.00154096  0.2635341 ]\n",
      "[ episode 166 ][ timestamp 2 ] state=[ 0.03244893 -0.15822956  0.00154096  0.2635341 ], action=0, reward=1.0, next_state=[ 0.02928433 -0.35337348  0.00681164  0.55670266]\n",
      "[ episode 166 ][ timestamp 3 ] state=[ 0.02928433 -0.35337348  0.00681164  0.55670266], action=0, reward=1.0, next_state=[ 0.02221686 -0.54859039  0.0179457   0.85152385]\n",
      "[ episode 166 ][ timestamp 4 ] state=[ 0.02221686 -0.54859039  0.0179457   0.85152385], action=1, reward=1.0, next_state=[ 0.01124506 -0.35371765  0.03497618  0.56453756]\n",
      "[ episode 166 ][ timestamp 5 ] state=[ 0.01124506 -0.35371765  0.03497618  0.56453756], action=0, reward=1.0, next_state=[ 0.0041707  -0.54931243  0.04626693  0.86803118]\n",
      "[ episode 166 ][ timestamp 6 ] state=[ 0.0041707  -0.54931243  0.04626693  0.86803118], action=0, reward=1.0, next_state=[-0.00681554 -0.74503233  0.06362755  1.1748945 ]\n",
      "[ episode 166 ][ timestamp 7 ] state=[-0.00681554 -0.74503233  0.06362755  1.1748945 ], action=0, reward=1.0, next_state=[-0.02171619 -0.94092076  0.08712544  1.48682643]\n",
      "[ episode 166 ][ timestamp 8 ] state=[-0.02171619 -0.94092076  0.08712544  1.48682643], action=0, reward=1.0, next_state=[-0.04053461 -1.13698961  0.11686197  1.80539665]\n",
      "[ episode 166 ][ timestamp 9 ] state=[-0.04053461 -1.13698961  0.11686197  1.80539665], action=0, reward=1.0, next_state=[-0.0632744  -1.33320636  0.1529699   2.13199358]\n",
      "[ episode 166 ][ timestamp 10 ] state=[-0.0632744  -1.33320636  0.1529699   2.13199358], action=1, reward=1.0, next_state=[-0.08993853 -1.13989686  0.19560977  1.89021323]\n",
      "[ episode 166 ][ timestamp 11 ] state=[-0.08993853 -1.13989686  0.19560977  1.89021323], action=0, reward=-1.0, next_state=[-0.11273646 -1.33653207  0.23341404  2.23668433]\n",
      "[ Ended! ] Episode 166: Exploration_rate=0.43732904629000013. Score=11.\n",
      "[ Experience replay ] starts\n",
      "[ episode 167 ] state=[-0.04823959 -0.04532804 -0.01390444 -0.00476109]\n",
      "[ episode 167 ][ timestamp 1 ] state=[-0.04823959 -0.04532804 -0.01390444 -0.00476109], action=0, reward=1.0, next_state=[-0.04914615 -0.24024785 -0.01399966  0.28350259]\n",
      "[ episode 167 ][ timestamp 2 ] state=[-0.04914615 -0.24024785 -0.01399966  0.28350259], action=0, reward=1.0, next_state=[-0.0539511  -0.43516736 -0.00832961  0.57173743]\n",
      "[ episode 167 ][ timestamp 3 ] state=[-0.0539511  -0.43516736 -0.00832961  0.57173743], action=0, reward=1.0, next_state=[-0.06265445 -0.63017152  0.00310514  0.86178465]\n",
      "[ episode 167 ][ timestamp 4 ] state=[-0.06265445 -0.63017152  0.00310514  0.86178465], action=0, reward=1.0, next_state=[-0.07525788 -0.82533562  0.02034083  1.15544229]\n",
      "[ episode 167 ][ timestamp 5 ] state=[-0.07525788 -0.82533562  0.02034083  1.15544229], action=0, reward=1.0, next_state=[-0.09176459 -1.0207168   0.04344967  1.45443323]\n",
      "[ episode 167 ][ timestamp 6 ] state=[-0.09176459 -1.0207168   0.04344967  1.45443323], action=1, reward=1.0, next_state=[-0.11217893 -0.82615441  0.07253834  1.17563527]\n",
      "[ episode 167 ][ timestamp 7 ] state=[-0.11217893 -0.82615441  0.07253834  1.17563527], action=0, reward=1.0, next_state=[-0.12870202 -1.02213998  0.09605104  1.4901481 ]\n",
      "[ episode 167 ][ timestamp 8 ] state=[-0.12870202 -1.02213998  0.09605104  1.4901481 ], action=0, reward=1.0, next_state=[-0.14914482 -1.21829121  0.12585401  1.81121436]\n",
      "[ episode 167 ][ timestamp 9 ] state=[-0.14914482 -1.21829121  0.12585401  1.81121436], action=1, reward=1.0, next_state=[-0.17351064 -1.02477656  0.16207829  1.56014167]\n",
      "[ episode 167 ][ timestamp 10 ] state=[-0.17351064 -1.02477656  0.16207829  1.56014167], action=0, reward=1.0, next_state=[-0.19400617 -1.22142457  0.19328113  1.89869045]\n",
      "[ episode 167 ][ timestamp 11 ] state=[-0.19400617 -1.22142457  0.19328113  1.89869045], action=1, reward=-1.0, next_state=[-0.21843466 -1.02885034  0.23125494  1.67167942]\n",
      "[ Ended! ] Episode 167: Exploration_rate=0.4351424010585501. Score=11.\n",
      "[ Experience replay ] starts\n",
      "[ episode 168 ] state=[ 0.00313351 -0.00710415  0.03041037  0.03271747]\n",
      "[ episode 168 ][ timestamp 1 ] state=[ 0.00313351 -0.00710415  0.03041037  0.03271747], action=0, reward=1.0, next_state=[ 0.00299143 -0.2026487   0.03106472  0.33483795]\n",
      "[ episode 168 ][ timestamp 2 ] state=[ 0.00299143 -0.2026487   0.03106472  0.33483795], action=0, reward=1.0, next_state=[-0.00106154 -0.39819868  0.03776148  0.63715296]\n",
      "[ episode 168 ][ timestamp 3 ] state=[-0.00106154 -0.39819868  0.03776148  0.63715296], action=0, reward=1.0, next_state=[-0.00902552 -0.59382632  0.05050453  0.94148446]\n",
      "[ episode 168 ][ timestamp 4 ] state=[-0.00902552 -0.59382632  0.05050453  0.94148446], action=1, reward=1.0, next_state=[-0.02090204 -0.39942004  0.06933422  0.66508889]\n",
      "[ episode 168 ][ timestamp 5 ] state=[-0.02090204 -0.39942004  0.06933422  0.66508889], action=1, reward=1.0, next_state=[-0.02889044 -0.20532752  0.082636    0.39501756]\n",
      "[ episode 168 ][ timestamp 6 ] state=[-0.02889044 -0.20532752  0.082636    0.39501756], action=1, reward=1.0, next_state=[-0.03299699 -0.01146938  0.09053635  0.12948998]\n",
      "[ episode 168 ][ timestamp 7 ] state=[-0.03299699 -0.01146938  0.09053635  0.12948998], action=1, reward=1.0, next_state=[-0.03322638  0.18224684  0.09312615 -0.13331294]\n",
      "[ episode 168 ][ timestamp 8 ] state=[-0.03322638  0.18224684  0.09312615 -0.13331294], action=0, reward=1.0, next_state=[-0.02958145 -0.01407713  0.09045989  0.18723652]\n",
      "[ episode 168 ][ timestamp 9 ] state=[-0.02958145 -0.01407713  0.09045989  0.18723652], action=0, reward=1.0, next_state=[-0.02986299 -0.21036905  0.09420462  0.50702947]\n",
      "[ episode 168 ][ timestamp 10 ] state=[-0.02986299 -0.21036905  0.09420462  0.50702947], action=0, reward=1.0, next_state=[-0.03407037 -0.40668336  0.10434521  0.82785047]\n",
      "[ episode 168 ][ timestamp 11 ] state=[-0.03407037 -0.40668336  0.10434521  0.82785047], action=0, reward=1.0, next_state=[-0.04220404 -0.60306541  0.12090222  1.15144321]\n",
      "[ episode 168 ][ timestamp 12 ] state=[-0.04220404 -0.60306541  0.12090222  1.15144321], action=1, reward=1.0, next_state=[-0.05426534 -0.40971032  0.14393109  0.89898647]\n",
      "[ episode 168 ][ timestamp 13 ] state=[-0.05426534 -0.40971032  0.14393109  0.89898647], action=1, reward=1.0, next_state=[-0.06245955 -0.21680147  0.16191082  0.65478505]\n",
      "[ episode 168 ][ timestamp 14 ] state=[-0.06245955 -0.21680147  0.16191082  0.65478505], action=1, reward=1.0, next_state=[-0.06679558 -0.02425987  0.17500652  0.41714409]\n",
      "[ episode 168 ][ timestamp 15 ] state=[-0.06679558 -0.02425987  0.17500652  0.41714409], action=0, reward=1.0, next_state=[-0.06728078 -0.22137393  0.1833494   0.75948861]\n",
      "[ episode 168 ][ timestamp 16 ] state=[-0.06728078 -0.22137393  0.1833494   0.75948861], action=1, reward=1.0, next_state=[-0.07170826 -0.0291878   0.19853917  0.52964461]\n",
      "[ episode 168 ][ timestamp 17 ] state=[-0.07170826 -0.0291878   0.19853917  0.52964461], action=1, reward=1.0, next_state=[-0.07229201  0.1626689   0.20913206  0.30550068]\n",
      "[ episode 168 ][ timestamp 18 ] state=[-0.07229201  0.1626689   0.20913206  0.30550068], action=1, reward=-1.0, next_state=[-0.06903863  0.35429283  0.21524208  0.0853652 ]\n",
      "[ Ended! ] Episode 168: Exploration_rate=0.43296668905325736. Score=18.\n",
      "[ Experience replay ] starts\n",
      "[ episode 169 ] state=[ 0.01762523 -0.02873718  0.00261088  0.02861057]\n",
      "[ episode 169 ][ timestamp 1 ] state=[ 0.01762523 -0.02873718  0.00261088  0.02861057], action=0, reward=1.0, next_state=[ 0.01705048 -0.22389648  0.00318309  0.32211611]\n",
      "[ episode 169 ][ timestamp 2 ] state=[ 0.01705048 -0.22389648  0.00318309  0.32211611], action=1, reward=1.0, next_state=[ 0.01257255 -0.02882     0.00962541  0.03043871]\n",
      "[ episode 169 ][ timestamp 3 ] state=[ 0.01257255 -0.02882     0.00962541  0.03043871], action=1, reward=1.0, next_state=[ 0.01199615  0.1661626   0.01023419 -0.25919183]\n",
      "[ episode 169 ][ timestamp 4 ] state=[ 0.01199615  0.1661626   0.01023419 -0.25919183], action=1, reward=1.0, next_state=[ 0.0153194   0.36113697  0.00505035 -0.54862926]\n",
      "[ episode 169 ][ timestamp 5 ] state=[ 0.0153194   0.36113697  0.00505035 -0.54862926], action=0, reward=1.0, next_state=[ 0.02254214  0.16594444 -0.00592224 -0.2543594 ]\n",
      "[ episode 169 ][ timestamp 6 ] state=[ 0.02254214  0.16594444 -0.00592224 -0.2543594 ], action=1, reward=1.0, next_state=[ 0.02586103  0.36115045 -0.01100942 -0.5489044 ]\n",
      "[ episode 169 ][ timestamp 7 ] state=[ 0.02586103  0.36115045 -0.01100942 -0.5489044 ], action=0, reward=1.0, next_state=[ 0.03308404  0.16618487 -0.02198751 -0.25971046]\n",
      "[ episode 169 ][ timestamp 8 ] state=[ 0.03308404  0.16618487 -0.02198751 -0.25971046], action=0, reward=1.0, next_state=[ 0.03640774 -0.02861641 -0.02718172  0.02595702]\n",
      "[ episode 169 ][ timestamp 9 ] state=[ 0.03640774 -0.02861641 -0.02718172  0.02595702], action=1, reward=1.0, next_state=[ 0.03583541  0.16688459 -0.02666258 -0.2751766 ]\n",
      "[ episode 169 ][ timestamp 10 ] state=[ 0.03583541  0.16688459 -0.02666258 -0.2751766 ], action=0, reward=1.0, next_state=[ 0.0391731  -0.027847   -0.03216611  0.00897911]\n",
      "[ episode 169 ][ timestamp 11 ] state=[ 0.0391731  -0.027847   -0.03216611  0.00897911], action=0, reward=1.0, next_state=[ 0.03861616 -0.22249324 -0.03198653  0.29134222]\n",
      "[ episode 169 ][ timestamp 12 ] state=[ 0.03861616 -0.22249324 -0.03198653  0.29134222], action=1, reward=1.0, next_state=[ 0.0341663  -0.02693015 -0.02615969 -0.0112548 ]\n",
      "[ episode 169 ][ timestamp 13 ] state=[ 0.0341663  -0.02693015 -0.02615969 -0.0112548 ], action=1, reward=1.0, next_state=[ 0.03362769  0.16855701 -0.02638478 -0.31207528]\n",
      "[ episode 169 ][ timestamp 14 ] state=[ 0.03362769  0.16855701 -0.02638478 -0.31207528], action=0, reward=1.0, next_state=[ 0.03699884 -0.02617931 -0.03262629 -0.0278287 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 169 ][ timestamp 15 ] state=[ 0.03699884 -0.02617931 -0.03262629 -0.0278287 ], action=1, reward=1.0, next_state=[ 0.03647525  0.16939497 -0.03318286 -0.33062441]\n",
      "[ episode 169 ][ timestamp 16 ] state=[ 0.03647525  0.16939497 -0.03318286 -0.33062441], action=1, reward=1.0, next_state=[ 0.03986315  0.36497317 -0.03979535 -0.63358419]\n",
      "[ episode 169 ][ timestamp 17 ] state=[ 0.03986315  0.36497317 -0.03979535 -0.63358419], action=0, reward=1.0, next_state=[ 0.04716261  0.17042829 -0.05246703 -0.35369466]\n",
      "[ episode 169 ][ timestamp 18 ] state=[ 0.04716261  0.17042829 -0.05246703 -0.35369466], action=0, reward=1.0, next_state=[ 0.05057118 -0.02390988 -0.05954093 -0.07800677]\n",
      "[ episode 169 ][ timestamp 19 ] state=[ 0.05057118 -0.02390988 -0.05954093 -0.07800677], action=1, reward=1.0, next_state=[ 0.05009298  0.17201284 -0.06110106 -0.38886477]\n",
      "[ episode 169 ][ timestamp 20 ] state=[ 0.05009298  0.17201284 -0.06110106 -0.38886477], action=0, reward=1.0, next_state=[ 0.05353324 -0.02219103 -0.06887836 -0.1160551 ]\n",
      "[ episode 169 ][ timestamp 21 ] state=[ 0.05353324 -0.02219103 -0.06887836 -0.1160551 ], action=1, reward=1.0, next_state=[ 0.05308942  0.17384676 -0.07119946 -0.42964876]\n",
      "[ episode 169 ][ timestamp 22 ] state=[ 0.05308942  0.17384676 -0.07119946 -0.42964876], action=1, reward=1.0, next_state=[ 0.05656635  0.36990096 -0.07979243 -0.74389993]\n",
      "[ episode 169 ][ timestamp 23 ] state=[ 0.05656635  0.36990096 -0.07979243 -0.74389993], action=0, reward=1.0, next_state=[ 0.06396437  0.17596562 -0.09467043 -0.47735659]\n",
      "[ episode 169 ][ timestamp 24 ] state=[ 0.06396437  0.17596562 -0.09467043 -0.47735659], action=1, reward=1.0, next_state=[ 0.06748368  0.37228781 -0.10421756 -0.79831275]\n",
      "[ episode 169 ][ timestamp 25 ] state=[ 0.06748368  0.37228781 -0.10421756 -0.79831275], action=0, reward=1.0, next_state=[ 0.07492944  0.17873828 -0.12018382 -0.5401482 ]\n",
      "[ episode 169 ][ timestamp 26 ] state=[ 0.07492944  0.17873828 -0.12018382 -0.5401482 ], action=0, reward=1.0, next_state=[ 0.0785042  -0.01450748 -0.13098678 -0.28761954]\n",
      "[ episode 169 ][ timestamp 27 ] state=[ 0.0785042  -0.01450748 -0.13098678 -0.28761954], action=0, reward=1.0, next_state=[ 0.07821406 -0.20754189 -0.13673917 -0.03894847]\n",
      "[ episode 169 ][ timestamp 28 ] state=[ 0.07821406 -0.20754189 -0.13673917 -0.03894847], action=1, reward=1.0, next_state=[ 0.07406322 -0.01075101 -0.13751814 -0.3714556 ]\n",
      "[ episode 169 ][ timestamp 29 ] state=[ 0.07406322 -0.01075101 -0.13751814 -0.3714556 ], action=0, reward=1.0, next_state=[ 0.0738482  -0.20367869 -0.14494726 -0.12509917]\n",
      "[ episode 169 ][ timestamp 30 ] state=[ 0.0738482  -0.20367869 -0.14494726 -0.12509917], action=1, reward=1.0, next_state=[ 0.06977462 -0.00680972 -0.14744924 -0.45977137]\n",
      "[ episode 169 ][ timestamp 31 ] state=[ 0.06977462 -0.00680972 -0.14744924 -0.45977137], action=0, reward=1.0, next_state=[ 0.06963843 -0.19957341 -0.15664467 -0.2169565 ]\n",
      "[ episode 169 ][ timestamp 32 ] state=[ 0.06963843 -0.19957341 -0.15664467 -0.2169565 ], action=1, reward=1.0, next_state=[ 0.06564696 -0.00259945 -0.1609838  -0.55466533]\n",
      "[ episode 169 ][ timestamp 33 ] state=[ 0.06564696 -0.00259945 -0.1609838  -0.55466533], action=0, reward=1.0, next_state=[ 0.06559497 -0.19513848 -0.1720771  -0.31671612]\n",
      "[ episode 169 ][ timestamp 34 ] state=[ 0.06559497 -0.19513848 -0.1720771  -0.31671612], action=1, reward=1.0, next_state=[ 0.0616922   0.00196327 -0.17841143 -0.6583437 ]\n",
      "[ episode 169 ][ timestamp 35 ] state=[ 0.0616922   0.00196327 -0.17841143 -0.6583437 ], action=0, reward=1.0, next_state=[ 0.06173147 -0.19028596 -0.1915783  -0.42672239]\n",
      "[ episode 169 ][ timestamp 36 ] state=[ 0.06173147 -0.19028596 -0.1915783  -0.42672239], action=0, reward=1.0, next_state=[ 0.05792575 -0.3822517  -0.20011275 -0.20002192]\n",
      "[ episode 169 ][ timestamp 37 ] state=[ 0.05792575 -0.3822517  -0.20011275 -0.20002192], action=1, reward=1.0, next_state=[ 0.05028071 -0.18491415 -0.20411319 -0.54856246]\n",
      "[ episode 169 ][ timestamp 38 ] state=[ 0.05028071 -0.18491415 -0.20411319 -0.54856246], action=0, reward=-1.0, next_state=[ 0.04658243 -0.37667247 -0.21508444 -0.32648948]\n",
      "[ Ended! ] Episode 169: Exploration_rate=0.43080185560799106. Score=38.\n",
      "[ Experience replay ] starts\n",
      "[ episode 170 ] state=[-0.03487704 -0.03487494  0.01501005  0.00505931]\n",
      "[ episode 170 ][ timestamp 1 ] state=[-0.03487704 -0.03487494  0.01501005  0.00505931], action=1, reward=1.0, next_state=[-0.03557454  0.16002857  0.01511123 -0.28285023]\n",
      "[ episode 170 ][ timestamp 2 ] state=[-0.03557454  0.16002857  0.01511123 -0.28285023], action=0, reward=1.0, next_state=[-0.03237397 -0.03530562  0.00945423  0.01456014]\n",
      "[ episode 170 ][ timestamp 3 ] state=[-0.03237397 -0.03530562  0.00945423  0.01456014], action=1, reward=1.0, next_state=[-0.03308008  0.15967948  0.00974543 -0.27512494]\n",
      "[ episode 170 ][ timestamp 4 ] state=[-0.03308008  0.15967948  0.00974543 -0.27512494], action=1, reward=1.0, next_state=[-0.02988649  0.35466104  0.00424293 -0.56471828]\n",
      "[ episode 170 ][ timestamp 5 ] state=[-0.02988649  0.35466104  0.00424293 -0.56471828], action=0, reward=1.0, next_state=[-0.02279327  0.15947981 -0.00705143 -0.27070166]\n",
      "[ episode 170 ][ timestamp 6 ] state=[-0.02279327  0.15947981 -0.00705143 -0.27070166], action=1, reward=1.0, next_state=[-0.01960367  0.35470167 -0.01246547 -0.56560028]\n",
      "[ episode 170 ][ timestamp 7 ] state=[-0.01960367  0.35470167 -0.01246547 -0.56560028], action=0, reward=1.0, next_state=[-0.01250964  0.1597568  -0.02377747 -0.27687044]\n",
      "[ episode 170 ][ timestamp 8 ] state=[-0.01250964  0.1597568  -0.02377747 -0.27687044], action=0, reward=1.0, next_state=[-0.0093145  -0.03501799 -0.02931488  0.00821925]\n",
      "[ episode 170 ][ timestamp 9 ] state=[-0.0093145  -0.03501799 -0.02931488  0.00821925], action=1, reward=1.0, next_state=[-0.01001486  0.16051184 -0.0291505  -0.29356683]\n",
      "[ episode 170 ][ timestamp 10 ] state=[-0.01001486  0.16051184 -0.0291505  -0.29356683], action=0, reward=1.0, next_state=[-0.00680463 -0.03418263 -0.03502183 -0.01021822]\n",
      "[ episode 170 ][ timestamp 11 ] state=[-0.00680463 -0.03418263 -0.03502183 -0.01021822], action=1, reward=1.0, next_state=[-0.00748828  0.16142362 -0.0352262  -0.313742  ]\n",
      "[ episode 170 ][ timestamp 12 ] state=[-0.00748828  0.16142362 -0.0352262  -0.313742  ], action=0, reward=1.0, next_state=[-0.00425981 -0.03317927 -0.04150104 -0.03237311]\n",
      "[ episode 170 ][ timestamp 13 ] state=[-0.00425981 -0.03317927 -0.04150104 -0.03237311], action=0, reward=1.0, next_state=[-0.00492339 -0.22768227 -0.0421485   0.24693236]\n",
      "[ episode 170 ][ timestamp 14 ] state=[-0.00492339 -0.22768227 -0.0421485   0.24693236], action=1, reward=1.0, next_state=[-0.00947704 -0.03198449 -0.03720985 -0.05874159]\n",
      "[ episode 170 ][ timestamp 15 ] state=[-0.00947704 -0.03198449 -0.03720985 -0.05874159], action=0, reward=1.0, next_state=[-0.01011673 -0.22655371 -0.03838469  0.22197305]\n",
      "[ episode 170 ][ timestamp 16 ] state=[-0.01011673 -0.22655371 -0.03838469  0.22197305], action=1, reward=1.0, next_state=[-0.0146478  -0.03090474 -0.03394522 -0.08256657]\n",
      "[ episode 170 ][ timestamp 17 ] state=[-0.0146478  -0.03090474 -0.03394522 -0.08256657], action=0, reward=1.0, next_state=[-0.0152659  -0.22552406 -0.03559656  0.19921626]\n",
      "[ episode 170 ][ timestamp 18 ] state=[-0.0152659  -0.22552406 -0.03559656  0.19921626], action=1, reward=1.0, next_state=[-0.01977638 -0.02991154 -0.03161223 -0.10447982]\n",
      "[ episode 170 ][ timestamp 19 ] state=[-0.01977638 -0.02991154 -0.03161223 -0.10447982], action=1, reward=1.0, next_state=[-0.02037461  0.16564884 -0.03370183 -0.40696628]\n",
      "[ episode 170 ][ timestamp 20 ] state=[-0.02037461  0.16564884 -0.03370183 -0.40696628], action=0, reward=1.0, next_state=[-0.01706163 -0.02897942 -0.04184115 -0.12509613]\n",
      "[ episode 170 ][ timestamp 21 ] state=[-0.01706163 -0.02897942 -0.04184115 -0.12509613], action=0, reward=1.0, next_state=[-0.01764122 -0.22347774 -0.04434307  0.15409831]\n",
      "[ episode 170 ][ timestamp 22 ] state=[-0.01764122 -0.22347774 -0.04434307  0.15409831], action=1, reward=1.0, next_state=[-0.02211078 -0.02774985 -0.04126111 -0.15223753]\n",
      "[ episode 170 ][ timestamp 23 ] state=[-0.02211078 -0.02774985 -0.04126111 -0.15223753], action=0, reward=1.0, next_state=[-0.02266577 -0.22225744 -0.04430586  0.12714822]\n",
      "[ episode 170 ][ timestamp 24 ] state=[-0.02266577 -0.22225744 -0.04430586  0.12714822], action=1, reward=1.0, next_state=[-0.02711092 -0.02652971 -0.04176289 -0.17917694]\n",
      "[ episode 170 ][ timestamp 25 ] state=[-0.02711092 -0.02652971 -0.04176289 -0.17917694], action=1, reward=1.0, next_state=[-0.02764152  0.16916422 -0.04534643 -0.4847366 ]\n",
      "[ episode 170 ][ timestamp 26 ] state=[-0.02764152  0.16916422 -0.04534643 -0.4847366 ], action=0, reward=1.0, next_state=[-0.02425823 -0.02528944 -0.05504117 -0.20668323]\n",
      "[ episode 170 ][ timestamp 27 ] state=[-0.02425823 -0.02528944 -0.05504117 -0.20668323], action=0, reward=1.0, next_state=[-0.02476402 -0.21958288 -0.05917483  0.06814164]\n",
      "[ episode 170 ][ timestamp 28 ] state=[-0.02476402 -0.21958288 -0.05917483  0.06814164], action=1, reward=1.0, next_state=[-0.02915568 -0.02366466 -0.057812   -0.24260855]\n",
      "[ episode 170 ][ timestamp 29 ] state=[-0.02915568 -0.02366466 -0.057812   -0.24260855], action=1, reward=1.0, next_state=[-0.02962897  0.17223338 -0.06266417 -0.55295196]\n",
      "[ episode 170 ][ timestamp 30 ] state=[-0.02962897  0.17223338 -0.06266417 -0.55295196], action=1, reward=1.0, next_state=[-0.0261843   0.36817677 -0.07372321 -0.86470137]\n",
      "[ episode 170 ][ timestamp 31 ] state=[-0.0261843   0.36817677 -0.07372321 -0.86470137], action=0, reward=1.0, next_state=[-0.01882077  0.17413159 -0.09101724 -0.59607923]\n",
      "[ episode 170 ][ timestamp 32 ] state=[-0.01882077  0.17413159 -0.09101724 -0.59607923], action=0, reward=1.0, next_state=[-0.01533814 -0.01960663 -0.10293882 -0.33339692]\n",
      "[ episode 170 ][ timestamp 33 ] state=[-0.01533814 -0.01960663 -0.10293882 -0.33339692], action=0, reward=1.0, next_state=[-0.01573027 -0.21312433 -0.10960676 -0.07486756]\n",
      "[ episode 170 ][ timestamp 34 ] state=[-0.01573027 -0.21312433 -0.10960676 -0.07486756], action=0, reward=1.0, next_state=[-0.01999276 -0.40651819 -0.11110411  0.18132256]\n",
      "[ episode 170 ][ timestamp 35 ] state=[-0.01999276 -0.40651819 -0.11110411  0.18132256], action=1, reward=1.0, next_state=[-0.02812312 -0.20999626 -0.10747766 -0.14424024]\n",
      "[ episode 170 ][ timestamp 36 ] state=[-0.02812312 -0.20999626 -0.10747766 -0.14424024], action=1, reward=1.0, next_state=[-0.03232304 -0.01351237 -0.11036246 -0.46880308]\n",
      "[ episode 170 ][ timestamp 37 ] state=[-0.03232304 -0.01351237 -0.11036246 -0.46880308], action=1, reward=1.0, next_state=[-0.03259329  0.18298149 -0.11973852 -0.79413149]\n",
      "[ episode 170 ][ timestamp 38 ] state=[-0.03259329  0.18298149 -0.11973852 -0.79413149], action=1, reward=1.0, next_state=[-0.02893366  0.3795256  -0.13562115 -1.12195581]\n",
      "[ episode 170 ][ timestamp 39 ] state=[-0.02893366  0.3795256  -0.13562115 -1.12195581], action=1, reward=1.0, next_state=[-0.02134315  0.57613976 -0.15806027 -1.45391945]\n",
      "[ episode 170 ][ timestamp 40 ] state=[-0.02134315  0.57613976 -0.15806027 -1.45391945], action=1, reward=1.0, next_state=[-0.00982035  0.77280981 -0.18713866 -1.79152359]\n",
      "[ episode 170 ][ timestamp 41 ] state=[-0.00982035  0.77280981 -0.18713866 -1.79152359], action=0, reward=-1.0, next_state=[ 0.00563584  0.5802147  -0.22296913 -1.56237301]\n",
      "[ Ended! ] Episode 170: Exploration_rate=0.4286478463299511. Score=41.\n",
      "[ Experience replay ] starts\n",
      "[ episode 171 ] state=[0.00828687 0.0323487  0.00601726 0.03939064]\n",
      "[ episode 171 ][ timestamp 1 ] state=[0.00828687 0.0323487  0.00601726 0.03939064], action=1, reward=1.0, next_state=[ 0.00893385  0.22738385  0.00680507 -0.25138772]\n",
      "[ episode 171 ][ timestamp 2 ] state=[ 0.00893385  0.22738385  0.00680507 -0.25138772], action=0, reward=1.0, next_state=[0.01348152 0.03216539 0.00177731 0.04343387]\n",
      "[ episode 171 ][ timestamp 3 ] state=[0.01348152 0.03216539 0.00177731 0.04343387], action=1, reward=1.0, next_state=[ 0.01412483  0.22726181  0.00264599 -0.24868777]\n",
      "[ episode 171 ][ timestamp 4 ] state=[ 0.01412483  0.22726181  0.00264599 -0.24868777], action=0, reward=1.0, next_state=[ 0.01867007  0.03210217 -0.00232776  0.04482858]\n",
      "[ episode 171 ][ timestamp 5 ] state=[ 0.01867007  0.03210217 -0.00232776  0.04482858], action=0, reward=1.0, next_state=[ 0.01931211 -0.16298632 -0.00143119  0.33677617]\n",
      "[ episode 171 ][ timestamp 6 ] state=[ 0.01931211 -0.16298632 -0.00143119  0.33677617], action=1, reward=1.0, next_state=[0.01605239 0.03215597 0.00530433 0.04364227]\n",
      "[ episode 171 ][ timestamp 7 ] state=[0.01605239 0.03215597 0.00530433 0.04364227], action=1, reward=1.0, next_state=[ 0.0166955   0.22720146  0.00617718 -0.24736239]\n",
      "[ episode 171 ][ timestamp 8 ] state=[ 0.0166955   0.22720146  0.00617718 -0.24736239], action=0, reward=1.0, next_state=[0.02123953 0.03199183 0.00122993 0.04726254]\n",
      "[ episode 171 ][ timestamp 9 ] state=[0.02123953 0.03199183 0.00122993 0.04726254], action=1, reward=1.0, next_state=[ 0.02187937  0.22709612  0.00217518 -0.24503208]\n",
      "[ episode 171 ][ timestamp 10 ] state=[ 0.02187937  0.22709612  0.00217518 -0.24503208], action=0, reward=1.0, next_state=[ 0.02642129  0.03194317 -0.00272546  0.04833616]\n",
      "[ episode 171 ][ timestamp 11 ] state=[ 0.02642129  0.03194317 -0.00272546  0.04833616], action=1, reward=1.0, next_state=[ 0.02706016  0.2271041  -0.00175874 -0.24520543]\n",
      "[ episode 171 ][ timestamp 12 ] state=[ 0.02706016  0.2271041  -0.00175874 -0.24520543], action=0, reward=1.0, next_state=[ 0.03160224  0.03200731 -0.00666285  0.04692223]\n",
      "[ episode 171 ][ timestamp 13 ] state=[ 0.03160224  0.03200731 -0.00666285  0.04692223], action=1, reward=1.0, next_state=[ 0.03224238  0.22722417 -0.0057244  -0.24785541]\n",
      "[ episode 171 ][ timestamp 14 ] state=[ 0.03224238  0.22722417 -0.0057244  -0.24785541], action=0, reward=1.0, next_state=[ 0.03678687  0.03218443 -0.01068151  0.04301643]\n",
      "[ episode 171 ][ timestamp 15 ] state=[ 0.03678687  0.03218443 -0.01068151  0.04301643], action=1, reward=1.0, next_state=[ 0.03743056  0.22745791 -0.00982118 -0.25301738]\n",
      "[ episode 171 ][ timestamp 16 ] state=[ 0.03743056  0.22745791 -0.00982118 -0.25301738], action=0, reward=1.0, next_state=[ 0.04197971  0.03247757 -0.01488153  0.03655165]\n",
      "[ episode 171 ][ timestamp 17 ] state=[ 0.04197971  0.03247757 -0.01488153  0.03655165], action=0, reward=1.0, next_state=[ 0.04262927 -0.16242786 -0.0141505   0.3245024 ]\n",
      "[ episode 171 ][ timestamp 18 ] state=[ 0.04262927 -0.16242786 -0.0141505   0.3245024 ], action=0, reward=1.0, next_state=[ 0.03938071 -0.35734549 -0.00766045  0.61268948]\n",
      "[ episode 171 ][ timestamp 19 ] state=[ 0.03938071 -0.35734549 -0.00766045  0.61268948], action=0, reward=1.0, next_state=[ 0.0322338  -0.55235955  0.00459334  0.90294984]\n",
      "[ episode 171 ][ timestamp 20 ] state=[ 0.0322338  -0.55235955  0.00459334  0.90294984], action=0, reward=1.0, next_state=[ 0.02118661 -0.74754343  0.02265234  1.19707299]\n",
      "[ episode 171 ][ timestamp 21 ] state=[ 0.02118661 -0.74754343  0.02265234  1.19707299], action=1, reward=1.0, next_state=[ 0.00623574 -0.55272188  0.0465938   0.91157486]\n",
      "[ episode 171 ][ timestamp 22 ] state=[ 0.00623574 -0.55272188  0.0465938   0.91157486], action=1, reward=1.0, next_state=[-0.0048187  -0.35826029  0.06482529  0.63389268]\n",
      "[ episode 171 ][ timestamp 23 ] state=[-0.0048187  -0.35826029  0.06482529  0.63389268], action=1, reward=1.0, next_state=[-0.0119839  -0.16409965  0.07750315  0.36230874]\n",
      "[ episode 171 ][ timestamp 24 ] state=[-0.0119839  -0.16409965  0.07750315  0.36230874], action=0, reward=1.0, next_state=[-0.0152659  -0.36023272  0.08474932  0.67838831]\n",
      "[ episode 171 ][ timestamp 25 ] state=[-0.0152659  -0.36023272  0.08474932  0.67838831], action=1, reward=1.0, next_state=[-0.02247055 -0.16638401  0.09831709  0.41354534]\n",
      "[ episode 171 ][ timestamp 26 ] state=[-0.02247055 -0.16638401  0.09831709  0.41354534], action=1, reward=1.0, next_state=[-0.02579823  0.02721688  0.106588    0.15340509]\n",
      "[ episode 171 ][ timestamp 27 ] state=[-0.02579823  0.02721688  0.106588    0.15340509], action=0, reward=1.0, next_state=[-0.02525389 -0.16925699  0.1096561   0.47772095]\n",
      "[ episode 171 ][ timestamp 28 ] state=[-0.02525389 -0.16925699  0.1096561   0.47772095], action=1, reward=1.0, next_state=[-0.02863903  0.02415978  0.11921052  0.22151267]\n",
      "[ episode 171 ][ timestamp 29 ] state=[-0.02863903  0.02415978  0.11921052  0.22151267], action=0, reward=1.0, next_state=[-0.02815584 -0.17244651  0.12364077  0.54929402]\n",
      "[ episode 171 ][ timestamp 30 ] state=[-0.02815584 -0.17244651  0.12364077  0.54929402], action=1, reward=1.0, next_state=[-0.03160477  0.02074158  0.13462665  0.29798187]\n",
      "[ episode 171 ][ timestamp 31 ] state=[-0.03160477  0.02074158  0.13462665  0.29798187], action=1, reward=1.0, next_state=[-0.03118994  0.21371329  0.14058629  0.05060424]\n",
      "[ episode 171 ][ timestamp 32 ] state=[-0.03118994  0.21371329  0.14058629  0.05060424], action=1, reward=1.0, next_state=[-0.02691567  0.40656877  0.14159837 -0.19462856]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 171 ][ timestamp 33 ] state=[-0.02691567  0.40656877  0.14159837 -0.19462856], action=1, reward=1.0, next_state=[-0.0187843   0.59941121  0.1377058  -0.43950624]\n",
      "[ episode 171 ][ timestamp 34 ] state=[-0.0187843   0.59941121  0.1377058  -0.43950624], action=0, reward=1.0, next_state=[-0.00679607  0.40263641  0.12891568 -0.10678051]\n",
      "[ episode 171 ][ timestamp 35 ] state=[-0.00679607  0.40263641  0.12891568 -0.10678051], action=0, reward=1.0, next_state=[0.00125666 0.20592535 0.12678007 0.2236339 ]\n",
      "[ episode 171 ][ timestamp 36 ] state=[0.00125666 0.20592535 0.12678007 0.2236339 ], action=0, reward=1.0, next_state=[0.00537516 0.00924086 0.13125274 0.55346637]\n",
      "[ episode 171 ][ timestamp 37 ] state=[0.00537516 0.00924086 0.13125274 0.55346637], action=1, reward=1.0, next_state=[0.00555998 0.20229915 0.14232207 0.30484736]\n",
      "[ episode 171 ][ timestamp 38 ] state=[0.00555998 0.20229915 0.14232207 0.30484736], action=1, reward=1.0, next_state=[0.00960596 0.39513642 0.14841902 0.06021761]\n",
      "[ episode 171 ][ timestamp 39 ] state=[0.00960596 0.39513642 0.14841902 0.06021761], action=1, reward=1.0, next_state=[ 0.01750869  0.58785312  0.14962337 -0.18220421]\n",
      "[ episode 171 ][ timestamp 40 ] state=[ 0.01750869  0.58785312  0.14962337 -0.18220421], action=1, reward=1.0, next_state=[ 0.02926575  0.78055265  0.14597929 -0.42419873]\n",
      "[ episode 171 ][ timestamp 41 ] state=[ 0.02926575  0.78055265  0.14597929 -0.42419873], action=0, reward=1.0, next_state=[ 0.04487681  0.58369707  0.13749531 -0.08929037]\n",
      "[ episode 171 ][ timestamp 42 ] state=[ 0.04487681  0.58369707  0.13749531 -0.08929037], action=1, reward=1.0, next_state=[ 0.05655075  0.77660779  0.13570951 -0.33562915]\n",
      "[ episode 171 ][ timestamp 43 ] state=[ 0.05655075  0.77660779  0.13570951 -0.33562915], action=0, reward=1.0, next_state=[ 0.07208291  0.57984178  0.12899692 -0.00341762]\n",
      "[ episode 171 ][ timestamp 44 ] state=[ 0.07208291  0.57984178  0.12899692 -0.00341762], action=1, reward=1.0, next_state=[ 0.08367974  0.77290042  0.12892857 -0.25277951]\n",
      "[ episode 171 ][ timestamp 45 ] state=[ 0.08367974  0.77290042  0.12892857 -0.25277951], action=1, reward=1.0, next_state=[ 0.09913775  0.96596826  0.12387298 -0.50217756]\n",
      "[ episode 171 ][ timestamp 46 ] state=[ 0.09913775  0.96596826  0.12387298 -0.50217756], action=0, reward=1.0, next_state=[ 0.11845711  0.76933795  0.11382943 -0.17316651]\n",
      "[ episode 171 ][ timestamp 47 ] state=[ 0.11845711  0.76933795  0.11382943 -0.17316651], action=0, reward=1.0, next_state=[0.13384387 0.57278645 0.1103661  0.15314637]\n",
      "[ episode 171 ][ timestamp 48 ] state=[0.13384387 0.57278645 0.1103661  0.15314637], action=1, reward=1.0, next_state=[ 0.1452996   0.76616927  0.11342903 -0.10278121]\n",
      "[ episode 171 ][ timestamp 49 ] state=[ 0.1452996   0.76616927  0.11342903 -0.10278121], action=0, reward=1.0, next_state=[0.16062299 0.56961991 0.1113734  0.22342491]\n",
      "[ episode 171 ][ timestamp 50 ] state=[0.16062299 0.56961991 0.1113734  0.22342491], action=1, reward=1.0, next_state=[ 0.17201539  0.76298841  0.1158419  -0.03215466]\n",
      "[ episode 171 ][ timestamp 51 ] state=[ 0.17201539  0.76298841  0.1158419  -0.03215466], action=1, reward=1.0, next_state=[ 0.18727515  0.95627502  0.11519881 -0.28616001]\n",
      "[ episode 171 ][ timestamp 52 ] state=[ 0.18727515  0.95627502  0.11519881 -0.28616001], action=1, reward=1.0, next_state=[ 0.20640065  1.14958173  0.10947561 -0.54040463]\n",
      "[ episode 171 ][ timestamp 53 ] state=[ 0.20640065  1.14958173  0.10947561 -0.54040463], action=0, reward=1.0, next_state=[ 0.22939229  0.953105    0.09866751 -0.21533225]\n",
      "[ episode 171 ][ timestamp 54 ] state=[ 0.22939229  0.953105    0.09866751 -0.21533225], action=0, reward=1.0, next_state=[0.24845439 0.75672107 0.09436087 0.10677212]\n",
      "[ episode 171 ][ timestamp 55 ] state=[0.24845439 0.75672107 0.09436087 0.10677212], action=1, reward=1.0, next_state=[ 0.26358881  0.95037302  0.09649631 -0.15471262]\n",
      "[ episode 171 ][ timestamp 56 ] state=[ 0.26358881  0.95037302  0.09649631 -0.15471262], action=0, reward=1.0, next_state=[0.28259627 0.75401137 0.09340206 0.16678551]\n",
      "[ episode 171 ][ timestamp 57 ] state=[0.28259627 0.75401137 0.09340206 0.16678551], action=0, reward=1.0, next_state=[0.2976765  0.55768516 0.09673777 0.48741149]\n",
      "[ episode 171 ][ timestamp 58 ] state=[0.2976765  0.55768516 0.09673777 0.48741149], action=0, reward=1.0, next_state=[0.3088302  0.36134091 0.106486   0.80894745]\n",
      "[ episode 171 ][ timestamp 59 ] state=[0.3088302  0.36134091 0.106486   0.80894745], action=1, reward=1.0, next_state=[0.31605702 0.55485511 0.12266495 0.55156807]\n",
      "[ episode 171 ][ timestamp 60 ] state=[0.31605702 0.55485511 0.12266495 0.55156807], action=0, reward=1.0, next_state=[0.32715412 0.35824326 0.13369631 0.88024299]\n",
      "[ episode 171 ][ timestamp 61 ] state=[0.32715412 0.35824326 0.13369631 0.88024299], action=0, reward=1.0, next_state=[0.33431899 0.16158312 0.15130117 1.21179041]\n",
      "[ episode 171 ][ timestamp 62 ] state=[0.33431899 0.16158312 0.15130117 1.21179041], action=1, reward=1.0, next_state=[0.33755065 0.35446347 0.17553698 0.97008817]\n",
      "[ episode 171 ][ timestamp 63 ] state=[0.33755065 0.35446347 0.17553698 0.97008817], action=1, reward=1.0, next_state=[0.34463992 0.54685037 0.19493874 0.7372857 ]\n",
      "[ episode 171 ][ timestamp 64 ] state=[0.34463992 0.54685037 0.19493874 0.7372857 ], action=1, reward=-1.0, next_state=[0.35557693 0.73882257 0.20968445 0.51173116]\n",
      "[ Ended! ] Episode 171: Exploration_rate=0.42650460709830135. Score=64.\n",
      "[ Experience replay ] starts\n",
      "[ episode 172 ] state=[-0.02081527  0.03717552 -0.01362286  0.02733094]\n",
      "[ episode 172 ][ timestamp 1 ] state=[-0.02081527  0.03717552 -0.01362286  0.02733094], action=1, reward=1.0, next_state=[-0.02007176  0.23249015 -0.01307624 -0.26961883]\n",
      "[ episode 172 ][ timestamp 2 ] state=[-0.02007176  0.23249015 -0.01307624 -0.26961883], action=0, reward=1.0, next_state=[-0.01542195  0.03755723 -0.01846862  0.01891126]\n",
      "[ episode 172 ][ timestamp 3 ] state=[-0.01542195  0.03755723 -0.01846862  0.01891126], action=0, reward=1.0, next_state=[-0.01467081 -0.15729506 -0.01809039  0.30571039]\n",
      "[ episode 172 ][ timestamp 4 ] state=[-0.01467081 -0.15729506 -0.01809039  0.30571039], action=1, reward=1.0, next_state=[-0.01781671  0.03807995 -0.01197619  0.00737754]\n",
      "[ episode 172 ][ timestamp 5 ] state=[-0.01781671  0.03807995 -0.01197619  0.00737754], action=0, reward=1.0, next_state=[-0.01705511 -0.15686822 -0.01182864  0.2962579 ]\n",
      "[ episode 172 ][ timestamp 6 ] state=[-0.01705511 -0.15686822 -0.01182864  0.2962579 ], action=0, reward=1.0, next_state=[-0.02019248 -0.35181956 -0.00590348  0.58518692]\n",
      "[ episode 172 ][ timestamp 7 ] state=[-0.02019248 -0.35181956 -0.00590348  0.58518692], action=1, reward=1.0, next_state=[-0.02722887 -0.15661542  0.00580026  0.2906502 ]\n",
      "[ episode 172 ][ timestamp 8 ] state=[-0.02722887 -0.15661542  0.00580026  0.2906502 ], action=0, reward=1.0, next_state=[-0.03036118 -0.3518196   0.01161327  0.5851568 ]\n",
      "[ episode 172 ][ timestamp 9 ] state=[-0.03036118 -0.3518196   0.01161327  0.5851568 ], action=0, reward=1.0, next_state=[-0.03739757 -0.54710228  0.0233164   0.8814753 ]\n",
      "[ episode 172 ][ timestamp 10 ] state=[-0.03739757 -0.54710228  0.0233164   0.8814753 ], action=1, reward=1.0, next_state=[-0.04833961 -0.35230467  0.04094591  0.59621271]\n",
      "[ episode 172 ][ timestamp 11 ] state=[-0.04833961 -0.35230467  0.04094591  0.59621271], action=1, reward=1.0, next_state=[-0.05538571 -0.15777894  0.05287016  0.31670342]\n",
      "[ episode 172 ][ timestamp 12 ] state=[-0.05538571 -0.15777894  0.05287016  0.31670342], action=0, reward=1.0, next_state=[-0.05854129 -0.35361253  0.05920423  0.62557993]\n",
      "[ episode 172 ][ timestamp 13 ] state=[-0.05854129 -0.35361253  0.05920423  0.62557993], action=1, reward=1.0, next_state=[-0.06561354 -0.15936484  0.07171583  0.35211477]\n",
      "[ episode 172 ][ timestamp 14 ] state=[-0.06561354 -0.15936484  0.07171583  0.35211477], action=0, reward=1.0, next_state=[-0.06880083 -0.35542945  0.07875812  0.6665221 ]\n",
      "[ episode 172 ][ timestamp 15 ] state=[-0.06880083 -0.35542945  0.07875812  0.6665221 ], action=1, reward=1.0, next_state=[-0.07590942 -0.16148608  0.09208857  0.3996398 ]\n",
      "[ episode 172 ][ timestamp 16 ] state=[-0.07590942 -0.16148608  0.09208857  0.3996398 ], action=0, reward=1.0, next_state=[-0.07913914 -0.35778548  0.10008136  0.71987704]\n",
      "[ episode 172 ][ timestamp 17 ] state=[-0.07913914 -0.35778548  0.10008136  0.71987704], action=0, reward=1.0, next_state=[-0.08629485 -0.55413923  0.1144789   1.04230869]\n",
      "[ episode 172 ][ timestamp 18 ] state=[-0.08629485 -0.55413923  0.1144789   1.04230869], action=1, reward=1.0, next_state=[-0.09737764 -0.36070838  0.13532508  0.7876449 ]\n",
      "[ episode 172 ][ timestamp 19 ] state=[-0.09737764 -0.36070838  0.13532508  0.7876449 ], action=1, reward=1.0, next_state=[-0.10459181 -0.167679    0.15107797  0.54041224]\n",
      "[ episode 172 ][ timestamp 20 ] state=[-0.10459181 -0.167679    0.15107797  0.54041224], action=1, reward=1.0, next_state=[-0.10794538  0.02503257  0.16188622  0.29888568]\n",
      "[ episode 172 ][ timestamp 21 ] state=[-0.10794538  0.02503257  0.16188622  0.29888568], action=1, reward=1.0, next_state=[-0.10744473  0.21752133  0.16786393  0.06131465]\n",
      "[ episode 172 ][ timestamp 22 ] state=[-0.10744473  0.21752133  0.16786393  0.06131465], action=0, reward=1.0, next_state=[-0.10309431  0.02044009  0.16909023  0.40190177]\n",
      "[ episode 172 ][ timestamp 23 ] state=[-0.10309431  0.02044009  0.16909023  0.40190177], action=0, reward=1.0, next_state=[-0.10268551 -0.17662602  0.17712826  0.74276116]\n",
      "[ episode 172 ][ timestamp 24 ] state=[-0.10268551 -0.17662602  0.17712826  0.74276116], action=1, reward=1.0, next_state=[-0.10621803  0.01566645  0.19198348  0.51063926]\n",
      "[ episode 172 ][ timestamp 25 ] state=[-0.10621803  0.01566645  0.19198348  0.51063926], action=1, reward=1.0, next_state=[-0.1059047   0.20763937  0.20219627  0.28406741]\n",
      "[ episode 172 ][ timestamp 26 ] state=[-0.1059047   0.20763937  0.20219627  0.28406741], action=1, reward=1.0, next_state=[-0.10175191  0.39938989  0.20787762  0.06134265]\n",
      "[ episode 172 ][ timestamp 27 ] state=[-0.10175191  0.39938989  0.20787762  0.06134265], action=0, reward=1.0, next_state=[-0.09376411  0.20198781  0.20910447  0.41174781]\n",
      "[ episode 172 ][ timestamp 28 ] state=[-0.09376411  0.20198781  0.20910447  0.41174781], action=1, reward=-1.0, next_state=[-0.08972436  0.39362764  0.21733943  0.19157942]\n",
      "[ Ended! ] Episode 172: Exploration_rate=0.42437208406280985. Score=28.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 173 ] state=[-0.04622466 -0.01658417  0.04646375  0.03353385]\n",
      "[ episode 173 ][ timestamp 1 ] state=[-0.04622466 -0.01658417  0.04646375  0.03353385], action=0, reward=1.0, next_state=[-0.04655634 -0.21234057  0.04713443  0.34050697]\n",
      "[ episode 173 ][ timestamp 2 ] state=[-0.04655634 -0.21234057  0.04713443  0.34050697], action=1, reward=1.0, next_state=[-0.05080316 -0.01791984  0.05394457  0.06305217]\n",
      "[ episode 173 ][ timestamp 3 ] state=[-0.05080316 -0.01791984  0.05394457  0.06305217], action=1, reward=1.0, next_state=[-0.05116155  0.17638884  0.05520561 -0.21213487]\n",
      "[ episode 173 ][ timestamp 4 ] state=[-0.05116155  0.17638884  0.05520561 -0.21213487], action=1, reward=1.0, next_state=[-0.04763378  0.37067981  0.05096291 -0.48690513]\n",
      "[ episode 173 ][ timestamp 5 ] state=[-0.04763378  0.37067981  0.05096291 -0.48690513], action=0, reward=1.0, next_state=[-0.04022018  0.1748772   0.04122481 -0.17860593]\n",
      "[ episode 173 ][ timestamp 6 ] state=[-0.04022018  0.1748772   0.04122481 -0.17860593], action=0, reward=1.0, next_state=[-0.03672264 -0.0208097   0.03765269  0.1267917 ]\n",
      "[ episode 173 ][ timestamp 7 ] state=[-0.03672264 -0.0208097   0.03765269  0.1267917 ], action=1, reward=1.0, next_state=[-0.03713883  0.17375318  0.04018853 -0.1537785 ]\n",
      "[ episode 173 ][ timestamp 8 ] state=[-0.03713883  0.17375318  0.04018853 -0.1537785 ], action=1, reward=1.0, next_state=[-0.03366377  0.36827734  0.03711296 -0.43351689]\n",
      "[ episode 173 ][ timestamp 9 ] state=[-0.03366377  0.36827734  0.03711296 -0.43351689], action=0, reward=1.0, next_state=[-0.02629822  0.17265013  0.02844262 -0.12936944]\n",
      "[ episode 173 ][ timestamp 10 ] state=[-0.02629822  0.17265013  0.02844262 -0.12936944], action=0, reward=1.0, next_state=[-0.02284522 -0.02286747  0.02585523  0.17214935]\n",
      "[ episode 173 ][ timestamp 11 ] state=[-0.02284522 -0.02286747  0.02585523  0.17214935], action=0, reward=1.0, next_state=[-0.02330257 -0.21834975  0.02929822  0.47287536]\n",
      "[ episode 173 ][ timestamp 12 ] state=[-0.02330257 -0.21834975  0.02929822  0.47287536], action=1, reward=1.0, next_state=[-0.02766956 -0.02365357  0.03875572  0.18956887]\n",
      "[ episode 173 ][ timestamp 13 ] state=[-0.02766956 -0.02365357  0.03875572  0.18956887], action=0, reward=1.0, next_state=[-0.02814263 -0.21930793  0.0425471   0.49422136]\n",
      "[ episode 173 ][ timestamp 14 ] state=[-0.02814263 -0.21930793  0.0425471   0.49422136], action=0, reward=1.0, next_state=[-0.03252879 -0.41500329  0.05243153  0.80000382]\n",
      "[ episode 173 ][ timestamp 15 ] state=[-0.03252879 -0.41500329  0.05243153  0.80000382], action=1, reward=1.0, next_state=[-0.04082886 -0.22063825  0.0684316   0.52426471]\n",
      "[ episode 173 ][ timestamp 16 ] state=[-0.04082886 -0.22063825  0.0684316   0.52426471], action=1, reward=1.0, next_state=[-0.04524162 -0.02654271  0.0789169   0.25390603]\n",
      "[ episode 173 ][ timestamp 17 ] state=[-0.04524162 -0.02654271  0.0789169   0.25390603], action=1, reward=1.0, next_state=[-0.04577248  0.16736892  0.08399502 -0.01287866]\n",
      "[ episode 173 ][ timestamp 18 ] state=[-0.04577248  0.16736892  0.08399502 -0.01287866], action=0, reward=1.0, next_state=[-0.0424251  -0.02885091  0.08373745  0.30507894]\n",
      "[ episode 173 ][ timestamp 19 ] state=[-0.0424251  -0.02885091  0.08373745  0.30507894], action=1, reward=1.0, next_state=[-0.04300212  0.16498411  0.08983902  0.03993524]\n",
      "[ episode 173 ][ timestamp 20 ] state=[-0.04300212  0.16498411  0.08983902  0.03993524], action=0, reward=1.0, next_state=[-0.03970243 -0.0313036   0.09063773  0.35955657]\n",
      "[ episode 173 ][ timestamp 21 ] state=[-0.03970243 -0.0313036   0.09063773  0.35955657], action=1, reward=1.0, next_state=[-0.04032851  0.16242087  0.09782886  0.09677369]\n",
      "[ episode 173 ][ timestamp 22 ] state=[-0.04032851  0.16242087  0.09782886  0.09677369], action=1, reward=1.0, next_state=[-0.03708009  0.3560145   0.09976433 -0.16351245]\n",
      "[ episode 173 ][ timestamp 23 ] state=[-0.03708009  0.3560145   0.09976433 -0.16351245], action=1, reward=1.0, next_state=[-0.0299598   0.54957726  0.09649409 -0.42313082]\n",
      "[ episode 173 ][ timestamp 24 ] state=[-0.0299598   0.54957726  0.09649409 -0.42313082], action=0, reward=1.0, next_state=[-0.01896825  0.3532302   0.08803147 -0.10165506]\n",
      "[ episode 173 ][ timestamp 25 ] state=[-0.01896825  0.3532302   0.08803147 -0.10165506], action=0, reward=1.0, next_state=[-0.01190365  0.15696415  0.08599837  0.21745185]\n",
      "[ episode 173 ][ timestamp 26 ] state=[-0.01190365  0.15696415  0.08599837  0.21745185], action=0, reward=1.0, next_state=[-0.00876437 -0.03927517  0.09034741  0.53597538]\n",
      "[ episode 173 ][ timestamp 27 ] state=[-0.00876437 -0.03927517  0.09034741  0.53597538], action=1, reward=1.0, next_state=[-0.00954987  0.15446799  0.10106691  0.27307194]\n",
      "[ episode 173 ][ timestamp 28 ] state=[-0.00954987  0.15446799  0.10106691  0.27307194], action=1, reward=1.0, next_state=[-0.00646051  0.34801352  0.10652835  0.01389823]\n",
      "[ episode 173 ][ timestamp 29 ] state=[-0.00646051  0.34801352  0.10652835  0.01389823], action=0, reward=1.0, next_state=[0.00049976 0.15153787 0.10680632 0.33820116]\n",
      "[ episode 173 ][ timestamp 30 ] state=[0.00049976 0.15153787 0.10680632 0.33820116], action=0, reward=1.0, next_state=[ 0.00353052 -0.04492895  0.11357034  0.66256347]\n",
      "[ episode 173 ][ timestamp 31 ] state=[ 0.00353052 -0.04492895  0.11357034  0.66256347], action=1, reward=1.0, next_state=[0.00263194 0.14844503 0.12682161 0.40768909]\n",
      "[ episode 173 ][ timestamp 32 ] state=[0.00263194 0.14844503 0.12682161 0.40768909], action=1, reward=1.0, next_state=[0.00560084 0.34156206 0.13497539 0.15752562]\n",
      "[ episode 173 ][ timestamp 33 ] state=[0.00560084 0.34156206 0.13497539 0.15752562], action=1, reward=1.0, next_state=[ 0.01243208  0.53451929  0.1381259  -0.08971531]\n",
      "[ episode 173 ][ timestamp 34 ] state=[ 0.01243208  0.53451929  0.1381259  -0.08971531], action=0, reward=1.0, next_state=[0.02312247 0.33771559 0.1363316  0.24315865]\n",
      "[ episode 173 ][ timestamp 35 ] state=[0.02312247 0.33771559 0.1363316  0.24315865], action=1, reward=1.0, next_state=[ 0.02987678  0.53065355  0.14119477 -0.00360552]\n",
      "[ episode 173 ][ timestamp 36 ] state=[ 0.02987678  0.53065355  0.14119477 -0.00360552], action=0, reward=1.0, next_state=[0.04048985 0.33381869 0.14112266 0.33008206]\n",
      "[ episode 173 ][ timestamp 37 ] state=[0.04048985 0.33381869 0.14112266 0.33008206], action=1, reward=1.0, next_state=[0.04716622 0.52667927 0.1477243  0.0850196 ]\n",
      "[ episode 173 ][ timestamp 38 ] state=[0.04716622 0.52667927 0.1477243  0.0850196 ], action=0, reward=1.0, next_state=[0.05769981 0.32978258 0.14942469 0.42042107]\n",
      "[ episode 173 ][ timestamp 39 ] state=[0.05769981 0.32978258 0.14942469 0.42042107], action=0, reward=1.0, next_state=[0.06429546 0.13289437 0.15783311 0.75623003]\n",
      "[ episode 173 ][ timestamp 40 ] state=[0.06429546 0.13289437 0.15783311 0.75623003], action=0, reward=1.0, next_state=[ 0.06695335 -0.0640101   0.17295771  1.09412602]\n",
      "[ episode 173 ][ timestamp 41 ] state=[ 0.06695335 -0.0640101   0.17295771  1.09412602], action=1, reward=1.0, next_state=[0.06567315 0.12846444 0.19484024 0.8603182 ]\n",
      "[ episode 173 ][ timestamp 42 ] state=[0.06567315 0.12846444 0.19484024 0.8603182 ], action=1, reward=-1.0, next_state=[0.06824243 0.32047544 0.2120466  0.63467265]\n",
      "[ Ended! ] Episode 173: Exploration_rate=0.4222502236424958. Score=42.\n",
      "[ Experience replay ] starts\n",
      "[ episode 174 ] state=[-0.01478623 -0.00603343  0.00132159  0.04332149]\n",
      "[ episode 174 ][ timestamp 1 ] state=[-0.01478623 -0.00603343  0.00132159  0.04332149], action=1, reward=1.0, next_state=[-0.0149069   0.18906955  0.00218802 -0.24894417]\n",
      "[ episode 174 ][ timestamp 2 ] state=[-0.0149069   0.18906955  0.00218802 -0.24894417], action=0, reward=1.0, next_state=[-0.01112551 -0.00608358 -0.00279087  0.0444281 ]\n",
      "[ episode 174 ][ timestamp 3 ] state=[-0.01112551 -0.00608358 -0.00279087  0.0444281 ], action=1, reward=1.0, next_state=[-0.01124718  0.18907828 -0.00190231 -0.24913407]\n",
      "[ episode 174 ][ timestamp 4 ] state=[-0.01124718  0.18907828 -0.00190231 -0.24913407], action=0, reward=1.0, next_state=[-0.00746562 -0.00601646 -0.00688499  0.04294823]\n",
      "[ episode 174 ][ timestamp 5 ] state=[-0.00746562 -0.00601646 -0.00688499  0.04294823], action=0, reward=1.0, next_state=[-0.00758595 -0.20103901 -0.00602602  0.33345095]\n",
      "[ episode 174 ][ timestamp 6 ] state=[-0.00758595 -0.20103901 -0.00602602  0.33345095], action=1, reward=1.0, next_state=[-0.01160673 -0.00583181  0.000643    0.03887383]\n",
      "[ episode 174 ][ timestamp 7 ] state=[-0.01160673 -0.00583181  0.000643    0.03887383], action=1, reward=1.0, next_state=[-0.01172336  0.18928092  0.00142047 -0.25360616]\n",
      "[ episode 174 ][ timestamp 8 ] state=[-0.01172336  0.18928092  0.00142047 -0.25360616], action=0, reward=1.0, next_state=[-0.00793774 -0.00586129 -0.00365165  0.03952447]\n",
      "[ episode 174 ][ timestamp 9 ] state=[-0.00793774 -0.00586129 -0.00365165  0.03952447], action=1, reward=1.0, next_state=[-0.00805497  0.18931284 -0.00286116 -0.25430835]\n",
      "[ episode 174 ][ timestamp 10 ] state=[-0.00805497  0.18931284 -0.00286116 -0.25430835], action=0, reward=1.0, next_state=[-0.00426871 -0.00576814 -0.00794733  0.03747075]\n",
      "[ episode 174 ][ timestamp 11 ] state=[-0.00426871 -0.00576814 -0.00794733  0.03747075], action=1, reward=1.0, next_state=[-0.00438408  0.18946687 -0.00719791 -0.25770901]\n",
      "[ episode 174 ][ timestamp 12 ] state=[-0.00438408  0.18946687 -0.00719791 -0.25770901], action=0, reward=1.0, next_state=[-0.00059474 -0.00555159 -0.01235209  0.03269493]\n",
      "[ episode 174 ][ timestamp 13 ] state=[-0.00059474 -0.00555159 -0.01235209  0.03269493], action=0, reward=1.0, next_state=[-0.00070577 -0.20049425 -0.01169819  0.32145518]\n",
      "[ episode 174 ][ timestamp 14 ] state=[-0.00070577 -0.20049425 -0.01169819  0.32145518], action=1, reward=1.0, next_state=[-0.00471566 -0.00520768 -0.00526909  0.02510618]\n",
      "[ episode 174 ][ timestamp 15 ] state=[-0.00471566 -0.00520768 -0.00526909  0.02510618], action=1, reward=1.0, next_state=[-0.00481981  0.18998944 -0.00476697 -0.26923454]\n",
      "[ episode 174 ][ timestamp 16 ] state=[-0.00481981  0.18998944 -0.00476697 -0.26923454], action=1, reward=1.0, next_state=[-0.00102002  0.38517909 -0.01015166 -0.56341717]\n",
      "[ episode 174 ][ timestamp 17 ] state=[-0.00102002  0.38517909 -0.01015166 -0.56341717], action=0, reward=1.0, next_state=[ 0.00668356  0.19020105 -0.02142    -0.27394971]\n",
      "[ episode 174 ][ timestamp 18 ] state=[ 0.00668356  0.19020105 -0.02142    -0.27394971], action=0, reward=1.0, next_state=[ 0.01048758 -0.00460883 -0.026899    0.01190108]\n",
      "[ episode 174 ][ timestamp 19 ] state=[ 0.01048758 -0.00460883 -0.026899    0.01190108], action=1, reward=1.0, next_state=[ 0.01039541  0.19088835 -0.02666097 -0.28914597]\n",
      "[ episode 174 ][ timestamp 20 ] state=[ 0.01039541  0.19088835 -0.02666097 -0.28914597], action=0, reward=1.0, next_state=[ 0.01421317 -0.00384347 -0.03244389 -0.00498944]\n",
      "[ episode 174 ][ timestamp 21 ] state=[ 0.01421317 -0.00384347 -0.03244389 -0.00498944], action=1, reward=1.0, next_state=[ 0.0141363   0.19172839 -0.03254368 -0.30772968]\n",
      "[ episode 174 ][ timestamp 22 ] state=[ 0.0141363   0.19172839 -0.03254368 -0.30772968], action=0, reward=1.0, next_state=[ 0.01797087 -0.00291509 -0.03869828 -0.0254852 ]\n",
      "[ episode 174 ][ timestamp 23 ] state=[ 0.01797087 -0.00291509 -0.03869828 -0.0254852 ], action=1, reward=1.0, next_state=[ 0.01791257  0.19273984 -0.03920798 -0.33012233]\n",
      "[ episode 174 ][ timestamp 24 ] state=[ 0.01791257  0.19273984 -0.03920798 -0.33012233], action=1, reward=1.0, next_state=[ 0.02176737  0.38839735 -0.04581043 -0.63490723]\n",
      "[ episode 174 ][ timestamp 25 ] state=[ 0.02176737  0.38839735 -0.04581043 -0.63490723], action=0, reward=1.0, next_state=[ 0.02953531  0.1939433  -0.05850857 -0.35699571]\n",
      "[ episode 174 ][ timestamp 26 ] state=[ 0.02953531  0.1939433  -0.05850857 -0.35699571], action=1, reward=1.0, next_state=[ 0.03341418  0.38984615 -0.06564849 -0.66753887]\n",
      "[ episode 174 ][ timestamp 27 ] state=[ 0.03341418  0.38984615 -0.06564849 -0.66753887], action=0, reward=1.0, next_state=[ 0.0412111   0.19569561 -0.07899926 -0.39622718]\n",
      "[ episode 174 ][ timestamp 28 ] state=[ 0.0412111   0.19569561 -0.07899926 -0.39622718], action=0, reward=1.0, next_state=[ 0.04512501  0.00177819 -0.08692381 -0.12945988]\n",
      "[ episode 174 ][ timestamp 29 ] state=[ 0.04512501  0.00177819 -0.08692381 -0.12945988], action=1, reward=1.0, next_state=[ 0.04516058  0.19803084 -0.089513   -0.44825086]\n",
      "[ episode 174 ][ timestamp 30 ] state=[ 0.04516058  0.19803084 -0.089513   -0.44825086], action=0, reward=1.0, next_state=[ 0.0491212   0.00428156 -0.09847802 -0.18507217]\n",
      "[ episode 174 ][ timestamp 31 ] state=[ 0.0491212   0.00428156 -0.09847802 -0.18507217], action=1, reward=1.0, next_state=[ 0.04920683  0.20066453 -0.10217946 -0.50712516]\n",
      "[ episode 174 ][ timestamp 32 ] state=[ 0.04920683  0.20066453 -0.10217946 -0.50712516], action=0, reward=1.0, next_state=[ 0.05322012  0.00711955 -0.11232197 -0.24831045]\n",
      "[ episode 174 ][ timestamp 33 ] state=[ 0.05322012  0.00711955 -0.11232197 -0.24831045], action=1, reward=1.0, next_state=[ 0.05336251  0.20365144 -0.11728818 -0.57420389]\n",
      "[ episode 174 ][ timestamp 34 ] state=[ 0.05336251  0.20365144 -0.11728818 -0.57420389], action=0, reward=1.0, next_state=[ 0.05743554  0.01035223 -0.12877225 -0.32065084]\n",
      "[ episode 174 ][ timestamp 35 ] state=[ 0.05743554  0.01035223 -0.12877225 -0.32065084], action=1, reward=1.0, next_state=[ 0.05764258  0.20705039 -0.13518527 -0.65100968]\n",
      "[ episode 174 ][ timestamp 36 ] state=[ 0.05764258  0.20705039 -0.13518527 -0.65100968], action=0, reward=1.0, next_state=[ 0.06178359  0.01404445 -0.14820546 -0.40376566]\n",
      "[ episode 174 ][ timestamp 37 ] state=[ 0.06178359  0.01404445 -0.14820546 -0.40376566], action=0, reward=1.0, next_state=[ 0.06206448 -0.17869898 -0.15628078 -0.16123297]\n",
      "[ episode 174 ][ timestamp 38 ] state=[ 0.06206448 -0.17869898 -0.15628078 -0.16123297], action=1, reward=1.0, next_state=[ 0.0584905   0.01827485 -0.15950544 -0.49885268]\n",
      "[ episode 174 ][ timestamp 39 ] state=[ 0.0584905   0.01827485 -0.15950544 -0.49885268], action=0, reward=1.0, next_state=[ 0.058856   -0.17428129 -0.16948249 -0.26038095]\n",
      "[ episode 174 ][ timestamp 40 ] state=[ 0.058856   -0.17428129 -0.16948249 -0.26038095], action=1, reward=1.0, next_state=[ 0.05537037  0.02280357 -0.17469011 -0.6013622 ]\n",
      "[ episode 174 ][ timestamp 41 ] state=[ 0.05537037  0.02280357 -0.17469011 -0.6013622 ], action=0, reward=1.0, next_state=[ 0.05582644 -0.1695     -0.18671735 -0.36839508]\n",
      "[ episode 174 ][ timestamp 42 ] state=[ 0.05582644 -0.1695     -0.18671735 -0.36839508], action=0, reward=1.0, next_state=[ 0.05243644 -0.36154629 -0.19408526 -0.13990909]\n",
      "[ episode 174 ][ timestamp 43 ] state=[ 0.05243644 -0.36154629 -0.19408526 -0.13990909], action=0, reward=1.0, next_state=[ 0.04520552 -0.55343554 -0.19688344  0.08581707]\n",
      "[ episode 174 ][ timestamp 44 ] state=[ 0.04520552 -0.55343554 -0.19688344  0.08581707], action=1, reward=1.0, next_state=[ 0.0341368  -0.35611627 -0.1951671  -0.26195431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 174 ][ timestamp 45 ] state=[ 0.0341368  -0.35611627 -0.1951671  -0.26195431], action=1, reward=1.0, next_state=[ 0.02701448 -0.15882179 -0.20040618 -0.60929322]\n",
      "[ episode 174 ][ timestamp 46 ] state=[ 0.02701448 -0.15882179 -0.20040618 -0.60929322], action=1, reward=-1.0, next_state=[ 0.02383804  0.03845382 -0.21259205 -0.95780999]\n",
      "[ Ended! ] Episode 174: Exploration_rate=0.42013897252428334. Score=46.\n",
      "[ Experience replay ] starts\n",
      "[ episode 175 ] state=[-0.01061661  0.00147769 -0.0350189   0.01378915]\n",
      "[ episode 175 ][ timestamp 1 ] state=[-0.01061661  0.00147769 -0.0350189   0.01378915], action=1, reward=1.0, next_state=[-0.01058705  0.1970839  -0.03474312 -0.28973373]\n",
      "[ episode 175 ][ timestamp 2 ] state=[-0.01058705  0.1970839  -0.03474312 -0.28973373], action=1, reward=1.0, next_state=[-0.00664537  0.3926836  -0.04053779 -0.59316864]\n",
      "[ episode 175 ][ timestamp 3 ] state=[-0.00664537  0.3926836  -0.04053779 -0.59316864], action=0, reward=1.0, next_state=[ 0.0012083   0.19815185 -0.05240116 -0.31352559]\n",
      "[ episode 175 ][ timestamp 4 ] state=[ 0.0012083   0.19815185 -0.05240116 -0.31352559], action=1, reward=1.0, next_state=[ 0.00517134  0.39397961 -0.05867168 -0.62226291]\n",
      "[ episode 175 ][ timestamp 5 ] state=[ 0.00517134  0.39397961 -0.05867168 -0.62226291], action=0, reward=1.0, next_state=[ 0.01305093  0.19972389 -0.07111693 -0.34862029]\n",
      "[ episode 175 ][ timestamp 6 ] state=[ 0.01305093  0.19972389 -0.07111693 -0.34862029], action=0, reward=1.0, next_state=[ 0.01704541  0.00568168 -0.07808934 -0.07918348]\n",
      "[ episode 175 ][ timestamp 7 ] state=[ 0.01704541  0.00568168 -0.07808934 -0.07918348], action=0, reward=1.0, next_state=[ 0.01715904 -0.18823903 -0.07967301  0.18787621]\n",
      "[ episode 175 ][ timestamp 8 ] state=[ 0.01715904 -0.18823903 -0.07967301  0.18787621], action=0, reward=1.0, next_state=[ 0.01339426 -0.38213605 -0.07591548  0.45440003]\n",
      "[ episode 175 ][ timestamp 9 ] state=[ 0.01339426 -0.38213605 -0.07591548  0.45440003], action=0, reward=1.0, next_state=[ 0.00575154 -0.57610708 -0.06682748  0.72222084]\n",
      "[ episode 175 ][ timestamp 10 ] state=[ 0.00575154 -0.57610708 -0.06682748  0.72222084], action=0, reward=1.0, next_state=[-0.0057706  -0.77024412 -0.05238307  0.99314373]\n",
      "[ episode 175 ][ timestamp 11 ] state=[-0.0057706  -0.77024412 -0.05238307  0.99314373], action=1, reward=1.0, next_state=[-0.02117549 -0.57446195 -0.03252019  0.68447972]\n",
      "[ episode 175 ][ timestamp 12 ] state=[-0.02117549 -0.57446195 -0.03252019  0.68447972], action=1, reward=1.0, next_state=[-0.03266473 -0.37890393 -0.0188306   0.38173854]\n",
      "[ episode 175 ][ timestamp 13 ] state=[-0.03266473 -0.37890393 -0.0188306   0.38173854], action=0, reward=1.0, next_state=[-0.0402428  -0.5737535  -0.01119583  0.66842522]\n",
      "[ episode 175 ][ timestamp 14 ] state=[-0.0402428  -0.5737535  -0.01119583  0.66842522], action=0, reward=1.0, next_state=[-0.05171787 -0.768718    0.00217268  0.95756212]\n",
      "[ episode 175 ][ timestamp 15 ] state=[-0.05171787 -0.768718    0.00217268  0.95756212], action=0, reward=1.0, next_state=[-0.06709223 -0.9638691   0.02132392  1.25092685]\n",
      "[ episode 175 ][ timestamp 16 ] state=[-0.06709223 -0.9638691   0.02132392  1.25092685], action=1, reward=1.0, next_state=[-0.08636962 -0.7690268   0.04634246  0.9649986 ]\n",
      "[ episode 175 ][ timestamp 17 ] state=[-0.08636962 -0.7690268   0.04634246  0.9649986 ], action=1, reward=1.0, next_state=[-0.10175015 -0.57455696  0.06564243  0.68722684]\n",
      "[ episode 175 ][ timestamp 18 ] state=[-0.10175015 -0.57455696  0.06564243  0.68722684], action=0, reward=1.0, next_state=[-0.11324129 -0.7705257   0.07938696  0.99983188]\n",
      "[ episode 175 ][ timestamp 19 ] state=[-0.11324129 -0.7705257   0.07938696  0.99983188], action=0, reward=1.0, next_state=[-0.12865181 -0.96661382  0.0993836   1.31635295]\n",
      "[ episode 175 ][ timestamp 20 ] state=[-0.12865181 -0.96661382  0.0993836   1.31635295], action=0, reward=1.0, next_state=[-0.14798408 -1.16284263  0.12571066  1.63841443]\n",
      "[ episode 175 ][ timestamp 21 ] state=[-0.14798408 -1.16284263  0.12571066  1.63841443], action=1, reward=1.0, next_state=[-0.17124093 -0.96939876  0.15847895  1.38740005]\n",
      "[ episode 175 ][ timestamp 22 ] state=[-0.17124093 -0.96939876  0.15847895  1.38740005], action=0, reward=1.0, next_state=[-0.19062891 -1.16610083  0.18622695  1.72515372]\n",
      "[ episode 175 ][ timestamp 23 ] state=[-0.19062891 -1.16610083  0.18622695  1.72515372], action=0, reward=-1.0, next_state=[-0.21395093 -1.36280123  0.22073003  2.06953763]\n",
      "[ Ended! ] Episode 175: Exploration_rate=0.4180382776616619. Score=23.\n",
      "[ Experience replay ] starts\n",
      "[ episode 176 ] state=[-0.02436186 -0.02133845 -0.02481327 -0.04995214]\n",
      "[ episode 176 ][ timestamp 1 ] state=[-0.02436186 -0.02133845 -0.02481327 -0.04995214], action=0, reward=1.0, next_state=[-0.02478863 -0.21609598 -0.02581231  0.23479987]\n",
      "[ episode 176 ][ timestamp 2 ] state=[-0.02478863 -0.21609598 -0.02581231  0.23479987], action=0, reward=1.0, next_state=[-0.02911055 -0.4108398  -0.02111631  0.51923032]\n",
      "[ episode 176 ][ timestamp 3 ] state=[-0.02911055 -0.4108398  -0.02111631  0.51923032], action=0, reward=1.0, next_state=[-0.03732735 -0.6056582  -0.01073171  0.80518504]\n",
      "[ episode 176 ][ timestamp 4 ] state=[-0.03732735 -0.6056582  -0.01073171  0.80518504], action=0, reward=1.0, next_state=[-0.04944051 -0.8006314   0.005372    1.09447293]\n",
      "[ episode 176 ][ timestamp 5 ] state=[-0.04944051 -0.8006314   0.005372    1.09447293], action=0, reward=1.0, next_state=[-0.06545314 -0.9958237   0.02726145  1.38883652]\n",
      "[ episode 176 ][ timestamp 6 ] state=[-0.06545314 -0.9958237   0.02726145  1.38883652], action=0, reward=1.0, next_state=[-0.08536961 -1.19127451  0.05503818  1.68991767]\n",
      "[ episode 176 ][ timestamp 7 ] state=[-0.08536961 -1.19127451  0.05503818  1.68991767], action=0, reward=1.0, next_state=[-0.1091951  -1.38698759  0.08883654  1.99921582]\n",
      "[ episode 176 ][ timestamp 8 ] state=[-0.1091951  -1.38698759  0.08883654  1.99921582], action=0, reward=1.0, next_state=[-0.13693486 -1.5829181   0.12882085  2.31803625]\n",
      "[ episode 176 ][ timestamp 9 ] state=[-0.13693486 -1.5829181   0.12882085  2.31803625], action=0, reward=1.0, next_state=[-0.16859322 -1.77895699  0.17518158  2.6474267 ]\n",
      "[ episode 176 ][ timestamp 10 ] state=[-0.16859322 -1.77895699  0.17518158  2.6474267 ], action=0, reward=-1.0, next_state=[-0.20417236 -1.97491292  0.22813011  2.98810227]\n",
      "[ Ended! ] Episode 176: Exploration_rate=0.4159480862733536. Score=10.\n",
      "[ Experience replay ] starts\n",
      "[ episode 177 ] state=[-0.00364855  0.04819682  0.02226791 -0.03699246]\n",
      "[ episode 177 ][ timestamp 1 ] state=[-0.00364855  0.04819682  0.02226791 -0.03699246], action=0, reward=1.0, next_state=[-0.00268462 -0.14723726  0.02152806  0.26263221]\n",
      "[ episode 177 ][ timestamp 2 ] state=[-0.00268462 -0.14723726  0.02152806  0.26263221], action=0, reward=1.0, next_state=[-0.00562936 -0.34265978  0.02678071  0.56202684]\n",
      "[ episode 177 ][ timestamp 3 ] state=[-0.00562936 -0.34265978  0.02678071  0.56202684], action=1, reward=1.0, next_state=[-0.01248256 -0.14792369  0.03802124  0.27790002]\n",
      "[ episode 177 ][ timestamp 4 ] state=[-0.01248256 -0.14792369  0.03802124  0.27790002], action=1, reward=1.0, next_state=[-0.01544103  0.0466358   0.04357924 -0.00255274]\n",
      "[ episode 177 ][ timestamp 5 ] state=[-0.01544103  0.0466358   0.04357924 -0.00255274], action=1, reward=1.0, next_state=[-0.01450832  0.24110655  0.04352819 -0.28117366]\n",
      "[ episode 177 ][ timestamp 6 ] state=[-0.01450832  0.24110655  0.04352819 -0.28117366], action=1, reward=1.0, next_state=[-0.00968618  0.43558144  0.03790471 -0.55981645]\n",
      "[ episode 177 ][ timestamp 7 ] state=[-0.00968618  0.43558144  0.03790471 -0.55981645], action=0, reward=1.0, next_state=[-0.00097456  0.23994855  0.02670839 -0.25543657]\n",
      "[ episode 177 ][ timestamp 8 ] state=[-0.00097456  0.23994855  0.02670839 -0.25543657], action=0, reward=1.0, next_state=[0.00382442 0.04445564 0.02159965 0.04554954]\n",
      "[ episode 177 ][ timestamp 9 ] state=[0.00382442 0.04445564 0.02159965 0.04554954], action=1, reward=1.0, next_state=[ 0.00471353  0.23926131  0.02251065 -0.240241  ]\n",
      "[ episode 177 ][ timestamp 10 ] state=[ 0.00471353  0.23926131  0.02251065 -0.240241  ], action=0, reward=1.0, next_state=[0.00949875 0.04382515 0.01770583 0.05945654]\n",
      "[ episode 177 ][ timestamp 11 ] state=[0.00949875 0.04382515 0.01770583 0.05945654], action=1, reward=1.0, next_state=[ 0.01037526  0.23868882  0.01889496 -0.2275879 ]\n",
      "[ episode 177 ][ timestamp 12 ] state=[ 0.01037526  0.23868882  0.01889496 -0.2275879 ], action=0, reward=1.0, next_state=[0.01514903 0.04330201 0.0143432  0.07099478]\n",
      "[ episode 177 ][ timestamp 13 ] state=[0.01514903 0.04330201 0.0143432  0.07099478], action=1, reward=1.0, next_state=[ 0.01601507  0.23821542  0.01576309 -0.21712851]\n",
      "[ episode 177 ][ timestamp 14 ] state=[ 0.01601507  0.23821542  0.01576309 -0.21712851], action=1, reward=1.0, next_state=[ 0.02077938  0.43310853  0.01142052 -0.50479769]\n",
      "[ episode 177 ][ timestamp 15 ] state=[ 0.02077938  0.43310853  0.01142052 -0.50479769], action=0, reward=1.0, next_state=[ 0.02944155  0.2378275   0.00132457 -0.2085377 ]\n",
      "[ episode 177 ][ timestamp 16 ] state=[ 0.02944155  0.2378275   0.00132457 -0.2085377 ], action=1, reward=1.0, next_state=[ 0.0341981   0.43293049 -0.00284618 -0.5008025 ]\n",
      "[ episode 177 ][ timestamp 17 ] state=[ 0.0341981   0.43293049 -0.00284618 -0.5008025 ], action=0, reward=1.0, next_state=[ 0.04285671  0.23784878 -0.01286223 -0.20901789]\n",
      "[ episode 177 ][ timestamp 18 ] state=[ 0.04285671  0.23784878 -0.01286223 -0.20901789], action=0, reward=1.0, next_state=[ 0.04761369  0.04291308 -0.01704259  0.07958007]\n",
      "[ episode 177 ][ timestamp 19 ] state=[ 0.04761369  0.04291308 -0.01704259  0.07958007], action=1, reward=1.0, next_state=[ 0.04847195  0.23827514 -0.01545099 -0.21843074]\n",
      "[ episode 177 ][ timestamp 20 ] state=[ 0.04847195  0.23827514 -0.01545099 -0.21843074], action=0, reward=1.0, next_state=[ 0.05323745  0.04337743 -0.01981961  0.06933852]\n",
      "[ episode 177 ][ timestamp 21 ] state=[ 0.05323745  0.04337743 -0.01981961  0.06933852], action=1, reward=1.0, next_state=[ 0.054105    0.23877784 -0.01843284 -0.22953112]\n",
      "[ episode 177 ][ timestamp 22 ] state=[ 0.054105    0.23877784 -0.01843284 -0.22953112], action=0, reward=1.0, next_state=[ 0.05888056  0.04392408 -0.02302346  0.05728093]\n",
      "[ episode 177 ][ timestamp 23 ] state=[ 0.05888056  0.04392408 -0.02302346  0.05728093], action=1, reward=1.0, next_state=[ 0.05975904  0.23936845 -0.02187784 -0.24257623]\n",
      "[ episode 177 ][ timestamp 24 ] state=[ 0.05975904  0.23936845 -0.02187784 -0.24257623], action=0, reward=1.0, next_state=[ 0.06454641  0.04456573 -0.02672936  0.04312636]\n",
      "[ episode 177 ][ timestamp 25 ] state=[ 0.06454641  0.04456573 -0.02672936  0.04312636], action=1, reward=1.0, next_state=[ 0.06543772  0.24006057 -0.02586684 -0.25786865]\n",
      "[ episode 177 ][ timestamp 26 ] state=[ 0.06543772  0.24006057 -0.02586684 -0.25786865], action=0, reward=1.0, next_state=[ 0.07023893  0.04531727 -0.03102421  0.02654457]\n",
      "[ episode 177 ][ timestamp 27 ] state=[ 0.07023893  0.04531727 -0.03102421  0.02654457], action=1, reward=1.0, next_state=[ 0.07114528  0.24087008 -0.03049332 -0.27576315]\n",
      "[ episode 177 ][ timestamp 28 ] state=[ 0.07114528  0.24087008 -0.03049332 -0.27576315], action=0, reward=1.0, next_state=[ 0.07596268  0.04619616 -0.03600858  0.00714834]\n",
      "[ episode 177 ][ timestamp 29 ] state=[ 0.07596268  0.04619616 -0.03600858  0.00714834], action=0, reward=1.0, next_state=[ 0.0768866  -0.14839137 -0.03586561  0.28825619]\n",
      "[ episode 177 ][ timestamp 30 ] state=[ 0.0768866  -0.14839137 -0.03586561  0.28825619], action=0, reward=1.0, next_state=[ 0.07391878 -0.342984   -0.03010049  0.56941518]\n",
      "[ episode 177 ][ timestamp 31 ] state=[ 0.07391878 -0.342984   -0.03010049  0.56941518], action=1, reward=1.0, next_state=[ 0.0670591  -0.1474531  -0.01871219  0.26740349]\n",
      "[ episode 177 ][ timestamp 32 ] state=[ 0.0670591  -0.1474531  -0.01871219  0.26740349], action=0, reward=1.0, next_state=[ 0.06411004 -0.34230307 -0.01336412  0.55412621]\n",
      "[ episode 177 ][ timestamp 33 ] state=[ 0.06411004 -0.34230307 -0.01336412  0.55412621], action=1, reward=1.0, next_state=[ 0.05726397 -0.14699604 -0.00228159  0.25726289]\n",
      "[ episode 177 ][ timestamp 34 ] state=[ 0.05726397 -0.14699604 -0.00228159  0.25726289], action=1, reward=1.0, next_state=[ 0.05432405  0.04815842  0.00286366 -0.03613881]\n",
      "[ episode 177 ][ timestamp 35 ] state=[ 0.05432405  0.04815842  0.00286366 -0.03613881], action=0, reward=1.0, next_state=[ 0.05528722 -0.14700448  0.00214089  0.25744625]\n",
      "[ episode 177 ][ timestamp 36 ] state=[ 0.05528722 -0.14700448  0.00214089  0.25744625], action=1, reward=1.0, next_state=[ 0.05234713  0.04808684  0.00728981 -0.03456064]\n",
      "[ episode 177 ][ timestamp 37 ] state=[ 0.05234713  0.04808684  0.00728981 -0.03456064], action=1, reward=1.0, next_state=[ 0.05330887  0.2431035   0.0065986  -0.32493467]\n",
      "[ episode 177 ][ timestamp 38 ] state=[ 0.05330887  0.2431035   0.0065986  -0.32493467], action=0, reward=1.0, next_state=[ 5.81709384e-02  4.78882149e-02  9.99072304e-05 -3.01781474e-02]\n",
      "[ episode 177 ][ timestamp 39 ] state=[ 5.81709384e-02  4.78882149e-02  9.99072304e-05 -3.01781474e-02], action=1, reward=1.0, next_state=[ 0.0591287   0.24300873 -0.00050366 -0.32282955]\n",
      "[ episode 177 ][ timestamp 40 ] state=[ 0.0591287   0.24300873 -0.00050366 -0.32282955], action=0, reward=1.0, next_state=[ 0.06398888  0.04789396 -0.00696025 -0.0303055 ]\n",
      "[ episode 177 ][ timestamp 41 ] state=[ 0.06398888  0.04789396 -0.00696025 -0.0303055 ], action=0, reward=1.0, next_state=[ 0.06494676 -0.14712749 -0.00756636  0.26017329]\n",
      "[ episode 177 ][ timestamp 42 ] state=[ 0.06494676 -0.14712749 -0.00756636  0.26017329], action=1, reward=1.0, next_state=[ 0.06200421  0.04810165 -0.00236289 -0.03488653]\n",
      "[ episode 177 ][ timestamp 43 ] state=[ 0.06200421  0.04810165 -0.00236289 -0.03488653], action=0, reward=1.0, next_state=[ 0.06296624 -0.14698634 -0.00306062  0.25704995]\n",
      "[ episode 177 ][ timestamp 44 ] state=[ 0.06296624 -0.14698634 -0.00306062  0.25704995], action=1, reward=1.0, next_state=[ 0.06002651  0.04817918  0.00208038 -0.03659677]\n",
      "[ episode 177 ][ timestamp 45 ] state=[ 0.06002651  0.04817918  0.00208038 -0.03659677], action=1, reward=1.0, next_state=[ 0.0609901   0.24327123  0.00134844 -0.32862259]\n",
      "[ episode 177 ][ timestamp 46 ] state=[ 0.0609901   0.24327123  0.00134844 -0.32862259], action=1, reward=1.0, next_state=[ 0.06585552  0.43837396 -0.00522401 -0.62087998]\n",
      "[ episode 177 ][ timestamp 47 ] state=[ 0.06585552  0.43837396 -0.00522401 -0.62087998], action=0, reward=1.0, next_state=[ 0.074623    0.24332535 -0.01764161 -0.32984691]\n",
      "[ episode 177 ][ timestamp 48 ] state=[ 0.074623    0.24332535 -0.01764161 -0.32984691], action=0, reward=1.0, next_state=[ 0.07948951  0.04845892 -0.02423855 -0.04277911]\n",
      "[ episode 177 ][ timestamp 49 ] state=[ 0.07948951  0.04845892 -0.02423855 -0.04277911], action=1, reward=1.0, next_state=[ 0.08045869  0.24391991 -0.02509413 -0.3430099 ]\n",
      "[ episode 177 ][ timestamp 50 ] state=[ 0.08045869  0.24391991 -0.02509413 -0.3430099 ], action=0, reward=1.0, next_state=[ 0.08533708  0.04916379 -0.03195433 -0.05834459]\n",
      "[ episode 177 ][ timestamp 51 ] state=[ 0.08533708  0.04916379 -0.03195433 -0.05834459], action=0, reward=1.0, next_state=[ 0.08632036 -0.14548577 -0.03312122  0.22408772]\n",
      "[ episode 177 ][ timestamp 52 ] state=[ 0.08632036 -0.14548577 -0.03312122  0.22408772], action=1, reward=1.0, next_state=[ 0.08341064  0.05009352 -0.02863947 -0.07885618]\n",
      "[ episode 177 ][ timestamp 53 ] state=[ 0.08341064  0.05009352 -0.02863947 -0.07885618], action=0, reward=1.0, next_state=[ 0.08441251 -0.14460642 -0.03021659  0.20465511]\n",
      "[ episode 177 ][ timestamp 54 ] state=[ 0.08441251 -0.14460642 -0.03021659  0.20465511], action=0, reward=1.0, next_state=[ 0.08152039 -0.33928351 -0.02612349  0.48765513]\n",
      "[ episode 177 ][ timestamp 55 ] state=[ 0.08152039 -0.33928351 -0.02612349  0.48765513], action=0, reward=1.0, next_state=[ 0.07473472 -0.53402732 -0.01637038  0.77199174]\n",
      "[ episode 177 ][ timestamp 56 ] state=[ 0.07473472 -0.53402732 -0.01637038  0.77199174], action=0, reward=1.0, next_state=[ 6.40541698e-02 -7.28920237e-01 -9.30549278e-04  1.05947926e+00]\n",
      "[ episode 177 ][ timestamp 57 ] state=[ 6.40541698e-02 -7.28920237e-01 -9.30549278e-04  1.05947926e+00], action=1, reward=1.0, next_state=[ 0.04947577 -0.53378597  0.02025904  0.76650441]\n",
      "[ episode 177 ][ timestamp 58 ] state=[ 0.04947577 -0.53378597  0.02025904  0.76650441], action=1, reward=1.0, next_state=[ 0.03880005 -0.33894872  0.03558912  0.48026426]\n",
      "[ episode 177 ][ timestamp 59 ] state=[ 0.03880005 -0.33894872  0.03558912  0.48026426], action=1, reward=1.0, next_state=[ 0.03202107 -0.14434676  0.04519441  0.19900715]\n",
      "[ episode 177 ][ timestamp 60 ] state=[ 0.03202107 -0.14434676  0.04519441  0.19900715], action=1, reward=1.0, next_state=[ 0.02913414  0.05010062  0.04917455 -0.07908346]\n",
      "[ episode 177 ][ timestamp 61 ] state=[ 0.02913414  0.05010062  0.04917455 -0.07908346], action=0, reward=1.0, next_state=[ 0.03013615 -0.14569051  0.04759288  0.22869972]\n",
      "[ episode 177 ][ timestamp 62 ] state=[ 0.03013615 -0.14569051  0.04759288  0.22869972], action=1, reward=1.0, next_state=[ 0.02722234  0.04872015  0.05216688 -0.04859904]\n",
      "[ episode 177 ][ timestamp 63 ] state=[ 0.02722234  0.04872015  0.05216688 -0.04859904], action=0, reward=1.0, next_state=[ 0.02819674 -0.14710952  0.0511949   0.26007595]\n",
      "[ episode 177 ][ timestamp 64 ] state=[ 0.02819674 -0.14710952  0.0511949   0.26007595], action=1, reward=1.0, next_state=[ 0.02525455  0.04724564  0.05639642 -0.0160301 ]\n",
      "[ episode 177 ][ timestamp 65 ] state=[ 0.02525455  0.04724564  0.05639642 -0.0160301 ], action=0, reward=1.0, next_state=[ 0.02619946 -0.14863785  0.05607581  0.29389975]\n",
      "[ episode 177 ][ timestamp 66 ] state=[ 0.02619946 -0.14863785  0.05607581  0.29389975], action=1, reward=1.0, next_state=[0.02322671 0.04564165 0.06195381 0.01941622]\n",
      "[ episode 177 ][ timestamp 67 ] state=[0.02322671 0.04564165 0.06195381 0.01941622], action=1, reward=1.0, next_state=[ 0.02413954  0.23982292  0.06234213 -0.2530941 ]\n",
      "[ episode 177 ][ timestamp 68 ] state=[ 0.02413954  0.23982292  0.06234213 -0.2530941 ], action=0, reward=1.0, next_state=[0.028936   0.04386875 0.05728025 0.05858286]\n",
      "[ episode 177 ][ timestamp 69 ] state=[0.028936   0.04386875 0.05728025 0.05858286], action=0, reward=1.0, next_state=[ 0.02981337 -0.15202572  0.05845191  0.36877383]\n",
      "[ episode 177 ][ timestamp 70 ] state=[ 0.02981337 -0.15202572  0.05845191  0.36877383], action=1, reward=1.0, next_state=[0.02677286 0.0422191  0.06582739 0.09507928]\n",
      "[ episode 177 ][ timestamp 71 ] state=[0.02677286 0.0422191  0.06582739 0.09507928], action=1, reward=1.0, next_state=[ 0.02761724  0.23633884  0.06772897 -0.17613039]\n",
      "[ episode 177 ][ timestamp 72 ] state=[ 0.02761724  0.23633884  0.06772897 -0.17613039], action=1, reward=1.0, next_state=[ 0.03234402  0.43042942  0.06420636 -0.44670168]\n",
      "[ episode 177 ][ timestamp 73 ] state=[ 0.03234402  0.43042942  0.06420636 -0.44670168], action=1, reward=1.0, next_state=[ 0.04095261  0.6245871   0.05527233 -0.71847439]\n",
      "[ episode 177 ][ timestamp 74 ] state=[ 0.04095261  0.6245871   0.05527233 -0.71847439], action=0, reward=1.0, next_state=[ 0.05344435  0.42874563  0.04090284 -0.408919  ]\n",
      "[ episode 177 ][ timestamp 75 ] state=[ 0.05344435  0.42874563  0.04090284 -0.408919  ], action=0, reward=1.0, next_state=[ 0.06201926  0.23306834  0.03272446 -0.10362648]\n",
      "[ episode 177 ][ timestamp 76 ] state=[ 0.06201926  0.23306834  0.03272446 -0.10362648], action=0, reward=1.0, next_state=[0.06668063 0.03749306 0.03065193 0.19919863]\n",
      "[ episode 177 ][ timestamp 77 ] state=[0.06668063 0.03749306 0.03065193 0.19919863], action=0, reward=1.0, next_state=[ 0.06743049 -0.15805358  0.0346359   0.50139108]\n",
      "[ episode 177 ][ timestamp 78 ] state=[ 0.06743049 -0.15805358  0.0346359   0.50139108], action=1, reward=1.0, next_state=[0.06426942 0.03656346 0.04466373 0.21982153]\n",
      "[ episode 177 ][ timestamp 79 ] state=[0.06426942 0.03656346 0.04466373 0.21982153], action=1, reward=1.0, next_state=[ 0.06500069  0.23101946  0.04906016 -0.05844482]\n",
      "[ episode 177 ][ timestamp 80 ] state=[ 0.06500069  0.23101946  0.04906016 -0.05844482], action=0, reward=1.0, next_state=[0.06962107 0.03522966 0.04789126 0.24930442]\n",
      "[ episode 177 ][ timestamp 81 ] state=[0.06962107 0.03522966 0.04789126 0.24930442], action=0, reward=1.0, next_state=[ 0.07032567 -0.16054234  0.05287735  0.55670037]\n",
      "[ episode 177 ][ timestamp 82 ] state=[ 0.07032567 -0.16054234  0.05287735  0.55670037], action=1, reward=1.0, next_state=[0.06711482 0.03379894 0.06401136 0.28113458]\n",
      "[ episode 177 ][ timestamp 83 ] state=[0.06711482 0.03379894 0.06401136 0.28113458], action=1, reward=1.0, next_state=[0.0677908  0.22795218 0.06963405 0.00930766]\n",
      "[ episode 177 ][ timestamp 84 ] state=[0.0677908  0.22795218 0.06963405 0.00930766], action=1, reward=1.0, next_state=[ 0.07234984  0.42200997  0.0698202  -0.26061772]\n",
      "[ episode 177 ][ timestamp 85 ] state=[ 0.07234984  0.42200997  0.0698202  -0.26061772], action=1, reward=1.0, next_state=[ 0.08079004  0.61606936  0.06460785 -0.53048712]\n",
      "[ episode 177 ][ timestamp 86 ] state=[ 0.08079004  0.61606936  0.06460785 -0.53048712], action=0, reward=1.0, next_state=[ 0.09311143  0.42010091  0.0539981  -0.21816624]\n",
      "[ episode 177 ][ timestamp 87 ] state=[ 0.09311143  0.42010091  0.0539981  -0.21816624], action=0, reward=1.0, next_state=[0.10151345 0.22425031 0.04963478 0.09104921]\n",
      "[ episode 177 ][ timestamp 88 ] state=[0.10151345 0.22425031 0.04963478 0.09104921], action=1, reward=1.0, next_state=[ 0.10599845  0.41862698  0.05145576 -0.18557009]\n",
      "[ episode 177 ][ timestamp 89 ] state=[ 0.10599845  0.41862698  0.05145576 -0.18557009], action=0, reward=1.0, next_state=[0.11437099 0.22280801 0.04774436 0.12289091]\n",
      "[ episode 177 ][ timestamp 90 ] state=[0.11437099 0.22280801 0.04774436 0.12289091], action=1, reward=1.0, next_state=[ 0.11882715  0.41721458  0.05020218 -0.15435513]\n",
      "[ episode 177 ][ timestamp 91 ] state=[ 0.11882715  0.41721458  0.05020218 -0.15435513], action=0, reward=1.0, next_state=[0.12717145 0.22141111 0.04711508 0.15373329]\n",
      "[ episode 177 ][ timestamp 92 ] state=[0.12717145 0.22141111 0.04711508 0.15373329], action=0, reward=1.0, next_state=[0.13159967 0.02564732 0.05018974 0.46089982]\n",
      "[ episode 177 ][ timestamp 93 ] state=[0.13159967 0.02564732 0.05018974 0.46089982], action=1, reward=1.0, next_state=[0.13211261 0.22002528 0.05940774 0.18444962]\n",
      "[ episode 177 ][ timestamp 94 ] state=[0.13211261 0.22002528 0.05940774 0.18444962], action=0, reward=1.0, next_state=[0.13651312 0.02410585 0.06309673 0.49526594]\n",
      "[ episode 177 ][ timestamp 95 ] state=[0.13651312 0.02410585 0.06309673 0.49526594], action=1, reward=1.0, next_state=[0.13699524 0.2182839  0.07300205 0.2231166 ]\n",
      "[ episode 177 ][ timestamp 96 ] state=[0.13699524 0.2182839  0.07300205 0.2231166 ], action=0, reward=1.0, next_state=[0.14136092 0.02219858 0.07746438 0.53790473]\n",
      "[ episode 177 ][ timestamp 97 ] state=[0.14136092 0.02219858 0.07746438 0.53790473], action=1, reward=1.0, next_state=[0.14180489 0.21615087 0.08822248 0.2706005 ]\n",
      "[ episode 177 ][ timestamp 98 ] state=[0.14180489 0.21615087 0.08822248 0.2706005 ], action=0, reward=1.0, next_state=[0.1461279  0.01988801 0.09363449 0.58975364]\n",
      "[ episode 177 ][ timestamp 99 ] state=[0.1461279  0.01988801 0.09363449 0.58975364], action=1, reward=1.0, next_state=[0.14652566 0.21358277 0.10542956 0.32797255]\n",
      "[ episode 177 ][ timestamp 100 ] state=[0.14652566 0.21358277 0.10542956 0.32797255], action=1, reward=1.0, next_state=[0.15079732 0.40705816 0.11198901 0.0703098 ]\n",
      "[ episode 177 ][ timestamp 101 ] state=[0.15079732 0.40705816 0.11198901 0.0703098 ], action=1, reward=1.0, next_state=[ 0.15893848  0.60041126  0.11339521 -0.18504706]\n",
      "[ episode 177 ][ timestamp 102 ] state=[ 0.15893848  0.60041126  0.11339521 -0.18504706], action=0, reward=1.0, next_state=[0.17094671 0.40386487 0.10969427 0.14114588]\n",
      "[ episode 177 ][ timestamp 103 ] state=[0.17094671 0.40386487 0.10969427 0.14114588], action=1, reward=1.0, next_state=[ 0.17902401  0.59725877  0.11251718 -0.11501594]\n",
      "[ episode 177 ][ timestamp 104 ] state=[ 0.17902401  0.59725877  0.11251718 -0.11501594], action=0, reward=1.0, next_state=[0.19096918 0.40071949 0.11021687 0.21093909]\n",
      "[ episode 177 ][ timestamp 105 ] state=[0.19096918 0.40071949 0.11021687 0.21093909], action=0, reward=1.0, next_state=[0.19898357 0.20420838 0.11443565 0.53625539]\n",
      "[ episode 177 ][ timestamp 106 ] state=[0.19898357 0.20420838 0.11443565 0.53625539], action=1, reward=1.0, next_state=[0.20306774 0.39755103 0.12516075 0.28170899]\n",
      "[ episode 177 ][ timestamp 107 ] state=[0.20306774 0.39755103 0.12516075 0.28170899], action=0, reward=1.0, next_state=[0.21101876 0.20088669 0.13079493 0.61109918]\n",
      "[ episode 177 ][ timestamp 108 ] state=[0.21101876 0.20088669 0.13079493 0.61109918], action=0, reward=1.0, next_state=[0.21503649 0.00420258 0.14301692 0.94194955]\n",
      "[ episode 177 ][ timestamp 109 ] state=[0.21503649 0.00420258 0.14301692 0.94194955], action=1, reward=1.0, next_state=[0.21512054 0.19713791 0.16185591 0.697405  ]\n",
      "[ episode 177 ][ timestamp 110 ] state=[0.21512054 0.19713791 0.16185591 0.697405  ], action=1, reward=1.0, next_state=[0.2190633  0.38968952 0.17580401 0.45973073]\n",
      "[ episode 177 ][ timestamp 111 ] state=[0.2190633  0.38968952 0.17580401 0.45973073], action=1, reward=1.0, next_state=[0.22685709 0.58194755 0.18499862 0.22720936]\n",
      "[ episode 177 ][ timestamp 112 ] state=[0.22685709 0.58194755 0.18499862 0.22720936], action=1, reward=1.0, next_state=[ 0.23849604  0.77401023  0.18954281 -0.00188889]\n",
      "[ episode 177 ][ timestamp 113 ] state=[ 0.23849604  0.77401023  0.18954281 -0.00188889], action=1, reward=1.0, next_state=[ 0.25397625  0.96597987  0.18950503 -0.22929371]\n",
      "[ episode 177 ][ timestamp 114 ] state=[ 0.25397625  0.96597987  0.18950503 -0.22929371], action=1, reward=1.0, next_state=[ 0.27329585  1.15795985  0.18491916 -0.45672671]\n",
      "[ episode 177 ][ timestamp 115 ] state=[ 0.27329585  1.15795985  0.18491916 -0.45672671], action=0, reward=1.0, next_state=[ 0.29645504  0.96077113  0.17578462 -0.11192949]\n",
      "[ episode 177 ][ timestamp 116 ] state=[ 0.29645504  0.96077113  0.17578462 -0.11192949], action=0, reward=1.0, next_state=[0.31567047 0.7636231  0.17354604 0.2306503 ]\n",
      "[ episode 177 ][ timestamp 117 ] state=[0.31567047 0.7636231  0.17354604 0.2306503 ], action=1, reward=1.0, next_state=[ 0.33094293  0.9558953   0.17815904 -0.0026589 ]\n",
      "[ episode 177 ][ timestamp 118 ] state=[ 0.33094293  0.9558953   0.17815904 -0.0026589 ], action=0, reward=1.0, next_state=[0.35006083 0.75872511 0.17810586 0.34051715]\n",
      "[ episode 177 ][ timestamp 119 ] state=[0.35006083 0.75872511 0.17810586 0.34051715], action=0, reward=1.0, next_state=[0.36523534 0.56157536 0.18491621 0.68365044]\n",
      "[ episode 177 ][ timestamp 120 ] state=[0.36523534 0.56157536 0.18491621 0.68365044], action=1, reward=1.0, next_state=[0.37646684 0.75371395 0.19858921 0.45441209]\n",
      "[ episode 177 ][ timestamp 121 ] state=[0.37646684 0.75371395 0.19858921 0.45441209], action=1, reward=1.0, next_state=[0.39154112 0.94555552 0.20767746 0.23030767]\n",
      "[ episode 177 ][ timestamp 122 ] state=[0.39154112 0.94555552 0.20767746 0.23030767], action=1, reward=-1.0, next_state=[0.41045223 1.1371985  0.21228361 0.00963934]\n",
      "[ Ended! ] Episode 177: Exploration_rate=0.41386834584198684. Score=122.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 178 ] state=[-0.03405801 -0.0408816   0.03339777 -0.01129282]\n",
      "[ episode 178 ][ timestamp 1 ] state=[-0.03405801 -0.0408816   0.03339777 -0.01129282], action=1, reward=1.0, next_state=[-0.03487564  0.15374586  0.03317191 -0.29325409]\n",
      "[ episode 178 ][ timestamp 2 ] state=[-0.03487564  0.15374586  0.03317191 -0.29325409], action=0, reward=1.0, next_state=[-0.03180072 -0.04183296  0.02730683  0.00970348]\n",
      "[ episode 178 ][ timestamp 3 ] state=[-0.03180072 -0.04183296  0.02730683  0.00970348], action=1, reward=1.0, next_state=[-0.03263738  0.15288695  0.0275009  -0.27424028]\n",
      "[ episode 178 ][ timestamp 4 ] state=[-0.03263738  0.15288695  0.0275009  -0.27424028], action=1, reward=1.0, next_state=[-0.02957964  0.34760594  0.02201609 -0.55812408]\n",
      "[ episode 178 ][ timestamp 5 ] state=[-0.02957964  0.34760594  0.02201609 -0.55812408], action=1, reward=1.0, next_state=[-0.02262752  0.54241204  0.01085361 -0.84379019]\n",
      "[ episode 178 ][ timestamp 6 ] state=[-0.02262752  0.54241204  0.01085361 -0.84379019], action=1, reward=1.0, next_state=[-0.01177928  0.7373842  -0.00602219 -1.13304031]\n",
      "[ episode 178 ][ timestamp 7 ] state=[-0.01177928  0.7373842  -0.00602219 -1.13304031], action=1, reward=1.0, next_state=[ 0.0029684   0.93258446 -0.028683   -1.4276059 ]\n",
      "[ episode 178 ][ timestamp 8 ] state=[ 0.0029684   0.93258446 -0.028683   -1.4276059 ], action=0, reward=1.0, next_state=[ 0.02162009  0.73782833 -0.05723512 -1.14402352]\n",
      "[ episode 178 ][ timestamp 9 ] state=[ 0.02162009  0.73782833 -0.05723512 -1.14402352], action=0, reward=1.0, next_state=[ 0.03637666  0.54349891 -0.08011559 -0.86982465]\n",
      "[ episode 178 ][ timestamp 10 ] state=[ 0.03637666  0.54349891 -0.08011559 -0.86982465], action=1, reward=1.0, next_state=[ 0.04724664  0.73961395 -0.09751208 -1.18658242]\n",
      "[ episode 178 ][ timestamp 11 ] state=[ 0.04724664  0.73961395 -0.09751208 -1.18658242], action=1, reward=1.0, next_state=[ 0.06203892  0.93585568 -0.12124373 -1.50816978]\n",
      "[ episode 178 ][ timestamp 12 ] state=[ 0.06203892  0.93585568 -0.12124373 -1.50816978], action=0, reward=1.0, next_state=[ 0.08075603  0.74239423 -0.15140712 -1.2556663 ]\n",
      "[ episode 178 ][ timestamp 13 ] state=[ 0.08075603  0.74239423 -0.15140712 -1.2556663 ], action=1, reward=1.0, next_state=[ 0.09560391  0.93909511 -0.17652045 -1.591686  ]\n",
      "[ episode 178 ][ timestamp 14 ] state=[ 0.09560391  0.93909511 -0.17652045 -1.591686  ], action=0, reward=1.0, next_state=[ 0.11438582  0.74645309 -0.20835417 -1.35884118]\n",
      "[ episode 178 ][ timestamp 15 ] state=[ 0.11438582  0.74645309 -0.20835417 -1.35884118], action=1, reward=-1.0, next_state=[ 0.12931488  0.94348793 -0.23553099 -1.70881531]\n",
      "[ Ended! ] Episode 178: Exploration_rate=0.4117990041127769. Score=15.\n",
      "[ Experience replay ] starts\n",
      "[ episode 179 ] state=[ 0.01770708 -0.02460655  0.0040765  -0.02219265]\n",
      "[ episode 179 ][ timestamp 1 ] state=[ 0.01770708 -0.02460655  0.0040765  -0.02219265], action=0, reward=1.0, next_state=[ 0.01721495 -0.21978672  0.00363265  0.27177367]\n",
      "[ episode 179 ][ timestamp 2 ] state=[ 0.01721495 -0.21978672  0.00363265  0.27177367], action=0, reward=1.0, next_state=[ 0.01281921 -0.41496032  0.00906813  0.56560013]\n",
      "[ episode 179 ][ timestamp 3 ] state=[ 0.01281921 -0.41496032  0.00906813  0.56560013], action=1, reward=1.0, next_state=[ 0.00452001 -0.21996676  0.02038013  0.2757878 ]\n",
      "[ episode 179 ][ timestamp 4 ] state=[ 0.00452001 -0.21996676  0.02038013  0.2757878 ], action=0, reward=1.0, next_state=[ 1.20673362e-04 -4.15373458e-01  2.58958837e-02  5.74828327e-01]\n",
      "[ episode 179 ][ timestamp 5 ] state=[ 1.20673362e-04 -4.15373458e-01  2.58958837e-02  5.74828327e-01], action=0, reward=1.0, next_state=[-0.0081868  -0.61084869  0.03739245  0.87555541]\n",
      "[ episode 179 ][ timestamp 6 ] state=[-0.0081868  -0.61084869  0.03739245  0.87555541], action=1, reward=1.0, next_state=[-0.02040377 -0.41625444  0.05490356  0.59485889]\n",
      "[ episode 179 ][ timestamp 7 ] state=[-0.02040377 -0.41625444  0.05490356  0.59485889], action=1, reward=1.0, next_state=[-0.02872886 -0.22194218  0.06680074  0.31996322]\n",
      "[ episode 179 ][ timestamp 8 ] state=[-0.02872886 -0.22194218  0.06680074  0.31996322], action=0, reward=1.0, next_state=[-0.0331677  -0.41794873  0.0732      0.63294212]\n",
      "[ episode 179 ][ timestamp 9 ] state=[-0.0331677  -0.41794873  0.0732      0.63294212], action=1, reward=1.0, next_state=[-0.04152668 -0.22392018  0.08585884  0.36418028]\n",
      "[ episode 179 ][ timestamp 10 ] state=[-0.04152668 -0.22392018  0.08585884  0.36418028], action=1, reward=1.0, next_state=[-0.04600508 -0.03011671  0.09314245  0.09975742]\n",
      "[ episode 179 ][ timestamp 11 ] state=[-0.04600508 -0.03011671  0.09314245  0.09975742], action=0, reward=1.0, next_state=[-0.04660741 -0.22644158  0.0951376   0.42031253]\n",
      "[ episode 179 ][ timestamp 12 ] state=[-0.04660741 -0.22644158  0.0951376   0.42031253], action=1, reward=1.0, next_state=[-0.05113625 -0.03278732  0.10354385  0.15907302]\n",
      "[ episode 179 ][ timestamp 13 ] state=[-0.05113625 -0.03278732  0.10354385  0.15907302], action=0, reward=1.0, next_state=[-0.05179199 -0.22922754  0.10672531  0.48254272]\n",
      "[ episode 179 ][ timestamp 14 ] state=[-0.05179199 -0.22922754  0.10672531  0.48254272], action=1, reward=1.0, next_state=[-0.05637654 -0.03576103  0.11637616  0.22531181]\n",
      "[ episode 179 ][ timestamp 15 ] state=[-0.05637654 -0.03576103  0.11637616  0.22531181], action=0, reward=1.0, next_state=[-0.05709176 -0.23233728  0.1208824   0.55231912]\n",
      "[ episode 179 ][ timestamp 16 ] state=[-0.05709176 -0.23233728  0.1208824   0.55231912], action=1, reward=1.0, next_state=[-0.06173851 -0.03910183  0.13192878  0.30003406]\n",
      "[ episode 179 ][ timestamp 17 ] state=[-0.06173851 -0.03910183  0.13192878  0.30003406], action=0, reward=1.0, next_state=[-0.06252055 -0.23583331  0.13792946  0.63124152]\n",
      "[ episode 179 ][ timestamp 18 ] state=[-0.06252055 -0.23583331  0.13792946  0.63124152], action=1, reward=1.0, next_state=[-0.06723721 -0.04287793  0.15055429  0.38498006]\n",
      "[ episode 179 ][ timestamp 19 ] state=[-0.06723721 -0.04287793  0.15055429  0.38498006], action=1, reward=1.0, next_state=[-0.06809477  0.14982183  0.15825389  0.14329606]\n",
      "[ episode 179 ][ timestamp 20 ] state=[-0.06809477  0.14982183  0.15825389  0.14329606], action=0, reward=1.0, next_state=[-0.06509833 -0.04717086  0.16111981  0.48142534]\n",
      "[ episode 179 ][ timestamp 21 ] state=[-0.06509833 -0.04717086  0.16111981  0.48142534], action=1, reward=1.0, next_state=[-0.06604175  0.14535391  0.17074832  0.24354303]\n",
      "[ episode 179 ][ timestamp 22 ] state=[-0.06604175  0.14535391  0.17074832  0.24354303], action=1, reward=1.0, next_state=[-0.06313467  0.33767792  0.17561918  0.00920864]\n",
      "[ episode 179 ][ timestamp 23 ] state=[-0.06313467  0.33767792  0.17561918  0.00920864], action=0, reward=1.0, next_state=[-0.05638111  0.14052917  0.17580335  0.35175015]\n",
      "[ episode 179 ][ timestamp 24 ] state=[-0.05638111  0.14052917  0.17580335  0.35175015], action=1, reward=1.0, next_state=[-0.05357053  0.3327723   0.18283836  0.11925057]\n",
      "[ episode 179 ][ timestamp 25 ] state=[-0.05357053  0.3327723   0.18283836  0.11925057], action=0, reward=1.0, next_state=[-0.04691509  0.13556578  0.18522337  0.46358514]\n",
      "[ episode 179 ][ timestamp 26 ] state=[-0.04691509  0.13556578  0.18522337  0.46358514], action=0, reward=1.0, next_state=[-0.04420377 -0.06162422  0.19449507  0.8084556 ]\n",
      "[ episode 179 ][ timestamp 27 ] state=[-0.04420377 -0.06162422  0.19449507  0.8084556 ], action=0, reward=-1.0, next_state=[-0.04543625 -0.25880353  0.21066418  1.15546967]\n",
      "[ Ended! ] Episode 179: Exploration_rate=0.40974000909221303. Score=27.\n",
      "[ Experience replay ] starts\n",
      "[ episode 180 ] state=[ 0.0445649   0.02762255  0.01477563 -0.04718067]\n",
      "[ episode 180 ][ timestamp 1 ] state=[ 0.0445649   0.02762255  0.01477563 -0.04718067], action=0, reward=1.0, next_state=[ 0.04511735 -0.16770812  0.01383201  0.25012723]\n",
      "[ episode 180 ][ timestamp 2 ] state=[ 0.04511735 -0.16770812  0.01383201  0.25012723], action=1, reward=1.0, next_state=[ 0.04176319  0.0272136   0.01883456 -0.0381609 ]\n",
      "[ episode 180 ][ timestamp 3 ] state=[ 0.04176319  0.0272136   0.01883456 -0.0381609 ], action=1, reward=1.0, next_state=[ 0.04230746  0.22206047  0.01807134 -0.32484234]\n",
      "[ episode 180 ][ timestamp 4 ] state=[ 0.04230746  0.22206047  0.01807134 -0.32484234], action=1, reward=1.0, next_state=[ 0.04674867  0.41692051  0.01157449 -0.61177199]\n",
      "[ episode 180 ][ timestamp 5 ] state=[ 0.04674867  0.41692051  0.01157449 -0.61177199], action=1, reward=1.0, next_state=[ 5.50870806e-02  6.11878798e-01 -6.60948037e-04 -9.00787007e-01]\n",
      "[ episode 180 ][ timestamp 6 ] state=[ 5.50870806e-02  6.11878798e-01 -6.60948037e-04 -9.00787007e-01], action=0, reward=1.0, next_state=[ 0.06732466  0.41676581 -0.01867669 -0.60831191]\n",
      "[ episode 180 ][ timestamp 7 ] state=[ 0.06732466  0.41676581 -0.01867669 -0.60831191], action=0, reward=1.0, next_state=[ 0.07565997  0.22190988 -0.03084293 -0.32156961]\n",
      "[ episode 180 ][ timestamp 8 ] state=[ 0.07565997  0.22190988 -0.03084293 -0.32156961], action=1, reward=1.0, next_state=[ 0.08009817  0.41745716 -0.03727432 -0.62381742]\n",
      "[ episode 180 ][ timestamp 9 ] state=[ 0.08009817  0.41745716 -0.03727432 -0.62381742], action=0, reward=1.0, next_state=[ 0.08844731  0.22287491 -0.04975067 -0.34310288]\n",
      "[ episode 180 ][ timestamp 10 ] state=[ 0.08844731  0.22287491 -0.04975067 -0.34310288], action=0, reward=1.0, next_state=[ 0.09290481  0.02849474 -0.05661272 -0.06651406]\n",
      "[ episode 180 ][ timestamp 11 ] state=[ 0.09290481  0.02849474 -0.05661272 -0.06651406], action=0, reward=1.0, next_state=[ 0.09347471 -0.16577177 -0.05794301  0.20778361]\n",
      "[ episode 180 ][ timestamp 12 ] state=[ 0.09347471 -0.16577177 -0.05794301  0.20778361], action=0, reward=1.0, next_state=[ 0.09015927 -0.36001937 -0.05378733  0.4816403 ]\n",
      "[ episode 180 ][ timestamp 13 ] state=[ 0.09015927 -0.36001937 -0.05378733  0.4816403 ], action=1, reward=1.0, next_state=[ 0.08295888 -0.1641811  -0.04415453  0.17250188]\n",
      "[ episode 180 ][ timestamp 14 ] state=[ 0.08295888 -0.1641811  -0.04415453  0.17250188], action=0, reward=1.0, next_state=[ 0.07967526 -0.35864419 -0.04070449  0.450935  ]\n",
      "[ episode 180 ][ timestamp 15 ] state=[ 0.07967526 -0.35864419 -0.04070449  0.450935  ], action=1, reward=1.0, next_state=[ 0.07250238 -0.1629709  -0.03168579  0.14570437]\n",
      "[ episode 180 ][ timestamp 16 ] state=[ 0.07250238 -0.1629709  -0.03168579  0.14570437], action=0, reward=1.0, next_state=[ 0.06924296 -0.3576251  -0.0287717   0.42822504]\n",
      "[ episode 180 ][ timestamp 17 ] state=[ 0.06924296 -0.3576251  -0.0287717   0.42822504], action=1, reward=1.0, next_state=[ 0.06209046 -0.16210773 -0.0202072   0.12661266]\n",
      "[ episode 180 ][ timestamp 18 ] state=[ 0.06209046 -0.16210773 -0.0202072   0.12661266], action=0, reward=1.0, next_state=[ 0.0588483  -0.35693446 -0.01767495  0.41285257]\n",
      "[ episode 180 ][ timestamp 19 ] state=[ 0.0588483  -0.35693446 -0.01767495  0.41285257], action=1, reward=1.0, next_state=[ 0.05170961 -0.16156648 -0.0094179   0.11465021]\n",
      "[ episode 180 ][ timestamp 20 ] state=[ 0.05170961 -0.16156648 -0.0094179   0.11465021], action=0, reward=1.0, next_state=[ 0.04847828 -0.35655223 -0.00712489  0.40434705]\n",
      "[ episode 180 ][ timestamp 21 ] state=[ 0.04847828 -0.35655223 -0.00712489  0.40434705], action=0, reward=1.0, next_state=[ 0.04134724 -0.55157241  0.00096205  0.6947752 ]\n",
      "[ episode 180 ][ timestamp 22 ] state=[ 0.04134724 -0.55157241  0.00096205  0.6947752 ], action=0, reward=1.0, next_state=[ 0.03031579 -0.74670769  0.01485755  0.98776083]\n",
      "[ episode 180 ][ timestamp 23 ] state=[ 0.03031579 -0.74670769  0.01485755  0.98776083], action=1, reward=1.0, next_state=[ 0.01538164 -0.5517878   0.03461277  0.69978121]\n",
      "[ episode 180 ][ timestamp 24 ] state=[ 0.01538164 -0.5517878   0.03461277  0.69978121], action=1, reward=1.0, next_state=[ 0.00434588 -0.35716237  0.04860839  0.41819205]\n",
      "[ episode 180 ][ timestamp 25 ] state=[ 0.00434588 -0.35716237  0.04860839  0.41819205], action=1, reward=1.0, next_state=[-0.00279736 -0.16276173  0.05697223  0.14122076]\n",
      "[ episode 180 ][ timestamp 26 ] state=[-0.00279736 -0.16276173  0.05697223  0.14122076], action=1, reward=1.0, next_state=[-0.0060526   0.03149995  0.05979665 -0.1329582 ]\n",
      "[ episode 180 ][ timestamp 27 ] state=[-0.0060526   0.03149995  0.05979665 -0.1329582 ], action=1, reward=1.0, next_state=[-0.0054226   0.22571665  0.05713749 -0.40619283]\n",
      "[ episode 180 ][ timestamp 28 ] state=[-0.0054226   0.22571665  0.05713749 -0.40619283], action=1, reward=1.0, next_state=[-0.00090827  0.41998378  0.04901363 -0.68032871]\n",
      "[ episode 180 ][ timestamp 29 ] state=[-0.00090827  0.41998378  0.04901363 -0.68032871], action=1, reward=1.0, next_state=[ 0.00749141  0.61439191  0.03540705 -0.95718646]\n",
      "[ episode 180 ][ timestamp 30 ] state=[ 0.00749141  0.61439191  0.03540705 -0.95718646], action=0, reward=1.0, next_state=[ 0.01977925  0.41881217  0.01626333 -0.65359322]\n",
      "[ episode 180 ][ timestamp 31 ] state=[ 0.01977925  0.41881217  0.01626333 -0.65359322], action=0, reward=1.0, next_state=[ 0.02815549  0.22346758  0.00319146 -0.35583388]\n",
      "[ episode 180 ][ timestamp 32 ] state=[ 0.02815549  0.22346758  0.00319146 -0.35583388], action=0, reward=1.0, next_state=[ 0.03262484  0.0283004  -0.00392522 -0.06214631]\n",
      "[ episode 180 ][ timestamp 33 ] state=[ 0.03262484  0.0283004  -0.00392522 -0.06214631], action=0, reward=1.0, next_state=[ 0.03319085 -0.16676506 -0.00516814  0.22929561]\n",
      "[ episode 180 ][ timestamp 34 ] state=[ 0.03319085 -0.16676506 -0.00516814  0.22929561], action=1, reward=1.0, next_state=[ 0.02985555  0.02843036 -0.00058223 -0.06501304]\n",
      "[ episode 180 ][ timestamp 35 ] state=[ 0.02985555  0.02843036 -0.00058223 -0.06501304], action=0, reward=1.0, next_state=[ 0.03042416 -0.16668323 -0.00188249  0.22748613]\n",
      "[ episode 180 ][ timestamp 36 ] state=[ 0.03042416 -0.16668323 -0.00188249  0.22748613], action=1, reward=1.0, next_state=[ 0.02709049  0.02846557  0.00266723 -0.06579001]\n",
      "[ episode 180 ][ timestamp 37 ] state=[ 0.02709049  0.02846557  0.00266723 -0.06579001], action=1, reward=1.0, next_state=[ 0.0276598   0.22354918  0.00135143 -0.35763022]\n",
      "[ episode 180 ][ timestamp 38 ] state=[ 0.0276598   0.22354918  0.00135143 -0.35763022], action=0, reward=1.0, next_state=[ 0.03213079  0.02840804 -0.00580117 -0.06452146]\n",
      "[ episode 180 ][ timestamp 39 ] state=[ 0.03213079  0.02840804 -0.00580117 -0.06452146], action=1, reward=1.0, next_state=[ 0.03269895  0.22361268 -0.0070916  -0.35902903]\n",
      "[ episode 180 ][ timestamp 40 ] state=[ 0.03269895  0.22361268 -0.0070916  -0.35902903], action=0, reward=1.0, next_state=[ 0.0371712   0.02859226 -0.01427218 -0.06859066]\n",
      "[ episode 180 ][ timestamp 41 ] state=[ 0.0371712   0.02859226 -0.01427218 -0.06859066], action=0, reward=1.0, next_state=[ 0.03774305 -0.1663222  -0.015644    0.21955537]\n",
      "[ episode 180 ][ timestamp 42 ] state=[ 0.03774305 -0.1663222  -0.015644    0.21955537], action=0, reward=1.0, next_state=[ 0.0344166  -0.36121707 -0.01125289  0.50726276]\n",
      "[ episode 180 ][ timestamp 43 ] state=[ 0.0344166  -0.36121707 -0.01125289  0.50726276], action=1, reward=1.0, next_state=[ 0.02719226 -0.16593839 -0.00110763  0.211055  ]\n",
      "[ episode 180 ][ timestamp 44 ] state=[ 0.02719226 -0.16593839 -0.00110763  0.211055  ], action=0, reward=1.0, next_state=[ 0.02387349 -0.36104448  0.00311347  0.50338832]\n",
      "[ episode 180 ][ timestamp 45 ] state=[ 0.02387349 -0.36104448  0.00311347  0.50338832], action=0, reward=1.0, next_state=[ 0.0166526  -0.55621018  0.01318123  0.7970508 ]\n",
      "[ episode 180 ][ timestamp 46 ] state=[ 0.0166526  -0.55621018  0.01318123  0.7970508 ], action=1, reward=1.0, next_state=[ 0.0055284  -0.36127155  0.02912225  0.50854343]\n",
      "[ episode 180 ][ timestamp 47 ] state=[ 0.0055284  -0.36127155  0.02912225  0.50854343], action=1, reward=1.0, next_state=[-0.00169703 -0.16657175  0.03929312  0.2251783 ]\n",
      "[ episode 180 ][ timestamp 48 ] state=[-0.00169703 -0.16657175  0.03929312  0.2251783 ], action=0, reward=1.0, next_state=[-0.00502847 -0.36223261  0.04379668  0.52999225]\n",
      "[ episode 180 ][ timestamp 49 ] state=[-0.00502847 -0.36223261  0.04379668  0.52999225], action=1, reward=1.0, next_state=[-0.01227312 -0.16775324  0.05439653  0.25142505]\n",
      "[ episode 180 ][ timestamp 50 ] state=[-0.01227312 -0.16775324  0.05439653  0.25142505], action=1, reward=1.0, next_state=[-0.01562818  0.02655145  0.05942503 -0.02361619]\n",
      "[ episode 180 ][ timestamp 51 ] state=[-0.01562818  0.02655145  0.05942503 -0.02361619], action=0, reward=1.0, next_state=[-0.01509716 -0.16937015  0.05895271  0.28720813]\n",
      "[ episode 180 ][ timestamp 52 ] state=[-0.01509716 -0.16937015  0.05895271  0.28720813], action=1, reward=1.0, next_state=[-0.01848456  0.0248637   0.06469687  0.01368555]\n",
      "[ episode 180 ][ timestamp 53 ] state=[-0.01848456  0.0248637   0.06469687  0.01368555], action=0, reward=1.0, next_state=[-0.01798728 -0.17112356  0.06497058  0.32605901]\n",
      "[ episode 180 ][ timestamp 54 ] state=[-0.01798728 -0.17112356  0.06497058  0.32605901], action=1, reward=1.0, next_state=[-0.02140976  0.02301609  0.07149176  0.05455185]\n",
      "[ episode 180 ][ timestamp 55 ] state=[-0.02140976  0.02301609  0.07149176  0.05455185], action=0, reward=1.0, next_state=[-0.02094943 -0.17305425  0.0725828   0.36890678]\n",
      "[ episode 180 ][ timestamp 56 ] state=[-0.02094943 -0.17305425  0.0725828   0.36890678], action=1, reward=1.0, next_state=[-0.02441052  0.02096537  0.07996093  0.09996422]\n",
      "[ episode 180 ][ timestamp 57 ] state=[-0.02441052  0.02096537  0.07996093  0.09996422], action=1, reward=1.0, next_state=[-0.02399121  0.21485571  0.08196022 -0.16645855]\n",
      "[ episode 180 ][ timestamp 58 ] state=[-0.02399121  0.21485571  0.08196022 -0.16645855], action=0, reward=1.0, next_state=[-0.0196941   0.01866202  0.07863104  0.15091343]\n",
      "[ episode 180 ][ timestamp 59 ] state=[-0.0196941   0.01866202  0.07863104  0.15091343], action=0, reward=1.0, next_state=[-0.01932086 -0.17749267  0.08164931  0.46733005]\n",
      "[ episode 180 ][ timestamp 60 ] state=[-0.01932086 -0.17749267  0.08164931  0.46733005], action=1, reward=1.0, next_state=[-0.02287071  0.01638652  0.09099591  0.20145835]\n",
      "[ episode 180 ][ timestamp 61 ] state=[-0.02287071  0.01638652  0.09099591  0.20145835], action=1, reward=1.0, next_state=[-0.02254298  0.21009721  0.09502508 -0.06118964]\n",
      "[ episode 180 ][ timestamp 62 ] state=[-0.02254298  0.21009721  0.09502508 -0.06118964], action=0, reward=1.0, next_state=[-0.01834104  0.01375033  0.09380129  0.2598973 ]\n",
      "[ episode 180 ][ timestamp 63 ] state=[-0.01834104  0.01375033  0.09380129  0.2598973 ], action=0, reward=1.0, next_state=[-0.01806603 -0.18257679  0.09899923  0.58063053]\n",
      "[ episode 180 ][ timestamp 64 ] state=[-0.01806603 -0.18257679  0.09899923  0.58063053], action=1, reward=1.0, next_state=[-0.02171756  0.01102875  0.11061185  0.32070244]\n",
      "[ episode 180 ][ timestamp 65 ] state=[-0.02171756  0.01102875  0.11061185  0.32070244], action=0, reward=1.0, next_state=[-0.02149699 -0.18548035  0.11702589  0.64611832]\n",
      "[ episode 180 ][ timestamp 66 ] state=[-0.02149699 -0.18548035  0.11702589  0.64611832], action=1, reward=1.0, next_state=[-0.0252066   0.00783328  0.12994826  0.39245832]\n",
      "[ episode 180 ][ timestamp 67 ] state=[-0.0252066   0.00783328  0.12994826  0.39245832], action=0, reward=1.0, next_state=[-0.02504993 -0.18887025  0.13779743  0.72312325]\n",
      "[ episode 180 ][ timestamp 68 ] state=[-0.02504993 -0.18887025  0.13779743  0.72312325], action=1, reward=1.0, next_state=[-0.02882734  0.00410406  0.15225989  0.47678996]\n",
      "[ episode 180 ][ timestamp 69 ] state=[-0.02882734  0.00410406  0.15225989  0.47678996], action=0, reward=1.0, next_state=[-0.02874526 -0.19280285  0.16179569  0.81332489]\n",
      "[ episode 180 ][ timestamp 70 ] state=[-0.02874526 -0.19280285  0.16179569  0.81332489], action=1, reward=1.0, next_state=[-3.26013120e-02 -2.22708666e-04  1.78062189e-01  5.75588108e-01]\n",
      "[ episode 180 ][ timestamp 71 ] state=[-3.26013120e-02 -2.22708666e-04  1.78062189e-01  5.75588108e-01], action=0, reward=1.0, next_state=[-0.03260577 -0.19733497  0.18957395  0.9186557 ]\n",
      "[ episode 180 ][ timestamp 72 ] state=[-0.03260577 -0.19733497  0.18957395  0.9186557 ], action=1, reward=1.0, next_state=[-0.03655247 -0.00521115  0.20794706  0.69103444]\n",
      "[ episode 180 ][ timestamp 73 ] state=[-0.03655247 -0.00521115  0.20794706  0.69103444], action=1, reward=-1.0, next_state=[-0.03665669  0.18651202  0.22176775  0.47034195]\n",
      "[ Ended! ] Episode 180: Exploration_rate=0.40769130904675194. Score=73.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 181 ] state=[ 0.01111134 -0.02687508  0.02909443 -0.03170128]\n",
      "[ episode 181 ][ timestamp 1 ] state=[ 0.01111134 -0.02687508  0.02909443 -0.03170128], action=0, reward=1.0, next_state=[ 0.01057383 -0.22240191  0.02846041  0.27001741]\n",
      "[ episode 181 ][ timestamp 2 ] state=[ 0.01057383 -0.22240191  0.02846041  0.27001741], action=1, reward=1.0, next_state=[ 0.0061258  -0.02769742  0.03386076 -0.01355483]\n",
      "[ episode 181 ][ timestamp 3 ] state=[ 0.0061258  -0.02769742  0.03386076 -0.01355483], action=0, reward=1.0, next_state=[ 0.00557185 -0.2232882   0.03358966  0.28961633]\n",
      "[ episode 181 ][ timestamp 4 ] state=[ 0.00557185 -0.2232882   0.03358966  0.28961633], action=1, reward=1.0, next_state=[ 0.00110608 -0.02866093  0.03938199  0.0077136 ]\n",
      "[ episode 181 ][ timestamp 5 ] state=[ 0.00110608 -0.02866093  0.03938199  0.0077136 ], action=0, reward=1.0, next_state=[ 0.00053287 -0.22432489  0.03953626  0.31255729]\n",
      "[ episode 181 ][ timestamp 6 ] state=[ 0.00053287 -0.22432489  0.03953626  0.31255729], action=1, reward=1.0, next_state=[-0.00395363 -0.02978783  0.0457874   0.03260036]\n",
      "[ episode 181 ][ timestamp 7 ] state=[-0.00395363 -0.02978783  0.0457874   0.03260036], action=1, reward=1.0, next_state=[-0.00454939  0.16464862  0.04643941 -0.24529185]\n",
      "[ episode 181 ][ timestamp 8 ] state=[-0.00454939  0.16464862  0.04643941 -0.24529185], action=0, reward=1.0, next_state=[-0.00125642 -0.03110479  0.04153357  0.06166998]\n",
      "[ episode 181 ][ timestamp 9 ] state=[-0.00125642 -0.03110479  0.04153357  0.06166998], action=1, reward=1.0, next_state=[-0.00187851  0.16339781  0.04276697 -0.21762496]\n",
      "[ episode 181 ][ timestamp 10 ] state=[-0.00187851  0.16339781  0.04276697 -0.21762496], action=1, reward=1.0, next_state=[ 0.00138944  0.35788313  0.03841447 -0.49651654]\n",
      "[ episode 181 ][ timestamp 11 ] state=[ 0.00138944  0.35788313  0.03841447 -0.49651654], action=0, reward=1.0, next_state=[ 0.00854711  0.16224115  0.02848414 -0.19197899]\n",
      "[ episode 181 ][ timestamp 12 ] state=[ 0.00854711  0.16224115  0.02848414 -0.19197899], action=0, reward=1.0, next_state=[ 0.01179193 -0.03327645  0.02464456  0.10955166]\n",
      "[ episode 181 ][ timestamp 13 ] state=[ 0.01179193 -0.03327645  0.02464456  0.10955166], action=0, reward=1.0, next_state=[ 0.0111264  -0.22874273  0.0268356   0.4099068 ]\n",
      "[ episode 181 ][ timestamp 14 ] state=[ 0.0111264  -0.22874273  0.0268356   0.4099068 ], action=0, reward=1.0, next_state=[ 0.00655155 -0.42423466  0.03503373  0.71092783]\n",
      "[ episode 181 ][ timestamp 15 ] state=[ 0.00655155 -0.42423466  0.03503373  0.71092783], action=1, reward=1.0, next_state=[-0.00193315 -0.22961493  0.04925229  0.42947519]\n",
      "[ episode 181 ][ timestamp 16 ] state=[-0.00193315 -0.22961493  0.04925229  0.42947519], action=1, reward=1.0, next_state=[-0.00652545 -0.0352238   0.05784179  0.15271641]\n",
      "[ episode 181 ][ timestamp 17 ] state=[-0.00652545 -0.0352238   0.05784179  0.15271641], action=0, reward=1.0, next_state=[-0.00722992 -0.23112422  0.06089612  0.46307162]\n",
      "[ episode 181 ][ timestamp 18 ] state=[-0.00722992 -0.23112422  0.06089612  0.46307162], action=0, reward=1.0, next_state=[-0.01185241 -0.42705152  0.07015755  0.77431021]\n",
      "[ episode 181 ][ timestamp 19 ] state=[-0.01185241 -0.42705152  0.07015755  0.77431021], action=1, reward=1.0, next_state=[-0.02039344 -0.23296122  0.08564376  0.50450036]\n",
      "[ episode 181 ][ timestamp 20 ] state=[-0.02039344 -0.23296122  0.08564376  0.50450036], action=1, reward=1.0, next_state=[-0.02505266 -0.03914406  0.09573376  0.23998867]\n",
      "[ episode 181 ][ timestamp 21 ] state=[-0.02505266 -0.03914406  0.09573376  0.23998867], action=1, reward=1.0, next_state=[-0.02583554  0.15448922  0.10053354 -0.02102853]\n",
      "[ episode 181 ][ timestamp 22 ] state=[-0.02583554  0.15448922  0.10053354 -0.02102853], action=1, reward=1.0, next_state=[-0.02274576  0.34803646  0.10011297 -0.28037639]\n",
      "[ episode 181 ][ timestamp 23 ] state=[-0.02274576  0.34803646  0.10011297 -0.28037639], action=0, reward=1.0, next_state=[-0.01578503  0.15163953  0.09450544  0.042128  ]\n",
      "[ episode 181 ][ timestamp 24 ] state=[-0.01578503  0.15163953  0.09450544  0.042128  ], action=1, reward=1.0, next_state=[-0.01275224  0.34528818  0.095348   -0.21930553]\n",
      "[ episode 181 ][ timestamp 25 ] state=[-0.01275224  0.34528818  0.095348   -0.21930553], action=0, reward=1.0, next_state=[-0.00584647  0.14894176  0.09096189  0.1018662 ]\n",
      "[ episode 181 ][ timestamp 26 ] state=[-0.00584647  0.14894176  0.09096189  0.1018662 ], action=1, reward=1.0, next_state=[-0.00286764  0.34265033  0.09299921 -0.16078949]\n",
      "[ episode 181 ][ timestamp 27 ] state=[-0.00286764  0.34265033  0.09299921 -0.16078949], action=0, reward=1.0, next_state=[0.00398537 0.14632854 0.08978342 0.15972302]\n",
      "[ episode 181 ][ timestamp 28 ] state=[0.00398537 0.14632854 0.08978342 0.15972302], action=1, reward=1.0, next_state=[ 0.00691194  0.34005804  0.09297788 -0.10333989]\n",
      "[ episode 181 ][ timestamp 29 ] state=[ 0.00691194  0.34005804  0.09297788 -0.10333989], action=0, reward=1.0, next_state=[0.0137131  0.14373512 0.09091109 0.21716865]\n",
      "[ episode 181 ][ timestamp 30 ] state=[0.0137131  0.14373512 0.09091109 0.21716865], action=1, reward=1.0, next_state=[ 0.0165878   0.3374478   0.09525446 -0.04550939]\n",
      "[ episode 181 ][ timestamp 31 ] state=[ 0.0165878   0.3374478   0.09525446 -0.04550939], action=0, reward=1.0, next_state=[0.02333676 0.14109816 0.09434427 0.27564238]\n",
      "[ episode 181 ][ timestamp 32 ] state=[0.02333676 0.14109816 0.09434427 0.27564238], action=1, reward=1.0, next_state=[0.02615872 0.33475632 0.09985712 0.01414306]\n",
      "[ episode 181 ][ timestamp 33 ] state=[0.02615872 0.33475632 0.09985712 0.01414306], action=1, reward=1.0, next_state=[ 0.03285385  0.52831494  0.10013998 -0.2454393 ]\n",
      "[ episode 181 ][ timestamp 34 ] state=[ 0.03285385  0.52831494  0.10013998 -0.2454393 ], action=0, reward=1.0, next_state=[0.04342015 0.33191592 0.09523119 0.07707531]\n",
      "[ episode 181 ][ timestamp 35 ] state=[0.04342015 0.33191592 0.09523119 0.07707531], action=1, reward=1.0, next_state=[ 0.05005846  0.52555281  0.0967727  -0.18410827]\n",
      "[ episode 181 ][ timestamp 36 ] state=[ 0.05005846  0.52555281  0.0967727  -0.18410827], action=1, reward=1.0, next_state=[ 0.06056952  0.71916648  0.09309053 -0.44476317]\n",
      "[ episode 181 ][ timestamp 37 ] state=[ 0.06056952  0.71916648  0.09309053 -0.44476317], action=0, reward=1.0, next_state=[ 0.07495285  0.52285923  0.08419527 -0.12424814]\n",
      "[ episode 181 ][ timestamp 38 ] state=[ 0.07495285  0.52285923  0.08419527 -0.12424814], action=1, reward=1.0, next_state=[ 0.08541003  0.71668037  0.08171031 -0.3892258 ]\n",
      "[ episode 181 ][ timestamp 39 ] state=[ 0.08541003  0.71668037  0.08171031 -0.3892258 ], action=0, reward=1.0, next_state=[ 0.09974364  0.52049947  0.07392579 -0.07194017]\n",
      "[ episode 181 ][ timestamp 40 ] state=[ 0.09974364  0.52049947  0.07392579 -0.07194017], action=0, reward=1.0, next_state=[0.11015363 0.32439982 0.07248699 0.2431203 ]\n",
      "[ episode 181 ][ timestamp 41 ] state=[0.11015363 0.32439982 0.07248699 0.2431203 ], action=1, reward=1.0, next_state=[ 0.11664163  0.51841556  0.07734939 -0.02584655]\n",
      "[ episode 181 ][ timestamp 42 ] state=[ 0.11664163  0.51841556  0.07734939 -0.02584655], action=0, reward=1.0, next_state=[0.12700994 0.32227446 0.07683246 0.29020346]\n",
      "[ episode 181 ][ timestamp 43 ] state=[0.12700994 0.32227446 0.07683246 0.29020346], action=0, reward=1.0, next_state=[0.13345543 0.12614581 0.08263653 0.60609505]\n",
      "[ episode 181 ][ timestamp 44 ] state=[0.13345543 0.12614581 0.08263653 0.60609505], action=1, reward=1.0, next_state=[0.13597834 0.32002095 0.09475843 0.34054222]\n",
      "[ episode 181 ][ timestamp 45 ] state=[0.13597834 0.32002095 0.09475843 0.34054222], action=1, reward=1.0, next_state=[0.14237876 0.5136759  0.10156928 0.07918027]\n",
      "[ episode 181 ][ timestamp 46 ] state=[0.14237876 0.5136759  0.10156928 0.07918027], action=1, reward=1.0, next_state=[ 0.15265228  0.70720623  0.10315288 -0.17980907]\n",
      "[ episode 181 ][ timestamp 47 ] state=[ 0.15265228  0.70720623  0.10315288 -0.17980907], action=0, reward=1.0, next_state=[0.16679641 0.51077103 0.0995567  0.14355068]\n",
      "[ episode 181 ][ timestamp 48 ] state=[0.16679641 0.51077103 0.0995567  0.14355068], action=1, reward=1.0, next_state=[ 0.17701183  0.70433669  0.10242772 -0.11613876]\n",
      "[ episode 181 ][ timestamp 49 ] state=[ 0.17701183  0.70433669  0.10242772 -0.11613876], action=0, reward=1.0, next_state=[0.19109856 0.50790764 0.10010494 0.20702167]\n",
      "[ episode 181 ][ timestamp 50 ] state=[0.19109856 0.50790764 0.10010494 0.20702167], action=1, reward=1.0, next_state=[ 0.20125671  0.70146624  0.10424537 -0.05248098]\n",
      "[ episode 181 ][ timestamp 51 ] state=[ 0.20125671  0.70146624  0.10424537 -0.05248098], action=0, reward=1.0, next_state=[0.21528604 0.50501599 0.10319575 0.27118738]\n",
      "[ episode 181 ][ timestamp 52 ] state=[0.21528604 0.50501599 0.10319575 0.27118738], action=0, reward=1.0, next_state=[0.22538636 0.30858446 0.1086195  0.5945529 ]\n",
      "[ episode 181 ][ timestamp 53 ] state=[0.22538636 0.30858446 0.1086195  0.5945529 ], action=1, reward=1.0, next_state=[0.23155805 0.50203191 0.12051056 0.33796316]\n",
      "[ episode 181 ][ timestamp 54 ] state=[0.23155805 0.50203191 0.12051056 0.33796316], action=1, reward=1.0, next_state=[0.24159869 0.69525131 0.12726982 0.08558048]\n",
      "[ episode 181 ][ timestamp 55 ] state=[0.24159869 0.69525131 0.12726982 0.08558048], action=1, reward=1.0, next_state=[ 0.25550371  0.88834097  0.12898143 -0.16439507]\n",
      "[ episode 181 ][ timestamp 56 ] state=[ 0.25550371  0.88834097  0.12898143 -0.16439507], action=0, reward=1.0, next_state=[0.27327053 0.6916312  0.12569353 0.16603409]\n",
      "[ episode 181 ][ timestamp 57 ] state=[0.27327053 0.6916312  0.12569353 0.16603409], action=1, reward=1.0, next_state=[ 0.28710316  0.88475074  0.12901421 -0.08450327]\n",
      "[ episode 181 ][ timestamp 58 ] state=[ 0.28710316  0.88475074  0.12901421 -0.08450327], action=0, reward=1.0, next_state=[0.30479817 0.68803815 0.12732415 0.24593841]\n",
      "[ episode 181 ][ timestamp 59 ] state=[0.30479817 0.68803815 0.12732415 0.24593841], action=1, reward=1.0, next_state=[ 0.31855893  0.88113344  0.13224292 -0.00402768]\n",
      "[ episode 181 ][ timestamp 60 ] state=[ 0.31855893  0.88113344  0.13224292 -0.00402768], action=0, reward=1.0, next_state=[0.3361816  0.68438726 0.13216236 0.32728098]\n",
      "[ episode 181 ][ timestamp 61 ] state=[0.3361816  0.68438726 0.13216236 0.32728098], action=1, reward=1.0, next_state=[0.34986935 0.87740432 0.13870798 0.07902299]\n",
      "[ episode 181 ][ timestamp 62 ] state=[0.34986935 0.87740432 0.13870798 0.07902299], action=0, reward=1.0, next_state=[0.36741743 0.68059465 0.14028844 0.4120516 ]\n",
      "[ episode 181 ][ timestamp 63 ] state=[0.36741743 0.68059465 0.14028844 0.4120516 ], action=1, reward=1.0, next_state=[0.38102933 0.87347817 0.14852947 0.16667838]\n",
      "[ episode 181 ][ timestamp 64 ] state=[0.38102933 0.87347817 0.14852947 0.16667838], action=1, reward=1.0, next_state=[ 0.39849889  1.06619638  0.15186304 -0.07570884]\n",
      "[ episode 181 ][ timestamp 65 ] state=[ 0.39849889  1.06619638  0.15186304 -0.07570884], action=1, reward=1.0, next_state=[ 0.41982282  1.25885196  0.15034886 -0.31688997]\n",
      "[ episode 181 ][ timestamp 66 ] state=[ 0.41982282  1.25885196  0.15034886 -0.31688997], action=0, reward=1.0, next_state=[0.44499986 1.06194405 0.14401107 0.01917611]\n",
      "[ episode 181 ][ timestamp 67 ] state=[0.44499986 1.06194405 0.14401107 0.01917611], action=0, reward=1.0, next_state=[0.46623874 0.86508193 0.14439459 0.35360556]\n",
      "[ episode 181 ][ timestamp 68 ] state=[0.46623874 0.86508193 0.14439459 0.35360556], action=1, reward=1.0, next_state=[0.48354038 1.05788717 0.1514667  0.10971207]\n",
      "[ episode 181 ][ timestamp 69 ] state=[0.48354038 1.05788717 0.1514667  0.10971207], action=0, reward=1.0, next_state=[0.50469812 0.86095588 0.15366094 0.4460881 ]\n",
      "[ episode 181 ][ timestamp 70 ] state=[0.50469812 0.86095588 0.15366094 0.4460881 ], action=1, reward=1.0, next_state=[0.52191724 1.05360811 0.1625827  0.20551341]\n",
      "[ episode 181 ][ timestamp 71 ] state=[0.52191724 1.05360811 0.1625827  0.20551341], action=1, reward=1.0, next_state=[ 0.5429894   1.24607694  0.16669297 -0.03179355]\n",
      "[ episode 181 ][ timestamp 72 ] state=[ 0.5429894   1.24607694  0.16669297 -0.03179355], action=0, reward=1.0, next_state=[0.56791094 1.04900555 0.1660571  0.30849717]\n",
      "[ episode 181 ][ timestamp 73 ] state=[0.56791094 1.04900555 0.1660571  0.30849717], action=0, reward=1.0, next_state=[0.58889105 0.85195493 0.17222704 0.64860394]\n",
      "[ episode 181 ][ timestamp 74 ] state=[0.58889105 0.85195493 0.17222704 0.64860394], action=1, reward=1.0, next_state=[0.60593015 1.0443124  0.18519912 0.41472126]\n",
      "[ episode 181 ][ timestamp 75 ] state=[0.60593015 1.0443124  0.18519912 0.41472126], action=1, reward=1.0, next_state=[0.6268164  1.23639297 0.19349355 0.18566521]\n",
      "[ episode 181 ][ timestamp 76 ] state=[0.6268164  1.23639297 0.19349355 0.18566521], action=1, reward=1.0, next_state=[ 0.65154426  1.42829597  0.19720685 -0.04028468]\n",
      "[ episode 181 ][ timestamp 77 ] state=[ 0.65154426  1.42829597  0.19720685 -0.04028468], action=0, reward=1.0, next_state=[0.68011017 1.23097312 0.19640116 0.30756647]\n",
      "[ episode 181 ][ timestamp 78 ] state=[0.68011017 1.23097312 0.19640116 0.30756647], action=1, reward=1.0, next_state=[0.70472964 1.42283369 0.20255249 0.08267977]\n",
      "[ episode 181 ][ timestamp 79 ] state=[0.70472964 1.42283369 0.20255249 0.08267977], action=1, reward=1.0, next_state=[ 0.73318631  1.61456314  0.20420608 -0.13989085]\n",
      "[ episode 181 ][ timestamp 80 ] state=[ 0.73318631  1.61456314  0.20420608 -0.13989085], action=0, reward=1.0, next_state=[0.76547757 1.41719109 0.20140827 0.20963602]\n",
      "[ episode 181 ][ timestamp 81 ] state=[0.76547757 1.41719109 0.20140827 0.20963602], action=1, reward=1.0, next_state=[ 0.7938214   1.60894909  0.20560099 -0.01337214]\n",
      "[ episode 181 ][ timestamp 82 ] state=[ 0.7938214   1.60894909  0.20560099 -0.01337214], action=1, reward=1.0, next_state=[ 0.82600038  1.80062031  0.20533354 -0.23480191]\n",
      "[ episode 181 ][ timestamp 83 ] state=[ 0.82600038  1.80062031  0.20533354 -0.23480191], action=1, reward=1.0, next_state=[ 0.86201278  1.99230743  0.2006375  -0.45634769]\n",
      "[ episode 181 ][ timestamp 84 ] state=[ 0.86201278  1.99230743  0.2006375  -0.45634769], action=0, reward=1.0, next_state=[ 0.90185893  1.79499856  0.19151055 -0.10772902]\n",
      "[ episode 181 ][ timestamp 85 ] state=[ 0.90185893  1.79499856  0.19151055 -0.10772902], action=0, reward=1.0, next_state=[0.9377589  1.59772186 0.18935597 0.23873662]\n",
      "[ episode 181 ][ timestamp 86 ] state=[0.9377589  1.59772186 0.18935597 0.23873662], action=0, reward=1.0, next_state=[0.96971334 1.40047054 0.1941307  0.58466357]\n",
      "[ episode 181 ][ timestamp 87 ] state=[0.96971334 1.40047054 0.1941307  0.58466357], action=1, reward=1.0, next_state=[0.99772275 1.59241943 0.20582397 0.35886526]\n",
      "[ episode 181 ][ timestamp 88 ] state=[0.99772275 1.59241943 0.20582397 0.35886526], action=1, reward=-1.0, next_state=[1.02957114 1.78411205 0.21300128 0.13748134]\n",
      "[ Ended! ] Episode 181: Exploration_rate=0.40565285250151817. Score=88.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 182 ] state=[ 0.03812662 -0.02905343  0.01954024 -0.04512028]\n",
      "[ episode 182 ][ timestamp 1 ] state=[ 0.03812662 -0.02905343  0.01954024 -0.04512028], action=1, reward=1.0, next_state=[ 0.03754555  0.16578296  0.01863784 -0.3315746 ]\n",
      "[ episode 182 ][ timestamp 2 ] state=[ 0.03754555  0.16578296  0.01863784 -0.3315746 ], action=1, reward=1.0, next_state=[ 0.04086121  0.36063472  0.01200634 -0.61832228]\n",
      "[ episode 182 ][ timestamp 3 ] state=[ 0.04086121  0.36063472  0.01200634 -0.61832228], action=1, reward=1.0, next_state=[ 4.80739080e-02  5.55586924e-01 -3.60101053e-04 -9.07199722e-01]\n",
      "[ episode 182 ][ timestamp 4 ] state=[ 4.80739080e-02  5.55586924e-01 -3.60101053e-04 -9.07199722e-01], action=0, reward=1.0, next_state=[ 0.05918565  0.36046985 -0.0185041  -0.61463   ]\n",
      "[ episode 182 ][ timestamp 5 ] state=[ 0.05918565  0.36046985 -0.0185041  -0.61463   ], action=1, reward=1.0, next_state=[ 0.06639504  0.5558454  -0.0307967  -0.91308305]\n",
      "[ episode 182 ][ timestamp 6 ] state=[ 0.06639504  0.5558454  -0.0307967  -0.91308305], action=1, reward=1.0, next_state=[ 0.07751195  0.75137013 -0.04905836 -1.21528387]\n",
      "[ episode 182 ][ timestamp 7 ] state=[ 0.07751195  0.75137013 -0.04905836 -1.21528387], action=1, reward=1.0, next_state=[ 0.09253935  0.94708943 -0.07336403 -1.52292698]\n",
      "[ episode 182 ][ timestamp 8 ] state=[ 0.09253935  0.94708943 -0.07336403 -1.52292698], action=0, reward=1.0, next_state=[ 0.11148114  0.75292633 -0.10382257 -1.25401544]\n",
      "[ episode 182 ][ timestamp 9 ] state=[ 0.11148114  0.75292633 -0.10382257 -1.25401544], action=0, reward=1.0, next_state=[ 0.12653967  0.55927586 -0.12890288 -0.99557289]\n",
      "[ episode 182 ][ timestamp 10 ] state=[ 0.12653967  0.55927586 -0.12890288 -0.99557289], action=0, reward=1.0, next_state=[ 0.13772519  0.36609142 -0.14881434 -0.74599295]\n",
      "[ episode 182 ][ timestamp 11 ] state=[ 0.13772519  0.36609142 -0.14881434 -0.74599295], action=0, reward=1.0, next_state=[ 0.14504701  0.17330192 -0.1637342  -0.503595  ]\n",
      "[ episode 182 ][ timestamp 12 ] state=[ 0.14504701  0.17330192 -0.1637342  -0.503595  ], action=1, reward=1.0, next_state=[ 0.14851305  0.37030697 -0.1738061  -0.84307336]\n",
      "[ episode 182 ][ timestamp 13 ] state=[ 0.14851305  0.37030697 -0.1738061  -0.84307336], action=1, reward=1.0, next_state=[ 0.15591919  0.56732064 -0.19066757 -1.18498357]\n",
      "[ episode 182 ][ timestamp 14 ] state=[ 0.15591919  0.56732064 -0.19066757 -1.18498357], action=0, reward=-1.0, next_state=[ 0.16726561  0.37511284 -0.21436724 -0.95761391]\n",
      "[ Ended! ] Episode 182: Exploration_rate=0.4036245882390106. Score=14.\n",
      "[ Experience replay ] starts\n",
      "[ episode 183 ] state=[-0.0360021   0.00160784 -0.00931792 -0.01975688]\n",
      "[ episode 183 ][ timestamp 1 ] state=[-0.0360021   0.00160784 -0.00931792 -0.01975688], action=1, reward=1.0, next_state=[-0.03596995  0.19686217 -0.00971305 -0.31536509]\n",
      "[ episode 183 ][ timestamp 2 ] state=[-0.03596995  0.19686217 -0.00971305 -0.31536509], action=1, reward=1.0, next_state=[-0.0320327   0.39212113 -0.01602036 -0.61109529]\n",
      "[ episode 183 ][ timestamp 3 ] state=[-0.0320327   0.39212113 -0.01602036 -0.61109529], action=1, reward=1.0, next_state=[-0.02419028  0.58746329 -0.02824226 -0.90878072]\n",
      "[ episode 183 ][ timestamp 4 ] state=[-0.02419028  0.58746329 -0.02824226 -0.90878072], action=0, reward=1.0, next_state=[-0.01244102  0.39273477 -0.04641788 -0.62510654]\n",
      "[ episode 183 ][ timestamp 5 ] state=[-0.01244102  0.39273477 -0.04641788 -0.62510654], action=0, reward=1.0, next_state=[-0.00458632  0.1982905  -0.05892001 -0.34739626]\n",
      "[ episode 183 ][ timestamp 6 ] state=[-0.00458632  0.1982905  -0.05892001 -0.34739626], action=0, reward=1.0, next_state=[-0.00062051  0.00405395 -0.06586793 -0.07385947]\n",
      "[ episode 183 ][ timestamp 7 ] state=[-0.00062051  0.00405395 -0.06586793 -0.07385947], action=1, reward=1.0, next_state=[-0.00053943  0.20005533 -0.06734512 -0.38657517]\n",
      "[ episode 183 ][ timestamp 8 ] state=[-0.00053943  0.20005533 -0.06734512 -0.38657517], action=1, reward=1.0, next_state=[ 0.00346167  0.39606543 -0.07507663 -0.69970834]\n",
      "[ episode 183 ][ timestamp 9 ] state=[ 0.00346167  0.39606543 -0.07507663 -0.69970834], action=0, reward=1.0, next_state=[ 0.01138298  0.20206018 -0.08907079 -0.43157201]\n",
      "[ episode 183 ][ timestamp 10 ] state=[ 0.01138298  0.20206018 -0.08907079 -0.43157201], action=0, reward=1.0, next_state=[ 0.01542419  0.0083049  -0.09770223 -0.16824342]\n",
      "[ episode 183 ][ timestamp 11 ] state=[ 0.01542419  0.0083049  -0.09770223 -0.16824342], action=1, reward=1.0, next_state=[ 0.01559028  0.20467973 -0.1010671  -0.49007965]\n",
      "[ episode 183 ][ timestamp 12 ] state=[ 0.01559028  0.20467973 -0.1010671  -0.49007965], action=1, reward=1.0, next_state=[ 0.01968388  0.40107135 -0.11086869 -0.812827  ]\n",
      "[ episode 183 ][ timestamp 13 ] state=[ 0.01968388  0.40107135 -0.11086869 -0.812827  ], action=0, reward=1.0, next_state=[ 0.02770531  0.20762838 -0.12712523 -0.5569727 ]\n",
      "[ episode 183 ][ timestamp 14 ] state=[ 0.02770531  0.20762838 -0.12712523 -0.5569727 ], action=0, reward=1.0, next_state=[ 0.03185787  0.01449879 -0.13826469 -0.30689024]\n",
      "[ episode 183 ][ timestamp 15 ] state=[ 0.03185787  0.01449879 -0.13826469 -0.30689024], action=1, reward=1.0, next_state=[ 0.03214785  0.21129229 -0.14440249 -0.63978382]\n",
      "[ episode 183 ][ timestamp 16 ] state=[ 0.03214785  0.21129229 -0.14440249 -0.63978382], action=0, reward=1.0, next_state=[ 0.0363737   0.01844734 -0.15719817 -0.39583402]\n",
      "[ episode 183 ][ timestamp 17 ] state=[ 0.0363737   0.01844734 -0.15719817 -0.39583402], action=1, reward=1.0, next_state=[ 0.03674264  0.21540965 -0.16511485 -0.73366075]\n",
      "[ episode 183 ][ timestamp 18 ] state=[ 0.03674264  0.21540965 -0.16511485 -0.73366075], action=1, reward=1.0, next_state=[ 0.04105084  0.4123811  -0.17978806 -1.07342305]\n",
      "[ episode 183 ][ timestamp 19 ] state=[ 0.04105084  0.4123811  -0.17978806 -1.07342305], action=0, reward=1.0, next_state=[ 0.04929846  0.2200314  -0.20125653 -0.84212244]\n",
      "[ episode 183 ][ timestamp 20 ] state=[ 0.04929846  0.2200314  -0.20125653 -0.84212244], action=1, reward=-1.0, next_state=[ 0.05369909  0.41724741 -0.21809897 -1.19074638]\n",
      "[ Ended! ] Episode 183: Exploration_rate=0.4016064652978155. Score=20.\n",
      "[ Experience replay ] starts\n",
      "[ episode 184 ] state=[-0.02334434 -0.02089062 -0.01269578 -0.04377504]\n",
      "[ episode 184 ][ timestamp 1 ] state=[-0.02334434 -0.02089062 -0.01269578 -0.04377504], action=1, reward=1.0, next_state=[-0.02376215  0.17441106 -0.01357128 -0.34043641]\n",
      "[ episode 184 ][ timestamp 2 ] state=[-0.02376215  0.17441106 -0.01357128 -0.34043641], action=0, reward=1.0, next_state=[-0.02027393 -0.02051519 -0.02038001 -0.0520638 ]\n",
      "[ episode 184 ][ timestamp 3 ] state=[-0.02027393 -0.02051519 -0.02038001 -0.0520638 ], action=1, reward=1.0, next_state=[-0.02068423  0.17489297 -0.02142128 -0.35110647]\n",
      "[ episode 184 ][ timestamp 4 ] state=[-0.02068423  0.17489297 -0.02142128 -0.35110647], action=0, reward=1.0, next_state=[-0.01718637 -0.0199179  -0.02844341 -0.06525459]\n",
      "[ episode 184 ][ timestamp 5 ] state=[-0.01718637 -0.0199179  -0.02844341 -0.06525459], action=1, reward=1.0, next_state=[-0.01758473  0.17560006 -0.02974851 -0.36677414]\n",
      "[ episode 184 ][ timestamp 6 ] state=[-0.01758473  0.17560006 -0.02974851 -0.36677414], action=1, reward=1.0, next_state=[-0.01407273  0.37113184 -0.03708399 -0.6686868 ]\n",
      "[ episode 184 ][ timestamp 7 ] state=[-0.01407273  0.37113184 -0.03708399 -0.6686868 ], action=0, reward=1.0, next_state=[-0.00665009  0.17654464 -0.05045772 -0.38790687]\n",
      "[ episode 184 ][ timestamp 8 ] state=[-0.00665009  0.17654464 -0.05045772 -0.38790687], action=1, reward=1.0, next_state=[-0.0031192   0.37234515 -0.05821586 -0.69606212]\n",
      "[ episode 184 ][ timestamp 9 ] state=[-0.0031192   0.37234515 -0.05821586 -0.69606212], action=0, reward=1.0, next_state=[ 0.0043277   0.17807683 -0.0721371  -0.42225909]\n",
      "[ episode 184 ][ timestamp 10 ] state=[ 0.0043277   0.17807683 -0.0721371  -0.42225909], action=1, reward=1.0, next_state=[ 0.00788924  0.37414269 -0.08058229 -0.73678292]\n",
      "[ episode 184 ][ timestamp 11 ] state=[ 0.00788924  0.37414269 -0.08058229 -0.73678292], action=0, reward=1.0, next_state=[ 0.01537209  0.18022072 -0.09531794 -0.47050944]\n",
      "[ episode 184 ][ timestamp 12 ] state=[ 0.01537209  0.18022072 -0.09531794 -0.47050944], action=1, reward=1.0, next_state=[ 0.01897651  0.37655072 -0.10472813 -0.7916487 ]\n",
      "[ episode 184 ][ timestamp 13 ] state=[ 0.01897651  0.37655072 -0.10472813 -0.7916487 ], action=0, reward=1.0, next_state=[ 0.02650752  0.18301061 -0.12056111 -0.53366295]\n",
      "[ episode 184 ][ timestamp 14 ] state=[ 0.02650752  0.18301061 -0.12056111 -0.53366295], action=1, reward=1.0, next_state=[ 0.03016773  0.37960339 -0.13123437 -0.86177077]\n",
      "[ episode 184 ][ timestamp 15 ] state=[ 0.03016773  0.37960339 -0.13123437 -0.86177077], action=0, reward=1.0, next_state=[ 0.0377598   0.18648915 -0.14846978 -0.61306251]\n",
      "[ episode 184 ][ timestamp 16 ] state=[ 0.0377598   0.18648915 -0.14846978 -0.61306251], action=0, reward=1.0, next_state=[ 0.04148958 -0.00628028 -0.16073103 -0.3705794 ]\n",
      "[ episode 184 ][ timestamp 17 ] state=[ 0.04148958 -0.00628028 -0.16073103 -0.3705794 ], action=1, reward=1.0, next_state=[ 0.04136398  0.19071666 -0.16814262 -0.70931775]\n",
      "[ episode 184 ][ timestamp 18 ] state=[ 0.04136398  0.19071666 -0.16814262 -0.70931775], action=0, reward=1.0, next_state=[ 0.04517831 -0.00172686 -0.18232897 -0.47392477]\n",
      "[ episode 184 ][ timestamp 19 ] state=[ 0.04517831 -0.00172686 -0.18232897 -0.47392477], action=1, reward=1.0, next_state=[ 0.04514377  0.19543835 -0.19180747 -0.81807849]\n",
      "[ episode 184 ][ timestamp 20 ] state=[ 0.04514377  0.19543835 -0.19180747 -0.81807849], action=0, reward=1.0, next_state=[ 0.04905254  0.00338644 -0.20816904 -0.59132986]\n",
      "[ episode 184 ][ timestamp 21 ] state=[ 0.04905254  0.00338644 -0.20816904 -0.59132986], action=0, reward=-1.0, next_state=[ 0.04912027 -0.18830697 -0.21999564 -0.37075809]\n",
      "[ Ended! ] Episode 184: Exploration_rate=0.3995984329713264. Score=21.\n",
      "[ Experience replay ] starts\n",
      "[ episode 185 ] state=[ 0.04632882  0.04150984 -0.04083728  0.02254559]\n",
      "[ episode 185 ][ timestamp 1 ] state=[ 0.04632882  0.04150984 -0.04083728  0.02254559], action=0, reward=1.0, next_state=[ 0.04715902 -0.1530034  -0.04038636  0.30206936]\n",
      "[ episode 185 ][ timestamp 2 ] state=[ 0.04715902 -0.1530034  -0.04038636  0.30206936], action=0, reward=1.0, next_state=[ 0.04409895 -0.34752717 -0.03434498  0.58174673]\n",
      "[ episode 185 ][ timestamp 3 ] state=[ 0.04409895 -0.34752717 -0.03434498  0.58174673], action=1, reward=1.0, next_state=[ 0.03714841 -0.15194126 -0.02271004  0.27844544]\n",
      "[ episode 185 ][ timestamp 4 ] state=[ 0.03714841 -0.15194126 -0.02271004  0.27844544], action=1, reward=1.0, next_state=[ 0.03410958  0.04349718 -0.01714113 -0.0213128 ]\n",
      "[ episode 185 ][ timestamp 5 ] state=[ 0.03410958  0.04349718 -0.01714113 -0.0213128 ], action=0, reward=1.0, next_state=[ 0.03497952 -0.15137481 -0.01756739  0.26591299]\n",
      "[ episode 185 ][ timestamp 6 ] state=[ 0.03497952 -0.15137481 -0.01756739  0.26591299], action=1, reward=1.0, next_state=[ 0.03195203  0.04399341 -0.01224913 -0.03225866]\n",
      "[ episode 185 ][ timestamp 7 ] state=[ 0.03195203  0.04399341 -0.01224913 -0.03225866], action=0, reward=1.0, next_state=[ 0.0328319  -0.15095076 -0.0128943   0.2565345 ]\n",
      "[ episode 185 ][ timestamp 8 ] state=[ 0.0328319  -0.15095076 -0.0128943   0.2565345 ], action=1, reward=1.0, next_state=[ 0.02981288  0.04435289 -0.00776361 -0.04018744]\n",
      "[ episode 185 ][ timestamp 9 ] state=[ 0.02981288  0.04435289 -0.00776361 -0.04018744], action=0, reward=1.0, next_state=[ 0.03069994 -0.15065687 -0.00856736  0.25003591]\n",
      "[ episode 185 ][ timestamp 10 ] state=[ 0.03069994 -0.15065687 -0.00856736  0.25003591], action=0, reward=1.0, next_state=[ 0.0276868  -0.34565544 -0.00356664  0.54000425]\n",
      "[ episode 185 ][ timestamp 11 ] state=[ 0.0276868  -0.34565544 -0.00356664  0.54000425], action=0, reward=1.0, next_state=[ 0.02077369 -0.54072707  0.00723344  0.83156125]\n",
      "[ episode 185 ][ timestamp 12 ] state=[ 0.02077369 -0.54072707  0.00723344  0.83156125], action=0, reward=1.0, next_state=[ 0.00995915 -0.73594713  0.02386467  1.12651029]\n",
      "[ episode 185 ][ timestamp 13 ] state=[ 0.00995915 -0.73594713  0.02386467  1.12651029], action=0, reward=1.0, next_state=[-0.00475979 -0.93137352  0.04639487  1.42658195]\n",
      "[ episode 185 ][ timestamp 14 ] state=[-0.00475979 -0.93137352  0.04639487  1.42658195], action=1, reward=1.0, next_state=[-0.02338726 -0.73685452  0.07492651  1.14875261]\n",
      "[ episode 185 ][ timestamp 15 ] state=[-0.02338726 -0.73685452  0.07492651  1.14875261], action=1, reward=1.0, next_state=[-0.03812435 -0.5427863   0.09790156  0.88047482]\n",
      "[ episode 185 ][ timestamp 16 ] state=[-0.03812435 -0.5427863   0.09790156  0.88047482], action=1, reward=1.0, next_state=[-0.04898008 -0.34912091  0.11551106  0.62010489]\n",
      "[ episode 185 ][ timestamp 17 ] state=[-0.04898008 -0.34912091  0.11551106  0.62010489], action=1, reward=1.0, next_state=[-0.0559625  -0.15578553  0.12791316  0.36591919]\n",
      "[ episode 185 ][ timestamp 18 ] state=[-0.0559625  -0.15578553  0.12791316  0.36591919], action=1, reward=1.0, next_state=[-0.05907821  0.03730865  0.13523154  0.11614821]\n",
      "[ episode 185 ][ timestamp 19 ] state=[-0.05907821  0.03730865  0.13523154  0.11614821], action=0, reward=1.0, next_state=[-0.05833203 -0.15946568  0.13755451  0.44825193]\n",
      "[ episode 185 ][ timestamp 20 ] state=[-0.05833203 -0.15946568  0.13755451  0.44825193], action=1, reward=1.0, next_state=[-0.06152135  0.03346977  0.14651954  0.20189598]\n",
      "[ episode 185 ][ timestamp 21 ] state=[-0.06152135  0.03346977  0.14651954  0.20189598], action=1, reward=1.0, next_state=[-0.06085195  0.22622554  0.15055746 -0.04121689]\n",
      "[ episode 185 ][ timestamp 22 ] state=[-0.06085195  0.22622554  0.15055746 -0.04121689], action=0, reward=1.0, next_state=[-0.05632744  0.02930127  0.14973313  0.29492486]\n",
      "[ episode 185 ][ timestamp 23 ] state=[-0.05632744  0.02930127  0.14973313  0.29492486], action=0, reward=1.0, next_state=[-0.05574142 -0.16760285  0.15563162  0.63083351]\n",
      "[ episode 185 ][ timestamp 24 ] state=[-0.05574142 -0.16760285  0.15563162  0.63083351], action=1, reward=1.0, next_state=[-0.05909347  0.02504448  0.16824829  0.39092627]\n",
      "[ episode 185 ][ timestamp 25 ] state=[-0.05909347  0.02504448  0.16824829  0.39092627], action=0, reward=1.0, next_state=[-0.05859258 -0.17201582  0.17606682  0.73157483]\n",
      "[ episode 185 ][ timestamp 26 ] state=[-0.05859258 -0.17201582  0.17606682  0.73157483], action=1, reward=1.0, next_state=[-0.0620329   0.02029268  0.19069832  0.49906827]\n",
      "[ episode 185 ][ timestamp 27 ] state=[-0.0620329   0.02029268  0.19069832  0.49906827], action=1, reward=1.0, next_state=[-0.06162705  0.2122869   0.20067968  0.27202372]\n",
      "[ episode 185 ][ timestamp 28 ] state=[-0.06162705  0.2122869   0.20067968  0.27202372], action=0, reward=1.0, next_state=[-0.05738131  0.01495168  0.20612016  0.62069077]\n",
      "[ episode 185 ][ timestamp 29 ] state=[-0.05738131  0.01495168  0.20612016  0.62069077], action=1, reward=-1.0, next_state=[-0.05708227  0.20668985  0.21853397  0.39934263]\n",
      "[ Ended! ] Episode 185: Exploration_rate=0.3976004408064698. Score=29.\n",
      "[ Experience replay ] starts\n",
      "[ episode 186 ] state=[-0.04080568  0.00208479 -0.02259522 -0.00050439]\n",
      "[ episode 186 ][ timestamp 1 ] state=[-0.04080568  0.00208479 -0.02259522 -0.00050439], action=0, reward=1.0, next_state=[-0.04076399 -0.19270595 -0.0226053   0.28496471]\n",
      "[ episode 186 ][ timestamp 2 ] state=[-0.04076399 -0.19270595 -0.0226053   0.28496471], action=1, reward=1.0, next_state=[-0.04461811  0.00273099 -0.01690601 -0.01476119]\n",
      "[ episode 186 ][ timestamp 3 ] state=[-0.04461811  0.00273099 -0.01690601 -0.01476119], action=0, reward=1.0, next_state=[-0.04456349 -0.19214448 -0.01720123  0.27254011]\n",
      "[ episode 186 ][ timestamp 4 ] state=[-0.04456349 -0.19214448 -0.01720123  0.27254011], action=1, reward=1.0, next_state=[-0.04840638  0.00321864 -0.01175043 -0.02551813]\n",
      "[ episode 186 ][ timestamp 5 ] state=[-0.04840638  0.00321864 -0.01175043 -0.02551813], action=0, reward=1.0, next_state=[-0.048342   -0.19173285 -0.01226079  0.26343437]\n",
      "[ episode 186 ][ timestamp 6 ] state=[-0.048342   -0.19173285 -0.01226079  0.26343437], action=1, reward=1.0, next_state=[-0.05217666  0.00356194 -0.00699211 -0.03309039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 186 ][ timestamp 7 ] state=[-0.05217666  0.00356194 -0.00699211 -0.03309039], action=0, reward=1.0, next_state=[-0.05210542 -0.19145904 -0.00765391  0.25737828]\n",
      "[ episode 186 ][ timestamp 8 ] state=[-0.05210542 -0.19145904 -0.00765391  0.25737828], action=1, reward=1.0, next_state=[-0.0559346   0.00377134 -0.00250635 -0.03770895]\n",
      "[ episode 186 ][ timestamp 9 ] state=[-0.0559346   0.00377134 -0.00250635 -0.03770895], action=0, reward=1.0, next_state=[-0.05585917 -0.19131458 -0.00326053  0.25418215]\n",
      "[ episode 186 ][ timestamp 10 ] state=[-0.05585917 -0.19131458 -0.00326053  0.25418215], action=1, reward=1.0, next_state=[-0.05968547  0.00385377  0.00182312 -0.03952742]\n",
      "[ episode 186 ][ timestamp 11 ] state=[-0.05968547  0.00385377  0.00182312 -0.03952742], action=0, reward=1.0, next_state=[-0.05960839 -0.19129427  0.00103257  0.25373016]\n",
      "[ episode 186 ][ timestamp 12 ] state=[-0.05960839 -0.19129427  0.00103257  0.25373016], action=1, reward=1.0, next_state=[-0.06343428  0.00381292  0.00610717 -0.0386269 ]\n",
      "[ episode 186 ][ timestamp 13 ] state=[-0.06343428  0.00381292  0.00610717 -0.0386269 ], action=0, reward=1.0, next_state=[-0.06335802 -0.19139607  0.00533463  0.25597663]\n",
      "[ episode 186 ][ timestamp 14 ] state=[-0.06335802 -0.19139607  0.00533463  0.25597663], action=1, reward=1.0, next_state=[-0.06718594  0.00364931  0.01045417 -0.03501891]\n",
      "[ episode 186 ][ timestamp 15 ] state=[-0.06718594  0.00364931  0.01045417 -0.03501891], action=0, reward=1.0, next_state=[-0.06711295 -0.19162099  0.00975379  0.260944  ]\n",
      "[ episode 186 ][ timestamp 16 ] state=[-0.06711295 -0.19162099  0.00975379  0.260944  ], action=1, reward=1.0, next_state=[-0.07094537  0.00336038  0.01497267 -0.02864657]\n",
      "[ episode 186 ][ timestamp 17 ] state=[-0.07094537  0.00336038  0.01497267 -0.02864657], action=0, reward=1.0, next_state=[-0.07087817 -0.19197305  0.01439974  0.26872254]\n",
      "[ episode 186 ][ timestamp 18 ] state=[-0.07087817 -0.19197305  0.01439974  0.26872254], action=1, reward=1.0, next_state=[-0.07471763  0.00294047  0.01977419 -0.01938406]\n",
      "[ episode 186 ][ timestamp 19 ] state=[-0.07471763  0.00294047  0.01977419 -0.01938406], action=0, reward=1.0, next_state=[-0.07465882 -0.1924594   0.01938651  0.27947167]\n",
      "[ episode 186 ][ timestamp 20 ] state=[-0.07465882 -0.1924594   0.01938651  0.27947167], action=1, reward=1.0, next_state=[-0.07850801  0.00238071  0.02497594 -0.0070343 ]\n",
      "[ episode 186 ][ timestamp 21 ] state=[-0.07850801  0.00238071  0.02497594 -0.0070343 ], action=0, reward=1.0, next_state=[-0.07846039 -0.19309036  0.02483525  0.29342303]\n",
      "[ episode 186 ][ timestamp 22 ] state=[-0.07846039 -0.19309036  0.02483525  0.29342303], action=1, reward=1.0, next_state=[-0.0823222   0.00166886  0.03070371  0.0086751 ]\n",
      "[ episode 186 ][ timestamp 23 ] state=[-0.0823222   0.00166886  0.03070371  0.0086751 ], action=1, reward=1.0, next_state=[-0.08228882  0.19633733  0.03087722 -0.2741645 ]\n",
      "[ episode 186 ][ timestamp 24 ] state=[-0.08228882  0.19633733  0.03087722 -0.2741645 ], action=0, reward=1.0, next_state=[-0.07836207  0.00078874  0.02539393  0.02809503]\n",
      "[ episode 186 ][ timestamp 25 ] state=[-0.07836207  0.00078874  0.02539393  0.02809503], action=0, reward=1.0, next_state=[-0.0783463  -0.194688    0.02595583  0.32868061]\n",
      "[ episode 186 ][ timestamp 26 ] state=[-0.0783463  -0.194688    0.02595583  0.32868061], action=1, reward=1.0, next_state=[-8.22400598e-02  5.50079301e-05  3.25294381e-02  4.42946468e-02]\n",
      "[ episode 186 ][ timestamp 27 ] state=[-8.22400598e-02  5.50079301e-05  3.25294381e-02  4.42946468e-02], action=0, reward=1.0, next_state=[-0.08223896 -0.19551793  0.03341533  0.34706083]\n",
      "[ episode 186 ][ timestamp 28 ] state=[-0.08223896 -0.19551793  0.03341533  0.34706083], action=1, reward=1.0, next_state=[-0.08614932 -0.00088682  0.04035655  0.06509942]\n",
      "[ episode 186 ][ timestamp 29 ] state=[-0.08614932 -0.00088682  0.04035655  0.06509942], action=1, reward=1.0, next_state=[-0.08616705  0.19363398  0.04165854 -0.21458261]\n",
      "[ episode 186 ][ timestamp 30 ] state=[-0.08616705  0.19363398  0.04165854 -0.21458261], action=0, reward=1.0, next_state=[-0.08229438 -0.00205801  0.03736688  0.09094478]\n",
      "[ episode 186 ][ timestamp 31 ] state=[-0.08229438 -0.00205801  0.03736688  0.09094478], action=0, reward=1.0, next_state=[-0.08233554 -0.19769508  0.03918578  0.39517884]\n",
      "[ episode 186 ][ timestamp 32 ] state=[-0.08233554 -0.19769508  0.03918578  0.39517884], action=0, reward=1.0, next_state=[-0.08628944 -0.3933505   0.04708936  0.69995434]\n",
      "[ episode 186 ][ timestamp 33 ] state=[-0.08628944 -0.3933505   0.04708936  0.69995434], action=1, reward=1.0, next_state=[-0.09415645 -0.1989119   0.06108844  0.42245891]\n",
      "[ episode 186 ][ timestamp 34 ] state=[-0.09415645 -0.1989119   0.06108844  0.42245891], action=1, reward=1.0, next_state=[-0.09813469 -0.00470621  0.06953762  0.14964258]\n",
      "[ episode 186 ][ timestamp 35 ] state=[-0.09813469 -0.00470621  0.06953762  0.14964258], action=0, reward=1.0, next_state=[-0.09822881 -0.20075145  0.07253047  0.46342733]\n",
      "[ episode 186 ][ timestamp 36 ] state=[-0.09822881 -0.20075145  0.07253047  0.46342733], action=1, reward=1.0, next_state=[-0.10224384 -0.00672542  0.08179902  0.19445875]\n",
      "[ episode 186 ][ timestamp 37 ] state=[-0.10224384 -0.00672542  0.08179902  0.19445875], action=1, reward=1.0, next_state=[-0.10237835  0.18713696  0.08568819 -0.07134039]\n",
      "[ episode 186 ][ timestamp 38 ] state=[-0.10237835  0.18713696  0.08568819 -0.07134039], action=0, reward=1.0, next_state=[-0.09863561 -0.00910229  0.08426139  0.24709998]\n",
      "[ episode 186 ][ timestamp 39 ] state=[-0.09863561 -0.00910229  0.08426139  0.24709998], action=1, reward=1.0, next_state=[-0.09881765  0.1847215   0.08920339 -0.01786066]\n",
      "[ episode 186 ][ timestamp 40 ] state=[-0.09881765  0.1847215   0.08920339 -0.01786066], action=0, reward=1.0, next_state=[-0.09512322 -0.01155897  0.08884617  0.30158047]\n",
      "[ episode 186 ][ timestamp 41 ] state=[-0.09512322 -0.01155897  0.08884617  0.30158047], action=1, reward=1.0, next_state=[-0.0953544   0.18219175  0.09487778  0.03818711]\n",
      "[ episode 186 ][ timestamp 42 ] state=[-0.0953544   0.18219175  0.09487778  0.03818711], action=1, reward=1.0, next_state=[-0.09171057  0.37583414  0.09564152 -0.22311787]\n",
      "[ episode 186 ][ timestamp 43 ] state=[-0.09171057  0.37583414  0.09564152 -0.22311787], action=0, reward=1.0, next_state=[-0.08419388  0.17948455  0.09117917  0.09813624]\n",
      "[ episode 186 ][ timestamp 44 ] state=[-0.08419388  0.17948455  0.09117917  0.09813624], action=0, reward=1.0, next_state=[-0.08060419 -0.01681787  0.09314189  0.41813627]\n",
      "[ episode 186 ][ timestamp 45 ] state=[-0.08060419 -0.01681787  0.09314189  0.41813627], action=1, reward=1.0, next_state=[-0.08094055  0.17686928  0.10150462  0.15620903]\n",
      "[ episode 186 ][ timestamp 46 ] state=[-0.08094055  0.17686928  0.10150462  0.15620903], action=1, reward=1.0, next_state=[-0.07740317  0.37040249  0.1046288  -0.10280543]\n",
      "[ episode 186 ][ timestamp 47 ] state=[-0.07740317  0.37040249  0.1046288  -0.10280543], action=0, reward=1.0, next_state=[-0.06999512  0.1739488   0.10257269  0.22096838]\n",
      "[ episode 186 ][ timestamp 48 ] state=[-0.06999512  0.1739488   0.10257269  0.22096838], action=1, reward=1.0, next_state=[-0.06651614  0.36746643  0.10699206 -0.03767886]\n",
      "[ episode 186 ][ timestamp 49 ] state=[-0.06651614  0.36746643  0.10699206 -0.03767886], action=0, reward=1.0, next_state=[-0.05916681  0.17098583  0.10623848  0.28675245]\n",
      "[ episode 186 ][ timestamp 50 ] state=[-0.05916681  0.17098583  0.10623848  0.28675245], action=1, reward=1.0, next_state=[-0.05574709  0.36444494  0.11197353  0.02937526]\n",
      "[ episode 186 ][ timestamp 51 ] state=[-0.05574709  0.36444494  0.11197353  0.02937526], action=0, reward=1.0, next_state=[-0.0484582   0.16791017  0.11256103  0.3551827 ]\n",
      "[ episode 186 ][ timestamp 52 ] state=[-0.0484582   0.16791017  0.11256103  0.3551827 ], action=0, reward=1.0, next_state=[-0.04509999 -0.02861722  0.11966469  0.68113136]\n",
      "[ episode 186 ][ timestamp 53 ] state=[-0.04509999 -0.02861722  0.11966469  0.68113136], action=1, reward=1.0, next_state=[-0.04567234  0.16465739  0.13328732  0.4283902 ]\n",
      "[ episode 186 ][ timestamp 54 ] state=[-0.04567234  0.16465739  0.13328732  0.4283902 ], action=1, reward=1.0, next_state=[-0.04237919  0.35766474  0.14185512  0.18051756]\n",
      "[ episode 186 ][ timestamp 55 ] state=[-0.04237919  0.35766474  0.14185512  0.18051756], action=1, reward=1.0, next_state=[-0.03522589  0.55050193  0.14546547 -0.06426709]\n",
      "[ episode 186 ][ timestamp 56 ] state=[-0.03522589  0.55050193  0.14546547 -0.06426709], action=0, reward=1.0, next_state=[-0.02421586  0.35362636  0.14418013  0.27054351]\n",
      "[ episode 186 ][ timestamp 57 ] state=[-0.02421586  0.35362636  0.14418013  0.27054351], action=1, reward=1.0, next_state=[-0.01714333  0.54642812  0.149591    0.02658388]\n",
      "[ episode 186 ][ timestamp 58 ] state=[-0.01714333  0.54642812  0.149591    0.02658388], action=1, reward=1.0, next_state=[-0.00621477  0.73912351  0.15012268 -0.2154153 ]\n",
      "[ episode 186 ][ timestamp 59 ] state=[-0.00621477  0.73912351  0.15012268 -0.2154153 ], action=0, reward=1.0, next_state=[0.0085677  0.54220984 0.14581437 0.12060357]\n",
      "[ episode 186 ][ timestamp 60 ] state=[0.0085677  0.54220984 0.14581437 0.12060357], action=0, reward=1.0, next_state=[0.0194119  0.3453324  0.14822644 0.45550348]\n",
      "[ episode 186 ][ timestamp 61 ] state=[0.0194119  0.3453324  0.14822644 0.45550348], action=1, reward=1.0, next_state=[0.02631855 0.53808185 0.15733651 0.21296884]\n",
      "[ episode 186 ][ timestamp 62 ] state=[0.02631855 0.53808185 0.15733651 0.21296884], action=1, reward=1.0, next_state=[ 0.03708019  0.73064543  0.16159589 -0.02624242]\n",
      "[ episode 186 ][ timestamp 63 ] state=[ 0.03708019  0.73064543  0.16159589 -0.02624242], action=0, reward=1.0, next_state=[0.05169309 0.5336194  0.16107104 0.31274897]\n",
      "[ episode 186 ][ timestamp 64 ] state=[0.05169309 0.5336194  0.16107104 0.31274897], action=1, reward=1.0, next_state=[0.06236548 0.72612412 0.16732602 0.07487993]\n",
      "[ episode 186 ][ timestamp 65 ] state=[0.06236548 0.72612412 0.16732602 0.07487993], action=0, reward=1.0, next_state=[0.07688796 0.5290479  0.16882362 0.41533021]\n",
      "[ episode 186 ][ timestamp 66 ] state=[0.07688796 0.5290479  0.16882362 0.41533021], action=1, reward=1.0, next_state=[0.08746892 0.72142529 0.17713022 0.18026534]\n",
      "[ episode 186 ][ timestamp 67 ] state=[0.08746892 0.72142529 0.17713022 0.18026534], action=1, reward=1.0, next_state=[ 0.10189743  0.91362867  0.18073553 -0.05172434]\n",
      "[ episode 186 ][ timestamp 68 ] state=[ 0.10189743  0.91362867  0.18073553 -0.05172434], action=0, reward=1.0, next_state=[0.12017    0.71643746 0.17970104 0.29209205]\n",
      "[ episode 186 ][ timestamp 69 ] state=[0.12017    0.71643746 0.17970104 0.29209205], action=1, reward=1.0, next_state=[0.13449875 0.90860305 0.18554288 0.06103351]\n",
      "[ episode 186 ][ timestamp 70 ] state=[0.13449875 0.90860305 0.18554288 0.06103351], action=1, reward=1.0, next_state=[ 0.15267081  1.10064713  0.18676355 -0.16785117]\n",
      "[ episode 186 ][ timestamp 71 ] state=[ 0.15267081  1.10064713  0.18676355 -0.16785117], action=0, reward=1.0, next_state=[0.17468375 0.90341124 0.18340653 0.1774477 ]\n",
      "[ episode 186 ][ timestamp 72 ] state=[0.17468375 0.90341124 0.18340653 0.1774477 ], action=1, reward=1.0, next_state=[ 0.19275198  1.09549956  0.18695548 -0.05223252]\n",
      "[ episode 186 ][ timestamp 73 ] state=[ 0.19275198  1.09549956  0.18695548 -0.05223252], action=0, reward=1.0, next_state=[0.21466197 0.89825752 0.18591083 0.2931203 ]\n",
      "[ episode 186 ][ timestamp 74 ] state=[0.21466197 0.89825752 0.18591083 0.2931203 ], action=1, reward=1.0, next_state=[0.23262712 1.09030962 0.19177324 0.06434971]\n",
      "[ episode 186 ][ timestamp 75 ] state=[0.23262712 1.09030962 0.19177324 0.06434971], action=1, reward=1.0, next_state=[ 0.25443331  1.28223879  0.19306023 -0.16222994]\n",
      "[ episode 186 ][ timestamp 76 ] state=[ 0.25443331  1.28223879  0.19306023 -0.16222994], action=0, reward=1.0, next_state=[0.28007809 1.08495268 0.18981563 0.18460913]\n",
      "[ episode 186 ][ timestamp 77 ] state=[0.28007809 1.08495268 0.18981563 0.18460913], action=1, reward=1.0, next_state=[ 0.30177714  1.27692353  0.19350782 -0.0427039 ]\n",
      "[ episode 186 ][ timestamp 78 ] state=[ 0.30177714  1.27692353  0.19350782 -0.0427039 ], action=0, reward=1.0, next_state=[0.32731561 1.07962935 0.19265374 0.30425075]\n",
      "[ episode 186 ][ timestamp 79 ] state=[0.32731561 1.07962935 0.19265374 0.30425075], action=1, reward=1.0, next_state=[0.3489082  1.2715587  0.19873875 0.07797336]\n",
      "[ episode 186 ][ timestamp 80 ] state=[0.3489082  1.2715587  0.19873875 0.07797336], action=1, reward=1.0, next_state=[ 0.37433937  1.46335903  0.20029822 -0.14601882]\n",
      "[ episode 186 ][ timestamp 81 ] state=[ 0.37433937  1.46335903  0.20029822 -0.14601882], action=0, reward=1.0, next_state=[0.40360655 1.26601647 0.19737785 0.20257161]\n",
      "[ episode 186 ][ timestamp 82 ] state=[0.40360655 1.26601647 0.19737785 0.20257161], action=1, reward=1.0, next_state=[ 0.42892688  1.45784876  0.20142928 -0.02193691]\n",
      "[ episode 186 ][ timestamp 83 ] state=[ 0.42892688  1.45784876  0.20142928 -0.02193691], action=1, reward=1.0, next_state=[ 0.45808386  1.64959791  0.20099054 -0.2449248 ]\n",
      "[ episode 186 ][ timestamp 84 ] state=[ 0.45808386  1.64959791  0.20099054 -0.2449248 ], action=1, reward=1.0, next_state=[ 0.49107582  1.8413668   0.19609204 -0.4680933 ]\n",
      "[ episode 186 ][ timestamp 85 ] state=[ 0.49107582  1.8413668   0.19609204 -0.4680933 ], action=0, reward=1.0, next_state=[ 0.52790315  1.64409375  0.18673018 -0.12057239]\n",
      "[ episode 186 ][ timestamp 86 ] state=[ 0.52790315  1.64409375  0.18673018 -0.12057239], action=0, reward=1.0, next_state=[0.56078503 1.44685567 0.18431873 0.22472191]\n",
      "[ episode 186 ][ timestamp 87 ] state=[0.56078503 1.44685567 0.18431873 0.22472191], action=1, reward=1.0, next_state=[ 0.58972214  1.63893062  0.18881317 -0.0046269 ]\n",
      "[ episode 186 ][ timestamp 88 ] state=[ 0.58972214  1.63893062  0.18881317 -0.0046269 ], action=0, reward=1.0, next_state=[0.62250075 1.44167319 0.18872063 0.34118249]\n",
      "[ episode 186 ][ timestamp 89 ] state=[0.62250075 1.44167319 0.18872063 0.34118249], action=1, reward=1.0, next_state=[0.65133422 1.63367927 0.19554428 0.11344207]\n",
      "[ episode 186 ][ timestamp 90 ] state=[0.65133422 1.63367927 0.19554428 0.11344207], action=0, reward=1.0, next_state=[0.6840078  1.43637135 0.19781312 0.46088786]\n",
      "[ episode 186 ][ timestamp 91 ] state=[0.6840078  1.43637135 0.19781312 0.46088786], action=1, reward=1.0, next_state=[0.71273523 1.62822834 0.20703088 0.23649309]\n",
      "[ episode 186 ][ timestamp 92 ] state=[0.71273523 1.62822834 0.20703088 0.23649309], action=0, reward=-1.0, next_state=[0.7452998  1.43084284 0.21176074 0.58668194]\n",
      "[ Ended! ] Episode 186: Exploration_rate=0.39561243860243744. Score=92.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 187 ] state=[-0.02295138  0.02522588  0.01809803 -0.04789758]\n",
      "[ episode 187 ][ timestamp 1 ] state=[-0.02295138  0.02522588  0.01809803 -0.04789758], action=0, reward=1.0, next_state=[-0.02244686 -0.17015085  0.01714008  0.25044004]\n",
      "[ episode 187 ][ timestamp 2 ] state=[-0.02244686 -0.17015085  0.01714008  0.25044004], action=1, reward=1.0, next_state=[-0.02584988  0.0247222   0.02214888 -0.03678765]\n",
      "[ episode 187 ][ timestamp 3 ] state=[-0.02584988  0.0247222   0.02214888 -0.03678765], action=1, reward=1.0, next_state=[-0.02535543  0.21951964  0.02141313 -0.32240091]\n",
      "[ episode 187 ][ timestamp 4 ] state=[-0.02535543  0.21951964  0.02141313 -0.32240091], action=0, reward=1.0, next_state=[-0.02096504  0.02409942  0.01496511 -0.02304279]\n",
      "[ episode 187 ][ timestamp 5 ] state=[-0.02096504  0.02409942  0.01496511 -0.02304279], action=1, reward=1.0, next_state=[-0.02048305  0.21900359  0.01450425 -0.31096674]\n",
      "[ episode 187 ][ timestamp 6 ] state=[-0.02048305  0.21900359  0.01450425 -0.31096674], action=0, reward=1.0, next_state=[-0.01610298  0.02367803  0.00828492 -0.01374512]\n",
      "[ episode 187 ][ timestamp 7 ] state=[-0.01610298  0.02367803  0.00828492 -0.01374512], action=0, reward=1.0, next_state=[-0.01562942 -0.17156175  0.00801002  0.28154024]\n",
      "[ episode 187 ][ timestamp 8 ] state=[-0.01562942 -0.17156175  0.00801002  0.28154024], action=1, reward=1.0, next_state=[-0.01906065  0.02344504  0.01364082 -0.00860563]\n",
      "[ episode 187 ][ timestamp 9 ] state=[-0.01906065  0.02344504  0.01364082 -0.00860563], action=0, reward=1.0, next_state=[-0.01859175 -0.17186986  0.01346871  0.28834973]\n",
      "[ episode 187 ][ timestamp 10 ] state=[-0.01859175 -0.17186986  0.01346871  0.28834973], action=1, reward=1.0, next_state=[-2.20291495e-02  2.30574605e-02  1.92357032e-02 -5.50471933e-05]\n",
      "[ episode 187 ][ timestamp 11 ] state=[-2.20291495e-02  2.30574605e-02  1.92357032e-02 -5.50471933e-05], action=1, reward=1.0, next_state=[-0.021568    0.21789834  0.0192346  -0.28660735]\n",
      "[ episode 187 ][ timestamp 12 ] state=[-0.021568    0.21789834  0.0192346  -0.28660735], action=1, reward=1.0, next_state=[-0.01721003  0.41274077  0.01350246 -0.57316231]\n",
      "[ episode 187 ][ timestamp 13 ] state=[-0.01721003  0.41274077  0.01350246 -0.57316231], action=0, reward=1.0, next_state=[-0.00895522  0.21743213  0.00203921 -0.27625645]\n",
      "[ episode 187 ][ timestamp 14 ] state=[-0.00895522  0.21743213  0.00203921 -0.27625645], action=0, reward=1.0, next_state=[-0.00460658  0.02228115 -0.00348592  0.01706894]\n",
      "[ episode 187 ][ timestamp 15 ] state=[-0.00460658  0.02228115 -0.00348592  0.01706894], action=1, reward=1.0, next_state=[-0.00416095  0.21745292 -0.00314454 -0.27671179]\n",
      "[ episode 187 ][ timestamp 16 ] state=[-0.00416095  0.21745292 -0.00314454 -0.27671179], action=1, reward=1.0, next_state=[ 1.88105783e-04  4.12619588e-01 -8.67877695e-03 -5.70384847e-01]\n",
      "[ episode 187 ][ timestamp 17 ] state=[ 1.88105783e-04  4.12619588e-01 -8.67877695e-03 -5.70384847e-01], action=0, reward=1.0, next_state=[ 0.0084405   0.21762042 -0.02008647 -0.28044863]\n",
      "[ episode 187 ][ timestamp 18 ] state=[ 0.0084405   0.21762042 -0.02008647 -0.28044863], action=0, reward=1.0, next_state=[ 0.01279291  0.02279067 -0.02569545  0.00583201]\n",
      "[ episode 187 ][ timestamp 19 ] state=[ 0.01279291  0.02279067 -0.02569545  0.00583201], action=0, reward=1.0, next_state=[ 0.01324872 -0.17195353 -0.02557881  0.29029825]\n",
      "[ episode 187 ][ timestamp 20 ] state=[ 0.01324872 -0.17195353 -0.02557881  0.29029825], action=1, reward=1.0, next_state=[ 0.00980965  0.02352364 -0.01977284 -0.01034094]\n",
      "[ episode 187 ][ timestamp 21 ] state=[ 0.00980965  0.02352364 -0.01977284 -0.01034094], action=0, reward=1.0, next_state=[ 0.01028012 -0.17130924 -0.01997966  0.27603842]\n",
      "[ episode 187 ][ timestamp 22 ] state=[ 0.01028012 -0.17130924 -0.01997966  0.27603842], action=1, reward=1.0, next_state=[ 0.00685394  0.02409198 -0.01445889 -0.02287854]\n",
      "[ episode 187 ][ timestamp 23 ] state=[ 0.00685394  0.02409198 -0.01445889 -0.02287854], action=0, reward=1.0, next_state=[ 0.00733578 -0.17081967 -0.01491646  0.2652076 ]\n",
      "[ episode 187 ][ timestamp 24 ] state=[ 0.00733578 -0.17081967 -0.01491646  0.2652076 ], action=1, reward=1.0, next_state=[ 0.00391938  0.02451197 -0.00961231 -0.03214254]\n",
      "[ episode 187 ][ timestamp 25 ] state=[ 0.00391938  0.02451197 -0.00961231 -0.03214254], action=0, reward=1.0, next_state=[ 0.00440962 -0.17047082 -0.01025516  0.25749216]\n",
      "[ episode 187 ][ timestamp 26 ] state=[ 0.00440962 -0.17047082 -0.01025516  0.25749216], action=1, reward=1.0, next_state=[ 0.00100021  0.02479603 -0.00510532 -0.03840767]\n",
      "[ episode 187 ][ timestamp 27 ] state=[ 0.00100021  0.02479603 -0.00510532 -0.03840767], action=0, reward=1.0, next_state=[ 0.00149613 -0.17025234 -0.00587347  0.25266011]\n",
      "[ episode 187 ][ timestamp 28 ] state=[ 0.00149613 -0.17025234 -0.00587347  0.25266011], action=1, reward=1.0, next_state=[-0.00190892  0.02495298 -0.00082027 -0.04186961]\n",
      "[ episode 187 ][ timestamp 29 ] state=[-0.00190892  0.02495298 -0.00082027 -0.04186961], action=0, reward=1.0, next_state=[-0.00140986 -0.1701572  -0.00165766  0.2505544 ]\n",
      "[ episode 187 ][ timestamp 30 ] state=[-0.00140986 -0.1701572  -0.00165766  0.2505544 ], action=1, reward=1.0, next_state=[-0.00481301  0.02498838  0.00335343 -0.04265093]\n",
      "[ episode 187 ][ timestamp 31 ] state=[-0.00481301  0.02498838  0.00335343 -0.04265093], action=0, reward=1.0, next_state=[-0.00431324 -0.17018149  0.00250041  0.25108815]\n",
      "[ episode 187 ][ timestamp 32 ] state=[-0.00431324 -0.17018149  0.00250041  0.25108815], action=0, reward=1.0, next_state=[-0.00771687 -0.36533906  0.00752217  0.5445587 ]\n",
      "[ episode 187 ][ timestamp 33 ] state=[-0.00771687 -0.36533906  0.00752217  0.5445587 ], action=1, reward=1.0, next_state=[-0.01502365 -0.17032361  0.01841334  0.25425531]\n",
      "[ episode 187 ][ timestamp 34 ] state=[-0.01502365 -0.17032361  0.01841334  0.25425531], action=1, reward=1.0, next_state=[-0.01843012  0.02453065  0.02349845 -0.03256332]\n",
      "[ episode 187 ][ timestamp 35 ] state=[-0.01843012  0.02453065  0.02349845 -0.03256332], action=0, reward=1.0, next_state=[-0.01793951 -0.17092026  0.02284718  0.26744001]\n",
      "[ episode 187 ][ timestamp 36 ] state=[-0.01793951 -0.17092026  0.02284718  0.26744001], action=0, reward=1.0, next_state=[-0.02135791 -0.3663607   0.02819598  0.56724065]\n",
      "[ episode 187 ][ timestamp 37 ] state=[-0.02135791 -0.3663607   0.02819598  0.56724065], action=1, reward=1.0, next_state=[-0.02868513 -0.17164538  0.0395408   0.28357228]\n",
      "[ episode 187 ][ timestamp 38 ] state=[-0.02868513 -0.17164538  0.0395408   0.28357228], action=1, reward=1.0, next_state=[-0.03211803  0.02289094  0.04521224  0.00361784]\n",
      "[ episode 187 ][ timestamp 39 ] state=[-0.03211803  0.02289094  0.04521224  0.00361784], action=0, reward=1.0, next_state=[-0.03166022 -0.17284927  0.0452846   0.310216  ]\n",
      "[ episode 187 ][ timestamp 40 ] state=[-0.03166022 -0.17284927  0.0452846   0.310216  ], action=0, reward=1.0, next_state=[-0.0351172  -0.36858619  0.05148892  0.6168295 ]\n",
      "[ episode 187 ][ timestamp 41 ] state=[-0.0351172  -0.36858619  0.05148892  0.6168295 ], action=1, reward=1.0, next_state=[-0.04248892 -0.17421992  0.06382551  0.34079754]\n",
      "[ episode 187 ][ timestamp 42 ] state=[-0.04248892 -0.17421992  0.06382551  0.34079754], action=1, reward=1.0, next_state=[-0.04597332  0.01993859  0.07064146  0.06890473]\n",
      "[ episode 187 ][ timestamp 43 ] state=[-0.04597332  0.01993859  0.07064146  0.06890473], action=0, reward=1.0, next_state=[-0.04557455 -0.1761213   0.07201956  0.3830124 ]\n",
      "[ episode 187 ][ timestamp 44 ] state=[-0.04557455 -0.1761213   0.07201956  0.3830124 ], action=1, reward=1.0, next_state=[-0.04909698  0.01790814  0.0796798   0.11387816]\n",
      "[ episode 187 ][ timestamp 45 ] state=[-0.04909698  0.01790814  0.0796798   0.11387816], action=1, reward=1.0, next_state=[-0.04873881  0.21180332  0.08195737 -0.15264075]\n",
      "[ episode 187 ][ timestamp 46 ] state=[-0.04873881  0.21180332  0.08195737 -0.15264075], action=0, reward=1.0, next_state=[-0.04450275  0.01560931  0.07890455  0.16473094]\n",
      "[ episode 187 ][ timestamp 47 ] state=[-0.04450275  0.01560931  0.07890455  0.16473094], action=1, reward=1.0, next_state=[-0.04419056  0.20951828  0.08219917 -0.10205366]\n",
      "[ episode 187 ][ timestamp 48 ] state=[-0.04419056  0.20951828  0.08219917 -0.10205366], action=0, reward=1.0, next_state=[-0.0400002   0.01332039  0.0801581   0.21538885]\n",
      "[ episode 187 ][ timestamp 49 ] state=[-0.0400002   0.01332039  0.0801581   0.21538885], action=1, reward=1.0, next_state=[-0.03973379  0.20721034  0.08446587 -0.05097097]\n",
      "[ episode 187 ][ timestamp 50 ] state=[-0.03973379  0.20721034  0.08446587 -0.05097097], action=1, reward=1.0, next_state=[-0.03558958  0.40102596  0.08344645 -0.31585448]\n",
      "[ episode 187 ][ timestamp 51 ] state=[-0.03558958  0.40102596  0.08344645 -0.31585448], action=0, reward=1.0, next_state=[-0.02756906  0.20482066  0.07712936  0.00193418]\n",
      "[ episode 187 ][ timestamp 52 ] state=[-0.02756906  0.20482066  0.07712936  0.00193418], action=1, reward=1.0, next_state=[-0.02347265  0.39875659  0.07716805 -0.26545131]\n",
      "[ episode 187 ][ timestamp 53 ] state=[-0.02347265  0.39875659  0.07716805 -0.26545131], action=0, reward=1.0, next_state=[-0.01549752  0.20262292  0.07185902  0.05053856]\n",
      "[ episode 187 ][ timestamp 54 ] state=[-0.01549752  0.20262292  0.07185902  0.05053856], action=0, reward=1.0, next_state=[-0.01144506  0.00654808  0.07286979  0.36500016]\n",
      "[ episode 187 ][ timestamp 55 ] state=[-0.01144506  0.00654808  0.07286979  0.36500016], action=1, reward=1.0, next_state=[-0.0113141   0.20056288  0.0801698   0.09615505]\n",
      "[ episode 187 ][ timestamp 56 ] state=[-0.0113141   0.20056288  0.0801698   0.09615505], action=0, reward=1.0, next_state=[-0.00730284  0.00438888  0.0820929   0.4130156 ]\n",
      "[ episode 187 ][ timestamp 57 ] state=[-0.00730284  0.00438888  0.0820929   0.4130156 ], action=0, reward=1.0, next_state=[-0.00721506 -0.19179495  0.09035321  0.73040852]\n",
      "[ episode 187 ][ timestamp 58 ] state=[-0.00721506 -0.19179495  0.09035321  0.73040852], action=1, reward=1.0, next_state=[-0.01105096  0.00196978  0.10496138  0.46747471]\n",
      "[ episode 187 ][ timestamp 59 ] state=[-0.01105096  0.00196978  0.10496138  0.46747471], action=1, reward=1.0, next_state=[-0.01101157  0.19546439  0.11431087  0.20963212]\n",
      "[ episode 187 ][ timestamp 60 ] state=[-0.01101157  0.19546439  0.11431087  0.20963212], action=1, reward=1.0, next_state=[-0.00710228  0.38878206  0.11850352 -0.04491763]\n",
      "[ episode 187 ][ timestamp 61 ] state=[-0.00710228  0.38878206  0.11850352 -0.04491763], action=0, reward=1.0, next_state=[0.00067336 0.19217777 0.11760516 0.28267907]\n",
      "[ episode 187 ][ timestamp 62 ] state=[0.00067336 0.19217777 0.11760516 0.28267907], action=1, reward=1.0, next_state=[0.00451692 0.38544306 0.12325875 0.02927989]\n",
      "[ episode 187 ][ timestamp 63 ] state=[0.00451692 0.38544306 0.12325875 0.02927989], action=1, reward=1.0, next_state=[ 0.01222578  0.57860161  0.12384434 -0.22211339]\n",
      "[ episode 187 ][ timestamp 64 ] state=[ 0.01222578  0.57860161  0.12384434 -0.22211339], action=0, reward=1.0, next_state=[0.02379781 0.38194717 0.11940208 0.10692626]\n",
      "[ episode 187 ][ timestamp 65 ] state=[0.02379781 0.38194717 0.11940208 0.10692626], action=0, reward=1.0, next_state=[0.03143676 0.18533451 0.1215406  0.4347663 ]\n",
      "[ episode 187 ][ timestamp 66 ] state=[0.03143676 0.18533451 0.1215406  0.4347663 ], action=1, reward=1.0, next_state=[0.03514345 0.37854505 0.13023593 0.18273348]\n",
      "[ episode 187 ][ timestamp 67 ] state=[0.03514345 0.37854505 0.13023593 0.18273348], action=1, reward=1.0, next_state=[ 0.04271435  0.57158628  0.1338906  -0.06619493]\n",
      "[ episode 187 ][ timestamp 68 ] state=[ 0.04271435  0.57158628  0.1338906  -0.06619493], action=1, reward=1.0, next_state=[ 0.05414607  0.76455986  0.1325667  -0.31381832]\n",
      "[ episode 187 ][ timestamp 69 ] state=[ 0.05414607  0.76455986  0.1325667  -0.31381832], action=0, reward=1.0, next_state=[0.06943727 0.56782311 0.12629033 0.01755808]\n",
      "[ episode 187 ][ timestamp 70 ] state=[0.06943727 0.56782311 0.12629033 0.01755808], action=1, reward=1.0, next_state=[ 0.08079373  0.76092895  0.12664149 -0.23276309]\n",
      "[ episode 187 ][ timestamp 71 ] state=[ 0.08079373  0.76092895  0.12664149 -0.23276309], action=0, reward=1.0, next_state=[0.09601231 0.56424638 0.12198623 0.09703127]\n",
      "[ episode 187 ][ timestamp 72 ] state=[0.09601231 0.56424638 0.12198623 0.09703127], action=0, reward=1.0, next_state=[0.10729724 0.36760644 0.12392686 0.42557438]\n",
      "[ episode 187 ][ timestamp 73 ] state=[0.10729724 0.36760644 0.12392686 0.42557438], action=1, reward=1.0, next_state=[0.11464937 0.56077512 0.13243834 0.17438482]\n",
      "[ episode 187 ][ timestamp 74 ] state=[0.11464937 0.56077512 0.13243834 0.17438482], action=1, reward=1.0, next_state=[ 0.12586487  0.7537775   0.13592604 -0.07376039]\n",
      "[ episode 187 ][ timestamp 75 ] state=[ 0.12586487  0.7537775   0.13592604 -0.07376039], action=0, reward=1.0, next_state=[0.14094042 0.55699521 0.13445083 0.25852977]\n",
      "[ episode 187 ][ timestamp 76 ] state=[0.14094042 0.55699521 0.13445083 0.25852977], action=1, reward=1.0, next_state=[0.15208032 0.74996712 0.13962143 0.01109379]\n",
      "[ episode 187 ][ timestamp 77 ] state=[0.15208032 0.74996712 0.13962143 0.01109379], action=1, reward=1.0, next_state=[ 0.16707967  0.94283931  0.1398433  -0.2344837 ]\n",
      "[ episode 187 ][ timestamp 78 ] state=[ 0.16707967  0.94283931  0.1398433  -0.2344837 ], action=0, reward=1.0, next_state=[0.18593645 0.74602512 0.13515363 0.09883565]\n",
      "[ episode 187 ][ timestamp 79 ] state=[0.18593645 0.74602512 0.13515363 0.09883565], action=0, reward=1.0, next_state=[0.20085696 0.54925107 0.13713034 0.43091934]\n",
      "[ episode 187 ][ timestamp 80 ] state=[0.20085696 0.54925107 0.13713034 0.43091934], action=1, reward=1.0, next_state=[0.21184198 0.74219188 0.14574873 0.1844151 ]\n",
      "[ episode 187 ][ timestamp 81 ] state=[0.21184198 0.74219188 0.14574873 0.1844151 ], action=0, reward=1.0, next_state=[0.22668581 0.54531783 0.14943703 0.51929371]\n",
      "[ episode 187 ][ timestamp 82 ] state=[0.22668581 0.54531783 0.14943703 0.51929371], action=1, reward=1.0, next_state=[0.23759217 0.73805497 0.15982291 0.27718122]\n",
      "[ episode 187 ][ timestamp 83 ] state=[0.23759217 0.73805497 0.15982291 0.27718122], action=0, reward=1.0, next_state=[0.25235327 0.54105676 0.16536653 0.61570072]\n",
      "[ episode 187 ][ timestamp 84 ] state=[0.25235327 0.54105676 0.16536653 0.61570072], action=1, reward=1.0, next_state=[0.26317441 0.73352936 0.17768054 0.37932683]\n",
      "[ episode 187 ][ timestamp 85 ] state=[0.26317441 0.73352936 0.17768054 0.37932683], action=1, reward=1.0, next_state=[0.27784499 0.92574186 0.18526708 0.14751094]\n",
      "[ episode 187 ][ timestamp 86 ] state=[0.27784499 0.92574186 0.18526708 0.14751094], action=1, reward=1.0, next_state=[ 0.29635983  1.11779425  0.1882173  -0.08148033]\n",
      "[ episode 187 ][ timestamp 87 ] state=[ 0.29635983  1.11779425  0.1882173  -0.08148033], action=0, reward=1.0, next_state=[0.31871572 0.9205428  0.18658769 0.26418122]\n",
      "[ episode 187 ][ timestamp 88 ] state=[0.31871572 0.9205428  0.18658769 0.26418122], action=0, reward=1.0, next_state=[0.33712657 0.72331583 0.19187132 0.60942578]\n",
      "[ episode 187 ][ timestamp 89 ] state=[0.33712657 0.72331583 0.19187132 0.60942578], action=0, reward=1.0, next_state=[0.35159289 0.52610306 0.20405983 0.95588108]\n",
      "[ episode 187 ][ timestamp 90 ] state=[0.35159289 0.52610306 0.20405983 0.95588108], action=1, reward=-1.0, next_state=[0.36211495 0.71798314 0.22317745 0.73361076]\n",
      "[ Ended! ] Episode 187: Exploration_rate=0.3936343764094253. Score=90.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 188 ] state=[ 0.04094592  0.01369824 -0.02072388  0.00309601]\n",
      "[ episode 188 ][ timestamp 1 ] state=[ 0.04094592  0.01369824 -0.02072388  0.00309601], action=0, reward=1.0, next_state=[ 0.04121988 -0.18112047 -0.02066196  0.28916893]\n",
      "[ episode 188 ][ timestamp 2 ] state=[ 0.04121988 -0.18112047 -0.02066196  0.28916893], action=1, reward=1.0, next_state=[ 0.03759747  0.01428993 -0.01487858 -0.00995829]\n",
      "[ episode 188 ][ timestamp 3 ] state=[ 0.03759747  0.01428993 -0.01487858 -0.00995829], action=0, reward=1.0, next_state=[ 0.03788327 -0.18061551 -0.01507775  0.27799338]\n",
      "[ episode 188 ][ timestamp 4 ] state=[ 0.03788327 -0.18061551 -0.01507775  0.27799338], action=0, reward=1.0, next_state=[ 0.03427096 -0.37551915 -0.00951788  0.56588292]\n",
      "[ episode 188 ][ timestamp 5 ] state=[ 0.03427096 -0.37551915 -0.00951788  0.56588292], action=1, reward=1.0, next_state=[ 0.02676058 -0.18026498  0.00179978  0.27021671]\n",
      "[ episode 188 ][ timestamp 6 ] state=[ 0.02676058 -0.18026498  0.00179978  0.27021671], action=1, reward=1.0, next_state=[ 0.02315528  0.01483124  0.00720411 -0.02189801]\n",
      "[ episode 188 ][ timestamp 7 ] state=[ 0.02315528  0.01483124  0.00720411 -0.02189801], action=0, reward=1.0, next_state=[ 0.0234519  -0.18039328  0.00676615  0.27304916]\n",
      "[ episode 188 ][ timestamp 8 ] state=[ 0.0234519  -0.18039328  0.00676615  0.27304916], action=1, reward=1.0, next_state=[ 0.01984404  0.01463148  0.01222714 -0.01749204]\n",
      "[ episode 188 ][ timestamp 9 ] state=[ 0.01984404  0.01463148  0.01222714 -0.01749204], action=0, reward=1.0, next_state=[ 0.02013667 -0.18066367  0.0118773   0.27902347]\n",
      "[ episode 188 ][ timestamp 10 ] state=[ 0.02013667 -0.18066367  0.0118773   0.27902347], action=1, reward=1.0, next_state=[ 0.01652339  0.01428685  0.01745777 -0.00988984]\n",
      "[ episode 188 ][ timestamp 11 ] state=[ 0.01652339  0.01428685  0.01745777 -0.00988984], action=1, reward=1.0, next_state=[ 0.01680913  0.20915414  0.01725997 -0.29701391]\n",
      "[ episode 188 ][ timestamp 12 ] state=[ 0.01680913  0.20915414  0.01725997 -0.29701391], action=1, reward=1.0, next_state=[ 0.02099221  0.40402584  0.01131969 -0.58420375]\n",
      "[ episode 188 ][ timestamp 13 ] state=[ 0.02099221  0.40402584  0.01131969 -0.58420375], action=0, reward=1.0, next_state=[ 0.02907273  0.20874717 -0.00036438 -0.28797658]\n",
      "[ episode 188 ][ timestamp 14 ] state=[ 0.02907273  0.20874717 -0.00036438 -0.28797658], action=0, reward=1.0, next_state=[ 0.03324767  0.01363041 -0.00612392  0.0045914 ]\n",
      "[ episode 188 ][ timestamp 15 ] state=[ 0.03324767  0.01363041 -0.00612392  0.0045914 ], action=1, reward=1.0, next_state=[ 0.03352028  0.20883965 -0.00603209 -0.29001739]\n",
      "[ episode 188 ][ timestamp 16 ] state=[ 0.03352028  0.20883965 -0.00603209 -0.29001739], action=0, reward=1.0, next_state=[ 0.03769707  0.01380423 -0.01183244  0.000757  ]\n",
      "[ episode 188 ][ timestamp 17 ] state=[ 0.03769707  0.01380423 -0.01183244  0.000757  ], action=0, reward=1.0, next_state=[ 0.03797316 -0.18114604 -0.0118173   0.28968329]\n",
      "[ episode 188 ][ timestamp 18 ] state=[ 0.03797316 -0.18114604 -0.0118173   0.28968329], action=0, reward=1.0, next_state=[ 0.03435024 -0.37609751 -0.00602363  0.57861586]\n",
      "[ episode 188 ][ timestamp 19 ] state=[ 0.03435024 -0.37609751 -0.00602363  0.57861586], action=1, reward=1.0, next_state=[ 0.02682829 -0.18089166  0.00554869  0.28404146]\n",
      "[ episode 188 ][ timestamp 20 ] state=[ 0.02682829 -0.18089166  0.00554869  0.28404146], action=1, reward=1.0, next_state=[ 0.02321046  0.01415072  0.01122952 -0.00688629]\n",
      "[ episode 188 ][ timestamp 21 ] state=[ 0.02321046  0.01415072  0.01122952 -0.00688629], action=0, reward=1.0, next_state=[ 0.02349347 -0.18113047  0.01109179  0.28931842]\n",
      "[ episode 188 ][ timestamp 22 ] state=[ 0.02349347 -0.18113047  0.01109179  0.28931842], action=1, reward=1.0, next_state=[0.01987086 0.01383157 0.01687816 0.00015427]\n",
      "[ episode 188 ][ timestamp 23 ] state=[0.01987086 0.01383157 0.01687816 0.00015427], action=0, reward=1.0, next_state=[ 0.02014749 -0.18152832  0.01688124  0.29811431]\n",
      "[ episode 188 ][ timestamp 24 ] state=[ 0.02014749 -0.18152832  0.01688124  0.29811431], action=1, reward=1.0, next_state=[0.01651693 0.01334898 0.02284353 0.01080287]\n",
      "[ episode 188 ][ timestamp 25 ] state=[0.01651693 0.01334898 0.02284353 0.01080287], action=0, reward=1.0, next_state=[ 0.0167839  -0.182093    0.02305959  0.31060477]\n",
      "[ episode 188 ][ timestamp 26 ] state=[ 0.0167839  -0.182093    0.02305959  0.31060477], action=1, reward=1.0, next_state=[0.01314204 0.01269295 0.02927168 0.02528244]\n",
      "[ episode 188 ][ timestamp 27 ] state=[0.01314204 0.01269295 0.02927168 0.02528244], action=1, reward=1.0, next_state=[ 0.0133959   0.20738316  0.02977733 -0.25802313]\n",
      "[ episode 188 ][ timestamp 28 ] state=[ 0.0133959   0.20738316  0.02977733 -0.25802313], action=1, reward=1.0, next_state=[ 0.01754357  0.40206762  0.02461687 -0.54116712]\n",
      "[ episode 188 ][ timestamp 29 ] state=[ 0.01754357  0.40206762  0.02461687 -0.54116712], action=0, reward=1.0, next_state=[ 0.02558492  0.20660846  0.01379353 -0.24083059]\n",
      "[ episode 188 ][ timestamp 30 ] state=[ 0.02558492  0.20660846  0.01379353 -0.24083059], action=0, reward=1.0, next_state=[0.02971709 0.01129222 0.00897692 0.05617108]\n",
      "[ episode 188 ][ timestamp 31 ] state=[0.02971709 0.01129222 0.00897692 0.05617108], action=1, reward=1.0, next_state=[ 0.02994293  0.20628431  0.01010034 -0.2336661 ]\n",
      "[ episode 188 ][ timestamp 32 ] state=[ 0.02994293  0.20628431  0.01010034 -0.2336661 ], action=1, reward=1.0, next_state=[ 0.03406862  0.4012605   0.00542702 -0.52314602]\n",
      "[ episode 188 ][ timestamp 33 ] state=[ 0.03406862  0.4012605   0.00542702 -0.52314602], action=0, reward=1.0, next_state=[ 0.04209383  0.20606259 -0.00503591 -0.22875793]\n",
      "[ episode 188 ][ timestamp 34 ] state=[ 0.04209383  0.20606259 -0.00503591 -0.22875793], action=0, reward=1.0, next_state=[ 0.04621508  0.01101297 -0.00961106  0.06233225]\n",
      "[ episode 188 ][ timestamp 35 ] state=[ 0.04621508  0.01101297 -0.00961106  0.06233225], action=0, reward=1.0, next_state=[ 0.04643534 -0.18396988 -0.00836442  0.3519674 ]\n",
      "[ episode 188 ][ timestamp 36 ] state=[ 0.04643534 -0.18396988 -0.00836442  0.3519674 ], action=1, reward=1.0, next_state=[ 0.04275594  0.01127002 -0.00132507  0.05665869]\n",
      "[ episode 188 ][ timestamp 37 ] state=[ 0.04275594  0.01127002 -0.00132507  0.05665869], action=1, reward=1.0, next_state=[ 4.29813432e-02  2.06410943e-01 -1.91896984e-04 -2.36442012e-01]\n",
      "[ episode 188 ][ timestamp 38 ] state=[ 4.29813432e-02  2.06410943e-01 -1.91896984e-04 -2.36442012e-01], action=0, reward=1.0, next_state=[ 0.04710956  0.01129173 -0.00492074  0.05618038]\n",
      "[ episode 188 ][ timestamp 39 ] state=[ 0.04710956  0.01129173 -0.00492074  0.05618038], action=1, reward=1.0, next_state=[ 0.0473354   0.20648389 -0.00379713 -0.23805101]\n",
      "[ episode 188 ][ timestamp 40 ] state=[ 0.0473354   0.20648389 -0.00379713 -0.23805101], action=0, reward=1.0, next_state=[ 0.05146507  0.01141639 -0.00855815  0.05343178]\n",
      "[ episode 188 ][ timestamp 41 ] state=[ 0.05146507  0.01141639 -0.00855815  0.05343178], action=0, reward=1.0, next_state=[ 0.0516934  -0.18358181 -0.00748951  0.3434023 ]\n",
      "[ episode 188 ][ timestamp 42 ] state=[ 0.0516934  -0.18358181 -0.00748951  0.3434023 ], action=1, reward=1.0, next_state=[ 0.04802177  0.01164589 -0.00062147  0.04836708]\n",
      "[ episode 188 ][ timestamp 43 ] state=[ 0.04802177  0.01164589 -0.00062147  0.04836708], action=1, reward=1.0, next_state=[ 0.04825468  0.20677675  0.00034587 -0.24451186]\n",
      "[ episode 188 ][ timestamp 44 ] state=[ 0.04825468  0.20677675  0.00034587 -0.24451186], action=0, reward=1.0, next_state=[ 0.05239022  0.01164986 -0.00454436  0.04828014]\n",
      "[ episode 188 ][ timestamp 45 ] state=[ 0.05239022  0.01164986 -0.00454436  0.04828014], action=0, reward=1.0, next_state=[ 0.05262322 -0.18340664 -0.00357876  0.33952582]\n",
      "[ episode 188 ][ timestamp 46 ] state=[ 0.05262322 -0.18340664 -0.00357876  0.33952582], action=1, reward=1.0, next_state=[0.04895508 0.01176605 0.00321176 0.04571651]\n",
      "[ episode 188 ][ timestamp 47 ] state=[0.04895508 0.01176605 0.00321176 0.04571651], action=0, reward=1.0, next_state=[ 0.0491904  -0.18340181  0.00412609  0.33941104]\n",
      "[ episode 188 ][ timestamp 48 ] state=[ 0.0491904  -0.18340181  0.00412609  0.33941104], action=1, reward=1.0, next_state=[0.04552237 0.01166119 0.01091431 0.0480321 ]\n",
      "[ episode 188 ][ timestamp 49 ] state=[0.04552237 0.01166119 0.01091431 0.0480321 ], action=0, reward=1.0, next_state=[ 0.04575559 -0.18361555  0.01187495  0.3441385 ]\n",
      "[ episode 188 ][ timestamp 50 ] state=[ 0.04575559 -0.18361555  0.01187495  0.3441385 ], action=1, reward=1.0, next_state=[0.04208328 0.01133548 0.01875772 0.05522374]\n",
      "[ episode 188 ][ timestamp 51 ] state=[0.04208328 0.01133548 0.01875772 0.05522374], action=1, reward=1.0, next_state=[ 0.04230999  0.20618352  0.01986219 -0.23148246]\n",
      "[ episode 188 ][ timestamp 52 ] state=[ 0.04230999  0.20618352  0.01986219 -0.23148246], action=0, reward=1.0, next_state=[0.04643366 0.01078347 0.01523254 0.06739891]\n",
      "[ episode 188 ][ timestamp 53 ] state=[0.04643366 0.01078347 0.01523254 0.06739891], action=0, reward=1.0, next_state=[ 0.04664933 -0.18455352  0.01658052  0.36484859]\n",
      "[ episode 188 ][ timestamp 54 ] state=[ 0.04664933 -0.18455352  0.01658052  0.36484859], action=1, reward=1.0, next_state=[0.04295826 0.01032891 0.02387749 0.07743957]\n",
      "[ episode 188 ][ timestamp 55 ] state=[0.04295826 0.01032891 0.02387749 0.07743957], action=0, reward=1.0, next_state=[ 0.04316484 -0.18512705  0.02542629  0.37755926]\n",
      "[ episode 188 ][ timestamp 56 ] state=[ 0.04316484 -0.18512705  0.02542629  0.37755926], action=1, reward=1.0, next_state=[0.0394623  0.00962473 0.03297747 0.09300053]\n",
      "[ episode 188 ][ timestamp 57 ] state=[0.0394623  0.00962473 0.03297747 0.09300053], action=1, reward=1.0, next_state=[ 0.03965479  0.20425888  0.03483748 -0.18909833]\n",
      "[ episode 188 ][ timestamp 58 ] state=[ 0.03965479  0.20425888  0.03483748 -0.18909833], action=0, reward=1.0, next_state=[0.04373997 0.00865629 0.03105551 0.11436767]\n",
      "[ episode 188 ][ timestamp 59 ] state=[0.04373997 0.00865629 0.03105551 0.11436767], action=0, reward=1.0, next_state=[ 0.04391309 -0.18689657  0.03334287  0.41668437]\n",
      "[ episode 188 ][ timestamp 60 ] state=[ 0.04391309 -0.18689657  0.03334287  0.41668437], action=1, reward=1.0, next_state=[0.04017516 0.00773737 0.04167656 0.13469672]\n",
      "[ episode 188 ][ timestamp 61 ] state=[0.04017516 0.00773737 0.04167656 0.13469672], action=1, reward=1.0, next_state=[ 0.04032991  0.20223834  0.04437049 -0.14455204]\n",
      "[ episode 188 ][ timestamp 62 ] state=[ 0.04032991  0.20223834  0.04437049 -0.14455204], action=0, reward=1.0, next_state=[0.04437468 0.00650997 0.04147945 0.16179221]\n",
      "[ episode 188 ][ timestamp 63 ] state=[0.04437468 0.00650997 0.04147945 0.16179221], action=0, reward=1.0, next_state=[ 0.04450488 -0.18918049  0.04471529  0.46726688]\n",
      "[ episode 188 ][ timestamp 64 ] state=[ 0.04450488 -0.18918049  0.04471529  0.46726688], action=1, reward=1.0, next_state=[0.04072127 0.00528212 0.05406063 0.18900645]\n",
      "[ episode 188 ][ timestamp 65 ] state=[0.04072127 0.00528212 0.05406063 0.18900645], action=1, reward=1.0, next_state=[ 0.04082691  0.19959064  0.05784076 -0.08614445]\n",
      "[ episode 188 ][ timestamp 66 ] state=[ 0.04082691  0.19959064  0.05784076 -0.08614445], action=0, reward=1.0, next_state=[0.04481872 0.00368934 0.05611787 0.22421179]\n",
      "[ episode 188 ][ timestamp 67 ] state=[0.04481872 0.00368934 0.05611787 0.22421179], action=1, reward=1.0, next_state=[ 0.04489251  0.1979662   0.06060211 -0.05025475]\n",
      "[ episode 188 ][ timestamp 68 ] state=[ 0.04489251  0.1979662   0.06060211 -0.05025475], action=0, reward=1.0, next_state=[0.04885183 0.00202999 0.05959701 0.26091614]\n",
      "[ episode 188 ][ timestamp 69 ] state=[0.04885183 0.00202999 0.05959701 0.26091614], action=1, reward=1.0, next_state=[ 0.04889243  0.1962528   0.06481533 -0.01238969]\n",
      "[ episode 188 ][ timestamp 70 ] state=[ 0.04889243  0.1962528   0.06481533 -0.01238969], action=0, reward=1.0, next_state=[5.28174893e-02 2.64069254e-04 6.45675409e-02 3.00018473e-01]\n",
      "[ episode 188 ][ timestamp 71 ] state=[5.28174893e-02 2.64069254e-04 6.45675409e-02 3.00018473e-01], action=1, reward=1.0, next_state=[0.05282277 0.19440912 0.07056791 0.02837739]\n",
      "[ episode 188 ][ timestamp 72 ] state=[0.05282277 0.19440912 0.07056791 0.02837739], action=0, reward=1.0, next_state=[ 0.05671095 -0.00165014  0.07113546  0.34246408]\n",
      "[ episode 188 ][ timestamp 73 ] state=[ 0.05671095 -0.00165014  0.07113546  0.34246408], action=0, reward=1.0, next_state=[ 0.05667795 -0.19770824  0.07798474  0.65670366]\n",
      "[ episode 188 ][ timestamp 74 ] state=[ 0.05667795 -0.19770824  0.07798474  0.65670366], action=1, reward=1.0, next_state=[ 0.05272379 -0.00375353  0.09111881  0.38956009]\n",
      "[ episode 188 ][ timestamp 75 ] state=[ 0.05272379 -0.00375353  0.09111881  0.38956009], action=0, reward=1.0, next_state=[ 0.05264872 -0.20004265  0.09891001  0.7095242 ]\n",
      "[ episode 188 ][ timestamp 76 ] state=[ 0.05264872 -0.20004265  0.09891001  0.7095242 ], action=0, reward=1.0, next_state=[ 0.04864786 -0.39638529  0.1131005   1.03163085]\n",
      "[ episode 188 ][ timestamp 77 ] state=[ 0.04864786 -0.39638529  0.1131005   1.03163085], action=1, reward=1.0, next_state=[ 0.04072016 -0.20293468  0.13373312  0.77648958]\n",
      "[ episode 188 ][ timestamp 78 ] state=[ 0.04072016 -0.20293468  0.13373312  0.77648958], action=1, reward=1.0, next_state=[ 0.03666146 -0.00988057  0.14926291  0.52869452]\n",
      "[ episode 188 ][ timestamp 79 ] state=[ 0.03666146 -0.00988057  0.14926291  0.52869452], action=1, reward=1.0, next_state=[0.03646385 0.18286106 0.1598368  0.28651726]\n",
      "[ episode 188 ][ timestamp 80 ] state=[0.03646385 0.18286106 0.1598368  0.28651726], action=1, reward=1.0, next_state=[0.04012107 0.37538547 0.16556714 0.04820392]\n",
      "[ episode 188 ][ timestamp 81 ] state=[0.04012107 0.37538547 0.16556714 0.04820392], action=0, reward=1.0, next_state=[0.04762878 0.17832423 0.16653122 0.38820821]\n",
      "[ episode 188 ][ timestamp 82 ] state=[0.04762878 0.17832423 0.16653122 0.38820821], action=1, reward=1.0, next_state=[0.05119527 0.37073931 0.17429539 0.15231268]\n",
      "[ episode 188 ][ timestamp 83 ] state=[0.05119527 0.37073931 0.17429539 0.15231268], action=0, reward=1.0, next_state=[0.05861005 0.17360573 0.17734164 0.49451668]\n",
      "[ episode 188 ][ timestamp 84 ] state=[0.05861005 0.17360573 0.17734164 0.49451668], action=1, reward=1.0, next_state=[0.06208217 0.36584166 0.18723197 0.26255086]\n",
      "[ episode 188 ][ timestamp 85 ] state=[0.06208217 0.36584166 0.18723197 0.26255086], action=1, reward=1.0, next_state=[0.069399   0.55786644 0.19248299 0.0342728 ]\n",
      "[ episode 188 ][ timestamp 86 ] state=[0.069399   0.55786644 0.19248299 0.0342728 ], action=1, reward=1.0, next_state=[ 0.08055633  0.74978192  0.19316845 -0.19204286]\n",
      "[ episode 188 ][ timestamp 87 ] state=[ 0.08055633  0.74978192  0.19316845 -0.19204286], action=1, reward=1.0, next_state=[ 0.09555197  0.94169138  0.18932759 -0.41811404]\n",
      "[ episode 188 ][ timestamp 88 ] state=[ 0.09555197  0.94169138  0.18932759 -0.41811404], action=0, reward=1.0, next_state=[ 0.1143858   0.74446186  0.18096531 -0.07222582]\n",
      "[ episode 188 ][ timestamp 89 ] state=[ 0.1143858   0.74446186  0.18096531 -0.07222582], action=1, reward=1.0, next_state=[ 0.12927503  0.93659033  0.17952079 -0.30279858]\n",
      "[ episode 188 ][ timestamp 90 ] state=[ 0.12927503  0.93659033  0.17952079 -0.30279858], action=1, reward=1.0, next_state=[ 0.14800684  1.12876031  0.17346482 -0.53392503]\n",
      "[ episode 188 ][ timestamp 91 ] state=[ 0.14800684  1.12876031  0.17346482 -0.53392503], action=0, reward=1.0, next_state=[ 0.17058205  0.93167781  0.16278632 -0.19199452]\n",
      "[ episode 188 ][ timestamp 92 ] state=[ 0.17058205  0.93167781  0.16278632 -0.19199452], action=1, reward=1.0, next_state=[ 0.1892156   1.12414212  0.15894643 -0.4292262 ]\n",
      "[ episode 188 ][ timestamp 93 ] state=[ 0.1892156   1.12414212  0.15894643 -0.4292262 ], action=0, reward=1.0, next_state=[ 0.21169844  0.92716836  0.15036191 -0.09095623]\n",
      "[ episode 188 ][ timestamp 94 ] state=[ 0.21169844  0.92716836  0.15036191 -0.09095623], action=1, reward=1.0, next_state=[ 0.23024181  1.11985111  0.14854278 -0.33267926]\n",
      "[ episode 188 ][ timestamp 95 ] state=[ 0.23024181  1.11985111  0.14854278 -0.33267926], action=0, reward=1.0, next_state=[0.25263883 0.92296156 0.1418892  0.00291393]\n",
      "[ episode 188 ][ timestamp 96 ] state=[0.25263883 0.92296156 0.1418892  0.00291393], action=0, reward=1.0, next_state=[0.27109806 0.72611994 0.14194747 0.33678473]\n",
      "[ episode 188 ][ timestamp 97 ] state=[0.27109806 0.72611994 0.14194747 0.33678473], action=1, reward=1.0, next_state=[0.28562046 0.91896664 0.14868317 0.09201662]\n",
      "[ episode 188 ][ timestamp 98 ] state=[0.28562046 0.91896664 0.14868317 0.09201662], action=0, reward=1.0, next_state=[0.3039998  0.72206103 0.1505235  0.42766832]\n",
      "[ episode 188 ][ timestamp 99 ] state=[0.3039998  0.72206103 0.1505235  0.42766832], action=1, reward=1.0, next_state=[0.31844102 0.9147664  0.15907687 0.18596571]\n",
      "[ episode 188 ][ timestamp 100 ] state=[0.31844102 0.9147664  0.15907687 0.18596571], action=1, reward=1.0, next_state=[ 0.33673634  1.1072971   0.16279618 -0.05261238]\n",
      "[ episode 188 ][ timestamp 101 ] state=[ 0.33673634  1.1072971   0.16279618 -0.05261238], action=0, reward=1.0, next_state=[0.35888229 0.91026061 0.16174393 0.28668545]\n",
      "[ episode 188 ][ timestamp 102 ] state=[0.35888229 0.91026061 0.16174393 0.28668545], action=1, reward=1.0, next_state=[0.3770875  1.10275081 0.16747764 0.04906439]\n",
      "[ episode 188 ][ timestamp 103 ] state=[0.3770875  1.10275081 0.16747764 0.04906439], action=1, reward=1.0, next_state=[ 0.39914252  1.2951248   0.16845893 -0.18645058]\n",
      "[ episode 188 ][ timestamp 104 ] state=[ 0.39914252  1.2951248   0.16845893 -0.18645058], action=0, reward=1.0, next_state=[0.42504501 1.09804341 0.16472992 0.15427977]\n",
      "[ episode 188 ][ timestamp 105 ] state=[0.42504501 1.09804341 0.16472992 0.15427977], action=1, reward=1.0, next_state=[ 0.44700588  1.29047058  0.16781551 -0.0822417 ]\n",
      "[ episode 188 ][ timestamp 106 ] state=[ 0.44700588  1.29047058  0.16781551 -0.0822417 ], action=0, reward=1.0, next_state=[0.47281529 1.09339026 0.16617068 0.25833242]\n",
      "[ episode 188 ][ timestamp 107 ] state=[0.47281529 1.09339026 0.16617068 0.25833242], action=1, reward=1.0, next_state=[0.4946831  1.28579829 0.17133733 0.02232555]\n",
      "[ episode 188 ][ timestamp 108 ] state=[0.4946831  1.28579829 0.17133733 0.02232555], action=0, reward=1.0, next_state=[0.52039906 1.08868628 0.17178384 0.36379136]\n",
      "[ episode 188 ][ timestamp 109 ] state=[0.52039906 1.08868628 0.17178384 0.36379136], action=1, reward=1.0, next_state=[0.54217279 1.28100366 0.17905967 0.12981767]\n",
      "[ episode 188 ][ timestamp 110 ] state=[0.54217279 1.28100366 0.17905967 0.12981767], action=1, reward=1.0, next_state=[ 0.56779286  1.47316907  0.18165602 -0.10145915]\n",
      "[ episode 188 ][ timestamp 111 ] state=[ 0.56779286  1.47316907  0.18165602 -0.10145915], action=0, reward=1.0, next_state=[0.59725624 1.27597159 0.17962684 0.24258364]\n",
      "[ episode 188 ][ timestamp 112 ] state=[0.59725624 1.27597159 0.17962684 0.24258364], action=0, reward=1.0, next_state=[0.62277567 1.07879936 0.18447851 0.58611011]\n",
      "[ episode 188 ][ timestamp 113 ] state=[0.62277567 1.07879936 0.18447851 0.58611011], action=1, reward=1.0, next_state=[0.64435166 1.2709237  0.19620071 0.35674311]\n",
      "[ episode 188 ][ timestamp 114 ] state=[0.64435166 1.2709237  0.19620071 0.35674311], action=0, reward=1.0, next_state=[0.66977014 1.07363242 0.20333557 0.70431592]\n",
      "[ episode 188 ][ timestamp 115 ] state=[0.66977014 1.07363242 0.20333557 0.70431592], action=1, reward=-1.0, next_state=[0.69124278 1.26544336 0.21742189 0.48189648]\n",
      "[ Ended! ] Episode 188: Exploration_rate=0.39166620452737816. Score=115.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 189 ] state=[-0.01301399 -0.03763022  0.04787367 -0.00685203]\n",
      "[ episode 189 ][ timestamp 1 ] state=[-0.01301399 -0.03763022  0.04787367 -0.00685203], action=1, reward=1.0, next_state=[-0.01376659  0.15677362  0.04773663 -0.28405422]\n",
      "[ episode 189 ][ timestamp 2 ] state=[-0.01376659  0.15677362  0.04773663 -0.28405422], action=1, reward=1.0, next_state=[-0.01063112  0.35118337  0.04205555 -0.56130739]\n",
      "[ episode 189 ][ timestamp 3 ] state=[-0.01063112  0.35118337  0.04205555 -0.56130739], action=0, reward=1.0, next_state=[-0.00360745  0.15549722  0.0308294  -0.25567702]\n",
      "[ episode 189 ][ timestamp 4 ] state=[-0.00360745  0.15549722  0.0308294  -0.25567702], action=1, reward=1.0, next_state=[-4.97506560e-04  3.50165746e-01  2.57158608e-02 -5.38478646e-01]\n",
      "[ episode 189 ][ timestamp 5 ] state=[-4.97506560e-04  3.50165746e-01  2.57158608e-02 -5.38478646e-01], action=0, reward=1.0, next_state=[ 0.00650581  0.15469188  0.01494629 -0.23780517]\n",
      "[ episode 189 ][ timestamp 6 ] state=[ 0.00650581  0.15469188  0.01494629 -0.23780517], action=0, reward=1.0, next_state=[ 0.00959965 -0.04064037  0.01019018  0.05955453]\n",
      "[ episode 189 ][ timestamp 7 ] state=[ 0.00959965 -0.04064037  0.01019018  0.05955453], action=1, reward=1.0, next_state=[ 0.00878684  0.154334    0.01138128 -0.22989598]\n",
      "[ episode 189 ][ timestamp 8 ] state=[ 0.00878684  0.154334    0.01138128 -0.22989598], action=0, reward=1.0, next_state=[ 0.01187352 -0.04094872  0.00678336  0.06635515]\n",
      "[ episode 189 ][ timestamp 9 ] state=[ 0.01187352 -0.04094872  0.00678336  0.06635515], action=1, reward=1.0, next_state=[ 0.01105454  0.15407532  0.00811046 -0.22417989]\n",
      "[ episode 189 ][ timestamp 10 ] state=[ 0.01105454  0.15407532  0.00811046 -0.22417989], action=0, reward=1.0, next_state=[ 0.01413605 -0.0411616   0.00362686  0.07105031]\n",
      "[ episode 189 ][ timestamp 11 ] state=[ 0.01413605 -0.0411616   0.00362686  0.07105031], action=1, reward=1.0, next_state=[ 0.01331282  0.15390817  0.00504787 -0.22048612]\n",
      "[ episode 189 ][ timestamp 12 ] state=[ 0.01331282  0.15390817  0.00504787 -0.22048612], action=1, reward=1.0, next_state=[ 0.01639098  0.3489576   0.00063814 -0.51157248]\n",
      "[ episode 189 ][ timestamp 13 ] state=[ 0.01639098  0.3489576   0.00063814 -0.51157248], action=0, reward=1.0, next_state=[ 0.02337013  0.15382667 -0.00959331 -0.21868852]\n",
      "[ episode 189 ][ timestamp 14 ] state=[ 0.02337013  0.15382667 -0.00959331 -0.21868852], action=1, reward=1.0, next_state=[ 0.02644667  0.34908443 -0.01396708 -0.51438207]\n",
      "[ episode 189 ][ timestamp 15 ] state=[ 0.02644667  0.34908443 -0.01396708 -0.51438207], action=0, reward=1.0, next_state=[ 0.03342836  0.15416194 -0.02425472 -0.22613304]\n",
      "[ episode 189 ][ timestamp 16 ] state=[ 0.03342836  0.15416194 -0.02425472 -0.22613304], action=0, reward=1.0, next_state=[ 0.03651159 -0.04060513 -0.02877738  0.05880144]\n",
      "[ episode 189 ][ timestamp 17 ] state=[ 0.03651159 -0.04060513 -0.02877738  0.05880144], action=1, reward=1.0, next_state=[ 0.03569949  0.15491736 -0.02760135 -0.24282025]\n",
      "[ episode 189 ][ timestamp 18 ] state=[ 0.03569949  0.15491736 -0.02760135 -0.24282025], action=0, reward=1.0, next_state=[ 0.03879784 -0.03979968 -0.03245775  0.0410303 ]\n",
      "[ episode 189 ][ timestamp 19 ] state=[ 0.03879784 -0.03979968 -0.03245775  0.0410303 ], action=1, reward=1.0, next_state=[ 0.03800185  0.15577231 -0.03163715 -0.26171407]\n",
      "[ episode 189 ][ timestamp 20 ] state=[ 0.03800185  0.15577231 -0.03163715 -0.26171407], action=0, reward=1.0, next_state=[ 0.04111729 -0.03888408 -0.03687143  0.02082463]\n",
      "[ episode 189 ][ timestamp 21 ] state=[ 0.04111729 -0.03888408 -0.03687143  0.02082463], action=1, reward=1.0, next_state=[ 0.04033961  0.15674671 -0.03645494 -0.28325986]\n",
      "[ episode 189 ][ timestamp 22 ] state=[ 0.04033961  0.15674671 -0.03645494 -0.28325986], action=0, reward=1.0, next_state=[ 0.04347454 -0.03783683 -0.04212013 -0.00229385]\n",
      "[ episode 189 ][ timestamp 23 ] state=[ 0.04347454 -0.03783683 -0.04212013 -0.00229385], action=1, reward=1.0, next_state=[ 0.04271781  0.15786309 -0.04216601 -0.30796302]\n",
      "[ episode 189 ][ timestamp 24 ] state=[ 0.04271781  0.15786309 -0.04216601 -0.30796302], action=0, reward=1.0, next_state=[ 0.04587507 -0.03663347 -0.04832527 -0.02887064]\n",
      "[ episode 189 ][ timestamp 25 ] state=[ 0.04587507 -0.03663347 -0.04832527 -0.02887064], action=0, reward=1.0, next_state=[ 0.0451424  -0.23103029 -0.04890268  0.24818207]\n",
      "[ episode 189 ][ timestamp 26 ] state=[ 0.0451424  -0.23103029 -0.04890268  0.24818207], action=0, reward=1.0, next_state=[ 0.04052179 -0.42542098 -0.04393904  0.52504785]\n",
      "[ episode 189 ][ timestamp 27 ] state=[ 0.04052179 -0.42542098 -0.04393904  0.52504785], action=0, reward=1.0, next_state=[ 0.03201337 -0.61989795 -0.03343809  0.80356783]\n",
      "[ episode 189 ][ timestamp 28 ] state=[ 0.03201337 -0.61989795 -0.03343809  0.80356783], action=1, reward=1.0, next_state=[ 0.01961542 -0.42433386 -0.01736673  0.50055671]\n",
      "[ episode 189 ][ timestamp 29 ] state=[ 0.01961542 -0.42433386 -0.01736673  0.50055671], action=1, reward=1.0, next_state=[ 0.01112874 -0.22897145 -0.00735559  0.20245172]\n",
      "[ episode 189 ][ timestamp 30 ] state=[ 0.01112874 -0.22897145 -0.00735559  0.20245172], action=1, reward=1.0, next_state=[ 0.00654931 -0.03374508 -0.00330656 -0.09254244]\n",
      "[ episode 189 ][ timestamp 31 ] state=[ 0.00654931 -0.03374508 -0.00330656 -0.09254244], action=1, reward=1.0, next_state=[ 0.00587441  0.16142411 -0.00515741 -0.38626675]\n",
      "[ episode 189 ][ timestamp 32 ] state=[ 0.00587441  0.16142411 -0.00515741 -0.38626675], action=1, reward=1.0, next_state=[ 0.00910289  0.3566189  -0.01288274 -0.6805713 ]\n",
      "[ episode 189 ][ timestamp 33 ] state=[ 0.00910289  0.3566189  -0.01288274 -0.6805713 ], action=0, reward=1.0, next_state=[ 0.01623527  0.16167823 -0.02649417 -0.39197198]\n",
      "[ episode 189 ][ timestamp 34 ] state=[ 0.01623527  0.16167823 -0.02649417 -0.39197198], action=0, reward=1.0, next_state=[ 0.01946883 -0.03305791 -0.03433361 -0.10775867]\n",
      "[ episode 189 ][ timestamp 35 ] state=[ 0.01946883 -0.03305791 -0.03433361 -0.10775867], action=1, reward=1.0, next_state=[ 0.01880767  0.1625388  -0.03648878 -0.41107292]\n",
      "[ episode 189 ][ timestamp 36 ] state=[ 0.01880767  0.1625388  -0.03648878 -0.41107292], action=0, reward=1.0, next_state=[ 0.02205845 -0.03204738 -0.04471024 -0.13011326]\n",
      "[ episode 189 ][ timestamp 37 ] state=[ 0.02205845 -0.03204738 -0.04471024 -0.13011326], action=1, reward=1.0, next_state=[ 0.0214175   0.16368558 -0.04731251 -0.43655972]\n",
      "[ episode 189 ][ timestamp 38 ] state=[ 0.0214175   0.16368558 -0.04731251 -0.43655972], action=0, reward=1.0, next_state=[ 0.02469121 -0.03073583 -0.0560437  -0.15915865]\n",
      "[ episode 189 ][ timestamp 39 ] state=[ 0.02469121 -0.03073583 -0.0560437  -0.15915865], action=1, reward=1.0, next_state=[ 0.0240765   0.16514184 -0.05922687 -0.46898206]\n",
      "[ episode 189 ][ timestamp 40 ] state=[ 0.0240765   0.16514184 -0.05922687 -0.46898206], action=0, reward=1.0, next_state=[ 0.02737933 -0.02909561 -0.06860652 -0.19553929]\n",
      "[ episode 189 ][ timestamp 41 ] state=[ 0.02737933 -0.02909561 -0.06860652 -0.19553929], action=0, reward=1.0, next_state=[ 0.02679742 -0.22317254 -0.0725173   0.07473677]\n",
      "[ episode 189 ][ timestamp 42 ] state=[ 0.02679742 -0.22317254 -0.0725173   0.07473677], action=0, reward=1.0, next_state=[ 0.02233397 -0.41718401 -0.07102257  0.3436877 ]\n",
      "[ episode 189 ][ timestamp 43 ] state=[ 0.02233397 -0.41718401 -0.07102257  0.3436877 ], action=1, reward=1.0, next_state=[ 0.01399029 -0.22112733 -0.06414881  0.029481  ]\n",
      "[ episode 189 ][ timestamp 44 ] state=[ 0.01399029 -0.22112733 -0.06414881  0.029481  ], action=0, reward=1.0, next_state=[ 0.00956775 -0.41527349 -0.06355919  0.30125443]\n",
      "[ episode 189 ][ timestamp 45 ] state=[ 0.00956775 -0.41527349 -0.06355919  0.30125443], action=1, reward=1.0, next_state=[ 0.00126228 -0.21930593 -0.0575341  -0.01077718]\n",
      "[ episode 189 ][ timestamp 46 ] state=[ 0.00126228 -0.21930593 -0.0575341  -0.01077718], action=1, reward=1.0, next_state=[-0.00312384 -0.02340808 -0.05774965 -0.32104344]\n",
      "[ episode 189 ][ timestamp 47 ] state=[-0.00312384 -0.02340808 -0.05774965 -0.32104344], action=0, reward=1.0, next_state=[-0.003592   -0.21766211 -0.06417052 -0.0471171 ]\n",
      "[ episode 189 ][ timestamp 48 ] state=[-0.003592   -0.21766211 -0.06417052 -0.0471171 ], action=1, reward=1.0, next_state=[-0.00794525 -0.02168149 -0.06511286 -0.35933616]\n",
      "[ episode 189 ][ timestamp 49 ] state=[-0.00794525 -0.02168149 -0.06511286 -0.35933616], action=1, reward=1.0, next_state=[-0.00837888  0.17430273 -0.07229958 -0.67181918]\n",
      "[ episode 189 ][ timestamp 50 ] state=[-0.00837888  0.17430273 -0.07229958 -0.67181918], action=0, reward=1.0, next_state=[-0.00489282 -0.01974367 -0.08573596 -0.40274756]\n",
      "[ episode 189 ][ timestamp 51 ] state=[-0.00489282 -0.01974367 -0.08573596 -0.40274756], action=1, reward=1.0, next_state=[-0.0052877   0.17648301 -0.09379092 -0.72118195]\n",
      "[ episode 189 ][ timestamp 52 ] state=[-0.0052877   0.17648301 -0.09379092 -0.72118195], action=1, reward=1.0, next_state=[-0.00175804  0.37276869 -0.10821455 -1.04185054]\n",
      "[ episode 189 ][ timestamp 53 ] state=[-0.00175804  0.37276869 -0.10821455 -1.04185054], action=0, reward=1.0, next_state=[ 0.00569734  0.17923742 -0.12905157 -0.78500473]\n",
      "[ episode 189 ][ timestamp 54 ] state=[ 0.00569734  0.17923742 -0.12905157 -0.78500473], action=0, reward=1.0, next_state=[ 0.00928209 -0.01389754 -0.14475166 -0.53554728]\n",
      "[ episode 189 ][ timestamp 55 ] state=[ 0.00928209 -0.01389754 -0.14475166 -0.53554728], action=0, reward=1.0, next_state=[ 0.00900414 -0.20671922 -0.15546261 -0.29174815]\n",
      "[ episode 189 ][ timestamp 56 ] state=[ 0.00900414 -0.20671922 -0.15546261 -0.29174815], action=1, reward=1.0, next_state=[ 0.00486975 -0.00976182 -0.16129757 -0.62914341]\n",
      "[ episode 189 ][ timestamp 57 ] state=[ 0.00486975 -0.00976182 -0.16129757 -0.62914341], action=0, reward=1.0, next_state=[ 0.00467451 -0.20230909 -0.17388044 -0.39128762]\n",
      "[ episode 189 ][ timestamp 58 ] state=[ 0.00467451 -0.20230909 -0.17388044 -0.39128762], action=1, reward=1.0, next_state=[ 6.28333214e-04 -5.20091691e-03 -1.81706189e-01 -7.33355171e-01]\n",
      "[ episode 189 ][ timestamp 59 ] state=[ 6.28333214e-04 -5.20091691e-03 -1.81706189e-01 -7.33355171e-01], action=0, reward=1.0, next_state=[ 0.00052431 -0.19740947 -0.19637329 -0.50291703]\n",
      "[ episode 189 ][ timestamp 60 ] state=[ 0.00052431 -0.19740947 -0.19637329 -0.50291703], action=1, reward=1.0, next_state=[-3.42387448e-03 -1.40731380e-04 -2.06431633e-01 -8.50496471e-01]\n",
      "[ episode 189 ][ timestamp 61 ] state=[-3.42387448e-03 -1.40731380e-04 -2.06431633e-01 -8.50496471e-01], action=0, reward=-1.0, next_state=[-0.00342669 -0.19194059 -0.22344156 -0.62916574]\n",
      "[ Ended! ] Episode 189: Exploration_rate=0.3897078735047413. Score=61.\n",
      "[ Experience replay ] starts\n",
      "[ episode 190 ] state=[-0.03029137 -0.01660116 -0.02513016  0.04076709]\n",
      "[ episode 190 ][ timestamp 1 ] state=[-0.03029137 -0.01660116 -0.02513016  0.04076709], action=0, reward=1.0, next_state=[-0.03062339 -0.2113539  -0.02431481  0.32541648]\n",
      "[ episode 190 ][ timestamp 2 ] state=[-0.03062339 -0.2113539  -0.02431481  0.32541648], action=0, reward=1.0, next_state=[-0.03485047 -0.40612137 -0.01780648  0.61033347]\n",
      "[ episode 190 ][ timestamp 3 ] state=[-0.03485047 -0.40612137 -0.01780648  0.61033347], action=1, reward=1.0, next_state=[-0.0429729  -0.2107551  -0.00559981  0.3120957 ]\n",
      "[ episode 190 ][ timestamp 4 ] state=[-0.0429729  -0.2107551  -0.00559981  0.3120957 ], action=0, reward=1.0, next_state=[-0.047188   -0.40579683  0.0006421   0.60300737]\n",
      "[ episode 190 ][ timestamp 5 ] state=[-0.047188   -0.40579683  0.0006421   0.60300737], action=0, reward=1.0, next_state=[-0.05530394 -0.60092775  0.01270225  0.89589248]\n",
      "[ episode 190 ][ timestamp 6 ] state=[-0.05530394 -0.60092775  0.01270225  0.89589248], action=0, reward=1.0, next_state=[-0.06732249 -0.7962196   0.0306201   1.19254098]\n",
      "[ episode 190 ][ timestamp 7 ] state=[-0.06732249 -0.7962196   0.0306201   1.19254098], action=1, reward=1.0, next_state=[-0.08324689 -0.60150739  0.05447092  0.90961047]\n",
      "[ episode 190 ][ timestamp 8 ] state=[-0.08324689 -0.60150739  0.05447092  0.90961047], action=1, reward=1.0, next_state=[-0.09527703 -0.40716329  0.07266313  0.63453322]\n",
      "[ episode 190 ][ timestamp 9 ] state=[-0.09527703 -0.40716329  0.07266313  0.63453322], action=0, reward=1.0, next_state=[-0.1034203  -0.60321955  0.08535379  0.94918574]\n",
      "[ episode 190 ][ timestamp 10 ] state=[-0.1034203  -0.60321955  0.08535379  0.94918574], action=1, reward=1.0, next_state=[-0.11548469 -0.40934392  0.1043375   0.68449454]\n",
      "[ episode 190 ][ timestamp 11 ] state=[-0.11548469 -0.40934392  0.1043375   0.68449454], action=1, reward=1.0, next_state=[-0.12367157 -0.21581348  0.1180274   0.42639717]\n",
      "[ episode 190 ][ timestamp 12 ] state=[-0.12367157 -0.21581348  0.1180274   0.42639717], action=0, reward=1.0, next_state=[-0.12798784 -0.41239211  0.12655534  0.75383322]\n",
      "[ episode 190 ][ timestamp 13 ] state=[-0.12798784 -0.41239211  0.12655534  0.75383322], action=1, reward=1.0, next_state=[-0.13623568 -0.21922103  0.141632    0.50350194]\n",
      "[ episode 190 ][ timestamp 14 ] state=[-0.13623568 -0.21922103  0.141632    0.50350194], action=1, reward=1.0, next_state=[-0.1406201  -0.02634952  0.15170204  0.25859227]\n",
      "[ episode 190 ][ timestamp 15 ] state=[-0.1406201  -0.02634952  0.15170204  0.25859227], action=1, reward=1.0, next_state=[-0.14114709  0.16631793  0.15687389  0.01733971]\n",
      "[ episode 190 ][ timestamp 16 ] state=[-0.14114709  0.16631793  0.15687389  0.01733971], action=0, reward=1.0, next_state=[-0.13782073 -0.03066524  0.15722068  0.35511818]\n",
      "[ episode 190 ][ timestamp 17 ] state=[-0.13782073 -0.03066524  0.15722068  0.35511818], action=1, reward=1.0, next_state=[-0.13843404  0.16191272  0.16432305  0.11584673]\n",
      "[ episode 190 ][ timestamp 18 ] state=[-0.13843404  0.16191272  0.16432305  0.11584673], action=0, reward=1.0, next_state=[-0.13519578 -0.0351357   0.16663998  0.45553164]\n",
      "[ episode 190 ][ timestamp 19 ] state=[-0.13519578 -0.0351357   0.16663998  0.45553164], action=1, reward=1.0, next_state=[-0.1358985   0.1572866   0.17575061  0.21966218]\n",
      "[ episode 190 ][ timestamp 20 ] state=[-0.1358985   0.1572866   0.17575061  0.21966218], action=1, reward=1.0, next_state=[-0.13275276  0.34951783  0.18014386 -0.01283777]\n",
      "[ episode 190 ][ timestamp 21 ] state=[-0.13275276  0.34951783  0.18014386 -0.01283777], action=0, reward=1.0, next_state=[-0.12576241  0.1523311   0.1798871   0.3308323 ]\n",
      "[ episode 190 ][ timestamp 22 ] state=[-0.12576241  0.1523311   0.1798871   0.3308323 ], action=1, reward=1.0, next_state=[-0.12271579  0.34449749  0.18650375  0.09983598]\n",
      "[ episode 190 ][ timestamp 23 ] state=[-0.12271579  0.34449749  0.18650375  0.09983598], action=0, reward=1.0, next_state=[-0.11582584  0.1472604   0.18850047  0.44507582]\n",
      "[ episode 190 ][ timestamp 24 ] state=[-0.11582584  0.1472604   0.18850047  0.44507582], action=1, reward=1.0, next_state=[-0.11288063  0.33928541  0.19740198  0.21723202]\n",
      "[ episode 190 ][ timestamp 25 ] state=[-0.11288063  0.33928541  0.19740198  0.21723202], action=0, reward=1.0, next_state=[-0.10609492  0.14196976  0.20174662  0.56511751]\n",
      "[ episode 190 ][ timestamp 26 ] state=[-0.10609492  0.14196976  0.20174662  0.56511751], action=1, reward=-1.0, next_state=[-0.10325552  0.33377516  0.21304897  0.34215666]\n",
      "[ Ended! ] Episode 190: Exploration_rate=0.3877593341372176. Score=26.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 191 ] state=[ 0.02869297  0.00877773 -0.03693703  0.0121021 ]\n",
      "[ episode 191 ][ timestamp 1 ] state=[ 0.02869297  0.00877773 -0.03693703  0.0121021 ], action=1, reward=1.0, next_state=[ 0.02886852  0.2044094  -0.03669498 -0.29200227]\n",
      "[ episode 191 ][ timestamp 2 ] state=[ 0.02886852  0.2044094  -0.03669498 -0.29200227], action=0, reward=1.0, next_state=[ 0.03295671  0.00982935 -0.04253503 -0.01111457]\n",
      "[ episode 191 ][ timestamp 3 ] state=[ 0.03295671  0.00982935 -0.04253503 -0.01111457], action=1, reward=1.0, next_state=[ 0.0331533   0.20553468 -0.04275732 -0.31690858]\n",
      "[ episode 191 ][ timestamp 4 ] state=[ 0.0331533   0.20553468 -0.04275732 -0.31690858], action=1, reward=1.0, next_state=[ 0.03726399  0.40123874 -0.04909549 -0.62276319]\n",
      "[ episode 191 ][ timestamp 5 ] state=[ 0.03726399  0.40123874 -0.04909549 -0.62276319], action=0, reward=1.0, next_state=[ 0.04528877  0.20683545 -0.06155076 -0.3459379 ]\n",
      "[ episode 191 ][ timestamp 6 ] state=[ 0.04528877  0.20683545 -0.06155076 -0.3459379 ], action=1, reward=1.0, next_state=[ 0.04942548  0.40277647 -0.06846951 -0.65737737]\n",
      "[ episode 191 ][ timestamp 7 ] state=[ 0.04942548  0.40277647 -0.06846951 -0.65737737], action=0, reward=1.0, next_state=[ 0.05748101  0.20867104 -0.08161706 -0.38701575]\n",
      "[ episode 191 ][ timestamp 8 ] state=[ 0.05748101  0.20867104 -0.08161706 -0.38701575], action=1, reward=1.0, next_state=[ 0.06165443  0.40485099 -0.08935738 -0.70427489]\n",
      "[ episode 191 ][ timestamp 9 ] state=[ 0.06165443  0.40485099 -0.08935738 -0.70427489], action=0, reward=1.0, next_state=[ 0.06975145  0.21107347 -0.10344287 -0.44100441]\n",
      "[ episode 191 ][ timestamp 10 ] state=[ 0.06975145  0.21107347 -0.10344287 -0.44100441], action=1, reward=1.0, next_state=[ 0.07397292  0.40749555 -0.11226296 -0.76442058]\n",
      "[ episode 191 ][ timestamp 11 ] state=[ 0.07397292  0.40749555 -0.11226296 -0.76442058], action=0, reward=1.0, next_state=[ 0.08212283  0.21408386 -0.12755137 -0.50906533]\n",
      "[ episode 191 ][ timestamp 12 ] state=[ 0.08212283  0.21408386 -0.12755137 -0.50906533], action=0, reward=1.0, next_state=[ 0.0864045   0.0209679  -0.13773268 -0.2591431 ]\n",
      "[ episode 191 ][ timestamp 13 ] state=[ 0.0864045   0.0209679  -0.13773268 -0.2591431 ], action=1, reward=1.0, next_state=[ 0.08682386  0.21775982 -0.14291554 -0.591901  ]\n",
      "[ episode 191 ][ timestamp 14 ] state=[ 0.08682386  0.21775982 -0.14291554 -0.591901  ], action=1, reward=1.0, next_state=[ 0.09117906  0.41456278 -0.15475356 -0.92597011]\n",
      "[ episode 191 ][ timestamp 15 ] state=[ 0.09117906  0.41456278 -0.15475356 -0.92597011], action=1, reward=1.0, next_state=[ 0.09947031  0.61139775 -0.17327297 -1.26301032]\n",
      "[ episode 191 ][ timestamp 16 ] state=[ 0.09947031  0.61139775 -0.17327297 -1.26301032], action=0, reward=1.0, next_state=[ 0.11169827  0.41886178 -0.19853317 -1.02921871]\n",
      "[ episode 191 ][ timestamp 17 ] state=[ 0.11169827  0.41886178 -0.19853317 -1.02921871], action=0, reward=-1.0, next_state=[ 0.1200755   0.22685555 -0.21911755 -0.80485281]\n",
      "[ Ended! ] Episode 191: Exploration_rate=0.3858205374665315. Score=17.\n",
      "[ Experience replay ] starts\n",
      "[ episode 192 ] state=[ 0.04522908  0.01981946  0.02629863 -0.03419746]\n",
      "[ episode 192 ][ timestamp 1 ] state=[ 0.04522908  0.01981946  0.02629863 -0.03419746], action=0, reward=1.0, next_state=[ 0.04562546 -0.17566956  0.02561468  0.26666557]\n",
      "[ episode 192 ][ timestamp 2 ] state=[ 0.04562546 -0.17566956  0.02561468  0.26666557], action=0, reward=1.0, next_state=[ 0.04211207 -0.37114754  0.03094799  0.56731625]\n",
      "[ episode 192 ][ timestamp 3 ] state=[ 0.04211207 -0.37114754  0.03094799  0.56731625], action=1, reward=1.0, next_state=[ 0.03468912 -0.17647307  0.04229431  0.28454163]\n",
      "[ episode 192 ][ timestamp 4 ] state=[ 0.03468912 -0.17647307  0.04229431  0.28454163], action=1, reward=1.0, next_state=[0.03115966 0.01802094 0.04798515 0.00549233]\n",
      "[ episode 192 ][ timestamp 5 ] state=[0.03115966 0.01802094 0.04798515 0.00549233], action=1, reward=1.0, next_state=[ 0.03152008  0.21242305  0.04809499 -0.27167295]\n",
      "[ episode 192 ][ timestamp 6 ] state=[ 0.03152008  0.21242305  0.04809499 -0.27167295], action=0, reward=1.0, next_state=[0.03576854 0.01664898 0.04266154 0.03578306]\n",
      "[ episode 192 ][ timestamp 7 ] state=[0.03576854 0.01664898 0.04266154 0.03578306], action=0, reward=1.0, next_state=[ 0.03610152 -0.17905796  0.0433772   0.34161506]\n",
      "[ episode 192 ][ timestamp 8 ] state=[ 0.03610152 -0.17905796  0.0433772   0.34161506], action=0, reward=1.0, next_state=[ 0.03252036 -0.37476936  0.0502095   0.64765491]\n",
      "[ episode 192 ][ timestamp 9 ] state=[ 0.03252036 -0.37476936  0.0502095   0.64765491], action=0, reward=1.0, next_state=[ 0.02502497 -0.57055356  0.0631626   0.95571649]\n",
      "[ episode 192 ][ timestamp 10 ] state=[ 0.02502497 -0.57055356  0.0631626   0.95571649], action=1, reward=1.0, next_state=[ 0.0136139  -0.37633541  0.08227693  0.68352767]\n",
      "[ episode 192 ][ timestamp 11 ] state=[ 0.0136139  -0.37633541  0.08227693  0.68352767], action=1, reward=1.0, next_state=[ 0.00608719 -0.18244648  0.09594748  0.41784024]\n",
      "[ episode 192 ][ timestamp 12 ] state=[ 0.00608719 -0.18244648  0.09594748  0.41784024], action=1, reward=1.0, next_state=[0.00243826 0.01119416 0.10430428 0.15688055]\n",
      "[ episode 192 ][ timestamp 13 ] state=[0.00243826 0.01119416 0.10430428 0.15688055], action=1, reward=1.0, next_state=[ 0.00266215  0.20468009  0.10744189 -0.10116114]\n",
      "[ episode 192 ][ timestamp 14 ] state=[ 0.00266215  0.20468009  0.10744189 -0.10116114], action=0, reward=1.0, next_state=[0.00675575 0.00819549 0.10541867 0.22339344]\n",
      "[ episode 192 ][ timestamp 15 ] state=[0.00675575 0.00819549 0.10541867 0.22339344], action=1, reward=1.0, next_state=[ 0.00691966  0.20166515  0.10988654 -0.03426428]\n",
      "[ episode 192 ][ timestamp 16 ] state=[ 0.00691966  0.20166515  0.10988654 -0.03426428], action=0, reward=1.0, next_state=[0.01095296 0.00515297 0.10920126 0.29096776]\n",
      "[ episode 192 ][ timestamp 17 ] state=[0.01095296 0.00515297 0.10920126 0.29096776], action=1, reward=1.0, next_state=[0.01105602 0.19856215 0.11502061 0.03462347]\n",
      "[ episode 192 ][ timestamp 18 ] state=[0.01105602 0.19856215 0.11502061 0.03462347], action=0, reward=1.0, next_state=[0.01502727 0.00199472 0.11571308 0.3612679 ]\n",
      "[ episode 192 ][ timestamp 19 ] state=[0.01502727 0.00199472 0.11571308 0.3612679 ], action=1, reward=1.0, next_state=[0.01506716 0.19529812 0.12293844 0.1071956 ]\n",
      "[ episode 192 ][ timestamp 20 ] state=[0.01506716 0.19529812 0.12293844 0.1071956 ], action=1, reward=1.0, next_state=[ 0.01897312  0.38846349  0.12508235 -0.14431268]\n",
      "[ episode 192 ][ timestamp 21 ] state=[ 0.01897312  0.38846349  0.12508235 -0.14431268], action=1, reward=1.0, next_state=[ 0.02674239  0.58159282  0.1221961  -0.39506502]\n",
      "[ episode 192 ][ timestamp 22 ] state=[ 0.02674239  0.58159282  0.1221961  -0.39506502], action=0, reward=1.0, next_state=[ 0.03837425  0.38496811  0.1142948  -0.06649089]\n",
      "[ episode 192 ][ timestamp 23 ] state=[ 0.03837425  0.38496811  0.1142948  -0.06649089], action=1, reward=1.0, next_state=[ 0.04607361  0.57828166  0.11296498 -0.32103974]\n",
      "[ episode 192 ][ timestamp 24 ] state=[ 0.04607361  0.57828166  0.11296498 -0.32103974], action=0, reward=1.0, next_state=[0.05763924 0.38174744 0.10654418 0.00502371]\n",
      "[ episode 192 ][ timestamp 25 ] state=[0.05763924 0.38174744 0.10654418 0.00502371], action=0, reward=1.0, next_state=[0.06527419 0.18527161 0.10664466 0.32933106]\n",
      "[ episode 192 ][ timestamp 26 ] state=[0.06527419 0.18527161 0.10664466 0.32933106], action=1, reward=1.0, next_state=[0.06897962 0.37872658 0.11323128 0.07209131]\n",
      "[ episode 192 ][ timestamp 27 ] state=[0.06897962 0.37872658 0.11323128 0.07209131], action=0, reward=1.0, next_state=[0.07655416 0.18217874 0.1146731  0.39824398]\n",
      "[ episode 192 ][ timestamp 28 ] state=[0.07655416 0.18217874 0.1146731  0.39824398], action=1, reward=1.0, next_state=[0.08019773 0.375503   0.12263798 0.14380221]\n",
      "[ episode 192 ][ timestamp 29 ] state=[0.08019773 0.375503   0.12263798 0.14380221], action=1, reward=1.0, next_state=[ 0.08770779  0.56867467  0.12551403 -0.10781378]\n",
      "[ episode 192 ][ timestamp 30 ] state=[ 0.08770779  0.56867467  0.12551403 -0.10781378], action=0, reward=1.0, next_state=[0.09908128 0.37199845 0.12335775 0.22168411]\n",
      "[ episode 192 ][ timestamp 31 ] state=[0.09908128 0.37199845 0.12335775 0.22168411], action=1, reward=1.0, next_state=[ 0.10652125  0.56516108  0.12779144 -0.02968282]\n",
      "[ episode 192 ][ timestamp 32 ] state=[ 0.10652125  0.56516108  0.12779144 -0.02968282], action=0, reward=1.0, next_state=[0.11782447 0.36846017 0.12719778 0.30043113]\n",
      "[ episode 192 ][ timestamp 33 ] state=[0.11782447 0.36846017 0.12719778 0.30043113], action=1, reward=1.0, next_state=[0.12519368 0.56156133 0.1332064  0.0504148 ]\n",
      "[ episode 192 ][ timestamp 34 ] state=[0.12519368 0.56156133 0.1332064  0.0504148 ], action=0, reward=1.0, next_state=[0.13642491 0.3648058  0.1342147  0.38198052]\n",
      "[ episode 192 ][ timestamp 35 ] state=[0.13642491 0.3648058  0.1342147  0.38198052], action=1, reward=1.0, next_state=[0.14372102 0.55779215 0.14185431 0.13444513]\n",
      "[ episode 192 ][ timestamp 36 ] state=[0.14372102 0.55779215 0.14185431 0.13444513], action=0, reward=1.0, next_state=[0.15487686 0.36095336 0.14454321 0.46830304]\n",
      "[ episode 192 ][ timestamp 37 ] state=[0.15487686 0.36095336 0.14454321 0.46830304], action=1, reward=1.0, next_state=[0.16209593 0.55376921 0.15390927 0.22444324]\n",
      "[ episode 192 ][ timestamp 38 ] state=[0.16209593 0.55376921 0.15390927 0.22444324], action=0, reward=1.0, next_state=[0.17317132 0.35682088 0.15839814 0.56144453]\n",
      "[ episode 192 ][ timestamp 39 ] state=[0.17317132 0.35682088 0.15839814 0.56144453], action=1, reward=1.0, next_state=[0.18030773 0.54940687 0.16962703 0.32255651]\n",
      "[ episode 192 ][ timestamp 40 ] state=[0.18030773 0.54940687 0.16962703 0.32255651], action=1, reward=1.0, next_state=[0.19129587 0.74175855 0.17607816 0.08780153]\n",
      "[ episode 192 ][ timestamp 41 ] state=[0.19129587 0.74175855 0.17607816 0.08780153], action=0, reward=1.0, next_state=[0.20613104 0.54460722 0.17783419 0.43045598]\n",
      "[ episode 192 ][ timestamp 42 ] state=[0.20613104 0.54460722 0.17783419 0.43045598], action=1, reward=1.0, next_state=[0.21702319 0.73682405 0.18644331 0.19868598]\n",
      "[ episode 192 ][ timestamp 43 ] state=[0.21702319 0.73682405 0.18644331 0.19868598], action=1, reward=1.0, next_state=[ 0.23175967  0.928858    0.19041703 -0.02987564]\n",
      "[ episode 192 ][ timestamp 44 ] state=[ 0.23175967  0.928858    0.19041703 -0.02987564], action=0, reward=1.0, next_state=[0.25033683 0.73158797 0.18981951 0.31632596]\n",
      "[ episode 192 ][ timestamp 45 ] state=[0.25033683 0.73158797 0.18981951 0.31632596], action=1, reward=1.0, next_state=[0.26496859 0.92357087 0.19614603 0.08899652]\n",
      "[ episode 192 ][ timestamp 46 ] state=[0.26496859 0.92357087 0.19614603 0.08899652], action=0, reward=1.0, next_state=[0.28344    0.72625737 0.19792596 0.43658939]\n",
      "[ episode 192 ][ timestamp 47 ] state=[0.28344    0.72625737 0.19792596 0.43658939], action=1, reward=1.0, next_state=[0.29796515 0.91810811 0.20665775 0.21224272]\n",
      "[ episode 192 ][ timestamp 48 ] state=[0.29796515 0.91810811 0.20665775 0.21224272], action=1, reward=-1.0, next_state=[ 0.31632731  1.10976853  0.21090261 -0.00880488]\n",
      "[ Ended! ] Episode 192: Exploration_rate=0.38389143477919885. Score=48.\n",
      "[ Experience replay ] starts\n",
      "[ episode 193 ] state=[ 0.02103456 -0.02086628  0.04777548 -0.01513857]\n",
      "[ episode 193 ][ timestamp 1 ] state=[ 0.02103456 -0.02086628  0.04777548 -0.01513857], action=0, reward=1.0, next_state=[ 0.02061724 -0.21663967  0.04747271  0.2922271 ]\n",
      "[ episode 193 ][ timestamp 2 ] state=[ 0.02061724 -0.21663967  0.04747271  0.2922271 ], action=1, reward=1.0, next_state=[ 0.01628444 -0.02222561  0.05331725  0.01488629]\n",
      "[ episode 193 ][ timestamp 3 ] state=[ 0.01628444 -0.02222561  0.05331725  0.01488629], action=1, reward=1.0, next_state=[ 0.01583993  0.17209277  0.05361498 -0.26050924]\n",
      "[ episode 193 ][ timestamp 4 ] state=[ 0.01583993  0.17209277  0.05361498 -0.26050924], action=1, reward=1.0, next_state=[ 0.01928178  0.36641     0.04840479 -0.53581099]\n",
      "[ episode 193 ][ timestamp 5 ] state=[ 0.01928178  0.36641     0.04840479 -0.53581099], action=0, reward=1.0, next_state=[ 0.02660998  0.17064202  0.03768857 -0.22827753]\n",
      "[ episode 193 ][ timestamp 6 ] state=[ 0.02660998  0.17064202  0.03768857 -0.22827753], action=0, reward=1.0, next_state=[ 0.03002283 -0.02499769  0.03312302  0.07605146]\n",
      "[ episode 193 ][ timestamp 7 ] state=[ 0.03002283 -0.02499769  0.03312302  0.07605146], action=0, reward=1.0, next_state=[ 0.02952287 -0.22057844  0.03464405  0.37899806]\n",
      "[ episode 193 ][ timestamp 8 ] state=[ 0.02952287 -0.22057844  0.03464405  0.37899806], action=1, reward=1.0, next_state=[ 0.0251113  -0.02596517  0.04222401  0.09743663]\n",
      "[ episode 193 ][ timestamp 9 ] state=[ 0.0251113  -0.02596517  0.04222401  0.09743663], action=1, reward=1.0, next_state=[ 0.024592    0.16852698  0.04417275 -0.18163139]\n",
      "[ episode 193 ][ timestamp 10 ] state=[ 0.024592    0.16852698  0.04417275 -0.18163139], action=0, reward=1.0, next_state=[ 0.02796254 -0.0271983   0.04054012  0.12465271]\n",
      "[ episode 193 ][ timestamp 11 ] state=[ 0.02796254 -0.0271983   0.04054012  0.12465271], action=1, reward=1.0, next_state=[ 0.02741857  0.16732012  0.04303317 -0.15496965]\n",
      "[ episode 193 ][ timestamp 12 ] state=[ 0.02741857  0.16732012  0.04303317 -0.15496965], action=0, reward=1.0, next_state=[ 0.03076498 -0.02839072  0.03993378  0.15097268]\n",
      "[ episode 193 ][ timestamp 13 ] state=[ 0.03076498 -0.02839072  0.03993378  0.15097268], action=1, reward=1.0, next_state=[ 0.03019716  0.16613733  0.04295323 -0.12884936]\n",
      "[ episode 193 ][ timestamp 14 ] state=[ 0.03019716  0.16613733  0.04295323 -0.12884936], action=0, reward=1.0, next_state=[ 0.03351991 -0.02957277  0.04037625  0.1770694 ]\n",
      "[ episode 193 ][ timestamp 15 ] state=[ 0.03351991 -0.02957277  0.04037625  0.1770694 ], action=1, reward=1.0, next_state=[ 0.03292845  0.1649488   0.04391763 -0.10260776]\n",
      "[ episode 193 ][ timestamp 16 ] state=[ 0.03292845  0.1649488   0.04391763 -0.10260776], action=0, reward=1.0, next_state=[ 0.03622743 -0.03077413  0.04186548  0.20360119]\n",
      "[ episode 193 ][ timestamp 17 ] state=[ 0.03622743 -0.03077413  0.04186548  0.20360119], action=1, reward=1.0, next_state=[ 0.03561195  0.16372487  0.0459375  -0.07558682]\n",
      "[ episode 193 ][ timestamp 18 ] state=[ 0.03561195  0.16372487  0.0459375  -0.07558682], action=0, reward=1.0, next_state=[ 0.03888644 -0.03202451  0.04442577  0.23122836]\n",
      "[ episode 193 ][ timestamp 19 ] state=[ 0.03888644 -0.03202451  0.04442577  0.23122836], action=1, reward=1.0, next_state=[ 0.03824595  0.1624354   0.04905033 -0.04711683]\n",
      "[ episode 193 ][ timestamp 20 ] state=[ 0.03824595  0.1624354   0.04905033 -0.04711683], action=1, reward=1.0, next_state=[ 0.04149466  0.35682094  0.048108   -0.32392943]\n",
      "[ episode 193 ][ timestamp 21 ] state=[ 0.04149466  0.35682094  0.048108   -0.32392943], action=0, reward=1.0, next_state=[ 0.04863108  0.16104817  0.04162941 -0.01647172]\n",
      "[ episode 193 ][ timestamp 22 ] state=[ 0.04863108  0.16104817  0.04162941 -0.01647172], action=0, reward=1.0, next_state=[ 0.05185204 -0.03464531  0.04129997  0.28904969]\n",
      "[ episode 193 ][ timestamp 23 ] state=[ 0.05185204 -0.03464531  0.04129997  0.28904969], action=1, reward=1.0, next_state=[0.05115914 0.15986412 0.04708097 0.00967308]\n",
      "[ episode 193 ][ timestamp 24 ] state=[0.05115914 0.15986412 0.04708097 0.00967308], action=1, reward=1.0, next_state=[ 0.05435642  0.35428035  0.04727443 -0.26779143]\n",
      "[ episode 193 ][ timestamp 25 ] state=[ 0.05435642  0.35428035  0.04727443 -0.26779143], action=1, reward=1.0, next_state=[ 0.06144203  0.54869686  0.0419186  -0.54519688]\n",
      "[ episode 193 ][ timestamp 26 ] state=[ 0.06144203  0.54869686  0.0419186  -0.54519688], action=0, reward=1.0, next_state=[ 0.07241596  0.35301174  0.03101466 -0.23960659]\n",
      "[ episode 193 ][ timestamp 27 ] state=[ 0.07241596  0.35301174  0.03101466 -0.23960659], action=0, reward=1.0, next_state=[0.0794762  0.15746077 0.02622253 0.06269565]\n",
      "[ episode 193 ][ timestamp 28 ] state=[0.0794762  0.15746077 0.02622253 0.06269565], action=1, reward=1.0, next_state=[ 0.08262541  0.35219713  0.02747644 -0.22159993]\n",
      "[ episode 193 ][ timestamp 29 ] state=[ 0.08262541  0.35219713  0.02747644 -0.22159993], action=0, reward=1.0, next_state=[0.08966936 0.15669344 0.02304445 0.07962198]\n",
      "[ episode 193 ][ timestamp 30 ] state=[0.08966936 0.15669344 0.02304445 0.07962198], action=1, reward=1.0, next_state=[ 0.09280323  0.35147759  0.02463689 -0.20570221]\n",
      "[ episode 193 ][ timestamp 31 ] state=[ 0.09280323  0.35147759  0.02463689 -0.20570221], action=0, reward=1.0, next_state=[0.09983278 0.15601215 0.02052284 0.09464949]\n",
      "[ episode 193 ][ timestamp 32 ] state=[0.09983278 0.15601215 0.02052284 0.09464949], action=1, reward=1.0, next_state=[ 0.10295302  0.35083404  0.02241583 -0.1914885 ]\n",
      "[ episode 193 ][ timestamp 33 ] state=[ 0.10295302  0.35083404  0.02241583 -0.1914885 ], action=0, reward=1.0, next_state=[0.1099697  0.1553987  0.01858606 0.10818055]\n",
      "[ episode 193 ][ timestamp 34 ] state=[0.1099697  0.1553987  0.01858606 0.10818055], action=1, reward=1.0, next_state=[ 0.11307767  0.35024945  0.02074967 -0.1785811 ]\n",
      "[ episode 193 ][ timestamp 35 ] state=[ 0.11307767  0.35024945  0.02074967 -0.1785811 ], action=0, reward=1.0, next_state=[0.12008266 0.15483681 0.01717805 0.12057473]\n",
      "[ episode 193 ][ timestamp 36 ] state=[0.12008266 0.15483681 0.01717805 0.12057473], action=0, reward=1.0, next_state=[ 0.1231794  -0.04052699  0.01958954  0.41862729]\n",
      "[ episode 193 ][ timestamp 37 ] state=[ 0.1231794  -0.04052699  0.01958954  0.41862729], action=1, reward=1.0, next_state=[0.12236886 0.15431196 0.02796209 0.13218389]\n",
      "[ episode 193 ][ timestamp 38 ] state=[0.12236886 0.15431196 0.02796209 0.13218389], action=1, reward=1.0, next_state=[ 0.1254551   0.34902245  0.03060577 -0.15154788]\n",
      "[ episode 193 ][ timestamp 39 ] state=[ 0.1254551   0.34902245  0.03060577 -0.15154788], action=0, reward=1.0, next_state=[0.13243555 0.15347592 0.02757481 0.15063123]\n",
      "[ episode 193 ][ timestamp 40 ] state=[0.13243555 0.15347592 0.02757481 0.15063123], action=1, reward=1.0, next_state=[ 0.13550507  0.34819239  0.03058744 -0.13322647]\n",
      "[ episode 193 ][ timestamp 41 ] state=[ 0.13550507  0.34819239  0.03058744 -0.13322647], action=1, reward=1.0, next_state=[ 0.14246891  0.54286315  0.02792291 -0.41610472]\n",
      "[ episode 193 ][ timestamp 42 ] state=[ 0.14246891  0.54286315  0.02792291 -0.41610472], action=1, reward=1.0, next_state=[ 0.15332618  0.73757847  0.01960081 -0.69985557]\n",
      "[ episode 193 ][ timestamp 43 ] state=[ 0.15332618  0.73757847  0.01960081 -0.69985557], action=0, reward=1.0, next_state=[ 0.16807775  0.54219034  0.0056037  -0.40106741]\n",
      "[ episode 193 ][ timestamp 44 ] state=[ 0.16807775  0.54219034  0.0056037  -0.40106741], action=0, reward=1.0, next_state=[ 0.17892155  0.34698936 -0.00241765 -0.10662305]\n",
      "[ episode 193 ][ timestamp 45 ] state=[ 0.17892155  0.34698936 -0.00241765 -0.10662305], action=0, reward=1.0, next_state=[ 0.18586134  0.15190213 -0.00455011  0.18529614]\n",
      "[ episode 193 ][ timestamp 46 ] state=[ 0.18586134  0.15190213 -0.00455011  0.18529614], action=0, reward=1.0, next_state=[ 0.18889938 -0.04315442 -0.00084419  0.47654022]\n",
      "[ episode 193 ][ timestamp 47 ] state=[ 0.18889938 -0.04315442 -0.00084419  0.47654022], action=1, reward=1.0, next_state=[0.1880363  0.15197944 0.00868662 0.18359134]\n",
      "[ episode 193 ][ timestamp 48 ] state=[0.1880363  0.15197944 0.00868662 0.18359134], action=1, reward=1.0, next_state=[ 0.19107588  0.34697603  0.01235845 -0.10633867]\n",
      "[ episode 193 ][ timestamp 49 ] state=[ 0.19107588  0.34697603  0.01235845 -0.10633867], action=0, reward=1.0, next_state=[0.1980154  0.15167918 0.01023167 0.19021753]\n",
      "[ episode 193 ][ timestamp 50 ] state=[0.1980154  0.15167918 0.01023167 0.19021753], action=1, reward=1.0, next_state=[ 0.20104899  0.34665327  0.01403602 -0.09922024]\n",
      "[ episode 193 ][ timestamp 51 ] state=[ 0.20104899  0.34665327  0.01403602 -0.09922024], action=1, reward=1.0, next_state=[ 0.20798205  0.54157127  0.01205162 -0.387442  ]\n",
      "[ episode 193 ][ timestamp 52 ] state=[ 0.20798205  0.54157127  0.01205162 -0.387442  ], action=1, reward=1.0, next_state=[ 0.21881348  0.7365201   0.00430278 -0.67630091]\n",
      "[ episode 193 ][ timestamp 53 ] state=[ 0.21881348  0.7365201   0.00430278 -0.67630091], action=1, reward=1.0, next_state=[ 0.23354388  0.931582   -0.00922324 -0.96762604]\n",
      "[ episode 193 ][ timestamp 54 ] state=[ 0.23354388  0.931582   -0.00922324 -0.96762604], action=1, reward=1.0, next_state=[ 0.25217552  1.12682658 -0.02857576 -1.26319204]\n",
      "[ episode 193 ][ timestamp 55 ] state=[ 0.25217552  1.12682658 -0.02857576 -1.26319204], action=0, reward=1.0, next_state=[ 0.27471205  0.93208137 -0.0538396  -0.97959363]\n",
      "[ episode 193 ][ timestamp 56 ] state=[ 0.27471205  0.93208137 -0.0538396  -0.97959363], action=1, reward=1.0, next_state=[ 0.29335368  1.1278821  -0.07343147 -1.28869034]\n",
      "[ episode 193 ][ timestamp 57 ] state=[ 0.29335368  1.1278821  -0.07343147 -1.28869034], action=0, reward=1.0, next_state=[ 0.31591132  0.93376707 -0.09920528 -1.01987192]\n",
      "[ episode 193 ][ timestamp 58 ] state=[ 0.31591132  0.93376707 -0.09920528 -1.01987192], action=0, reward=1.0, next_state=[ 0.33458666  0.74009705 -0.11960272 -0.75991379]\n",
      "[ episode 193 ][ timestamp 59 ] state=[ 0.33458666  0.74009705 -0.11960272 -0.75991379], action=0, reward=1.0, next_state=[ 0.3493886   0.5468082  -0.134801   -0.50713119]\n",
      "[ episode 193 ][ timestamp 60 ] state=[ 0.3493886   0.5468082  -0.134801   -0.50713119], action=1, reward=1.0, next_state=[ 0.36032477  0.74354633 -0.14494362 -0.83907278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 193 ][ timestamp 61 ] state=[ 0.36032477  0.74354633 -0.14494362 -0.83907278], action=0, reward=1.0, next_state=[ 0.37519569  0.55066931 -0.16172508 -0.59525537]\n",
      "[ episode 193 ][ timestamp 62 ] state=[ 0.37519569  0.55066931 -0.16172508 -0.59525537], action=0, reward=1.0, next_state=[ 0.38620908  0.3581361  -0.17363018 -0.35756429]\n",
      "[ episode 193 ][ timestamp 63 ] state=[ 0.38620908  0.3581361  -0.17363018 -0.35756429], action=0, reward=1.0, next_state=[ 0.3933718   0.16585288 -0.18078147 -0.12426735]\n",
      "[ episode 193 ][ timestamp 64 ] state=[ 0.3933718   0.16585288 -0.18078147 -0.12426735], action=1, reward=1.0, next_state=[ 0.39668886  0.36304224 -0.18326682 -0.46809185]\n",
      "[ episode 193 ][ timestamp 65 ] state=[ 0.39668886  0.36304224 -0.18326682 -0.46809185], action=0, reward=1.0, next_state=[ 0.40394971  0.17091808 -0.19262865 -0.23831104]\n",
      "[ episode 193 ][ timestamp 66 ] state=[ 0.40394971  0.17091808 -0.19262865 -0.23831104], action=0, reward=1.0, next_state=[ 0.40736807 -0.02100506 -0.19739487 -0.01203417]\n",
      "[ episode 193 ][ timestamp 67 ] state=[ 0.40736807 -0.02100506 -0.19739487 -0.01203417], action=1, reward=1.0, next_state=[ 0.40694797  0.1763195  -0.19763556 -0.35993114]\n",
      "[ episode 193 ][ timestamp 68 ] state=[ 0.40694797  0.1763195  -0.19763556 -0.35993114], action=1, reward=1.0, next_state=[ 0.41047436  0.3736212  -0.20483418 -0.70784988]\n",
      "[ episode 193 ][ timestamp 69 ] state=[ 0.41047436  0.3736212  -0.20483418 -0.70784988], action=0, reward=-1.0, next_state=[ 0.41794678  0.18183655 -0.21899118 -0.48598788]\n",
      "[ Ended! ] Episode 193: Exploration_rate=0.3819719776053028. Score=69.\n",
      "[ Experience replay ] starts\n",
      "[ episode 194 ] state=[-0.03341301  0.00494057  0.00551717  0.0094093 ]\n",
      "[ episode 194 ][ timestamp 1 ] state=[-0.03341301  0.00494057  0.00551717  0.0094093 ], action=0, reward=1.0, next_state=[-0.0333142  -0.19026007  0.00570535  0.30382784]\n",
      "[ episode 194 ][ timestamp 2 ] state=[-0.0333142  -0.19026007  0.00570535  0.30382784], action=1, reward=1.0, next_state=[-0.0371194   0.00478011  0.01178191  0.0129497 ]\n",
      "[ episode 194 ][ timestamp 3 ] state=[-0.0371194   0.00478011  0.01178191  0.0129497 ], action=0, reward=1.0, next_state=[-0.0370238  -0.19050881  0.0120409   0.30932655]\n",
      "[ episode 194 ][ timestamp 4 ] state=[-0.0370238  -0.19050881  0.0120409   0.30932655], action=1, reward=1.0, next_state=[-0.04083397  0.00443953  0.01822743  0.02046517]\n",
      "[ episode 194 ][ timestamp 5 ] state=[-0.04083397  0.00443953  0.01822743  0.02046517], action=0, reward=1.0, next_state=[-0.04074518 -0.19093902  0.01863674  0.31884288]\n",
      "[ episode 194 ][ timestamp 6 ] state=[-0.04074518 -0.19093902  0.01863674  0.31884288], action=1, reward=1.0, next_state=[-0.04456396  0.00391261  0.02501359  0.03209507]\n",
      "[ episode 194 ][ timestamp 7 ] state=[-0.04456396  0.00391261  0.02501359  0.03209507], action=0, reward=1.0, next_state=[-0.04448571 -0.19155895  0.0256555   0.33256392]\n",
      "[ episode 194 ][ timestamp 8 ] state=[-0.04448571 -0.19155895  0.0256555   0.33256392], action=1, reward=1.0, next_state=[-0.04831689  0.00318862  0.03230677  0.04808059]\n",
      "[ episode 194 ][ timestamp 9 ] state=[-0.04831689  0.00318862  0.03230677  0.04808059], action=0, reward=1.0, next_state=[-0.04825312 -0.19238133  0.03326839  0.35077898]\n",
      "[ episode 194 ][ timestamp 10 ] state=[-0.04825312 -0.19238133  0.03326839  0.35077898], action=1, reward=1.0, next_state=[-0.05210074  0.00225209  0.04028397  0.0687695 ]\n",
      "[ episode 194 ][ timestamp 11 ] state=[-0.05210074  0.00225209  0.04028397  0.0687695 ], action=0, reward=1.0, next_state=[-0.0520557  -0.19342356  0.04165936  0.37388513]\n",
      "[ episode 194 ][ timestamp 12 ] state=[-0.0520557  -0.19342356  0.04165936  0.37388513], action=1, reward=1.0, next_state=[-0.05592417  0.00108262  0.04913706  0.0946233 ]\n",
      "[ episode 194 ][ timestamp 13 ] state=[-0.05592417  0.00108262  0.04913706  0.0946233 ], action=0, reward=1.0, next_state=[-0.05590252 -0.1947079   0.05102952  0.4023951 ]\n",
      "[ episode 194 ][ timestamp 14 ] state=[-0.05590252 -0.1947079   0.05102952  0.4023951 ], action=1, reward=1.0, next_state=[-0.05979668 -0.00034546  0.05907743  0.12622712]\n",
      "[ episode 194 ][ timestamp 15 ] state=[-0.05979668 -0.00034546  0.05907743  0.12622712], action=1, reward=1.0, next_state=[-0.05980359  0.19388257  0.06160197 -0.14724801]\n",
      "[ episode 194 ][ timestamp 16 ] state=[-0.05980359  0.19388257  0.06160197 -0.14724801], action=1, reward=1.0, next_state=[-0.05592594  0.38807075  0.05865701 -0.41987824]\n",
      "[ episode 194 ][ timestamp 17 ] state=[-0.05592594  0.38807075  0.05865701 -0.41987824], action=1, reward=1.0, next_state=[-0.04816452  0.58231464  0.05025944 -0.6935077 ]\n",
      "[ episode 194 ][ timestamp 18 ] state=[-0.04816452  0.58231464  0.05025944 -0.6935077 ], action=0, reward=1.0, next_state=[-0.03651823  0.38653283  0.03638929 -0.38543578]\n",
      "[ episode 194 ][ timestamp 19 ] state=[-0.03651823  0.38653283  0.03638929 -0.38543578], action=1, reward=1.0, next_state=[-0.02878757  0.5811198   0.02868057 -0.6664269 ]\n",
      "[ episode 194 ][ timestamp 20 ] state=[-0.02878757  0.5811198   0.02868057 -0.6664269 ], action=0, reward=1.0, next_state=[-0.01716518  0.38561094  0.01535204 -0.36485329]\n",
      "[ episode 194 ][ timestamp 21 ] state=[-0.01716518  0.38561094  0.01535204 -0.36485329], action=1, reward=1.0, next_state=[-0.00945296  0.58051139  0.00805497 -0.65265619]\n",
      "[ episode 194 ][ timestamp 22 ] state=[-0.00945296  0.58051139  0.00805497 -0.65265619], action=0, reward=1.0, next_state=[ 0.00215727  0.3852782  -0.00499815 -0.35744777]\n",
      "[ episode 194 ][ timestamp 23 ] state=[ 0.00215727  0.3852782  -0.00499815 -0.35744777], action=1, reward=1.0, next_state=[ 0.00986283  0.58047085 -0.01214711 -0.65170254]\n",
      "[ episode 194 ][ timestamp 24 ] state=[ 0.00986283  0.58047085 -0.01214711 -0.65170254], action=1, reward=1.0, next_state=[ 0.02147225  0.77575985 -0.02518116 -0.94818559]\n",
      "[ episode 194 ][ timestamp 25 ] state=[ 0.02147225  0.77575985 -0.02518116 -0.94818559], action=1, reward=1.0, next_state=[ 0.03698745  0.97121163 -0.04414487 -1.2486728 ]\n",
      "[ episode 194 ][ timestamp 26 ] state=[ 0.03698745  0.97121163 -0.04414487 -1.2486728 ], action=0, reward=1.0, next_state=[ 0.05641168  0.77668255 -0.06911833 -0.97013783]\n",
      "[ episode 194 ][ timestamp 27 ] state=[ 0.05641168  0.77668255 -0.06911833 -0.97013783], action=0, reward=1.0, next_state=[ 0.07194533  0.58255304 -0.08852108 -0.69994346]\n",
      "[ episode 194 ][ timestamp 28 ] state=[ 0.07194533  0.58255304 -0.08852108 -0.69994346], action=0, reward=1.0, next_state=[ 0.08359639  0.38876255 -0.10251995 -0.43638712]\n",
      "[ episode 194 ][ timestamp 29 ] state=[ 0.08359639  0.38876255 -0.10251995 -0.43638712], action=0, reward=1.0, next_state=[ 0.09137164  0.19522991 -0.1112477  -0.17770049]\n",
      "[ episode 194 ][ timestamp 30 ] state=[ 0.09137164  0.19522991 -0.1112477  -0.17770049], action=0, reward=1.0, next_state=[ 0.09527624  0.00186125 -0.11480171  0.0779201 ]\n",
      "[ episode 194 ][ timestamp 31 ] state=[ 0.09527624  0.00186125 -0.11480171  0.0779201 ], action=0, reward=1.0, next_state=[ 0.09531347 -0.19144379 -0.1132433   0.3322914 ]\n",
      "[ episode 194 ][ timestamp 32 ] state=[ 0.09531347 -0.19144379 -0.1132433   0.3322914 ], action=1, reward=1.0, next_state=[ 0.09148459  0.00509258 -0.10659748  0.0061527 ]\n",
      "[ episode 194 ][ timestamp 33 ] state=[ 0.09148459  0.00509258 -0.10659748  0.0061527 ], action=0, reward=1.0, next_state=[ 0.09158644 -0.18835193 -0.10647442  0.26339211]\n",
      "[ episode 194 ][ timestamp 34 ] state=[ 0.09158644 -0.18835193 -0.10647442  0.26339211], action=0, reward=1.0, next_state=[ 0.0878194  -0.38180572 -0.10120658  0.52068512]\n",
      "[ episode 194 ][ timestamp 35 ] state=[ 0.0878194  -0.38180572 -0.10120658  0.52068512], action=1, reward=1.0, next_state=[ 0.08018329 -0.18541562 -0.09079288  0.19790341]\n",
      "[ episode 194 ][ timestamp 36 ] state=[ 0.08018329 -0.18541562 -0.09079288  0.19790341], action=0, reward=1.0, next_state=[ 0.07647498 -0.37912956 -0.08683481  0.46062104]\n",
      "[ episode 194 ][ timestamp 37 ] state=[ 0.07647498 -0.37912956 -0.08683481  0.46062104], action=1, reward=1.0, next_state=[ 0.06889239 -0.18289447 -0.07762239  0.1418801 ]\n",
      "[ episode 194 ][ timestamp 38 ] state=[ 0.06889239 -0.18289447 -0.07762239  0.1418801 ], action=0, reward=1.0, next_state=[ 0.0652345  -0.37682386 -0.07478479  0.40910021]\n",
      "[ episode 194 ][ timestamp 39 ] state=[ 0.0652345  -0.37682386 -0.07478479  0.40910021], action=1, reward=1.0, next_state=[ 0.05769802 -0.18072568 -0.06660278  0.09380886]\n",
      "[ episode 194 ][ timestamp 40 ] state=[ 0.05769802 -0.18072568 -0.06660278  0.09380886], action=0, reward=1.0, next_state=[ 0.05408351 -0.37483294 -0.0647266   0.36475745]\n",
      "[ episode 194 ][ timestamp 41 ] state=[ 0.05408351 -0.37483294 -0.0647266   0.36475745], action=1, reward=1.0, next_state=[ 0.04658685 -0.17885369 -0.05743146  0.05238783]\n",
      "[ episode 194 ][ timestamp 42 ] state=[ 0.04658685 -0.17885369 -0.05743146  0.05238783], action=0, reward=1.0, next_state=[ 0.04300977 -0.37310712 -0.0563837   0.326412  ]\n",
      "[ episode 194 ][ timestamp 43 ] state=[ 0.04300977 -0.37310712 -0.0563837   0.326412  ], action=1, reward=1.0, next_state=[ 0.03554763 -0.17722963 -0.04985546  0.01649466]\n",
      "[ episode 194 ][ timestamp 44 ] state=[ 0.03554763 -0.17722963 -0.04985546  0.01649466], action=1, reward=1.0, next_state=[ 0.03200304  0.01857054 -0.04952557 -0.2914921 ]\n",
      "[ episode 194 ][ timestamp 45 ] state=[ 0.03200304  0.01857054 -0.04952557 -0.2914921 ], action=0, reward=1.0, next_state=[ 0.03237445 -0.17581155 -0.05535541 -0.01483104]\n",
      "[ episode 194 ][ timestamp 46 ] state=[ 0.03237445 -0.17581155 -0.05535541 -0.01483104], action=1, reward=1.0, next_state=[ 0.02885822  0.02005877 -0.05565203 -0.32445268]\n",
      "[ episode 194 ][ timestamp 47 ] state=[ 0.02885822  0.02005877 -0.05565203 -0.32445268], action=0, reward=1.0, next_state=[ 0.02925939 -0.17422842 -0.06214108 -0.04982633]\n",
      "[ episode 194 ][ timestamp 48 ] state=[ 0.02925939 -0.17422842 -0.06214108 -0.04982633], action=0, reward=1.0, next_state=[ 0.02577483 -0.36840683 -0.06313761  0.22262137]\n",
      "[ episode 194 ][ timestamp 49 ] state=[ 0.02577483 -0.36840683 -0.06313761  0.22262137], action=0, reward=1.0, next_state=[ 0.01840669 -0.56257219 -0.05868518  0.49473897]\n",
      "[ episode 194 ][ timestamp 50 ] state=[ 0.01840669 -0.56257219 -0.05868518  0.49473897], action=1, reward=1.0, next_state=[ 0.00715524 -0.36667385 -0.0487904   0.18415377]\n",
      "[ episode 194 ][ timestamp 51 ] state=[ 0.00715524 -0.36667385 -0.0487904   0.18415377], action=0, reward=1.0, next_state=[-1.78232061e-04 -5.61064967e-01 -4.51073268e-02  4.61054769e-01]\n",
      "[ episode 194 ][ timestamp 52 ] state=[-1.78232061e-04 -5.61064967e-01 -4.51073268e-02  4.61054769e-01], action=1, reward=1.0, next_state=[-0.01139953 -0.36533546 -0.03588623  0.15450208]\n",
      "[ episode 194 ][ timestamp 53 ] state=[-0.01139953 -0.36533546 -0.03588623  0.15450208], action=1, reward=1.0, next_state=[-0.01870624 -0.16971855 -0.03279619 -0.14928265]\n",
      "[ episode 194 ][ timestamp 54 ] state=[-0.01870624 -0.16971855 -0.03279619 -0.14928265], action=0, reward=1.0, next_state=[-0.02210061 -0.36435589 -0.03578184  0.13287602]\n",
      "[ episode 194 ][ timestamp 55 ] state=[-0.02210061 -0.36435589 -0.03578184  0.13287602], action=0, reward=1.0, next_state=[-0.02938773 -0.55894751 -0.03312432  0.41405899]\n",
      "[ episode 194 ][ timestamp 56 ] state=[-0.02938773 -0.55894751 -0.03312432  0.41405899], action=1, reward=1.0, next_state=[-0.04056668 -0.36337209 -0.02484314  0.11112002]\n",
      "[ episode 194 ][ timestamp 57 ] state=[-0.04056668 -0.36337209 -0.02484314  0.11112002], action=0, reward=1.0, next_state=[-0.04783412 -0.55812941 -0.02262074  0.39586271]\n",
      "[ episode 194 ][ timestamp 58 ] state=[-0.04783412 -0.55812941 -0.02262074  0.39586271], action=1, reward=1.0, next_state=[-0.05899671 -0.36269393 -0.01470349  0.09613456]\n",
      "[ episode 194 ][ timestamp 59 ] state=[-0.05899671 -0.36269393 -0.01470349  0.09613456], action=0, reward=1.0, next_state=[-0.06625059 -0.55760209 -0.0127808   0.38414253]\n",
      "[ episode 194 ][ timestamp 60 ] state=[-0.06625059 -0.55760209 -0.0127808   0.38414253], action=1, reward=1.0, next_state=[-0.07740263 -0.36230104 -0.00509795  0.08745743]\n",
      "[ episode 194 ][ timestamp 61 ] state=[-0.07740263 -0.36230104 -0.00509795  0.08745743], action=0, reward=1.0, next_state=[-0.08464865 -0.55734954 -0.0033488   0.3785276 ]\n",
      "[ episode 194 ][ timestamp 62 ] state=[-0.08464865 -0.55734954 -0.0033488   0.3785276 ], action=0, reward=1.0, next_state=[-0.09579564 -0.75242378  0.00422175  0.67015276]\n",
      "[ episode 194 ][ timestamp 63 ] state=[-0.09579564 -0.75242378  0.00422175  0.67015276], action=1, reward=1.0, next_state=[-0.11084412 -0.55736078  0.01762481  0.37880206]\n",
      "[ episode 194 ][ timestamp 64 ] state=[-0.11084412 -0.55736078  0.01762481  0.37880206], action=1, reward=1.0, next_state=[-0.12199133 -0.3624935   0.02520085  0.09172797]\n",
      "[ episode 194 ][ timestamp 65 ] state=[-0.12199133 -0.3624935   0.02520085  0.09172797], action=0, reward=1.0, next_state=[-0.1292412  -0.55796743  0.02703541  0.39225402]\n",
      "[ episode 194 ][ timestamp 66 ] state=[-0.1292412  -0.55796743  0.02703541  0.39225402], action=0, reward=1.0, next_state=[-0.14040055 -0.75346241  0.03488049  0.69333677]\n",
      "[ episode 194 ][ timestamp 67 ] state=[-0.14040055 -0.75346241  0.03488049  0.69333677], action=1, reward=1.0, next_state=[-0.1554698  -0.55884125  0.04874723  0.41183539]\n",
      "[ episode 194 ][ timestamp 68 ] state=[-0.1554698  -0.55884125  0.04874723  0.41183539], action=1, reward=1.0, next_state=[-0.16664662 -0.36444301  0.05698393  0.13491043]\n",
      "[ episode 194 ][ timestamp 69 ] state=[-0.16664662 -0.36444301  0.05698393  0.13491043], action=0, reward=1.0, next_state=[-0.17393548 -0.56033292  0.05968214  0.44501258]\n",
      "[ episode 194 ][ timestamp 70 ] state=[-0.17393548 -0.56033292  0.05968214  0.44501258], action=1, reward=1.0, next_state=[-0.18514214 -0.36610391  0.06858239  0.17172391]\n",
      "[ episode 194 ][ timestamp 71 ] state=[-0.18514214 -0.36610391  0.06858239  0.17172391], action=1, reward=1.0, next_state=[-0.19246422 -0.17202716  0.07201687 -0.09855941]\n",
      "[ episode 194 ][ timestamp 72 ] state=[-0.19246422 -0.17202716  0.07201687 -0.09855941], action=0, reward=1.0, next_state=[-0.19590476 -0.36810341  0.07004568  0.21594725]\n",
      "[ episode 194 ][ timestamp 73 ] state=[-0.19590476 -0.36810341  0.07004568  0.21594725], action=0, reward=1.0, next_state=[-0.20326683 -0.56415319  0.07436463  0.52987738]\n",
      "[ episode 194 ][ timestamp 74 ] state=[-0.20326683 -0.56415319  0.07436463  0.52987738], action=1, reward=1.0, next_state=[-0.2145499  -0.37015181  0.08496218  0.26152263]\n",
      "[ episode 194 ][ timestamp 75 ] state=[-0.2145499  -0.37015181  0.08496218  0.26152263], action=1, reward=1.0, next_state=[-0.22195293 -0.17633895  0.09019263 -0.00319917]\n",
      "[ episode 194 ][ timestamp 76 ] state=[-0.22195293 -0.17633895  0.09019263 -0.00319917], action=0, reward=1.0, next_state=[-0.22547971 -0.37263087  0.09012865  0.31652264]\n",
      "[ episode 194 ][ timestamp 77 ] state=[-0.22547971 -0.37263087  0.09012865  0.31652264], action=1, reward=1.0, next_state=[-0.23293233 -0.17890054  0.0964591   0.0535686 ]\n",
      "[ episode 194 ][ timestamp 78 ] state=[-0.23293233 -0.17890054  0.0964591   0.0535686 ], action=0, reward=1.0, next_state=[-0.23651034 -0.37526375  0.09753047  0.37505922]\n",
      "[ episode 194 ][ timestamp 79 ] state=[-0.23651034 -0.37526375  0.09753047  0.37505922], action=1, reward=1.0, next_state=[-0.24401561 -0.18165264  0.10503165  0.11465123]\n",
      "[ episode 194 ][ timestamp 80 ] state=[-0.24401561 -0.18165264  0.10503165  0.11465123], action=0, reward=1.0, next_state=[-0.24764867 -0.37811051  0.10732468  0.43853664]\n",
      "[ episode 194 ][ timestamp 81 ] state=[-0.24764867 -0.37811051  0.10732468  0.43853664], action=1, reward=1.0, next_state=[-0.25521088 -0.18465828  0.11609541  0.18152083]\n",
      "[ episode 194 ][ timestamp 82 ] state=[-0.25521088 -0.18465828  0.11609541  0.18152083], action=1, reward=1.0, next_state=[-0.25890404  0.00862757  0.11972583 -0.07240086]\n",
      "[ episode 194 ][ timestamp 83 ] state=[-0.25890404  0.00862757  0.11972583 -0.07240086], action=0, reward=1.0, next_state=[-0.25873149 -0.18798921  0.11827781  0.25552843]\n",
      "[ episode 194 ][ timestamp 84 ] state=[-0.25873149 -0.18798921  0.11827781  0.25552843], action=1, reward=1.0, next_state=[-0.26249128  0.00526286  0.12338838  0.00236827]\n",
      "[ episode 194 ][ timestamp 85 ] state=[-0.26249128  0.00526286  0.12338838  0.00236827], action=1, reward=1.0, next_state=[-0.26238602  0.19841906  0.12343575 -0.24897907]\n",
      "[ episode 194 ][ timestamp 86 ] state=[-0.26238602  0.19841906  0.12343575 -0.24897907], action=1, reward=1.0, next_state=[-0.25841764  0.39158188  0.11845616 -0.50032074]\n",
      "[ episode 194 ][ timestamp 87 ] state=[-0.25841764  0.39158188  0.11845616 -0.50032074], action=0, reward=1.0, next_state=[-0.250586    0.19500668  0.10844975 -0.17277955]\n",
      "[ episode 194 ][ timestamp 88 ] state=[-0.250586    0.19500668  0.10844975 -0.17277955], action=0, reward=1.0, next_state=[-0.24668587 -0.00148684  0.10499416  0.15205092]\n",
      "[ episode 194 ][ timestamp 89 ] state=[-0.24668587 -0.00148684  0.10499416  0.15205092], action=0, reward=1.0, next_state=[-0.2467156  -0.19794327  0.10803518  0.47592439]\n",
      "[ episode 194 ][ timestamp 90 ] state=[-0.2467156  -0.19794327  0.10803518  0.47592439], action=1, reward=1.0, next_state=[-0.25067447 -0.00449941  0.11755366  0.2191509 ]\n",
      "[ episode 194 ][ timestamp 91 ] state=[-0.25067447 -0.00449941  0.11755366  0.2191509 ], action=1, reward=1.0, next_state=[-0.25076446  0.18876312  0.12193668 -0.03426097]\n",
      "[ episode 194 ][ timestamp 92 ] state=[-0.25076446  0.18876312  0.12193668 -0.03426097], action=0, reward=1.0, next_state=[-0.24698919 -0.00787729  0.12125146  0.29427015]\n",
      "[ episode 194 ][ timestamp 93 ] state=[-0.24698919 -0.00787729  0.12125146  0.29427015], action=1, reward=1.0, next_state=[-0.24714674  0.18532613  0.12713687  0.0421534 ]\n",
      "[ episode 194 ][ timestamp 94 ] state=[-0.24714674  0.18532613  0.12713687  0.0421534 ], action=0, reward=1.0, next_state=[-0.24344022 -0.01136796  0.12797993  0.37209089]\n",
      "[ episode 194 ][ timestamp 95 ] state=[-0.24344022 -0.01136796  0.12797993  0.37209089], action=1, reward=1.0, next_state=[-0.24366758  0.18172563  0.13542175  0.12234272]\n",
      "[ episode 194 ][ timestamp 96 ] state=[-0.24366758  0.18172563  0.13542175  0.12234272], action=0, reward=1.0, next_state=[-0.24003306 -0.0150504   0.13786861  0.4544968 ]\n",
      "[ episode 194 ][ timestamp 97 ] state=[-0.24003306 -0.0150504   0.13786861  0.4544968 ], action=0, reward=1.0, next_state=[-0.24033407 -0.21182498  0.14695854  0.78726302]\n",
      "[ episode 194 ][ timestamp 98 ] state=[-0.24033407 -0.21182498  0.14695854  0.78726302], action=1, reward=1.0, next_state=[-0.24457057 -0.01899441  0.1627038   0.5441854 ]\n",
      "[ episode 194 ][ timestamp 99 ] state=[-0.24457057 -0.01899441  0.1627038   0.5441854 ], action=1, reward=1.0, next_state=[-0.24495046  0.17351229  0.17358751  0.30686318]\n",
      "[ episode 194 ][ timestamp 100 ] state=[-0.24495046  0.17351229  0.17358751  0.30686318], action=0, reward=1.0, next_state=[-0.24148021 -0.02360333  0.17972477  0.64887189]\n",
      "[ episode 194 ][ timestamp 101 ] state=[-0.24148021 -0.02360333  0.17972477  0.64887189], action=1, reward=1.0, next_state=[-0.24195228  0.16862023  0.19270221  0.41773586]\n",
      "[ episode 194 ][ timestamp 102 ] state=[-0.24195228  0.16862023  0.19270221  0.41773586], action=1, reward=1.0, next_state=[-0.23857988  0.36056395  0.20105693  0.19145397]\n",
      "[ episode 194 ][ timestamp 103 ] state=[-0.23857988  0.36056395  0.20105693  0.19145397], action=1, reward=1.0, next_state=[-0.2313686   0.55232707  0.20488601 -0.03168311]\n",
      "[ episode 194 ][ timestamp 104 ] state=[-0.2313686   0.55232707  0.20488601 -0.03168311], action=0, reward=1.0, next_state=[-0.22032206  0.35494634  0.20425235  0.31801136]\n",
      "[ episode 194 ][ timestamp 105 ] state=[-0.22032206  0.35494634  0.20425235  0.31801136], action=0, reward=-1.0, next_state=[-0.21322313  0.15759003  0.21061257  0.66752564]\n",
      "[ Ended! ] Episode 194: Exploration_rate=0.3800621177172763. Score=105.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 195 ] state=[-0.02503787  0.00643576  0.03536584  0.00891365]\n",
      "[ episode 195 ][ timestamp 1 ] state=[-0.02503787  0.00643576  0.03536584  0.00891365], action=1, reward=1.0, next_state=[-0.02490915  0.20103314  0.03554411 -0.2724045 ]\n",
      "[ episode 195 ][ timestamp 2 ] state=[-0.02490915  0.20103314  0.03554411 -0.2724045 ], action=0, reward=1.0, next_state=[-0.02088849  0.00542251  0.03009602  0.03127388]\n",
      "[ episode 195 ][ timestamp 3 ] state=[-0.02088849  0.00542251  0.03009602  0.03127388], action=1, reward=1.0, next_state=[-0.02078004  0.20010023  0.0307215  -0.25176357]\n",
      "[ episode 195 ][ timestamp 4 ] state=[-0.02078004  0.20010023  0.0307215  -0.25176357], action=0, reward=1.0, next_state=[-0.01677803  0.00455337  0.02568623  0.05044903]\n",
      "[ episode 195 ][ timestamp 5 ] state=[-0.01677803  0.00455337  0.02568623  0.05044903], action=0, reward=1.0, next_state=[-0.01668697 -0.1909273   0.02669521  0.35112422]\n",
      "[ episode 195 ][ timestamp 6 ] state=[-0.01668697 -0.1909273   0.02669521  0.35112422], action=1, reward=1.0, next_state=[-0.02050551  0.00380504  0.0337177   0.06697724]\n",
      "[ episode 195 ][ timestamp 7 ] state=[-0.02050551  0.00380504  0.0337177   0.06697724], action=0, reward=1.0, next_state=[-0.02042941 -0.19178369  0.03505724  0.37010471]\n",
      "[ episode 195 ][ timestamp 8 ] state=[-0.02042941 -0.19178369  0.03505724  0.37010471], action=1, reward=1.0, next_state=[-0.02426509  0.00282309  0.04245933  0.08867862]\n",
      "[ episode 195 ][ timestamp 9 ] state=[-0.02426509  0.00282309  0.04245933  0.08867862], action=1, reward=1.0, next_state=[-0.02420862  0.19731153  0.04423291 -0.19031182]\n",
      "[ episode 195 ][ timestamp 10 ] state=[-0.02420862  0.19731153  0.04423291 -0.19031182], action=0, reward=1.0, next_state=[-0.02026239  0.0015856   0.04042667  0.11599014]\n",
      "[ episode 195 ][ timestamp 11 ] state=[-0.02026239  0.0015856   0.04042667  0.11599014], action=1, reward=1.0, next_state=[-0.02023068  0.19610569  0.04274647 -0.16366939]\n",
      "[ episode 195 ][ timestamp 12 ] state=[-0.02023068  0.19610569  0.04274647 -0.16366939], action=0, reward=1.0, next_state=[-0.01630857  0.00039871  0.03947309  0.14218656]\n",
      "[ episode 195 ][ timestamp 13 ] state=[-0.01630857  0.00039871  0.03947309  0.14218656], action=1, reward=1.0, next_state=[-0.01630059  0.19493375  0.04231682 -0.13778664]\n",
      "[ episode 195 ][ timestamp 14 ] state=[-0.01630059  0.19493375  0.04231682 -0.13778664], action=1, reward=1.0, next_state=[-0.01240192  0.38942486  0.03956108 -0.4168247 ]\n",
      "[ episode 195 ][ timestamp 15 ] state=[-0.01240192  0.38942486  0.03956108 -0.4168247 ], action=0, reward=1.0, next_state=[-0.00461342  0.19376524  0.03122459 -0.11193697]\n",
      "[ episode 195 ][ timestamp 16 ] state=[-0.00461342  0.19376524  0.03122459 -0.11193697], action=1, reward=1.0, next_state=[-0.00073812  0.38842617  0.02898585 -0.39460751]\n",
      "[ episode 195 ][ timestamp 17 ] state=[-0.00073812  0.38842617  0.02898585 -0.39460751], action=0, reward=1.0, next_state=[ 0.00703041  0.19290517  0.0210937  -0.09292856]\n",
      "[ episode 195 ][ timestamp 18 ] state=[ 0.00703041  0.19290517  0.0210937  -0.09292856], action=1, reward=1.0, next_state=[ 0.01088851  0.38771853  0.01923513 -0.37888251]\n",
      "[ episode 195 ][ timestamp 19 ] state=[ 0.01088851  0.38771853  0.01923513 -0.37888251], action=0, reward=1.0, next_state=[ 0.01864288  0.19232877  0.01165748 -0.0801973 ]\n",
      "[ episode 195 ][ timestamp 20 ] state=[ 0.01864288  0.19232877  0.01165748 -0.0801973 ], action=1, reward=1.0, next_state=[ 0.02248946  0.38728169  0.01005353 -0.36917959]\n",
      "[ episode 195 ][ timestamp 21 ] state=[ 0.02248946  0.38728169  0.01005353 -0.36917959], action=0, reward=1.0, next_state=[ 0.03023509  0.19201835  0.00266994 -0.07334368]\n",
      "[ episode 195 ][ timestamp 22 ] state=[ 0.03023509  0.19201835  0.00266994 -0.07334368], action=1, reward=1.0, next_state=[ 0.03407546  0.38710192  0.00120307 -0.36518304]\n",
      "[ episode 195 ][ timestamp 23 ] state=[ 0.03407546  0.38710192  0.00120307 -0.36518304], action=0, reward=1.0, next_state=[ 0.0418175   0.19196289 -0.00610059 -0.07212101]\n",
      "[ episode 195 ][ timestamp 24 ] state=[ 0.0418175   0.19196289 -0.00610059 -0.07212101], action=0, reward=1.0, next_state=[ 0.04565675 -0.00307107 -0.00754301  0.21863093]\n",
      "[ episode 195 ][ timestamp 25 ] state=[ 0.04565675 -0.00307107 -0.00754301  0.21863093], action=1, reward=1.0, next_state=[ 0.04559533  0.19215789 -0.0031704  -0.07642181]\n",
      "[ episode 195 ][ timestamp 26 ] state=[ 0.04559533  0.19215789 -0.0031704  -0.07642181], action=0, reward=1.0, next_state=[ 0.04943849 -0.00291847 -0.00469883  0.21525916]\n",
      "[ episode 195 ][ timestamp 27 ] state=[ 0.04943849 -0.00291847 -0.00469883  0.21525916], action=1, reward=1.0, next_state=[ 0.04938012  0.19227034 -0.00039365 -0.07890227]\n",
      "[ episode 195 ][ timestamp 28 ] state=[ 0.04938012  0.19227034 -0.00039365 -0.07890227], action=1, reward=1.0, next_state=[ 0.05322553  0.38739793 -0.00197169 -0.37170937]\n",
      "[ episode 195 ][ timestamp 29 ] state=[ 0.05322553  0.38739793 -0.00197169 -0.37170937], action=0, reward=1.0, next_state=[ 0.06097349  0.19230405 -0.00940588 -0.07964879]\n",
      "[ episode 195 ][ timestamp 30 ] state=[ 0.06097349  0.19230405 -0.00940588 -0.07964879], action=1, reward=1.0, next_state=[ 0.06481957  0.38755957 -0.01099886 -0.37528439]\n",
      "[ episode 195 ][ timestamp 31 ] state=[ 0.06481957  0.38755957 -0.01099886 -0.37528439], action=0, reward=1.0, next_state=[ 0.07257076  0.19259556 -0.01850454 -0.08608967]\n",
      "[ episode 195 ][ timestamp 32 ] state=[ 0.07257076  0.19259556 -0.01850454 -0.08608967], action=1, reward=1.0, next_state=[ 0.07642267  0.3879778  -0.02022634 -0.38455288]\n",
      "[ episode 195 ][ timestamp 33 ] state=[ 0.07642267  0.3879778  -0.02022634 -0.38455288], action=1, reward=1.0, next_state=[ 0.08418223  0.58338098 -0.0279174  -0.68354384]\n",
      "[ episode 195 ][ timestamp 34 ] state=[ 0.08418223  0.58338098 -0.0279174  -0.68354384], action=0, reward=1.0, next_state=[ 0.09584985  0.38865758 -0.04158827 -0.39977919]\n",
      "[ episode 195 ][ timestamp 35 ] state=[ 0.09584985  0.38865758 -0.04158827 -0.39977919], action=1, reward=1.0, next_state=[ 0.103623    0.58434404 -0.04958386 -0.70527852]\n",
      "[ episode 195 ][ timestamp 36 ] state=[ 0.103623    0.58434404 -0.04958386 -0.70527852], action=0, reward=1.0, next_state=[ 0.11530988  0.38994292 -0.06368943 -0.4286069 ]\n",
      "[ episode 195 ][ timestamp 37 ] state=[ 0.11530988  0.38994292 -0.06368943 -0.4286069 ], action=0, reward=1.0, next_state=[ 0.12310874  0.19577805 -0.07226156 -0.15666213]\n",
      "[ episode 195 ][ timestamp 38 ] state=[ 0.12310874  0.19577805 -0.07226156 -0.15666213], action=1, reward=1.0, next_state=[ 0.1270243   0.39185622 -0.07539481 -0.47123823]\n",
      "[ episode 195 ][ timestamp 39 ] state=[ 0.1270243   0.39185622 -0.07539481 -0.47123823], action=0, reward=1.0, next_state=[ 0.13486142  0.19787565 -0.08481957 -0.20323907]\n",
      "[ episode 195 ][ timestamp 40 ] state=[ 0.13486142  0.19787565 -0.08481957 -0.20323907], action=1, reward=1.0, next_state=[ 0.13881893  0.39410175 -0.08888435 -0.52142712]\n",
      "[ episode 195 ][ timestamp 41 ] state=[ 0.13881893  0.39410175 -0.08888435 -0.52142712], action=0, reward=1.0, next_state=[ 0.14670097  0.200336   -0.0993129  -0.25802347]\n",
      "[ episode 195 ][ timestamp 42 ] state=[ 0.14670097  0.200336   -0.0993129  -0.25802347], action=1, reward=1.0, next_state=[ 0.15070769  0.39672519 -0.10447336 -0.58030572]\n",
      "[ episode 195 ][ timestamp 43 ] state=[ 0.15070769  0.39672519 -0.10447336 -0.58030572], action=0, reward=1.0, next_state=[ 0.15864219  0.20321039 -0.11607948 -0.32227551]\n",
      "[ episode 195 ][ timestamp 44 ] state=[ 0.15864219  0.20321039 -0.11607948 -0.32227551], action=0, reward=1.0, next_state=[ 0.1627064   0.00991626 -0.12252499 -0.0683363 ]\n",
      "[ episode 195 ][ timestamp 45 ] state=[ 0.1627064   0.00991626 -0.12252499 -0.0683363 ], action=1, reward=1.0, next_state=[ 0.16290473  0.20656238 -0.12389172 -0.39702644]\n",
      "[ episode 195 ][ timestamp 46 ] state=[ 0.16290473  0.20656238 -0.12389172 -0.39702644], action=1, reward=1.0, next_state=[ 0.16703597  0.40320426 -0.13183224 -0.7260595 ]\n",
      "[ episode 195 ][ timestamp 47 ] state=[ 0.16703597  0.40320426 -0.13183224 -0.7260595 ], action=0, reward=1.0, next_state=[ 0.17510006  0.21012766 -0.14635343 -0.47760417]\n",
      "[ episode 195 ][ timestamp 48 ] state=[ 0.17510006  0.21012766 -0.14635343 -0.47760417], action=1, reward=1.0, next_state=[ 0.17930261  0.40697991 -0.15590552 -0.81260033]\n",
      "[ episode 195 ][ timestamp 49 ] state=[ 0.17930261  0.40697991 -0.15590552 -0.81260033], action=0, reward=1.0, next_state=[ 0.18744221  0.21429771 -0.17215752 -0.57273326]\n",
      "[ episode 195 ][ timestamp 50 ] state=[ 0.18744221  0.21429771 -0.17215752 -0.57273326], action=1, reward=1.0, next_state=[ 0.19172816  0.41136217 -0.18361219 -0.91432495]\n",
      "[ episode 195 ][ timestamp 51 ] state=[ 0.19172816  0.41136217 -0.18361219 -0.91432495], action=0, reward=1.0, next_state=[ 0.19995541  0.21913468 -0.20189869 -0.68450973]\n",
      "[ episode 195 ][ timestamp 52 ] state=[ 0.19995541  0.21913468 -0.20189869 -0.68450973], action=1, reward=-1.0, next_state=[ 0.2043381   0.41640201 -0.21558888 -1.03335602]\n",
      "[ Ended! ] Episode 195: Exploration_rate=0.37816180712868996. Score=52.\n",
      "[ Experience replay ] starts\n",
      "[ episode 196 ] state=[-0.04779114  0.01644928  0.04742369 -0.04810195]\n",
      "[ episode 196 ][ timestamp 1 ] state=[-0.04779114  0.01644928  0.04742369 -0.04810195], action=0, reward=1.0, next_state=[-0.04746216 -0.17931947  0.04646165  0.25915838]\n",
      "[ episode 196 ][ timestamp 2 ] state=[-0.04746216 -0.17931947  0.04646165  0.25915838], action=1, reward=1.0, next_state=[-0.05104854  0.01510946  0.05164482 -0.01851549]\n",
      "[ episode 196 ][ timestamp 3 ] state=[-0.05104854  0.01510946  0.05164482 -0.01851549], action=1, reward=1.0, next_state=[-0.05074636  0.20945419  0.05127451 -0.29446707]\n",
      "[ episode 196 ][ timestamp 4 ] state=[-0.05074636  0.20945419  0.05127451 -0.29446707], action=0, reward=1.0, next_state=[-0.04655727  0.01364015  0.04538517  0.01393607]\n",
      "[ episode 196 ][ timestamp 5 ] state=[-0.04655727  0.01364015  0.04538517  0.01393607], action=0, reward=1.0, next_state=[-0.04628447 -0.18210231  0.04566389  0.32058607]\n",
      "[ episode 196 ][ timestamp 6 ] state=[-0.04628447 -0.18210231  0.04566389  0.32058607], action=1, reward=1.0, next_state=[-0.04992651  0.01234059  0.05207561  0.04264627]\n",
      "[ episode 196 ][ timestamp 7 ] state=[-0.04992651  0.01234059  0.05207561  0.04264627], action=1, reward=1.0, next_state=[-0.0496797   0.20667862  0.05292854 -0.23316228]\n",
      "[ episode 196 ][ timestamp 8 ] state=[-0.0496797   0.20667862  0.05292854 -0.23316228], action=1, reward=1.0, next_state=[-0.04554613  0.40100592  0.04826529 -0.50869131]\n",
      "[ episode 196 ][ timestamp 9 ] state=[-0.04554613  0.40100592  0.04826529 -0.50869131], action=0, reward=1.0, next_state=[-0.03752601  0.20523838  0.03809147 -0.20119747]\n",
      "[ episode 196 ][ timestamp 10 ] state=[-0.03752601  0.20523838  0.03809147 -0.20119747], action=0, reward=1.0, next_state=[-0.03342124  0.00959293  0.03406752  0.103254  ]\n",
      "[ episode 196 ][ timestamp 11 ] state=[-0.03342124  0.00959293  0.03406752  0.103254  ], action=1, reward=1.0, next_state=[-0.03322939  0.20421052  0.0361326  -0.17848908]\n",
      "[ episode 196 ][ timestamp 12 ] state=[-0.03322939  0.20421052  0.0361326  -0.17848908], action=1, reward=1.0, next_state=[-0.02914518  0.39879727  0.03256281 -0.45955802]\n",
      "[ episode 196 ][ timestamp 13 ] state=[-0.02914518  0.39879727  0.03256281 -0.45955802], action=0, reward=1.0, next_state=[-0.02116923  0.20323052  0.02337165 -0.15679164]\n",
      "[ episode 196 ][ timestamp 14 ] state=[-0.02116923  0.20323052  0.02337165 -0.15679164], action=1, reward=1.0, next_state=[-0.01710462  0.39801019  0.02023582 -0.4420107 ]\n",
      "[ episode 196 ][ timestamp 15 ] state=[-0.01710462  0.39801019  0.02023582 -0.4420107 ], action=0, reward=1.0, next_state=[-0.00914442  0.20260782  0.01139561 -0.14301823]\n",
      "[ episode 196 ][ timestamp 16 ] state=[-0.00914442  0.20260782  0.01139561 -0.14301823], action=0, reward=1.0, next_state=[-0.00509226  0.00732453  0.00853524  0.15323791]\n",
      "[ episode 196 ][ timestamp 17 ] state=[-0.00509226  0.00732453  0.00853524  0.15323791], action=0, reward=1.0, next_state=[-0.00494577 -0.18791858  0.0116      0.44860125]\n",
      "[ episode 196 ][ timestamp 18 ] state=[-0.00494577 -0.18791858  0.0116      0.44860125], action=1, reward=1.0, next_state=[-0.00870414  0.00703738  0.02057203  0.15959731]\n",
      "[ episode 196 ][ timestamp 19 ] state=[-0.00870414  0.00703738  0.02057203  0.15959731], action=0, reward=1.0, next_state=[-0.00856339 -0.18837296  0.02376397  0.45869854]\n",
      "[ episode 196 ][ timestamp 20 ] state=[-0.00856339 -0.18837296  0.02376397  0.45869854], action=1, reward=1.0, next_state=[-0.01233085  0.00640514  0.03293794  0.17359984]\n",
      "[ episode 196 ][ timestamp 21 ] state=[-0.01233085  0.00640514  0.03293794  0.17359984], action=1, reward=1.0, next_state=[-0.01220275  0.20104058  0.03640994 -0.10851296]\n",
      "[ episode 196 ][ timestamp 22 ] state=[-0.01220275  0.20104058  0.03640994 -0.10851296], action=0, reward=1.0, next_state=[-0.00818194  0.0054163   0.03423968  0.19543113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 196 ][ timestamp 23 ] state=[-0.00818194  0.0054163   0.03423968  0.19543113], action=1, reward=1.0, next_state=[-0.00807361  0.20003218  0.0381483  -0.08625708]\n",
      "[ episode 196 ][ timestamp 24 ] state=[-0.00807361  0.20003218  0.0381483  -0.08625708], action=0, reward=1.0, next_state=[-0.00407297  0.00438475  0.03642316  0.21821342]\n",
      "[ episode 196 ][ timestamp 25 ] state=[-0.00407297  0.00438475  0.03642316  0.21821342], action=0, reward=1.0, next_state=[-0.00398527 -0.19123843  0.04078743  0.52215961]\n",
      "[ episode 196 ][ timestamp 26 ] state=[-0.00398527 -0.19123843  0.04078743  0.52215961], action=1, reward=1.0, next_state=[-0.00781004  0.0032864   0.05123062  0.24260322]\n",
      "[ episode 196 ][ timestamp 27 ] state=[-0.00781004  0.0032864   0.05123062  0.24260322], action=0, reward=1.0, next_state=[-0.00774431 -0.19252847  0.05608269  0.55099539]\n",
      "[ episode 196 ][ timestamp 28 ] state=[-0.00774431 -0.19252847  0.05608269  0.55099539], action=0, reward=1.0, next_state=[-0.01159488 -0.38839141  0.06710259  0.86080756]\n",
      "[ episode 196 ][ timestamp 29 ] state=[-0.01159488 -0.38839141  0.06710259  0.86080756], action=1, reward=1.0, next_state=[-0.01936271 -0.19424432  0.08431875  0.58995568]\n",
      "[ episode 196 ][ timestamp 30 ] state=[-0.01936271 -0.19424432  0.08431875  0.58995568], action=1, reward=1.0, next_state=[-0.0232476  -0.00039791  0.09611786  0.32497943]\n",
      "[ episode 196 ][ timestamp 31 ] state=[-0.0232476  -0.00039791  0.09611786  0.32497943], action=1, reward=1.0, next_state=[-0.02325556  0.19323344  0.10261745  0.0640882 ]\n",
      "[ episode 196 ][ timestamp 32 ] state=[-0.02325556  0.19323344  0.10261745  0.0640882 ], action=0, reward=1.0, next_state=[-0.01939089 -0.00319865  0.10389921  0.38730293]\n",
      "[ episode 196 ][ timestamp 33 ] state=[-0.01939089 -0.00319865  0.10389921  0.38730293], action=1, reward=1.0, next_state=[-0.01945486  0.19030682  0.11164527  0.12910144]\n",
      "[ episode 196 ][ timestamp 34 ] state=[-0.01945486  0.19030682  0.11164527  0.12910144], action=0, reward=1.0, next_state=[-0.01564872 -0.00622271  0.1142273   0.45481594]\n",
      "[ episode 196 ][ timestamp 35 ] state=[-0.01564872 -0.00622271  0.1142273   0.45481594], action=1, reward=1.0, next_state=[-0.01577318  0.18711449  0.12332362  0.20020991]\n",
      "[ episode 196 ][ timestamp 36 ] state=[-0.01577318  0.18711449  0.12332362  0.20020991], action=0, reward=1.0, next_state=[-0.01203089 -0.00953568  0.12732782  0.52911022]\n",
      "[ episode 196 ][ timestamp 37 ] state=[-0.01203089 -0.00953568  0.12732782  0.52911022], action=0, reward=1.0, next_state=[-0.0122216  -0.20619731  0.13791002  0.85904794]\n",
      "[ episode 196 ][ timestamp 38 ] state=[-0.0122216  -0.20619731  0.13791002  0.85904794], action=1, reward=1.0, next_state=[-0.01634555 -0.01319612  0.15509098  0.61271198]\n",
      "[ episode 196 ][ timestamp 39 ] state=[-0.01634555 -0.01319612  0.15509098  0.61271198], action=1, reward=1.0, next_state=[-0.01660947  0.17945731  0.16734522  0.3726145 ]\n",
      "[ episode 196 ][ timestamp 40 ] state=[-0.01660947  0.17945731  0.16734522  0.3726145 ], action=1, reward=1.0, next_state=[-0.01302032  0.37185583  0.17479751  0.13701849]\n",
      "[ episode 196 ][ timestamp 41 ] state=[-0.01302032  0.37185583  0.17479751  0.13701849], action=1, reward=1.0, next_state=[-0.00558321  0.56409943  0.17753788 -0.09582357]\n",
      "[ episode 196 ][ timestamp 42 ] state=[-0.00558321  0.56409943  0.17753788 -0.09582357], action=0, reward=1.0, next_state=[0.00569878 0.36693604 0.17562141 0.24719522]\n",
      "[ episode 196 ][ timestamp 43 ] state=[0.00569878 0.36693604 0.17562141 0.24719522], action=1, reward=1.0, next_state=[0.0130375  0.55917181 0.18056531 0.01464465]\n",
      "[ episode 196 ][ timestamp 44 ] state=[0.0130375  0.55917181 0.18056531 0.01464465], action=0, reward=1.0, next_state=[0.02422094 0.36198158 0.1808582  0.35841942]\n",
      "[ episode 196 ][ timestamp 45 ] state=[0.02422094 0.36198158 0.1808582  0.35841942], action=1, reward=1.0, next_state=[0.03146057 0.5541336  0.18802659 0.1277754 ]\n",
      "[ episode 196 ][ timestamp 46 ] state=[0.03146057 0.5541336  0.18802659 0.1277754 ], action=1, reward=1.0, next_state=[ 0.04254324  0.7461342   0.1905821  -0.10019483]\n",
      "[ episode 196 ][ timestamp 47 ] state=[ 0.04254324  0.7461342   0.1905821  -0.10019483], action=0, reward=1.0, next_state=[0.05746593 0.54886455 0.1885782  0.24604461]\n",
      "[ episode 196 ][ timestamp 48 ] state=[0.05746593 0.54886455 0.1885782  0.24604461], action=1, reward=1.0, next_state=[0.06844322 0.74086304 0.1934991  0.01826653]\n",
      "[ episode 196 ][ timestamp 49 ] state=[0.06844322 0.74086304 0.1934991  0.01826653], action=1, reward=1.0, next_state=[ 0.08326048  0.93275955  0.19386443 -0.2076719 ]\n",
      "[ episode 196 ][ timestamp 50 ] state=[ 0.08326048  0.93275955  0.19386443 -0.2076719 ], action=1, reward=1.0, next_state=[ 0.10191567  1.1246573   0.18971099 -0.43348653]\n",
      "[ episode 196 ][ timestamp 51 ] state=[ 0.10191567  1.1246573   0.18971099 -0.43348653], action=0, reward=1.0, next_state=[ 0.12440881  0.92742719  0.18104126 -0.08750812]\n",
      "[ episode 196 ][ timestamp 52 ] state=[ 0.12440881  0.92742719  0.18104126 -0.08750812], action=0, reward=1.0, next_state=[0.14295736 0.73023432 0.1792911  0.25638288]\n",
      "[ episode 196 ][ timestamp 53 ] state=[0.14295736 0.73023432 0.1792911  0.25638288], action=1, reward=1.0, next_state=[0.15756204 0.92240399 0.18441875 0.02517863]\n",
      "[ episode 196 ][ timestamp 54 ] state=[0.15756204 0.92240399 0.18441875 0.02517863], action=0, reward=1.0, next_state=[0.17601012 0.72518213 0.18492233 0.36990727]\n",
      "[ episode 196 ][ timestamp 55 ] state=[0.17601012 0.72518213 0.18492233 0.36990727], action=1, reward=1.0, next_state=[0.19051377 0.91726146 0.19232047 0.14075839]\n",
      "[ episode 196 ][ timestamp 56 ] state=[0.19051377 0.91726146 0.19232047 0.14075839], action=1, reward=1.0, next_state=[ 0.208859    1.10918342  0.19513564 -0.08562264]\n",
      "[ episode 196 ][ timestamp 57 ] state=[ 0.208859    1.10918342  0.19513564 -0.08562264], action=1, reward=1.0, next_state=[ 0.23104266  1.3010509   0.19342319 -0.31095531]\n",
      "[ episode 196 ][ timestamp 58 ] state=[ 0.23104266  1.3010509   0.19342319 -0.31095531], action=0, reward=1.0, next_state=[0.25706368 1.10377512 0.18720408 0.03595264]\n",
      "[ episode 196 ][ timestamp 59 ] state=[0.25706368 1.10377512 0.18720408 0.03595264], action=1, reward=1.0, next_state=[ 0.27913918  1.29578816  0.18792313 -0.19231767]\n",
      "[ episode 196 ][ timestamp 60 ] state=[ 0.27913918  1.29578816  0.18792313 -0.19231767], action=1, reward=1.0, next_state=[ 0.30505495  1.48779443  0.18407678 -0.4203317 ]\n",
      "[ episode 196 ][ timestamp 61 ] state=[ 0.30505495  1.48779443  0.18407678 -0.4203317 ], action=0, reward=1.0, next_state=[ 0.33481084  1.29060673  0.17567015 -0.07573372]\n",
      "[ episode 196 ][ timestamp 62 ] state=[ 0.33481084  1.29060673  0.17567015 -0.07573372], action=1, reward=1.0, next_state=[ 0.36062297  1.4828322   0.17415547 -0.30825249]\n",
      "[ episode 196 ][ timestamp 63 ] state=[ 0.36062297  1.4828322   0.17415547 -0.30825249], action=0, reward=1.0, next_state=[0.39027962 1.28571192 0.16799042 0.0338985 ]\n",
      "[ episode 196 ][ timestamp 64 ] state=[0.39027962 1.28571192 0.16799042 0.0338985 ], action=0, reward=1.0, next_state=[0.41599385 1.08862915 0.16866839 0.3745183 ]\n",
      "[ episode 196 ][ timestamp 65 ] state=[0.41599385 1.08862915 0.16866839 0.3745183 ], action=1, reward=1.0, next_state=[0.43776644 1.28100407 0.17615876 0.13940458]\n",
      "[ episode 196 ][ timestamp 66 ] state=[0.43776644 1.28100407 0.17615876 0.13940458], action=1, reward=1.0, next_state=[ 0.46338652  1.47322295  0.17894685 -0.09293838]\n",
      "[ episode 196 ][ timestamp 67 ] state=[ 0.46338652  1.47322295  0.17894685 -0.09293838], action=0, reward=1.0, next_state=[0.49285098 1.27604766 0.17708808 0.25043176]\n",
      "[ episode 196 ][ timestamp 68 ] state=[0.49285098 1.27604766 0.17708808 0.25043176], action=1, reward=1.0, next_state=[0.51837193 1.46825699 0.18209672 0.01841895]\n",
      "[ episode 196 ][ timestamp 69 ] state=[0.51837193 1.46825699 0.18209672 0.01841895], action=0, reward=1.0, next_state=[0.54773707 1.27105412 0.1824651  0.36257353]\n",
      "[ episode 196 ][ timestamp 70 ] state=[0.54773707 1.27105412 0.1824651  0.36257353], action=1, reward=1.0, next_state=[0.57315815 1.46317736 0.18971657 0.13252028]\n",
      "[ episode 196 ][ timestamp 71 ] state=[0.57315815 1.46317736 0.18971657 0.13252028], action=1, reward=1.0, next_state=[ 0.6024217   1.65514702  0.19236697 -0.09482496]\n",
      "[ episode 196 ][ timestamp 72 ] state=[ 0.6024217   1.65514702  0.19236697 -0.09482496], action=0, reward=1.0, next_state=[0.63552464 1.4578631  0.19047047 0.25185012]\n",
      "[ episode 196 ][ timestamp 73 ] state=[0.63552464 1.4578631  0.19047047 0.25185012], action=1, reward=1.0, next_state=[0.6646819  1.64982728 0.19550747 0.02477162]\n",
      "[ episode 196 ][ timestamp 74 ] state=[0.6646819  1.64982728 0.19550747 0.02477162], action=0, reward=1.0, next_state=[0.69767845 1.45251734 0.19600291 0.3722119 ]\n",
      "[ episode 196 ][ timestamp 75 ] state=[0.69767845 1.45251734 0.19600291 0.3722119 ], action=1, reward=1.0, next_state=[0.72672879 1.64439359 0.20344714 0.14716494]\n",
      "[ episode 196 ][ timestamp 76 ] state=[0.72672879 1.64439359 0.20344714 0.14716494], action=1, reward=1.0, next_state=[ 0.75961667  1.83610926  0.20639044 -0.07507594]\n",
      "[ episode 196 ][ timestamp 77 ] state=[ 0.75961667  1.83610926  0.20639044 -0.07507594], action=0, reward=1.0, next_state=[0.79633885 1.6387184  0.20488893 0.27497544]\n",
      "[ episode 196 ][ timestamp 78 ] state=[0.79633885 1.6387184  0.20488893 0.27497544], action=1, reward=-1.0, next_state=[0.82911322 1.83041785 0.21038843 0.05325753]\n",
      "[ Ended! ] Episode 196: Exploration_rate=0.37627099809304654. Score=78.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 197 ] state=[ 0.03145619  0.02278509 -0.01538882  0.00777884]\n",
      "[ episode 197 ][ timestamp 1 ] state=[ 0.03145619  0.02278509 -0.01538882  0.00777884], action=1, reward=1.0, next_state=[ 0.03191189  0.21812432 -0.01523324 -0.28971944]\n",
      "[ episode 197 ][ timestamp 2 ] state=[ 0.03191189  0.21812432 -0.01523324 -0.28971944], action=0, reward=1.0, next_state=[ 0.03627438  0.02322286 -0.02102763 -0.00187958]\n",
      "[ episode 197 ][ timestamp 3 ] state=[ 0.03627438  0.02322286 -0.02102763 -0.00187958], action=0, reward=1.0, next_state=[ 0.03673884 -0.17159131 -0.02106522  0.28409541]\n",
      "[ episode 197 ][ timestamp 4 ] state=[ 0.03673884 -0.17159131 -0.02106522  0.28409541], action=1, reward=1.0, next_state=[ 0.03330701  0.02382465 -0.01538331 -0.01515622]\n",
      "[ episode 197 ][ timestamp 5 ] state=[ 0.03330701  0.02382465 -0.01538331 -0.01515622], action=0, reward=1.0, next_state=[ 0.03378351 -0.17107334 -0.01568644  0.27263367]\n",
      "[ episode 197 ][ timestamp 6 ] state=[ 0.03378351 -0.17107334 -0.01568644  0.27263367], action=1, reward=1.0, next_state=[ 0.03036204  0.02426889 -0.01023376 -0.02495525]\n",
      "[ episode 197 ][ timestamp 7 ] state=[ 0.03036204  0.02426889 -0.01023376 -0.02495525], action=1, reward=1.0, next_state=[ 0.03084742  0.21953609 -0.01073287 -0.32084939]\n",
      "[ episode 197 ][ timestamp 8 ] state=[ 0.03084742  0.21953609 -0.01073287 -0.32084939], action=0, reward=1.0, next_state=[ 0.03523814  0.02456862 -0.01714985 -0.03157043]\n",
      "[ episode 197 ][ timestamp 9 ] state=[ 0.03523814  0.02456862 -0.01714985 -0.03157043], action=1, reward=1.0, next_state=[ 0.03572951  0.21993225 -0.01778126 -0.3296146 ]\n",
      "[ episode 197 ][ timestamp 10 ] state=[ 0.03572951  0.21993225 -0.01778126 -0.3296146 ], action=0, reward=1.0, next_state=[ 0.04012816  0.02506788 -0.02437356 -0.04259166]\n",
      "[ episode 197 ][ timestamp 11 ] state=[ 0.04012816  0.02506788 -0.02437356 -0.04259166], action=1, reward=1.0, next_state=[ 0.04062951  0.22053071 -0.02522539 -0.34286393]\n",
      "[ episode 197 ][ timestamp 12 ] state=[ 0.04062951  0.22053071 -0.02522539 -0.34286393], action=0, reward=1.0, next_state=[ 0.04504013  0.02577655 -0.03208267 -0.05824111]\n",
      "[ episode 197 ][ timestamp 13 ] state=[ 0.04504013  0.02577655 -0.03208267 -0.05824111], action=1, reward=1.0, next_state=[ 0.04555566  0.22134346 -0.03324749 -0.36087121]\n",
      "[ episode 197 ][ timestamp 14 ] state=[ 0.04555566  0.22134346 -0.03324749 -0.36087121], action=0, reward=1.0, next_state=[ 0.04998253  0.02670949 -0.04046491 -0.07885456]\n",
      "[ episode 197 ][ timestamp 15 ] state=[ 0.04998253  0.02670949 -0.04046491 -0.07885456], action=1, reward=1.0, next_state=[ 0.05051672  0.22238746 -0.042042   -0.38402468]\n",
      "[ episode 197 ][ timestamp 16 ] state=[ 0.05051672  0.22238746 -0.042042   -0.38402468], action=0, reward=1.0, next_state=[ 0.05496447  0.02788684 -0.0497225  -0.10488825]\n",
      "[ episode 197 ][ timestamp 17 ] state=[ 0.05496447  0.02788684 -0.0497225  -0.10488825], action=1, reward=1.0, next_state=[ 0.0555222   0.22368478 -0.05182026 -0.41283458]\n",
      "[ episode 197 ][ timestamp 18 ] state=[ 0.0555222   0.22368478 -0.05182026 -0.41283458], action=0, reward=1.0, next_state=[ 0.0599959   0.02933422 -0.06007695 -0.13692841]\n",
      "[ episode 197 ][ timestamp 19 ] state=[ 0.0599959   0.02933422 -0.06007695 -0.13692841], action=1, reward=1.0, next_state=[ 0.06058258  0.22526291 -0.06281552 -0.44794325]\n",
      "[ episode 197 ][ timestamp 20 ] state=[ 0.06058258  0.22526291 -0.06281552 -0.44794325], action=0, reward=1.0, next_state=[ 0.06508784  0.03108316 -0.07177439 -0.17570369]\n",
      "[ episode 197 ][ timestamp 21 ] state=[ 0.06508784  0.03108316 -0.07177439 -0.17570369], action=1, reward=1.0, next_state=[ 0.0657095   0.22715499 -0.07528846 -0.49013776]\n",
      "[ episode 197 ][ timestamp 22 ] state=[ 0.0657095   0.22715499 -0.07528846 -0.49013776], action=0, reward=1.0, next_state=[ 0.0702526   0.03317138 -0.08509122 -0.22210054]\n",
      "[ episode 197 ][ timestamp 23 ] state=[ 0.0702526   0.03317138 -0.08509122 -0.22210054], action=1, reward=1.0, next_state=[ 0.07091603  0.22939998 -0.08953323 -0.54036513]\n",
      "[ episode 197 ][ timestamp 24 ] state=[ 0.07091603  0.22939998 -0.08953323 -0.54036513], action=0, reward=1.0, next_state=[ 0.07550403  0.03564309 -0.10034053 -0.27718153]\n",
      "[ episode 197 ][ timestamp 25 ] state=[ 0.07550403  0.03564309 -0.10034053 -0.27718153], action=1, reward=1.0, next_state=[ 0.07621689  0.23204272 -0.10588416 -0.59974981]\n",
      "[ episode 197 ][ timestamp 26 ] state=[ 0.07621689  0.23204272 -0.10588416 -0.59974981], action=0, reward=1.0, next_state=[ 0.08085775  0.03854903 -0.11787916 -0.34220657]\n",
      "[ episode 197 ][ timestamp 27 ] state=[ 0.08085775  0.03854903 -0.11787916 -0.34220657], action=1, reward=1.0, next_state=[ 0.08162873  0.23513354 -0.12472329 -0.66961324]\n",
      "[ episode 197 ][ timestamp 28 ] state=[ 0.08162873  0.23513354 -0.12472329 -0.66961324], action=0, reward=1.0, next_state=[ 0.0863314   0.04194615 -0.13811555 -0.41865678]\n",
      "[ episode 197 ][ timestamp 29 ] state=[ 0.0863314   0.04194615 -0.13811555 -0.41865678], action=1, reward=1.0, next_state=[ 0.08717032  0.23872732 -0.14648869 -0.75149467]\n",
      "[ episode 197 ][ timestamp 30 ] state=[ 0.08717032  0.23872732 -0.14648869 -0.75149467], action=0, reward=1.0, next_state=[ 0.09194487  0.04589649 -0.16151858 -0.50826016]\n",
      "[ episode 197 ][ timestamp 31 ] state=[ 0.09194487  0.04589649 -0.16151858 -0.50826016], action=1, reward=1.0, next_state=[ 0.0928628   0.24288149 -0.17168379 -0.84717203]\n",
      "[ episode 197 ][ timestamp 32 ] state=[ 0.0928628   0.24288149 -0.17168379 -0.84717203], action=0, reward=1.0, next_state=[ 0.09772043  0.05046494 -0.18862723 -0.61301787]\n",
      "[ episode 197 ][ timestamp 33 ] state=[ 0.09772043  0.05046494 -0.18862723 -0.61301787], action=1, reward=1.0, next_state=[ 0.09872973  0.24765232 -0.20088758 -0.95868067]\n",
      "[ episode 197 ][ timestamp 34 ] state=[ 0.09872973  0.24765232 -0.20088758 -0.95868067], action=0, reward=-1.0, next_state=[ 0.10368277  0.05571477 -0.2200612  -0.73522869]\n",
      "[ Ended! ] Episode 197: Exploration_rate=0.3743896431025813. Score=34.\n",
      "[ Experience replay ] starts\n",
      "[ episode 198 ] state=[-0.02861371  0.01038549  0.01862435  0.00107502]\n",
      "[ episode 198 ][ timestamp 1 ] state=[-0.02861371  0.01038549  0.01862435  0.00107502], action=0, reward=1.0, next_state=[-0.028406   -0.18499854  0.01864585  0.29957548]\n",
      "[ episode 198 ][ timestamp 2 ] state=[-0.028406   -0.18499854  0.01864585  0.29957548], action=1, reward=1.0, next_state=[-0.03210597  0.00985274  0.02463736  0.01283093]\n",
      "[ episode 198 ][ timestamp 3 ] state=[-0.03210597  0.00985274  0.02463736  0.01283093], action=0, reward=1.0, next_state=[-0.03190892 -0.18561372  0.02489398  0.31318429]\n",
      "[ episode 198 ][ timestamp 4 ] state=[-0.03190892 -0.18561372  0.02489398  0.31318429], action=0, reward=1.0, next_state=[-0.03562119 -0.3810813   0.03115766  0.61361288]\n",
      "[ episode 198 ][ timestamp 5 ] state=[-0.03562119 -0.3810813   0.03115766  0.61361288], action=1, reward=1.0, next_state=[-0.04324282 -0.18640828  0.04342992  0.33090396]\n",
      "[ episode 198 ][ timestamp 6 ] state=[-0.04324282 -0.18640828  0.04342992  0.33090396], action=1, reward=1.0, next_state=[-0.04697098  0.00806942  0.050048    0.05222687]\n",
      "[ episode 198 ][ timestamp 7 ] state=[-0.04697098  0.00806942  0.050048    0.05222687], action=0, reward=1.0, next_state=[-0.04680959 -0.18773311  0.05109254  0.36027087]\n",
      "[ episode 198 ][ timestamp 8 ] state=[-0.04680959 -0.18773311  0.05109254  0.36027087], action=0, reward=1.0, next_state=[-0.05056426 -0.38354269  0.05829795  0.66861663]\n",
      "[ episode 198 ][ timestamp 9 ] state=[-0.05056426 -0.38354269  0.05829795  0.66861663], action=1, reward=1.0, next_state=[-0.05823511 -0.18927777  0.07167029  0.39484418]\n",
      "[ episode 198 ][ timestamp 10 ] state=[-0.05823511 -0.18927777  0.07167029  0.39484418], action=0, reward=1.0, next_state=[-0.06202067 -0.3853396   0.07956717  0.70923496]\n",
      "[ episode 198 ][ timestamp 11 ] state=[-0.06202067 -0.3853396   0.07956717  0.70923496], action=1, reward=1.0, next_state=[-0.06972746 -0.1914046   0.09375187  0.44262088]\n",
      "[ episode 198 ][ timestamp 12 ] state=[-0.06972746 -0.1914046   0.09375187  0.44262088], action=1, reward=1.0, next_state=[-0.07355555  0.00227438  0.10260429  0.18090092]\n",
      "[ episode 198 ][ timestamp 13 ] state=[-0.07355555  0.00227438  0.10260429  0.18090092], action=0, reward=1.0, next_state=[-0.07351006 -0.19415471  0.10622231  0.50410772]\n",
      "[ episode 198 ][ timestamp 14 ] state=[-0.07351006 -0.19415471  0.10622231  0.50410772], action=0, reward=1.0, next_state=[-0.07739316 -0.39060075  0.11630446  0.8282866 ]\n",
      "[ episode 198 ][ timestamp 15 ] state=[-0.07739316 -0.39060075  0.11630446  0.8282866 ], action=1, reward=1.0, next_state=[-0.08520517 -0.19724462  0.13287019  0.57432828]\n",
      "[ episode 198 ][ timestamp 16 ] state=[-0.08520517 -0.19724462  0.13287019  0.57432828], action=1, reward=1.0, next_state=[-0.08915006 -0.00421113  0.14435676  0.32627922]\n",
      "[ episode 198 ][ timestamp 17 ] state=[-0.08915006 -0.00421113  0.14435676  0.32627922], action=1, reward=1.0, next_state=[-0.08923429  0.18859216  0.15088234  0.08237603]\n",
      "[ episode 198 ][ timestamp 18 ] state=[-0.08923429  0.18859216  0.15088234  0.08237603], action=1, reward=1.0, next_state=[-0.08546244  0.38126541  0.15252986 -0.15915906]\n",
      "[ episode 198 ][ timestamp 19 ] state=[-0.08546244  0.38126541  0.15252986 -0.15915906], action=0, reward=1.0, next_state=[-0.07783714  0.18432615  0.14934668  0.17749019]\n",
      "[ episode 198 ][ timestamp 20 ] state=[-0.07783714  0.18432615  0.14934668  0.17749019], action=1, reward=1.0, next_state=[-0.07415061  0.37703036  0.15289649 -0.06460361]\n",
      "[ episode 198 ][ timestamp 21 ] state=[-0.07415061  0.37703036  0.15289649 -0.06460361], action=1, reward=1.0, next_state=[-0.06661001  0.56966724  0.15160441 -0.30541137]\n",
      "[ episode 198 ][ timestamp 22 ] state=[-0.06661001  0.56966724  0.15160441 -0.30541137], action=1, reward=1.0, next_state=[-0.05521666  0.76234031  0.14549619 -0.5467049 ]\n",
      "[ episode 198 ][ timestamp 23 ] state=[-0.05521666  0.76234031  0.14549619 -0.5467049 ], action=1, reward=1.0, next_state=[-0.03996985  0.95515068  0.13456209 -0.79023951]\n",
      "[ episode 198 ][ timestamp 24 ] state=[-0.03996985  0.95515068  0.13456209 -0.79023951], action=1, reward=1.0, next_state=[-0.02086684  1.14819354  0.1187573  -1.03774421]\n",
      "[ episode 198 ][ timestamp 25 ] state=[-0.02086684  1.14819354  0.1187573  -1.03774421], action=0, reward=1.0, next_state=[ 0.00209703  0.95171069  0.09800241 -0.71026315]\n",
      "[ episode 198 ][ timestamp 26 ] state=[ 0.00209703  0.95171069  0.09800241 -0.71026315], action=0, reward=1.0, next_state=[ 0.02113124  0.75537791  0.08379715 -0.3884105 ]\n",
      "[ episode 198 ][ timestamp 27 ] state=[ 0.02113124  0.75537791  0.08379715 -0.3884105 ], action=0, reward=1.0, next_state=[ 0.0362388   0.5591727   0.07602894 -0.07052785]\n",
      "[ episode 198 ][ timestamp 28 ] state=[ 0.0362388   0.5591727   0.07602894 -0.07052785], action=0, reward=1.0, next_state=[0.04742226 0.36304774 0.07461838 0.24514072]\n",
      "[ episode 198 ][ timestamp 29 ] state=[0.04742226 0.36304774 0.07461838 0.24514072], action=0, reward=1.0, next_state=[0.05468321 0.16694374 0.0795212  0.56039564]\n",
      "[ episode 198 ][ timestamp 30 ] state=[0.05468321 0.16694374 0.0795212  0.56039564], action=0, reward=1.0, next_state=[ 0.05802209 -0.02919896  0.09072911  0.87703453]\n",
      "[ episode 198 ][ timestamp 31 ] state=[ 0.05802209 -0.02919896  0.09072911  0.87703453], action=1, reward=1.0, next_state=[0.05743811 0.16458053 0.1082698  0.61419862]\n",
      "[ episode 198 ][ timestamp 32 ] state=[0.05743811 0.16458053 0.1082698  0.61419862], action=1, reward=1.0, next_state=[0.06072972 0.35803632 0.12055377 0.35748325]\n",
      "[ episode 198 ][ timestamp 33 ] state=[0.06072972 0.35803632 0.12055377 0.35748325], action=1, reward=1.0, next_state=[0.06789044 0.55125657 0.12770344 0.10511343]\n",
      "[ episode 198 ][ timestamp 34 ] state=[0.06789044 0.55125657 0.12770344 0.10511343], action=1, reward=1.0, next_state=[ 0.07891557  0.74433914  0.12980571 -0.14470916]\n",
      "[ episode 198 ][ timestamp 35 ] state=[ 0.07891557  0.74433914  0.12980571 -0.14470916], action=0, reward=1.0, next_state=[0.09380236 0.54762024 0.12691152 0.1859425 ]\n",
      "[ episode 198 ][ timestamp 36 ] state=[0.09380236 0.54762024 0.12691152 0.1859425 ], action=1, reward=1.0, next_state=[ 0.10475476  0.74071949  0.13063037 -0.06416498]\n",
      "[ episode 198 ][ timestamp 37 ] state=[ 0.10475476  0.74071949  0.13063037 -0.06416498], action=0, reward=1.0, next_state=[0.11956915 0.54399012 0.12934707 0.26671108]\n",
      "[ episode 198 ][ timestamp 38 ] state=[0.11956915 0.54399012 0.12934707 0.26671108], action=1, reward=1.0, next_state=[0.13044895 0.73705158 0.1346813  0.01746012]\n",
      "[ episode 198 ][ timestamp 39 ] state=[0.13044895 0.73705158 0.1346813  0.01746012], action=0, reward=1.0, next_state=[0.14518999 0.54028101 0.1350305  0.34941981]\n",
      "[ episode 198 ][ timestamp 40 ] state=[0.14518999 0.54028101 0.1350305  0.34941981], action=1, reward=1.0, next_state=[0.15599561 0.73325003 0.14201889 0.10217955]\n",
      "[ episode 198 ][ timestamp 41 ] state=[0.15599561 0.73325003 0.14201889 0.10217955], action=1, reward=1.0, next_state=[ 0.17066061  0.92608126  0.14406249 -0.1425419 ]\n",
      "[ episode 198 ][ timestamp 42 ] state=[ 0.17066061  0.92608126  0.14406249 -0.1425419 ], action=0, reward=1.0, next_state=[0.18918223 0.72922144 0.14121165 0.19189692]\n",
      "[ episode 198 ][ timestamp 43 ] state=[0.18918223 0.72922144 0.14121165 0.19189692], action=1, reward=1.0, next_state=[ 0.20376666  0.92207057  0.14504959 -0.05311901]\n",
      "[ episode 198 ][ timestamp 44 ] state=[ 0.20376666  0.92207057  0.14504959 -0.05311901], action=0, reward=1.0, next_state=[0.22220807 0.72519881 0.14398721 0.28158273]\n",
      "[ episode 198 ][ timestamp 45 ] state=[0.22220807 0.72519881 0.14398721 0.28158273], action=1, reward=1.0, next_state=[0.23671205 0.91800482 0.14961886 0.03755263]\n",
      "[ episode 198 ][ timestamp 46 ] state=[0.23671205 0.91800482 0.14961886 0.03755263], action=0, reward=1.0, next_state=[0.25507214 0.72108935 0.15036991 0.37344994]\n",
      "[ episode 198 ][ timestamp 47 ] state=[0.25507214 0.72108935 0.15036991 0.37344994], action=0, reward=1.0, next_state=[0.26949393 0.52418694 0.15783891 0.70951304]\n",
      "[ episode 198 ][ timestamp 48 ] state=[0.26949393 0.52418694 0.15783891 0.70951304], action=1, reward=1.0, next_state=[0.27997767 0.71681162 0.17202917 0.47037991]\n",
      "[ episode 198 ][ timestamp 49 ] state=[0.27997767 0.71681162 0.17202917 0.47037991], action=1, reward=1.0, next_state=[0.2943139  0.90913939 0.18143677 0.23647405]\n",
      "[ episode 198 ][ timestamp 50 ] state=[0.2943139  0.90913939 0.18143677 0.23647405], action=1, reward=1.0, next_state=[0.31249669 1.10126811 0.18616625 0.00606174]\n",
      "[ episode 198 ][ timestamp 51 ] state=[0.31249669 1.10126811 0.18616625 0.00606174], action=0, reward=1.0, next_state=[0.33452205 0.90403196 0.18628749 0.35122119]\n",
      "[ episode 198 ][ timestamp 52 ] state=[0.33452205 0.90403196 0.18628749 0.35122119], action=1, reward=1.0, next_state=[0.35260269 1.0960839  0.19331191 0.12257973]\n",
      "[ episode 198 ][ timestamp 53 ] state=[0.35260269 1.0960839  0.19331191 0.12257973], action=1, reward=1.0, next_state=[ 0.37452437  1.28798662  0.19576351 -0.1034322 ]\n",
      "[ episode 198 ][ timestamp 54 ] state=[ 0.37452437  1.28798662  0.19576351 -0.1034322 ], action=0, reward=1.0, next_state=[0.4002841  1.0906766  0.19369486 0.24406729]\n",
      "[ episode 198 ][ timestamp 55 ] state=[0.4002841  1.0906766  0.19369486 0.24406729], action=1, reward=1.0, next_state=[0.42209763 1.28258057 0.19857621 0.01818519]\n",
      "[ episode 198 ][ timestamp 56 ] state=[0.42209763 1.28258057 0.19857621 0.01818519], action=1, reward=1.0, next_state=[ 0.44774925  1.47438282  0.19893991 -0.20586589]\n",
      "[ episode 198 ][ timestamp 57 ] state=[ 0.44774925  1.47438282  0.19893991 -0.20586589], action=0, reward=1.0, next_state=[0.4772369  1.27705457 0.19482259 0.14239181]\n",
      "[ episode 198 ][ timestamp 58 ] state=[0.4772369  1.27705457 0.19482259 0.14239181], action=1, reward=1.0, next_state=[ 0.50277799  1.4689303   0.19767043 -0.08306074]\n",
      "[ episode 198 ][ timestamp 59 ] state=[ 0.50277799  1.4689303   0.19767043 -0.08306074], action=0, reward=1.0, next_state=[0.5321566  1.27160491 0.19600921 0.26490088]\n",
      "[ episode 198 ][ timestamp 60 ] state=[0.5321566  1.27160491 0.19600921 0.26490088], action=1, reward=1.0, next_state=[0.5575887  1.46346809 0.20130723 0.03987532]\n",
      "[ episode 198 ][ timestamp 61 ] state=[0.5575887  1.46346809 0.20130723 0.03987532], action=1, reward=1.0, next_state=[ 0.58685806  1.65521973  0.20210474 -0.1831584 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 198 ][ timestamp 62 ] state=[ 0.58685806  1.65521973  0.20210474 -0.1831584 ], action=0, reward=1.0, next_state=[0.61996245 1.45786601 0.19844157 0.16586196]\n",
      "[ episode 198 ][ timestamp 63 ] state=[0.61996245 1.45786601 0.19844157 0.16586196], action=1, reward=1.0, next_state=[ 0.64911977  1.64967598  0.20175881 -0.05824691]\n",
      "[ episode 198 ][ timestamp 64 ] state=[ 0.64911977  1.64967598  0.20175881 -0.05824691], action=0, reward=1.0, next_state=[0.68211329 1.45231896 0.20059387 0.29069919]\n",
      "[ episode 198 ][ timestamp 65 ] state=[0.68211329 1.45231896 0.20059387 0.29069919], action=1, reward=1.0, next_state=[0.71115967 1.64409997 0.20640785 0.06737584]\n",
      "[ episode 198 ][ timestamp 66 ] state=[0.71115967 1.64409997 0.20640785 0.06737584], action=1, reward=1.0, next_state=[ 0.74404167  1.83575697  0.20775537 -0.1537534 ]\n",
      "[ episode 198 ][ timestamp 67 ] state=[ 0.74404167  1.83575697  0.20775537 -0.1537534 ], action=1, reward=1.0, next_state=[ 0.78075681  2.02739259  0.2046803  -0.3743839 ]\n",
      "[ episode 198 ][ timestamp 68 ] state=[ 0.78075681  2.02739259  0.2046803  -0.3743839 ], action=0, reward=1.0, next_state=[ 0.82130466  1.83004088  0.19719263 -0.02477889]\n",
      "[ episode 198 ][ timestamp 69 ] state=[ 0.82130466  1.83004088  0.19719263 -0.02477889], action=0, reward=1.0, next_state=[0.85790548 1.63271795 0.19669705 0.3230691 ]\n",
      "[ episode 198 ][ timestamp 70 ] state=[0.85790548 1.63271795 0.19669705 0.3230691 ], action=1, reward=1.0, next_state=[0.89055984 1.82457491 0.20315843 0.09828966]\n",
      "[ episode 198 ][ timestamp 71 ] state=[0.89055984 1.82457491 0.20315843 0.09828966], action=1, reward=1.0, next_state=[ 0.92705134  2.01629361  0.20512422 -0.12405557]\n",
      "[ episode 198 ][ timestamp 72 ] state=[ 0.92705134  2.01629361  0.20512422 -0.12405557], action=0, reward=1.0, next_state=[0.96737721 1.81891397 0.20264311 0.22569151]\n",
      "[ episode 198 ][ timestamp 73 ] state=[0.96737721 1.81891397 0.20264311 0.22569151], action=1, reward=1.0, next_state=[1.00375549 2.01065037 0.20715694 0.00314203]\n",
      "[ episode 198 ][ timestamp 74 ] state=[1.00375549 2.01065037 0.20715694 0.00314203], action=0, reward=1.0, next_state=[1.0439685  1.81325277 0.20721978 0.35337723]\n",
      "[ episode 198 ][ timestamp 75 ] state=[1.0439685  1.81325277 0.20721978 0.35337723], action=1, reward=-1.0, next_state=[1.08023355 2.00491867 0.21428733 0.13251649]\n",
      "[ Ended! ] Episode 198: Exploration_rate=0.37251769488706843. Score=75.\n",
      "[ Experience replay ] starts\n",
      "[ episode 199 ] state=[ 3.89629549e-02  1.68746965e-02 -7.08327466e-05 -4.33471963e-02]\n",
      "[ episode 199 ][ timestamp 1 ] state=[ 3.89629549e-02  1.68746965e-02 -7.08327466e-05 -4.33471963e-02], action=1, reward=1.0, next_state=[ 0.03930045  0.21199766 -0.00093778 -0.33605247]\n",
      "[ episode 199 ][ timestamp 2 ] state=[ 0.03930045  0.21199766 -0.00093778 -0.33605247], action=0, reward=1.0, next_state=[ 0.0435404   0.01688907 -0.00765883 -0.04366542]\n",
      "[ episode 199 ][ timestamp 3 ] state=[ 0.0435404   0.01688907 -0.00765883 -0.04366542], action=1, reward=1.0, next_state=[ 0.04387818  0.21212    -0.00853213 -0.3387549 ]\n",
      "[ episode 199 ][ timestamp 4 ] state=[ 0.04387818  0.21212    -0.00853213 -0.3387549 ], action=0, reward=1.0, next_state=[ 0.04812058  0.01712049 -0.01530723 -0.0487747 ]\n",
      "[ episode 199 ][ timestamp 5 ] state=[ 0.04812058  0.01712049 -0.01530723 -0.0487747 ], action=0, reward=1.0, next_state=[ 0.04846299 -0.17777866 -0.01628273  0.23903963]\n",
      "[ episode 199 ][ timestamp 6 ] state=[ 0.04846299 -0.17777866 -0.01628273  0.23903963], action=1, reward=1.0, next_state=[ 0.04490742  0.01757207 -0.01150193 -0.05873453]\n",
      "[ episode 199 ][ timestamp 7 ] state=[ 0.04490742  0.01757207 -0.01150193 -0.05873453], action=1, reward=1.0, next_state=[ 0.04525886  0.21285704 -0.01267662 -0.3550241 ]\n",
      "[ episode 199 ][ timestamp 8 ] state=[ 0.04525886  0.21285704 -0.01267662 -0.3550241 ], action=0, reward=1.0, next_state=[ 0.049516    0.0179176  -0.01977711 -0.06636526]\n",
      "[ episode 199 ][ timestamp 9 ] state=[ 0.049516    0.0179176  -0.01977711 -0.06636526], action=1, reward=1.0, next_state=[ 0.04987435  0.21331743 -0.02110441 -0.36522179]\n",
      "[ episode 199 ][ timestamp 10 ] state=[ 0.04987435  0.21331743 -0.02110441 -0.36522179], action=1, reward=1.0, next_state=[ 0.0541407   0.40873285 -0.02840885 -0.66448387]\n",
      "[ episode 199 ][ timestamp 11 ] state=[ 0.0541407   0.40873285 -0.02840885 -0.66448387], action=0, reward=1.0, next_state=[ 0.06231536  0.21401736 -0.04169852 -0.38087957]\n",
      "[ episode 199 ][ timestamp 12 ] state=[ 0.06231536  0.21401736 -0.04169852 -0.38087957], action=1, reward=1.0, next_state=[ 0.06659571  0.40970586 -0.04931612 -0.68641297]\n",
      "[ episode 199 ][ timestamp 13 ] state=[ 0.06659571  0.40970586 -0.04931612 -0.68641297], action=0, reward=1.0, next_state=[ 0.07478982  0.21530193 -0.06304438 -0.40965468]\n",
      "[ episode 199 ][ timestamp 14 ] state=[ 0.07478982  0.21530193 -0.06304438 -0.40965468], action=1, reward=1.0, next_state=[ 0.07909586  0.41125841 -0.07123747 -0.72152822]\n",
      "[ episode 199 ][ timestamp 15 ] state=[ 0.07909586  0.41125841 -0.07123747 -0.72152822], action=0, reward=1.0, next_state=[ 0.08732103  0.21719044 -0.08566803 -0.45209071]\n",
      "[ episode 199 ][ timestamp 16 ] state=[ 0.08732103  0.21719044 -0.08566803 -0.45209071], action=0, reward=1.0, next_state=[ 0.09166484  0.02337786 -0.09470985 -0.18759357]\n",
      "[ episode 199 ][ timestamp 17 ] state=[ 0.09166484  0.02337786 -0.09470985 -0.18759357], action=1, reward=1.0, next_state=[ 0.0921324   0.21971825 -0.09846172 -0.50858736]\n",
      "[ episode 199 ][ timestamp 18 ] state=[ 0.0921324   0.21971825 -0.09846172 -0.50858736], action=0, reward=1.0, next_state=[ 0.09652676  0.02611136 -0.10863347 -0.24848461]\n",
      "[ episode 199 ][ timestamp 19 ] state=[ 0.09652676  0.02611136 -0.10863347 -0.24848461], action=1, reward=1.0, next_state=[ 0.09704899  0.22260351 -0.11360316 -0.57336087]\n",
      "[ episode 199 ][ timestamp 20 ] state=[ 0.09704899  0.22260351 -0.11360316 -0.57336087], action=0, reward=1.0, next_state=[ 0.10150106  0.02924227 -0.12507038 -0.31851612]\n",
      "[ episode 199 ][ timestamp 21 ] state=[ 0.10150106  0.02924227 -0.12507038 -0.31851612], action=1, reward=1.0, next_state=[ 0.10208591  0.225903   -0.1314407  -0.6478779 ]\n",
      "[ episode 199 ][ timestamp 22 ] state=[ 0.10208591  0.225903   -0.1314407  -0.6478779 ], action=0, reward=1.0, next_state=[ 0.10660396  0.03283345 -0.14439826 -0.39930408]\n",
      "[ episode 199 ][ timestamp 23 ] state=[ 0.10660396  0.03283345 -0.14439826 -0.39930408], action=1, reward=1.0, next_state=[ 0.10726063  0.229677   -0.15238434 -0.73380218]\n",
      "[ episode 199 ][ timestamp 24 ] state=[ 0.10726063  0.229677   -0.15238434 -0.73380218], action=0, reward=1.0, next_state=[ 0.11185417  0.03695198 -0.16706038 -0.49269242]\n",
      "[ episode 199 ][ timestamp 25 ] state=[ 0.11185417  0.03695198 -0.16706038 -0.49269242], action=1, reward=1.0, next_state=[ 0.11259321  0.23398746 -0.17691423 -0.83301852]\n",
      "[ episode 199 ][ timestamp 26 ] state=[ 0.11259321  0.23398746 -0.17691423 -0.83301852], action=0, reward=1.0, next_state=[ 0.11727296  0.04166683 -0.1935746  -0.60078222]\n",
      "[ episode 199 ][ timestamp 27 ] state=[ 0.11727296  0.04166683 -0.1935746  -0.60078222], action=1, reward=1.0, next_state=[ 0.1181063   0.23889432 -0.20559024 -0.94765417]\n",
      "[ episode 199 ][ timestamp 28 ] state=[ 0.1181063   0.23889432 -0.20559024 -0.94765417], action=0, reward=-1.0, next_state=[ 0.12288419  0.04704462 -0.22454333 -0.72595859]\n",
      "[ Ended! ] Episode 199: Exploration_rate=0.3706551064126331. Score=28.\n",
      "[ Experience replay ] starts\n",
      "[ episode 200 ] state=[ 0.0109699   0.03241533 -0.02406924  0.00413821]\n",
      "[ episode 200 ][ timestamp 1 ] state=[ 0.0109699   0.03241533 -0.02406924  0.00413821], action=0, reward=1.0, next_state=[ 0.0116182  -0.16235331 -0.02398647  0.28913088]\n",
      "[ episode 200 ][ timestamp 2 ] state=[ 0.0116182  -0.16235331 -0.02398647  0.28913088], action=0, reward=1.0, next_state=[ 0.00837114 -0.35712515 -0.01820386  0.57415325]\n",
      "[ episode 200 ][ timestamp 3 ] state=[ 0.00837114 -0.35712515 -0.01820386  0.57415325], action=0, reward=1.0, next_state=[ 0.00122863 -0.55198722 -0.00672079  0.86104628]\n",
      "[ episode 200 ][ timestamp 4 ] state=[ 0.00122863 -0.55198722 -0.00672079  0.86104628], action=0, reward=1.0, next_state=[-0.00981111 -0.74701701  0.01050013  1.15160845]\n",
      "[ episode 200 ][ timestamp 5 ] state=[-0.00981111 -0.74701701  0.01050013  1.15160845], action=1, reward=1.0, next_state=[-0.02475145 -0.55203362  0.0335323   0.86223648]\n",
      "[ episode 200 ][ timestamp 6 ] state=[-0.02475145 -0.55203362  0.0335323   0.86223648], action=0, reward=1.0, next_state=[-0.03579212 -0.74759571  0.05077703  1.16527136]\n",
      "[ episode 200 ][ timestamp 7 ] state=[-0.03579212 -0.74759571  0.05077703  1.16527136], action=1, reward=1.0, next_state=[-0.05074404 -0.55317014  0.07408246  0.88893092]\n",
      "[ episode 200 ][ timestamp 8 ] state=[-0.05074404 -0.55317014  0.07408246  0.88893092], action=1, reward=1.0, next_state=[-0.06180744 -0.35912747  0.09186108  0.6204256 ]\n",
      "[ episode 200 ][ timestamp 9 ] state=[-0.06180744 -0.35912747  0.09186108  0.6204256 ], action=1, reward=1.0, next_state=[-0.06898999 -0.16540038  0.10426959  0.35802935]\n",
      "[ episode 200 ][ timestamp 10 ] state=[-0.06898999 -0.16540038  0.10426959  0.35802935], action=0, reward=1.0, next_state=[-0.072298   -0.36183817  0.11143018  0.68168545]\n",
      "[ episode 200 ][ timestamp 11 ] state=[-0.072298   -0.36183817  0.11143018  0.68168545], action=1, reward=1.0, next_state=[-0.07953476 -0.16842568  0.12506389  0.42605873]\n",
      "[ episode 200 ][ timestamp 12 ] state=[-0.07953476 -0.16842568  0.12506389  0.42605873], action=1, reward=1.0, next_state=[-0.08290328  0.02472351  0.13358506  0.17527079]\n",
      "[ episode 200 ][ timestamp 13 ] state=[-0.08290328  0.02472351  0.13358506  0.17527079], action=0, reward=1.0, next_state=[-0.08240881 -0.17203216  0.13709048  0.50693218]\n",
      "[ episode 200 ][ timestamp 14 ] state=[-0.08240881 -0.17203216  0.13709048  0.50693218], action=1, reward=1.0, next_state=[-0.08584945  0.02091883  0.14722912  0.26039962]\n",
      "[ episode 200 ][ timestamp 15 ] state=[-0.08584945  0.02091883  0.14722912  0.26039962], action=0, reward=1.0, next_state=[-0.08543107 -0.17596449  0.15243711  0.59565874]\n",
      "[ episode 200 ][ timestamp 16 ] state=[-0.08543107 -0.17596449  0.15243711  0.59565874], action=1, reward=1.0, next_state=[-0.08895036  0.01673246  0.16435029  0.35460825]\n",
      "[ episode 200 ][ timestamp 17 ] state=[-0.08895036  0.01673246  0.16435029  0.35460825], action=1, reward=1.0, next_state=[-0.08861571  0.20918272  0.17144245  0.11792455]\n",
      "[ episode 200 ][ timestamp 18 ] state=[-0.08861571  0.20918272  0.17144245  0.11792455], action=0, reward=1.0, next_state=[-0.08443206  0.01207203  0.17380094  0.45941356]\n",
      "[ episode 200 ][ timestamp 19 ] state=[-0.08443206  0.01207203  0.17380094  0.45941356], action=1, reward=1.0, next_state=[-0.08419062  0.20436616  0.18298921  0.22615846]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 200 ][ timestamp 20 ] state=[-0.08419062  0.20436616  0.18298921  0.22615846], action=1, reward=1.0, next_state=[-0.08010329  0.39646562  0.18751238 -0.00368073]\n",
      "[ episode 200 ][ timestamp 21 ] state=[-0.08010329  0.39646562  0.18751238 -0.00368073], action=0, reward=1.0, next_state=[-0.07217398  0.19921861  0.18743877  0.34180961]\n",
      "[ episode 200 ][ timestamp 22 ] state=[-0.07217398  0.19921861  0.18743877  0.34180961], action=0, reward=1.0, next_state=[-0.06818961  0.00199337  0.19427496  0.68725066]\n",
      "[ episode 200 ][ timestamp 23 ] state=[-0.06818961  0.00199337  0.19427496  0.68725066], action=1, reward=1.0, next_state=[-0.06814974  0.19396415  0.20801997  0.46146978]\n",
      "[ episode 200 ][ timestamp 24 ] state=[-0.06814974  0.19396415  0.20801997  0.46146978], action=1, reward=-1.0, next_state=[-0.06427046  0.38563285  0.21724937  0.24088255]\n",
      "[ Ended! ] Episode 200: Exploration_rate=0.36880183088056995. Score=24.\n",
      "[ Experience replay ] starts\n",
      "[ episode 201 ] state=[-0.0340596  -0.02738938 -0.03179443  0.04312717]\n",
      "[ episode 201 ][ timestamp 1 ] state=[-0.0340596  -0.02738938 -0.03179443  0.04312717], action=0, reward=1.0, next_state=[-0.03460739 -0.22204132 -0.03093189  0.32561153]\n",
      "[ episode 201 ][ timestamp 2 ] state=[-0.03460739 -0.22204132 -0.03093189  0.32561153], action=0, reward=1.0, next_state=[-0.03904822 -0.41670952 -0.02441965  0.60838163]\n",
      "[ episode 201 ][ timestamp 3 ] state=[-0.03904822 -0.41670952 -0.02441965  0.60838163], action=0, reward=1.0, next_state=[-0.04738241 -0.61148172 -0.01225202  0.89327415]\n",
      "[ episode 201 ][ timestamp 4 ] state=[-0.04738241 -0.61148172 -0.01225202  0.89327415], action=0, reward=1.0, next_state=[-0.05961204 -0.80643537  0.00561346  1.18208068]\n",
      "[ episode 201 ][ timestamp 5 ] state=[-0.05961204 -0.80643537  0.00561346  1.18208068], action=0, reward=1.0, next_state=[-0.07574075 -1.00162972  0.02925507  1.47651794]\n",
      "[ episode 201 ][ timestamp 6 ] state=[-0.07574075 -1.00162972  0.02925507  1.47651794], action=1, reward=1.0, next_state=[-0.09577334 -0.80687707  0.05878543  1.19311373]\n",
      "[ episode 201 ][ timestamp 7 ] state=[-0.09577334 -0.80687707  0.05878543  1.19311373], action=1, reward=1.0, next_state=[-0.11191089 -0.61256373  0.08264771  0.91942016]\n",
      "[ episode 201 ][ timestamp 8 ] state=[-0.11191089 -0.61256373  0.08264771  0.91942016], action=1, reward=1.0, next_state=[-0.12416216 -0.41865029  0.10103611  0.65381363]\n",
      "[ episode 201 ][ timestamp 9 ] state=[-0.12416216 -0.41865029  0.10103611  0.65381363], action=1, reward=1.0, next_state=[-0.13253517 -0.22506955  0.11411238  0.39457745]\n",
      "[ episode 201 ][ timestamp 10 ] state=[-0.13253517 -0.22506955  0.11411238  0.39457745], action=1, reward=1.0, next_state=[-0.13703656 -0.03173608  0.12200393  0.13993962]\n",
      "[ episode 201 ][ timestamp 11 ] state=[-0.13703656 -0.03173608  0.12200393  0.13993962], action=1, reward=1.0, next_state=[-0.13767128  0.16144643  0.12480273 -0.11189995]\n",
      "[ episode 201 ][ timestamp 12 ] state=[-0.13767128  0.16144643  0.12480273 -0.11189995], action=1, reward=1.0, next_state=[-0.13444235  0.35457961  0.12256473 -0.36274968]\n",
      "[ episode 201 ][ timestamp 13 ] state=[-0.13444235  0.35457961  0.12256473 -0.36274968], action=0, reward=1.0, next_state=[-0.12735076  0.1579482   0.11530973 -0.03407127]\n",
      "[ episode 201 ][ timestamp 14 ] state=[-0.12735076  0.1579482   0.11530973 -0.03407127], action=1, reward=1.0, next_state=[-0.12419179  0.35124396  0.11462831 -0.28826348]\n",
      "[ episode 201 ][ timestamp 15 ] state=[-0.12419179  0.35124396  0.11462831 -0.28826348], action=0, reward=1.0, next_state=[-0.11716691  0.15468986  0.10886304  0.03825978]\n",
      "[ episode 201 ][ timestamp 16 ] state=[-0.11716691  0.15468986  0.10886304  0.03825978], action=0, reward=1.0, next_state=[-0.11407312 -0.04181116  0.10962823  0.36320901]\n",
      "[ episode 201 ][ timestamp 17 ] state=[-0.11407312 -0.04181116  0.10962823  0.36320901], action=1, reward=1.0, next_state=[-0.11490934  0.15159581  0.11689241  0.10700632]\n",
      "[ episode 201 ][ timestamp 18 ] state=[-0.11490934  0.15159581  0.11689241  0.10700632], action=1, reward=1.0, next_state=[-0.11187742  0.34486553  0.11903254 -0.14663176]\n",
      "[ episode 201 ][ timestamp 19 ] state=[-0.11187742  0.34486553  0.11903254 -0.14663176], action=0, reward=1.0, next_state=[-0.10498011  0.14825792  0.1160999   0.18110585]\n",
      "[ episode 201 ][ timestamp 20 ] state=[-0.10498011  0.14825792  0.1160999   0.18110585], action=1, reward=1.0, next_state=[-0.10201495  0.34154368  0.11972202 -0.07281424]\n",
      "[ episode 201 ][ timestamp 21 ] state=[-0.10201495  0.34154368  0.11972202 -0.07281424], action=0, reward=1.0, next_state=[-0.09518408  0.14492694  0.11826574  0.255114  ]\n",
      "[ episode 201 ][ timestamp 22 ] state=[-0.09518408  0.14492694  0.11826574  0.255114  ], action=1, reward=1.0, next_state=[-0.09228554  0.33817919  0.12336802  0.00194963]\n",
      "[ episode 201 ][ timestamp 23 ] state=[-0.09228554  0.33817919  0.12336802  0.00194963], action=1, reward=1.0, next_state=[-0.08552196  0.53133575  0.12340701 -0.2494049 ]\n",
      "[ episode 201 ][ timestamp 24 ] state=[-0.08552196  0.53133575  0.12340701 -0.2494049 ], action=0, reward=1.0, next_state=[-0.07489524  0.33468737  0.11841891  0.07951405]\n",
      "[ episode 201 ][ timestamp 25 ] state=[-0.07489524  0.33468737  0.11841891  0.07951405], action=1, reward=1.0, next_state=[-0.0682015   0.52793022  0.12000919 -0.17358637]\n",
      "[ episode 201 ][ timestamp 26 ] state=[-0.0682015   0.52793022  0.12000919 -0.17358637], action=0, reward=1.0, next_state=[-0.05764289  0.33131336  0.11653746  0.15441574]\n",
      "[ episode 201 ][ timestamp 27 ] state=[-0.05764289  0.33131336  0.11653746  0.15441574], action=1, reward=1.0, next_state=[-0.05101662  0.5245906   0.11962578 -0.09934915]\n",
      "[ episode 201 ][ timestamp 28 ] state=[-0.05101662  0.5245906   0.11962578 -0.09934915], action=0, reward=1.0, next_state=[-0.04052481  0.3279754   0.1176388   0.2285521 ]\n",
      "[ episode 201 ][ timestamp 29 ] state=[-0.04052481  0.3279754   0.1176388   0.2285521 ], action=1, reward=1.0, next_state=[-0.0339653   0.52123695  0.12220984 -0.02483055]\n",
      "[ episode 201 ][ timestamp 30 ] state=[-0.0339653   0.52123695  0.12220984 -0.02483055], action=0, reward=1.0, next_state=[-0.02354057  0.32459363  0.12171323  0.3037748 ]\n",
      "[ episode 201 ][ timestamp 31 ] state=[-0.02354057  0.32459363  0.12171323  0.3037748 ], action=0, reward=1.0, next_state=[-0.01704869  0.12796634  0.12778872  0.6322292 ]\n",
      "[ episode 201 ][ timestamp 32 ] state=[-0.01704869  0.12796634  0.12778872  0.6322292 ], action=1, reward=1.0, next_state=[-0.01448937  0.32109568  0.14043331  0.38236502]\n",
      "[ episode 201 ][ timestamp 33 ] state=[-0.01448937  0.32109568  0.14043331  0.38236502], action=1, reward=1.0, next_state=[-0.00806745  0.51397345  0.14808061  0.13704838]\n",
      "[ episode 201 ][ timestamp 34 ] state=[-0.00806745  0.51397345  0.14808061  0.13704838], action=0, reward=1.0, next_state=[0.00221202 0.31707504 0.15082158 0.47254051]\n",
      "[ episode 201 ][ timestamp 35 ] state=[0.00221202 0.31707504 0.15082158 0.47254051], action=1, reward=1.0, next_state=[0.00855352 0.50978105 0.16027239 0.23093653]\n",
      "[ episode 201 ][ timestamp 36 ] state=[0.00855352 0.50978105 0.16027239 0.23093653], action=1, reward=1.0, next_state=[ 0.01874914  0.70229318  0.16489112 -0.00721216]\n",
      "[ episode 201 ][ timestamp 37 ] state=[ 0.01874914  0.70229318  0.16489112 -0.00721216], action=1, reward=1.0, next_state=[ 0.032795    0.89471363  0.16474687 -0.24366932]\n",
      "[ episode 201 ][ timestamp 38 ] state=[ 0.032795    0.89471363  0.16474687 -0.24366932], action=1, reward=1.0, next_state=[ 0.05068927  1.08714617  0.15987349 -0.48019302]\n",
      "[ episode 201 ][ timestamp 39 ] state=[ 0.05068927  1.08714617  0.15987349 -0.48019302], action=0, reward=1.0, next_state=[ 0.0724322   0.89017134  0.15026963 -0.14169584]\n",
      "[ episode 201 ][ timestamp 40 ] state=[ 0.0724322   0.89017134  0.15026963 -0.14169584], action=1, reward=1.0, next_state=[ 0.09023562  1.08285746  0.14743571 -0.38345468]\n",
      "[ episode 201 ][ timestamp 41 ] state=[ 0.09023562  1.08285746  0.14743571 -0.38345468], action=0, reward=1.0, next_state=[ 0.11189277  0.88598351  0.13976662 -0.04815836]\n",
      "[ episode 201 ][ timestamp 42 ] state=[ 0.11189277  0.88598351  0.13976662 -0.04815836], action=1, reward=1.0, next_state=[ 0.12961244  1.07885343  0.13880345 -0.29368436]\n",
      "[ episode 201 ][ timestamp 43 ] state=[ 0.12961244  1.07885343  0.13880345 -0.29368436], action=0, reward=1.0, next_state=[0.15118951 0.88205361 0.13292976 0.03935353]\n",
      "[ episode 201 ][ timestamp 44 ] state=[0.15118951 0.88205361 0.13292976 0.03935353], action=1, reward=1.0, next_state=[ 0.16883058  1.07504367  0.13371683 -0.20861133]\n",
      "[ episode 201 ][ timestamp 45 ] state=[ 0.16883058  1.07504367  0.13371683 -0.20861133], action=0, reward=1.0, next_state=[0.19033146 0.87828835 0.12954461 0.12308277]\n",
      "[ episode 201 ][ timestamp 46 ] state=[0.19033146 0.87828835 0.12954461 0.12308277], action=0, reward=1.0, next_state=[0.20789723 0.68157137 0.13200626 0.45366542]\n",
      "[ episode 201 ][ timestamp 47 ] state=[0.20789723 0.68157137 0.13200626 0.45366542], action=1, reward=1.0, next_state=[0.22152865 0.87460381 0.14107957 0.20533312]\n",
      "[ episode 201 ][ timestamp 48 ] state=[0.22152865 0.87460381 0.14107957 0.20533312], action=1, reward=1.0, next_state=[ 0.23902073  1.06745599  0.14518623 -0.03973117]\n",
      "[ episode 201 ][ timestamp 49 ] state=[ 0.23902073  1.06745599  0.14518623 -0.03973117], action=0, reward=1.0, next_state=[0.26036985 0.87058274 0.14439161 0.29500669]\n",
      "[ episode 201 ][ timestamp 50 ] state=[0.26036985 0.87058274 0.14439161 0.29500669], action=1, reward=1.0, next_state=[0.2777815  1.0633827  0.15029174 0.05112005]\n",
      "[ episode 201 ][ timestamp 51 ] state=[0.2777815  1.0633827  0.15029174 0.05112005], action=0, reward=1.0, next_state=[0.29904916 0.86646106 0.15131414 0.38719242]\n",
      "[ episode 201 ][ timestamp 52 ] state=[0.29904916 0.86646106 0.15131414 0.38719242], action=1, reward=1.0, next_state=[0.31637838 1.05914761 0.15905799 0.14578188]\n",
      "[ episode 201 ][ timestamp 53 ] state=[0.31637838 1.05914761 0.15905799 0.14578188], action=1, reward=1.0, next_state=[ 0.33756133  1.25167659  0.16197363 -0.09280001]\n",
      "[ episode 201 ][ timestamp 54 ] state=[ 0.33756133  1.25167659  0.16197363 -0.09280001], action=1, reward=1.0, next_state=[ 0.36259486  1.44415111  0.16011763 -0.33032051]\n",
      "[ episode 201 ][ timestamp 55 ] state=[ 0.36259486  1.44415111  0.16011763 -0.33032051], action=0, reward=1.0, next_state=[0.39147788 1.24715526 0.15351122 0.00826716]\n",
      "[ episode 201 ][ timestamp 56 ] state=[0.39147788 1.24715526 0.15351122 0.00826716], action=0, reward=1.0, next_state=[0.41642099 1.05020311 0.15367656 0.34517646]\n",
      "[ episode 201 ][ timestamp 57 ] state=[0.41642099 1.05020311 0.15367656 0.34517646], action=0, reward=1.0, next_state=[0.43742505 0.85326719 0.16058009 0.68210229]\n",
      "[ episode 201 ][ timestamp 58 ] state=[0.43742505 0.85326719 0.16058009 0.68210229], action=1, reward=1.0, next_state=[0.4544904  1.04583792 0.17422214 0.44397033]\n",
      "[ episode 201 ][ timestamp 59 ] state=[0.4544904  1.04583792 0.17422214 0.44397033], action=1, reward=1.0, next_state=[0.47540715 1.2381221  0.18310154 0.21087292]\n",
      "[ episode 201 ][ timestamp 60 ] state=[0.47540715 1.2381221  0.18310154 0.21087292], action=0, reward=1.0, next_state=[0.5001696  1.04091879 0.187319   0.55526468]\n",
      "[ episode 201 ][ timestamp 61 ] state=[0.5001696  1.04091879 0.187319   0.55526468], action=1, reward=1.0, next_state=[0.52098797 1.23298535 0.1984243  0.32695485]\n",
      "[ episode 201 ][ timestamp 62 ] state=[0.52098797 1.23298535 0.1984243  0.32695485], action=1, reward=1.0, next_state=[0.54564768 1.42481086 0.20496339 0.10281716]\n",
      "[ episode 201 ][ timestamp 63 ] state=[0.54564768 1.42481086 0.20496339 0.10281716], action=1, reward=1.0, next_state=[ 0.5741439   1.61649606  0.20701974 -0.11885402]\n",
      "[ episode 201 ][ timestamp 64 ] state=[ 0.5741439   1.61649606  0.20701974 -0.11885402], action=1, reward=1.0, next_state=[ 0.60647382  1.80814353  0.20464266 -0.33975711]\n",
      "[ episode 201 ][ timestamp 65 ] state=[ 0.60647382  1.80814353  0.20464266 -0.33975711], action=0, reward=1.0, next_state=[0.64263669 1.61078721 0.19784751 0.0098461 ]\n",
      "[ episode 201 ][ timestamp 66 ] state=[0.64263669 1.61078721 0.19784751 0.0098461 ], action=1, reward=1.0, next_state=[ 0.67485243  1.80260296  0.19804444 -0.21447616]\n",
      "[ episode 201 ][ timestamp 67 ] state=[ 0.67485243  1.80260296  0.19804444 -0.21447616], action=0, reward=1.0, next_state=[0.71090449 1.6052822  0.19375491 0.1335647 ]\n",
      "[ episode 201 ][ timestamp 68 ] state=[0.71090449 1.6052822  0.19375491 0.1335647 ], action=1, reward=1.0, next_state=[ 0.74301014  1.79717724  0.19642621 -0.09228361]\n",
      "[ episode 201 ][ timestamp 69 ] state=[ 0.74301014  1.79717724  0.19642621 -0.09228361], action=1, reward=1.0, next_state=[ 0.77895368  1.989021    0.19458053 -0.31713695]\n",
      "[ episode 201 ][ timestamp 70 ] state=[ 0.77895368  1.989021    0.19458053 -0.31713695], action=1, reward=1.0, next_state=[ 0.8187341   2.18091632  0.18823779 -0.54270163]\n",
      "[ episode 201 ][ timestamp 71 ] state=[ 0.8187341   2.18091632  0.18823779 -0.54270163], action=1, reward=1.0, next_state=[ 0.86235243  2.37296369  0.17738376 -0.77066839]\n",
      "[ episode 201 ][ timestamp 72 ] state=[ 0.86235243  2.37296369  0.17738376 -0.77066839], action=0, reward=1.0, next_state=[ 0.9098117   2.17590203  0.16197039 -0.42783636]\n",
      "[ episode 201 ][ timestamp 73 ] state=[ 0.9098117   2.17590203  0.16197039 -0.42783636], action=0, reward=1.0, next_state=[ 0.95332974  1.97890126  0.15341367 -0.08879153]\n",
      "[ episode 201 ][ timestamp 74 ] state=[ 0.95332974  1.97890126  0.15341367 -0.08879153], action=1, reward=1.0, next_state=[ 0.99290777  2.17152944  0.15163784 -0.32941332]\n",
      "[ episode 201 ][ timestamp 75 ] state=[ 0.99290777  2.17152944  0.15163784 -0.32941332], action=1, reward=1.0, next_state=[ 1.03633835  2.36420416  0.14504957 -0.57069812]\n",
      "[ episode 201 ][ timestamp 76 ] state=[ 1.03633835  2.36420416  0.14504957 -0.57069812], action=1, reward=1.0, next_state=[ 1.08362244  2.5570261   0.13363561 -0.81439853]\n",
      "[ episode 201 ][ timestamp 77 ] state=[ 1.08362244  2.5570261   0.13363561 -0.81439853], action=0, reward=1.0, next_state=[ 1.13476296  2.36035204  0.11734764 -0.48284572]\n",
      "[ episode 201 ][ timestamp 78 ] state=[ 1.13476296  2.36035204  0.11734764 -0.48284572], action=0, reward=1.0, next_state=[ 1.18197     2.16378634  0.10769072 -0.15560386]\n",
      "[ episode 201 ][ timestamp 79 ] state=[ 1.18197     2.16378634  0.10769072 -0.15560386], action=0, reward=1.0, next_state=[1.22524573 1.96730049 0.10457865 0.16901745]\n",
      "[ episode 201 ][ timestamp 80 ] state=[1.22524573 1.96730049 0.10457865 0.16901745], action=0, reward=1.0, next_state=[1.26459174 1.77084919 0.10795899 0.49277459]\n",
      "[ episode 201 ][ timestamp 81 ] state=[1.26459174 1.77084919 0.10795899 0.49277459], action=1, reward=1.0, next_state=[1.30000872 1.96429604 0.11781449 0.23597198]\n",
      "[ episode 201 ][ timestamp 82 ] state=[1.30000872 1.96429604 0.11781449 0.23597198], action=0, reward=1.0, next_state=[1.33929464 1.76770518 0.12253393 0.56337148]\n",
      "[ episode 201 ][ timestamp 83 ] state=[1.33929464 1.76770518 0.12253393 0.56337148], action=1, reward=1.0, next_state=[1.37464875 1.96091399 0.13380136 0.31166613]\n",
      "[ episode 201 ][ timestamp 84 ] state=[1.37464875 1.96091399 0.13380136 0.31166613], action=1, reward=1.0, next_state=[1.41386703 2.1539012  0.14003468 0.06399305]\n",
      "[ episode 201 ][ timestamp 85 ] state=[1.41386703 2.1539012  0.14003468 0.06399305], action=0, reward=1.0, next_state=[1.45694505 1.95707823 0.14131454 0.39737327]\n",
      "[ episode 201 ][ timestamp 86 ] state=[1.45694505 1.95707823 0.14131454 0.39737327], action=1, reward=1.0, next_state=[1.49608661 2.14994215 0.149262   0.15236951]\n",
      "[ episode 201 ][ timestamp 87 ] state=[1.49608661 2.14994215 0.149262   0.15236951], action=0, reward=1.0, next_state=[1.53908546 1.95303321 0.15230939 0.48816907]\n",
      "[ episode 201 ][ timestamp 88 ] state=[1.53908546 1.95303321 0.15230939 0.48816907], action=1, reward=1.0, next_state=[1.57814612 2.14571518 0.16207278 0.24709807]\n",
      "[ episode 201 ][ timestamp 89 ] state=[1.57814612 2.14571518 0.16207278 0.24709807], action=0, reward=1.0, next_state=[1.62106042 1.94869434 0.16701474 0.58619743]\n",
      "[ episode 201 ][ timestamp 90 ] state=[1.62106042 1.94869434 0.16701474 0.58619743], action=0, reward=1.0, next_state=[1.66003431 1.75167557 0.17873869 0.92648781]\n",
      "[ episode 201 ][ timestamp 91 ] state=[1.66003431 1.75167557 0.17873869 0.92648781], action=1, reward=1.0, next_state=[1.69506782 1.94399249 0.19726844 0.69487804]\n",
      "[ episode 201 ][ timestamp 92 ] state=[1.69506782 1.94399249 0.19726844 0.69487804], action=1, reward=-1.0, next_state=[1.73394767 2.13591105 0.211166   0.47020491]\n",
      "[ Ended! ] Episode 201: Exploration_rate=0.3669578217261671. Score=92.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 202 ] state=[-0.01763633 -0.04762195 -0.04983239 -0.00271119]\n",
      "[ episode 202 ][ timestamp 1 ] state=[-0.01763633 -0.04762195 -0.04983239 -0.00271119], action=1, reward=1.0, next_state=[-0.01858876  0.14817794 -0.04988661 -0.31069109]\n",
      "[ episode 202 ][ timestamp 2 ] state=[-0.01858876  0.14817794 -0.04988661 -0.31069109], action=1, reward=1.0, next_state=[-0.01562521  0.34397383 -0.05610043 -0.61868013]\n",
      "[ episode 202 ][ timestamp 3 ] state=[-0.01562521  0.34397383 -0.05610043 -0.61868013], action=0, reward=1.0, next_state=[-0.00874573  0.14967852 -0.06847403 -0.34418054]\n",
      "[ episode 202 ][ timestamp 4 ] state=[-0.00874573  0.14967852 -0.06847403 -0.34418054], action=1, reward=1.0, next_state=[-0.00575216  0.34570436 -0.07535765 -0.65764589]\n",
      "[ episode 202 ][ timestamp 5 ] state=[-0.00575216  0.34570436 -0.07535765 -0.65764589], action=0, reward=1.0, next_state=[ 0.00116193  0.15170775 -0.08851056 -0.38961101]\n",
      "[ episode 202 ][ timestamp 6 ] state=[ 0.00116193  0.15170775 -0.08851056 -0.38961101], action=0, reward=1.0, next_state=[ 0.00419608 -0.04205376 -0.09630278 -0.12609461]\n",
      "[ episode 202 ][ timestamp 7 ] state=[ 0.00419608 -0.04205376 -0.09630278 -0.12609461], action=1, reward=1.0, next_state=[ 0.00335501  0.15430646 -0.09882468 -0.44753945]\n",
      "[ episode 202 ][ timestamp 8 ] state=[ 0.00335501  0.15430646 -0.09882468 -0.44753945], action=0, reward=1.0, next_state=[ 0.00644114 -0.03928879 -0.10777546 -0.18757064]\n",
      "[ episode 202 ][ timestamp 9 ] state=[ 0.00644114 -0.03928879 -0.10777546 -0.18757064], action=1, reward=1.0, next_state=[ 0.00565536  0.15719683 -0.11152688 -0.51221369]\n",
      "[ episode 202 ][ timestamp 10 ] state=[ 0.00565536  0.15719683 -0.11152688 -0.51221369], action=1, reward=1.0, next_state=[ 0.0087993   0.35369843 -0.12177115 -0.83785585]\n",
      "[ episode 202 ][ timestamp 11 ] state=[ 0.0087993   0.35369843 -0.12177115 -0.83785585], action=0, reward=1.0, next_state=[ 0.01587327  0.16043111 -0.13852827 -0.58581388]\n",
      "[ episode 202 ][ timestamp 12 ] state=[ 0.01587327  0.16043111 -0.13852827 -0.58581388], action=1, reward=1.0, next_state=[ 0.01908189  0.35719368 -0.15024455 -0.91872753]\n",
      "[ episode 202 ][ timestamp 13 ] state=[ 0.01908189  0.35719368 -0.15024455 -0.91872753], action=0, reward=1.0, next_state=[ 0.02622576  0.16438699 -0.1686191  -0.67678149]\n",
      "[ episode 202 ][ timestamp 14 ] state=[ 0.02622576  0.16438699 -0.1686191  -0.67678149], action=1, reward=1.0, next_state=[ 0.0295135   0.36140062 -0.18215473 -1.01745012]\n",
      "[ episode 202 ][ timestamp 15 ] state=[ 0.0295135   0.36140062 -0.18215473 -1.01745012], action=0, reward=1.0, next_state=[ 0.03674152  0.16911238 -0.20250373 -0.78704752]\n",
      "[ episode 202 ][ timestamp 16 ] state=[ 0.03674152  0.16911238 -0.20250373 -0.78704752], action=1, reward=-1.0, next_state=[ 0.04012376  0.36635444 -0.21824468 -1.135995  ]\n",
      "[ Ended! ] Episode 202: Exploration_rate=0.36512303261753626. Score=16.\n",
      "[ Experience replay ] starts\n",
      "[ episode 203 ] state=[-0.00711852 -0.04237722 -0.02606771  0.04374384]\n",
      "[ episode 203 ][ timestamp 1 ] state=[-0.00711852 -0.04237722 -0.02606771  0.04374384], action=0, reward=1.0, next_state=[-0.00796607 -0.23711586 -0.02519283  0.32808952]\n",
      "[ episode 203 ][ timestamp 2 ] state=[-0.00796607 -0.23711586 -0.02519283  0.32808952], action=1, reward=1.0, next_state=[-0.01270839 -0.04164448 -0.01863104  0.02756958]\n",
      "[ episode 203 ][ timestamp 3 ] state=[-0.01270839 -0.04164448 -0.01863104  0.02756958], action=1, reward=1.0, next_state=[-0.01354128  0.15373963 -0.01807965 -0.27093293]\n",
      "[ episode 203 ][ timestamp 4 ] state=[-0.01354128  0.15373963 -0.01807965 -0.27093293], action=0, reward=1.0, next_state=[-0.01046648 -0.04111972 -0.02349831  0.01599321]\n",
      "[ episode 203 ][ timestamp 5 ] state=[-0.01046648 -0.04111972 -0.02349831  0.01599321], action=0, reward=1.0, next_state=[-0.01128888 -0.23589694 -0.02317845  0.3011705 ]\n",
      "[ episode 203 ][ timestamp 6 ] state=[-0.01128888 -0.23589694 -0.02317845  0.3011705 ], action=0, reward=1.0, next_state=[-0.01600682 -0.43068099 -0.01715504  0.58645424]\n",
      "[ episode 203 ][ timestamp 7 ] state=[-0.01600682 -0.43068099 -0.01715504  0.58645424], action=1, reward=1.0, next_state=[-0.02462044 -0.23532302 -0.00542595  0.28841707]\n",
      "[ episode 203 ][ timestamp 8 ] state=[-0.02462044 -0.23532302 -0.00542595  0.28841707], action=0, reward=1.0, next_state=[-2.93268960e-02 -4.30367176e-01  3.42389778e-04  5.79383781e-01]\n",
      "[ episode 203 ][ timestamp 9 ] state=[-2.93268960e-02 -4.30367176e-01  3.42389778e-04  5.79383781e-01], action=1, reward=1.0, next_state=[-0.03793424 -0.23525002  0.01193007  0.28680873]\n",
      "[ episode 203 ][ timestamp 10 ] state=[-0.03793424 -0.23525002  0.01193007  0.28680873], action=1, reward=1.0, next_state=[-0.04263924 -0.04030022  0.01766624 -0.0020878 ]\n",
      "[ episode 203 ][ timestamp 11 ] state=[-0.04263924 -0.04030022  0.01766624 -0.0020878 ], action=0, reward=1.0, next_state=[-0.04344524 -0.23567102  0.01762448  0.29611627]\n",
      "[ episode 203 ][ timestamp 12 ] state=[-0.04344524 -0.23567102  0.01762448  0.29611627], action=0, reward=1.0, next_state=[-0.04815866 -0.43103973  0.02354681  0.59430516]\n",
      "[ episode 203 ][ timestamp 13 ] state=[-0.04815866 -0.43103973  0.02354681  0.59430516], action=1, reward=1.0, next_state=[-0.05677946 -0.23625514  0.03543291  0.30913138]\n",
      "[ episode 203 ][ timestamp 14 ] state=[-0.05677946 -0.23625514  0.03543291  0.30913138], action=1, reward=1.0, next_state=[-0.06150456 -0.04165549  0.04161554  0.02783023]\n",
      "[ episode 203 ][ timestamp 15 ] state=[-0.06150456 -0.04165549  0.04161554  0.02783023], action=0, reward=1.0, next_state=[-0.06233767 -0.23734877  0.04217214  0.33334743]\n",
      "[ episode 203 ][ timestamp 16 ] state=[-0.06233767 -0.23734877  0.04217214  0.33334743], action=1, reward=1.0, next_state=[-0.06708465 -0.04285163  0.04883909  0.05425606]\n",
      "[ episode 203 ][ timestamp 17 ] state=[-0.06708465 -0.04285163  0.04883909  0.05425606], action=1, reward=1.0, next_state=[-0.06794168  0.15153725  0.04992421 -0.22262659]\n",
      "[ episode 203 ][ timestamp 18 ] state=[-0.06794168  0.15153725  0.04992421 -0.22262659], action=0, reward=1.0, next_state=[-0.06491094 -0.04426141  0.04547168  0.08537708]\n",
      "[ episode 203 ][ timestamp 19 ] state=[-0.06491094 -0.04426141  0.04547168  0.08537708], action=0, reward=1.0, next_state=[-0.06579616 -0.24000467  0.04717922  0.39205255]\n",
      "[ episode 203 ][ timestamp 20 ] state=[-0.06579616 -0.24000467  0.04717922  0.39205255], action=1, reward=1.0, next_state=[-0.07059626 -0.04558291  0.05502028  0.11460995]\n",
      "[ episode 203 ][ timestamp 21 ] state=[-0.07059626 -0.04558291  0.05502028  0.11460995], action=0, reward=1.0, next_state=[-0.07150792 -0.24144829  0.05731247  0.42413125]\n",
      "[ episode 203 ][ timestamp 22 ] state=[-0.07150792 -0.24144829  0.05731247  0.42413125], action=1, reward=1.0, next_state=[-0.07633688 -0.04718308  0.0657951   0.15005251]\n",
      "[ episode 203 ][ timestamp 23 ] state=[-0.07633688 -0.04718308  0.0657951   0.15005251], action=0, reward=1.0, next_state=[-0.07728054 -0.24318247  0.06879615  0.46274527]\n",
      "[ episode 203 ][ timestamp 24 ] state=[-0.07728054 -0.24318247  0.06879615  0.46274527], action=1, reward=1.0, next_state=[-0.08214419 -0.04909679  0.07805105  0.19251555]\n",
      "[ episode 203 ][ timestamp 25 ] state=[-0.08214419 -0.04909679  0.07805105  0.19251555], action=1, reward=1.0, next_state=[-0.08312613  0.14482689  0.08190137 -0.07456067]\n",
      "[ episode 203 ][ timestamp 26 ] state=[-0.08312613  0.14482689  0.08190137 -0.07456067], action=0, reward=1.0, next_state=[-0.08022959 -0.05136788  0.08041015  0.24279709]\n",
      "[ episode 203 ][ timestamp 27 ] state=[-0.08022959 -0.05136788  0.08041015  0.24279709], action=1, reward=1.0, next_state=[-0.08125695  0.14251893  0.08526609 -0.02347828]\n",
      "[ episode 203 ][ timestamp 28 ] state=[-0.08125695  0.14251893  0.08526609 -0.02347828], action=0, reward=1.0, next_state=[-0.07840657 -0.05371576  0.08479653  0.29484225]\n",
      "[ episode 203 ][ timestamp 29 ] state=[-0.07840657 -0.05371576  0.08479653  0.29484225], action=1, reward=1.0, next_state=[-0.07948088  0.14010136  0.09069337  0.03006148]\n",
      "[ episode 203 ][ timestamp 30 ] state=[-0.07948088  0.14010136  0.09069337  0.03006148], action=0, reward=1.0, next_state=[-0.07667886 -0.05619625  0.0912946   0.34992509]\n",
      "[ episode 203 ][ timestamp 31 ] state=[-0.07667886 -0.05619625  0.0912946   0.34992509], action=0, reward=1.0, next_state=[-0.07780278 -0.25248997  0.09829311  0.66994282]\n",
      "[ episode 203 ][ timestamp 32 ] state=[-0.07780278 -0.25248997  0.09829311  0.66994282], action=1, reward=1.0, next_state=[-0.08285258 -0.05886209  0.11169196  0.4097546 ]\n",
      "[ episode 203 ][ timestamp 33 ] state=[-0.08285258 -0.05886209  0.11169196  0.4097546 ], action=1, reward=1.0, next_state=[-0.08402982  0.1345138   0.11988705  0.15426738]\n",
      "[ episode 203 ][ timestamp 34 ] state=[-0.08402982  0.1345138   0.11988705  0.15426738], action=1, reward=1.0, next_state=[-0.08133955  0.32773336  0.1229724  -0.0983192 ]\n",
      "[ episode 203 ][ timestamp 35 ] state=[-0.08133955  0.32773336  0.1229724  -0.0983192 ], action=0, reward=1.0, next_state=[-0.07478488  0.13108319  0.12100602  0.23049136]\n",
      "[ episode 203 ][ timestamp 36 ] state=[-0.07478488  0.13108319  0.12100602  0.23049136], action=1, reward=1.0, next_state=[-0.07216322  0.3242869   0.12561584 -0.02170604]\n",
      "[ episode 203 ][ timestamp 37 ] state=[-0.07216322  0.3242869   0.12561584 -0.02170604], action=0, reward=1.0, next_state=[-0.06567748  0.12760827  0.12518172  0.30782138]\n",
      "[ episode 203 ][ timestamp 38 ] state=[-0.06567748  0.12760827  0.12518172  0.30782138], action=0, reward=1.0, next_state=[-0.06312531 -0.06905441  0.13133815  0.63721445]\n",
      "[ episode 203 ][ timestamp 39 ] state=[-0.06312531 -0.06905441  0.13133815  0.63721445], action=1, reward=1.0, next_state=[-0.0645064   0.12401513  0.14408244  0.38860684]\n",
      "[ episode 203 ][ timestamp 40 ] state=[-0.0645064   0.12401513  0.14408244  0.38860684], action=0, reward=1.0, next_state=[-0.0620261  -0.0728266   0.15185458  0.72302377]\n",
      "[ episode 203 ][ timestamp 41 ] state=[-0.0620261  -0.0728266   0.15185458  0.72302377], action=1, reward=1.0, next_state=[-0.06348263  0.11990531  0.16631505  0.48172664]\n",
      "[ episode 203 ][ timestamp 42 ] state=[-0.06348263  0.11990531  0.16631505  0.48172664], action=1, reward=1.0, next_state=[-0.06108452  0.31233739  0.17594958  0.24573295]\n",
      "[ episode 203 ][ timestamp 43 ] state=[-0.06108452  0.31233739  0.17594958  0.24573295], action=0, reward=1.0, next_state=[-0.05483778  0.11519611  0.18086424  0.58834198]\n",
      "[ episode 203 ][ timestamp 44 ] state=[-0.05483778  0.11519611  0.18086424  0.58834198], action=0, reward=1.0, next_state=[-0.05253385 -0.08193603  0.19263108  0.9321016 ]\n",
      "[ episode 203 ][ timestamp 45 ] state=[-0.05253385 -0.08193603  0.19263108  0.9321016 ], action=1, reward=-1.0, next_state=[-0.05417257  0.11013833  0.21127312  0.70560293]\n",
      "[ Ended! ] Episode 203: Exploration_rate=0.3632974174544486. Score=45.\n",
      "[ Experience replay ] starts\n",
      "[ episode 204 ] state=[-0.01320333  0.01765117 -0.03462434 -0.03615609]\n",
      "[ episode 204 ][ timestamp 1 ] state=[-0.01320333  0.01765117 -0.03462434 -0.03615609], action=0, reward=1.0, next_state=[-0.01285031 -0.17695759 -0.03534746  0.24540457]\n",
      "[ episode 204 ][ timestamp 2 ] state=[-0.01285031 -0.17695759 -0.03534746  0.24540457], action=1, reward=1.0, next_state=[-0.01638946  0.01865092 -0.03043937 -0.0582149 ]\n",
      "[ episode 204 ][ timestamp 3 ] state=[-0.01638946  0.01865092 -0.03043937 -0.0582149 ], action=0, reward=1.0, next_state=[-0.01601644 -0.17602166 -0.03160367  0.22471091]\n",
      "[ episode 204 ][ timestamp 4 ] state=[-0.01601644 -0.17602166 -0.03160367  0.22471091], action=1, reward=1.0, next_state=[-0.01953687  0.01953739 -0.02710945 -0.07777112]\n",
      "[ episode 204 ][ timestamp 5 ] state=[-0.01953687  0.01953739 -0.02710945 -0.07777112], action=0, reward=1.0, next_state=[-0.01914613 -0.17518566 -0.02866487  0.20623692]\n",
      "[ episode 204 ][ timestamp 6 ] state=[-0.01914613 -0.17518566 -0.02866487  0.20623692], action=1, reward=1.0, next_state=[-0.02264984  0.02033422 -0.02454013 -0.09534874]\n",
      "[ episode 204 ][ timestamp 7 ] state=[-0.02264984  0.02033422 -0.02454013 -0.09534874], action=1, reward=1.0, next_state=[-0.02224315  0.21579915 -0.02644711 -0.39567192]\n",
      "[ episode 204 ][ timestamp 8 ] state=[-0.02224315  0.21579915 -0.02644711 -0.39567192], action=0, reward=1.0, next_state=[-0.01792717  0.02106223 -0.03436055 -0.11144324]\n",
      "[ episode 204 ][ timestamp 9 ] state=[-0.01792717  0.02106223 -0.03436055 -0.11144324], action=0, reward=1.0, next_state=[-0.01750593 -0.17355094 -0.03658941  0.17020419]\n",
      "[ episode 204 ][ timestamp 10 ] state=[-0.01750593 -0.17355094 -0.03658941  0.17020419], action=0, reward=1.0, next_state=[-0.02097694 -0.36813059 -0.03318533  0.45112343]\n",
      "[ episode 204 ][ timestamp 11 ] state=[-0.02097694 -0.36813059 -0.03318533  0.45112343], action=1, reward=1.0, next_state=[-0.02833956 -0.1725554  -0.02416286  0.14816748]\n",
      "[ episode 204 ][ timestamp 12 ] state=[-0.02833956 -0.1725554  -0.02416286  0.14816748], action=0, reward=1.0, next_state=[-0.03179066 -0.36732316 -0.02119951  0.43313063]\n",
      "[ episode 204 ][ timestamp 13 ] state=[-0.03179066 -0.36732316 -0.02119951  0.43313063], action=0, reward=1.0, next_state=[-0.03913713 -0.56213864 -0.0125369   0.71905601]\n",
      "[ episode 204 ][ timestamp 14 ] state=[-0.03913713 -0.56213864 -0.0125369   0.71905601], action=1, reward=1.0, next_state=[-0.0503799  -0.36684548  0.00184422  0.42245354]\n",
      "[ episode 204 ][ timestamp 15 ] state=[-0.0503799  -0.36684548  0.00184422  0.42245354], action=1, reward=1.0, next_state=[-0.05771681 -0.17174971  0.01029329  0.13035258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 204 ][ timestamp 16 ] state=[-0.05771681 -0.17174971  0.01029329  0.13035258], action=1, reward=1.0, next_state=[-0.0611518   0.02322329  0.01290034 -0.15906526]\n",
      "[ episode 204 ][ timestamp 17 ] state=[-0.0611518   0.02322329  0.01290034 -0.15906526], action=0, reward=1.0, next_state=[-0.06068734 -0.17208095  0.00971904  0.13765933]\n",
      "[ episode 204 ][ timestamp 18 ] state=[-0.06068734 -0.17208095  0.00971904  0.13765933], action=0, reward=1.0, next_state=[-0.06412896 -0.36734075  0.01247223  0.43339255]\n",
      "[ episode 204 ][ timestamp 19 ] state=[-0.06412896 -0.36734075  0.01247223  0.43339255], action=1, reward=1.0, next_state=[-0.07147577 -0.17239758  0.02114008  0.14466728]\n",
      "[ episode 204 ][ timestamp 20 ] state=[-0.07147577 -0.17239758  0.02114008  0.14466728], action=1, reward=1.0, next_state=[-0.07492372  0.02241534  0.02403342 -0.14127209]\n",
      "[ episode 204 ][ timestamp 21 ] state=[-0.07492372  0.02241534  0.02403342 -0.14127209], action=1, reward=1.0, next_state=[-0.07447542  0.21718499  0.02120798 -0.42627705]\n",
      "[ episode 204 ][ timestamp 22 ] state=[-0.07447542  0.21718499  0.02120798 -0.42627705], action=1, reward=1.0, next_state=[-0.07013172  0.41200022  0.01268244 -0.71219951]\n",
      "[ episode 204 ][ timestamp 23 ] state=[-0.07013172  0.41200022  0.01268244 -0.71219951], action=0, reward=1.0, next_state=[-0.06189171  0.21670498 -0.00156155 -0.41555167]\n",
      "[ episode 204 ][ timestamp 24 ] state=[-0.06189171  0.21670498 -0.00156155 -0.41555167], action=0, reward=1.0, next_state=[-0.05755761  0.0216052  -0.00987258 -0.12336144]\n",
      "[ episode 204 ][ timestamp 25 ] state=[-0.05755761  0.0216052  -0.00987258 -0.12336144], action=0, reward=1.0, next_state=[-0.05712551 -0.17337393 -0.01233981  0.16619051]\n",
      "[ episode 204 ][ timestamp 26 ] state=[-0.05712551 -0.17337393 -0.01233981  0.16619051], action=1, reward=1.0, next_state=[-0.06059299  0.02192246 -0.009016   -0.1303596 ]\n",
      "[ episode 204 ][ timestamp 27 ] state=[-0.06059299  0.02192246 -0.009016   -0.1303596 ], action=0, reward=1.0, next_state=[-0.06015454 -0.17306918 -0.01162319  0.15946531]\n",
      "[ episode 204 ][ timestamp 28 ] state=[-0.06015454 -0.17306918 -0.01162319  0.15946531], action=1, reward=1.0, next_state=[-0.06361592  0.02221723 -0.00843389 -0.13686166]\n",
      "[ episode 204 ][ timestamp 29 ] state=[-0.06361592  0.02221723 -0.00843389 -0.13686166], action=0, reward=1.0, next_state=[-0.06317158 -0.17278291 -0.01117112  0.15314862]\n",
      "[ episode 204 ][ timestamp 30 ] state=[-0.06317158 -0.17278291 -0.01117112  0.15314862], action=1, reward=1.0, next_state=[-0.06662724  0.0224972  -0.00810815 -0.14303751]\n",
      "[ episode 204 ][ timestamp 31 ] state=[-0.06662724  0.0224972  -0.00810815 -0.14303751], action=0, reward=1.0, next_state=[-0.06617729 -0.1725077  -0.0109689   0.14707645]\n",
      "[ episode 204 ][ timestamp 32 ] state=[-0.06617729 -0.1725077  -0.0109689   0.14707645], action=1, reward=1.0, next_state=[-0.06962745  0.0227696  -0.00802737 -0.14904667]\n",
      "[ episode 204 ][ timestamp 33 ] state=[-0.06962745  0.0227696  -0.00802737 -0.14904667], action=0, reward=1.0, next_state=[-0.06917205 -0.17223649 -0.0110083   0.14109302]\n",
      "[ episode 204 ][ timestamp 34 ] state=[-0.06917205 -0.17223649 -0.0110083   0.14109302], action=1, reward=1.0, next_state=[-0.07261678  0.02304138 -0.00818644 -0.15504241]\n",
      "[ episode 204 ][ timestamp 35 ] state=[-0.07261678  0.02304138 -0.00818644 -0.15504241], action=0, reward=1.0, next_state=[-0.07215596 -0.17196241 -0.01128729  0.13504668]\n",
      "[ episode 204 ][ timestamp 36 ] state=[-0.07215596 -0.17196241 -0.01128729  0.13504668], action=0, reward=1.0, next_state=[-0.0755952  -0.36692088 -0.00858636  0.42414736]\n",
      "[ episode 204 ][ timestamp 37 ] state=[-0.0755952  -0.36692088 -0.00858636  0.42414736], action=1, reward=1.0, next_state=[-8.29336224e-02 -1.71678352e-01 -1.03410420e-04  1.28770014e-01]\n",
      "[ episode 204 ][ timestamp 38 ] state=[-8.29336224e-02 -1.71678352e-01 -1.03410420e-04  1.28770014e-01], action=1, reward=1.0, next_state=[-0.08636719  0.02344508  0.00247199 -0.16394554]\n",
      "[ episode 204 ][ timestamp 39 ] state=[-0.08636719  0.02344508  0.00247199 -0.16394554], action=0, reward=1.0, next_state=[-0.08589829 -0.17171217 -0.00080692  0.12951621]\n",
      "[ episode 204 ][ timestamp 40 ] state=[-0.08589829 -0.17171217 -0.00080692  0.12951621], action=1, reward=1.0, next_state=[-0.08933253  0.02342133  0.0017834  -0.16342118]\n",
      "[ episode 204 ][ timestamp 41 ] state=[-0.08933253  0.02342133  0.0017834  -0.16342118], action=0, reward=1.0, next_state=[-0.0888641  -0.17172611 -0.00148502  0.12982383]\n",
      "[ episode 204 ][ timestamp 42 ] state=[-0.0888641  -0.17172611 -0.00148502  0.12982383], action=1, reward=1.0, next_state=[-0.09229863  0.02341709  0.00111146 -0.16332723]\n",
      "[ episode 204 ][ timestamp 43 ] state=[-0.09229863  0.02341709  0.00111146 -0.16332723], action=0, reward=1.0, next_state=[-0.09183029 -0.17172076 -0.00215509  0.12970612]\n",
      "[ episode 204 ][ timestamp 44 ] state=[-0.09183029 -0.17172076 -0.00215509  0.12970612], action=1, reward=1.0, next_state=[-0.0952647   0.023432    0.00043903 -0.16365593]\n",
      "[ episode 204 ][ timestamp 45 ] state=[-0.0952647   0.023432    0.00043903 -0.16365593], action=0, reward=1.0, next_state=[-0.09479606 -0.17169623 -0.00283408  0.12916547]\n",
      "[ episode 204 ][ timestamp 46 ] state=[-0.09479606 -0.17169623 -0.00283408  0.12916547], action=0, reward=1.0, next_state=[-9.82299849e-02 -3.66777472e-01 -2.50775308e-04  4.20952929e-01]\n",
      "[ episode 204 ][ timestamp 47 ] state=[-9.82299849e-02 -3.66777472e-01 -2.50775308e-04  4.20952929e-01], action=1, reward=1.0, next_state=[-0.10556553 -0.17165197  0.00816828  0.12819096]\n",
      "[ episode 204 ][ timestamp 48 ] state=[-0.10556553 -0.17165197  0.00816828  0.12819096], action=1, reward=1.0, next_state=[-0.10899857  0.02335202  0.0107321  -0.16190382]\n",
      "[ episode 204 ][ timestamp 49 ] state=[-0.10899857  0.02335202  0.0107321  -0.16190382], action=0, reward=1.0, next_state=[-0.10853153 -0.17192191  0.00749403  0.13414539]\n",
      "[ episode 204 ][ timestamp 50 ] state=[-0.10853153 -0.17192191  0.00749403  0.13414539], action=1, reward=1.0, next_state=[-0.11196997  0.0230919   0.01017693 -0.15616389]\n",
      "[ episode 204 ][ timestamp 51 ] state=[-0.11196997  0.0230919   0.01017693 -0.15616389], action=0, reward=1.0, next_state=[-0.11150813 -0.17217427  0.00705366  0.13971216]\n",
      "[ episode 204 ][ timestamp 52 ] state=[-0.11150813 -0.17217427  0.00705366  0.13971216], action=0, reward=1.0, next_state=[-0.11495162 -0.36739654  0.0098479   0.43461203]\n",
      "[ episode 204 ][ timestamp 53 ] state=[-0.11495162 -0.36739654  0.0098479   0.43461203], action=1, reward=1.0, next_state=[-0.12229955 -0.17241538  0.01854014  0.14504971]\n",
      "[ episode 204 ][ timestamp 54 ] state=[-0.12229955 -0.17241538  0.01854014  0.14504971], action=1, reward=1.0, next_state=[-0.12574786  0.02243622  0.02144113 -0.14172697]\n",
      "[ episode 204 ][ timestamp 55 ] state=[-0.12574786  0.02243622  0.02144113 -0.14172697], action=0, reward=1.0, next_state=[-0.12529913 -0.17298614  0.01860659  0.15764241]\n",
      "[ episode 204 ][ timestamp 56 ] state=[-0.12529913 -0.17298614  0.01860659  0.15764241], action=1, reward=1.0, next_state=[-0.12875886  0.02186454  0.02175944 -0.129113  ]\n",
      "[ episode 204 ][ timestamp 57 ] state=[-0.12875886  0.02186454  0.02175944 -0.129113  ], action=0, reward=1.0, next_state=[-0.12832156 -0.17356225  0.01917718  0.17035456]\n",
      "[ episode 204 ][ timestamp 58 ] state=[-0.12832156 -0.17356225  0.01917718  0.17035456], action=1, reward=1.0, next_state=[-0.13179281  0.02128004  0.02258427 -0.11621739]\n",
      "[ episode 204 ][ timestamp 59 ] state=[-0.13179281  0.02128004  0.02258427 -0.11621739], action=1, reward=1.0, next_state=[-0.13136721  0.21607124  0.02025993 -0.40169046]\n",
      "[ episode 204 ][ timestamp 60 ] state=[-0.13136721  0.21607124  0.02025993 -0.40169046], action=0, reward=1.0, next_state=[-0.12704578  0.02066787  0.01222612 -0.10268954]\n",
      "[ episode 204 ][ timestamp 61 ] state=[-0.12704578  0.02066787  0.01222612 -0.10268954], action=0, reward=1.0, next_state=[-0.12663243 -0.17462714  0.01017233  0.19382547]\n",
      "[ episode 204 ][ timestamp 62 ] state=[-0.12663243 -0.17462714  0.01017233  0.19382547], action=1, reward=1.0, next_state=[-0.13012497  0.02034783  0.01404884 -0.09563125]\n",
      "[ episode 204 ][ timestamp 63 ] state=[-0.13012497  0.02034783  0.01404884 -0.09563125], action=0, reward=1.0, next_state=[-0.12971801 -0.17497263  0.01213621  0.20145075]\n",
      "[ episode 204 ][ timestamp 64 ] state=[-0.12971801 -0.17497263  0.01213621  0.20145075], action=1, reward=1.0, next_state=[-0.13321747  0.01997367  0.01616523 -0.0873792 ]\n",
      "[ episode 204 ][ timestamp 65 ] state=[-0.13321747  0.01997367  0.01616523 -0.0873792 ], action=0, reward=1.0, next_state=[-0.13281799 -0.17537622  0.01441764  0.21035972]\n",
      "[ episode 204 ][ timestamp 66 ] state=[-0.13281799 -0.17537622  0.01441764  0.21035972], action=1, reward=1.0, next_state=[-0.13632552  0.01953665  0.01862484 -0.07774056]\n",
      "[ episode 204 ][ timestamp 67 ] state=[-0.13632552  0.01953665  0.01862484 -0.07774056], action=0, reward=1.0, next_state=[-0.13593478 -0.17584728  0.01707002  0.22075989]\n",
      "[ episode 204 ][ timestamp 68 ] state=[-0.13593478 -0.17584728  0.01707002  0.22075989], action=1, reward=1.0, next_state=[-0.13945173  0.01902656  0.02148522 -0.06648995]\n",
      "[ episode 204 ][ timestamp 69 ] state=[-0.13945173  0.01902656  0.02148522 -0.06648995], action=1, reward=1.0, next_state=[-0.1390712   0.21383399  0.02015542 -0.35231748]\n",
      "[ episode 204 ][ timestamp 70 ] state=[-0.1390712   0.21383399  0.02015542 -0.35231748], action=0, reward=1.0, next_state=[-0.13479452  0.01843131  0.01310907 -0.0533477 ]\n",
      "[ episode 204 ][ timestamp 71 ] state=[-0.13479452  0.01843131  0.01310907 -0.0533477 ], action=0, reward=1.0, next_state=[-0.13442589 -0.17687613  0.01204212  0.24344225]\n",
      "[ episode 204 ][ timestamp 72 ] state=[-0.13442589 -0.17687613  0.01204212  0.24344225], action=1, reward=1.0, next_state=[-0.13796341  0.01807176  0.01691096 -0.04541809]\n",
      "[ episode 204 ][ timestamp 73 ] state=[-0.13796341  0.01807176  0.01691096 -0.04541809], action=0, reward=1.0, next_state=[-0.13760198 -0.17728855  0.0160026   0.25255206]\n",
      "[ episode 204 ][ timestamp 74 ] state=[-0.13760198 -0.17728855  0.0160026   0.25255206], action=1, reward=1.0, next_state=[-0.14114775  0.01760129  0.02105364 -0.0350407 ]\n",
      "[ episode 204 ][ timestamp 75 ] state=[-0.14114775  0.01760129  0.02105364 -0.0350407 ], action=0, reward=1.0, next_state=[-0.14079572 -0.17781615  0.02035283  0.26420981]\n",
      "[ episode 204 ][ timestamp 76 ] state=[-0.14079572 -0.17781615  0.02035283  0.26420981], action=1, reward=1.0, next_state=[-0.14435205  0.01700947  0.02563703 -0.02198478]\n",
      "[ episode 204 ][ timestamp 77 ] state=[-0.14435205  0.01700947  0.02563703 -0.02198478], action=0, reward=1.0, next_state=[-0.14401186 -0.17847058  0.02519733  0.27867541]\n",
      "[ episode 204 ][ timestamp 78 ] state=[-0.14401186 -0.17847058  0.02519733  0.27867541], action=1, reward=1.0, next_state=[-0.14758127  0.01628302  0.03077084 -0.00595503]\n",
      "[ episode 204 ][ timestamp 79 ] state=[-0.14758127  0.01628302  0.03077084 -0.00595503], action=0, reward=1.0, next_state=[-0.14725561 -0.17926641  0.03065174  0.29627545]\n",
      "[ episode 204 ][ timestamp 80 ] state=[-0.14725561 -0.17926641  0.03065174  0.29627545], action=1, reward=1.0, next_state=[-0.15084094  0.01540547  0.03657725  0.01341499]\n",
      "[ episode 204 ][ timestamp 81 ] state=[-0.15084094  0.01540547  0.03657725  0.01341499], action=0, reward=1.0, next_state=[-0.15053283 -0.18022143  0.03684555  0.31741039]\n",
      "[ episode 204 ][ timestamp 82 ] state=[-0.15053283 -0.18022143  0.03684555  0.31741039], action=0, reward=1.0, next_state=[-0.15413726 -0.37584828  0.04319376  0.62148164]\n",
      "[ episode 204 ][ timestamp 83 ] state=[-0.15413726 -0.37584828  0.04319376  0.62148164], action=1, reward=1.0, next_state=[-0.16165422 -0.18135528  0.05562339  0.34270927]\n",
      "[ episode 204 ][ timestamp 84 ] state=[-0.16165422 -0.18135528  0.05562339  0.34270927], action=1, reward=1.0, next_state=[-0.16528133  0.01293302  0.06247757  0.06807238]\n",
      "[ episode 204 ][ timestamp 85 ] state=[-0.16528133  0.01293302  0.06247757  0.06807238], action=1, reward=1.0, next_state=[-0.16502267  0.20710618  0.06383902 -0.20426263]\n",
      "[ episode 204 ][ timestamp 86 ] state=[-0.16502267  0.20710618  0.06383902 -0.20426263], action=0, reward=1.0, next_state=[-0.16088054  0.01113213  0.05975377  0.10785558]\n",
      "[ episode 204 ][ timestamp 87 ] state=[-0.16088054  0.01113213  0.05975377  0.10785558], action=1, reward=1.0, next_state=[-0.1606579   0.20534916  0.06191088 -0.16539288]\n",
      "[ episode 204 ][ timestamp 88 ] state=[-0.1606579   0.20534916  0.06191088 -0.16539288], action=0, reward=1.0, next_state=[-0.15655092  0.00939812  0.05860302  0.14616073]\n",
      "[ episode 204 ][ timestamp 89 ] state=[-0.15655092  0.00939812  0.05860302  0.14616073], action=1, reward=1.0, next_state=[-0.15636296  0.20363401  0.06152624 -0.12747352]\n",
      "[ episode 204 ][ timestamp 90 ] state=[-0.15636296  0.20363401  0.06152624 -0.12747352], action=0, reward=1.0, next_state=[-0.15229028  0.00768708  0.05897677  0.18396803]\n",
      "[ episode 204 ][ timestamp 91 ] state=[-0.15229028  0.00768708  0.05897677  0.18396803], action=1, reward=1.0, next_state=[-0.15213653  0.20191775  0.06265613 -0.08954231]\n",
      "[ episode 204 ][ timestamp 92 ] state=[-0.15213653  0.20191775  0.06265613 -0.08954231], action=0, reward=1.0, next_state=[-0.14809818  0.00595628  0.06086528  0.22223196]\n",
      "[ episode 204 ][ timestamp 93 ] state=[-0.14809818  0.00595628  0.06086528  0.22223196], action=0, reward=1.0, next_state=[-0.14797905 -0.18998044  0.06530992  0.53347616]\n",
      "[ episode 204 ][ timestamp 94 ] state=[-0.14797905 -0.18998044  0.06530992  0.53347616], action=1, reward=1.0, next_state=[-0.15177866  0.00416515  0.07597944  0.2620661 ]\n",
      "[ episode 204 ][ timestamp 95 ] state=[-0.15177866  0.00416515  0.07597944  0.2620661 ], action=1, reward=1.0, next_state=[-0.15169536  0.19812494  0.08122077 -0.00571775]\n",
      "[ episode 204 ][ timestamp 96 ] state=[-0.15169536  0.19812494  0.08122077 -0.00571775], action=1, reward=1.0, next_state=[-0.14773286  0.39199382  0.08110641 -0.27170973]\n",
      "[ episode 204 ][ timestamp 97 ] state=[-0.14773286  0.39199382  0.08110641 -0.27170973], action=0, reward=1.0, next_state=[-0.13989298  0.19581383  0.07567222  0.04541203]\n",
      "[ episode 204 ][ timestamp 98 ] state=[-0.13989298  0.19581383  0.07567222  0.04541203], action=1, reward=1.0, next_state=[-0.13597671  0.38977371  0.07658046 -0.22246877]\n",
      "[ episode 204 ][ timestamp 99 ] state=[-0.13597671  0.38977371  0.07658046 -0.22246877], action=0, reward=1.0, next_state=[-0.12818123  0.19364546  0.07213108  0.09335402]\n",
      "[ episode 204 ][ timestamp 100 ] state=[-0.12818123  0.19364546  0.07213108  0.09335402], action=1, reward=1.0, next_state=[-0.12430832  0.38766341  0.07399816 -0.17572799]\n",
      "[ episode 204 ][ timestamp 101 ] state=[-0.12430832  0.38766341  0.07399816 -0.17572799], action=0, reward=1.0, next_state=[-0.11655506  0.19156474  0.0704836   0.13935066]\n",
      "[ episode 204 ][ timestamp 102 ] state=[-0.11655506  0.19156474  0.0704836   0.13935066], action=1, reward=1.0, next_state=[-0.11272376  0.38561011  0.07327061 -0.13028967]\n",
      "[ episode 204 ][ timestamp 103 ] state=[-0.11272376  0.38561011  0.07327061 -0.13028967], action=0, reward=1.0, next_state=[-0.10501156  0.18951921  0.07066482  0.18457977]\n",
      "[ episode 204 ][ timestamp 104 ] state=[-0.10501156  0.18951921  0.07066482  0.18457977], action=0, reward=1.0, next_state=[-0.10122117 -0.00653894  0.07435642  0.49869122]\n",
      "[ episode 204 ][ timestamp 105 ] state=[-0.10122117 -0.00653894  0.07435642  0.49869122], action=1, reward=1.0, next_state=[-0.10135195  0.18746024  0.08433024  0.23033717]\n",
      "[ episode 204 ][ timestamp 106 ] state=[-0.10135195  0.18746024  0.08433024  0.23033717], action=1, reward=1.0, next_state=[-0.09760275  0.38128224  0.08893698 -0.03459894]\n",
      "[ episode 204 ][ timestamp 107 ] state=[-0.09760275  0.38128224  0.08893698 -0.03459894], action=0, reward=1.0, next_state=[-0.0899771   0.18500492  0.08824501  0.28476644]\n",
      "[ episode 204 ][ timestamp 108 ] state=[-0.0899771   0.18500492  0.08824501  0.28476644], action=0, reward=1.0, next_state=[-0.08627701 -0.01125752  0.09394033  0.60392497]\n",
      "[ episode 204 ][ timestamp 109 ] state=[-0.08627701 -0.01125752  0.09394033  0.60392497], action=0, reward=1.0, next_state=[-0.08650216 -0.20755904  0.10601883  0.92465682]\n",
      "[ episode 204 ][ timestamp 110 ] state=[-0.08650216 -0.20755904  0.10601883  0.92465682], action=1, reward=1.0, next_state=[-0.09065334 -0.0140165   0.12451197  0.66708422]\n",
      "[ episode 204 ][ timestamp 111 ] state=[-0.09065334 -0.0140165   0.12451197  0.66708422], action=1, reward=1.0, next_state=[-0.09093367  0.17917406  0.13785365  0.4160538 ]\n",
      "[ episode 204 ][ timestamp 112 ] state=[-0.09093367  0.17917406  0.13785365  0.4160538 ], action=1, reward=1.0, next_state=[-0.08735019  0.37210058  0.14617473  0.16981013]\n",
      "[ episode 204 ][ timestamp 113 ] state=[-0.08735019  0.37210058  0.14617473  0.16981013], action=1, reward=1.0, next_state=[-0.07990817  0.56486077  0.14957093 -0.07342413]\n",
      "[ episode 204 ][ timestamp 114 ] state=[-0.07990817  0.56486077  0.14957093 -0.07342413], action=1, reward=1.0, next_state=[-0.06861096  0.7575572   0.14810245 -0.31543155]\n",
      "[ episode 204 ][ timestamp 115 ] state=[-0.06861096  0.7575572   0.14810245 -0.31543155], action=0, reward=1.0, next_state=[-0.05345981  0.56067019  0.14179382  0.02004908]\n",
      "[ episode 204 ][ timestamp 116 ] state=[-0.05345981  0.56067019  0.14179382  0.02004908], action=1, reward=1.0, next_state=[-0.04224641  0.75350402  0.1421948  -0.22475095]\n",
      "[ episode 204 ][ timestamp 117 ] state=[-0.04224641  0.75350402  0.1421948  -0.22475095], action=0, reward=1.0, next_state=[-0.02717633  0.55666641  0.13769978  0.10919008]\n",
      "[ episode 204 ][ timestamp 118 ] state=[-0.02717633  0.55666641  0.13769978  0.10919008], action=1, reward=1.0, next_state=[-0.016043    0.74957405  0.13988358 -0.13707647]\n",
      "[ episode 204 ][ timestamp 119 ] state=[-0.016043    0.74957405  0.13988358 -0.13707647], action=1, reward=1.0, next_state=[-0.00105152  0.94244414  0.13714205 -0.38256396]\n",
      "[ episode 204 ][ timestamp 120 ] state=[-0.00105152  0.94244414  0.13714205 -0.38256396], action=0, reward=1.0, next_state=[ 0.01779736  0.74566857  0.12949078 -0.04997847]\n",
      "[ episode 204 ][ timestamp 121 ] state=[ 0.01779736  0.74566857  0.12949078 -0.04997847], action=0, reward=1.0, next_state=[0.03271073 0.54895055 0.12849121 0.2805921 ]\n",
      "[ episode 204 ][ timestamp 122 ] state=[0.03271073 0.54895055 0.12849121 0.2805921 ], action=1, reward=1.0, next_state=[0.04368974 0.74202784 0.13410305 0.03103621]\n",
      "[ episode 204 ][ timestamp 123 ] state=[0.04368974 0.74202784 0.13410305 0.03103621], action=1, reward=1.0, next_state=[ 0.0585303   0.93499725  0.13472377 -0.21651086]\n",
      "[ episode 204 ][ timestamp 124 ] state=[ 0.0585303   0.93499725  0.13472377 -0.21651086], action=0, reward=1.0, next_state=[0.07723025 0.73823235 0.13039355 0.11545109]\n",
      "[ episode 204 ][ timestamp 125 ] state=[0.07723025 0.73823235 0.13039355 0.11545109], action=1, reward=1.0, next_state=[ 0.09199489  0.93126828  0.13270258 -0.13341757]\n",
      "[ episode 204 ][ timestamp 126 ] state=[ 0.09199489  0.93126828  0.13270258 -0.13341757], action=0, reward=1.0, next_state=[0.11062026 0.73451978 0.13003423 0.1980106 ]\n",
      "[ episode 204 ][ timestamp 127 ] state=[0.11062026 0.73451978 0.13003423 0.1980106 ], action=1, reward=1.0, next_state=[ 0.12531065  0.92756526  0.13399444 -0.05099052]\n",
      "[ episode 204 ][ timestamp 128 ] state=[ 0.12531065  0.92756526  0.13399444 -0.05099052], action=1, reward=1.0, next_state=[ 0.14386196  1.12053679  0.13297463 -0.29857658]\n",
      "[ episode 204 ][ timestamp 129 ] state=[ 0.14386196  1.12053679  0.13297463 -0.29857658], action=0, reward=1.0, next_state=[0.1662727  0.92379477 0.1270031  0.03291058]\n",
      "[ episode 204 ][ timestamp 130 ] state=[0.1662727  0.92379477 0.1270031  0.03291058], action=1, reward=1.0, next_state=[ 0.18474859  1.1168883   0.12766131 -0.21715832]\n",
      "[ episode 204 ][ timestamp 131 ] state=[ 0.18474859  1.1168883   0.12766131 -0.21715832], action=0, reward=1.0, next_state=[0.20708636 0.92019447 0.12331814 0.11291206]\n",
      "[ episode 204 ][ timestamp 132 ] state=[0.20708636 0.92019447 0.12331814 0.11291206], action=1, reward=1.0, next_state=[ 0.22549025  1.11335342  0.12557638 -0.13846237]\n",
      "[ episode 204 ][ timestamp 133 ] state=[ 0.22549025  1.11335342  0.12557638 -0.13846237], action=0, reward=1.0, next_state=[0.24775731 0.91667748 0.12280713 0.191051  ]\n",
      "[ episode 204 ][ timestamp 134 ] state=[0.24775731 0.91667748 0.12280713 0.191051  ], action=1, reward=1.0, next_state=[ 0.26609086  1.10984811  0.12662815 -0.0605081 ]\n",
      "[ episode 204 ][ timestamp 135 ] state=[ 0.26609086  1.10984811  0.12662815 -0.0605081 ], action=1, reward=1.0, next_state=[ 0.28828783  1.30294848  0.12541799 -0.31071026]\n",
      "[ episode 204 ][ timestamp 136 ] state=[ 0.28828783  1.30294848  0.12541799 -0.31071026], action=0, reward=1.0, next_state=[0.3143468  1.1062836  0.11920379 0.01874629]\n",
      "[ episode 204 ][ timestamp 137 ] state=[0.3143468  1.1062836  0.11920379 0.01874629], action=1, reward=1.0, next_state=[ 0.33647247  1.29951223  0.11957871 -0.23407685]\n",
      "[ episode 204 ][ timestamp 138 ] state=[ 0.33647247  1.29951223  0.11957871 -0.23407685], action=0, reward=1.0, next_state=[0.36246271 1.10290276 0.11489718 0.0938038 ]\n",
      "[ episode 204 ][ timestamp 139 ] state=[0.36246271 1.10290276 0.11489718 0.0938038 ], action=1, reward=1.0, next_state=[ 0.38452077  1.29620646  0.11677325 -0.16053446]\n",
      "[ episode 204 ][ timestamp 140 ] state=[ 0.38452077  1.29620646  0.11677325 -0.16053446], action=1, reward=1.0, next_state=[ 0.4104449   1.48947986  0.11356256 -0.41421684]\n",
      "[ episode 204 ][ timestamp 141 ] state=[ 0.4104449   1.48947986  0.11356256 -0.41421684], action=0, reward=1.0, next_state=[ 0.44023449  1.29294685  0.10527823 -0.08800054]\n",
      "[ episode 204 ][ timestamp 142 ] state=[ 0.44023449  1.29294685  0.10527823 -0.08800054], action=0, reward=1.0, next_state=[0.46609343 1.09648572 0.10351821 0.23595421]\n",
      "[ episode 204 ][ timestamp 143 ] state=[0.46609343 1.09648572 0.10351821 0.23595421], action=1, reward=1.0, next_state=[ 0.48802315  1.28998808  0.1082373  -0.02236551]\n",
      "[ episode 204 ][ timestamp 144 ] state=[ 0.48802315  1.28998808  0.1082373  -0.02236551], action=0, reward=1.0, next_state=[0.51382291 1.09349378 0.10778999 0.3024108 ]\n",
      "[ episode 204 ][ timestamp 145 ] state=[0.51382291 1.09349378 0.10778999 0.3024108 ], action=1, reward=1.0, next_state=[0.53569278 1.2869276  0.1138382  0.04557295]\n",
      "[ episode 204 ][ timestamp 146 ] state=[0.53569278 1.2869276  0.1138382  0.04557295], action=1, reward=1.0, next_state=[ 0.56143134  1.48024872  0.11474966 -0.20913561]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 204 ][ timestamp 147 ] state=[ 0.56143134  1.48024872  0.11474966 -0.20913561], action=0, reward=1.0, next_state=[0.59103631 1.28368893 0.11056695 0.11742746]\n",
      "[ episode 204 ][ timestamp 148 ] state=[0.59103631 1.28368893 0.11056695 0.11742746], action=1, reward=1.0, next_state=[ 0.61671009  1.47706729  0.1129155  -0.13842834]\n",
      "[ episode 204 ][ timestamp 149 ] state=[ 0.61671009  1.47706729  0.1129155  -0.13842834], action=0, reward=1.0, next_state=[0.64625143 1.28052438 0.11014693 0.18763524]\n",
      "[ episode 204 ][ timestamp 150 ] state=[0.64625143 1.28052438 0.11014693 0.18763524], action=1, reward=1.0, next_state=[ 0.67186192  1.47391221  0.11389964 -0.06837082]\n",
      "[ episode 204 ][ timestamp 151 ] state=[ 0.67186192  1.47391221  0.11389964 -0.06837082], action=0, reward=1.0, next_state=[0.70134017 1.27735715 0.11253222 0.25796551]\n",
      "[ episode 204 ][ timestamp 152 ] state=[0.70134017 1.27735715 0.11253222 0.25796551], action=0, reward=1.0, next_state=[0.72688731 1.08082354 0.11769153 0.58391598]\n",
      "[ episode 204 ][ timestamp 153 ] state=[0.72688731 1.08082354 0.11769153 0.58391598], action=1, reward=1.0, next_state=[0.74850378 1.27411723 0.12936985 0.33050265]\n",
      "[ episode 204 ][ timestamp 154 ] state=[0.74850378 1.27411723 0.12936985 0.33050265], action=1, reward=1.0, next_state=[0.77398612 1.46718309 0.13597991 0.08125266]\n",
      "[ episode 204 ][ timestamp 155 ] state=[0.77398612 1.46718309 0.13597991 0.08125266], action=1, reward=1.0, next_state=[ 0.80332979  1.66012028  0.13760496 -0.16562661]\n",
      "[ episode 204 ][ timestamp 156 ] state=[ 0.80332979  1.66012028  0.13760496 -0.16562661], action=0, reward=1.0, next_state=[0.83653219 1.4633243  0.13429243 0.16710529]\n",
      "[ episode 204 ][ timestamp 157 ] state=[0.83653219 1.4633243  0.13429243 0.16710529], action=1, reward=1.0, next_state=[ 0.86579868  1.65629391  0.13763453 -0.08037955]\n",
      "[ episode 204 ][ timestamp 158 ] state=[ 0.86579868  1.65629391  0.13763453 -0.08037955], action=0, reward=1.0, next_state=[0.89892456 1.45949484 0.13602694 0.25236437]\n",
      "[ episode 204 ][ timestamp 159 ] state=[0.89892456 1.45949484 0.13602694 0.25236437], action=1, reward=1.0, next_state=[0.92811445 1.65243875 0.14107423 0.00549067]\n",
      "[ episode 204 ][ timestamp 160 ] state=[0.92811445 1.65243875 0.14107423 0.00549067], action=1, reward=1.0, next_state=[ 0.96116323  1.84528526  0.14118404 -0.23956697]\n",
      "[ episode 204 ][ timestamp 161 ] state=[ 0.96116323  1.84528526  0.14118404 -0.23956697], action=0, reward=1.0, next_state=[0.99806893 1.64845837 0.1363927  0.09410609]\n",
      "[ episode 204 ][ timestamp 162 ] state=[0.99806893 1.64845837 0.1363927  0.09410609], action=1, reward=1.0, next_state=[ 1.0310381   1.8413886   0.13827482 -0.15262638]\n",
      "[ episode 204 ][ timestamp 163 ] state=[ 1.0310381   1.8413886   0.13827482 -0.15262638], action=1, reward=1.0, next_state=[ 1.06786587  2.03428765  0.1352223  -0.39868982]\n",
      "[ episode 204 ][ timestamp 164 ] state=[ 1.06786587  2.03428765  0.1352223  -0.39868982], action=0, reward=1.0, next_state=[ 1.10855163  1.83753252  0.1272485  -0.06661696]\n",
      "[ episode 204 ][ timestamp 165 ] state=[ 1.10855163  1.83753252  0.1272485  -0.06661696], action=0, reward=1.0, next_state=[1.14530228 1.64083761 0.12591616 0.26335011]\n",
      "[ episode 204 ][ timestamp 166 ] state=[1.14530228 1.64083761 0.12591616 0.26335011], action=1, reward=1.0, next_state=[1.17811903 1.8339584  0.13118316 0.01288393]\n",
      "[ episode 204 ][ timestamp 167 ] state=[1.17811903 1.8339584  0.13118316 0.01288393], action=1, reward=1.0, next_state=[ 1.2147982   2.02697887  0.13144084 -0.23570174]\n",
      "[ episode 204 ][ timestamp 168 ] state=[ 1.2147982   2.02697887  0.13144084 -0.23570174], action=0, reward=1.0, next_state=[1.25533777 1.83024787 0.12672681 0.09538171]\n",
      "[ episode 204 ][ timestamp 169 ] state=[1.25533777 1.83024787 0.12672681 0.09538171], action=0, reward=1.0, next_state=[1.29194273 1.63355888 0.12863444 0.42520732]\n",
      "[ episode 204 ][ timestamp 170 ] state=[1.29194273 1.63355888 0.12863444 0.42520732], action=1, reward=1.0, next_state=[1.32461391 1.82664645 0.13713859 0.17568323]\n",
      "[ episode 204 ][ timestamp 171 ] state=[1.32461391 1.82664645 0.13713859 0.17568323], action=1, reward=1.0, next_state=[ 1.36114684  2.01956649  0.14065225 -0.07078743]\n",
      "[ episode 204 ][ timestamp 172 ] state=[ 1.36114684  2.01956649  0.14065225 -0.07078743], action=0, reward=1.0, next_state=[1.40153817 1.82273761 0.1392365  0.26275585]\n",
      "[ episode 204 ][ timestamp 173 ] state=[1.40153817 1.82273761 0.1392365  0.26275585], action=0, reward=1.0, next_state=[1.43799292 1.62593124 0.14449162 0.59591183]\n",
      "[ episode 204 ][ timestamp 174 ] state=[1.43799292 1.62593124 0.14449162 0.59591183], action=1, reward=1.0, next_state=[1.47051154 1.81876704 0.15640986 0.35200525]\n",
      "[ episode 204 ][ timestamp 175 ] state=[1.47051154 1.81876704 0.15640986 0.35200525], action=1, reward=1.0, next_state=[1.50688688 2.01135913 0.16344996 0.11244083]\n",
      "[ episode 204 ][ timestamp 176 ] state=[1.50688688 2.01135913 0.16344996 0.11244083], action=1, reward=1.0, next_state=[ 1.54711407  2.20380764  0.16569878 -0.12454383]\n",
      "[ episode 204 ][ timestamp 177 ] state=[ 1.54711407  2.20380764  0.16569878 -0.12454383], action=0, reward=1.0, next_state=[1.59119022 2.00674736 0.1632079  0.21549081]\n",
      "[ episode 204 ][ timestamp 178 ] state=[1.59119022 2.00674736 0.1632079  0.21549081], action=1, reward=1.0, next_state=[ 1.63132517  2.19920559  0.16751772 -0.02158981]\n",
      "[ episode 204 ][ timestamp 179 ] state=[ 1.63132517  2.19920559  0.16751772 -0.02158981], action=0, reward=1.0, next_state=[1.67530928 2.00212685 0.16708592 0.31891032]\n",
      "[ episode 204 ][ timestamp 180 ] state=[1.67530928 2.00212685 0.16708592 0.31891032], action=1, reward=1.0, next_state=[1.71535182 2.19452402 0.17346413 0.08322868]\n",
      "[ episode 204 ][ timestamp 181 ] state=[1.71535182 2.19452402 0.17346413 0.08322868], action=1, reward=1.0, next_state=[ 1.7592423   2.38678993  0.1751287  -0.15009904]\n",
      "[ episode 204 ][ timestamp 182 ] state=[ 1.7592423   2.38678993  0.1751287  -0.15009904], action=0, reward=1.0, next_state=[1.8069781  2.18964916 0.17212672 0.19231402]\n",
      "[ episode 204 ][ timestamp 183 ] state=[1.8069781  2.18964916 0.17212672 0.19231402], action=1, reward=1.0, next_state=[ 1.85077108  2.38194444  0.175973   -0.04151077]\n",
      "[ episode 204 ][ timestamp 184 ] state=[ 1.85077108  2.38194444  0.175973   -0.04151077], action=0, reward=1.0, next_state=[1.89840997 2.18479297 0.17514279 0.30111888]\n",
      "[ episode 204 ][ timestamp 185 ] state=[1.89840997 2.18479297 0.17514279 0.30111888], action=0, reward=1.0, next_state=[1.94210583 1.98766364 0.18116516 0.64351839]\n",
      "[ episode 204 ][ timestamp 186 ] state=[1.94210583 1.98766364 0.18116516 0.64351839], action=1, reward=1.0, next_state=[1.9818591  2.17986015 0.19403553 0.41291341]\n",
      "[ episode 204 ][ timestamp 187 ] state=[1.9818591  2.17986015 0.19403553 0.41291341], action=1, reward=1.0, next_state=[2.0254563  2.37177864 0.2022938  0.18712712]\n",
      "[ episode 204 ][ timestamp 188 ] state=[2.0254563  2.37177864 0.2022938  0.18712712], action=1, reward=1.0, next_state=[ 2.07289188  2.56351842  0.20603634 -0.03554814]\n",
      "[ episode 204 ][ timestamp 189 ] state=[ 2.07289188  2.56351842  0.20603634 -0.03554814], action=0, reward=1.0, next_state=[2.12416224 2.36612927 0.20532538 0.31442028]\n",
      "[ episode 204 ][ timestamp 190 ] state=[2.12416224 2.36612927 0.20532538 0.31442028], action=1, reward=-1.0, next_state=[2.17148483 2.55782522 0.21161379 0.09285872]\n",
      "[ Ended! ] Episode 204: Exploration_rate=0.3614809303671764. Score=190.\n",
      "[ Experience replay ] starts\n",
      "[ episode 205 ] state=[-0.03036048 -0.04278719  0.00271697  0.04763562]\n",
      "[ episode 205 ][ timestamp 1 ] state=[-0.03036048 -0.04278719  0.00271697  0.04763562], action=1, reward=1.0, next_state=[-0.03121622  0.1522957   0.00366968 -0.24418884]\n",
      "[ episode 205 ][ timestamp 2 ] state=[-0.03121622  0.1522957   0.00366968 -0.24418884], action=1, reward=1.0, next_state=[-0.02817031  0.34736505 -0.00121409 -0.535712  ]\n",
      "[ episode 205 ][ timestamp 3 ] state=[-0.02817031  0.34736505 -0.00121409 -0.535712  ], action=1, reward=1.0, next_state=[-0.02122301  0.54250405 -0.01192833 -0.82877723]\n",
      "[ episode 205 ][ timestamp 4 ] state=[-0.02122301  0.54250405 -0.01192833 -0.82877723], action=0, reward=1.0, next_state=[-0.01037293  0.34754719 -0.02850388 -0.53986959]\n",
      "[ episode 205 ][ timestamp 5 ] state=[-0.01037293  0.34754719 -0.02850388 -0.53986959], action=0, reward=1.0, next_state=[-0.00342198  0.15283727 -0.03930127 -0.25630235]\n",
      "[ episode 205 ][ timestamp 6 ] state=[-0.00342198  0.15283727 -0.03930127 -0.25630235], action=0, reward=1.0, next_state=[-0.00036524 -0.04170217 -0.04442732  0.02372987]\n",
      "[ episode 205 ][ timestamp 7 ] state=[-0.00036524 -0.04170217 -0.04442732  0.02372987], action=1, reward=1.0, next_state=[-0.00119928  0.15402782 -0.04395272 -0.28263276]\n",
      "[ episode 205 ][ timestamp 8 ] state=[-0.00119928  0.15402782 -0.04395272 -0.28263276], action=1, reward=1.0, next_state=[ 0.00188127  0.34974824 -0.04960537 -0.58884778]\n",
      "[ episode 205 ][ timestamp 9 ] state=[ 0.00188127  0.34974824 -0.04960537 -0.58884778], action=0, reward=1.0, next_state=[ 0.00887624  0.15535473 -0.06138233 -0.31219421]\n",
      "[ episode 205 ][ timestamp 10 ] state=[ 0.00887624  0.15535473 -0.06138233 -0.31219421], action=0, reward=1.0, next_state=[ 0.01198333 -0.03884148 -0.06762621 -0.03948356]\n",
      "[ episode 205 ][ timestamp 11 ] state=[ 0.01198333 -0.03884148 -0.06762621 -0.03948356], action=0, reward=1.0, next_state=[ 0.0112065  -0.23293178 -0.06841588  0.23111946]\n",
      "[ episode 205 ][ timestamp 12 ] state=[ 0.0112065  -0.23293178 -0.06841588  0.23111946], action=1, reward=1.0, next_state=[ 0.00654787 -0.0369023  -0.06379349 -0.08233544]\n",
      "[ episode 205 ][ timestamp 13 ] state=[ 0.00654787 -0.0369023  -0.06379349 -0.08233544], action=1, reward=1.0, next_state=[ 0.00580982  0.15907337 -0.0654402  -0.39444355]\n",
      "[ episode 205 ][ timestamp 14 ] state=[ 0.00580982  0.15907337 -0.0654402  -0.39444355], action=1, reward=1.0, next_state=[ 0.00899129  0.3550599  -0.07332907 -0.70701979]\n",
      "[ episode 205 ][ timestamp 15 ] state=[ 0.00899129  0.3550599  -0.07332907 -0.70701979], action=0, reward=1.0, next_state=[ 0.01609249  0.1610263  -0.08746947 -0.43829099]\n",
      "[ episode 205 ][ timestamp 16 ] state=[ 0.01609249  0.1610263  -0.08746947 -0.43829099], action=1, reward=1.0, next_state=[ 0.01931301  0.35727035 -0.09623529 -0.75721494]\n",
      "[ episode 205 ][ timestamp 17 ] state=[ 0.01931301  0.35727035 -0.09623529 -0.75721494], action=0, reward=1.0, next_state=[ 0.02645842  0.16359714 -0.11137959 -0.49629885]\n",
      "[ episode 205 ][ timestamp 18 ] state=[ 0.02645842  0.16359714 -0.11137959 -0.49629885], action=0, reward=1.0, next_state=[ 0.02973036 -0.02979254 -0.12130557 -0.24068972]\n",
      "[ episode 205 ][ timestamp 19 ] state=[ 0.02973036 -0.02979254 -0.12130557 -0.24068972], action=0, reward=1.0, next_state=[ 0.02913451 -0.22299164 -0.12611936  0.01140292]\n",
      "[ episode 205 ][ timestamp 20 ] state=[ 0.02913451 -0.22299164 -0.12611936  0.01140292], action=1, reward=1.0, next_state=[ 0.02467468 -0.02630779 -0.1258913  -0.31826049]\n",
      "[ episode 205 ][ timestamp 21 ] state=[ 0.02467468 -0.02630779 -0.1258913  -0.31826049], action=0, reward=1.0, next_state=[ 0.02414852 -0.21943292 -0.13225651 -0.06777969]\n",
      "[ episode 205 ][ timestamp 22 ] state=[ 0.02414852 -0.21943292 -0.13225651 -0.06777969], action=1, reward=1.0, next_state=[ 0.01975987 -0.0226872  -0.13361211 -0.39909112]\n",
      "[ episode 205 ][ timestamp 23 ] state=[ 0.01975987 -0.0226872  -0.13361211 -0.39909112], action=0, reward=1.0, next_state=[ 0.01930612 -0.21568577 -0.14159393 -0.15133869]\n",
      "[ episode 205 ][ timestamp 24 ] state=[ 0.01930612 -0.21568577 -0.14159393 -0.15133869], action=1, reward=1.0, next_state=[ 0.01499241 -0.01885017 -0.1446207  -0.48512694]\n",
      "[ episode 205 ][ timestamp 25 ] state=[ 0.01499241 -0.01885017 -0.1446207  -0.48512694], action=1, reward=1.0, next_state=[ 0.0146154   0.17798482 -0.15432324 -0.81966761]\n",
      "[ episode 205 ][ timestamp 26 ] state=[ 0.0146154   0.17798482 -0.15432324 -0.81966761], action=0, reward=1.0, next_state=[ 0.0181751  -0.01472647 -0.17071659 -0.57922716]\n",
      "[ episode 205 ][ timestamp 27 ] state=[ 0.0181751  -0.01472647 -0.17071659 -0.57922716], action=0, reward=1.0, next_state=[ 0.01788057 -0.20709674 -0.18230114 -0.34481365]\n",
      "[ episode 205 ][ timestamp 28 ] state=[ 0.01788057 -0.20709674 -0.18230114 -0.34481365], action=1, reward=1.0, next_state=[ 0.01373863 -0.0099131  -0.18919741 -0.688988  ]\n",
      "[ episode 205 ][ timestamp 29 ] state=[ 0.01373863 -0.0099131  -0.18919741 -0.688988  ], action=0, reward=1.0, next_state=[ 0.01354037 -0.201976   -0.20297717 -0.46132734]\n",
      "[ episode 205 ][ timestamp 30 ] state=[ 0.01354037 -0.201976   -0.20297717 -0.46132734], action=1, reward=-1.0, next_state=[ 0.00950085 -0.00465095 -0.21220372 -0.81050488]\n",
      "[ Ended! ] Episode 205: Exploration_rate=0.3596735257153405. Score=30.\n",
      "[ Experience replay ] starts\n",
      "[ episode 206 ] state=[-0.03141849 -0.0398899   0.02263565 -0.00177845]\n",
      "[ episode 206 ][ timestamp 1 ] state=[-0.03141849 -0.0398899   0.02263565 -0.00177845], action=1, reward=1.0, next_state=[-0.03221629  0.15490023  0.02260008 -0.28723449]\n",
      "[ episode 206 ][ timestamp 2 ] state=[-0.03221629  0.15490023  0.02260008 -0.28723449], action=0, reward=1.0, next_state=[-0.02911828 -0.04053661  0.01685539  0.01248976]\n",
      "[ episode 206 ][ timestamp 3 ] state=[-0.02911828 -0.04053661  0.01685539  0.01248976], action=0, reward=1.0, next_state=[-0.02992902 -0.23589618  0.01710518  0.31044275]\n",
      "[ episode 206 ][ timestamp 4 ] state=[-0.02992902 -0.23589618  0.01710518  0.31044275], action=0, reward=1.0, next_state=[-0.03464694 -0.43125761  0.02331404  0.6084707 ]\n",
      "[ episode 206 ][ timestamp 5 ] state=[-0.03464694 -0.43125761  0.02331404  0.6084707 ], action=0, reward=1.0, next_state=[-0.04327209 -0.6266976   0.03548345  0.90840473]\n",
      "[ episode 206 ][ timestamp 6 ] state=[-0.04327209 -0.6266976   0.03548345  0.90840473], action=0, reward=1.0, next_state=[-0.05580604 -0.82228144  0.05365155  1.21202576]\n",
      "[ episode 206 ][ timestamp 7 ] state=[-0.05580604 -0.82228144  0.05365155  1.21202576], action=0, reward=1.0, next_state=[-0.07225167 -1.01805331  0.07789206  1.521027  ]\n",
      "[ episode 206 ][ timestamp 8 ] state=[-0.07225167 -1.01805331  0.07789206  1.521027  ], action=0, reward=1.0, next_state=[-0.09261274 -1.21402537  0.1083126   1.83697093]\n",
      "[ episode 206 ][ timestamp 9 ] state=[-0.09261274 -1.21402537  0.1083126   1.83697093], action=1, reward=1.0, next_state=[-0.11689325 -1.02025446  0.14505202  1.57979951]\n",
      "[ episode 206 ][ timestamp 10 ] state=[-0.11689325 -1.02025446  0.14505202  1.57979951], action=0, reward=1.0, next_state=[-0.13729833 -1.21677524  0.17664801  1.9139809 ]\n",
      "[ episode 206 ][ timestamp 11 ] state=[-0.13729833 -1.21677524  0.17664801  1.9139809 ], action=0, reward=-1.0, next_state=[-0.16163384 -1.41330607  0.21492763  2.25585444]\n",
      "[ Ended! ] Episode 206: Exploration_rate=0.3578751580867638. Score=11.\n",
      "[ Experience replay ] starts\n",
      "[ episode 207 ] state=[-0.02078018  0.04952517 -0.03391884  0.04052092]\n",
      "[ episode 207 ][ timestamp 1 ] state=[-0.02078018  0.04952517 -0.03391884  0.04052092], action=0, reward=1.0, next_state=[-0.01978968 -0.14509438 -0.03310842  0.32231211]\n",
      "[ episode 207 ][ timestamp 2 ] state=[-0.01978968 -0.14509438 -0.03310842  0.32231211], action=0, reward=1.0, next_state=[-0.02269157 -0.33972961 -0.02666218  0.60437284]\n",
      "[ episode 207 ][ timestamp 3 ] state=[-0.02269157 -0.33972961 -0.02666218  0.60437284], action=0, reward=1.0, next_state=[-0.02948616 -0.53446874 -0.01457472  0.88853997]\n",
      "[ episode 207 ][ timestamp 4 ] state=[-0.02948616 -0.53446874 -0.01457472  0.88853997], action=0, reward=1.0, next_state=[-0.04017553 -0.72938989  0.00319607  1.17660583]\n",
      "[ episode 207 ][ timestamp 5 ] state=[-0.04017553 -0.72938989  0.00319607  1.17660583], action=0, reward=1.0, next_state=[-0.05476333 -0.92455322  0.02672819  1.47028896]\n",
      "[ episode 207 ][ timestamp 6 ] state=[-0.05476333 -0.92455322  0.02672819  1.47028896], action=0, reward=1.0, next_state=[-0.0732544  -1.11999173  0.05613397  1.77119918]\n",
      "[ episode 207 ][ timestamp 7 ] state=[-0.0732544  -1.11999173  0.05613397  1.77119918], action=0, reward=1.0, next_state=[-0.09565423 -1.31570025  0.09155795  2.08079428]\n",
      "[ episode 207 ][ timestamp 8 ] state=[-0.09565423 -1.31570025  0.09155795  2.08079428], action=1, reward=1.0, next_state=[-0.12196823 -1.12161655  0.13317384  1.81776856]\n",
      "[ episode 207 ][ timestamp 9 ] state=[-0.12196823 -1.12161655  0.13317384  1.81776856], action=0, reward=1.0, next_state=[-0.14440057 -1.31794459  0.16952921  2.14869051]\n",
      "[ episode 207 ][ timestamp 10 ] state=[-0.14440057 -1.31794459  0.16952921  2.14869051], action=0, reward=-1.0, next_state=[-0.17075946 -1.51428262  0.21250302  2.48857878]\n",
      "[ Ended! ] Episode 207: Exploration_rate=0.35608578229633. Score=10.\n",
      "[ Experience replay ] starts\n",
      "[ episode 208 ] state=[ 0.00134464 -0.01834486  0.0285467   0.0112791 ]\n",
      "[ episode 208 ][ timestamp 1 ] state=[ 0.00134464 -0.01834486  0.0285467   0.0112791 ], action=1, reward=1.0, next_state=[ 0.00097774  0.17635631  0.02877228 -0.27226208]\n",
      "[ episode 208 ][ timestamp 2 ] state=[ 0.00097774  0.17635631  0.02877228 -0.27226208], action=0, reward=1.0, next_state=[ 0.00450486 -0.01916413  0.02332704  0.02935508]\n",
      "[ episode 208 ][ timestamp 3 ] state=[ 0.00450486 -0.01916413  0.02332704  0.02935508], action=0, reward=1.0, next_state=[ 0.00412158 -0.21461271  0.02391414  0.32930571]\n",
      "[ episode 208 ][ timestamp 4 ] state=[ 0.00412158 -0.21461271  0.02391414  0.32930571], action=0, reward=1.0, next_state=[-1.70672318e-04 -4.10066785e-01  3.05002572e-02  6.29433082e-01]\n",
      "[ episode 208 ][ timestamp 5 ] state=[-1.70672318e-04 -4.10066785e-01  3.05002572e-02  6.29433082e-01], action=1, reward=1.0, next_state=[-0.00837201 -0.21538344  0.04308892  0.34650957]\n",
      "[ episode 208 ][ timestamp 6 ] state=[-0.00837201 -0.21538344  0.04308892  0.34650957], action=0, reward=1.0, next_state=[-0.01267968 -0.41109097  0.05001911  0.65246261]\n",
      "[ episode 208 ][ timestamp 7 ] state=[-0.01267968 -0.41109097  0.05001911  0.65246261], action=1, reward=1.0, next_state=[-0.0209015  -0.21669996  0.06306836  0.37594027]\n",
      "[ episode 208 ][ timestamp 8 ] state=[-0.0209015  -0.21669996  0.06306836  0.37594027], action=0, reward=1.0, next_state=[-0.0252355  -0.41265836  0.07058717  0.68782328]\n",
      "[ episode 208 ][ timestamp 9 ] state=[-0.0252355  -0.41265836  0.07058717  0.68782328], action=0, reward=1.0, next_state=[-0.03348866 -0.60868537  0.08434363  1.00186697]\n",
      "[ episode 208 ][ timestamp 10 ] state=[-0.03348866 -0.60868537  0.08434363  1.00186697], action=1, reward=1.0, next_state=[-0.04566237 -0.4147855   0.10438097  0.73681872]\n",
      "[ episode 208 ][ timestamp 11 ] state=[-0.04566237 -0.4147855   0.10438097  0.73681872], action=0, reward=1.0, next_state=[-0.05395808 -0.61118239  0.11911735  1.06044295]\n",
      "[ episode 208 ][ timestamp 12 ] state=[-0.05395808 -0.61118239  0.11911735  1.06044295], action=1, reward=1.0, next_state=[-0.06618173 -0.41782207  0.14032621  0.80739546]\n",
      "[ episode 208 ][ timestamp 13 ] state=[-0.06618173 -0.41782207  0.14032621  0.80739546], action=1, reward=1.0, next_state=[-0.07453817 -0.22487352  0.15647412  0.56193818]\n",
      "[ episode 208 ][ timestamp 14 ] state=[-0.07453817 -0.22487352  0.15647412  0.56193818], action=1, reward=1.0, next_state=[-0.07903564 -0.03225345  0.16771288  0.32235386]\n",
      "[ episode 208 ][ timestamp 15 ] state=[-0.07903564 -0.03225345  0.16771288  0.32235386], action=0, reward=1.0, next_state=[-0.07968071 -0.22931708  0.17415996  0.66287862]\n",
      "[ episode 208 ][ timestamp 16 ] state=[-0.07968071 -0.22931708  0.17415996  0.66287862], action=0, reward=1.0, next_state=[-0.08426705 -0.42637931  0.18741753  1.00494492]\n",
      "[ episode 208 ][ timestamp 17 ] state=[-0.08426705 -0.42637931  0.18741753  1.00494492], action=1, reward=1.0, next_state=[-0.09279464 -0.23418732  0.20751643  0.77648399]\n",
      "[ episode 208 ][ timestamp 18 ] state=[-0.09279464 -0.23418732  0.20751643  0.77648399], action=1, reward=-1.0, next_state=[-0.09747838 -0.04243115  0.22304611  0.55559364]\n",
      "[ Ended! ] Episode 208: Exploration_rate=0.3543053533848483. Score=18.\n",
      "[ Experience replay ] starts\n",
      "[ episode 209 ] state=[-0.00254428  0.0314629  -0.03206711 -0.04317586]\n",
      "[ episode 209 ][ timestamp 1 ] state=[-0.00254428  0.0314629  -0.03206711 -0.04317586], action=0, reward=1.0, next_state=[-0.00191502 -0.1631849  -0.03293063  0.23921962]\n",
      "[ episode 209 ][ timestamp 2 ] state=[-0.00191502 -0.1631849  -0.03293063  0.23921962], action=0, reward=1.0, next_state=[-0.00517872 -0.35782131 -0.02814623  0.5213361 ]\n",
      "[ episode 209 ][ timestamp 3 ] state=[-0.00517872 -0.35782131 -0.02814623  0.5213361 ], action=1, reward=1.0, next_state=[-0.01233515 -0.16231471 -0.01771951  0.21991845]\n",
      "[ episode 209 ][ timestamp 4 ] state=[-0.01233515 -0.16231471 -0.01771951  0.21991845], action=0, reward=1.0, next_state=[-0.01558144 -0.35717895 -0.01332114  0.50695966]\n",
      "[ episode 209 ][ timestamp 5 ] state=[-0.01558144 -0.35717895 -0.01332114  0.50695966], action=0, reward=1.0, next_state=[-0.02272502 -0.55211069 -0.00318195  0.79541503]\n",
      "[ episode 209 ][ timestamp 6 ] state=[-0.02272502 -0.55211069 -0.00318195  0.79541503], action=1, reward=1.0, next_state=[-0.03376723 -0.35694521  0.01272635  0.5017328 ]\n",
      "[ episode 209 ][ timestamp 7 ] state=[-0.03376723 -0.35694521  0.01272635  0.5017328 ], action=1, reward=1.0, next_state=[-0.04090614 -0.16200494  0.02276101  0.21308752]\n",
      "[ episode 209 ][ timestamp 8 ] state=[-0.04090614 -0.16200494  0.02276101  0.21308752], action=0, reward=1.0, next_state=[-0.04414624 -0.35744479  0.02702276  0.51286252]\n",
      "[ episode 209 ][ timestamp 9 ] state=[-0.04414624 -0.35744479  0.02702276  0.51286252], action=1, reward=1.0, next_state=[-0.05129513 -0.16271367  0.03728001  0.2288162 ]\n",
      "[ episode 209 ][ timestamp 10 ] state=[-0.05129513 -0.16271367  0.03728001  0.2288162 ], action=1, reward=1.0, next_state=[-0.0545494   0.03185626  0.04185633 -0.05187812]\n",
      "[ episode 209 ][ timestamp 11 ] state=[-0.0545494   0.03185626  0.04185633 -0.05187812], action=0, reward=1.0, next_state=[-0.05391228 -0.16384009  0.04081877  0.25371147]\n",
      "[ episode 209 ][ timestamp 12 ] state=[-0.05391228 -0.16384009  0.04081877  0.25371147], action=1, reward=1.0, next_state=[-0.05718908  0.03067597  0.045893   -0.02582219]\n",
      "[ episode 209 ][ timestamp 13 ] state=[-0.05718908  0.03067597  0.045893   -0.02582219], action=1, reward=1.0, next_state=[-0.05657556  0.22511076  0.04537656 -0.30367949]\n",
      "[ episode 209 ][ timestamp 14 ] state=[-0.05657556  0.22511076  0.04537656 -0.30367949], action=0, reward=1.0, next_state=[-0.05207335  0.02937249  0.03930297  0.00296183]\n",
      "[ episode 209 ][ timestamp 15 ] state=[-0.05207335  0.02937249  0.03930297  0.00296183], action=1, reward=1.0, next_state=[-0.0514859   0.22390938  0.0393622  -0.27706606]\n",
      "[ episode 209 ][ timestamp 16 ] state=[-0.0514859   0.22390938  0.0393622  -0.27706606], action=0, reward=1.0, next_state=[-0.04700771  0.02824862  0.03382088  0.02776724]\n",
      "[ episode 209 ][ timestamp 17 ] state=[-0.04700771  0.02824862  0.03382088  0.02776724], action=0, reward=1.0, next_state=[-0.04644274 -0.16734162  0.03437623  0.33092626]\n",
      "[ episode 209 ][ timestamp 18 ] state=[-0.04644274 -0.16734162  0.03437623  0.33092626], action=1, reward=1.0, next_state=[-0.04978957  0.02727457  0.04099475  0.04927907]\n",
      "[ episode 209 ][ timestamp 19 ] state=[-0.04978957  0.02727457  0.04099475  0.04927907], action=0, reward=1.0, next_state=[-0.04924408 -0.1684105   0.04198033  0.35460915]\n",
      "[ episode 209 ][ timestamp 20 ] state=[-0.04924408 -0.1684105   0.04198033  0.35460915], action=1, reward=1.0, next_state=[-0.05261229  0.02609018  0.04907252  0.07545376]\n",
      "[ episode 209 ][ timestamp 21 ] state=[-0.05261229  0.02609018  0.04907252  0.07545376], action=0, reward=1.0, next_state=[-0.05209048 -0.16969967  0.05058159  0.38320653]\n",
      "[ episode 209 ][ timestamp 22 ] state=[-0.05209048 -0.16969967  0.05058159  0.38320653], action=1, reward=1.0, next_state=[-0.05548448  0.02466899  0.05824572  0.10689107]\n",
      "[ episode 209 ][ timestamp 23 ] state=[-0.05548448  0.02466899  0.05824572  0.10689107], action=0, reward=1.0, next_state=[-0.0549911  -0.17123718  0.06038354  0.41736656]\n",
      "[ episode 209 ][ timestamp 24 ] state=[-0.0549911  -0.17123718  0.06038354  0.41736656], action=1, reward=1.0, next_state=[-0.05841584  0.02297939  0.06873087  0.14431463]\n",
      "[ episode 209 ][ timestamp 25 ] state=[-0.05841584  0.02297939  0.06873087  0.14431463], action=1, reward=1.0, next_state=[-0.05795625  0.21705316  0.07161717 -0.12591773]\n",
      "[ episode 209 ][ timestamp 26 ] state=[-0.05795625  0.21705316  0.07161717 -0.12591773], action=0, reward=1.0, next_state=[-0.05361519  0.02098219  0.06909881  0.18847226]\n",
      "[ episode 209 ][ timestamp 27 ] state=[-0.05361519  0.02098219  0.06909881  0.18847226], action=1, reward=1.0, next_state=[-0.05319555  0.21505101  0.07286826 -0.08163741]\n",
      "[ episode 209 ][ timestamp 28 ] state=[-0.05319555  0.21505101  0.07286826 -0.08163741], action=0, reward=1.0, next_state=[-0.04889453  0.01896424  0.07123551  0.23311653]\n",
      "[ episode 209 ][ timestamp 29 ] state=[-0.04889453  0.01896424  0.07123551  0.23311653], action=0, reward=1.0, next_state=[-0.04851524 -0.17709943  0.07589784  0.54739169]\n",
      "[ episode 209 ][ timestamp 30 ] state=[-0.04851524 -0.17709943  0.07589784  0.54739169], action=0, reward=1.0, next_state=[-0.05205723 -0.37320103  0.08684567  0.86298981]\n",
      "[ episode 209 ][ timestamp 31 ] state=[-0.05205723 -0.37320103  0.08684567  0.86298981], action=0, reward=1.0, next_state=[-0.05952125 -0.5693912   0.10410547  1.18166653]\n",
      "[ episode 209 ][ timestamp 32 ] state=[-0.05952125 -0.5693912   0.10410547  1.18166653], action=1, reward=1.0, next_state=[-0.07090907 -0.37576294  0.1277388   0.92334838]\n",
      "[ episode 209 ][ timestamp 33 ] state=[-0.07090907 -0.37576294  0.1277388   0.92334838], action=1, reward=1.0, next_state=[-0.07842433 -0.18257653  0.14620577  0.6733829 ]\n",
      "[ episode 209 ][ timestamp 34 ] state=[-0.07842433 -0.18257653  0.14620577  0.6733829 ], action=1, reward=1.0, next_state=[-0.08207586  0.01024337  0.15967343  0.43007037]\n",
      "[ episode 209 ][ timestamp 35 ] state=[-0.08207586  0.01024337  0.15967343  0.43007037], action=1, reward=1.0, next_state=[-0.081871    0.20278664  0.16827483  0.19167418]\n",
      "[ episode 209 ][ timestamp 36 ] state=[-0.081871    0.20278664  0.16827483  0.19167418], action=1, reward=1.0, next_state=[-0.07781526  0.39515176  0.17210832 -0.04355817]\n",
      "[ episode 209 ][ timestamp 37 ] state=[-0.07781526  0.39515176  0.17210832 -0.04355817], action=1, reward=1.0, next_state=[-0.06991223  0.58744153  0.17123715 -0.27738105]\n",
      "[ episode 209 ][ timestamp 38 ] state=[-0.06991223  0.58744153  0.17123715 -0.27738105], action=1, reward=1.0, next_state=[-0.0581634   0.7797596   0.16568953 -0.51154104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 209 ][ timestamp 39 ] state=[-0.0581634   0.7797596   0.16568953 -0.51154104], action=0, reward=1.0, next_state=[-0.04256821  0.58273893  0.15545871 -0.17156723]\n",
      "[ episode 209 ][ timestamp 40 ] state=[-0.04256821  0.58273893  0.15545871 -0.17156723], action=0, reward=1.0, next_state=[-0.03091343  0.38577318  0.15202737  0.16583946]\n",
      "[ episode 209 ][ timestamp 41 ] state=[-0.03091343  0.38577318  0.15202737  0.16583946], action=1, reward=1.0, next_state=[-0.02319796  0.57842903  0.15534416 -0.07528713]\n",
      "[ episode 209 ][ timestamp 42 ] state=[-0.02319796  0.57842903  0.15534416 -0.07528713], action=0, reward=1.0, next_state=[-0.01162938  0.38146075  0.15383841  0.26209527]\n",
      "[ episode 209 ][ timestamp 43 ] state=[-0.01162938  0.38146075  0.15383841  0.26209527], action=1, reward=1.0, next_state=[-0.00400017  0.5740904   0.15908032  0.02161348]\n",
      "[ episode 209 ][ timestamp 44 ] state=[-0.00400017  0.5740904   0.15908032  0.02161348], action=1, reward=1.0, next_state=[ 0.00748164  0.76661577  0.15951259 -0.21695556]\n",
      "[ episode 209 ][ timestamp 45 ] state=[ 0.00748164  0.76661577  0.15951259 -0.21695556], action=0, reward=1.0, next_state=[0.02281396 0.56961575 0.15517348 0.12149113]\n",
      "[ episode 209 ][ timestamp 46 ] state=[0.02281396 0.56961575 0.15517348 0.12149113], action=1, reward=1.0, next_state=[ 0.03420627  0.76221344  0.1576033  -0.11849611]\n",
      "[ episode 209 ][ timestamp 47 ] state=[ 0.03420627  0.76221344  0.1576033  -0.11849611], action=0, reward=1.0, next_state=[0.04945054 0.56522569 0.15523338 0.21946719]\n",
      "[ episode 209 ][ timestamp 48 ] state=[0.04945054 0.56522569 0.15523338 0.21946719], action=0, reward=1.0, next_state=[0.06075505 0.36826483 0.15962272 0.55681148]\n",
      "[ episode 209 ][ timestamp 49 ] state=[0.06075505 0.36826483 0.15962272 0.55681148], action=1, reward=1.0, next_state=[0.06812035 0.56082835 0.17075895 0.31836824]\n",
      "[ episode 209 ][ timestamp 50 ] state=[0.06812035 0.56082835 0.17075895 0.31836824], action=0, reward=1.0, next_state=[0.07933692 0.36373803 0.17712632 0.65966353]\n",
      "[ episode 209 ][ timestamp 51 ] state=[0.07933692 0.36373803 0.17712632 0.65966353], action=1, reward=1.0, next_state=[0.08661168 0.55601055 0.19031959 0.42757044]\n",
      "[ episode 209 ][ timestamp 52 ] state=[0.08661168 0.55601055 0.19031959 0.42757044], action=1, reward=1.0, next_state=[0.09773189 0.74799949 0.198871   0.20040369]\n",
      "[ episode 209 ][ timestamp 53 ] state=[0.09773189 0.74799949 0.198871   0.20040369], action=1, reward=1.0, next_state=[ 0.11269188  0.93980391  0.20287907 -0.02354888]\n",
      "[ episode 209 ][ timestamp 54 ] state=[ 0.11269188  0.93980391  0.20287907 -0.02354888], action=1, reward=1.0, next_state=[ 0.13148796  1.13152604  0.20240809 -0.24599578]\n",
      "[ episode 209 ][ timestamp 55 ] state=[ 0.13148796  1.13152604  0.20240809 -0.24599578], action=0, reward=1.0, next_state=[0.15411848 0.93417533 0.19748818 0.10308948]\n",
      "[ episode 209 ][ timestamp 56 ] state=[0.15411848 0.93417533 0.19748818 0.10308948], action=1, reward=1.0, next_state=[ 0.17280198  1.12599977  0.19954997 -0.12136946]\n",
      "[ episode 209 ][ timestamp 57 ] state=[ 0.17280198  1.12599977  0.19954997 -0.12136946], action=1, reward=1.0, next_state=[ 0.19532198  1.31778668  0.19712258 -0.34506195]\n",
      "[ episode 209 ][ timestamp 58 ] state=[ 0.19532198  1.31778668  0.19712258 -0.34506195], action=0, reward=1.0, next_state=[0.22167771 1.12048686 0.19022134 0.00273591]\n",
      "[ episode 209 ][ timestamp 59 ] state=[0.22167771 1.12048686 0.19022134 0.00273591], action=1, reward=1.0, next_state=[ 0.24408745  1.31244396  0.19027606 -0.22441766]\n",
      "[ episode 209 ][ timestamp 60 ] state=[ 0.24408745  1.31244396  0.19027606 -0.22441766], action=0, reward=1.0, next_state=[0.27033633 1.11518415 0.1857877  0.12173607]\n",
      "[ episode 209 ][ timestamp 61 ] state=[0.27033633 1.11518415 0.1857877  0.12173607], action=1, reward=1.0, next_state=[ 0.29264001  1.30722572  0.18822242 -0.10706114]\n",
      "[ episode 209 ][ timestamp 62 ] state=[ 0.29264001  1.30722572  0.18822242 -0.10706114], action=1, reward=1.0, next_state=[ 0.31878453  1.49922183  0.1860812  -0.33495764]\n",
      "[ episode 209 ][ timestamp 63 ] state=[ 0.31878453  1.49922183  0.1860812  -0.33495764], action=0, reward=1.0, next_state=[0.34876896 1.30200656 0.17938205 0.01015111]\n",
      "[ episode 209 ][ timestamp 64 ] state=[0.34876896 1.30200656 0.17938205 0.01015111], action=1, reward=1.0, next_state=[ 0.37480909  1.49416317  0.17958507 -0.22100288]\n",
      "[ episode 209 ][ timestamp 65 ] state=[ 0.37480909  1.49416317  0.17958507 -0.22100288], action=1, reward=1.0, next_state=[ 0.40469236  1.68632452  0.17516501 -0.45209471]\n",
      "[ episode 209 ][ timestamp 66 ] state=[ 0.40469236  1.68632452  0.17516501 -0.45209471], action=0, reward=1.0, next_state=[ 0.43841885  1.48921429  0.16612312 -0.10971813]\n",
      "[ episode 209 ][ timestamp 67 ] state=[ 0.43841885  1.48921429  0.16612312 -0.10971813], action=1, reward=1.0, next_state=[ 0.46820313  1.68161438  0.16392876 -0.3457293 ]\n",
      "[ episode 209 ][ timestamp 68 ] state=[ 0.46820313  1.68161438  0.16392876 -0.3457293 ], action=0, reward=1.0, next_state=[ 0.50183542  1.48458633  0.15701417 -0.00616985]\n",
      "[ episode 209 ][ timestamp 69 ] state=[ 0.50183542  1.48458633  0.15701417 -0.00616985], action=1, reward=1.0, next_state=[ 0.53152715  1.67714878  0.15689077 -0.24548762]\n",
      "[ episode 209 ][ timestamp 70 ] state=[ 0.53152715  1.67714878  0.15689077 -0.24548762], action=0, reward=1.0, next_state=[0.56507012 1.48017458 0.15198102 0.09228168]\n",
      "[ episode 209 ][ timestamp 71 ] state=[0.56507012 1.48017458 0.15198102 0.09228168], action=1, reward=1.0, next_state=[ 0.59467361  1.67282846  0.15382665 -0.14885748]\n",
      "[ episode 209 ][ timestamp 72 ] state=[ 0.59467361  1.67282846  0.15382665 -0.14885748], action=1, reward=1.0, next_state=[ 0.62813018  1.86545137  0.1508495  -0.38933322]\n",
      "[ episode 209 ][ timestamp 73 ] state=[ 0.62813018  1.86545137  0.1508495  -0.38933322], action=1, reward=1.0, next_state=[ 0.66543921  2.05814639  0.14306284 -0.63091158]\n",
      "[ episode 209 ][ timestamp 74 ] state=[ 0.66543921  2.05814639  0.14306284 -0.63091158], action=0, reward=1.0, next_state=[ 0.70660214  1.86134868  0.13044461 -0.2968136 ]\n",
      "[ episode 209 ][ timestamp 75 ] state=[ 0.70660214  1.86134868  0.13044461 -0.2968136 ], action=0, reward=1.0, next_state=[0.74382911 1.66463183 0.12450834 0.03399681]\n",
      "[ episode 209 ][ timestamp 76 ] state=[0.74382911 1.66463183 0.12450834 0.03399681], action=1, reward=1.0, next_state=[ 0.77712175  1.85776874  0.12518827 -0.21695496]\n",
      "[ episode 209 ][ timestamp 77 ] state=[ 0.77712175  1.85776874  0.12518827 -0.21695496], action=0, reward=1.0, next_state=[0.81427712 1.66110019 0.12084917 0.11244852]\n",
      "[ episode 209 ][ timestamp 78 ] state=[0.81427712 1.66110019 0.12084917 0.11244852], action=1, reward=1.0, next_state=[ 0.84749913  1.85430184  0.12309814 -0.13979708]\n",
      "[ episode 209 ][ timestamp 79 ] state=[ 0.84749913  1.85430184  0.12309814 -0.13979708], action=0, reward=1.0, next_state=[0.88458516 1.65765154 0.1203022  0.1890458 ]\n",
      "[ episode 209 ][ timestamp 80 ] state=[0.88458516 1.65765154 0.1203022  0.1890458 ], action=1, reward=1.0, next_state=[ 0.9177382   1.85086534  0.12408312 -0.0633966 ]\n",
      "[ episode 209 ][ timestamp 81 ] state=[ 0.9177382   1.85086534  0.12408312 -0.0633966 ], action=1, reward=1.0, next_state=[ 0.9547555   2.04400998  0.12281519 -0.31449919]\n",
      "[ episode 209 ][ timestamp 82 ] state=[ 0.9547555   2.04400998  0.12281519 -0.31449919], action=0, reward=1.0, next_state=[0.9956357  1.84737209 0.1165252  0.0142529 ]\n",
      "[ episode 209 ][ timestamp 83 ] state=[0.9956357  1.84737209 0.1165252  0.0142529 ], action=1, reward=1.0, next_state=[ 1.03258314  2.04064686  0.11681026 -0.23951231]\n",
      "[ episode 209 ][ timestamp 84 ] state=[ 1.03258314  2.04064686  0.11681026 -0.23951231], action=1, reward=1.0, next_state=[ 1.07339608  2.23392321  0.11202001 -0.49318702]\n",
      "[ episode 209 ][ timestamp 85 ] state=[ 1.07339608  2.23392321  0.11202001 -0.49318702], action=0, reward=1.0, next_state=[ 1.11807455  2.03741434  0.10215627 -0.16740615]\n",
      "[ episode 209 ][ timestamp 86 ] state=[ 1.11807455  2.03741434  0.10215627 -0.16740615], action=1, reward=1.0, next_state=[ 1.15882283  2.23093692  0.09880815 -0.42619492]\n",
      "[ episode 209 ][ timestamp 87 ] state=[ 1.15882283  2.23093692  0.09880815 -0.42619492], action=0, reward=1.0, next_state=[ 1.20344157  2.03456442  0.09028425 -0.10407055]\n",
      "[ episode 209 ][ timestamp 88 ] state=[ 1.20344157  2.03456442  0.09028425 -0.10407055], action=0, reward=1.0, next_state=[1.24413286 1.8382724  0.08820284 0.2156758 ]\n",
      "[ episode 209 ][ timestamp 89 ] state=[1.24413286 1.8382724  0.08820284 0.2156758 ], action=1, reward=1.0, next_state=[ 1.28089831  2.03202993  0.09251636 -0.04793266]\n",
      "[ episode 209 ][ timestamp 90 ] state=[ 1.28089831  2.03202993  0.09251636 -0.04793266], action=0, reward=1.0, next_state=[1.32153891 1.83571153 0.0915577  0.27244661]\n",
      "[ episode 209 ][ timestamp 91 ] state=[1.32153891 1.83571153 0.0915577  0.27244661], action=1, reward=1.0, next_state=[1.35825314 2.02941587 0.09700664 0.00998745]\n",
      "[ episode 209 ][ timestamp 92 ] state=[1.35825314 2.02941587 0.09700664 0.00998745], action=1, reward=1.0, next_state=[ 1.39884145  2.22302243  0.09720638 -0.2505818 ]\n",
      "[ episode 209 ][ timestamp 93 ] state=[ 1.39884145  2.22302243  0.09720638 -0.2505818 ], action=0, reward=1.0, next_state=[1.4433019  2.02665645 0.09219475 0.07111035]\n",
      "[ episode 209 ][ timestamp 94 ] state=[1.4433019  2.02665645 0.09219475 0.07111035], action=0, reward=1.0, next_state=[1.48383503 1.83034198 0.09361696 0.39139834]\n",
      "[ episode 209 ][ timestamp 95 ] state=[1.48383503 1.83034198 0.09361696 0.39139834], action=1, reward=1.0, next_state=[1.52044187 2.02401929 0.10144492 0.1296377 ]\n",
      "[ episode 209 ][ timestamp 96 ] state=[1.52044187 2.02401929 0.10144492 0.1296377 ], action=1, reward=1.0, next_state=[ 1.56092226  2.21755276  0.10403768 -0.12939636]\n",
      "[ episode 209 ][ timestamp 97 ] state=[ 1.56092226  2.21755276  0.10403768 -0.12939636], action=0, reward=1.0, next_state=[1.60527331 2.02110622 0.10144975 0.19421209]\n",
      "[ episode 209 ][ timestamp 98 ] state=[1.60527331 2.02110622 0.10144975 0.19421209], action=1, reward=1.0, next_state=[ 1.64569544  2.21464167  0.10533399 -0.06482337]\n",
      "[ episode 209 ][ timestamp 99 ] state=[ 1.64569544  2.21464167  0.10533399 -0.06482337], action=0, reward=1.0, next_state=[1.68998827 2.01817957 0.10403752 0.25914741]\n",
      "[ episode 209 ][ timestamp 100 ] state=[1.68998827 2.01817957 0.10403752 0.25914741], action=1, reward=1.0, next_state=[1.73035186e+00 2.21167432e+00 1.09220472e-01 1.00650603e-03]\n",
      "[ episode 209 ][ timestamp 101 ] state=[1.73035186e+00 2.21167432e+00 1.09220472e-01 1.00650603e-03], action=0, reward=1.0, next_state=[1.77458535 2.01516926 0.1092406  0.32605477]\n",
      "[ episode 209 ][ timestamp 102 ] state=[1.77458535 2.01516926 0.1092406  0.32605477], action=1, reward=1.0, next_state=[1.81488873 2.20858007 0.1157617  0.06972078]\n",
      "[ episode 209 ][ timestamp 103 ] state=[1.81488873 2.20858007 0.1157617  0.06972078], action=1, reward=1.0, next_state=[ 1.85906033  2.40186848  0.11715611 -0.18431338]\n",
      "[ episode 209 ][ timestamp 104 ] state=[ 1.85906033  2.40186848  0.11715611 -0.18431338], action=0, reward=1.0, next_state=[1.9070977  2.20528208 0.11346985 0.14291001]\n",
      "[ episode 209 ][ timestamp 105 ] state=[1.9070977  2.20528208 0.11346985 0.14291001], action=1, reward=1.0, next_state=[ 1.95120334  2.39861154  0.11632805 -0.1119307 ]\n",
      "[ episode 209 ][ timestamp 106 ] state=[ 1.95120334  2.39861154  0.11632805 -0.1119307 ], action=0, reward=1.0, next_state=[1.99917558 2.20203147 0.11408943 0.21506989]\n",
      "[ episode 209 ][ timestamp 107 ] state=[1.99917558 2.20203147 0.11408943 0.21506989], action=0, reward=1.0, next_state=[2.0432162  2.005479   0.11839083 0.54145145]\n",
      "[ episode 209 ][ timestamp 108 ] state=[2.0432162  2.005479   0.11839083 0.54145145], action=0, reward=1.0, next_state=[2.08332578 1.80890942 0.12921986 0.86896749]\n",
      "[ episode 209 ][ timestamp 109 ] state=[2.08332578 1.80890942 0.12921986 0.86896749], action=1, reward=1.0, next_state=[2.11950397 2.00205898 0.14659921 0.61954366]\n",
      "[ episode 209 ][ timestamp 110 ] state=[2.11950397 2.00205898 0.14659921 0.61954366], action=1, reward=1.0, next_state=[2.15954515 2.19486215 0.15899008 0.37638699]\n",
      "[ episode 209 ][ timestamp 111 ] state=[2.15954515 2.19486215 0.15899008 0.37638699], action=1, reward=1.0, next_state=[2.2034424  2.38741092 0.16651782 0.13775298]\n",
      "[ episode 209 ][ timestamp 112 ] state=[2.2034424  2.38741092 0.16651782 0.13775298], action=1, reward=1.0, next_state=[ 2.25119061  2.57980498  0.16927288 -0.09811599]\n",
      "[ episode 209 ][ timestamp 113 ] state=[ 2.25119061  2.57980498  0.16927288 -0.09811599], action=0, reward=1.0, next_state=[2.30278671 2.38271239 0.16731056 0.24282639]\n",
      "[ episode 209 ][ timestamp 114 ] state=[2.30278671 2.38271239 0.16731056 0.24282639], action=1, reward=1.0, next_state=[2.35044096 2.57509858 0.17216709 0.00723691]\n",
      "[ episode 209 ][ timestamp 115 ] state=[2.35044096 2.57509858 0.17216709 0.00723691], action=0, reward=-1.0, next_state=[2.40194293 2.37797933 0.17231183 0.34891184]\n",
      "[ Ended! ] Episode 209: Exploration_rate=0.35253382661792404. Score=115.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 210 ] state=[-0.01491978  0.0400659  -0.04173926  0.00602452]\n",
      "[ episode 210 ][ timestamp 1 ] state=[-0.01491978  0.0400659  -0.04173926  0.00602452], action=0, reward=1.0, next_state=[-0.01411846 -0.15443337 -0.04161877  0.28525154]\n",
      "[ episode 210 ][ timestamp 2 ] state=[-0.01411846 -0.15443337 -0.04161877  0.28525154], action=0, reward=1.0, next_state=[-0.01720713 -0.3489378  -0.03591374  0.56452315]\n",
      "[ episode 210 ][ timestamp 3 ] state=[-0.01720713 -0.3489378  -0.03591374  0.56452315], action=0, reward=1.0, next_state=[-0.02418589 -0.54353795 -0.02462328  0.84567878]\n",
      "[ episode 210 ][ timestamp 4 ] state=[-0.02418589 -0.54353795 -0.02462328  0.84567878], action=1, reward=1.0, next_state=[-0.03505665 -0.34808885 -0.0077097   0.54535549]\n",
      "[ episode 210 ][ timestamp 5 ] state=[-0.03505665 -0.34808885 -0.0077097   0.54535549], action=1, reward=1.0, next_state=[-0.04201842 -0.15285942  0.00319741  0.25025342]\n",
      "[ episode 210 ][ timestamp 6 ] state=[-0.04201842 -0.15285942  0.00319741  0.25025342], action=1, reward=1.0, next_state=[-0.04507561  0.04221672  0.00820248 -0.04141926]\n",
      "[ episode 210 ][ timestamp 7 ] state=[-0.04507561  0.04221672  0.00820248 -0.04141926], action=1, reward=1.0, next_state=[-0.04423128  0.2372201   0.00737409 -0.33150298]\n",
      "[ episode 210 ][ timestamp 8 ] state=[-0.04423128  0.2372201   0.00737409 -0.33150298], action=1, reward=1.0, next_state=[-0.03948688  0.43223631  0.00074403 -0.62185139]\n",
      "[ episode 210 ][ timestamp 9 ] state=[-0.03948688  0.43223631  0.00074403 -0.62185139], action=0, reward=1.0, next_state=[-0.03084215  0.23710398 -0.01169299 -0.32893422]\n",
      "[ episode 210 ][ timestamp 10 ] state=[-0.03084215  0.23710398 -0.01169299 -0.32893422], action=0, reward=1.0, next_state=[-0.02610007  0.04215042 -0.01827168 -0.03996154]\n",
      "[ episode 210 ][ timestamp 11 ] state=[-0.02610007  0.04215042 -0.01827168 -0.03996154], action=1, reward=1.0, next_state=[-0.02525706  0.23752956 -0.01907091 -0.3383529 ]\n",
      "[ episode 210 ][ timestamp 12 ] state=[-0.02525706  0.23752956 -0.01907091 -0.3383529 ], action=1, reward=1.0, next_state=[-0.02050647  0.43291762 -0.02583797 -0.6369882 ]\n",
      "[ episode 210 ][ timestamp 13 ] state=[-0.02050647  0.43291762 -0.02583797 -0.6369882 ], action=1, reward=1.0, next_state=[-0.01184812  0.62839018 -0.03857773 -0.9376947 ]\n",
      "[ episode 210 ][ timestamp 14 ] state=[-0.01184812  0.62839018 -0.03857773 -0.9376947 ], action=1, reward=1.0, next_state=[ 7.19685889e-04  8.24010472e-01 -5.73316252e-02 -1.24224585e+00]\n",
      "[ episode 210 ][ timestamp 15 ] state=[ 7.19685889e-04  8.24010472e-01 -5.73316252e-02 -1.24224585e+00], action=0, reward=1.0, next_state=[ 0.0171999   0.62966936 -0.08217654 -0.96805941]\n",
      "[ episode 210 ][ timestamp 16 ] state=[ 0.0171999   0.62966936 -0.08217654 -0.96805941], action=1, reward=1.0, next_state=[ 0.02979328  0.82579281 -0.10153773 -1.28538454]\n",
      "[ episode 210 ][ timestamp 17 ] state=[ 0.02979328  0.82579281 -0.10153773 -1.28538454], action=0, reward=1.0, next_state=[ 0.04630914  0.6320993  -0.12724542 -1.02614153]\n",
      "[ episode 210 ][ timestamp 18 ] state=[ 0.04630914  0.6320993  -0.12724542 -1.02614153], action=0, reward=1.0, next_state=[ 0.05895112  0.43887991 -0.14776825 -0.77596494]\n",
      "[ episode 210 ][ timestamp 19 ] state=[ 0.05895112  0.43887991 -0.14776825 -0.77596494], action=1, reward=1.0, next_state=[ 0.06772872  0.6356917  -0.16328755 -1.11125132]\n",
      "[ episode 210 ][ timestamp 20 ] state=[ 0.06772872  0.6356917  -0.16328755 -1.11125132], action=0, reward=1.0, next_state=[ 0.08044256  0.44304679 -0.18551258 -0.87392124]\n",
      "[ episode 210 ][ timestamp 21 ] state=[ 0.08044256  0.44304679 -0.18551258 -0.87392124], action=0, reward=1.0, next_state=[ 0.08930349  0.25086572 -0.202991   -0.64482424]\n",
      "[ episode 210 ][ timestamp 22 ] state=[ 0.08930349  0.25086572 -0.202991   -0.64482424], action=0, reward=-1.0, next_state=[ 0.09432081  0.05906425 -0.21588749 -0.42229949]\n",
      "[ Ended! ] Episode 210: Exploration_rate=0.3507711574848344. Score=22.\n",
      "[ Experience replay ] starts\n",
      "[ episode 211 ] state=[ 0.02199773 -0.03788185  0.01362846  0.03747212]\n",
      "[ episode 211 ][ timestamp 1 ] state=[ 0.02199773 -0.03788185  0.01362846  0.03747212], action=1, reward=1.0, next_state=[ 0.02124009  0.15704204  0.0143779  -0.25087992]\n",
      "[ episode 211 ][ timestamp 2 ] state=[ 0.02124009  0.15704204  0.0143779  -0.25087992], action=1, reward=1.0, next_state=[ 0.02438093  0.35195576  0.0093603  -0.53899332]\n",
      "[ episode 211 ][ timestamp 3 ] state=[ 0.02438093  0.35195576  0.0093603  -0.53899332], action=1, reward=1.0, next_state=[ 0.03142004  0.54694488 -0.00141956 -0.8287123 ]\n",
      "[ episode 211 ][ timestamp 4 ] state=[ 0.03142004  0.54694488 -0.00141956 -0.8287123 ], action=1, reward=1.0, next_state=[ 0.04235894  0.74208621 -0.01799381 -1.12184135]\n",
      "[ episode 211 ][ timestamp 5 ] state=[ 0.04235894  0.74208621 -0.01799381 -1.12184135], action=1, reward=1.0, next_state=[ 0.05720067  0.93743944 -0.04043064 -1.42011365]\n",
      "[ episode 211 ][ timestamp 6 ] state=[ 0.05720067  0.93743944 -0.04043064 -1.42011365], action=0, reward=1.0, next_state=[ 0.07594946  0.74284043 -0.06883291 -1.14033705]\n",
      "[ episode 211 ][ timestamp 7 ] state=[ 0.07594946  0.74284043 -0.06883291 -1.14033705], action=0, reward=1.0, next_state=[ 0.09080626  0.54868248 -0.09163965 -0.87001068]\n",
      "[ episode 211 ][ timestamp 8 ] state=[ 0.09080626  0.54868248 -0.09163965 -0.87001068], action=1, reward=1.0, next_state=[ 0.10177991  0.7449235  -0.10903986 -1.19004145]\n",
      "[ episode 211 ][ timestamp 9 ] state=[ 0.10177991  0.7449235  -0.10903986 -1.19004145], action=0, reward=1.0, next_state=[ 0.11667838  0.55137034 -0.13284069 -0.93343019]\n",
      "[ episode 211 ][ timestamp 10 ] state=[ 0.11667838  0.55137034 -0.13284069 -0.93343019], action=1, reward=1.0, next_state=[ 0.12770579  0.74801009 -0.1515093  -1.26473151]\n",
      "[ episode 211 ][ timestamp 11 ] state=[ 0.12770579  0.74801009 -0.1515093  -1.26473151], action=1, reward=1.0, next_state=[ 0.14266599  0.94470841 -0.17680393 -1.60077256]\n",
      "[ episode 211 ][ timestamp 12 ] state=[ 0.14266599  0.94470841 -0.17680393 -1.60077256], action=1, reward=1.0, next_state=[ 0.16156016  1.14142848 -0.20881938 -1.94296258]\n",
      "[ episode 211 ][ timestamp 13 ] state=[ 0.16156016  1.14142848 -0.20881938 -1.94296258], action=1, reward=-1.0, next_state=[ 0.18438873  1.33807703 -0.24767863 -2.2924752 ]\n",
      "[ Ended! ] Episode 211: Exploration_rate=0.34901730169741024. Score=13.\n",
      "[ Experience replay ] starts\n",
      "[ episode 212 ] state=[-0.01494122  0.00728337 -0.03881968  0.04721626]\n",
      "[ episode 212 ][ timestamp 1 ] state=[-0.01494122  0.00728337 -0.03881968  0.04721626], action=0, reward=1.0, next_state=[-0.01479555 -0.18726105 -0.03787535  0.32740292]\n",
      "[ episode 212 ][ timestamp 2 ] state=[-0.01479555 -0.18726105 -0.03787535  0.32740292], action=1, reward=1.0, next_state=[-0.01854077  0.00837908 -0.03132729  0.0230205 ]\n",
      "[ episode 212 ][ timestamp 3 ] state=[-0.01854077  0.00837908 -0.03132729  0.0230205 ], action=0, reward=1.0, next_state=[-0.01837319 -0.18627993 -0.03086688  0.30565702]\n",
      "[ episode 212 ][ timestamp 4 ] state=[-0.01837319 -0.18627993 -0.03086688  0.30565702], action=1, reward=1.0, next_state=[-0.02209879  0.00926798 -0.02475374  0.00340147]\n",
      "[ episode 212 ][ timestamp 5 ] state=[-0.02209879  0.00926798 -0.02475374  0.00340147], action=1, reward=1.0, next_state=[-0.02191343  0.20473603 -0.02468571 -0.29698764]\n",
      "[ episode 212 ][ timestamp 6 ] state=[-0.02191343  0.20473603 -0.02468571 -0.29698764], action=1, reward=1.0, next_state=[-0.01781871  0.40020103 -0.03062547 -0.59735267]\n",
      "[ episode 212 ][ timestamp 7 ] state=[-0.01781871  0.40020103 -0.03062547 -0.59735267], action=1, reward=1.0, next_state=[-0.00981469  0.59573784 -0.04257252 -0.89952284]\n",
      "[ episode 212 ][ timestamp 8 ] state=[-0.00981469  0.59573784 -0.04257252 -0.89952284], action=1, reward=1.0, next_state=[ 0.00210007  0.79141008 -0.06056298 -1.20527779]\n",
      "[ episode 212 ][ timestamp 9 ] state=[ 0.00210007  0.79141008 -0.06056298 -1.20527779], action=0, reward=1.0, next_state=[ 0.01792827  0.59712086 -0.08466853 -0.93217291]\n",
      "[ episode 212 ][ timestamp 10 ] state=[ 0.01792827  0.59712086 -0.08466853 -0.93217291], action=1, reward=1.0, next_state=[ 0.02987069  0.79327695 -0.10331199 -1.25021584]\n",
      "[ episode 212 ][ timestamp 11 ] state=[ 0.02987069  0.79327695 -0.10331199 -1.25021584], action=0, reward=1.0, next_state=[ 0.04573623  0.59961957 -0.12831631 -0.99159835]\n",
      "[ episode 212 ][ timestamp 12 ] state=[ 0.04573623  0.59961957 -0.12831631 -0.99159835], action=0, reward=1.0, next_state=[ 0.05772862  0.40642645 -0.14814828 -0.74181266]\n",
      "[ episode 212 ][ timestamp 13 ] state=[ 0.05772862  0.40642645 -0.14814828 -0.74181266], action=1, reward=1.0, next_state=[ 0.06585715  0.60324909 -0.16298453 -1.07720909]\n",
      "[ episode 212 ][ timestamp 14 ] state=[ 0.06585715  0.60324909 -0.16298453 -1.07720909], action=0, reward=1.0, next_state=[ 0.07792213  0.41061083 -0.18452871 -0.83978673]\n",
      "[ episode 212 ][ timestamp 15 ] state=[ 0.07792213  0.41061083 -0.18452871 -0.83978673], action=0, reward=1.0, next_state=[ 0.08613435  0.21842281 -0.20132445 -0.61034299]\n",
      "[ episode 212 ][ timestamp 16 ] state=[ 0.08613435  0.21842281 -0.20132445 -0.61034299], action=1, reward=-1.0, next_state=[ 0.0905028   0.41570477 -0.21353131 -0.95907941]\n",
      "[ Ended! ] Episode 212: Exploration_rate=0.3472722151889232. Score=16.\n",
      "[ Experience replay ] starts\n",
      "[ episode 213 ] state=[-0.0270265   0.02428721  0.00760582  0.03840293]\n",
      "[ episode 213 ][ timestamp 1 ] state=[-0.0270265   0.02428721  0.00760582  0.03840293], action=1, reward=1.0, next_state=[-0.02654076  0.21929928  0.00837388 -0.25187062]\n",
      "[ episode 213 ][ timestamp 2 ] state=[-0.02654076  0.21929928  0.00837388 -0.25187062], action=1, reward=1.0, next_state=[-0.02215477  0.41430066  0.00333647 -0.54190054]\n",
      "[ episode 213 ][ timestamp 3 ] state=[-0.02215477  0.41430066  0.00333647 -0.54190054], action=0, reward=1.0, next_state=[-0.01386876  0.21913197 -0.00750154 -0.24816822]\n",
      "[ episode 213 ][ timestamp 4 ] state=[-0.01386876  0.21913197 -0.00750154 -0.24816822], action=1, reward=1.0, next_state=[-0.00948612  0.41436025 -0.01246491 -0.54320783]\n",
      "[ episode 213 ][ timestamp 5 ] state=[-0.00948612  0.41436025 -0.01246491 -0.54320783], action=1, reward=1.0, next_state=[-0.00119892  0.60965514 -0.02332906 -0.83979199]\n",
      "[ episode 213 ][ timestamp 6 ] state=[-0.00119892  0.60965514 -0.02332906 -0.83979199], action=0, reward=1.0, next_state=[ 0.01099419  0.41485934 -0.0401249  -0.55453593]\n",
      "[ episode 213 ][ timestamp 7 ] state=[ 0.01099419  0.41485934 -0.0401249  -0.55453593], action=1, reward=1.0, next_state=[ 0.01929137  0.61052105 -0.05121562 -0.85958582]\n",
      "[ episode 213 ][ timestamp 8 ] state=[ 0.01929137  0.61052105 -0.05121562 -0.85958582], action=0, reward=1.0, next_state=[ 0.03150179  0.4161327  -0.06840734 -0.58343643]\n",
      "[ episode 213 ][ timestamp 9 ] state=[ 0.03150179  0.4161327  -0.06840734 -0.58343643], action=0, reward=1.0, next_state=[ 0.03982445  0.22203241 -0.08007607 -0.31306304]\n",
      "[ episode 213 ][ timestamp 10 ] state=[ 0.03982445  0.22203241 -0.08007607 -0.31306304], action=1, reward=1.0, next_state=[ 0.0442651   0.41819837 -0.08633733 -0.62988631]\n",
      "[ episode 213 ][ timestamp 11 ] state=[ 0.0442651   0.41819837 -0.08633733 -0.62988631], action=0, reward=1.0, next_state=[ 0.05262906  0.22438056 -0.09893505 -0.36559412]\n",
      "[ episode 213 ][ timestamp 12 ] state=[ 0.05262906  0.22438056 -0.09893505 -0.36559412], action=0, reward=1.0, next_state=[ 0.05711667  0.03079355 -0.10624693 -0.10567308]\n",
      "[ episode 213 ][ timestamp 13 ] state=[ 0.05711667  0.03079355 -0.10624693 -0.10567308], action=0, reward=1.0, next_state=[ 0.05773255 -0.16265807 -0.1083604   0.15169021]\n",
      "[ episode 213 ][ timestamp 14 ] state=[ 0.05773255 -0.16265807 -0.1083604   0.15169021], action=1, reward=1.0, next_state=[ 0.05447938  0.0338352  -0.10532659 -0.17311662]\n",
      "[ episode 213 ][ timestamp 15 ] state=[ 0.05447938  0.0338352  -0.10532659 -0.17311662], action=0, reward=1.0, next_state=[ 0.05515609 -0.15963397 -0.10878892  0.08457011]\n",
      "[ episode 213 ][ timestamp 16 ] state=[ 0.05515609 -0.15963397 -0.10878892  0.08457011], action=1, reward=1.0, next_state=[ 0.05196341  0.03686564 -0.10709752 -0.24035774]\n",
      "[ episode 213 ][ timestamp 17 ] state=[ 0.05196341  0.03686564 -0.10709752 -0.24035774], action=1, reward=1.0, next_state=[ 0.05270072  0.23334152 -0.11190468 -0.56480952]\n",
      "[ episode 213 ][ timestamp 18 ] state=[ 0.05270072  0.23334152 -0.11190468 -0.56480952], action=0, reward=1.0, next_state=[ 0.05736755  0.03995279 -0.12320087 -0.30937218]\n",
      "[ episode 213 ][ timestamp 19 ] state=[ 0.05736755  0.03995279 -0.12320087 -0.30937218], action=1, reward=1.0, next_state=[ 0.05816661  0.23659503 -0.12938831 -0.63822932]\n",
      "[ episode 213 ][ timestamp 20 ] state=[ 0.05816661  0.23659503 -0.12938831 -0.63822932], action=0, reward=1.0, next_state=[ 0.06289851  0.04349201 -0.1421529  -0.38893012]\n",
      "[ episode 213 ][ timestamp 21 ] state=[ 0.06289851  0.04349201 -0.1421529  -0.38893012], action=1, reward=1.0, next_state=[ 0.06376835  0.24031531 -0.1499315  -0.72283947]\n",
      "[ episode 213 ][ timestamp 22 ] state=[ 0.06376835  0.24031531 -0.1499315  -0.72283947], action=1, reward=1.0, next_state=[ 0.06857466  0.43715793 -0.16438829 -1.05870581]\n",
      "[ episode 213 ][ timestamp 23 ] state=[ 0.06857466  0.43715793 -0.16438829 -1.05870581], action=0, reward=1.0, next_state=[ 0.07731781  0.24454978 -0.18556241 -0.82180129]\n",
      "[ episode 213 ][ timestamp 24 ] state=[ 0.07731781  0.24454978 -0.18556241 -0.82180129], action=0, reward=1.0, next_state=[ 0.08220881  0.05238546 -0.20199843 -0.59274604]\n",
      "[ episode 213 ][ timestamp 25 ] state=[ 0.08220881  0.05238546 -0.20199843 -0.59274604], action=0, reward=-1.0, next_state=[ 0.08325652 -0.13942156 -0.21385335 -0.36986985]\n",
      "[ Ended! ] Episode 213: Exploration_rate=0.3455358541129786. Score=25.\n",
      "[ Experience replay ] starts\n",
      "[ episode 214 ] state=[ 0.04715388 -0.00318172  0.04697186 -0.04852134]\n",
      "[ episode 214 ][ timestamp 1 ] state=[ 0.04715388 -0.00318172  0.04697186 -0.04852134], action=1, reward=1.0, next_state=[ 0.04709024  0.19123632  0.04600144 -0.3260221 ]\n",
      "[ episode 214 ][ timestamp 2 ] state=[ 0.04709024  0.19123632  0.04600144 -0.3260221 ], action=1, reward=1.0, next_state=[ 0.05091497  0.38567416  0.039481   -0.60385066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 214 ][ timestamp 3 ] state=[ 0.05091497  0.38567416  0.039481   -0.60385066], action=1, reward=1.0, next_state=[ 0.05862845  0.58022235  0.02740398 -0.88384113]\n",
      "[ episode 214 ][ timestamp 4 ] state=[ 0.05862845  0.58022235  0.02740398 -0.88384113], action=0, reward=1.0, next_state=[ 0.0702329   0.3847392   0.00972716 -0.58267075]\n",
      "[ episode 214 ][ timestamp 5 ] state=[ 0.0702329   0.3847392   0.00972716 -0.58267075], action=0, reward=1.0, next_state=[ 0.07792768  0.18948233 -0.00192626 -0.28693956]\n",
      "[ episode 214 ][ timestamp 6 ] state=[ 0.07792768  0.18948233 -0.00192626 -0.28693956], action=0, reward=1.0, next_state=[ 0.08171733 -0.0056121  -0.00766505  0.00513522]\n",
      "[ episode 214 ][ timestamp 7 ] state=[ 0.08171733 -0.0056121  -0.00766505  0.00513522], action=0, reward=1.0, next_state=[ 0.08160509 -0.20062329 -0.00756234  0.29538991]\n",
      "[ episode 214 ][ timestamp 8 ] state=[ 0.08160509 -0.20062329 -0.00756234  0.29538991], action=1, reward=1.0, next_state=[ 0.07759262 -0.00539434 -0.00165454  0.00033156]\n",
      "[ episode 214 ][ timestamp 9 ] state=[ 0.07759262 -0.00539434 -0.00165454  0.00033156], action=0, reward=1.0, next_state=[ 0.07748474 -0.20049253 -0.00164791  0.292492  ]\n",
      "[ episode 214 ][ timestamp 10 ] state=[ 0.07748474 -0.20049253 -0.00164791  0.292492  ], action=1, reward=1.0, next_state=[ 0.07347489 -0.00534712  0.00420193 -0.0007102 ]\n",
      "[ episode 214 ][ timestamp 11 ] state=[ 0.07347489 -0.00534712  0.00420193 -0.0007102 ], action=0, reward=1.0, next_state=[ 0.07336794 -0.20052908  0.00418772  0.29329552]\n",
      "[ episode 214 ][ timestamp 12 ] state=[ 0.07336794 -0.20052908  0.00418772  0.29329552], action=1, reward=1.0, next_state=[ 0.06935736 -0.00546709  0.01005363  0.00193628]\n",
      "[ episode 214 ][ timestamp 13 ] state=[ 0.06935736 -0.00546709  0.01005363  0.00193628], action=0, reward=1.0, next_state=[ 0.06924802 -0.20073177  0.01009236  0.29777422]\n",
      "[ episode 214 ][ timestamp 14 ] state=[ 0.06924802 -0.20073177  0.01009236  0.29777422], action=1, reward=1.0, next_state=[ 0.06523338 -0.00575512  0.01604784  0.00829125]\n",
      "[ episode 214 ][ timestamp 15 ] state=[ 0.06523338 -0.00575512  0.01604784  0.00829125], action=0, reward=1.0, next_state=[ 0.06511828 -0.2011035   0.01621367  0.30599396]\n",
      "[ episode 214 ][ timestamp 16 ] state=[ 0.06511828 -0.2011035   0.01621367  0.30599396], action=1, reward=1.0, next_state=[ 0.06109621 -0.00621631  0.02233355  0.01846819]\n",
      "[ episode 214 ][ timestamp 17 ] state=[ 0.06109621 -0.00621631  0.02233355  0.01846819], action=0, reward=1.0, next_state=[ 0.06097189 -0.20165131  0.02270291  0.31811311]\n",
      "[ episode 214 ][ timestamp 18 ] state=[ 0.06097189 -0.20165131  0.02270291  0.31811311], action=0, reward=1.0, next_state=[ 0.05693886 -0.39708913  0.02906517  0.61786837]\n",
      "[ episode 214 ][ timestamp 19 ] state=[ 0.05693886 -0.39708913  0.02906517  0.61786837], action=1, reward=1.0, next_state=[ 0.04899708 -0.20238499  0.04142254  0.33447947]\n",
      "[ episode 214 ][ timestamp 20 ] state=[ 0.04899708 -0.20238499  0.04142254  0.33447947], action=1, reward=1.0, next_state=[ 0.04494938 -0.00787631  0.04811213  0.05514146]\n",
      "[ episode 214 ][ timestamp 21 ] state=[ 0.04494938 -0.00787631  0.04811213  0.05514146], action=0, reward=1.0, next_state=[ 0.04479185 -0.20365391  0.04921496  0.36260757]\n",
      "[ episode 214 ][ timestamp 22 ] state=[ 0.04479185 -0.20365391  0.04921496  0.36260757], action=1, reward=1.0, next_state=[ 0.04071877 -0.00926475  0.05646711  0.08584023]\n",
      "[ episode 214 ][ timestamp 23 ] state=[ 0.04071877 -0.00926475  0.05646711  0.08584023], action=0, reward=1.0, next_state=[ 0.04053348 -0.20514874  0.05818392  0.39579042]\n",
      "[ episode 214 ][ timestamp 24 ] state=[ 0.04053348 -0.20514874  0.05818392  0.39579042], action=1, reward=1.0, next_state=[ 0.0364305  -0.01089853  0.06609972  0.12200459]\n",
      "[ episode 214 ][ timestamp 25 ] state=[ 0.0364305  -0.01089853  0.06609972  0.12200459], action=0, reward=1.0, next_state=[ 0.03621253 -0.20690216  0.06853982  0.43478716]\n",
      "[ episode 214 ][ timestamp 26 ] state=[ 0.03621253 -0.20690216  0.06853982  0.43478716], action=1, reward=1.0, next_state=[ 0.03207449 -0.01281407  0.07723556  0.16447352]\n",
      "[ episode 214 ][ timestamp 27 ] state=[ 0.03207449 -0.01281407  0.07723556  0.16447352], action=1, reward=1.0, next_state=[ 0.03181821  0.18112216  0.08052503 -0.10287891]\n",
      "[ episode 214 ][ timestamp 28 ] state=[ 0.03181821  0.18112216  0.08052503 -0.10287891], action=1, reward=1.0, next_state=[ 0.03544065  0.37500329  0.07846745 -0.36910945]\n",
      "[ episode 214 ][ timestamp 29 ] state=[ 0.03544065  0.37500329  0.07846745 -0.36910945], action=1, reward=1.0, next_state=[ 0.04294072  0.56892775  0.07108526 -0.63605531]\n",
      "[ episode 214 ][ timestamp 30 ] state=[ 0.04294072  0.56892775  0.07108526 -0.63605531], action=1, reward=1.0, next_state=[ 0.05431927  0.76299007  0.05836416 -0.90553216]\n",
      "[ episode 214 ][ timestamp 31 ] state=[ 0.05431927  0.76299007  0.05836416 -0.90553216], action=0, reward=1.0, next_state=[ 0.06957907  0.56712841  0.04025351 -0.5950906 ]\n",
      "[ episode 214 ][ timestamp 32 ] state=[ 0.06957907  0.56712841  0.04025351 -0.5950906 ], action=0, reward=1.0, next_state=[ 0.08092164  0.37146688  0.0283517  -0.29000471]\n",
      "[ episode 214 ][ timestamp 33 ] state=[ 0.08092164  0.37146688  0.0283517  -0.29000471], action=0, reward=1.0, next_state=[0.08835098 0.17595236 0.02255161 0.01148348]\n",
      "[ episode 214 ][ timestamp 34 ] state=[0.08835098 0.17595236 0.02255161 0.01148348], action=0, reward=1.0, next_state=[ 0.09187003 -0.01948563  0.02278128  0.31119553]\n",
      "[ episode 214 ][ timestamp 35 ] state=[ 0.09187003 -0.01948563  0.02278128  0.31119553], action=1, reward=1.0, next_state=[0.09148031 0.17530448 0.02900519 0.02578331]\n",
      "[ episode 214 ][ timestamp 36 ] state=[0.09148031 0.17530448 0.02900519 0.02578331], action=0, reward=1.0, next_state=[ 0.0949864  -0.02022116  0.02952085  0.32747474]\n",
      "[ episode 214 ][ timestamp 37 ] state=[ 0.0949864  -0.02022116  0.02952085  0.32747474], action=1, reward=1.0, next_state=[0.09458198 0.17446834 0.03607035 0.0442456 ]\n",
      "[ episode 214 ][ timestamp 38 ] state=[0.09458198 0.17446834 0.03607035 0.0442456 ], action=1, reward=1.0, next_state=[ 0.09807135  0.36905499  0.03695526 -0.23684213]\n",
      "[ episode 214 ][ timestamp 39 ] state=[ 0.09807135  0.36905499  0.03695526 -0.23684213], action=0, reward=1.0, next_state=[0.10545245 0.17342509 0.03221842 0.06726473]\n",
      "[ episode 214 ][ timestamp 40 ] state=[0.10545245 0.17342509 0.03221842 0.06726473], action=0, reward=1.0, next_state=[ 0.10892095 -0.02214361  0.03356371  0.36993612]\n",
      "[ episode 214 ][ timestamp 41 ] state=[ 0.10892095 -0.02214361  0.03356371  0.36993612], action=1, reward=1.0, next_state=[0.10847808 0.17248579 0.04096243 0.08802232]\n",
      "[ episode 214 ][ timestamp 42 ] state=[0.10847808 0.17248579 0.04096243 0.08802232], action=0, reward=1.0, next_state=[ 0.11192779 -0.02319864  0.04272288  0.39334234]\n",
      "[ episode 214 ][ timestamp 43 ] state=[ 0.11192779 -0.02319864  0.04272288  0.39334234], action=1, reward=1.0, next_state=[0.11146382 0.17129183 0.05058973 0.11442953]\n",
      "[ episode 214 ][ timestamp 44 ] state=[0.11146382 0.17129183 0.05058973 0.11442953], action=0, reward=1.0, next_state=[ 0.11488966 -0.02451713  0.05287832  0.42263424]\n",
      "[ episode 214 ][ timestamp 45 ] state=[ 0.11488966 -0.02451713  0.05287832  0.42263424], action=1, reward=1.0, next_state=[0.11439931 0.16981737 0.061331   0.14707891]\n",
      "[ episode 214 ][ timestamp 46 ] state=[0.11439931 0.16981737 0.061331   0.14707891], action=0, reward=1.0, next_state=[ 0.11779566 -0.0261268   0.06427258  0.45846257]\n",
      "[ episode 214 ][ timestamp 47 ] state=[ 0.11779566 -0.0261268   0.06427258  0.45846257], action=0, reward=1.0, next_state=[ 0.11727312 -0.22209565  0.07344183  0.77069203]\n",
      "[ episode 214 ][ timestamp 48 ] state=[ 0.11727312 -0.22209565  0.07344183  0.77069203], action=1, reward=1.0, next_state=[ 0.11283121 -0.0280571   0.08885567  0.50199129]\n",
      "[ episode 214 ][ timestamp 49 ] state=[ 0.11283121 -0.0280571   0.08885567  0.50199129], action=1, reward=1.0, next_state=[0.11227007 0.1657074  0.0988955  0.23858037]\n",
      "[ episode 214 ][ timestamp 50 ] state=[0.11227007 0.1657074  0.0988955  0.23858037], action=1, reward=1.0, next_state=[ 0.11558422  0.35928767  0.10366711 -0.02134334]\n",
      "[ episode 214 ][ timestamp 51 ] state=[ 0.11558422  0.35928767  0.10366711 -0.02134334], action=1, reward=1.0, next_state=[ 0.12276997  0.55278195  0.10324024 -0.27960299]\n",
      "[ episode 214 ][ timestamp 52 ] state=[ 0.12276997  0.55278195  0.10324024 -0.27960299], action=0, reward=1.0, next_state=[0.13382561 0.35635039 0.09764818 0.04377422]\n",
      "[ episode 214 ][ timestamp 53 ] state=[0.13382561 0.35635039 0.09764818 0.04377422], action=0, reward=1.0, next_state=[0.14095262 0.15997367 0.09852366 0.36559903]\n",
      "[ episode 214 ][ timestamp 54 ] state=[0.14095262 0.15997367 0.09852366 0.36559903], action=1, reward=1.0, next_state=[0.14415209 0.35356754 0.10583564 0.1055356 ]\n",
      "[ episode 214 ][ timestamp 55 ] state=[0.14415209 0.35356754 0.10583564 0.1055356 ], action=1, reward=1.0, next_state=[ 0.15122344  0.54702614  0.10794636 -0.15197098]\n",
      "[ episode 214 ][ timestamp 56 ] state=[ 0.15122344  0.54702614  0.10794636 -0.15197098], action=0, reward=1.0, next_state=[0.16216397 0.35053739 0.10490694 0.17272128]\n",
      "[ episode 214 ][ timestamp 57 ] state=[0.16216397 0.35053739 0.10490694 0.17272128], action=1, reward=1.0, next_state=[ 0.16917471  0.54401365  0.10836136 -0.08511151]\n",
      "[ episode 214 ][ timestamp 58 ] state=[ 0.16917471  0.54401365  0.10836136 -0.08511151], action=0, reward=1.0, next_state=[0.18005499 0.34751871 0.10665913 0.23969807]\n",
      "[ episode 214 ][ timestamp 59 ] state=[0.18005499 0.34751871 0.10665913 0.23969807], action=1, reward=1.0, next_state=[ 0.18700536  0.54096813  0.11145309 -0.01752874]\n",
      "[ episode 214 ][ timestamp 60 ] state=[ 0.18700536  0.54096813  0.11145309 -0.01752874], action=0, reward=1.0, next_state=[0.19782472 0.34443891 0.11110252 0.30813547]\n",
      "[ episode 214 ][ timestamp 61 ] state=[0.19782472 0.34443891 0.11110252 0.30813547], action=1, reward=1.0, next_state=[0.2047135  0.5378169  0.11726523 0.05245389]\n",
      "[ episode 214 ][ timestamp 62 ] state=[0.2047135  0.5378169  0.11726523 0.05245389], action=1, reward=1.0, next_state=[ 0.21546984  0.73107921  0.11831431 -0.20105167]\n",
      "[ episode 214 ][ timestamp 63 ] state=[ 0.21546984  0.73107921  0.11831431 -0.20105167], action=1, reward=1.0, next_state=[ 0.23009142  0.9243278   0.11429327 -0.45419473]\n",
      "[ episode 214 ][ timestamp 64 ] state=[ 0.23009142  0.9243278   0.11429327 -0.45419473], action=0, reward=1.0, next_state=[ 0.24857798  0.72779094  0.10520938 -0.12778376]\n",
      "[ episode 214 ][ timestamp 65 ] state=[ 0.24857798  0.72779094  0.10520938 -0.12778376], action=0, reward=1.0, next_state=[0.2631338  0.53133145 0.1026537  0.19615055]\n",
      "[ episode 214 ][ timestamp 66 ] state=[0.2631338  0.53133145 0.1026537  0.19615055], action=1, reward=1.0, next_state=[ 0.27376043  0.72484667  0.10657671 -0.062467  ]\n",
      "[ episode 214 ][ timestamp 67 ] state=[ 0.27376043  0.72484667  0.10657671 -0.062467  ], action=0, reward=1.0, next_state=[0.28825736 0.52837089 0.10532737 0.26184877]\n",
      "[ episode 214 ][ timestamp 68 ] state=[0.28825736 0.52837089 0.10532737 0.26184877], action=1, reward=1.0, next_state=[0.29882478 0.721844   0.11056435 0.00415641]\n",
      "[ episode 214 ][ timestamp 69 ] state=[0.29882478 0.721844   0.11056435 0.00415641], action=0, reward=1.0, next_state=[0.31326166 0.52532435 0.11064748 0.32957568]\n",
      "[ episode 214 ][ timestamp 70 ] state=[0.31326166 0.52532435 0.11064748 0.32957568], action=1, reward=1.0, next_state=[0.32376814 0.71871154 0.11723899 0.07373281]\n",
      "[ episode 214 ][ timestamp 71 ] state=[0.32376814 0.71871154 0.11723899 0.07373281], action=1, reward=1.0, next_state=[ 0.33814238  0.91197462  0.11871365 -0.17978243]\n",
      "[ episode 214 ][ timestamp 72 ] state=[ 0.33814238  0.91197462  0.11871365 -0.17978243], action=0, reward=1.0, next_state=[0.35638187 0.71537161 0.115118   0.14786638]\n",
      "[ episode 214 ][ timestamp 73 ] state=[0.35638187 0.71537161 0.115118   0.14786638], action=0, reward=1.0, next_state=[0.3706893  0.51880546 0.11807533 0.47453406]\n",
      "[ episode 214 ][ timestamp 74 ] state=[0.3706893  0.51880546 0.11807533 0.47453406], action=1, reward=1.0, next_state=[0.38106541 0.71207936 0.12756601 0.22127534]\n",
      "[ episode 214 ][ timestamp 75 ] state=[0.38106541 0.71207936 0.12756601 0.22127534], action=1, reward=1.0, next_state=[ 0.395307    0.90516902  0.13199152 -0.02860295]\n",
      "[ episode 214 ][ timestamp 76 ] state=[ 0.395307    0.90516902  0.13199152 -0.02860295], action=0, reward=1.0, next_state=[0.41341038 0.70842548 0.13141946 0.3026383 ]\n",
      "[ episode 214 ][ timestamp 77 ] state=[0.41341038 0.70842548 0.13141946 0.3026383 ], action=1, reward=1.0, next_state=[0.42757889 0.90145348 0.13747222 0.05411925]\n",
      "[ episode 214 ][ timestamp 78 ] state=[0.42757889 0.90145348 0.13747222 0.05411925], action=1, reward=1.0, next_state=[ 0.44560796  1.09436393  0.13855461 -0.19222678]\n",
      "[ episode 214 ][ timestamp 79 ] state=[ 0.44560796  1.09436393  0.13855461 -0.19222678], action=0, reward=1.0, next_state=[0.46749524 0.8975599  0.13471007 0.14075506]\n",
      "[ episode 214 ][ timestamp 80 ] state=[0.46749524 0.8975599  0.13471007 0.14075506], action=0, reward=1.0, next_state=[0.48544643 0.70079159 0.13752517 0.47271862]\n",
      "[ episode 214 ][ timestamp 81 ] state=[0.48544643 0.70079159 0.13752517 0.47271862], action=0, reward=1.0, next_state=[0.49946226 0.50402258 0.14697955 0.80539045]\n",
      "[ episode 214 ][ timestamp 82 ] state=[0.49946226 0.50402258 0.14697955 0.80539045], action=0, reward=1.0, next_state=[0.50954272 0.30722448 0.16308735 1.14046134]\n",
      "[ episode 214 ][ timestamp 83 ] state=[0.50954272 0.30722448 0.16308735 1.14046134], action=1, reward=1.0, next_state=[0.51568721 0.49988318 0.18589658 0.90304339]\n",
      "[ episode 214 ][ timestamp 84 ] state=[0.51568721 0.49988318 0.18589658 0.90304339], action=1, reward=1.0, next_state=[0.52568487 0.69206676 0.20395745 0.67407408]\n",
      "[ episode 214 ][ timestamp 85 ] state=[0.52568487 0.69206676 0.20395745 0.67407408], action=1, reward=-1.0, next_state=[0.53952621 0.88385817 0.21743893 0.45189856]\n",
      "[ Ended! ] Episode 214: Exploration_rate=0.3438081748424137. Score=85.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 215 ] state=[ 0.01938205  0.04911825 -0.02405234 -0.03285232]\n",
      "[ episode 215 ][ timestamp 1 ] state=[ 0.01938205  0.04911825 -0.02405234 -0.03285232], action=1, reward=1.0, next_state=[ 0.02036442  0.24457671 -0.02470939 -0.33302592]\n",
      "[ episode 215 ][ timestamp 2 ] state=[ 0.02036442  0.24457671 -0.02470939 -0.33302592], action=1, reward=1.0, next_state=[ 0.02525595  0.44004149 -0.03136991 -0.6333974 ]\n",
      "[ episode 215 ][ timestamp 3 ] state=[ 0.02525595  0.44004149 -0.03136991 -0.6333974 ], action=0, reward=1.0, next_state=[ 0.03405678  0.24537087 -0.04403786 -0.35075638]\n",
      "[ episode 215 ][ timestamp 4 ] state=[ 0.03405678  0.24537087 -0.04403786 -0.35075638], action=0, reward=1.0, next_state=[ 0.0389642   0.05090196 -0.05105298 -0.07227877]\n",
      "[ episode 215 ][ timestamp 5 ] state=[ 0.0389642   0.05090196 -0.05105298 -0.07227877], action=1, reward=1.0, next_state=[ 0.03998224  0.24671724 -0.05249856 -0.38062206]\n",
      "[ episode 215 ][ timestamp 6 ] state=[ 0.03998224  0.24671724 -0.05249856 -0.38062206], action=0, reward=1.0, next_state=[ 0.04491658  0.05237855 -0.060111   -0.10494313]\n",
      "[ episode 215 ][ timestamp 7 ] state=[ 0.04491658  0.05237855 -0.060111   -0.10494313], action=0, reward=1.0, next_state=[ 0.04596415 -0.14183275 -0.06220986  0.16818568]\n",
      "[ episode 215 ][ timestamp 8 ] state=[ 0.04596415 -0.14183275 -0.06220986  0.16818568], action=1, reward=1.0, next_state=[ 0.0431275   0.05412194 -0.05884615 -0.14345569]\n",
      "[ episode 215 ][ timestamp 9 ] state=[ 0.0431275   0.05412194 -0.05884615 -0.14345569], action=1, reward=1.0, next_state=[ 0.04420994  0.25003512 -0.06171526 -0.45410757]\n",
      "[ episode 215 ][ timestamp 10 ] state=[ 0.04420994  0.25003512 -0.06171526 -0.45410757], action=1, reward=1.0, next_state=[ 0.04921064  0.44597297 -0.07079742 -0.76558758]\n",
      "[ episode 215 ][ timestamp 11 ] state=[ 0.04921064  0.44597297 -0.07079742 -0.76558758], action=0, reward=1.0, next_state=[ 0.0581301   0.25189358 -0.08610917 -0.49599483]\n",
      "[ episode 215 ][ timestamp 12 ] state=[ 0.0581301   0.25189358 -0.08610917 -0.49599483], action=0, reward=1.0, next_state=[ 0.06316797  0.05808471 -0.09602906 -0.23164348]\n",
      "[ episode 215 ][ timestamp 13 ] state=[ 0.06316797  0.05808471 -0.09602906 -0.23164348], action=1, reward=1.0, next_state=[ 0.06432967  0.25443831 -0.10066193 -0.55300607]\n",
      "[ episode 215 ][ timestamp 14 ] state=[ 0.06432967  0.25443831 -0.10066193 -0.55300607], action=0, reward=1.0, next_state=[ 0.06941843  0.06086332 -0.11172205 -0.29365809]\n",
      "[ episode 215 ][ timestamp 15 ] state=[ 0.06941843  0.06086332 -0.11172205 -0.29365809], action=0, reward=1.0, next_state=[ 0.0706357  -0.13250318 -0.11759522 -0.03819462]\n",
      "[ episode 215 ][ timestamp 16 ] state=[ 0.0706357  -0.13250318 -0.11759522 -0.03819462], action=1, reward=1.0, next_state=[ 0.06798564  0.06409157 -0.11835911 -0.3655435 ]\n",
      "[ episode 215 ][ timestamp 17 ] state=[ 0.06798564  0.06409157 -0.11835911 -0.3655435 ], action=1, reward=1.0, next_state=[ 0.06926747  0.26067919 -0.12566998 -0.69307825]\n",
      "[ episode 215 ][ timestamp 18 ] state=[ 0.06926747  0.26067919 -0.12566998 -0.69307825], action=0, reward=1.0, next_state=[ 0.07448105  0.06750394 -0.13953154 -0.44245026]\n",
      "[ episode 215 ][ timestamp 19 ] state=[ 0.07448105  0.06750394 -0.13953154 -0.44245026], action=1, reward=1.0, next_state=[ 0.07583113  0.26429601 -0.14838055 -0.77565881]\n",
      "[ episode 215 ][ timestamp 20 ] state=[ 0.07583113  0.26429601 -0.14838055 -0.77565881], action=1, reward=1.0, next_state=[ 0.08111705  0.46111333 -0.16389373 -1.11110477]\n",
      "[ episode 215 ][ timestamp 21 ] state=[ 0.08111705  0.46111333 -0.16389373 -1.11110477], action=1, reward=1.0, next_state=[ 0.09033932  0.65796393 -0.18611582 -1.45039313]\n",
      "[ episode 215 ][ timestamp 22 ] state=[ 0.09033932  0.65796393 -0.18611582 -1.45039313], action=0, reward=-1.0, next_state=[ 0.10349859  0.46555224 -0.21512368 -1.22116262]\n",
      "[ Ended! ] Episode 215: Exploration_rate=0.3420891339682016. Score=22.\n",
      "[ Experience replay ] starts\n",
      "[ episode 216 ] state=[ 0.03232636  0.01450196 -0.01931103 -0.04486163]\n",
      "[ episode 216 ][ timestamp 1 ] state=[ 0.03232636  0.01450196 -0.01931103 -0.04486163], action=0, reward=1.0, next_state=[ 0.0326164  -0.18033783 -0.02020827  0.24166648]\n",
      "[ episode 216 ][ timestamp 2 ] state=[ 0.0326164  -0.18033783 -0.02020827  0.24166648], action=1, reward=1.0, next_state=[ 0.02900964  0.01506687 -0.01537494 -0.05732155]\n",
      "[ episode 216 ][ timestamp 3 ] state=[ 0.02900964  0.01506687 -0.01537494 -0.05732155], action=0, reward=1.0, next_state=[ 0.02931098 -0.1798313  -0.01652137  0.23047109]\n",
      "[ episode 216 ][ timestamp 4 ] state=[ 0.02931098 -0.1798313  -0.01652137  0.23047109], action=1, reward=1.0, next_state=[ 0.02571435  0.0155228  -0.01191195 -0.06737711]\n",
      "[ episode 216 ][ timestamp 5 ] state=[ 0.02571435  0.0155228  -0.01191195 -0.06737711], action=0, reward=1.0, next_state=[ 0.02602481 -0.17942637 -0.01325949  0.22152385]\n",
      "[ episode 216 ][ timestamp 6 ] state=[ 0.02602481 -0.17942637 -0.01325949  0.22152385], action=1, reward=1.0, next_state=[ 0.02243628  0.01588258 -0.00882901 -0.07531198]\n",
      "[ episode 216 ][ timestamp 7 ] state=[ 0.02243628  0.01588258 -0.00882901 -0.07531198], action=0, reward=1.0, next_state=[ 0.02275393 -0.1791117  -0.01033525  0.21457233]\n",
      "[ episode 216 ][ timestamp 8 ] state=[ 0.02275393 -0.1791117  -0.01033525  0.21457233], action=0, reward=1.0, next_state=[ 0.0191717  -0.37408438 -0.0060438   0.50397722]\n",
      "[ episode 216 ][ timestamp 9 ] state=[ 0.0191717  -0.37408438 -0.0060438   0.50397722], action=1, reward=1.0, next_state=[ 0.01169001 -0.17887777  0.00403574  0.20939579]\n",
      "[ episode 216 ][ timestamp 10 ] state=[ 0.01169001 -0.17887777  0.00403574  0.20939579], action=1, reward=1.0, next_state=[ 0.00811246  0.01618624  0.00822366 -0.08201134]\n",
      "[ episode 216 ][ timestamp 11 ] state=[ 0.00811246  0.01618624  0.00822366 -0.08201134], action=0, reward=1.0, next_state=[ 0.00843618 -0.17905262  0.00658343  0.21325478]\n",
      "[ episode 216 ][ timestamp 12 ] state=[ 0.00843618 -0.17905262  0.00658343  0.21325478], action=1, reward=1.0, next_state=[ 0.00485513  0.01597459  0.01084853 -0.07734418]\n",
      "[ episode 216 ][ timestamp 13 ] state=[ 0.00485513  0.01597459  0.01084853 -0.07734418], action=0, reward=1.0, next_state=[ 0.00517462 -0.17930119  0.00930164  0.21874165]\n",
      "[ episode 216 ][ timestamp 14 ] state=[ 0.00517462 -0.17930119  0.00930164  0.21874165], action=1, reward=1.0, next_state=[ 0.0015886   0.01568657  0.01367647 -0.07099269]\n",
      "[ episode 216 ][ timestamp 15 ] state=[ 0.0015886   0.01568657  0.01367647 -0.07099269], action=1, reward=1.0, next_state=[ 0.00190233  0.21060981  0.01225662 -0.35932944]\n",
      "[ episode 216 ][ timestamp 16 ] state=[ 0.00190233  0.21060981  0.01225662 -0.35932944], action=0, reward=1.0, next_state=[ 0.00611452  0.01531579  0.00507003 -0.06280705]\n",
      "[ episode 216 ][ timestamp 17 ] state=[ 0.00611452  0.01531579  0.00507003 -0.06280705], action=1, reward=1.0, next_state=[ 0.00642084  0.21036468  0.00381389 -0.35388605]\n",
      "[ episode 216 ][ timestamp 18 ] state=[ 0.00642084  0.21036468  0.00381389 -0.35388605], action=0, reward=1.0, next_state=[ 0.01062813  0.0151887  -0.00326383 -0.06000294]\n",
      "[ episode 216 ][ timestamp 19 ] state=[ 0.01062813  0.0151887  -0.00326383 -0.06000294], action=0, reward=1.0, next_state=[ 0.01093191 -0.1798863  -0.00446389  0.23164844]\n",
      "[ episode 216 ][ timestamp 20 ] state=[ 0.01093191 -0.1798863  -0.00446389  0.23164844], action=1, reward=1.0, next_state=[ 0.00733418  0.01529915  0.00016908 -0.06243919]\n",
      "[ episode 216 ][ timestamp 21 ] state=[ 0.00733418  0.01529915  0.00016908 -0.06243919], action=0, reward=1.0, next_state=[ 0.00764017 -0.17982522 -0.0010797   0.23029707]\n",
      "[ episode 216 ][ timestamp 22 ] state=[ 0.00764017 -0.17982522 -0.0010797   0.23029707], action=1, reward=1.0, next_state=[ 0.00404366  0.01531214  0.00352624 -0.06272623]\n",
      "[ episode 216 ][ timestamp 23 ] state=[ 0.00404366  0.01531214  0.00352624 -0.06272623], action=0, reward=1.0, next_state=[ 0.0043499  -0.17986019  0.00227171  0.23106715]\n",
      "[ episode 216 ][ timestamp 24 ] state=[ 0.0043499  -0.17986019  0.00227171  0.23106715], action=1, reward=1.0, next_state=[ 0.0007527   0.01522923  0.00689306 -0.06089833]\n",
      "[ episode 216 ][ timestamp 25 ] state=[ 0.0007527   0.01522923  0.00689306 -0.06089833], action=0, reward=1.0, next_state=[ 0.00105728 -0.17999087  0.00567509  0.23395141]\n",
      "[ episode 216 ][ timestamp 26 ] state=[ 0.00105728 -0.17999087  0.00567509  0.23395141], action=1, reward=1.0, next_state=[-0.00254253  0.01504953  0.01035412 -0.05693603]\n",
      "[ episode 216 ][ timestamp 27 ] state=[-0.00254253  0.01504953  0.01035412 -0.05693603], action=0, reward=1.0, next_state=[-0.00224154 -0.18021934  0.0092154   0.23899563]\n",
      "[ episode 216 ][ timestamp 28 ] state=[-0.00224154 -0.18021934  0.0092154   0.23899563], action=1, reward=1.0, next_state=[-0.00584593  0.01476976  0.01399531 -0.05076631]\n",
      "[ episode 216 ][ timestamp 29 ] state=[-0.00584593  0.01476976  0.01399531 -0.05076631], action=0, reward=1.0, next_state=[-0.00555053 -0.18055004  0.01297998  0.24629919]\n",
      "[ episode 216 ][ timestamp 30 ] state=[-0.00555053 -0.18055004  0.01297998  0.24629919], action=1, reward=1.0, next_state=[-0.00916153  0.01438414  0.01790597 -0.04226144]\n",
      "[ episode 216 ][ timestamp 31 ] state=[-0.00916153  0.01438414  0.01790597 -0.04226144], action=0, reward=1.0, next_state=[-0.00887385 -0.18098994  0.01706074  0.25601677]\n",
      "[ episode 216 ][ timestamp 32 ] state=[-0.00887385 -0.18098994  0.01706074  0.25601677], action=1, reward=1.0, next_state=[-0.01249365  0.01388433  0.02218107 -0.03123647]\n",
      "[ episode 216 ][ timestamp 33 ] state=[-0.01249365  0.01388433  0.02218107 -0.03123647], action=0, reward=1.0, next_state=[-0.01221596 -0.18154857  0.02155634  0.26836147]\n",
      "[ episode 216 ][ timestamp 34 ] state=[-0.01221596 -0.18154857  0.02155634  0.26836147], action=1, reward=1.0, next_state=[-0.01584694  0.01325922  0.02692357 -0.01744525]\n",
      "[ episode 216 ][ timestamp 35 ] state=[-0.01584694  0.01325922  0.02692357 -0.01744525], action=0, reward=1.0, next_state=[-0.01558175 -0.1822383   0.02657467  0.28360931]\n",
      "[ episode 216 ][ timestamp 36 ] state=[-0.01558175 -0.1822383   0.02657467  0.28360931], action=1, reward=1.0, next_state=[-0.01922652  0.01249474  0.03224686 -0.00057507]\n",
      "[ episode 216 ][ timestamp 37 ] state=[-0.01922652  0.01249474  0.03224686 -0.00057507], action=0, reward=1.0, next_state=[-0.01897662 -0.18307448  0.03223535  0.30210519]\n",
      "[ episode 216 ][ timestamp 38 ] state=[-0.01897662 -0.18307448  0.03223535  0.30210519], action=1, reward=1.0, next_state=[-0.02263811  0.01157356  0.03827746  0.01976036]\n",
      "[ episode 216 ][ timestamp 39 ] state=[-0.02263811  0.01157356  0.03827746  0.01976036], action=0, reward=1.0, next_state=[-0.02240664 -0.18407583  0.03867266  0.3242703 ]\n",
      "[ episode 216 ][ timestamp 40 ] state=[-0.02240664 -0.18407583  0.03867266  0.3242703 ], action=1, reward=1.0, next_state=[-0.02608816  0.01047474  0.04515807  0.04402957]\n",
      "[ episode 216 ][ timestamp 41 ] state=[-0.02608816  0.01047474  0.04515807  0.04402957], action=0, reward=1.0, next_state=[-0.02587866 -0.18526469  0.04603866  0.35061136]\n",
      "[ episode 216 ][ timestamp 42 ] state=[-0.02587866 -0.18526469  0.04603866  0.35061136], action=0, reward=1.0, next_state=[-0.02958396 -0.38101012  0.05305089  0.65744897]\n",
      "[ episode 216 ][ timestamp 43 ] state=[-0.02958396 -0.38101012  0.05305089  0.65744897], action=1, reward=1.0, next_state=[-0.03720416 -0.18666519  0.06619987  0.38193135]\n",
      "[ episode 216 ][ timestamp 44 ] state=[-0.03720416 -0.18666519  0.06619987  0.38193135], action=1, reward=1.0, next_state=[-0.04093746  0.00745739  0.0738385   0.11083385]\n",
      "[ episode 216 ][ timestamp 45 ] state=[-0.04093746  0.00745739  0.0738385   0.11083385], action=0, reward=1.0, next_state=[-0.04078832 -0.1886407   0.07605517  0.42586827]\n",
      "[ episode 216 ][ timestamp 46 ] state=[-0.04078832 -0.1886407   0.07605517  0.42586827], action=0, reward=1.0, next_state=[-0.04456113 -0.3847529   0.08457254  0.74152487]\n",
      "[ episode 216 ][ timestamp 47 ] state=[-0.04456113 -0.3847529   0.08457254  0.74152487], action=1, reward=1.0, next_state=[-0.05225619 -0.19089397  0.09940304  0.47661048]\n",
      "[ episode 216 ][ timestamp 48 ] state=[-0.05225619 -0.19089397  0.09940304  0.47661048], action=1, reward=1.0, next_state=[-0.05607407  0.00269426  0.10893525  0.21683797]\n",
      "[ episode 216 ][ timestamp 49 ] state=[-0.05607407  0.00269426  0.10893525  0.21683797], action=0, reward=1.0, next_state=[-0.05602018 -0.19380272  0.113272    0.54179997]\n",
      "[ episode 216 ][ timestamp 50 ] state=[-0.05602018 -0.19380272  0.113272    0.54179997], action=1, reward=1.0, next_state=[-0.05989624 -0.00043979  0.124108    0.28684509]\n",
      "[ episode 216 ][ timestamp 51 ] state=[-0.05989624 -0.00043979  0.124108    0.28684509], action=1, reward=1.0, next_state=[-0.05990503  0.19271386  0.12984491  0.03573725]\n",
      "[ episode 216 ][ timestamp 52 ] state=[-0.05990503  0.19271386  0.12984491  0.03573725], action=1, reward=1.0, next_state=[-0.05605075  0.38575785  0.13055965 -0.21332395]\n",
      "[ episode 216 ][ timestamp 53 ] state=[-0.05605075  0.38575785  0.13055965 -0.21332395], action=0, reward=1.0, next_state=[-0.0483356   0.18903444  0.12629317  0.11752533]\n",
      "[ episode 216 ][ timestamp 54 ] state=[-0.0483356   0.18903444  0.12629317  0.11752533], action=0, reward=1.0, next_state=[-0.04455491 -0.00764951  0.12864368  0.44723314]\n",
      "[ episode 216 ][ timestamp 55 ] state=[-0.04455491 -0.00764951  0.12864368  0.44723314], action=1, reward=1.0, next_state=[-0.0447079   0.18544029  0.13758834  0.19770874]\n",
      "[ episode 216 ][ timestamp 56 ] state=[-0.0447079   0.18544029  0.13758834  0.19770874], action=1, reward=1.0, next_state=[-0.04099909  0.37835353  0.14154252 -0.048603  ]\n",
      "[ episode 216 ][ timestamp 57 ] state=[-0.04099909  0.37835353  0.14154252 -0.048603  ], action=0, reward=1.0, next_state=[-0.03343202  0.18151561  0.14057046  0.28517588]\n",
      "[ episode 216 ][ timestamp 58 ] state=[-0.03343202  0.18151561  0.14057046  0.28517588], action=1, reward=1.0, next_state=[-0.02980171  0.37438212  0.14627397  0.03992144]\n",
      "[ episode 216 ][ timestamp 59 ] state=[-0.02980171  0.37438212  0.14627397  0.03992144], action=1, reward=1.0, next_state=[-0.02231407  0.56713668  0.1470724  -0.20327142]\n",
      "[ episode 216 ][ timestamp 60 ] state=[-0.02231407  0.56713668  0.1470724  -0.20327142], action=0, reward=1.0, next_state=[-0.01097133  0.37025106  0.14300697  0.13195232]\n",
      "[ episode 216 ][ timestamp 61 ] state=[-0.01097133  0.37025106  0.14300697  0.13195232], action=1, reward=1.0, next_state=[-0.00356631  0.56306578  0.14564602 -0.11241647]\n",
      "[ episode 216 ][ timestamp 62 ] state=[-0.00356631  0.56306578  0.14564602 -0.11241647], action=0, reward=1.0, next_state=[0.007695   0.36618969 0.14339769 0.2224397 ]\n",
      "[ episode 216 ][ timestamp 63 ] state=[0.007695   0.36618969 0.14339769 0.2224397 ], action=1, reward=1.0, next_state=[ 0.0150188   0.55900196  0.14784649 -0.02179563]\n",
      "[ episode 216 ][ timestamp 64 ] state=[ 0.0150188   0.55900196  0.14784649 -0.02179563], action=1, reward=1.0, next_state=[ 0.02619884  0.7517284   0.14741057 -0.26442281]\n",
      "[ episode 216 ][ timestamp 65 ] state=[ 0.02619884  0.7517284   0.14741057 -0.26442281], action=0, reward=1.0, next_state=[0.0412334  0.55484366 0.14212212 0.07088332]\n",
      "[ episode 216 ][ timestamp 66 ] state=[0.0412334  0.55484366 0.14212212 0.07088332], action=1, reward=1.0, next_state=[ 0.05233028  0.74767232  0.14353978 -0.17380005]\n",
      "[ episode 216 ][ timestamp 67 ] state=[ 0.05233028  0.74767232  0.14353978 -0.17380005], action=0, reward=1.0, next_state=[0.06728372 0.55081892 0.14006378 0.16049929]\n",
      "[ episode 216 ][ timestamp 68 ] state=[0.06728372 0.55081892 0.14006378 0.16049929], action=1, reward=1.0, next_state=[ 0.0783001   0.74368678  0.14327377 -0.08492516]\n",
      "[ episode 216 ][ timestamp 69 ] state=[ 0.0783001   0.74368678  0.14327377 -0.08492516], action=0, reward=1.0, next_state=[0.09317384 0.54683274 0.14157526 0.24930894]\n",
      "[ episode 216 ][ timestamp 70 ] state=[0.09317384 0.54683274 0.14157526 0.24930894], action=0, reward=1.0, next_state=[0.10411049 0.35000271 0.14656144 0.58308425]\n",
      "[ episode 216 ][ timestamp 71 ] state=[0.10411049 0.35000271 0.14656144 0.58308425], action=1, reward=1.0, next_state=[0.11111055 0.54280031 0.15822313 0.33992328]\n",
      "[ episode 216 ][ timestamp 72 ] state=[0.11111055 0.54280031 0.15822313 0.33992328], action=1, reward=1.0, next_state=[0.12196655 0.73535876 0.16502159 0.10101728]\n",
      "[ episode 216 ][ timestamp 73 ] state=[0.12196655 0.73535876 0.16502159 0.10101728], action=1, reward=1.0, next_state=[ 0.13667373  0.92777848  0.16704194 -0.13539477]\n",
      "[ episode 216 ][ timestamp 74 ] state=[ 0.13667373  0.92777848  0.16704194 -0.13539477], action=1, reward=1.0, next_state=[ 0.1552293   1.12016295  0.16433404 -0.37107248]\n",
      "[ episode 216 ][ timestamp 75 ] state=[ 0.1552293   1.12016295  0.16433404 -0.37107248], action=0, reward=1.0, next_state=[ 0.17763256  0.92313423  0.15691259 -0.03141405]\n",
      "[ episode 216 ][ timestamp 76 ] state=[ 0.17763256  0.92313423  0.15691259 -0.03141405], action=1, reward=1.0, next_state=[ 0.19609524  1.11569865  0.15628431 -0.27076882]\n",
      "[ episode 216 ][ timestamp 77 ] state=[ 0.19609524  1.11569865  0.15628431 -0.27076882], action=0, reward=1.0, next_state=[0.21840921 0.91873196 0.15086894 0.06684117]\n",
      "[ episode 216 ][ timestamp 78 ] state=[0.21840921 0.91873196 0.15086894 0.06684117], action=1, reward=1.0, next_state=[ 0.23678385  1.1114051   0.15220576 -0.17469824]\n",
      "[ episode 216 ][ timestamp 79 ] state=[ 0.23678385  1.1114051   0.15220576 -0.17469824], action=0, reward=1.0, next_state=[0.25901195 0.91446963 0.1487118  0.16186574]\n",
      "[ episode 216 ][ timestamp 80 ] state=[0.25901195 0.91446963 0.1487118  0.16186574], action=1, reward=1.0, next_state=[ 0.27730135  1.10718437  0.15194911 -0.08045552]\n",
      "[ episode 216 ][ timestamp 81 ] state=[ 0.27730135  1.10718437  0.15194911 -0.08045552], action=1, reward=1.0, next_state=[ 0.29944503  1.29983852  0.15034    -0.32160575]\n",
      "[ episode 216 ][ timestamp 82 ] state=[ 0.29944503  1.29983852  0.15034    -0.32160575], action=0, reward=1.0, next_state=[0.3254418  1.10293113 0.14390789 0.01445737]\n",
      "[ episode 216 ][ timestamp 83 ] state=[0.3254418  1.10293113 0.14390789 0.01445737], action=1, reward=1.0, next_state=[ 0.34750043  1.2957275   0.14419703 -0.22958478]\n",
      "[ episode 216 ][ timestamp 84 ] state=[ 0.34750043  1.2957275   0.14419703 -0.22958478], action=0, reward=1.0, next_state=[0.37341498 1.09887092 0.13960534 0.10488267]\n",
      "[ episode 216 ][ timestamp 85 ] state=[0.37341498 1.09887092 0.13960534 0.10488267], action=1, reward=1.0, next_state=[ 0.3953924   1.29174486  0.14170299 -0.14070275]\n",
      "[ episode 216 ][ timestamp 86 ] state=[ 0.3953924   1.29174486  0.14170299 -0.14070275], action=1, reward=1.0, next_state=[ 0.42122729  1.48458297  0.13888894 -0.38553927]\n",
      "[ episode 216 ][ timestamp 87 ] state=[ 0.42122729  1.48458297  0.13888894 -0.38553927], action=1, reward=1.0, next_state=[ 0.45091895  1.67748811  0.13117815 -0.6314084 ]\n",
      "[ episode 216 ][ timestamp 88 ] state=[ 0.45091895  1.67748811  0.13117815 -0.6314084 ], action=0, reward=1.0, next_state=[ 0.48446871  1.48080349  0.11854998 -0.30046033]\n",
      "[ episode 216 ][ timestamp 89 ] state=[ 0.48446871  1.48080349  0.11854998 -0.30046033], action=0, reward=1.0, next_state=[0.51408478 1.28420888 0.11254078 0.0271339 ]\n",
      "[ episode 216 ][ timestamp 90 ] state=[0.51408478 1.28420888 0.11254078 0.0271339 ], action=0, reward=1.0, next_state=[0.53976896 1.08766798 0.11308345 0.35309746]\n",
      "[ episode 216 ][ timestamp 91 ] state=[0.53976896 1.08766798 0.11308345 0.35309746], action=1, reward=1.0, next_state=[0.56152232 1.28101552 0.1201454  0.09810427]\n",
      "[ episode 216 ][ timestamp 92 ] state=[0.56152232 1.28101552 0.1201454  0.09810427], action=0, reward=1.0, next_state=[0.58714263 1.08439484 0.12210749 0.42614702]\n",
      "[ episode 216 ][ timestamp 93 ] state=[0.58714263 1.08439484 0.12210749 0.42614702], action=1, reward=1.0, next_state=[0.60883053 1.27759479 0.13063043 0.17431536]\n",
      "[ episode 216 ][ timestamp 94 ] state=[0.60883053 1.27759479 0.13063043 0.17431536], action=1, reward=1.0, next_state=[ 0.63438242  1.47062876  0.13411674 -0.07447239]\n",
      "[ episode 216 ][ timestamp 95 ] state=[ 0.63438242  1.47062876  0.13411674 -0.07447239], action=1, reward=1.0, next_state=[ 0.663795    1.66359853  0.13262729 -0.32201547]\n",
      "[ episode 216 ][ timestamp 96 ] state=[ 0.663795    1.66359853  0.13262729 -0.32201547], action=0, reward=1.0, next_state=[0.69706697 1.46686185 0.12618698 0.00937611]\n",
      "[ episode 216 ][ timestamp 97 ] state=[0.69706697 1.46686185 0.12618698 0.00937611], action=1, reward=1.0, next_state=[ 0.72640421  1.65996945  0.1263745  -0.2409816 ]\n",
      "[ episode 216 ][ timestamp 98 ] state=[ 0.72640421  1.65996945  0.1263745  -0.2409816 ], action=0, reward=1.0, next_state=[0.7596036  1.46329009 0.12155487 0.08874006]\n",
      "[ episode 216 ][ timestamp 99 ] state=[0.7596036  1.46329009 0.12155487 0.08874006], action=0, reward=1.0, next_state=[0.7888694  1.26665447 0.12332967 0.41716632]\n",
      "[ episode 216 ][ timestamp 100 ] state=[0.7888694  1.26665447 0.12332967 0.41716632], action=1, reward=1.0, next_state=[0.81420249 1.45983255 0.131673   0.16576718]\n",
      "[ episode 216 ][ timestamp 101 ] state=[0.81420249 1.45983255 0.131673   0.16576718], action=0, reward=1.0, next_state=[0.84339914 1.26309563 0.13498834 0.49691812]\n",
      "[ episode 216 ][ timestamp 102 ] state=[0.84339914 1.26309563 0.13498834 0.49691812], action=0, reward=1.0, next_state=[0.86866105 1.06635433 0.1449267  0.82891156]\n",
      "[ episode 216 ][ timestamp 103 ] state=[0.86866105 1.06635433 0.1449267  0.82891156], action=0, reward=1.0, next_state=[0.88998814 0.86958001 0.16150493 1.16343816]\n",
      "[ episode 216 ][ timestamp 104 ] state=[0.88998814 0.86958001 0.16150493 1.16343816], action=0, reward=1.0, next_state=[0.90737974 0.67276664 0.1847737  1.50209264]\n",
      "[ episode 216 ][ timestamp 105 ] state=[0.90737974 0.67276664 0.1847737  1.50209264], action=1, reward=-1.0, next_state=[0.92083507 0.86522765 0.21481555 1.27233016]\n",
      "[ Ended! ] Episode 216: Exploration_rate=0.3403786882983606. Score=105.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 217 ] state=[-0.00467696  0.02125767 -0.04370973  0.04548176]\n",
      "[ episode 217 ][ timestamp 1 ] state=[-0.00467696  0.02125767 -0.04370973  0.04548176], action=1, reward=1.0, next_state=[-0.0042518   0.21697825 -0.0428001  -0.26066528]\n",
      "[ episode 217 ][ timestamp 2 ] state=[-0.0042518   0.21697825 -0.0428001  -0.26066528], action=1, reward=1.0, next_state=[ 8.77625176e-05  4.12684216e-01 -4.80134016e-02 -5.66534774e-01]\n",
      "[ episode 217 ][ timestamp 3 ] state=[ 8.77625176e-05  4.12684216e-01 -4.80134016e-02 -5.66534774e-01], action=1, reward=1.0, next_state=[ 0.00834145  0.60844567 -0.0593441  -0.87394907]\n",
      "[ episode 217 ][ timestamp 4 ] state=[ 0.00834145  0.60844567 -0.0593441  -0.87394907], action=0, reward=1.0, next_state=[ 0.02051036  0.41417861 -0.07682308 -0.60049837]\n",
      "[ episode 217 ][ timestamp 5 ] state=[ 0.02051036  0.41417861 -0.07682308 -0.60049837], action=0, reward=1.0, next_state=[ 0.02879393  0.22021068 -0.08883305 -0.3329684 ]\n",
      "[ episode 217 ][ timestamp 6 ] state=[ 0.02879393  0.22021068 -0.08883305 -0.3329684 ], action=1, reward=1.0, next_state=[ 0.03319815  0.41647731 -0.09549241 -0.65229009]\n",
      "[ episode 217 ][ timestamp 7 ] state=[ 0.03319815  0.41647731 -0.09549241 -0.65229009], action=0, reward=1.0, next_state=[ 0.04152769  0.2228058  -0.10853822 -0.39113848]\n",
      "[ episode 217 ][ timestamp 8 ] state=[ 0.04152769  0.2228058  -0.10853822 -0.39113848], action=1, reward=1.0, next_state=[ 0.04598381  0.41928728 -0.11636099 -0.71597403]\n",
      "[ episode 217 ][ timestamp 9 ] state=[ 0.04598381  0.41928728 -0.11636099 -0.71597403], action=1, reward=1.0, next_state=[ 0.05436955  0.61581111 -0.13068047 -1.04289932]\n",
      "[ episode 217 ][ timestamp 10 ] state=[ 0.05436955  0.61581111 -0.13068047 -1.04289932], action=0, reward=1.0, next_state=[ 0.06668578  0.42264378 -0.15153845 -0.7939297 ]\n",
      "[ episode 217 ][ timestamp 11 ] state=[ 0.06668578  0.42264378 -0.15153845 -0.7939297 ], action=0, reward=1.0, next_state=[ 0.07513865  0.22989057 -0.16741705 -0.55249529]\n",
      "[ episode 217 ][ timestamp 12 ] state=[ 0.07513865  0.22989057 -0.16741705 -0.55249529], action=1, reward=1.0, next_state=[ 0.07973646  0.42691897 -0.17846695 -0.89289676]\n",
      "[ episode 217 ][ timestamp 13 ] state=[ 0.07973646  0.42691897 -0.17846695 -0.89289676], action=0, reward=1.0, next_state=[ 0.08827484  0.23460786 -0.19632489 -0.66120301]\n",
      "[ episode 217 ][ timestamp 14 ] state=[ 0.08827484  0.23460786 -0.19632489 -0.66120301], action=0, reward=-1.0, next_state=[ 0.092967    0.04268087 -0.20954895 -0.43619232]\n",
      "[ Ended! ] Episode 217: Exploration_rate=0.3386767948568688. Score=14.\n",
      "[ Experience replay ] starts\n",
      "[ episode 218 ] state=[-0.02431798 -0.00383046 -0.01255144  0.03690203]\n",
      "[ episode 218 ][ timestamp 1 ] state=[-0.02431798 -0.00383046 -0.01255144  0.03690203], action=0, reward=1.0, next_state=[-0.02439459 -0.19877019 -0.0118134   0.32559857]\n",
      "[ episode 218 ][ timestamp 2 ] state=[-0.02439459 -0.19877019 -0.0118134   0.32559857], action=1, reward=1.0, next_state=[-0.02837    -0.00348205 -0.00530143  0.02921374]\n",
      "[ episode 218 ][ timestamp 3 ] state=[-0.02837    -0.00348205 -0.00530143  0.02921374], action=0, reward=1.0, next_state=[-0.02843964 -0.19852757 -0.00471716  0.3202193 ]\n",
      "[ episode 218 ][ timestamp 4 ] state=[-0.02843964 -0.19852757 -0.00471716  0.3202193 ], action=1, reward=1.0, next_state=[-0.03241019 -0.00333876  0.00168723  0.0260525 ]\n",
      "[ episode 218 ][ timestamp 5 ] state=[-0.03241019 -0.00333876  0.00168723  0.0260525 ], action=0, reward=1.0, next_state=[-0.03247696 -0.19848487  0.00220828  0.31926729]\n",
      "[ episode 218 ][ timestamp 6 ] state=[-0.03247696 -0.19848487  0.00220828  0.31926729], action=1, reward=1.0, next_state=[-0.03644666 -0.00339444  0.00859362  0.02728159]\n",
      "[ episode 218 ][ timestamp 7 ] state=[-0.03644666 -0.00339444  0.00859362  0.02728159], action=0, reward=1.0, next_state=[-0.03651455 -0.19863857  0.00913926  0.32266347]\n",
      "[ episode 218 ][ timestamp 8 ] state=[-0.03651455 -0.19863857  0.00913926  0.32266347], action=1, reward=1.0, next_state=[-0.04048732 -0.00364794  0.01559253  0.03287665]\n",
      "[ episode 218 ][ timestamp 9 ] state=[-0.04048732 -0.00364794  0.01559253  0.03287665], action=0, reward=1.0, next_state=[-0.04056028 -0.19898999  0.01625006  0.33043811]\n",
      "[ episode 218 ][ timestamp 10 ] state=[-0.04056028 -0.19898999  0.01625006  0.33043811], action=1, reward=1.0, next_state=[-0.04454008 -0.00410308  0.02285882  0.04292365]\n",
      "[ episode 218 ][ timestamp 11 ] state=[-0.04454008 -0.00410308  0.02285882  0.04292365], action=0, reward=1.0, next_state=[-0.04462214 -0.19954523  0.02371729  0.3427302 ]\n",
      "[ episode 218 ][ timestamp 12 ] state=[-0.04462214 -0.19954523  0.02371729  0.3427302 ], action=1, reward=1.0, next_state=[-0.04861305 -0.00476859  0.0305719   0.05761964]\n",
      "[ episode 218 ][ timestamp 13 ] state=[-0.04861305 -0.00476859  0.0305719   0.05761964], action=0, reward=1.0, next_state=[-0.04870842 -0.20031525  0.03172429  0.3597893 ]\n",
      "[ episode 218 ][ timestamp 14 ] state=[-0.04870842 -0.20031525  0.03172429  0.3597893 ], action=1, reward=1.0, next_state=[-0.05271472 -0.00565829  0.03892008  0.07727615]\n",
      "[ episode 218 ][ timestamp 15 ] state=[-0.05271472 -0.00565829  0.03892008  0.07727615], action=0, reward=1.0, next_state=[-0.05282789 -0.20131594  0.0404656   0.38198   ]\n",
      "[ episode 218 ][ timestamp 16 ] state=[-0.05282789 -0.20131594  0.0404656   0.38198   ], action=1, reward=1.0, next_state=[-0.05685421 -0.00679123  0.0481052   0.10232543]\n",
      "[ episode 218 ][ timestamp 17 ] state=[-0.05685421 -0.00679123  0.0481052   0.10232543], action=0, reward=1.0, next_state=[-0.05699003 -0.2025684   0.05015171  0.40978894]\n",
      "[ episode 218 ][ timestamp 18 ] state=[-0.05699003 -0.2025684   0.05015171  0.40978894], action=1, reward=1.0, next_state=[-0.0610414  -0.00819202  0.05834749  0.13332939]\n",
      "[ episode 218 ][ timestamp 19 ] state=[-0.0610414  -0.00819202  0.05834749  0.13332939], action=0, reward=1.0, next_state=[-0.06120524 -0.20409911  0.06101407  0.44383437]\n",
      "[ episode 218 ][ timestamp 20 ] state=[-0.06120524 -0.20409911  0.06101407  0.44383437], action=1, reward=1.0, next_state=[-0.06528722 -0.00989113  0.06989076  0.17099149]\n",
      "[ episode 218 ][ timestamp 21 ] state=[-0.06528722 -0.00989113  0.06989076  0.17099149], action=1, reward=1.0, next_state=[-0.06548505  0.18416449  0.07331059 -0.09885014]\n",
      "[ episode 218 ][ timestamp 22 ] state=[-0.06548505  0.18416449  0.07331059 -0.09885014], action=0, reward=1.0, next_state=[-0.06180176 -0.01192741  0.07133359  0.21603165]\n",
      "[ episode 218 ][ timestamp 23 ] state=[-0.06180176 -0.01192741  0.07133359  0.21603165], action=1, reward=1.0, next_state=[-0.0620403   0.18210611  0.07565422 -0.05332413]\n",
      "[ episode 218 ][ timestamp 24 ] state=[-0.0620403   0.18210611  0.07565422 -0.05332413], action=0, reward=1.0, next_state=[-0.05839818 -0.01401451  0.07458774  0.26223645]\n",
      "[ episode 218 ][ timestamp 25 ] state=[-0.05839818 -0.01401451  0.07458774  0.26223645], action=1, reward=1.0, next_state=[-0.05867847  0.17996788  0.07983247 -0.00601967]\n",
      "[ episode 218 ][ timestamp 26 ] state=[-0.05867847  0.17996788  0.07983247 -0.00601967], action=0, reward=1.0, next_state=[-0.05507911 -0.01620283  0.07971207  0.31074505]\n",
      "[ episode 218 ][ timestamp 27 ] state=[-0.05507911 -0.01620283  0.07971207  0.31074505], action=0, reward=1.0, next_state=[-0.05540317 -0.21236463  0.08592698  0.62746396]\n",
      "[ episode 218 ][ timestamp 28 ] state=[-0.05540317 -0.21236463  0.08592698  0.62746396], action=0, reward=1.0, next_state=[-0.05965046 -0.40857413  0.09847626  0.94592382]\n",
      "[ episode 218 ][ timestamp 29 ] state=[-0.05965046 -0.40857413  0.09847626  0.94592382], action=1, reward=1.0, next_state=[-0.06782195 -0.21490652  0.11739473  0.68573508]\n",
      "[ episode 218 ][ timestamp 30 ] state=[-0.06782195 -0.21490652  0.11739473  0.68573508], action=1, reward=1.0, next_state=[-0.07212008 -0.02159306  0.13110943  0.43219555]\n",
      "[ episode 218 ][ timestamp 31 ] state=[-0.07212008 -0.02159306  0.13110943  0.43219555], action=0, reward=1.0, next_state=[-0.07255194 -0.21830401  0.13975334  0.76316539]\n",
      "[ episode 218 ][ timestamp 32 ] state=[-0.07255194 -0.21830401  0.13975334  0.76316539], action=1, reward=1.0, next_state=[-0.07691802 -0.02535513  0.15501665  0.51751771]\n",
      "[ episode 218 ][ timestamp 33 ] state=[-0.07691802 -0.02535513  0.15501665  0.51751771], action=1, reward=1.0, next_state=[-0.07742512  0.16728343  0.16536701  0.27741736]\n",
      "[ episode 218 ][ timestamp 34 ] state=[-0.07742512  0.16728343  0.16536701  0.27741736], action=1, reward=1.0, next_state=[-0.07407945  0.3597076   0.17091535  0.04111529]\n",
      "[ episode 218 ][ timestamp 35 ] state=[-0.07407945  0.3597076   0.17091535  0.04111529], action=1, reward=1.0, next_state=[-0.0668853   0.55201904  0.17173766 -0.19314393]\n",
      "[ episode 218 ][ timestamp 36 ] state=[-0.0668853   0.55201904  0.17173766 -0.19314393], action=1, reward=1.0, next_state=[-0.05584492  0.74432145  0.16787478 -0.42711114]\n",
      "[ episode 218 ][ timestamp 37 ] state=[-0.05584492  0.74432145  0.16787478 -0.42711114], action=0, reward=1.0, next_state=[-0.04095849  0.54726918  0.15933256 -0.08656426]\n",
      "[ episode 218 ][ timestamp 38 ] state=[-0.04095849  0.54726918  0.15933256 -0.08656426], action=1, reward=1.0, next_state=[-0.03001311  0.7397911   0.15760127 -0.32504342]\n",
      "[ episode 218 ][ timestamp 39 ] state=[-0.03001311  0.7397911   0.15760127 -0.32504342], action=0, reward=1.0, next_state=[-0.01521728  0.54281736  0.1511004   0.01289862]\n",
      "[ episode 218 ][ timestamp 40 ] state=[-0.01521728  0.54281736  0.1511004   0.01289862], action=1, reward=1.0, next_state=[-0.00436094  0.73548574  0.15135838 -0.2285564 ]\n",
      "[ episode 218 ][ timestamp 41 ] state=[-0.00436094  0.73548574  0.15135838 -0.2285564 ], action=0, reward=1.0, next_state=[0.01034878 0.53856137 0.14678725 0.10778271]\n",
      "[ episode 218 ][ timestamp 42 ] state=[0.01034878 0.53856137 0.14678725 0.10778271], action=1, reward=1.0, next_state=[ 0.02112     0.73130824  0.1489429  -0.13522778]\n",
      "[ episode 218 ][ timestamp 43 ] state=[ 0.02112     0.73130824  0.1489429  -0.13522778], action=0, reward=1.0, next_state=[0.03574617 0.53440159 0.14623835 0.2004896 ]\n",
      "[ episode 218 ][ timestamp 44 ] state=[0.03574617 0.53440159 0.14623835 0.2004896 ], action=1, reward=1.0, next_state=[ 0.0464342   0.72716227  0.15024814 -0.04272419]\n",
      "[ episode 218 ][ timestamp 45 ] state=[ 0.0464342   0.72716227  0.15024814 -0.04272419], action=1, reward=1.0, next_state=[ 0.06097745  0.91984611  0.14939366 -0.28448683]\n",
      "[ episode 218 ][ timestamp 46 ] state=[ 0.06097745  0.91984611  0.14939366 -0.28448683], action=0, reward=1.0, next_state=[0.07937437 0.72294429 0.14370392 0.05133466]\n",
      "[ episode 218 ][ timestamp 47 ] state=[0.07937437 0.72294429 0.14370392 0.05133466], action=1, reward=1.0, next_state=[ 0.09383325  0.91574461  0.14473061 -0.19278116]\n",
      "[ episode 218 ][ timestamp 48 ] state=[ 0.09383325  0.91574461  0.14473061 -0.19278116], action=0, reward=1.0, next_state=[0.11214815 0.71888074 0.14087499 0.14182967]\n",
      "[ episode 218 ][ timestamp 49 ] state=[0.11214815 0.71888074 0.14087499 0.14182967], action=1, reward=1.0, next_state=[ 0.12652576  0.91173352  0.14371158 -0.10330337]\n",
      "[ episode 218 ][ timestamp 50 ] state=[ 0.12652576  0.91173352  0.14371158 -0.10330337], action=0, reward=1.0, next_state=[0.14476043 0.71487573 0.14164551 0.2310452 ]\n",
      "[ episode 218 ][ timestamp 51 ] state=[0.14476043 0.71487573 0.14164551 0.2310452 ], action=1, reward=1.0, next_state=[ 0.15905795  0.90771948  0.14626642 -0.01381877]\n",
      "[ episode 218 ][ timestamp 52 ] state=[ 0.15905795  0.90771948  0.14626642 -0.01381877], action=1, reward=1.0, next_state=[ 0.17721234  1.10047398  0.14599004 -0.25701405]\n",
      "[ episode 218 ][ timestamp 53 ] state=[ 0.17721234  1.10047398  0.14599004 -0.25701405], action=0, reward=1.0, next_state=[0.19922182 0.90360216 0.14084976 0.07792108]\n",
      "[ episode 218 ][ timestamp 54 ] state=[0.19922182 0.90360216 0.14084976 0.07792108], action=1, reward=1.0, next_state=[ 0.21729386  1.09645346  0.14240818 -0.16721814]\n",
      "[ episode 218 ][ timestamp 55 ] state=[ 0.21729386  1.09645346  0.14240818 -0.16721814], action=0, reward=1.0, next_state=[0.23922293 0.89961067 0.13906382 0.16678375]\n",
      "[ episode 218 ][ timestamp 56 ] state=[0.23922293 0.89961067 0.13906382 0.16678375], action=0, reward=1.0, next_state=[0.25721514 0.70280042 0.1423995  0.49990229]\n",
      "[ episode 218 ][ timestamp 57 ] state=[0.25721514 0.70280042 0.1423995  0.49990229], action=1, reward=1.0, next_state=[0.27127115 0.89565803 0.15239754 0.25526805]\n",
      "[ episode 218 ][ timestamp 58 ] state=[0.27127115 0.89565803 0.15239754 0.25526805], action=1, reward=1.0, next_state=[0.28918431 1.08831284 0.1575029  0.0142668 ]\n",
      "[ episode 218 ][ timestamp 59 ] state=[0.28918431 1.08831284 0.1575029  0.0142668 ], action=1, reward=1.0, next_state=[ 0.31095057  1.28086654  0.15778824 -0.22487398]\n",
      "[ episode 218 ][ timestamp 60 ] state=[ 0.31095057  1.28086654  0.15778824 -0.22487398], action=0, reward=1.0, next_state=[0.3365679  1.08388268 0.15329076 0.11312867]\n",
      "[ episode 218 ][ timestamp 61 ] state=[0.3365679  1.08388268 0.15329076 0.11312867], action=0, reward=1.0, next_state=[0.35824555 0.88693448 0.15555333 0.44997802]\n",
      "[ episode 218 ][ timestamp 62 ] state=[0.35824555 0.88693448 0.15555333 0.44997802], action=1, reward=1.0, next_state=[0.37598424 1.0795537  0.16455289 0.21008621]\n",
      "[ episode 218 ][ timestamp 63 ] state=[0.37598424 1.0795537  0.16455289 0.21008621], action=0, reward=1.0, next_state=[0.39757532 0.88250814 0.16875462 0.54982244]\n",
      "[ episode 218 ][ timestamp 64 ] state=[0.39757532 0.88250814 0.16875462 0.54982244], action=1, reward=1.0, next_state=[0.41522548 1.074908   0.17975107 0.314701  ]\n",
      "[ episode 218 ][ timestamp 65 ] state=[0.41522548 1.074908   0.17975107 0.314701  ], action=1, reward=1.0, next_state=[0.43672364 1.26707506 0.18604509 0.08365733]\n",
      "[ episode 218 ][ timestamp 66 ] state=[0.43672364 1.26707506 0.18604509 0.08365733], action=1, reward=1.0, next_state=[ 0.46206514  1.45911048  0.18771823 -0.14504275]\n",
      "[ episode 218 ][ timestamp 67 ] state=[ 0.46206514  1.45911048  0.18771823 -0.14504275], action=0, reward=1.0, next_state=[0.49124735 1.26186564 0.18481738 0.20049251]\n",
      "[ episode 218 ][ timestamp 68 ] state=[0.49124735 1.26186564 0.18481738 0.20049251], action=1, reward=1.0, next_state=[ 0.51648466  1.4539296   0.18882723 -0.02866961]\n",
      "[ episode 218 ][ timestamp 69 ] state=[ 0.51648466  1.4539296   0.18882723 -0.02866961], action=1, reward=1.0, next_state=[ 0.54556325  1.64591261  0.18825384 -0.2563395 ]\n",
      "[ episode 218 ][ timestamp 70 ] state=[ 0.54556325  1.64591261  0.18825384 -0.2563395 ], action=0, reward=1.0, next_state=[0.57848151 1.44867161 0.18312705 0.08931516]\n",
      "[ episode 218 ][ timestamp 71 ] state=[0.57848151 1.44867161 0.18312705 0.08931516], action=1, reward=1.0, next_state=[ 0.60745494  1.64076089  0.18491335 -0.14046195]\n",
      "[ episode 218 ][ timestamp 72 ] state=[ 0.60745494  1.64076089  0.18491335 -0.14046195], action=0, reward=1.0, next_state=[0.64027016 1.44353843 0.18210411 0.20438367]\n",
      "[ episode 218 ][ timestamp 73 ] state=[0.64027016 1.44353843 0.18210411 0.20438367], action=1, reward=1.0, next_state=[ 0.66914092  1.63565244  0.18619178 -0.0257792 ]\n",
      "[ episode 218 ][ timestamp 74 ] state=[ 0.66914092  1.63565244  0.18619178 -0.0257792 ], action=0, reward=1.0, next_state=[0.70185397 1.4384162  0.1856762  0.31938636]\n",
      "[ episode 218 ][ timestamp 75 ] state=[0.70185397 1.4384162  0.1856762  0.31938636], action=1, reward=1.0, next_state=[0.7306223  1.63047549 0.19206393 0.09052488]\n",
      "[ episode 218 ][ timestamp 76 ] state=[0.7306223  1.63047549 0.19206393 0.09052488], action=0, reward=1.0, next_state=[0.76323181 1.43319381 0.19387442 0.43712636]\n",
      "[ episode 218 ][ timestamp 77 ] state=[0.76323181 1.43319381 0.19387442 0.43712636], action=1, reward=1.0, next_state=[0.79189568 1.62511911 0.20261695 0.21127462]\n",
      "[ episode 218 ][ timestamp 78 ] state=[0.79189568 1.62511911 0.20261695 0.21127462], action=1, reward=1.0, next_state=[ 0.82439807  1.81685476  0.20684244 -0.01128281]\n",
      "[ episode 218 ][ timestamp 79 ] state=[ 0.82439807  1.81685476  0.20684244 -0.01128281], action=1, reward=1.0, next_state=[ 0.86073516  2.00850275  0.20661679 -0.23224814]\n",
      "[ episode 218 ][ timestamp 80 ] state=[ 0.86073516  2.00850275  0.20661679 -0.23224814], action=0, reward=1.0, next_state=[0.90090522 1.81111987 0.20197183 0.11784291]\n",
      "[ episode 218 ][ timestamp 81 ] state=[0.90090522 1.81111987 0.20197183 0.11784291], action=1, reward=1.0, next_state=[ 0.93712761  2.00286152  0.20432868 -0.10494643]\n",
      "[ episode 218 ][ timestamp 82 ] state=[ 0.93712761  2.00286152  0.20432868 -0.10494643], action=0, reward=1.0, next_state=[0.97718484 1.80548688 0.20222975 0.24461219]\n",
      "[ episode 218 ][ timestamp 83 ] state=[0.97718484 1.80548688 0.20222975 0.24461219], action=0, reward=1.0, next_state=[1.01329458 1.60813736 0.207122   0.59365494]\n",
      "[ episode 218 ][ timestamp 84 ] state=[1.01329458 1.60813736 0.207122   0.59365494], action=1, reward=-1.0, next_state=[1.04545733 1.79985058 0.2189951  0.37269081]\n",
      "[ Ended! ] Episode 218: Exploration_rate=0.33698341088258443. Score=84.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 219 ] state=[ 0.03384541 -0.0436589  -0.04551415  0.00712714]\n",
      "[ episode 219 ][ timestamp 1 ] state=[ 0.03384541 -0.0436589  -0.04551415  0.00712714], action=1, reward=1.0, next_state=[ 0.03297223  0.15208523 -0.04537161 -0.29956155]\n",
      "[ episode 219 ][ timestamp 2 ] state=[ 0.03297223  0.15208523 -0.04537161 -0.29956155], action=1, reward=1.0, next_state=[ 0.03601394  0.34782355 -0.05136284 -0.60620155]\n",
      "[ episode 219 ][ timestamp 3 ] state=[ 0.03601394  0.34782355 -0.05136284 -0.60620155], action=1, reward=1.0, next_state=[ 0.04297041  0.54362466 -0.06348687 -0.91460991]\n",
      "[ episode 219 ][ timestamp 4 ] state=[ 0.04297041  0.54362466 -0.06348687 -0.91460991], action=1, reward=1.0, next_state=[ 0.0538429   0.73954518 -0.08177907 -1.22655124]\n",
      "[ episode 219 ][ timestamp 5 ] state=[ 0.0538429   0.73954518 -0.08177907 -1.22655124], action=0, reward=1.0, next_state=[ 0.0686338   0.54556565 -0.1063101  -0.96057064]\n",
      "[ episode 219 ][ timestamp 6 ] state=[ 0.0686338   0.54556565 -0.1063101  -0.96057064], action=0, reward=1.0, next_state=[ 0.07954512  0.35202082 -0.12552151 -0.70308873]\n",
      "[ episode 219 ][ timestamp 7 ] state=[ 0.07954512  0.35202082 -0.12552151 -0.70308873], action=0, reward=1.0, next_state=[ 0.08658553  0.15884134 -0.13958328 -0.45240576]\n",
      "[ episode 219 ][ timestamp 8 ] state=[ 0.08658553  0.15884134 -0.13958328 -0.45240576], action=1, reward=1.0, next_state=[ 0.08976236  0.35563271 -0.1486314  -0.7856262 ]\n",
      "[ episode 219 ][ timestamp 9 ] state=[ 0.08976236  0.35563271 -0.1486314  -0.7856262 ], action=1, reward=1.0, next_state=[ 0.09687501  0.55245002 -0.16434392 -1.12113412]\n",
      "[ episode 219 ][ timestamp 10 ] state=[ 0.09687501  0.55245002 -0.16434392 -1.12113412], action=0, reward=1.0, next_state=[ 0.10792401  0.35981944 -0.1867666  -0.88418144]\n",
      "[ episode 219 ][ timestamp 11 ] state=[ 0.10792401  0.35981944 -0.1867666  -0.88418144], action=1, reward=1.0, next_state=[ 0.1151204   0.55691918 -0.20445023 -1.22928035]\n",
      "[ episode 219 ][ timestamp 12 ] state=[ 0.1151204   0.55691918 -0.20445023 -1.22928035], action=0, reward=-1.0, next_state=[ 0.12625879  0.36492807 -0.22903584 -1.00698214]\n",
      "[ Ended! ] Episode 219: Exploration_rate=0.3352984938281715. Score=12.\n",
      "[ Experience replay ] starts\n",
      "[ episode 220 ] state=[-0.02114179  0.02686959 -0.03888854  0.04580384]\n",
      "[ episode 220 ][ timestamp 1 ] state=[-0.02114179  0.02686959 -0.03888854  0.04580384], action=1, reward=1.0, next_state=[-0.02060439  0.22252698 -0.03797246 -0.25889069]\n",
      "[ episode 220 ][ timestamp 2 ] state=[-0.02060439  0.22252698 -0.03797246 -0.25889069], action=0, reward=1.0, next_state=[-0.01615385  0.02796712 -0.04315027  0.0215775 ]\n",
      "[ episode 220 ][ timestamp 3 ] state=[-0.01615385  0.02796712 -0.04315027  0.0215775 ], action=1, reward=1.0, next_state=[-0.01559451  0.22368047 -0.04271872 -0.28440151]\n",
      "[ episode 220 ][ timestamp 4 ] state=[-0.01559451  0.22368047 -0.04271872 -0.28440151], action=0, reward=1.0, next_state=[-0.0111209   0.02919301 -0.04840675 -0.00549195]\n",
      "[ episode 220 ][ timestamp 5 ] state=[-0.0111209   0.02919301 -0.04840675 -0.00549195], action=1, reward=1.0, next_state=[-0.01053704  0.22497456 -0.04851659 -0.3130463 ]\n",
      "[ episode 220 ][ timestamp 6 ] state=[-0.01053704  0.22497456 -0.04851659 -0.3130463 ], action=1, reward=1.0, next_state=[-0.00603755  0.42075289 -0.05477752 -0.62062652]\n",
      "[ episode 220 ][ timestamp 7 ] state=[-0.00603755  0.42075289 -0.05477752 -0.62062652], action=0, reward=1.0, next_state=[ 0.00237751  0.226437   -0.06719005 -0.34568641]\n",
      "[ episode 220 ][ timestamp 8 ] state=[ 0.00237751  0.226437   -0.06719005 -0.34568641], action=0, reward=1.0, next_state=[ 0.00690625  0.03233195 -0.07410378 -0.07492482]\n",
      "[ episode 220 ][ timestamp 9 ] state=[ 0.00690625  0.03233195 -0.07410378 -0.07492482], action=1, reward=1.0, next_state=[ 0.00755289  0.22843371 -0.07560227 -0.39003677]\n",
      "[ episode 220 ][ timestamp 10 ] state=[ 0.00755289  0.22843371 -0.07560227 -0.39003677], action=1, reward=1.0, next_state=[ 0.01212156  0.42454271 -0.08340301 -0.70556589]\n",
      "[ episode 220 ][ timestamp 11 ] state=[ 0.01212156  0.42454271 -0.08340301 -0.70556589], action=0, reward=1.0, next_state=[ 0.02061241  0.23066933 -0.09751433 -0.44025875]\n",
      "[ episode 220 ][ timestamp 12 ] state=[ 0.02061241  0.23066933 -0.09751433 -0.44025875], action=0, reward=1.0, next_state=[ 0.0252258   0.03705291 -0.1063195  -0.17983765]\n",
      "[ episode 220 ][ timestamp 13 ] state=[ 0.0252258   0.03705291 -0.1063195  -0.17983765], action=1, reward=1.0, next_state=[ 0.02596686  0.23352293 -0.10991626 -0.50407768]\n",
      "[ episode 220 ][ timestamp 14 ] state=[ 0.02596686  0.23352293 -0.10991626 -0.50407768], action=0, reward=1.0, next_state=[ 0.03063732  0.04010778 -0.11999781 -0.2479561 ]\n",
      "[ episode 220 ][ timestamp 15 ] state=[ 0.03063732  0.04010778 -0.11999781 -0.2479561 ], action=1, reward=1.0, next_state=[ 0.03143947  0.23672086 -0.12495693 -0.57594968]\n",
      "[ episode 220 ][ timestamp 16 ] state=[ 0.03143947  0.23672086 -0.12495693 -0.57594968], action=0, reward=1.0, next_state=[ 0.03617389  0.04355159 -0.13647592 -0.32509678]\n",
      "[ episode 220 ][ timestamp 17 ] state=[ 0.03617389  0.04355159 -0.13647592 -0.32509678], action=1, reward=1.0, next_state=[ 0.03704492  0.24032605 -0.14297786 -0.65751343]\n",
      "[ episode 220 ][ timestamp 18 ] state=[ 0.03704492  0.24032605 -0.14297786 -0.65751343], action=0, reward=1.0, next_state=[ 0.04185144  0.04745322 -0.15612813 -0.41304869]\n",
      "[ episode 220 ][ timestamp 19 ] state=[ 0.04185144  0.04745322 -0.15612813 -0.41304869], action=1, reward=1.0, next_state=[ 0.04280051  0.24440375 -0.1643891  -0.75059656]\n",
      "[ episode 220 ][ timestamp 20 ] state=[ 0.04280051  0.24440375 -0.1643891  -0.75059656], action=0, reward=1.0, next_state=[ 0.04768858  0.05188444 -0.17940103 -0.51382378]\n",
      "[ episode 220 ][ timestamp 21 ] state=[ 0.04768858  0.05188444 -0.17940103 -0.51382378], action=1, reward=1.0, next_state=[ 0.04872627  0.24901917 -0.18967751 -0.8572415 ]\n",
      "[ episode 220 ][ timestamp 22 ] state=[ 0.04872627  0.24901917 -0.18967751 -0.8572415 ], action=0, reward=1.0, next_state=[ 0.05370665  0.05691719 -0.20682234 -0.62968794]\n",
      "[ episode 220 ][ timestamp 23 ] state=[ 0.05370665  0.05691719 -0.20682234 -0.62968794], action=1, reward=-1.0, next_state=[ 0.054845    0.25423315 -0.2194161  -0.97972739]\n",
      "[ Ended! ] Episode 220: Exploration_rate=0.33362200135903064. Score=23.\n",
      "[ Experience replay ] starts\n",
      "[ episode 221 ] state=[-0.04764164 -0.00310302 -0.02114421 -0.03046382]\n",
      "[ episode 221 ][ timestamp 1 ] state=[-0.04764164 -0.00310302 -0.02114421 -0.03046382], action=0, reward=1.0, next_state=[-0.0477037  -0.19791547 -0.02175349  0.2554736 ]\n",
      "[ episode 221 ][ timestamp 2 ] state=[-0.0477037  -0.19791547 -0.02175349  0.2554736 ], action=0, reward=1.0, next_state=[-0.05166201 -0.39272018 -0.01664401  0.54121652]\n",
      "[ episode 221 ][ timestamp 3 ] state=[-0.05166201 -0.39272018 -0.01664401  0.54121652], action=0, reward=1.0, next_state=[-0.05951642 -0.58760429 -0.00581968  0.82860907]\n",
      "[ episode 221 ][ timestamp 4 ] state=[-0.05951642 -0.58760429 -0.00581968  0.82860907], action=1, reward=1.0, next_state=[-0.0712685  -0.39240325  0.0107525   0.5341015 ]\n",
      "[ episode 221 ][ timestamp 5 ] state=[-0.0712685  -0.39240325  0.0107525   0.5341015 ], action=0, reward=1.0, next_state=[-0.07911657 -0.58767476  0.02143453  0.83015299]\n",
      "[ episode 221 ][ timestamp 6 ] state=[-0.07911657 -0.58767476  0.02143453  0.83015299], action=0, reward=1.0, next_state=[-0.09087006 -0.78308304  0.03803759  1.12949935]\n",
      "[ episode 221 ][ timestamp 7 ] state=[-0.09087006 -0.78308304  0.03803759  1.12949935], action=0, reward=1.0, next_state=[-0.10653172 -0.97868195  0.06062757  1.43386584]\n",
      "[ episode 221 ][ timestamp 8 ] state=[-0.10653172 -0.97868195  0.06062757  1.43386584], action=1, reward=1.0, next_state=[-0.12610536 -0.78435802  0.08930489  1.16072909]\n",
      "[ episode 221 ][ timestamp 9 ] state=[-0.12610536 -0.78435802  0.08930489  1.16072909], action=0, reward=1.0, next_state=[-0.14179252 -0.98052254  0.11251947  1.48002403]\n",
      "[ episode 221 ][ timestamp 10 ] state=[-0.14179252 -0.98052254  0.11251947  1.48002403], action=1, reward=1.0, next_state=[-0.16140297 -0.78693932  0.14211995  1.2244964 ]\n",
      "[ episode 221 ][ timestamp 11 ] state=[-0.16140297 -0.78693932  0.14211995  1.2244964 ], action=1, reward=1.0, next_state=[-0.17714176 -0.59390442  0.16660988  0.97950608]\n",
      "[ episode 221 ][ timestamp 12 ] state=[-0.17714176 -0.59390442  0.16660988  0.97950608], action=1, reward=1.0, next_state=[-0.18901985 -0.40136017  0.1862      0.74344604]\n",
      "[ episode 221 ][ timestamp 13 ] state=[-0.18901985 -0.40136017  0.1862      0.74344604], action=1, reward=1.0, next_state=[-0.19704705 -0.20922928  0.20106892  0.51465826]\n",
      "[ episode 221 ][ timestamp 14 ] state=[-0.19704705 -0.20922928  0.20106892  0.51465826], action=1, reward=-1.0, next_state=[-0.20123164 -0.01742204  0.21136209  0.29146048]\n",
      "[ Ended! ] Episode 221: Exploration_rate=0.33195389135223546. Score=14.\n",
      "[ Experience replay ] starts\n",
      "[ episode 222 ] state=[ 0.01692828 -0.01809465 -0.03060311  0.00055529]\n",
      "[ episode 222 ][ timestamp 1 ] state=[ 0.01692828 -0.01809465 -0.03060311  0.00055529], action=0, reward=1.0, next_state=[ 0.01656639 -0.21276465 -0.030592    0.28342765]\n",
      "[ episode 222 ][ timestamp 2 ] state=[ 0.01656639 -0.21276465 -0.030592    0.28342765], action=0, reward=1.0, next_state=[ 0.01231109 -0.40743721 -0.02492345  0.56630721]\n",
      "[ episode 222 ][ timestamp 3 ] state=[ 0.01231109 -0.40743721 -0.02492345  0.56630721], action=0, reward=1.0, next_state=[ 0.00416235 -0.60220081 -0.0135973   0.85103516]\n",
      "[ episode 222 ][ timestamp 4 ] state=[ 0.00416235 -0.60220081 -0.0135973   0.85103516], action=1, reward=1.0, next_state=[-0.00788167 -0.40689613  0.0034234   0.55410773]\n",
      "[ episode 222 ][ timestamp 5 ] state=[-0.00788167 -0.40689613  0.0034234   0.55410773], action=1, reward=1.0, next_state=[-0.01601959 -0.21182242  0.01450555  0.26250535]\n",
      "[ episode 222 ][ timestamp 6 ] state=[-0.01601959 -0.21182242  0.01450555  0.26250535], action=0, reward=1.0, next_state=[-0.02025604 -0.40714839  0.01975566  0.55972797]\n",
      "[ episode 222 ][ timestamp 7 ] state=[-0.02025604 -0.40714839  0.01975566  0.55972797], action=0, reward=1.0, next_state=[-0.02839901 -0.60254197  0.03095022  0.85856894]\n",
      "[ episode 222 ][ timestamp 8 ] state=[-0.02839901 -0.60254197  0.03095022  0.85856894], action=1, reward=1.0, next_state=[-0.04044985 -0.407855    0.0481216   0.57577625]\n",
      "[ episode 222 ][ timestamp 9 ] state=[-0.04044985 -0.407855    0.0481216   0.57577625], action=0, reward=1.0, next_state=[-0.04860695 -0.60361732  0.05963712  0.88322209]\n",
      "[ episode 222 ][ timestamp 10 ] state=[-0.04860695 -0.60361732  0.05963712  0.88322209], action=1, reward=1.0, next_state=[-0.06067929 -0.40935376  0.07730157  0.6098677 ]\n",
      "[ episode 222 ][ timestamp 11 ] state=[-0.06067929 -0.40935376  0.07730157  0.6098677 ], action=0, reward=1.0, next_state=[-0.06886637 -0.60546631  0.08949892  0.9258621 ]\n",
      "[ episode 222 ][ timestamp 12 ] state=[-0.06886637 -0.60546631  0.08949892  0.9258621 ], action=0, reward=1.0, next_state=[-0.08097569 -0.80167553  0.10801616  1.24527554]\n",
      "[ episode 222 ][ timestamp 13 ] state=[-0.08097569 -0.80167553  0.10801616  1.24527554], action=0, reward=1.0, next_state=[-0.0970092  -0.99800451  0.13292167  1.56974772]\n",
      "[ episode 222 ][ timestamp 14 ] state=[-0.0970092  -0.99800451  0.13292167  1.56974772], action=1, reward=1.0, next_state=[-0.11696929 -0.80469631  0.16431663  1.3213072 ]\n",
      "[ episode 222 ][ timestamp 15 ] state=[-0.11696929 -0.80469631  0.16431663  1.3213072 ], action=1, reward=1.0, next_state=[-0.13306322 -0.61198739  0.19074277  1.0842294 ]\n",
      "[ episode 222 ][ timestamp 16 ] state=[-0.13306322 -0.61198739  0.19074277  1.0842294 ], action=1, reward=-1.0, next_state=[-0.14530297 -0.41982307  0.21242736  0.85694958]\n",
      "[ Ended! ] Episode 222: Exploration_rate=0.3302941218954743. Score=16.\n",
      "[ Experience replay ] starts\n",
      "[ episode 223 ] state=[ 0.00203269 -0.0416713   0.00070437 -0.01157174]\n",
      "[ episode 223 ][ timestamp 1 ] state=[ 0.00203269 -0.0416713   0.00070437 -0.01157174], action=0, reward=1.0, next_state=[ 0.00119926 -0.23680335  0.00047294  0.28133334]\n",
      "[ episode 223 ][ timestamp 2 ] state=[ 0.00119926 -0.23680335  0.00047294  0.28133334], action=0, reward=1.0, next_state=[-0.0035368  -0.43193204  0.00609961  0.57416539]\n",
      "[ episode 223 ][ timestamp 3 ] state=[-0.0035368  -0.43193204  0.00609961  0.57416539], action=0, reward=1.0, next_state=[-0.01217545 -0.62713897  0.01758291  0.86876362]\n",
      "[ episode 223 ][ timestamp 4 ] state=[-0.01217545 -0.62713897  0.01758291  0.86876362], action=1, reward=1.0, next_state=[-0.02471822 -0.4322606   0.03495819  0.58166035]\n",
      "[ episode 223 ][ timestamp 5 ] state=[-0.02471822 -0.4322606   0.03495819  0.58166035], action=0, reward=1.0, next_state=[-0.03336344 -0.62785447  0.04659139  0.88514752]\n",
      "[ episode 223 ][ timestamp 6 ] state=[-0.03336344 -0.62785447  0.04659139  0.88514752], action=1, reward=1.0, next_state=[-0.04592053 -0.43339501  0.06429434  0.60746777]\n",
      "[ episode 223 ][ timestamp 7 ] state=[-0.04592053 -0.43339501  0.06429434  0.60746777], action=0, reward=1.0, next_state=[-0.05458843 -0.62935417  0.0764437   0.91968871]\n",
      "[ episode 223 ][ timestamp 8 ] state=[-0.05458843 -0.62935417  0.0764437   0.91968871], action=1, reward=1.0, next_state=[-0.06717551 -0.43534404  0.09483747  0.65197596]\n",
      "[ episode 223 ][ timestamp 9 ] state=[-0.06717551 -0.43534404  0.09483747  0.65197596], action=0, reward=1.0, next_state=[-0.07588239 -0.63164989  0.10787699  0.97295196]\n",
      "[ episode 223 ][ timestamp 10 ] state=[-0.07588239 -0.63164989  0.10787699  0.97295196], action=1, reward=1.0, next_state=[-0.08851539 -0.43812779  0.12733603  0.71601061]\n",
      "[ episode 223 ][ timestamp 11 ] state=[-0.08851539 -0.43812779  0.12733603  0.71601061], action=1, reward=1.0, next_state=[-0.09727794 -0.24497675  0.14165624  0.46596547]\n",
      "[ episode 223 ][ timestamp 12 ] state=[-0.09727794 -0.24497675  0.14165624  0.46596547], action=1, reward=1.0, next_state=[-0.10217748 -0.05211068  0.15097555  0.2210719 ]\n",
      "[ episode 223 ][ timestamp 13 ] state=[-0.10217748 -0.05211068  0.15097555  0.2210719 ], action=0, reward=1.0, next_state=[-0.10321969 -0.24903197  0.15539699  0.55731221]\n",
      "[ episode 223 ][ timestamp 14 ] state=[-0.10321969 -0.24903197  0.15539699  0.55731221], action=0, reward=1.0, next_state=[-0.10820033 -0.44595478  0.16654324  0.89464015]\n",
      "[ episode 223 ][ timestamp 15 ] state=[-0.10820033 -0.44595478  0.16654324  0.89464015], action=0, reward=1.0, next_state=[-0.11711943 -0.64289582  0.18443604  1.23470199]\n",
      "[ episode 223 ][ timestamp 16 ] state=[-0.11711943 -0.64289582  0.18443604  1.23470199], action=1, reward=1.0, next_state=[-0.12997734 -0.45055994  0.20913008  1.00500854]\n",
      "[ episode 223 ][ timestamp 17 ] state=[-0.12997734 -0.45055994  0.20913008  1.00500854], action=1, reward=-1.0, next_state=[-0.13898854 -0.25875088  0.22923025  0.78460073]\n",
      "[ Ended! ] Episode 223: Exploration_rate=0.32864265128599696. Score=17.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 224 ] state=[-0.04304758  0.0377657   0.02964109 -0.00928898]\n",
      "[ episode 224 ][ timestamp 1 ] state=[-0.04304758  0.0377657   0.02964109 -0.00928898], action=0, reward=1.0, next_state=[-0.04229226 -0.15776853  0.02945531  0.29259674]\n",
      "[ episode 224 ][ timestamp 2 ] state=[-0.04229226 -0.15776853  0.02945531  0.29259674], action=1, reward=1.0, next_state=[-0.04544763  0.03692134  0.03530724  0.00934722]\n",
      "[ episode 224 ][ timestamp 3 ] state=[-0.04544763  0.03692134  0.03530724  0.00934722], action=0, reward=1.0, next_state=[-0.04470921 -0.15868871  0.03549419  0.3129576 ]\n",
      "[ episode 224 ][ timestamp 4 ] state=[-0.04470921 -0.15868871  0.03549419  0.3129576 ], action=0, reward=1.0, next_state=[-0.04788298 -0.35429786  0.04175334  0.61661962]\n",
      "[ episode 224 ][ timestamp 5 ] state=[-0.04788298 -0.35429786  0.04175334  0.61661962], action=1, reward=1.0, next_state=[-0.05496894 -0.15978333  0.05408573  0.33737403]\n",
      "[ episode 224 ][ timestamp 6 ] state=[-0.05496894 -0.15978333  0.05408573  0.33737403], action=0, reward=1.0, next_state=[-0.0581646  -0.35563156  0.06083321  0.64661025]\n",
      "[ episode 224 ][ timestamp 7 ] state=[-0.0581646  -0.35563156  0.06083321  0.64661025], action=0, reward=1.0, next_state=[-0.06527724 -0.55154601  0.07376542  0.95781227]\n",
      "[ episode 224 ][ timestamp 8 ] state=[-0.06527724 -0.55154601  0.07376542  0.95781227], action=1, reward=1.0, next_state=[-0.07630816 -0.35748929  0.09292166  0.68918615]\n",
      "[ episode 224 ][ timestamp 9 ] state=[-0.07630816 -0.35748929  0.09292166  0.68918615], action=1, reward=1.0, next_state=[-0.08345794 -0.16377136  0.10670539  0.4271425 ]\n",
      "[ episode 224 ][ timestamp 10 ] state=[-0.08345794 -0.16377136  0.10670539  0.4271425 ], action=0, reward=1.0, next_state=[-0.08673337 -0.36023002  0.11524824  0.7514663 ]\n",
      "[ episode 224 ][ timestamp 11 ] state=[-0.08673337 -0.36023002  0.11524824  0.7514663 ], action=1, reward=1.0, next_state=[-0.09393797 -0.16687003  0.13027756  0.4971584 ]\n",
      "[ episode 224 ][ timestamp 12 ] state=[-0.09393797 -0.16687003  0.13027756  0.4971584 ], action=0, reward=1.0, next_state=[-0.09727537 -0.36356507  0.14022073  0.82789408]\n",
      "[ episode 224 ][ timestamp 13 ] state=[-0.09727537 -0.36356507  0.14022073  0.82789408], action=1, reward=1.0, next_state=[-0.10454667 -0.17061016  0.15677861  0.58239237]\n",
      "[ episode 224 ][ timestamp 14 ] state=[-0.10454667 -0.17061016  0.15677861  0.58239237], action=1, reward=1.0, next_state=[-0.10795887  0.02200809  0.16842646  0.3429129 ]\n",
      "[ episode 224 ][ timestamp 15 ] state=[-0.10795887  0.02200809  0.16842646  0.3429129 ], action=0, reward=1.0, next_state=[-0.10751871 -0.17505949  0.17528472  0.68361505]\n",
      "[ episode 224 ][ timestamp 16 ] state=[-0.10751871 -0.17505949  0.17528472  0.68361505], action=1, reward=1.0, next_state=[-0.1110199   0.01725137  0.18895702  0.45083916]\n",
      "[ episode 224 ][ timestamp 17 ] state=[-0.1110199   0.01725137  0.18895702  0.45083916], action=1, reward=1.0, next_state=[-0.11067488  0.20926898  0.1979738   0.22316279]\n",
      "[ episode 224 ][ timestamp 18 ] state=[-0.11067488  0.20926898  0.1979738   0.22316279], action=0, reward=1.0, next_state=[-0.1064895   0.01194948  0.20243706  0.57118554]\n",
      "[ episode 224 ][ timestamp 19 ] state=[-0.1064895   0.01194948  0.20243706  0.57118554], action=1, reward=-1.0, next_state=[-0.10625051  0.20374356  0.21386077  0.34848005]\n",
      "[ Ended! ] Episode 224: Exploration_rate=0.326999438029567. Score=19.\n",
      "[ Experience replay ] starts\n",
      "[ episode 225 ] state=[-0.0159128  -0.02188011 -0.02001048  0.04966652]\n",
      "[ episode 225 ][ timestamp 1 ] state=[-0.0159128  -0.02188011 -0.02001048  0.04966652], action=1, reward=1.0, next_state=[-0.0163504   0.17352298 -0.01901715 -0.24926211]\n",
      "[ episode 225 ][ timestamp 2 ] state=[-0.0163504   0.17352298 -0.01901715 -0.24926211], action=0, reward=1.0, next_state=[-0.01287995 -0.0213223  -0.02400239  0.03736226]\n",
      "[ episode 225 ][ timestamp 3 ] state=[-0.01287995 -0.0213223  -0.02400239  0.03736226], action=0, reward=1.0, next_state=[-0.01330639 -0.21609198 -0.02325515  0.32237659]\n",
      "[ episode 225 ][ timestamp 4 ] state=[-0.01330639 -0.21609198 -0.02325515  0.32237659], action=1, reward=1.0, next_state=[-0.01762823 -0.02064673 -0.01680762  0.02245158]\n",
      "[ episode 225 ][ timestamp 5 ] state=[-0.01762823 -0.02064673 -0.01680762  0.02245158], action=0, reward=1.0, next_state=[-0.01804117 -0.21552366 -0.01635858  0.30978449]\n",
      "[ episode 225 ][ timestamp 6 ] state=[-0.01804117 -0.21552366 -0.01635858  0.30978449], action=1, reward=1.0, next_state=[-0.02235164 -0.0201725  -0.0101629   0.01198775]\n",
      "[ episode 225 ][ timestamp 7 ] state=[-0.02235164 -0.0201725  -0.0101629   0.01198775], action=0, reward=1.0, next_state=[-0.02275509 -0.21514724 -0.00992314  0.30144691]\n",
      "[ episode 225 ][ timestamp 8 ] state=[-0.02275509 -0.21514724 -0.00992314  0.30144691], action=1, reward=1.0, next_state=[-0.02705803 -0.01988527 -0.0038942   0.00565102]\n",
      "[ episode 225 ][ timestamp 9 ] state=[-0.02705803 -0.01988527 -0.0038942   0.00565102], action=0, reward=1.0, next_state=[-0.02745574 -0.21495116 -0.00378118  0.29710274]\n",
      "[ episode 225 ][ timestamp 10 ] state=[-0.02745574 -0.21495116 -0.00378118  0.29710274], action=1, reward=1.0, next_state=[-0.03175476 -0.01977551  0.00216087  0.0032297 ]\n",
      "[ episode 225 ][ timestamp 11 ] state=[-0.03175476 -0.01977551  0.00216087  0.0032297 ], action=1, reward=1.0, next_state=[-0.03215027  0.17531538  0.00222547 -0.28877066]\n",
      "[ episode 225 ][ timestamp 12 ] state=[-0.03215027  0.17531538  0.00222547 -0.28877066], action=0, reward=1.0, next_state=[-0.02864396 -0.01983823 -0.00354995  0.00461332]\n",
      "[ episode 225 ][ timestamp 13 ] state=[-0.02864396 -0.01983823 -0.00354995  0.00461332], action=1, reward=1.0, next_state=[-0.02904073  0.17533445 -0.00345768 -0.28918754]\n",
      "[ episode 225 ][ timestamp 14 ] state=[-0.02904073  0.17533445 -0.00345768 -0.28918754], action=0, reward=1.0, next_state=[-0.02553404 -0.01973802 -0.00924143  0.00240287]\n",
      "[ episode 225 ][ timestamp 15 ] state=[-0.02553404 -0.01973802 -0.00924143  0.00240287], action=0, reward=1.0, next_state=[-0.0259288  -0.21472623 -0.00919337  0.29215574]\n",
      "[ episode 225 ][ timestamp 16 ] state=[-0.0259288  -0.21472623 -0.00919337  0.29215574], action=0, reward=1.0, next_state=[-0.03022333 -0.4097159  -0.00335026  0.58192508]\n",
      "[ episode 225 ][ timestamp 17 ] state=[-0.03022333 -0.4097159  -0.00335026  0.58192508], action=0, reward=1.0, next_state=[-0.03841764 -0.60479075  0.00828824  0.87355074]\n",
      "[ episode 225 ][ timestamp 18 ] state=[-0.03841764 -0.60479075  0.00828824  0.87355074], action=1, reward=1.0, next_state=[-0.05051346 -0.40978247  0.02575926  0.58348508]\n",
      "[ episode 225 ][ timestamp 19 ] state=[-0.05051346 -0.40978247  0.02575926  0.58348508], action=1, reward=1.0, next_state=[-0.05870911 -0.21503068  0.03742896  0.29902669]\n",
      "[ episode 225 ][ timestamp 20 ] state=[-0.05870911 -0.21503068  0.03742896  0.29902669], action=1, reward=1.0, next_state=[-0.06300972 -0.02046168  0.04340949  0.01837915]\n",
      "[ episode 225 ][ timestamp 21 ] state=[-0.06300972 -0.02046168  0.04340949  0.01837915], action=1, reward=1.0, next_state=[-0.06341896  0.17401171  0.04377708 -0.26029775]\n",
      "[ episode 225 ][ timestamp 22 ] state=[-0.06341896  0.17401171  0.04377708 -0.26029775], action=1, reward=1.0, next_state=[-0.05993872  0.36848228  0.03857112 -0.53885778]\n",
      "[ episode 225 ][ timestamp 23 ] state=[-0.05993872  0.36848228  0.03857112 -0.53885778], action=0, reward=1.0, next_state=[-0.05256908  0.17283992  0.02779397 -0.23427541]\n",
      "[ episode 225 ][ timestamp 24 ] state=[-0.05256908  0.17283992  0.02779397 -0.23427541], action=0, reward=1.0, next_state=[-0.04911228 -0.0226679   0.02310846  0.06704342]\n",
      "[ episode 225 ][ timestamp 25 ] state=[-0.04911228 -0.0226679   0.02310846  0.06704342], action=0, reward=1.0, next_state=[-0.04956563 -0.2181134   0.02444933  0.36692669]\n",
      "[ episode 225 ][ timestamp 26 ] state=[-0.04956563 -0.2181134   0.02444933  0.36692669], action=1, reward=1.0, next_state=[-0.0539279  -0.02334726  0.03178786  0.08205217]\n",
      "[ episode 225 ][ timestamp 27 ] state=[-0.0539279  -0.02334726  0.03178786  0.08205217], action=0, reward=1.0, next_state=[-0.05439485 -0.21891012  0.0334289   0.38459233]\n",
      "[ episode 225 ][ timestamp 28 ] state=[-0.05439485 -0.21891012  0.0334289   0.38459233], action=0, reward=1.0, next_state=[-0.05877305 -0.41449033  0.04112075  0.687625  ]\n",
      "[ episode 225 ][ timestamp 29 ] state=[-0.05877305 -0.41449033  0.04112075  0.687625  ], action=1, reward=1.0, next_state=[-0.06706286 -0.21996253  0.05487325  0.40816606]\n",
      "[ episode 225 ][ timestamp 30 ] state=[-0.06706286 -0.21996253  0.05487325  0.40816606], action=1, reward=1.0, next_state=[-0.07146211 -0.02565982  0.06303657  0.13327532]\n",
      "[ episode 225 ][ timestamp 31 ] state=[-0.07146211 -0.02565982  0.06303657  0.13327532], action=0, reward=1.0, next_state=[-0.0719753  -0.22162542  0.06570208  0.44516037]\n",
      "[ episode 225 ][ timestamp 32 ] state=[-0.0719753  -0.22162542  0.06570208  0.44516037], action=1, reward=1.0, next_state=[-0.07640781 -0.02749156  0.07460528  0.1738904 ]\n",
      "[ episode 225 ][ timestamp 33 ] state=[-0.07640781 -0.02749156  0.07460528  0.1738904 ], action=0, reward=1.0, next_state=[-0.07695764 -0.22359758  0.07808309  0.48914478]\n",
      "[ episode 225 ][ timestamp 34 ] state=[-0.07695764 -0.22359758  0.07808309  0.48914478], action=1, reward=1.0, next_state=[-0.0814296  -0.02965904  0.08786599  0.22205646]\n",
      "[ episode 225 ][ timestamp 35 ] state=[-0.0814296  -0.02965904  0.08786599  0.22205646], action=1, reward=1.0, next_state=[-0.08202278  0.1641043   0.09230712 -0.04166795]\n",
      "[ episode 225 ][ timestamp 36 ] state=[-0.08202278  0.1641043   0.09230712 -0.04166795], action=0, reward=1.0, next_state=[-0.07874069 -0.03221176  0.09147376  0.27865225]\n",
      "[ episode 225 ][ timestamp 37 ] state=[-0.07874069 -0.03221176  0.09147376  0.27865225], action=0, reward=1.0, next_state=[-0.07938493 -0.2285115   0.0970468   0.59872662]\n",
      "[ episode 225 ][ timestamp 38 ] state=[-0.07938493 -0.2285115   0.0970468   0.59872662], action=1, reward=1.0, next_state=[-0.08395516 -0.03487176  0.10902134  0.33812072]\n",
      "[ episode 225 ][ timestamp 39 ] state=[-0.08395516 -0.03487176  0.10902134  0.33812072], action=1, reward=1.0, next_state=[-0.08465259  0.15854362  0.11578375  0.08170891]\n",
      "[ episode 225 ][ timestamp 40 ] state=[-0.08465259  0.15854362  0.11578375  0.08170891], action=0, reward=1.0, next_state=[-0.08148172 -0.03803132  0.11741793  0.40856151]\n",
      "[ episode 225 ][ timestamp 41 ] state=[-0.08148172 -0.03803132  0.11741793  0.40856151], action=1, reward=1.0, next_state=[-0.08224234  0.15524711  0.12558916  0.15508171]\n",
      "[ episode 225 ][ timestamp 42 ] state=[-0.08224234  0.15524711  0.12558916  0.15508171], action=1, reward=1.0, next_state=[-0.0791374   0.34836804  0.12869079 -0.09549194]\n",
      "[ episode 225 ][ timestamp 43 ] state=[-0.0791374   0.34836804  0.12869079 -0.09549194], action=0, reward=1.0, next_state=[-0.07217004  0.15165898  0.12678095  0.23486244]\n",
      "[ episode 225 ][ timestamp 44 ] state=[-0.07217004  0.15165898  0.12678095  0.23486244], action=1, reward=1.0, next_state=[-0.06913686  0.34476304  0.1314782  -0.01529505]\n",
      "[ episode 225 ][ timestamp 45 ] state=[-0.06913686  0.34476304  0.1314782  -0.01529505], action=1, reward=1.0, next_state=[-0.0622416   0.53777835  0.1311723  -0.26377588]\n",
      "[ episode 225 ][ timestamp 46 ] state=[-0.0622416   0.53777835  0.1311723  -0.26377588], action=0, reward=1.0, next_state=[-0.05148603  0.34105184  0.12589678  0.067233  ]\n",
      "[ episode 225 ][ timestamp 47 ] state=[-0.05148603  0.34105184  0.12589678  0.067233  ], action=1, reward=1.0, next_state=[-0.044665    0.53416503  0.12724144 -0.18322824]\n",
      "[ episode 225 ][ timestamp 48 ] state=[-0.044665    0.53416503  0.12724144 -0.18322824], action=0, reward=1.0, next_state=[-0.0339817   0.3374738   0.12357688  0.14673158]\n",
      "[ episode 225 ][ timestamp 49 ] state=[-0.0339817   0.3374738   0.12357688  0.14673158], action=0, reward=1.0, next_state=[-0.02723222  0.14081878  0.12651151  0.4757038 ]\n",
      "[ episode 225 ][ timestamp 50 ] state=[-0.02723222  0.14081878  0.12651151  0.4757038 ], action=1, reward=1.0, next_state=[-0.02441585  0.33394855  0.13602559  0.2254196 ]\n",
      "[ episode 225 ][ timestamp 51 ] state=[-0.02441585  0.33394855  0.13602559  0.2254196 ], action=0, reward=1.0, next_state=[-0.01773687  0.13717127  0.14053398  0.55772732]\n",
      "[ episode 225 ][ timestamp 52 ] state=[-0.01773687  0.13717127  0.14053398  0.55772732], action=0, reward=1.0, next_state=[-0.01499345 -0.0596147   0.15168853  0.89117733]\n",
      "[ episode 225 ][ timestamp 53 ] state=[-0.01499345 -0.0596147   0.15168853  0.89117733], action=1, reward=1.0, next_state=[-0.01618574  0.13316004  0.16951207  0.64976117]\n",
      "[ episode 225 ][ timestamp 54 ] state=[-0.01618574  0.13316004  0.16951207  0.64976117], action=1, reward=1.0, next_state=[-0.01352254  0.32556605  0.18250729  0.41488695]\n",
      "[ episode 225 ][ timestamp 55 ] state=[-0.01352254  0.32556605  0.18250729  0.41488695], action=0, reward=1.0, next_state=[-0.00701122  0.12839015  0.19080503  0.75909844]\n",
      "[ episode 225 ][ timestamp 56 ] state=[-0.00701122  0.12839015  0.19080503  0.75909844], action=1, reward=1.0, next_state=[-0.00444342  0.3204428   0.205987    0.53200448]\n",
      "[ episode 225 ][ timestamp 57 ] state=[-0.00444342  0.3204428   0.205987    0.53200448], action=1, reward=-1.0, next_state=[0.00196544 0.51216307 0.21662709 0.31063646]\n",
      "[ Ended! ] Episode 225: Exploration_rate=0.3253644408394192. Score=57.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 226 ] state=[ 0.04817374 -0.02906303  0.01302784  0.04200233]\n",
      "[ episode 226 ][ timestamp 1 ] state=[ 0.04817374 -0.02906303  0.01302784  0.04200233], action=0, reward=1.0, next_state=[ 0.04759248 -0.22436935  0.01386789  0.33876702]\n",
      "[ episode 226 ][ timestamp 2 ] state=[ 0.04759248 -0.22436935  0.01386789  0.33876702], action=1, reward=1.0, next_state=[ 0.0431051  -0.02944745  0.02064323  0.05048931]\n",
      "[ episode 226 ][ timestamp 3 ] state=[ 0.0431051  -0.02944745  0.02064323  0.05048931], action=1, reward=1.0, next_state=[ 0.04251615  0.16537251  0.02165301 -0.23560968]\n",
      "[ episode 226 ][ timestamp 4 ] state=[ 0.04251615  0.16537251  0.02165301 -0.23560968], action=1, reward=1.0, next_state=[ 0.0458236   0.36017851  0.01694082 -0.5213847 ]\n",
      "[ episode 226 ][ timestamp 5 ] state=[ 0.0458236   0.36017851  0.01694082 -0.5213847 ], action=0, reward=1.0, next_state=[ 0.05302717  0.16482225  0.00651313 -0.22341199]\n",
      "[ episode 226 ][ timestamp 6 ] state=[ 0.05302717  0.16482225  0.00651313 -0.22341199], action=1, reward=1.0, next_state=[ 0.05632361  0.3598505   0.00204489 -0.51403332]\n",
      "[ episode 226 ][ timestamp 7 ] state=[ 0.05632361  0.3598505   0.00204489 -0.51403332], action=0, reward=1.0, next_state=[ 0.06352062  0.16469981 -0.00823578 -0.2207067 ]\n",
      "[ episode 226 ][ timestamp 8 ] state=[ 0.06352062  0.16469981 -0.00823578 -0.2207067 ], action=1, reward=1.0, next_state=[ 0.06681462  0.35993851 -0.01264992 -0.51597611]\n",
      "[ episode 226 ][ timestamp 9 ] state=[ 0.06681462  0.35993851 -0.01264992 -0.51597611], action=0, reward=1.0, next_state=[ 0.07401339  0.16499696 -0.02296944 -0.22730615]\n",
      "[ episode 226 ][ timestamp 10 ] state=[ 0.07401339  0.16499696 -0.02296944 -0.22730615], action=1, reward=1.0, next_state=[ 0.07731333  0.36043951 -0.02751556 -0.52714506]\n",
      "[ episode 226 ][ timestamp 11 ] state=[ 0.07731333  0.36043951 -0.02751556 -0.52714506], action=0, reward=1.0, next_state=[ 0.08452212  0.1657153  -0.03805846 -0.24325786]\n",
      "[ episode 226 ][ timestamp 12 ] state=[ 0.08452212  0.1657153  -0.03805846 -0.24325786], action=1, reward=1.0, next_state=[ 0.08783642  0.36135961 -0.04292362 -0.54769831]\n",
      "[ episode 226 ][ timestamp 13 ] state=[ 0.08783642  0.36135961 -0.04292362 -0.54769831], action=1, reward=1.0, next_state=[ 0.09506362  0.55705747 -0.05387759 -0.85359039]\n",
      "[ episode 226 ][ timestamp 14 ] state=[ 0.09506362  0.55705747 -0.05387759 -0.85359039], action=0, reward=1.0, next_state=[ 0.10620477  0.36270967 -0.07094939 -0.57832405]\n",
      "[ episode 226 ][ timestamp 15 ] state=[ 0.10620477  0.36270967 -0.07094939 -0.57832405], action=0, reward=1.0, next_state=[ 0.11345896  0.16865005 -0.08251587 -0.30880858]\n",
      "[ episode 226 ][ timestamp 16 ] state=[ 0.11345896  0.16865005 -0.08251587 -0.30880858], action=1, reward=1.0, next_state=[ 0.11683196  0.36484484 -0.08869205 -0.62633159]\n",
      "[ episode 226 ][ timestamp 17 ] state=[ 0.11683196  0.36484484 -0.08869205 -0.62633159], action=0, reward=1.0, next_state=[ 0.12412886  0.17106555 -0.10121868 -0.36284643]\n",
      "[ episode 226 ][ timestamp 18 ] state=[ 0.12412886  0.17106555 -0.10121868 -0.36284643], action=1, reward=1.0, next_state=[ 0.12755017  0.36746952 -0.10847561 -0.68565202]\n",
      "[ episode 226 ][ timestamp 19 ] state=[ 0.12755017  0.36746952 -0.10847561 -0.68565202], action=0, reward=1.0, next_state=[ 0.13489956  0.17400735 -0.12218865 -0.42899377]\n",
      "[ episode 226 ][ timestamp 20 ] state=[ 0.13489956  0.17400735 -0.12218865 -0.42899377], action=0, reward=1.0, next_state=[ 0.13837971 -0.0191915  -0.13076852 -0.17719029]\n",
      "[ episode 226 ][ timestamp 21 ] state=[ 0.13837971 -0.0191915  -0.13076852 -0.17719029], action=1, reward=1.0, next_state=[ 0.13799588  0.17753581 -0.13431233 -0.50809823]\n",
      "[ episode 226 ][ timestamp 22 ] state=[ 0.13799588  0.17753581 -0.13431233 -0.50809823], action=0, reward=1.0, next_state=[ 0.14154659 -0.01546349 -0.14447429 -0.26057581]\n",
      "[ episode 226 ][ timestamp 23 ] state=[ 0.14154659 -0.01546349 -0.14447429 -0.26057581], action=1, reward=1.0, next_state=[ 0.14123732  0.18139362 -0.14968581 -0.59511294]\n",
      "[ episode 226 ][ timestamp 24 ] state=[ 0.14123732  0.18139362 -0.14968581 -0.59511294], action=0, reward=1.0, next_state=[ 0.14486519 -0.01135143 -0.16158807 -0.35307176]\n",
      "[ episode 226 ][ timestamp 25 ] state=[ 0.14486519 -0.01135143 -0.16158807 -0.35307176], action=1, reward=1.0, next_state=[ 0.14463817  0.18565511 -0.1686495  -0.6920324 ]\n",
      "[ episode 226 ][ timestamp 26 ] state=[ 0.14463817  0.18565511 -0.1686495  -0.6920324 ], action=0, reward=1.0, next_state=[ 0.14835127 -0.00677549 -0.18249015 -0.45682995]\n",
      "[ episode 226 ][ timestamp 27 ] state=[ 0.14835127 -0.00677549 -0.18249015 -0.45682995], action=1, reward=1.0, next_state=[ 0.14821576  0.19039382 -0.19162675 -0.80102768]\n",
      "[ episode 226 ][ timestamp 28 ] state=[ 0.14821576  0.19039382 -0.19162675 -0.80102768], action=0, reward=1.0, next_state=[ 0.15202364 -0.0016562  -0.2076473  -0.57421975]\n",
      "[ episode 226 ][ timestamp 29 ] state=[ 0.15202364 -0.0016562  -0.2076473  -0.57421975], action=1, reward=-1.0, next_state=[ 0.15199051  0.19567885 -0.2191317  -0.92447434]\n",
      "[ Ended! ] Episode 226: Exploration_rate=0.3237376186352221. Score=29.\n",
      "[ Experience replay ] starts\n",
      "[ episode 227 ] state=[-0.00799127 -0.04159598 -0.01770179 -0.00688262]\n",
      "[ episode 227 ][ timestamp 1 ] state=[-0.00799127 -0.04159598 -0.01770179 -0.00688262], action=0, reward=1.0, next_state=[-0.00882319 -0.23645964 -0.01783944  0.28016303]\n",
      "[ episode 227 ][ timestamp 2 ] state=[-0.00882319 -0.23645964 -0.01783944  0.28016303], action=1, reward=1.0, next_state=[-0.01355239 -0.04108782 -0.01223618 -0.01809259]\n",
      "[ episode 227 ][ timestamp 3 ] state=[-0.01355239 -0.04108782 -0.01223618 -0.01809259], action=0, reward=1.0, next_state=[-0.01437414 -0.23603217 -0.01259804  0.2707047 ]\n",
      "[ episode 227 ][ timestamp 4 ] state=[-0.01437414 -0.23603217 -0.01259804  0.2707047 ], action=1, reward=1.0, next_state=[-0.01909479 -0.04073273 -0.00718394 -0.02592494]\n",
      "[ episode 227 ][ timestamp 5 ] state=[-0.01909479 -0.04073273 -0.00718394 -0.02592494], action=0, reward=1.0, next_state=[-0.01990944 -0.23575093 -0.00770244  0.26448275]\n",
      "[ episode 227 ][ timestamp 6 ] state=[-0.01990944 -0.23575093 -0.00770244  0.26448275], action=1, reward=1.0, next_state=[-0.02462446 -0.04051989 -0.00241278 -0.03061962]\n",
      "[ episode 227 ][ timestamp 7 ] state=[-0.02462446 -0.04051989 -0.00241278 -0.03061962], action=0, reward=1.0, next_state=[-0.02543486 -0.23560716 -0.00302518  0.26130107]\n",
      "[ episode 227 ][ timestamp 8 ] state=[-0.02543486 -0.23560716 -0.00302518  0.26130107], action=1, reward=1.0, next_state=[-0.030147   -0.04044215  0.00220084 -0.03233449]\n",
      "[ episode 227 ][ timestamp 9 ] state=[-0.030147   -0.04044215  0.00220084 -0.03233449], action=0, reward=1.0, next_state=[-0.03095584 -0.2355956   0.00155415  0.26104201]\n",
      "[ episode 227 ][ timestamp 10 ] state=[-0.03095584 -0.2355956   0.00155415  0.26104201], action=1, reward=1.0, next_state=[-0.03566776 -0.04049587  0.00677499 -0.03115031]\n",
      "[ episode 227 ][ timestamp 11 ] state=[-0.03566776 -0.04049587  0.00677499 -0.03115031], action=0, reward=1.0, next_state=[-0.03647767 -0.23571432  0.00615199  0.26366247]\n",
      "[ episode 227 ][ timestamp 12 ] state=[-0.03647767 -0.23571432  0.00615199  0.26366247], action=1, reward=1.0, next_state=[-0.04119196 -0.04068071  0.01142524 -0.02707372]\n",
      "[ episode 227 ][ timestamp 13 ] state=[-0.04119196 -0.04068071  0.01142524 -0.02707372], action=0, reward=1.0, next_state=[-0.04200557 -0.23596463  0.01088376  0.26919199]\n",
      "[ episode 227 ][ timestamp 14 ] state=[-0.04200557 -0.23596463  0.01088376  0.26919199], action=1, reward=1.0, next_state=[-0.04672487 -0.04099968  0.0162676  -0.02003836]\n",
      "[ episode 227 ][ timestamp 15 ] state=[-0.04672487 -0.04099968  0.0162676  -0.02003836], action=1, reward=1.0, next_state=[-0.04754486  0.15388525  0.01586684 -0.3075446 ]\n",
      "[ episode 227 ][ timestamp 16 ] state=[-0.04754486  0.15388525  0.01586684 -0.3075446 ], action=0, reward=1.0, next_state=[-0.04446715 -0.04145916  0.00971594 -0.00990023]\n",
      "[ episode 227 ][ timestamp 17 ] state=[-0.04446715 -0.04145916  0.00971594 -0.00990023], action=0, reward=1.0, next_state=[-0.04529634 -0.23671909  0.00951794  0.28583229]\n",
      "[ episode 227 ][ timestamp 18 ] state=[-0.04529634 -0.23671909  0.00951794  0.28583229], action=1, reward=1.0, next_state=[-0.05003072 -0.04173417  0.01523458 -0.00383362]\n",
      "[ episode 227 ][ timestamp 19 ] state=[-0.05003072 -0.04173417  0.01523458 -0.00383362], action=0, reward=1.0, next_state=[-0.0508654  -0.23707125  0.01515791  0.29361681]\n",
      "[ episode 227 ][ timestamp 20 ] state=[-0.0508654  -0.23707125  0.01515791  0.29361681], action=1, reward=1.0, next_state=[-0.05560683 -0.04216866  0.02103025  0.00575275]\n",
      "[ episode 227 ][ timestamp 21 ] state=[-0.05560683 -0.04216866  0.02103025  0.00575275], action=0, reward=1.0, next_state=[-0.0564502  -0.2375858   0.0211453   0.30499609]\n",
      "[ episode 227 ][ timestamp 22 ] state=[-0.0564502  -0.2375858   0.0211453   0.30499609], action=1, reward=1.0, next_state=[-0.06120192 -0.04277147  0.02724523  0.01905617]\n",
      "[ episode 227 ][ timestamp 23 ] state=[-0.06120192 -0.04277147  0.02724523  0.01905617], action=0, reward=1.0, next_state=[-0.06205735 -0.23827334  0.02762635  0.32020925]\n",
      "[ episode 227 ][ timestamp 24 ] state=[-0.06205735 -0.23827334  0.02762635  0.32020925], action=1, reward=1.0, next_state=[-0.06682281 -0.04355549  0.03403053  0.03636505]\n",
      "[ episode 227 ][ timestamp 25 ] state=[-0.06682281 -0.04355549  0.03403053  0.03636505], action=0, reward=1.0, next_state=[-0.06769392 -0.2391485   0.03475783  0.33958774]\n",
      "[ episode 227 ][ timestamp 26 ] state=[-0.06769392 -0.2391485   0.03475783  0.33958774], action=1, reward=1.0, next_state=[-0.07247689 -0.04453792  0.04154959  0.05806492]\n",
      "[ episode 227 ][ timestamp 27 ] state=[-0.07247689 -0.04453792  0.04154959  0.05806492], action=0, reward=1.0, next_state=[-0.07336765 -0.24023022  0.04271089  0.3635621 ]\n",
      "[ episode 227 ][ timestamp 28 ] state=[-0.07336765 -0.24023022  0.04271089  0.3635621 ], action=1, reward=1.0, next_state=[-0.07817226 -0.0457405   0.04998213  0.08464676]\n",
      "[ episode 227 ][ timestamp 29 ] state=[-0.07817226 -0.0457405   0.04998213  0.08464676], action=0, reward=1.0, next_state=[-0.07908707 -0.24154196  0.05167507  0.39267079]\n",
      "[ episode 227 ][ timestamp 30 ] state=[-0.07908707 -0.24154196  0.05167507  0.39267079], action=1, reward=1.0, next_state=[-0.08391791 -0.04718996  0.05952848  0.11671765]\n",
      "[ episode 227 ][ timestamp 31 ] state=[-0.08391791 -0.04718996  0.05952848  0.11671765], action=1, reward=1.0, next_state=[-0.0848617   0.14703077  0.06186283 -0.15660638]\n",
      "[ episode 227 ][ timestamp 32 ] state=[-0.0848617   0.14703077  0.06186283 -0.15660638], action=1, reward=1.0, next_state=[-0.08192109  0.34121495  0.05873071 -0.4291494 ]\n",
      "[ episode 227 ][ timestamp 33 ] state=[-0.08192109  0.34121495  0.05873071 -0.4291494 ], action=0, reward=1.0, next_state=[-0.07509679  0.14531259  0.05014772 -0.11854559]\n",
      "[ episode 227 ][ timestamp 34 ] state=[-0.07509679  0.14531259  0.05014772 -0.11854559], action=0, reward=1.0, next_state=[-0.07219054 -0.05049067  0.04777681  0.18952731]\n",
      "[ episode 227 ][ timestamp 35 ] state=[-0.07219054 -0.05049067  0.04777681  0.18952731], action=1, reward=1.0, next_state=[-0.07320035  0.14391637  0.05156735 -0.08770944]\n",
      "[ episode 227 ][ timestamp 36 ] state=[-0.07320035  0.14391637  0.05156735 -0.08770944], action=0, reward=1.0, next_state=[-0.07032202 -0.05190537  0.04981316  0.22078679]\n",
      "[ episode 227 ][ timestamp 37 ] state=[-0.07032202 -0.05190537  0.04981316  0.22078679], action=0, reward=1.0, next_state=[-0.07136013 -0.24770265  0.0542289   0.52875741]\n",
      "[ episode 227 ][ timestamp 38 ] state=[-0.07136013 -0.24770265  0.0542289   0.52875741], action=1, reward=1.0, next_state=[-0.07631418 -0.05338388  0.06480405  0.25364322]\n",
      "[ episode 227 ][ timestamp 39 ] state=[-0.07631418 -0.05338388  0.06480405  0.25364322], action=1, reward=1.0, next_state=[-0.07738186  0.14075578  0.06987691 -0.01791594]\n",
      "[ episode 227 ][ timestamp 40 ] state=[-0.07738186  0.14075578  0.06987691 -0.01791594], action=0, reward=1.0, next_state=[-0.07456675 -0.0552951   0.06951859  0.29596981]\n",
      "[ episode 227 ][ timestamp 41 ] state=[-0.07456675 -0.0552951   0.06951859  0.29596981], action=1, reward=1.0, next_state=[-0.07567265  0.13877048  0.07543799  0.02599657]\n",
      "[ episode 227 ][ timestamp 42 ] state=[-0.07567265  0.13877048  0.07543799  0.02599657], action=0, reward=1.0, next_state=[-0.07289724 -0.0573477   0.07595792  0.34149492]\n",
      "[ episode 227 ][ timestamp 43 ] state=[-0.07289724 -0.0573477   0.07595792  0.34149492], action=1, reward=1.0, next_state=[-0.07404419  0.136616    0.08278782  0.07369845]\n",
      "[ episode 227 ][ timestamp 44 ] state=[-0.07404419  0.136616    0.08278782  0.07369845], action=1, reward=1.0, next_state=[-0.07131187  0.33045951  0.08426179 -0.19175914]\n",
      "[ episode 227 ][ timestamp 45 ] state=[-0.07131187  0.33045951  0.08426179 -0.19175914], action=1, reward=1.0, next_state=[-0.06470268  0.5242813   0.08042661 -0.45671667]\n",
      "[ episode 227 ][ timestamp 46 ] state=[-0.06470268  0.5242813   0.08042661 -0.45671667], action=0, reward=1.0, next_state=[-0.05421706  0.32811988  0.07129227 -0.13980572]\n",
      "[ episode 227 ][ timestamp 47 ] state=[-0.05421706  0.32811988  0.07129227 -0.13980572], action=0, reward=1.0, next_state=[-0.04765466  0.1320531   0.06849616  0.17448954]\n",
      "[ episode 227 ][ timestamp 48 ] state=[-0.04765466  0.1320531   0.06849616  0.17448954], action=1, reward=1.0, next_state=[-0.0450136   0.3261313   0.07198595 -0.09582297]\n",
      "[ episode 227 ][ timestamp 49 ] state=[-0.0450136   0.3261313   0.07198595 -0.09582297], action=0, reward=1.0, next_state=[-0.03849097  0.13005539  0.07006949  0.21867477]\n",
      "[ episode 227 ][ timestamp 50 ] state=[-0.03849097  0.13005539  0.07006949  0.21867477], action=1, reward=1.0, next_state=[-0.03588986  0.32410939  0.07444298 -0.05110839]\n",
      "[ episode 227 ][ timestamp 51 ] state=[-0.03588986  0.32410939  0.07444298 -0.05110839], action=0, reward=1.0, next_state=[-0.02940768  0.12800332  0.07342082  0.26410204]\n",
      "[ episode 227 ][ timestamp 52 ] state=[-0.02940768  0.12800332  0.07342082  0.26410204], action=1, reward=1.0, next_state=[-0.02684761  0.3220047   0.07870286 -0.0045497 ]\n",
      "[ episode 227 ][ timestamp 53 ] state=[-0.02684761  0.3220047   0.07870286 -0.0045497 ], action=0, reward=1.0, next_state=[-0.02040752  0.12584741  0.07861186  0.31189019]\n",
      "[ episode 227 ][ timestamp 54 ] state=[-0.02040752  0.12584741  0.07861186  0.31189019], action=1, reward=1.0, next_state=[-0.01789057  0.31976654  0.08484967  0.04499792]\n",
      "[ episode 227 ][ timestamp 55 ] state=[-0.01789057  0.31976654  0.08484967  0.04499792], action=1, reward=1.0, next_state=[-0.01149524  0.51357577  0.08574963 -0.21975418]\n",
      "[ episode 227 ][ timestamp 56 ] state=[-0.01149524  0.51357577  0.08574963 -0.21975418], action=0, reward=1.0, next_state=[-0.00122372  0.31733942  0.08135454  0.09869832]\n",
      "[ episode 227 ][ timestamp 57 ] state=[-0.00122372  0.31733942  0.08135454  0.09869832], action=1, reward=1.0, next_state=[ 0.00512307  0.51120686  0.08332851 -0.16724917]\n",
      "[ episode 227 ][ timestamp 58 ] state=[ 0.00512307  0.51120686  0.08332851 -0.16724917], action=0, reward=1.0, next_state=[0.01534721 0.31499711 0.07998353 0.15051448]\n",
      "[ episode 227 ][ timestamp 59 ] state=[0.01534721 0.31499711 0.07998353 0.15051448], action=1, reward=1.0, next_state=[ 0.02164715  0.50888807  0.08299382 -0.11590207]\n",
      "[ episode 227 ][ timestamp 60 ] state=[ 0.02164715  0.50888807  0.08299382 -0.11590207], action=0, reward=1.0, next_state=[0.03182491 0.31268107 0.08067577 0.20176759]\n",
      "[ episode 227 ][ timestamp 61 ] state=[0.03182491 0.31268107 0.08067577 0.20176759], action=1, reward=1.0, next_state=[ 0.03807853  0.50656209  0.08471113 -0.06441508]\n",
      "[ episode 227 ][ timestamp 62 ] state=[ 0.03807853  0.50656209  0.08471113 -0.06441508], action=0, reward=1.0, next_state=[0.04820977 0.31033419 0.08342282 0.2537466 ]\n",
      "[ episode 227 ][ timestamp 63 ] state=[0.04820977 0.31033419 0.08342282 0.2537466 ], action=1, reward=1.0, next_state=[ 0.05441646  0.50417204  0.08849776 -0.01150114]\n",
      "[ episode 227 ][ timestamp 64 ] state=[ 0.05441646  0.50417204  0.08849776 -0.01150114], action=1, reward=1.0, next_state=[ 0.0644999   0.69792069  0.08826773 -0.27500241]\n",
      "[ episode 227 ][ timestamp 65 ] state=[ 0.0644999   0.69792069  0.08826773 -0.27500241], action=0, reward=1.0, next_state=[0.07845831 0.50165752 0.08276768 0.04416328]\n",
      "[ episode 227 ][ timestamp 66 ] state=[0.07845831 0.50165752 0.08276768 0.04416328], action=1, reward=1.0, next_state=[ 0.08849146  0.69550109  0.08365095 -0.22130078]\n",
      "[ episode 227 ][ timestamp 67 ] state=[ 0.08849146  0.69550109  0.08365095 -0.22130078], action=0, reward=1.0, next_state=[0.10240148 0.49928927 0.07922493 0.09655252]\n",
      "[ episode 227 ][ timestamp 68 ] state=[0.10240148 0.49928927 0.07922493 0.09655252], action=0, reward=1.0, next_state=[0.11238727 0.30312648 0.08115599 0.41314152]\n",
      "[ episode 227 ][ timestamp 69 ] state=[0.11238727 0.30312648 0.08115599 0.41314152], action=1, reward=1.0, next_state=[0.1184498  0.49700991 0.08941882 0.14710726]\n",
      "[ episode 227 ][ timestamp 70 ] state=[0.1184498  0.49700991 0.08941882 0.14710726], action=1, reward=1.0, next_state=[ 0.12839     0.69074513  0.09236096 -0.11608044]\n",
      "[ episode 227 ][ timestamp 71 ] state=[ 0.12839     0.69074513  0.09236096 -0.11608044], action=0, reward=1.0, next_state=[0.1422049  0.49442951 0.09003935 0.20425341]\n",
      "[ episode 227 ][ timestamp 72 ] state=[0.1422049  0.49442951 0.09003935 0.20425341], action=1, reward=1.0, next_state=[ 0.15209349  0.68815619  0.09412442 -0.05872366]\n",
      "[ episode 227 ][ timestamp 73 ] state=[ 0.15209349  0.68815619  0.09412442 -0.05872366], action=0, reward=1.0, next_state=[0.16585661 0.49181953 0.09294995 0.26210945]\n",
      "[ episode 227 ][ timestamp 74 ] state=[0.16585661 0.49181953 0.09294995 0.26210945], action=1, reward=1.0, next_state=[1.75693003e-01 6.85500238e-01 9.81921361e-02 1.30441696e-04]\n",
      "[ episode 227 ][ timestamp 75 ] state=[1.75693003e-01 6.85500238e-01 9.81921361e-02 1.30441696e-04], action=1, reward=1.0, next_state=[ 0.18940301  0.87908687  0.09819474 -0.26002863]\n",
      "[ episode 227 ][ timestamp 76 ] state=[ 0.18940301  0.87908687  0.09819474 -0.26002863], action=0, reward=1.0, next_state=[0.20698474 0.68271027 0.09299417 0.06194017]\n",
      "[ episode 227 ][ timestamp 77 ] state=[0.20698474 0.68271027 0.09299417 0.06194017], action=1, reward=1.0, next_state=[ 0.22063895  0.87638437  0.09423298 -0.20001484]\n",
      "[ episode 227 ][ timestamp 78 ] state=[ 0.22063895  0.87638437  0.09423298 -0.20001484], action=0, reward=1.0, next_state=[0.23816664 0.68004983 0.09023268 0.12084389]\n",
      "[ episode 227 ][ timestamp 79 ] state=[0.23816664 0.68004983 0.09023268 0.12084389], action=1, reward=1.0, next_state=[ 0.25176763  0.87377091  0.09264956 -0.14206317]\n",
      "[ episode 227 ][ timestamp 80 ] state=[ 0.25176763  0.87377091  0.09264956 -0.14206317], action=0, reward=1.0, next_state=[0.26924305 0.6774526  0.08980829 0.17835134]\n",
      "[ episode 227 ][ timestamp 81 ] state=[0.26924305 0.6774526  0.08980829 0.17835134], action=1, reward=1.0, next_state=[ 0.2827921   0.87118224  0.09337532 -0.08470386]\n",
      "[ episode 227 ][ timestamp 82 ] state=[ 0.2827921   0.87118224  0.09337532 -0.08470386], action=0, reward=1.0, next_state=[0.30021575 0.67485446 0.09168124 0.23591738]\n",
      "[ episode 227 ][ timestamp 83 ] state=[0.30021575 0.67485446 0.09168124 0.23591738], action=1, reward=1.0, next_state=[ 0.31371284  0.8685551   0.09639959 -0.02649678]\n",
      "[ episode 227 ][ timestamp 84 ] state=[ 0.31371284  0.8685551   0.09639959 -0.02649678], action=0, reward=1.0, next_state=[0.33108394 0.67219236 0.09586965 0.29497742]\n",
      "[ episode 227 ][ timestamp 85 ] state=[0.33108394 0.67219236 0.09586965 0.29497742], action=1, reward=1.0, next_state=[0.34452779 0.86582611 0.1017692  0.03400305]\n",
      "[ episode 227 ][ timestamp 86 ] state=[0.34452779 0.86582611 0.1017692  0.03400305], action=1, reward=1.0, next_state=[ 0.36184431  1.05935256  0.10244926 -0.22491613]\n",
      "[ episode 227 ][ timestamp 87 ] state=[ 0.36184431  1.05935256  0.10244926 -0.22491613], action=0, reward=1.0, next_state=[0.38303136 0.86292697 0.09795094 0.09824479]\n",
      "[ episode 227 ][ timestamp 88 ] state=[0.38303136 0.86292697 0.09795094 0.09824479], action=1, reward=1.0, next_state=[ 0.4002899   1.05651857  0.09991584 -0.16199914]\n",
      "[ episode 227 ][ timestamp 89 ] state=[ 0.4002899   1.05651857  0.09991584 -0.16199914], action=0, reward=1.0, next_state=[0.42142027 0.86011874 0.09667585 0.16045771]\n",
      "[ episode 227 ][ timestamp 90 ] state=[0.42142027 0.86011874 0.09667585 0.16045771], action=0, reward=1.0, next_state=[0.43862265 0.66375524 0.09988501 0.48200605]\n",
      "[ episode 227 ][ timestamp 91 ] state=[0.43862265 0.66375524 0.09988501 0.48200605], action=0, reward=1.0, next_state=[0.45189775 0.46737579 0.10952513 0.80442437]\n",
      "[ episode 227 ][ timestamp 92 ] state=[0.45189775 0.46737579 0.10952513 0.80442437], action=1, reward=1.0, next_state=[0.46124527 0.66083939 0.12561362 0.54810383]\n",
      "[ episode 227 ][ timestamp 93 ] state=[0.46124527 0.66083939 0.12561362 0.54810383], action=1, reward=1.0, next_state=[0.47446205 0.85399363 0.13657569 0.29748862]\n",
      "[ episode 227 ][ timestamp 94 ] state=[0.47446205 0.85399363 0.13657569 0.29748862], action=1, reward=1.0, next_state=[0.49154193 1.04693122 0.14252547 0.05080574]\n",
      "[ episode 227 ][ timestamp 95 ] state=[0.49154193 1.04693122 0.14252547 0.05080574], action=1, reward=1.0, next_state=[ 0.51248055  1.2397524   0.14354158 -0.19373259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 227 ][ timestamp 96 ] state=[ 0.51248055  1.2397524   0.14354158 -0.19373259], action=0, reward=1.0, next_state=[0.5372756  1.0429     0.13966693 0.1405657 ]\n",
      "[ episode 227 ][ timestamp 97 ] state=[0.5372756  1.0429     0.13966693 0.1405657 ], action=0, reward=1.0, next_state=[0.5581336  0.84608275 0.14247824 0.47384549]\n",
      "[ episode 227 ][ timestamp 98 ] state=[0.5581336  0.84608275 0.14247824 0.47384549], action=1, reward=1.0, next_state=[0.57505526 1.03893548 0.15195515 0.22924465]\n",
      "[ episode 227 ][ timestamp 99 ] state=[0.57505526 1.03893548 0.15195515 0.22924465], action=1, reward=1.0, next_state=[ 0.59583396  1.23159632  0.15654005 -0.01191347]\n",
      "[ episode 227 ][ timestamp 100 ] state=[ 0.59583396  1.23159632  0.15654005 -0.01191347], action=1, reward=1.0, next_state=[ 0.62046589  1.4241673   0.15630178 -0.25140294]\n",
      "[ episode 227 ][ timestamp 101 ] state=[ 0.62046589  1.4241673   0.15630178 -0.25140294], action=0, reward=1.0, next_state=[0.64894924 1.22719892 0.15127372 0.08621383]\n",
      "[ episode 227 ][ timestamp 102 ] state=[0.64894924 1.22719892 0.15127372 0.08621383], action=1, reward=1.0, next_state=[ 0.67349322  1.41986527  0.15299799 -0.15518027]\n",
      "[ episode 227 ][ timestamp 103 ] state=[ 0.67349322  1.41986527  0.15299799 -0.15518027], action=0, reward=1.0, next_state=[0.70189052 1.22292147 0.14989439 0.1815907 ]\n",
      "[ episode 227 ][ timestamp 104 ] state=[0.70189052 1.22292147 0.14989439 0.1815907 ], action=1, reward=1.0, next_state=[ 0.72634895  1.41561615  0.1535262  -0.06030614]\n",
      "[ episode 227 ][ timestamp 105 ] state=[ 0.72634895  1.41561615  0.1535262  -0.06030614], action=1, reward=1.0, next_state=[ 0.75466127  1.60824167  0.15232008 -0.30088634]\n",
      "[ episode 227 ][ timestamp 106 ] state=[ 0.75466127  1.60824167  0.15232008 -0.30088634], action=0, reward=1.0, next_state=[0.78682611 1.411314   0.14630235 0.03569418]\n",
      "[ episode 227 ][ timestamp 107 ] state=[0.78682611 1.411314   0.14630235 0.03569418], action=1, reward=1.0, next_state=[ 0.81505239  1.60406802  0.14701624 -0.20748842]\n",
      "[ episode 227 ][ timestamp 108 ] state=[ 0.81505239  1.60406802  0.14701624 -0.20748842], action=0, reward=1.0, next_state=[0.84713375 1.40718318 0.14286647 0.12772026]\n",
      "[ episode 227 ][ timestamp 109 ] state=[0.84713375 1.40718318 0.14286647 0.12772026], action=1, reward=1.0, next_state=[ 0.87527741  1.60000023  0.14542087 -0.11669867]\n",
      "[ episode 227 ][ timestamp 110 ] state=[ 0.87527741  1.60000023  0.14542087 -0.11669867], action=0, reward=1.0, next_state=[0.90727742 1.40312643 0.1430869  0.21809824]\n",
      "[ episode 227 ][ timestamp 111 ] state=[0.90727742 1.40312643 0.1430869  0.21809824], action=1, reward=1.0, next_state=[ 0.93533994  1.59594393  0.14744886 -0.02624812]\n",
      "[ episode 227 ][ timestamp 112 ] state=[ 0.93533994  1.59594393  0.14744886 -0.02624812], action=0, reward=1.0, next_state=[0.96725882 1.39904892 0.1469239  0.30908272]\n",
      "[ episode 227 ][ timestamp 113 ] state=[0.96725882 1.39904892 0.1469239  0.30908272], action=1, reward=1.0, next_state=[0.9952398  1.59180533 0.15310556 0.06610361]\n",
      "[ episode 227 ][ timestamp 114 ] state=[0.9952398  1.59180533 0.15310556 0.06610361], action=1, reward=1.0, next_state=[ 1.02707591  1.78443849  0.15442763 -0.17462869]\n",
      "[ episode 227 ][ timestamp 115 ] state=[ 1.02707591  1.78443849  0.15442763 -0.17462869], action=0, reward=1.0, next_state=[1.06276468 1.5874824  0.15093506 0.16251118]\n",
      "[ episode 227 ][ timestamp 116 ] state=[1.06276468 1.5874824  0.15093506 0.16251118], action=1, reward=1.0, next_state=[ 1.09451433  1.78015758  0.15418528 -0.07900916]\n",
      "[ episode 227 ][ timestamp 117 ] state=[ 1.09451433  1.78015758  0.15418528 -0.07900916], action=0, reward=1.0, next_state=[1.13011748 1.5832001  0.1526051  0.25807337]\n",
      "[ episode 227 ][ timestamp 118 ] state=[1.13011748 1.5832001  0.1526051  0.25807337], action=1, reward=1.0, next_state=[1.16178148 1.77585143 0.15776656 0.01714671]\n",
      "[ episode 227 ][ timestamp 119 ] state=[1.16178148 1.77585143 0.15776656 0.01714671], action=0, reward=1.0, next_state=[1.19729851 1.57886008 0.1581095  0.35515517]\n",
      "[ episode 227 ][ timestamp 120 ] state=[1.19729851 1.57886008 0.1581095  0.35515517], action=1, reward=1.0, next_state=[1.22887571 1.77142218 0.1652126  0.1162056 ]\n",
      "[ episode 227 ][ timestamp 121 ] state=[1.22887571 1.77142218 0.1652126  0.1162056 ], action=0, reward=1.0, next_state=[1.26430415 1.57436588 0.16753671 0.45611703]\n",
      "[ episode 227 ][ timestamp 122 ] state=[1.26430415 1.57436588 0.16753671 0.45611703], action=1, reward=1.0, next_state=[1.29579147 1.76677219 0.17665905 0.22057423]\n",
      "[ episode 227 ][ timestamp 123 ] state=[1.29579147 1.76677219 0.17665905 0.22057423], action=1, reward=1.0, next_state=[ 1.33112691  1.95898693  0.18107054 -0.0115925 ]\n",
      "[ episode 227 ][ timestamp 124 ] state=[ 1.33112691  1.95898693  0.18107054 -0.0115925 ], action=1, reward=1.0, next_state=[ 1.37030665  2.15111258  0.18083869 -0.2421252 ]\n",
      "[ episode 227 ][ timestamp 125 ] state=[ 1.37030665  2.15111258  0.18083869 -0.2421252 ], action=1, reward=1.0, next_state=[ 1.4133289   2.34325272  0.17599618 -0.47275836]\n",
      "[ episode 227 ][ timestamp 126 ] state=[ 1.4133289   2.34325272  0.17599618 -0.47275836], action=0, reward=1.0, next_state=[ 1.46019396  2.14613886  0.16654102 -0.13017874]\n",
      "[ episode 227 ][ timestamp 127 ] state=[ 1.46019396  2.14613886  0.16654102 -0.13017874], action=0, reward=1.0, next_state=[1.50311674 1.94907138 0.16393744 0.21006959]\n",
      "[ episode 227 ][ timestamp 128 ] state=[1.50311674 1.94907138 0.16393744 0.21006959], action=1, reward=1.0, next_state=[ 1.54209816  2.14151609  0.16813883 -0.02674511]\n",
      "[ episode 227 ][ timestamp 129 ] state=[ 1.54209816  2.14151609  0.16813883 -0.02674511], action=1, reward=1.0, next_state=[ 1.58492849  2.33387781  0.16760393 -0.26201841]\n",
      "[ episode 227 ][ timestamp 130 ] state=[ 1.58492849  2.33387781  0.16760393 -0.26201841], action=1, reward=1.0, next_state=[ 1.63160604  2.52626027  0.16236356 -0.49750323]\n",
      "[ episode 227 ][ timestamp 131 ] state=[ 1.63160604  2.52626027  0.16236356 -0.49750323], action=1, reward=1.0, next_state=[ 1.68213125  2.71876536  0.1524135  -0.73493768]\n",
      "[ episode 227 ][ timestamp 132 ] state=[ 1.68213125  2.71876536  0.1524135  -0.73493768], action=0, reward=1.0, next_state=[ 1.73650655  2.52190331  0.13771475 -0.39843149]\n",
      "[ episode 227 ][ timestamp 133 ] state=[ 1.73650655  2.52190331  0.13771475 -0.39843149], action=0, reward=1.0, next_state=[ 1.78694462  2.32512381  0.12974612 -0.06569655]\n",
      "[ episode 227 ][ timestamp 134 ] state=[ 1.78694462  2.32512381  0.12974612 -0.06569655], action=1, reward=1.0, next_state=[ 1.8334471   2.51816992  0.12843218 -0.31479339]\n",
      "[ episode 227 ][ timestamp 135 ] state=[ 1.8334471   2.51816992  0.12843218 -0.31479339], action=1, reward=1.0, next_state=[ 1.88381049  2.71125078  0.12213632 -0.56437398]\n",
      "[ episode 227 ][ timestamp 136 ] state=[ 1.88381049  2.71125078  0.12213632 -0.56437398], action=1, reward=1.0, next_state=[ 1.93803551  2.90446649  0.11084884 -0.81621968]\n",
      "[ episode 227 ][ timestamp 137 ] state=[ 1.93803551  2.90446649  0.11084884 -0.81621968], action=0, reward=1.0, next_state=[ 1.99612484  2.7080156   0.09452444 -0.49082905]\n",
      "[ episode 227 ][ timestamp 138 ] state=[ 1.99612484  2.7080156   0.09452444 -0.49082905], action=0, reward=1.0, next_state=[ 2.05028515  2.51169624  0.08470786 -0.16991577]\n",
      "[ episode 227 ][ timestamp 139 ] state=[ 2.05028515  2.51169624  0.08470786 -0.16991577], action=1, reward=1.0, next_state=[ 2.10051908  2.70551003  0.08130955 -0.43471972]\n",
      "[ episode 227 ][ timestamp 140 ] state=[ 2.10051908  2.70551003  0.08130955 -0.43471972], action=0, reward=1.0, next_state=[ 2.15462928  2.50933677  0.07261515 -0.11755334]\n",
      "[ episode 227 ][ timestamp 141 ] state=[ 2.15462928  2.50933677  0.07261515 -0.11755334], action=1, reward=1.0, next_state=[ 2.20481601  2.70334723  0.07026409 -0.38647201]\n",
      "[ episode 227 ][ timestamp 142 ] state=[ 2.20481601  2.70334723  0.07026409 -0.38647201], action=0, reward=1.0, next_state=[ 2.25888296  2.50730184  0.06253465 -0.07248888]\n",
      "[ episode 227 ][ timestamp 143 ] state=[ 2.25888296  2.50730184  0.06253465 -0.07248888], action=0, reward=1.0, next_state=[2.30902899 2.3113417  0.06108487 0.23924997]\n",
      "[ episode 227 ][ timestamp 144 ] state=[2.30902899 2.3113417  0.06108487 0.23924997], action=1, reward=1.0, next_state=[ 2.35525583  2.50554024  0.06586987 -0.03355675]\n",
      "[ episode 227 ][ timestamp 145 ] state=[ 2.35525583  2.50554024  0.06586987 -0.03355675], action=0, reward=-1.0, next_state=[2.40536663 2.30953856 0.06519873 0.27915993]\n",
      "[ Ended! ] Episode 227: Exploration_rate=0.322118930542046. Score=145.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 228 ] state=[-0.04757463 -0.00927954 -0.03012321 -0.00839117]\n",
      "[ episode 228 ][ timestamp 1 ] state=[-0.04757463 -0.00927954 -0.03012321 -0.00839117], action=0, reward=1.0, next_state=[-0.04776022 -0.20395682 -0.03029103  0.27463739]\n",
      "[ episode 228 ][ timestamp 2 ] state=[-0.04776022 -0.20395682 -0.03029103  0.27463739], action=1, reward=1.0, next_state=[-0.05183936 -0.00841607 -0.02479829 -0.02744338]\n",
      "[ episode 228 ][ timestamp 3 ] state=[-0.05183936 -0.00841607 -0.02479829 -0.02744338], action=0, reward=1.0, next_state=[-0.05200768 -0.20317378 -0.02534715  0.25731341]\n",
      "[ episode 228 ][ timestamp 4 ] state=[-0.05200768 -0.20317378 -0.02534715  0.25731341], action=1, reward=1.0, next_state=[-0.05607116 -0.00769929 -0.02020089 -0.0432554 ]\n",
      "[ episode 228 ][ timestamp 5 ] state=[-0.05607116 -0.00769929 -0.02020089 -0.0432554 ], action=0, reward=1.0, next_state=[-0.05622514 -0.20252583 -0.02106599  0.24298613]\n",
      "[ episode 228 ][ timestamp 6 ] state=[-0.05622514 -0.20252583 -0.02106599  0.24298613], action=1, reward=1.0, next_state=[-0.06027566 -0.00710941 -0.01620627 -0.0562664 ]\n",
      "[ episode 228 ][ timestamp 7 ] state=[-0.06027566 -0.00710941 -0.01620627 -0.0562664 ], action=0, reward=1.0, next_state=[-0.06041785 -0.20199528 -0.0173316   0.23125959]\n",
      "[ episode 228 ][ timestamp 8 ] state=[-0.06041785 -0.20199528 -0.0173316   0.23125959], action=1, reward=1.0, next_state=[-0.06445775 -0.00663002 -0.01270641 -0.06683953]\n",
      "[ episode 228 ][ timestamp 9 ] state=[-0.06445775 -0.00663002 -0.01270641 -0.06683953], action=1, reward=1.0, next_state=[-0.06459035  0.18867178 -0.0140432  -0.36350416]\n",
      "[ episode 228 ][ timestamp 10 ] state=[-0.06459035  0.18867178 -0.0140432  -0.36350416], action=1, reward=1.0, next_state=[-0.06081692  0.38399047 -0.02131328 -0.66058188]\n",
      "[ episode 228 ][ timestamp 11 ] state=[-0.06081692  0.38399047 -0.02131328 -0.66058188], action=1, reward=1.0, next_state=[-0.05313711  0.57940243 -0.03452492 -0.95989887]\n",
      "[ episode 228 ][ timestamp 12 ] state=[-0.05313711  0.57940243 -0.03452492 -0.95989887], action=0, reward=1.0, next_state=[-0.04154906  0.38476116 -0.0537229  -0.67825927]\n",
      "[ episode 228 ][ timestamp 13 ] state=[-0.04154906  0.38476116 -0.0537229  -0.67825927], action=0, reward=1.0, next_state=[-0.03385384  0.19042509 -0.06728808 -0.40296267]\n",
      "[ episode 228 ][ timestamp 14 ] state=[-0.03385384  0.19042509 -0.06728808 -0.40296267], action=0, reward=1.0, next_state=[-0.03004533 -0.0036812  -0.07534733 -0.13222989]\n",
      "[ episode 228 ][ timestamp 15 ] state=[-0.03004533 -0.0036812  -0.07534733 -0.13222989], action=0, reward=1.0, next_state=[-0.03011896 -0.19764751 -0.07799193  0.13576291]\n",
      "[ episode 228 ][ timestamp 16 ] state=[-0.03011896 -0.19764751 -0.07799193  0.13576291], action=1, reward=1.0, next_state=[-0.03407191 -0.00150009 -0.07527667 -0.18047022]\n",
      "[ episode 228 ][ timestamp 17 ] state=[-0.03407191 -0.00150009 -0.07527667 -0.18047022], action=0, reward=1.0, next_state=[-0.03410191 -0.19546866 -0.07888608  0.08754822]\n",
      "[ episode 228 ][ timestamp 18 ] state=[-0.03410191 -0.19546866 -0.07888608  0.08754822], action=1, reward=1.0, next_state=[-0.03801128  0.00069023 -0.07713511 -0.22894352]\n",
      "[ episode 228 ][ timestamp 19 ] state=[-0.03801128  0.00069023 -0.07713511 -0.22894352], action=0, reward=1.0, next_state=[-0.03799748 -0.19324955 -0.08171398  0.03844591]\n",
      "[ episode 228 ][ timestamp 20 ] state=[-0.03799748 -0.19324955 -0.08171398  0.03844591], action=1, reward=1.0, next_state=[-0.04186247  0.00294333 -0.08094507 -0.27885863]\n",
      "[ episode 228 ][ timestamp 21 ] state=[-0.04186247  0.00294333 -0.08094507 -0.27885863], action=1, reward=1.0, next_state=[-0.0418036   0.1991211  -0.08652224 -0.59593366]\n",
      "[ episode 228 ][ timestamp 22 ] state=[-0.0418036   0.1991211  -0.08652224 -0.59593366], action=0, reward=1.0, next_state=[-0.03782118  0.00530978 -0.09844091 -0.33170998]\n",
      "[ episode 228 ][ timestamp 23 ] state=[-0.03782118  0.00530978 -0.09844091 -0.33170998], action=0, reward=1.0, next_state=[-0.03771499 -0.18828321 -0.10507511 -0.07162129]\n",
      "[ episode 228 ][ timestamp 24 ] state=[-0.03771499 -0.18828321 -0.10507511 -0.07162129], action=1, reward=1.0, next_state=[-0.04148065  0.00817595 -0.10650754 -0.39552   ]\n",
      "[ episode 228 ][ timestamp 25 ] state=[-0.04148065  0.00817595 -0.10650754 -0.39552   ], action=0, reward=1.0, next_state=[-0.04131713 -0.1852863  -0.11441794 -0.13822507]\n",
      "[ episode 228 ][ timestamp 26 ] state=[-0.04131713 -0.1852863  -0.11441794 -0.13822507], action=1, reward=1.0, next_state=[-0.04502286  0.01127269 -0.11718244 -0.46470126]\n",
      "[ episode 228 ][ timestamp 27 ] state=[-0.04502286  0.01127269 -0.11718244 -0.46470126], action=0, reward=1.0, next_state=[-0.0447974  -0.18201534 -0.12647646 -0.21113041]\n",
      "[ episode 228 ][ timestamp 28 ] state=[-0.0447974  -0.18201534 -0.12647646 -0.21113041], action=0, reward=1.0, next_state=[-0.04843771 -0.37512338 -0.13069907  0.03913295]\n",
      "[ episode 228 ][ timestamp 29 ] state=[-0.04843771 -0.37512338 -0.13069907  0.03913295], action=1, reward=1.0, next_state=[-0.05594018 -0.17839298 -0.12991641 -0.29176201]\n",
      "[ episode 228 ][ timestamp 30 ] state=[-0.05594018 -0.17839298 -0.12991641 -0.29176201], action=0, reward=1.0, next_state=[-0.05950804 -0.37144631 -0.13575165 -0.04271045]\n",
      "[ episode 228 ][ timestamp 31 ] state=[-0.05950804 -0.37144631 -0.13575165 -0.04271045], action=1, reward=1.0, next_state=[-0.06693696 -0.17466528 -0.13660586 -0.37495491]\n",
      "[ episode 228 ][ timestamp 32 ] state=[-0.06693696 -0.17466528 -0.13660586 -0.37495491], action=0, reward=1.0, next_state=[-0.07043027 -0.36760924 -0.14410496 -0.12827251]\n",
      "[ episode 228 ][ timestamp 33 ] state=[-0.07043027 -0.36760924 -0.14410496 -0.12827251], action=1, reward=1.0, next_state=[-0.07778245 -0.17074847 -0.14667041 -0.4627233 ]\n",
      "[ episode 228 ][ timestamp 34 ] state=[-0.07778245 -0.17074847 -0.14667041 -0.4627233 ], action=0, reward=1.0, next_state=[-0.08119742 -0.36352624 -0.15592488 -0.21962804]\n",
      "[ episode 228 ][ timestamp 35 ] state=[-0.08119742 -0.36352624 -0.15592488 -0.21962804], action=0, reward=1.0, next_state=[-0.08846795 -0.55611554 -0.16031744  0.02009487]\n",
      "[ episode 228 ][ timestamp 36 ] state=[-0.08846795 -0.55611554 -0.16031744  0.02009487], action=1, reward=1.0, next_state=[-0.09959026 -0.359101   -0.15991554 -0.31856904]\n",
      "[ episode 228 ][ timestamp 37 ] state=[-0.09959026 -0.359101   -0.15991554 -0.31856904], action=1, reward=1.0, next_state=[-0.10677228 -0.16210578 -0.16628692 -0.65710664]\n",
      "[ episode 228 ][ timestamp 38 ] state=[-0.10677228 -0.16210578 -0.16628692 -0.65710664], action=0, reward=1.0, next_state=[-0.11001439 -0.35457055 -0.17942905 -0.4210551 ]\n",
      "[ episode 228 ][ timestamp 39 ] state=[-0.11001439 -0.35457055 -0.17942905 -0.4210551 ], action=0, reward=1.0, next_state=[-0.11710581 -0.54675707 -0.18785016 -0.18987297]\n",
      "[ episode 228 ][ timestamp 40 ] state=[-0.11710581 -0.54675707 -0.18785016 -0.18987297], action=1, reward=1.0, next_state=[-0.12804095 -0.3495139  -0.19164762 -0.53543658]\n",
      "[ episode 228 ][ timestamp 41 ] state=[-0.12804095 -0.3495139  -0.19164762 -0.53543658], action=0, reward=1.0, next_state=[-0.13503122 -0.54149776 -0.20235635 -0.30873324]\n",
      "[ episode 228 ][ timestamp 42 ] state=[-0.13503122 -0.54149776 -0.20235635 -0.30873324], action=1, reward=1.0, next_state=[-0.14586118 -0.34415424 -0.20853101 -0.65779612]\n",
      "[ episode 228 ][ timestamp 43 ] state=[-0.14586118 -0.34415424 -0.20853101 -0.65779612], action=0, reward=-1.0, next_state=[-0.15274426 -0.53585771 -0.22168693 -0.43733523]\n",
      "[ Ended! ] Episode 228: Exploration_rate=0.32050833588933575. Score=43.\n",
      "[ Experience replay ] starts\n",
      "[ episode 229 ] state=[-0.01808813  0.00075821  0.00255436 -0.03214828]\n",
      "[ episode 229 ][ timestamp 1 ] state=[-0.01808813  0.00075821  0.00255436 -0.03214828], action=1, reward=1.0, next_state=[-0.01807297  0.19584344  0.0019114  -0.32402418]\n",
      "[ episode 229 ][ timestamp 2 ] state=[-0.01807297  0.19584344  0.0019114  -0.32402418], action=0, reward=1.0, next_state=[-0.0141561   0.00069432 -0.00456909 -0.03073909]\n",
      "[ episode 229 ][ timestamp 3 ] state=[-0.0141561   0.00069432 -0.00456909 -0.03073909], action=0, reward=1.0, next_state=[-0.01414221 -0.19436181 -0.00518387  0.26049874]\n",
      "[ episode 229 ][ timestamp 4 ] state=[-0.01414221 -0.19436181 -0.00518387  0.26049874], action=0, reward=1.0, next_state=[-1.80294475e-02 -3.89409378e-01  2.61078910e-05  5.51542114e-01]\n",
      "[ episode 229 ][ timestamp 5 ] state=[-1.80294475e-02 -3.89409378e-01  2.61078910e-05  5.51542114e-01], action=1, reward=1.0, next_state=[-0.02581764 -0.19428779  0.01105695  0.25886741]\n",
      "[ episode 229 ][ timestamp 6 ] state=[-0.02581764 -0.19428779  0.01105695  0.25886741], action=0, reward=1.0, next_state=[-0.02970339 -0.38956584  0.0162343   0.55501725]\n",
      "[ episode 229 ][ timestamp 7 ] state=[-0.02970339 -0.38956584  0.0162343   0.55501725], action=1, reward=1.0, next_state=[-0.03749471 -0.19467555  0.02733464  0.26749301]\n",
      "[ episode 229 ][ timestamp 8 ] state=[-0.03749471 -0.19467555  0.02733464  0.26749301], action=0, reward=1.0, next_state=[-0.04138822 -0.39017673  0.0326845   0.56867062]\n",
      "[ episode 229 ][ timestamp 9 ] state=[-0.04138822 -0.39017673  0.0326845   0.56867062], action=1, reward=1.0, next_state=[-0.04919175 -0.19552809  0.04405792  0.28646113]\n",
      "[ episode 229 ][ timestamp 10 ] state=[-0.04919175 -0.19552809  0.04405792  0.28646113], action=0, reward=1.0, next_state=[-0.05310231 -0.39124977  0.04978714  0.5927076 ]\n",
      "[ episode 229 ][ timestamp 11 ] state=[-0.05310231 -0.39124977  0.04978714  0.5927076 ], action=1, reward=1.0, next_state=[-0.06092731 -0.19685883  0.06164129  0.31611387]\n",
      "[ episode 229 ][ timestamp 12 ] state=[-0.06092731 -0.19685883  0.06164129  0.31611387], action=0, reward=1.0, next_state=[-0.06486449 -0.39280216  0.06796357  0.62758172]\n",
      "[ episode 229 ][ timestamp 13 ] state=[-0.06486449 -0.39280216  0.06796357  0.62758172], action=1, reward=1.0, next_state=[-0.07272053 -0.19869131  0.0805152   0.35705356]\n",
      "[ episode 229 ][ timestamp 14 ] state=[-0.07272053 -0.19869131  0.0805152   0.35705356], action=0, reward=1.0, next_state=[-0.07669436 -0.39486013  0.08765627  0.67399943]\n",
      "[ episode 229 ][ timestamp 15 ] state=[-0.07669436 -0.39486013  0.08765627  0.67399943], action=1, reward=1.0, next_state=[-0.08459156 -0.20105875  0.10113626  0.41015142]\n",
      "[ episode 229 ][ timestamp 16 ] state=[-0.08459156 -0.20105875  0.10113626  0.41015142], action=0, reward=1.0, next_state=[-0.08861273 -0.39745821  0.10933929  0.73292863]\n",
      "[ episode 229 ][ timestamp 17 ] state=[-0.08861273 -0.39745821  0.10933929  0.73292863], action=1, reward=1.0, next_state=[-0.0965619  -0.20400324  0.12399786  0.47656077]\n",
      "[ episode 229 ][ timestamp 18 ] state=[-0.0965619  -0.20400324  0.12399786  0.47656077], action=1, reward=1.0, next_state=[-0.10064196 -0.01083024  0.13352908  0.22538803]\n",
      "[ episode 229 ][ timestamp 19 ] state=[-0.10064196 -0.01083024  0.13352908  0.22538803], action=0, reward=1.0, next_state=[-0.10085857 -0.20758275  0.13803684  0.55703061]\n",
      "[ episode 229 ][ timestamp 20 ] state=[-0.10085857 -0.20758275  0.13803684  0.55703061], action=1, reward=1.0, next_state=[-0.10501022 -0.01464104  0.14917745  0.310825  ]\n",
      "[ episode 229 ][ timestamp 21 ] state=[-0.10501022 -0.01464104  0.14917745  0.310825  ], action=0, reward=1.0, next_state=[-0.10530304 -0.21153856  0.15539395  0.64658675]\n",
      "[ episode 229 ][ timestamp 22 ] state=[-0.10530304 -0.21153856  0.15539395  0.64658675], action=1, reward=1.0, next_state=[-0.10953381 -0.01888402  0.16832569  0.40658917]\n",
      "[ episode 229 ][ timestamp 23 ] state=[-0.10953381 -0.01888402  0.16832569  0.40658917], action=1, reward=1.0, next_state=[-0.10991149  0.17350116  0.17645747  0.17134436]\n",
      "[ episode 229 ][ timestamp 24 ] state=[-0.10991149  0.17350116  0.17645747  0.17134436], action=0, reward=1.0, next_state=[-0.10644147 -0.02364968  0.17988436  0.5140882 ]\n",
      "[ episode 229 ][ timestamp 25 ] state=[-0.10644147 -0.02364968  0.17988436  0.5140882 ], action=1, reward=1.0, next_state=[-0.10691447  0.16854372  0.19016612  0.28305107]\n",
      "[ episode 229 ][ timestamp 26 ] state=[-0.10691447  0.16854372  0.19016612  0.28305107], action=0, reward=1.0, next_state=[-0.10354359 -0.02870975  0.19582714  0.62916988]\n",
      "[ episode 229 ][ timestamp 27 ] state=[-0.10354359 -0.02870975  0.19582714  0.62916988], action=1, reward=1.0, next_state=[-0.10411779  0.16321845  0.20841054  0.40398596]\n",
      "[ episode 229 ][ timestamp 28 ] state=[-0.10411779  0.16321845  0.20841054  0.40398596], action=1, reward=-1.0, next_state=[-0.10085342  0.3548699   0.21649026  0.18355959]\n",
      "[ Ended! ] Episode 229: Exploration_rate=0.31890579420988907. Score=28.\n",
      "[ Experience replay ] starts\n",
      "[ episode 230 ] state=[-0.02226316 -0.02403012 -0.03711227  0.04422864]\n",
      "[ episode 230 ][ timestamp 1 ] state=[-0.02226316 -0.02403012 -0.03711227  0.04422864], action=1, reward=1.0, next_state=[-0.02274376  0.1716038  -0.0362277  -0.25992869]\n",
      "[ episode 230 ][ timestamp 2 ] state=[-0.02274376  0.1716038  -0.0362277  -0.25992869], action=1, reward=1.0, next_state=[-0.01931168  0.36722369 -0.04142627 -0.5638146 ]\n",
      "[ episode 230 ][ timestamp 3 ] state=[-0.01931168  0.36722369 -0.04142627 -0.5638146 ], action=0, reward=1.0, next_state=[-0.01196721  0.17270674 -0.05270257 -0.28446535]\n",
      "[ episode 230 ][ timestamp 4 ] state=[-0.01196721  0.17270674 -0.05270257 -0.28446535], action=1, reward=1.0, next_state=[-0.00851308  0.36853921 -0.05839187 -0.59329357]\n",
      "[ episode 230 ][ timestamp 5 ] state=[-0.00851308  0.36853921 -0.05839187 -0.59329357], action=0, reward=1.0, next_state=[-0.00114229  0.17428115 -0.07025774 -0.31956055]\n",
      "[ episode 230 ][ timestamp 6 ] state=[-0.00114229  0.17428115 -0.07025774 -0.31956055], action=0, reward=1.0, next_state=[ 0.00234333 -0.01977354 -0.07664896 -0.04983543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 230 ][ timestamp 7 ] state=[ 0.00234333 -0.01977354 -0.07664896 -0.04983543], action=1, reward=1.0, next_state=[ 0.00194786  0.17635904 -0.07764566 -0.36568323]\n",
      "[ episode 230 ][ timestamp 8 ] state=[ 0.00194786  0.17635904 -0.07764566 -0.36568323], action=0, reward=1.0, next_state=[ 0.00547504 -0.01757857 -0.08495933 -0.09845818]\n",
      "[ episode 230 ][ timestamp 9 ] state=[ 0.00547504 -0.01757857 -0.08495933 -0.09845818], action=1, reward=1.0, next_state=[ 0.00512347  0.17865177 -0.08692849 -0.41669004]\n",
      "[ episode 230 ][ timestamp 10 ] state=[ 0.00512347  0.17865177 -0.08692849 -0.41669004], action=0, reward=1.0, next_state=[ 0.00869651 -0.01513759 -0.09526229 -0.15262838]\n",
      "[ episode 230 ][ timestamp 11 ] state=[ 0.00869651 -0.01513759 -0.09526229 -0.15262838], action=1, reward=1.0, next_state=[ 0.00839375  0.18121016 -0.09831486 -0.47377942]\n",
      "[ episode 230 ][ timestamp 12 ] state=[ 0.00839375  0.18121016 -0.09831486 -0.47377942], action=0, reward=1.0, next_state=[ 0.01201796 -0.01239589 -0.10779045 -0.21363077]\n",
      "[ episode 230 ][ timestamp 13 ] state=[ 0.01201796 -0.01239589 -0.10779045 -0.21363077], action=1, reward=1.0, next_state=[ 0.01177004  0.1840888  -0.11206306 -0.53827634]\n",
      "[ episode 230 ][ timestamp 14 ] state=[ 0.01177004  0.1840888  -0.11206306 -0.53827634], action=0, reward=1.0, next_state=[ 0.01545182 -0.00929408 -0.12282859 -0.28289913]\n",
      "[ episode 230 ][ timestamp 15 ] state=[ 0.01545182 -0.00929408 -0.12282859 -0.28289913], action=1, reward=1.0, next_state=[ 0.01526593  0.1873462  -0.12848657 -0.6116582 ]\n",
      "[ episode 230 ][ timestamp 16 ] state=[ 0.01526593  0.1873462  -0.12848657 -0.6116582 ], action=1, reward=1.0, next_state=[ 0.01901286  0.3840076  -0.14071974 -0.94188987]\n",
      "[ episode 230 ][ timestamp 17 ] state=[ 0.01901286  0.3840076  -0.14071974 -0.94188987], action=0, reward=1.0, next_state=[ 0.02669301  0.19103361 -0.15955754 -0.69652532]\n",
      "[ episode 230 ][ timestamp 18 ] state=[ 0.02669301  0.19103361 -0.15955754 -0.69652532], action=0, reward=1.0, next_state=[ 0.03051368 -0.00155816 -0.17348804 -0.45801833]\n",
      "[ episode 230 ][ timestamp 19 ] state=[ 0.03051368 -0.00155816 -0.17348804 -0.45801833], action=1, reward=1.0, next_state=[ 0.03048252  0.19553718 -0.18264841 -0.79997335]\n",
      "[ episode 230 ][ timestamp 20 ] state=[ 0.03048252  0.19553718 -0.18264841 -0.79997335], action=1, reward=1.0, next_state=[ 0.03439326  0.39263152 -0.19864788 -1.14409775]\n",
      "[ episode 230 ][ timestamp 21 ] state=[ 0.03439326  0.39263152 -0.19864788 -1.14409775], action=0, reward=-1.0, next_state=[ 0.04224589  0.20057937 -0.22152983 -0.91970392]\n",
      "[ Ended! ] Episode 230: Exploration_rate=0.3173112652388396. Score=21.\n",
      "[ Experience replay ] starts\n",
      "[ episode 231 ] state=[ 0.04323152 -0.00927428  0.0379635   0.0385324 ]\n",
      "[ episode 231 ][ timestamp 1 ] state=[ 0.04323152 -0.00927428  0.0379635   0.0385324 ], action=0, reward=1.0, next_state=[ 0.04304603 -0.20491948  0.03873415  0.34294735]\n",
      "[ episode 231 ][ timestamp 2 ] state=[ 0.04304603 -0.20491948  0.03873415  0.34294735], action=0, reward=1.0, next_state=[ 0.03894764 -0.40057047  0.0455931   0.6475887 ]\n",
      "[ episode 231 ][ timestamp 3 ] state=[ 0.03894764 -0.40057047  0.0455931   0.6475887 ], action=0, reward=1.0, next_state=[ 0.03093623 -0.59629699  0.05854487  0.95427311]\n",
      "[ episode 231 ][ timestamp 4 ] state=[ 0.03093623 -0.59629699  0.05854487  0.95427311], action=1, reward=1.0, next_state=[ 0.01901029 -0.40200943  0.07763034  0.68054343]\n",
      "[ episode 231 ][ timestamp 5 ] state=[ 0.01901029 -0.40200943  0.07763034  0.68054343], action=0, reward=1.0, next_state=[ 0.01097011 -0.5981189   0.0912412   0.99662209]\n",
      "[ episode 231 ][ timestamp 6 ] state=[ 0.01097011 -0.5981189   0.0912412   0.99662209], action=1, reward=1.0, next_state=[-0.00099227 -0.40432766  0.11117365  0.73393208]\n",
      "[ episode 231 ][ timestamp 7 ] state=[-0.00099227 -0.40432766  0.11117365  0.73393208], action=0, reward=1.0, next_state=[-0.00907883 -0.60079565  0.12585229  1.0594325 ]\n",
      "[ episode 231 ][ timestamp 8 ] state=[-0.00907883 -0.60079565  0.12585229  1.0594325 ], action=0, reward=1.0, next_state=[-0.02109474 -0.79733948  0.14704094  1.38881954]\n",
      "[ episode 231 ][ timestamp 9 ] state=[-0.02109474 -0.79733948  0.14704094  1.38881954], action=1, reward=1.0, next_state=[-0.03704153 -0.60432351  0.17481733  1.14549427]\n",
      "[ episode 231 ][ timestamp 10 ] state=[-0.03704153 -0.60432351  0.17481733  1.14549427], action=1, reward=1.0, next_state=[-0.049128   -0.41186128  0.19772721  0.91233601]\n",
      "[ episode 231 ][ timestamp 11 ] state=[-0.049128   -0.41186128  0.19772721  0.91233601], action=0, reward=-1.0, next_state=[-0.05736522 -0.60902935  0.21597393  1.26007931]\n",
      "[ Ended! ] Episode 231: Exploration_rate=0.3157247089126454. Score=11.\n",
      "[ Experience replay ] starts\n",
      "[ episode 232 ] state=[ 0.02802768 -0.00989843 -0.02091588 -0.0185934 ]\n",
      "[ episode 232 ][ timestamp 1 ] state=[ 0.02802768 -0.00989843 -0.02091588 -0.0185934 ], action=0, reward=1.0, next_state=[ 0.02782972 -0.20471428 -0.02128775  0.26741763]\n",
      "[ episode 232 ][ timestamp 2 ] state=[ 0.02782972 -0.20471428 -0.02128775  0.26741763], action=0, reward=1.0, next_state=[ 0.02373543 -0.39952605 -0.01593939  0.55331096]\n",
      "[ episode 232 ][ timestamp 3 ] state=[ 0.02373543 -0.39952605 -0.01593939  0.55331096], action=0, reward=1.0, next_state=[ 0.01574491 -0.59442058 -0.00487317  0.84092964]\n",
      "[ episode 232 ][ timestamp 4 ] state=[ 0.01574491 -0.59442058 -0.00487317  0.84092964], action=0, reward=1.0, next_state=[ 0.0038565  -0.78947567  0.01194542  1.13207609]\n",
      "[ episode 232 ][ timestamp 5 ] state=[ 0.0038565  -0.78947567  0.01194542  1.13207609], action=1, reward=1.0, next_state=[-0.01193302 -0.59451212  0.03458694  0.84316349]\n",
      "[ episode 232 ][ timestamp 6 ] state=[-0.01193302 -0.59451212  0.03458694  0.84316349], action=0, reward=1.0, next_state=[-0.02382326 -0.7900886   0.05145021  1.1465193 ]\n",
      "[ episode 232 ][ timestamp 7 ] state=[-0.02382326 -0.7900886   0.05145021  1.1465193 ], action=1, reward=1.0, next_state=[-0.03962503 -0.59567489  0.0743806   0.87040432]\n",
      "[ episode 232 ][ timestamp 8 ] state=[-0.03962503 -0.59567489  0.0743806   0.87040432], action=1, reward=1.0, next_state=[-0.05153853 -0.40163921  0.09178868  0.60200329]\n",
      "[ episode 232 ][ timestamp 9 ] state=[-0.05153853 -0.40163921  0.09178868  0.60200329], action=0, reward=1.0, next_state=[-0.05957131 -0.5979171   0.10382875  0.92212873]\n",
      "[ episode 232 ][ timestamp 10 ] state=[-0.05957131 -0.5979171   0.10382875  0.92212873], action=1, reward=1.0, next_state=[-0.07152965 -0.40433969  0.12227132  0.66379719]\n",
      "[ episode 232 ][ timestamp 11 ] state=[-0.07152965 -0.40433969  0.12227132  0.66379719], action=0, reward=1.0, next_state=[-0.07961645 -0.60093135  0.13554727  0.99234136]\n",
      "[ episode 232 ][ timestamp 12 ] state=[-0.07961645 -0.60093135  0.13554727  0.99234136], action=1, reward=1.0, next_state=[-0.09163508 -0.40785777  0.1553941   0.74511642]\n",
      "[ episode 232 ][ timestamp 13 ] state=[-0.09163508 -0.40785777  0.1553941   0.74511642], action=1, reward=1.0, next_state=[-0.09979223 -0.21518257  0.17029642  0.50508825]\n",
      "[ episode 232 ][ timestamp 14 ] state=[-0.09979223 -0.21518257  0.17029642  0.50508825], action=0, reward=1.0, next_state=[-0.10409588 -0.41224355  0.18039819  0.84622938]\n",
      "[ episode 232 ][ timestamp 15 ] state=[-0.10409588 -0.41224355  0.18039819  0.84622938], action=0, reward=1.0, next_state=[-0.11234075 -0.60930738  0.19732278  1.18977814]\n",
      "[ episode 232 ][ timestamp 16 ] state=[-0.11234075 -0.60930738  0.19732278  1.18977814], action=1, reward=-1.0, next_state=[-0.1245269  -0.4172119   0.22111834  0.96486353]\n",
      "[ Ended! ] Episode 232: Exploration_rate=0.3141460853680822. Score=16.\n",
      "[ Experience replay ] starts\n",
      "[ episode 233 ] state=[-0.01423558  0.0090645   0.00763651 -0.00306171]\n",
      "[ episode 233 ][ timestamp 1 ] state=[-0.01423558  0.0090645   0.00763651 -0.00306171], action=0, reward=1.0, next_state=[-0.01405429 -0.18616613  0.00757527  0.29202081]\n",
      "[ episode 233 ][ timestamp 2 ] state=[-0.01405429 -0.18616613  0.00757527  0.29202081], action=1, reward=1.0, next_state=[-0.01777762  0.008847    0.01341569  0.00173662]\n",
      "[ episode 233 ][ timestamp 3 ] state=[-0.01777762  0.008847    0.01341569  0.00173662], action=0, reward=1.0, next_state=[-0.01760068 -0.18646476  0.01345042  0.29862199]\n",
      "[ episode 233 ][ timestamp 4 ] state=[-0.01760068 -0.18646476  0.01345042  0.29862199], action=1, reward=1.0, next_state=[-0.02132997  0.0084629   0.01942286  0.01021124]\n",
      "[ episode 233 ][ timestamp 5 ] state=[-0.02132997  0.0084629   0.01942286  0.01021124], action=0, reward=1.0, next_state=[-0.02116071 -0.18693213  0.01962709  0.30895848]\n",
      "[ episode 233 ][ timestamp 6 ] state=[-0.02116071 -0.18693213  0.01962709  0.30895848], action=1, reward=1.0, next_state=[-0.02489935  0.00790475  0.02580625  0.02252944]\n",
      "[ episode 233 ][ timestamp 7 ] state=[-0.02489935  0.00790475  0.02580625  0.02252944], action=0, reward=1.0, next_state=[-0.02474126 -0.1875776   0.02625684  0.32324153]\n",
      "[ episode 233 ][ timestamp 8 ] state=[-0.02474126 -0.1875776   0.02625684  0.32324153], action=1, reward=1.0, next_state=[-0.02849281  0.00716081  0.03272167  0.03895322]\n",
      "[ episode 233 ][ timestamp 9 ] state=[-0.02849281  0.00716081  0.03272167  0.03895322], action=0, reward=1.0, next_state=[-0.0283496  -0.18841472  0.03350074  0.34177793]\n",
      "[ episode 233 ][ timestamp 10 ] state=[-0.0283496  -0.18841472  0.03350074  0.34177793], action=0, reward=1.0, next_state=[-0.03211789 -0.38399688  0.0403363   0.64483395]\n",
      "[ episode 233 ][ timestamp 11 ] state=[-0.03211789 -0.38399688  0.0403363   0.64483395], action=1, reward=1.0, next_state=[-0.03979783 -0.18945958  0.05323298  0.36512099]\n",
      "[ episode 233 ][ timestamp 12 ] state=[-0.03979783 -0.18945958  0.05323298  0.36512099], action=1, reward=1.0, next_state=[-0.04358702  0.00486704  0.0605354   0.08968708]\n",
      "[ episode 233 ][ timestamp 13 ] state=[-0.04358702  0.00486704  0.0605354   0.08968708], action=0, reward=1.0, next_state=[-0.04348968 -0.19106801  0.06232914  0.40083785]\n",
      "[ episode 233 ][ timestamp 14 ] state=[-0.04348968 -0.19106801  0.06232914  0.40083785], action=1, reward=1.0, next_state=[-0.04731104  0.003117    0.07034589  0.12843885]\n",
      "[ episode 233 ][ timestamp 15 ] state=[-0.04731104  0.003117    0.07034589  0.12843885], action=0, reward=1.0, next_state=[-0.0472487  -0.19293848  0.07291467  0.44245937]\n",
      "[ episode 233 ][ timestamp 16 ] state=[-0.0472487  -0.19293848  0.07291467  0.44245937], action=0, reward=1.0, next_state=[-0.05110747 -0.38901238  0.08176386  0.75720666]\n",
      "[ episode 233 ][ timestamp 17 ] state=[-0.05110747 -0.38901238  0.08176386  0.75720666], action=1, reward=1.0, next_state=[-0.05888772 -0.19510678  0.09690799  0.49133176]\n",
      "[ episode 233 ][ timestamp 18 ] state=[-0.05888772 -0.19510678  0.09690799  0.49133176], action=1, reward=1.0, next_state=[-0.06278985 -0.0014758   0.10673463  0.23069441]\n",
      "[ episode 233 ][ timestamp 19 ] state=[-0.06278985 -0.0014758   0.10673463  0.23069441], action=1, reward=1.0, next_state=[-0.06281937  0.19197191  0.11134852 -0.02650543]\n",
      "[ episode 233 ][ timestamp 20 ] state=[-0.06281937  0.19197191  0.11134852 -0.02650543], action=0, reward=1.0, next_state=[-0.05897993 -0.00455614  0.11081841  0.29912991]\n",
      "[ episode 233 ][ timestamp 21 ] state=[-0.05897993 -0.00455614  0.11081841  0.29912991], action=1, reward=1.0, next_state=[-0.05907105  0.18882608  0.116801    0.04334987]\n",
      "[ episode 233 ][ timestamp 22 ] state=[-0.05907105  0.18882608  0.116801    0.04334987], action=0, reward=1.0, next_state=[-0.05529453 -0.00776021  0.117668    0.37048162]\n",
      "[ episode 233 ][ timestamp 23 ] state=[-0.05529453 -0.00776021  0.117668    0.37048162], action=1, reward=1.0, next_state=[-0.05544973  0.18551056  0.12507763  0.11709475]\n",
      "[ episode 233 ][ timestamp 24 ] state=[-0.05544973  0.18551056  0.12507763  0.11709475], action=0, reward=1.0, next_state=[-0.05173952 -0.0111609   0.12741953  0.44647437]\n",
      "[ episode 233 ][ timestamp 25 ] state=[-0.05173952 -0.0111609   0.12741953  0.44647437], action=1, reward=1.0, next_state=[-0.05196274  0.18194992  0.13634902  0.19651648]\n",
      "[ episode 233 ][ timestamp 26 ] state=[-0.05196274  0.18194992  0.13634902  0.19651648], action=1, reward=1.0, next_state=[-0.04832374  0.37488486  0.14027935 -0.05023743]\n",
      "[ episode 233 ][ timestamp 27 ] state=[-0.04832374  0.37488486  0.14027935 -0.05023743], action=0, reward=1.0, next_state=[-0.04082605  0.17805928  0.1392746   0.2832078 ]\n",
      "[ episode 233 ][ timestamp 28 ] state=[-0.04082605  0.17805928  0.1392746   0.2832078 ], action=1, reward=1.0, next_state=[-0.03726486  0.37094842  0.14493875  0.03749021]\n",
      "[ episode 233 ][ timestamp 29 ] state=[-0.03726486  0.37094842  0.14493875  0.03749021], action=0, reward=1.0, next_state=[-0.02984589  0.17407752  0.14568856  0.37216315]\n",
      "[ episode 233 ][ timestamp 30 ] state=[-0.02984589  0.17407752  0.14568856  0.37216315], action=1, reward=1.0, next_state=[-0.02636434  0.36686185  0.15313182  0.12873122]\n",
      "[ episode 233 ][ timestamp 31 ] state=[-0.02636434  0.36686185  0.15313182  0.12873122], action=1, reward=1.0, next_state=[-0.0190271   0.55949636  0.15570645 -0.11199428]\n",
      "[ episode 233 ][ timestamp 32 ] state=[-0.0190271   0.55949636  0.15570645 -0.11199428], action=0, reward=1.0, next_state=[-0.00783718  0.36252577  0.15346656  0.22548018]\n",
      "[ episode 233 ][ timestamp 33 ] state=[-0.00783718  0.36252577  0.15346656  0.22548018], action=0, reward=1.0, next_state=[-0.00058666  0.1655816   0.15797616  0.56236671]\n",
      "[ episode 233 ][ timestamp 34 ] state=[-0.00058666  0.1655816   0.15797616  0.56236671], action=1, reward=1.0, next_state=[0.00272497 0.35817522 0.1692235  0.32332569]\n",
      "[ episode 233 ][ timestamp 35 ] state=[0.00272497 0.35817522 0.1692235  0.32332569], action=1, reward=1.0, next_state=[0.00988847 0.55053427 0.17569001 0.08842323]\n",
      "[ episode 233 ][ timestamp 36 ] state=[0.00988847 0.55053427 0.17569001 0.08842323], action=0, reward=1.0, next_state=[0.02089916 0.35338624 0.17745848 0.43098053]\n",
      "[ episode 233 ][ timestamp 37 ] state=[0.02089916 0.35338624 0.17745848 0.43098053], action=1, reward=1.0, next_state=[0.02796688 0.54560996 0.18607809 0.1990725 ]\n",
      "[ episode 233 ][ timestamp 38 ] state=[0.02796688 0.54560996 0.18607809 0.1990725 ], action=1, reward=1.0, next_state=[ 0.03887908  0.73765065  0.19005954 -0.02962405]\n",
      "[ episode 233 ][ timestamp 39 ] state=[ 0.03887908  0.73765065  0.19005954 -0.02962405], action=0, reward=1.0, next_state=[0.0536321  0.54038345 0.18946706 0.31649017]\n",
      "[ episode 233 ][ timestamp 40 ] state=[0.0536321  0.54038345 0.18946706 0.31649017], action=1, reward=1.0, next_state=[0.06443977 0.73237285 0.19579686 0.08903026]\n",
      "[ episode 233 ][ timestamp 41 ] state=[0.06443977 0.73237285 0.19579686 0.08903026], action=0, reward=1.0, next_state=[0.07908722 0.53506205 0.19757746 0.4365386 ]\n",
      "[ episode 233 ][ timestamp 42 ] state=[0.07908722 0.53506205 0.19757746 0.4365386 ], action=1, reward=1.0, next_state=[0.08978846 0.72691919 0.20630824 0.21206234]\n",
      "[ episode 233 ][ timestamp 43 ] state=[0.08978846 0.72691919 0.20630824 0.21206234], action=1, reward=-1.0, next_state=[ 0.10432685  0.91858611  0.21054948 -0.009116  ]\n",
      "[ Ended! ] Episode 233: Exploration_rate=0.3125753549412418. Score=43.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 234 ] state=[ 0.01541073  0.02670262 -0.04253533 -0.03401008]\n",
      "[ episode 234 ][ timestamp 1 ] state=[ 0.01541073  0.02670262 -0.04253533 -0.03401008], action=0, reward=1.0, next_state=[ 0.01594478 -0.16778437 -0.04321553  0.24495491]\n",
      "[ episode 234 ][ timestamp 2 ] state=[ 0.01594478 -0.16778437 -0.04321553  0.24495491], action=1, reward=1.0, next_state=[ 0.01258909  0.02792733 -0.03831643 -0.06103996]\n",
      "[ episode 234 ][ timestamp 3 ] state=[ 0.01258909  0.02792733 -0.03831643 -0.06103996], action=0, reward=1.0, next_state=[ 0.01314764 -0.1666249  -0.03953723  0.21931191]\n",
      "[ episode 234 ][ timestamp 4 ] state=[ 0.01314764 -0.1666249  -0.03953723  0.21931191], action=1, reward=1.0, next_state=[ 0.00981514  0.02903926 -0.03515099 -0.08557588]\n",
      "[ episode 234 ][ timestamp 5 ] state=[ 0.00981514  0.02903926 -0.03515099 -0.08557588], action=1, reward=1.0, next_state=[ 0.01039593  0.22464699 -0.03686251 -0.38913848]\n",
      "[ episode 234 ][ timestamp 6 ] state=[ 0.01039593  0.22464699 -0.03686251 -0.38913848], action=0, reward=1.0, next_state=[ 0.01488887  0.03006711 -0.04464528 -0.10830207]\n",
      "[ episode 234 ][ timestamp 7 ] state=[ 0.01488887  0.03006711 -0.04464528 -0.10830207], action=1, reward=1.0, next_state=[ 0.01549021  0.22579945 -0.04681132 -0.41472938]\n",
      "[ episode 234 ][ timestamp 8 ] state=[ 0.01549021  0.22579945 -0.04681132 -0.41472938], action=1, reward=1.0, next_state=[ 0.0200062   0.42155255 -0.05510591 -0.72179488]\n",
      "[ episode 234 ][ timestamp 9 ] state=[ 0.0200062   0.42155255 -0.05510591 -0.72179488], action=1, reward=1.0, next_state=[ 0.02843725  0.61739174 -0.0695418  -1.03130068]\n",
      "[ episode 234 ][ timestamp 10 ] state=[ 0.02843725  0.61739174 -0.0695418  -1.03130068], action=0, reward=1.0, next_state=[ 0.04078508  0.42326039 -0.09016782 -0.76123632]\n",
      "[ episode 234 ][ timestamp 11 ] state=[ 0.04078508  0.42326039 -0.09016782 -0.76123632], action=1, reward=1.0, next_state=[ 0.04925029  0.61950115 -0.10539254 -1.08087508]\n",
      "[ episode 234 ][ timestamp 12 ] state=[ 0.04925029  0.61950115 -0.10539254 -1.08087508], action=1, reward=1.0, next_state=[ 0.06164031  0.81584452 -0.12701005 -1.40468406]\n",
      "[ episode 234 ][ timestamp 13 ] state=[ 0.06164031  0.81584452 -0.12701005 -1.40468406], action=0, reward=1.0, next_state=[ 0.0779572   0.6225077  -0.15510373 -1.15425544]\n",
      "[ episode 234 ][ timestamp 14 ] state=[ 0.0779572   0.6225077  -0.15510373 -1.15425544], action=1, reward=1.0, next_state=[ 0.09040736  0.81927417 -0.17818884 -1.4912799 ]\n",
      "[ episode 234 ][ timestamp 15 ] state=[ 0.09040736  0.81927417 -0.17818884 -1.4912799 ], action=1, reward=1.0, next_state=[ 0.10679284  1.01606092 -0.20801443 -1.83389697]\n",
      "[ episode 234 ][ timestamp 16 ] state=[ 0.10679284  1.01606092 -0.20801443 -1.83389697], action=0, reward=-1.0, next_state=[ 0.12711406  0.82375938 -0.24469237 -1.61237902]\n",
      "[ Ended! ] Episode 234: Exploration_rate=0.31101247816653554. Score=16.\n",
      "[ Experience replay ] starts\n",
      "[ episode 235 ] state=[-0.00834208  0.02232874  0.0339267   0.04692574]\n",
      "[ episode 235 ][ timestamp 1 ] state=[-0.00834208  0.02232874  0.0339267   0.04692574], action=1, reward=1.0, next_state=[-0.0078955   0.2169482   0.03486521 -0.23486292]\n",
      "[ episode 235 ][ timestamp 2 ] state=[-0.0078955   0.2169482   0.03486521 -0.23486292], action=1, reward=1.0, next_state=[-0.00355654  0.41155511  0.03016796 -0.51634759]\n",
      "[ episode 235 ][ timestamp 3 ] state=[-0.00355654  0.41155511  0.03016796 -0.51634759], action=0, reward=1.0, next_state=[ 0.00467456  0.21602163  0.019841   -0.21431279]\n",
      "[ episode 235 ][ timestamp 4 ] state=[ 0.00467456  0.21602163  0.019841   -0.21431279], action=1, reward=1.0, next_state=[ 0.008995    0.41085438  0.01555475 -0.50067153]\n",
      "[ episode 235 ][ timestamp 5 ] state=[ 0.008995    0.41085438  0.01555475 -0.50067153], action=1, reward=1.0, next_state=[ 0.01721208  0.60575365  0.00554132 -0.78841215]\n",
      "[ episode 235 ][ timestamp 6 ] state=[ 0.01721208  0.60575365  0.00554132 -0.78841215], action=0, reward=1.0, next_state=[ 0.02932716  0.41055603 -0.01022693 -0.49399107]\n",
      "[ episode 235 ][ timestamp 7 ] state=[ 0.02932716  0.41055603 -0.01022693 -0.49399107], action=0, reward=1.0, next_state=[ 0.03753828  0.21557979 -0.02010675 -0.20454868]\n",
      "[ episode 235 ][ timestamp 8 ] state=[ 0.03753828  0.21557979 -0.02010675 -0.20454868], action=0, reward=1.0, next_state=[ 0.04184987  0.02075106 -0.02419772  0.08172436]\n",
      "[ episode 235 ][ timestamp 9 ] state=[ 0.04184987  0.02075106 -0.02419772  0.08172436], action=1, reward=1.0, next_state=[ 0.04226489  0.21621138 -0.02256323 -0.21849372]\n",
      "[ episode 235 ][ timestamp 10 ] state=[ 0.04226489  0.21621138 -0.02256323 -0.21849372], action=0, reward=1.0, next_state=[ 0.04658912  0.02141911 -0.02693311  0.06698728]\n",
      "[ episode 235 ][ timestamp 11 ] state=[ 0.04658912  0.02141911 -0.02693311  0.06698728], action=1, reward=1.0, next_state=[ 0.0470175   0.21691665 -0.02559336 -0.23407005]\n",
      "[ episode 235 ][ timestamp 12 ] state=[ 0.0470175   0.21691665 -0.02559336 -0.23407005], action=0, reward=1.0, next_state=[ 0.05135584  0.02216955 -0.03027476  0.05043131]\n",
      "[ episode 235 ][ timestamp 13 ] state=[ 0.05135584  0.02216955 -0.03027476  0.05043131], action=1, reward=1.0, next_state=[ 0.05179923  0.21771223 -0.02926614 -0.25164773]\n",
      "[ episode 235 ][ timestamp 14 ] state=[ 0.05179923  0.21771223 -0.02926614 -0.25164773], action=0, reward=1.0, next_state=[ 0.05615347  0.02302015 -0.03429909  0.03166232]\n",
      "[ episode 235 ][ timestamp 15 ] state=[ 0.05615347  0.02302015 -0.03429909  0.03166232], action=0, reward=1.0, next_state=[ 0.05661388 -0.17159358 -0.03366585  0.31332926]\n",
      "[ episode 235 ][ timestamp 16 ] state=[ 0.05661388 -0.17159358 -0.03366585  0.31332926], action=0, reward=1.0, next_state=[ 0.053182   -0.36622016 -0.02739926  0.59520783]\n",
      "[ episode 235 ][ timestamp 17 ] state=[ 0.053182   -0.36622016 -0.02739926  0.59520783], action=1, reward=1.0, next_state=[ 0.0458576  -0.17072567 -0.0154951   0.29402177]\n",
      "[ episode 235 ][ timestamp 18 ] state=[ 0.0458576  -0.17072567 -0.0154951   0.29402177], action=1, reward=1.0, next_state=[ 0.04244309  0.02461373 -0.00961467 -0.00350753]\n",
      "[ episode 235 ][ timestamp 19 ] state=[ 0.04244309  0.02461373 -0.00961467 -0.00350753], action=1, reward=1.0, next_state=[ 0.04293536  0.21987224 -0.00968482 -0.29920843]\n",
      "[ episode 235 ][ timestamp 20 ] state=[ 0.04293536  0.21987224 -0.00968482 -0.29920843], action=0, reward=1.0, next_state=[ 0.04733281  0.02488967 -0.01566899 -0.00959558]\n",
      "[ episode 235 ][ timestamp 21 ] state=[ 0.04733281  0.02488967 -0.01566899 -0.00959558], action=1, reward=1.0, next_state=[ 0.0478306   0.22023279 -0.0158609  -0.30718078]\n",
      "[ episode 235 ][ timestamp 22 ] state=[ 0.0478306   0.22023279 -0.0158609  -0.30718078], action=0, reward=1.0, next_state=[ 0.05223526  0.02534039 -0.02200451 -0.01954187]\n",
      "[ episode 235 ][ timestamp 23 ] state=[ 0.05223526  0.02534039 -0.02200451 -0.01954187], action=1, reward=1.0, next_state=[ 0.05274206  0.22077089 -0.02239535 -0.31908545]\n",
      "[ episode 235 ][ timestamp 24 ] state=[ 0.05274206  0.22077089 -0.02239535 -0.31908545], action=0, reward=1.0, next_state=[ 0.05715748  0.02597494 -0.02877706 -0.03354848]\n",
      "[ episode 235 ][ timestamp 25 ] state=[ 0.05715748  0.02597494 -0.02877706 -0.03354848], action=1, reward=1.0, next_state=[ 0.05767698  0.22149749 -0.02944803 -0.33517016]\n",
      "[ episode 235 ][ timestamp 26 ] state=[ 0.05767698  0.22149749 -0.02944803 -0.33517016], action=0, reward=1.0, next_state=[ 0.06210693  0.02680674 -0.03615143 -0.05191713]\n",
      "[ episode 235 ][ timestamp 27 ] state=[ 0.06210693  0.02680674 -0.03615143 -0.05191713], action=1, reward=1.0, next_state=[ 0.06264306  0.22242792 -0.03718978 -0.35578337]\n",
      "[ episode 235 ][ timestamp 28 ] state=[ 0.06264306  0.22242792 -0.03718978 -0.35578337], action=0, reward=1.0, next_state=[ 0.06709162  0.02785392 -0.04430544 -0.07505546]\n",
      "[ episode 235 ][ timestamp 29 ] state=[ 0.06709162  0.02785392 -0.04430544 -0.07505546], action=1, reward=1.0, next_state=[ 0.0676487   0.22358211 -0.04580655 -0.38138117]\n",
      "[ episode 235 ][ timestamp 30 ] state=[ 0.0676487   0.22358211 -0.04580655 -0.38138117], action=0, reward=1.0, next_state=[ 0.07212034  0.0291395  -0.05343418 -0.10348562]\n",
      "[ episode 235 ][ timestamp 31 ] state=[ 0.07212034  0.0291395  -0.05343418 -0.10348562], action=1, reward=1.0, next_state=[ 0.07270313  0.22498489 -0.05550389 -0.41253658]\n",
      "[ episode 235 ][ timestamp 32 ] state=[ 0.07270313  0.22498489 -0.05550389 -0.41253658], action=0, reward=1.0, next_state=[ 0.07720283  0.03069185 -0.06375462 -0.13785559]\n",
      "[ episode 235 ][ timestamp 33 ] state=[ 0.07720283  0.03069185 -0.06375462 -0.13785559], action=1, reward=1.0, next_state=[ 0.07781667  0.22666627 -0.06651173 -0.44995117]\n",
      "[ episode 235 ][ timestamp 34 ] state=[ 0.07781667  0.22666627 -0.06651173 -0.44995117], action=0, reward=1.0, next_state=[ 0.08234999  0.032545   -0.07551076 -0.17895312]\n",
      "[ episode 235 ][ timestamp 35 ] state=[ 0.08234999  0.032545   -0.07551076 -0.17895312], action=1, reward=1.0, next_state=[ 0.08300089  0.22866175 -0.07908982 -0.49446904]\n",
      "[ episode 235 ][ timestamp 36 ] state=[ 0.08300089  0.22866175 -0.07908982 -0.49446904], action=0, reward=1.0, next_state=[ 0.08757413  0.03473907 -0.0889792  -0.22772248]\n",
      "[ episode 235 ][ timestamp 37 ] state=[ 0.08757413  0.03473907 -0.0889792  -0.22772248], action=1, reward=1.0, next_state=[ 0.08826891  0.23101248 -0.09353365 -0.54709329]\n",
      "[ episode 235 ][ timestamp 38 ] state=[ 0.08826891  0.23101248 -0.09353365 -0.54709329], action=0, reward=1.0, next_state=[ 0.09288916  0.03732049 -0.10447551 -0.28528408]\n",
      "[ episode 235 ][ timestamp 39 ] state=[ 0.09288916  0.03732049 -0.10447551 -0.28528408], action=1, reward=1.0, next_state=[ 0.09363557  0.23376529 -0.1101812  -0.60900453]\n",
      "[ episode 235 ][ timestamp 40 ] state=[ 0.09363557  0.23376529 -0.1101812  -0.60900453], action=1, reward=1.0, next_state=[ 0.09831088  0.43024103 -0.12236129 -0.93425883]\n",
      "[ episode 235 ][ timestamp 41 ] state=[ 0.09831088  0.43024103 -0.12236129 -0.93425883], action=0, reward=1.0, next_state=[ 0.1069157   0.23696321 -0.14104646 -0.68239426]\n",
      "[ episode 235 ][ timestamp 42 ] state=[ 0.1069157   0.23696321 -0.14104646 -0.68239426], action=0, reward=1.0, next_state=[ 0.11165496  0.04405244 -0.15469435 -0.43723198]\n",
      "[ episode 235 ][ timestamp 43 ] state=[ 0.11165496  0.04405244 -0.15469435 -0.43723198], action=1, reward=1.0, next_state=[ 0.11253601  0.24098688 -0.16343899 -0.77440512]\n",
      "[ episode 235 ][ timestamp 44 ] state=[ 0.11253601  0.24098688 -0.16343899 -0.77440512], action=0, reward=1.0, next_state=[ 0.11735575  0.04844516 -0.17892709 -0.5372788 ]\n",
      "[ episode 235 ][ timestamp 45 ] state=[ 0.11735575  0.04844516 -0.17892709 -0.5372788 ], action=1, reward=1.0, next_state=[ 0.11832465  0.24557177 -0.18967267 -0.88057242]\n",
      "[ episode 235 ][ timestamp 46 ] state=[ 0.11832465  0.24557177 -0.18967267 -0.88057242], action=0, reward=1.0, next_state=[ 0.12323609  0.05346227 -0.20728411 -0.65300612]\n",
      "[ episode 235 ][ timestamp 47 ] state=[ 0.12323609  0.05346227 -0.20728411 -0.65300612], action=1, reward=-1.0, next_state=[ 0.12430533  0.25077544 -0.22034424 -1.00314623]\n",
      "[ Ended! ] Episode 235: Exploration_rate=0.30945741577570285. Score=47.\n",
      "[ Experience replay ] starts\n",
      "[ episode 236 ] state=[-0.02177098  0.02255062 -0.02459229 -0.01149009]\n",
      "[ episode 236 ][ timestamp 1 ] state=[-0.02177098  0.02255062 -0.02459229 -0.01149009], action=1, reward=1.0, next_state=[-0.02131997  0.21801647 -0.02482209 -0.31182961]\n",
      "[ episode 236 ][ timestamp 2 ] state=[-0.02131997  0.21801647 -0.02482209 -0.31182961], action=0, reward=1.0, next_state=[-0.01695964  0.02325678 -0.03105868 -0.02707701]\n",
      "[ episode 236 ][ timestamp 3 ] state=[-0.01695964  0.02325678 -0.03105868 -0.02707701], action=0, reward=1.0, next_state=[-0.01649451 -0.17140632 -0.03160022  0.25564703]\n",
      "[ episode 236 ][ timestamp 4 ] state=[-0.01649451 -0.17140632 -0.03160022  0.25564703], action=0, reward=1.0, next_state=[-0.01992263 -0.36606317 -0.02648728  0.53819762]\n",
      "[ episode 236 ][ timestamp 5 ] state=[-0.01992263 -0.36606317 -0.02648728  0.53819762], action=0, reward=1.0, next_state=[-0.0272439  -0.56080293 -0.01572333  0.82241843]\n",
      "[ episode 236 ][ timestamp 6 ] state=[-0.0272439  -0.56080293 -0.01572333  0.82241843], action=1, reward=1.0, next_state=[-0.03845995 -0.36546942  0.00072504  0.52483193]\n",
      "[ episode 236 ][ timestamp 7 ] state=[-0.03845995 -0.36546942  0.00072504  0.52483193], action=1, reward=1.0, next_state=[-0.04576934 -0.17035768  0.01122168  0.23237756]\n",
      "[ episode 236 ][ timestamp 8 ] state=[-0.04576934 -0.17035768  0.01122168  0.23237756], action=0, reward=1.0, next_state=[-0.0491765  -0.36563817  0.01586923  0.52857894]\n",
      "[ episode 236 ][ timestamp 9 ] state=[-0.0491765  -0.36563817  0.01586923  0.52857894], action=1, reward=1.0, next_state=[-0.05648926 -0.17074303  0.02644081  0.24093841]\n",
      "[ episode 236 ][ timestamp 10 ] state=[-0.05648926 -0.17074303  0.02644081  0.24093841], action=0, reward=1.0, next_state=[-0.05990412 -0.36623251  0.03125958  0.54184282]\n",
      "[ episode 236 ][ timestamp 11 ] state=[-0.05990412 -0.36623251  0.03125958  0.54184282], action=1, reward=1.0, next_state=[-0.06722877 -0.17156354  0.04209643  0.25917084]\n",
      "[ episode 236 ][ timestamp 12 ] state=[-0.06722877 -0.17156354  0.04209643  0.25917084], action=0, reward=1.0, next_state=[-0.07066004 -0.36726038  0.04727985  0.56482874]\n",
      "[ episode 236 ][ timestamp 13 ] state=[-0.07066004 -0.36726038  0.04727985  0.56482874], action=1, reward=1.0, next_state=[-0.07800525 -0.17283255  0.05857642  0.287408  ]\n",
      "[ episode 236 ][ timestamp 14 ] state=[-0.07800525 -0.17283255  0.05857642  0.287408  ], action=0, reward=1.0, next_state=[-0.0814619  -0.3687388   0.06432458  0.597975  ]\n",
      "[ episode 236 ][ timestamp 15 ] state=[-0.0814619  -0.3687388   0.06432458  0.597975  ], action=0, reward=1.0, next_state=[-0.08883668 -0.56469905  0.07628408  0.91020586]\n",
      "[ episode 236 ][ timestamp 16 ] state=[-0.08883668 -0.56469905  0.07628408  0.91020586], action=1, reward=1.0, next_state=[-0.10013066 -0.37068774  0.0944882   0.64244101]\n",
      "[ episode 236 ][ timestamp 17 ] state=[-0.10013066 -0.37068774  0.0944882   0.64244101], action=1, reward=1.0, next_state=[-0.10754441 -0.177001    0.10733702  0.38094507]\n",
      "[ episode 236 ][ timestamp 18 ] state=[-0.10754441 -0.177001    0.10733702  0.38094507], action=1, reward=1.0, next_state=[-0.11108443  0.01644609  0.11495592  0.12394092]\n",
      "[ episode 236 ][ timestamp 19 ] state=[-0.11108443  0.01644609  0.11495592  0.12394092], action=0, reward=1.0, next_state=[-0.11075551 -0.18011906  0.11743474  0.45056526]\n",
      "[ episode 236 ][ timestamp 20 ] state=[-0.11075551 -0.18011906  0.11743474  0.45056526], action=1, reward=1.0, next_state=[-0.11435789  0.01316321  0.12644605  0.19708523]\n",
      "[ episode 236 ][ timestamp 21 ] state=[-0.11435789  0.01316321  0.12644605  0.19708523], action=0, reward=1.0, next_state=[-0.11409463 -0.18351924  0.13038775  0.52682969]\n",
      "[ episode 236 ][ timestamp 22 ] state=[-0.11409463 -0.18351924  0.13038775  0.52682969], action=1, reward=1.0, next_state=[-0.11776501  0.00955027  0.14092435  0.27790918]\n",
      "[ episode 236 ][ timestamp 23 ] state=[-0.11776501  0.00955027  0.14092435  0.27790918], action=1, reward=1.0, next_state=[-0.11757401  0.20240999  0.14648253  0.0327822 ]\n",
      "[ episode 236 ][ timestamp 24 ] state=[-0.11757401  0.20240999  0.14648253  0.0327822 ], action=0, reward=1.0, next_state=[-0.11352581  0.00552426  0.14713817  0.36786003]\n",
      "[ episode 236 ][ timestamp 25 ] state=[-0.11352581  0.00552426  0.14713817  0.36786003], action=1, reward=1.0, next_state=[-0.11341532  0.19828256  0.15449537  0.12494951]\n",
      "[ episode 236 ][ timestamp 26 ] state=[-0.11341532  0.19828256  0.15449537  0.12494951], action=0, reward=1.0, next_state=[-0.10944967  0.00132361  0.15699436  0.46211021]\n",
      "[ episode 236 ][ timestamp 27 ] state=[-0.10944967  0.00132361  0.15699436  0.46211021], action=1, reward=1.0, next_state=[-0.1094232   0.19391893  0.16623657  0.22273711]\n",
      "[ episode 236 ][ timestamp 28 ] state=[-0.1094232   0.19391893  0.16623657  0.22273711], action=1, reward=1.0, next_state=[-0.10554482  0.38632301  0.17069131 -0.01324167]\n",
      "[ episode 236 ][ timestamp 29 ] state=[-0.10554482  0.38632301  0.17069131 -0.01324167], action=0, reward=1.0, next_state=[-0.09781836  0.18921654  0.17042648  0.3280613 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 236 ][ timestamp 30 ] state=[-0.09781836  0.18921654  0.17042648  0.3280613 ], action=1, reward=1.0, next_state=[-0.09403403  0.38155436  0.1769877   0.09359748]\n",
      "[ episode 236 ][ timestamp 31 ] state=[-0.09403403  0.38155436  0.1769877   0.09359748], action=0, reward=1.0, next_state=[-0.08640294  0.18439553  0.17885965  0.43647901]\n",
      "[ episode 236 ][ timestamp 32 ] state=[-0.08640294  0.18439553  0.17885965  0.43647901], action=1, reward=1.0, next_state=[-0.08271503  0.37659467  0.18758923  0.20508428]\n",
      "[ episode 236 ][ timestamp 33 ] state=[-0.08271503  0.37659467  0.18758923  0.20508428], action=1, reward=1.0, next_state=[-0.07518314  0.56860801  0.19169092 -0.02305456]\n",
      "[ episode 236 ][ timestamp 34 ] state=[-0.07518314  0.56860801  0.19169092 -0.02305456], action=0, reward=1.0, next_state=[-0.06381098  0.37132784  0.19122983  0.32345809]\n",
      "[ episode 236 ][ timestamp 35 ] state=[-0.06381098  0.37132784  0.19122983  0.32345809], action=1, reward=1.0, next_state=[-0.05638442  0.56328565  0.19769899  0.09664965]\n",
      "[ episode 236 ][ timestamp 36 ] state=[-0.05638442  0.56328565  0.19769899  0.09664965], action=0, reward=1.0, next_state=[-0.04511871  0.3659605   0.19963198  0.44461747]\n",
      "[ episode 236 ][ timestamp 37 ] state=[-0.04511871  0.3659605   0.19963198  0.44461747], action=1, reward=1.0, next_state=[-0.0377995   0.55778118  0.20852433  0.22090364]\n",
      "[ episode 236 ][ timestamp 38 ] state=[-0.0377995   0.55778118  0.20852433  0.22090364], action=1, reward=-1.0, next_state=[-2.66438742e-02  7.49407481e-01  2.12942404e-01  5.53684383e-04]\n",
      "[ Ended! ] Episode 236: Exploration_rate=0.3079101286968243. Score=38.\n",
      "[ Experience replay ] starts\n",
      "[ episode 237 ] state=[-0.02140228  0.02868376  0.02616207  0.02768414]\n",
      "[ episode 237 ][ timestamp 1 ] state=[-0.02140228  0.02868376  0.02616207  0.02768414], action=1, reward=1.0, next_state=[-0.0208286   0.22342095  0.02671575 -0.25663091]\n",
      "[ episode 237 ][ timestamp 2 ] state=[-0.0208286   0.22342095  0.02671575 -0.25663091], action=1, reward=1.0, next_state=[-0.01636018  0.41815149  0.02158313 -0.54076899]\n",
      "[ episode 237 ][ timestamp 3 ] state=[-0.01636018  0.41815149  0.02158313 -0.54076899], action=0, reward=1.0, next_state=[-0.00799715  0.22273292  0.01076775 -0.24136446]\n",
      "[ episode 237 ][ timestamp 4 ] state=[-0.00799715  0.22273292  0.01076775 -0.24136446], action=1, reward=1.0, next_state=[-0.0035425   0.41769942  0.00594046 -0.53063159]\n",
      "[ episode 237 ][ timestamp 5 ] state=[-0.0035425   0.41769942  0.00594046 -0.53063159], action=1, reward=1.0, next_state=[ 0.00481149  0.6127373  -0.00467217 -0.82143677]\n",
      "[ episode 237 ][ timestamp 6 ] state=[ 0.00481149  0.6127373  -0.00467217 -0.82143677], action=1, reward=1.0, next_state=[ 0.01706624  0.80792287 -0.0211009  -1.11558554]\n",
      "[ episode 237 ][ timestamp 7 ] state=[ 0.01706624  0.80792287 -0.0211009  -1.11558554], action=0, reward=1.0, next_state=[ 0.0332247   0.61308418 -0.04341261 -0.82959577]\n",
      "[ episode 237 ][ timestamp 8 ] state=[ 0.0332247   0.61308418 -0.04341261 -0.82959577], action=0, reward=1.0, next_state=[ 0.04548638  0.41858171 -0.06000453 -0.55087625]\n",
      "[ episode 237 ][ timestamp 9 ] state=[ 0.04548638  0.41858171 -0.06000453 -0.55087625], action=0, reward=1.0, next_state=[ 0.05385801  0.2243516  -0.07102205 -0.27768618]\n",
      "[ episode 237 ][ timestamp 10 ] state=[ 0.05385801  0.2243516  -0.07102205 -0.27768618], action=1, reward=1.0, next_state=[ 0.05834505  0.42041111 -0.07657578 -0.59189698]\n",
      "[ episode 237 ][ timestamp 11 ] state=[ 0.05834505  0.42041111 -0.07657578 -0.59189698], action=0, reward=1.0, next_state=[ 0.06675327  0.22644    -0.08841372 -0.32428424]\n",
      "[ episode 237 ][ timestamp 12 ] state=[ 0.06675327  0.22644    -0.08841372 -0.32428424], action=1, reward=1.0, next_state=[ 0.07128207  0.42270231 -0.0948994  -0.6434876 ]\n",
      "[ episode 237 ][ timestamp 13 ] state=[ 0.07128207  0.42270231 -0.0948994  -0.6434876 ], action=0, reward=1.0, next_state=[ 0.07973611  0.22902216 -0.10776915 -0.38213316]\n",
      "[ episode 237 ][ timestamp 14 ] state=[ 0.07973611  0.22902216 -0.10776915 -0.38213316], action=0, reward=1.0, next_state=[ 0.08431656  0.03558227 -0.11541182 -0.12527951]\n",
      "[ episode 237 ][ timestamp 15 ] state=[ 0.08431656  0.03558227 -0.11541182 -0.12527951], action=1, reward=1.0, next_state=[ 0.0850282   0.23215224 -0.11791741 -0.45202873]\n",
      "[ episode 237 ][ timestamp 16 ] state=[ 0.0850282   0.23215224 -0.11791741 -0.45202873], action=1, reward=1.0, next_state=[ 0.08967125  0.42872715 -0.12695798 -0.77943094]\n",
      "[ episode 237 ][ timestamp 17 ] state=[ 0.08967125  0.42872715 -0.12695798 -0.77943094], action=0, reward=1.0, next_state=[ 0.09824579  0.23555801 -0.1425466  -0.52923473]\n",
      "[ episode 237 ][ timestamp 18 ] state=[ 0.09824579  0.23555801 -0.1425466  -0.52923473], action=1, reward=1.0, next_state=[ 0.10295695  0.43236725 -0.1531313  -0.86322127]\n",
      "[ episode 237 ][ timestamp 19 ] state=[ 0.10295695  0.43236725 -0.1531313  -0.86322127], action=0, reward=1.0, next_state=[ 0.1116043   0.2396245  -0.17039572 -0.62233513]\n",
      "[ episode 237 ][ timestamp 20 ] state=[ 0.1116043   0.2396245  -0.17039572 -0.62233513], action=1, reward=1.0, next_state=[ 0.11639679  0.4366645  -0.18284242 -0.96346902]\n",
      "[ episode 237 ][ timestamp 21 ] state=[ 0.11639679  0.4366645  -0.18284242 -0.96346902], action=0, reward=1.0, next_state=[ 0.12513008  0.24440713 -0.2021118  -0.73334679]\n",
      "[ episode 237 ][ timestamp 22 ] state=[ 0.12513008  0.24440713 -0.2021118  -0.73334679], action=0, reward=-1.0, next_state=[ 0.13001822  0.05256579 -0.21677874 -0.51045935]\n",
      "[ Ended! ] Episode 237: Exploration_rate=0.3063705780533402. Score=22.\n",
      "[ Experience replay ] starts\n",
      "[ episode 238 ] state=[-0.00862332  0.03673552  0.0017195  -0.00299467]\n",
      "[ episode 238 ][ timestamp 1 ] state=[-0.00862332  0.03673552  0.0017195  -0.00299467], action=1, reward=1.0, next_state=[-0.00788861  0.23183277  0.0016596  -0.29513458]\n",
      "[ episode 238 ][ timestamp 2 ] state=[-0.00788861  0.23183277  0.0016596  -0.29513458], action=0, reward=1.0, next_state=[-0.00325196  0.0366872  -0.00424309 -0.0019287 ]\n",
      "[ episode 238 ][ timestamp 3 ] state=[-0.00325196  0.0366872  -0.00424309 -0.0019287 ], action=0, reward=1.0, next_state=[-0.00251821 -0.15837364 -0.00428166  0.28941247]\n",
      "[ episode 238 ][ timestamp 4 ] state=[-0.00251821 -0.15837364 -0.00428166  0.28941247], action=1, reward=1.0, next_state=[-0.00568569  0.0368091   0.00150659 -0.00461777]\n",
      "[ episode 238 ][ timestamp 5 ] state=[-0.00568569  0.0368091   0.00150659 -0.00461777], action=1, reward=1.0, next_state=[-0.0049495   0.23190941  0.00141423 -0.29682497]\n",
      "[ episode 238 ][ timestamp 6 ] state=[-0.0049495   0.23190941  0.00141423 -0.29682497], action=0, reward=1.0, next_state=[-0.00031132  0.03676733 -0.00452227 -0.00369635]\n",
      "[ episode 238 ][ timestamp 7 ] state=[-0.00031132  0.03676733 -0.00452227 -0.00369635], action=0, reward=1.0, next_state=[ 0.00042403 -0.15828947 -0.00459619  0.28755632]\n",
      "[ episode 238 ][ timestamp 8 ] state=[ 0.00042403 -0.15828947 -0.00459619  0.28755632], action=1, reward=1.0, next_state=[-0.00274176  0.03689772  0.00115493 -0.00657265]\n",
      "[ episode 238 ][ timestamp 9 ] state=[-0.00274176  0.03689772  0.00115493 -0.00657265], action=0, reward=1.0, next_state=[-0.0020038  -0.15824077  0.00102348  0.28647445]\n",
      "[ episode 238 ][ timestamp 10 ] state=[-0.0020038  -0.15824077  0.00102348  0.28647445], action=0, reward=1.0, next_state=[-0.00516862 -0.35337731  0.00675297  0.57948   ]\n",
      "[ episode 238 ][ timestamp 11 ] state=[-0.00516862 -0.35337731  0.00675297  0.57948   ], action=0, reward=1.0, next_state=[-0.01223617 -0.54859324  0.01834257  0.87428257]\n",
      "[ episode 238 ][ timestamp 12 ] state=[-0.01223617 -0.54859324  0.01834257  0.87428257], action=1, reward=1.0, next_state=[-0.02320803 -0.35372541  0.03582822  0.58742241]\n",
      "[ episode 238 ][ timestamp 13 ] state=[-0.02320803 -0.35372541  0.03582822  0.58742241], action=1, reward=1.0, next_state=[-0.03028254 -0.15912306  0.04757667  0.30623746]\n",
      "[ episode 238 ][ timestamp 14 ] state=[-0.03028254 -0.15912306  0.04757667  0.30623746], action=1, reward=1.0, next_state=[-0.033465    0.03528978  0.05370142  0.02893045]\n",
      "[ episode 238 ][ timestamp 15 ] state=[-0.033465    0.03528978  0.05370142  0.02893045], action=0, reward=1.0, next_state=[-0.0327592  -0.16055952  0.05428003  0.33806153]\n",
      "[ episode 238 ][ timestamp 16 ] state=[-0.0327592  -0.16055952  0.05428003  0.33806153], action=0, reward=1.0, next_state=[-0.03597039 -0.35641017  0.06104126  0.64735532]\n",
      "[ episode 238 ][ timestamp 17 ] state=[-0.03597039 -0.35641017  0.06104126  0.64735532], action=1, reward=1.0, next_state=[-0.0430986  -0.16218941  0.07398836  0.37450176]\n",
      "[ episode 238 ][ timestamp 18 ] state=[-0.0430986  -0.16218941  0.07398836  0.37450176], action=0, reward=1.0, next_state=[-0.04634239 -0.35828007  0.0814784   0.68956578]\n",
      "[ episode 238 ][ timestamp 19 ] state=[-0.04634239 -0.35828007  0.0814784   0.68956578], action=0, reward=1.0, next_state=[-0.05350799 -0.55443255  0.09526972  1.00674653]\n",
      "[ episode 238 ][ timestamp 20 ] state=[-0.05350799 -0.55443255  0.09526972  1.00674653], action=1, reward=1.0, next_state=[-0.06459664 -0.36070286  0.11540465  0.74543671]\n",
      "[ episode 238 ][ timestamp 21 ] state=[-0.06459664 -0.36070286  0.11540465  0.74543671], action=0, reward=1.0, next_state=[-0.0718107  -0.55721216  0.13031338  1.07209367]\n",
      "[ episode 238 ][ timestamp 22 ] state=[-0.0718107  -0.55721216  0.13031338  1.07209367], action=1, reward=1.0, next_state=[-0.08295494 -0.36403097  0.15175525  0.82298259]\n",
      "[ episode 238 ][ timestamp 23 ] state=[-0.08295494 -0.36403097  0.15175525  0.82298259], action=1, reward=1.0, next_state=[-0.09023556 -0.17127459  0.16821491  0.58161596]\n",
      "[ episode 238 ][ timestamp 24 ] state=[-0.09023556 -0.17127459  0.16821491  0.58161596], action=1, reward=1.0, next_state=[-0.09366105  0.02114078  0.17984722  0.34628905]\n",
      "[ episode 238 ][ timestamp 25 ] state=[-0.09366105  0.02114078  0.17984722  0.34628905], action=0, reward=1.0, next_state=[-0.09323823 -0.17602264  0.18677301  0.68985462]\n",
      "[ episode 238 ][ timestamp 26 ] state=[-0.09323823 -0.17602264  0.18677301  0.68985462], action=1, reward=1.0, next_state=[-0.09675869  0.01608411  0.2005701   0.46129857]\n",
      "[ episode 238 ][ timestamp 27 ] state=[-0.09675869  0.01608411  0.2005701   0.46129857], action=0, reward=-1.0, next_state=[-0.09643701 -0.18122339  0.20979607  0.80989977]\n",
      "[ Ended! ] Episode 238: Exploration_rate=0.30483872516307353. Score=27.\n",
      "[ Experience replay ] starts\n",
      "[ episode 239 ] state=[-0.03989988  0.01565507  0.01961039  0.00860984]\n",
      "[ episode 239 ][ timestamp 1 ] state=[-0.03989988  0.01565507  0.01961039  0.00860984], action=0, reward=1.0, next_state=[-0.03958678 -0.17974256  0.01978259  0.307415  ]\n",
      "[ episode 239 ][ timestamp 2 ] state=[-0.03958678 -0.17974256  0.01978259  0.307415  ], action=1, reward=1.0, next_state=[-0.04318163  0.015092    0.02593089  0.02103605]\n",
      "[ episode 239 ][ timestamp 3 ] state=[-0.04318163  0.015092    0.02593089  0.02103605], action=1, reward=1.0, next_state=[-0.04287979  0.20983267  0.02635161 -0.26335391]\n",
      "[ episode 239 ][ timestamp 4 ] state=[-0.04287979  0.20983267  0.02635161 -0.26335391], action=0, reward=1.0, next_state=[-0.03868314  0.01434468  0.02108453  0.03752273]\n",
      "[ episode 239 ][ timestamp 5 ] state=[-0.03868314  0.01434468  0.02108453  0.03752273], action=0, reward=1.0, next_state=[-0.03839624 -0.18107318  0.02183499  0.33678276]\n",
      "[ episode 239 ][ timestamp 6 ] state=[-0.03839624 -0.18107318  0.02183499  0.33678276], action=1, reward=1.0, next_state=[-0.04201771  0.01373135  0.02857064  0.05106461]\n",
      "[ episode 239 ][ timestamp 7 ] state=[-0.04201771  0.01373135  0.02857064  0.05106461], action=0, reward=1.0, next_state=[-0.04174308 -0.18178838  0.02959194  0.35262313]\n",
      "[ episode 239 ][ timestamp 8 ] state=[-0.04174308 -0.18178838  0.02959194  0.35262313], action=1, reward=1.0, next_state=[-0.04537885  0.01290055  0.0366444   0.06941635]\n",
      "[ episode 239 ][ timestamp 9 ] state=[-0.04537885  0.01290055  0.0366444   0.06941635], action=0, reward=1.0, next_state=[-0.04512084 -0.18272708  0.03803273  0.37343185]\n",
      "[ episode 239 ][ timestamp 10 ] state=[-0.04512084 -0.18272708  0.03803273  0.37343185], action=0, reward=1.0, next_state=[-0.04877538 -0.37836808  0.04550136  0.67786005]\n",
      "[ episode 239 ][ timestamp 11 ] state=[-0.04877538 -0.37836808  0.04550136  0.67786005], action=1, reward=1.0, next_state=[-0.05634274 -0.18390683  0.05905856  0.39984287]\n",
      "[ episode 239 ][ timestamp 12 ] state=[-0.05634274 -0.18390683  0.05905856  0.39984287], action=1, reward=1.0, next_state=[-0.06002088  0.01032979  0.06705542  0.12634902]\n",
      "[ episode 239 ][ timestamp 13 ] state=[-0.06002088  0.01032979  0.06705542  0.12634902], action=0, reward=1.0, next_state=[-0.05981428 -0.18568552  0.0695824   0.43941073]\n",
      "[ episode 239 ][ timestamp 14 ] state=[-0.05981428 -0.18568552  0.0695824   0.43941073], action=1, reward=1.0, next_state=[-0.06352799  0.00838619  0.07837062  0.16944833]\n",
      "[ episode 239 ][ timestamp 15 ] state=[-0.06352799  0.00838619  0.07837062  0.16944833], action=0, reward=1.0, next_state=[-0.06336027 -0.18776495  0.08175958  0.48578932]\n",
      "[ episode 239 ][ timestamp 16 ] state=[-0.06336027 -0.18776495  0.08175958  0.48578932], action=1, reward=1.0, next_state=[-0.06711557  0.00611386  0.09147537  0.21995313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 239 ][ timestamp 17 ] state=[-0.06711557  0.00611386  0.09147537  0.21995313], action=1, reward=1.0, next_state=[-0.06699329  0.19981727  0.09587443 -0.04253092]\n",
      "[ episode 239 ][ timestamp 18 ] state=[-0.06699329  0.19981727  0.09587443 -0.04253092], action=0, reward=1.0, next_state=[-0.06299694  0.00346058  0.09502381  0.2787954 ]\n",
      "[ episode 239 ][ timestamp 19 ] state=[-0.06299694  0.00346058  0.09502381  0.2787954 ], action=1, reward=1.0, next_state=[-0.06292773  0.19710757  0.10059972  0.0175303 ]\n",
      "[ episode 239 ][ timestamp 20 ] state=[-0.06292773  0.19710757  0.10059972  0.0175303 ], action=0, reward=1.0, next_state=[-0.05898558  0.00069757  0.10095033  0.34018222]\n",
      "[ episode 239 ][ timestamp 21 ] state=[-0.05898558  0.00069757  0.10095033  0.34018222], action=1, reward=1.0, next_state=[-0.05897163  0.1942491   0.10775397  0.08096204]\n",
      "[ episode 239 ][ timestamp 22 ] state=[-0.05897163  0.1942491   0.10775397  0.08096204], action=0, reward=1.0, next_state=[-0.05508665 -0.00223929  0.10937321  0.40560362]\n",
      "[ episode 239 ][ timestamp 23 ] state=[-0.05508665 -0.00223929  0.10937321  0.40560362], action=0, reward=1.0, next_state=[-0.05513143 -0.19872851  0.11748528  0.73066798]\n",
      "[ episode 239 ][ timestamp 24 ] state=[-0.05513143 -0.19872851  0.11748528  0.73066798], action=1, reward=1.0, next_state=[-0.059106   -0.0054093   0.13209864  0.4771494 ]\n",
      "[ episode 239 ][ timestamp 25 ] state=[-0.059106   -0.0054093   0.13209864  0.4771494 ], action=1, reward=1.0, next_state=[-0.05921419  0.18762434  0.14164163  0.22884575]\n",
      "[ episode 239 ][ timestamp 26 ] state=[-0.05921419  0.18762434  0.14164163  0.22884575], action=1, reward=1.0, next_state=[-0.0554617   0.38046801  0.14621855 -0.0160194 ]\n",
      "[ episode 239 ][ timestamp 27 ] state=[-0.0554617   0.38046801  0.14621855 -0.0160194 ], action=0, reward=1.0, next_state=[-0.04785234  0.18358468  0.14589816  0.31898945]\n",
      "[ episode 239 ][ timestamp 28 ] state=[-0.04785234  0.18358468  0.14589816  0.31898945], action=1, reward=1.0, next_state=[-0.04418065  0.37636011  0.15227795  0.0756405 ]\n",
      "[ episode 239 ][ timestamp 29 ] state=[-0.04418065  0.37636011  0.15227795  0.0756405 ], action=1, reward=1.0, next_state=[-0.03665345  0.56900827  0.15379076 -0.16539089]\n",
      "[ episode 239 ][ timestamp 30 ] state=[-0.03665345  0.56900827  0.15379076 -0.16539089], action=0, reward=1.0, next_state=[-0.02527328  0.3720576   0.15048294  0.17158481]\n",
      "[ episode 239 ][ timestamp 31 ] state=[-0.02527328  0.3720576   0.15048294  0.17158481], action=1, reward=1.0, next_state=[-0.01783213  0.56474128  0.15391464 -0.07009918]\n",
      "[ episode 239 ][ timestamp 32 ] state=[-0.01783213  0.56474128  0.15391464 -0.07009918], action=0, reward=1.0, next_state=[-0.0065373   0.36778611  0.15251265  0.26691357]\n",
      "[ episode 239 ][ timestamp 33 ] state=[-0.0065373   0.36778611  0.15251265  0.26691357], action=1, reward=1.0, next_state=[0.00081842 0.56043977 0.15785093 0.02595252]\n",
      "[ episode 239 ][ timestamp 34 ] state=[0.00081842 0.56043977 0.15785093 0.02595252], action=1, reward=1.0, next_state=[ 0.01202721  0.75298729  0.15836998 -0.21306226]\n",
      "[ episode 239 ][ timestamp 35 ] state=[ 0.01202721  0.75298729  0.15836998 -0.21306226], action=0, reward=1.0, next_state=[0.02708696 0.55599736 0.15410873 0.12509123]\n",
      "[ episode 239 ][ timestamp 36 ] state=[0.02708696 0.55599736 0.15410873 0.12509123], action=0, reward=1.0, next_state=[0.03820691 0.35904199 0.15661055 0.46215186]\n",
      "[ episode 239 ][ timestamp 37 ] state=[0.03820691 0.35904199 0.15661055 0.46215186], action=1, reward=1.0, next_state=[0.04538775 0.55164413 0.16585359 0.22263988]\n",
      "[ episode 239 ][ timestamp 38 ] state=[0.04538775 0.55164413 0.16585359 0.22263988], action=1, reward=1.0, next_state=[ 0.05642063  0.74405513  0.17030639 -0.01347844]\n",
      "[ episode 239 ][ timestamp 39 ] state=[ 0.05642063  0.74405513  0.17030639 -0.01347844], action=1, reward=1.0, next_state=[ 0.07130173  0.93637739  0.17003682 -0.24795993]\n",
      "[ episode 239 ][ timestamp 40 ] state=[ 0.07130173  0.93637739  0.17003682 -0.24795993], action=0, reward=1.0, next_state=[0.09002928 0.7392867  0.16507762 0.09316288]\n",
      "[ episode 239 ][ timestamp 41 ] state=[0.09002928 0.7392867  0.16507762 0.09316288], action=1, reward=1.0, next_state=[ 0.10481501  0.93170516  0.16694088 -0.1432284 ]\n",
      "[ episode 239 ][ timestamp 42 ] state=[ 0.10481501  0.93170516  0.16694088 -0.1432284 ], action=0, reward=1.0, next_state=[0.12344912 0.73463474 0.16407631 0.19712059]\n",
      "[ episode 239 ][ timestamp 43 ] state=[0.12344912 0.73463474 0.16407631 0.19712059], action=1, reward=1.0, next_state=[ 0.13814181  0.92707611  0.16801872 -0.03964233]\n",
      "[ episode 239 ][ timestamp 44 ] state=[ 0.13814181  0.92707611  0.16801872 -0.03964233], action=1, reward=1.0, next_state=[ 0.15668333  1.11944015  0.16722588 -0.27495968]\n",
      "[ episode 239 ][ timestamp 45 ] state=[ 0.15668333  1.11944015  0.16722588 -0.27495968], action=0, reward=1.0, next_state=[0.17907214 0.92237616 0.16172668 0.06544842]\n",
      "[ episode 239 ][ timestamp 46 ] state=[0.17907214 0.92237616 0.16172668 0.06544842], action=1, reward=1.0, next_state=[ 0.19751966  1.11485445  0.16303565 -0.17216082]\n",
      "[ episode 239 ][ timestamp 47 ] state=[ 0.19751966  1.11485445  0.16303565 -0.17216082], action=0, reward=1.0, next_state=[0.21981675 0.91782007 0.15959244 0.16719189]\n",
      "[ episode 239 ][ timestamp 48 ] state=[0.21981675 0.91782007 0.15959244 0.16719189], action=0, reward=1.0, next_state=[0.23817315 0.72081637 0.16293627 0.50566345]\n",
      "[ episode 239 ][ timestamp 49 ] state=[0.23817315 0.72081637 0.16293627 0.50566345], action=0, reward=1.0, next_state=[0.25258948 0.52381858 0.17304954 0.84493794]\n",
      "[ episode 239 ][ timestamp 50 ] state=[0.25258948 0.52381858 0.17304954 0.84493794], action=0, reward=1.0, next_state=[0.26306585 0.32681141 0.1899483  1.18665803]\n",
      "[ episode 239 ][ timestamp 51 ] state=[0.26306585 0.32681141 0.1899483  1.18665803], action=1, reward=-1.0, next_state=[0.26960208 0.51903227 0.21368146 0.95902226]\n",
      "[ Ended! ] Episode 239: Exploration_rate=0.3033145315372582. Score=51.\n",
      "[ Experience replay ] starts\n",
      "[ episode 240 ] state=[-0.01466157  0.02685285 -0.01967169 -0.02940872]\n",
      "[ episode 240 ][ timestamp 1 ] state=[-0.01466157  0.02685285 -0.01967169 -0.02940872], action=1, reward=1.0, next_state=[-0.01412451  0.2222513  -0.02025987 -0.32823279]\n",
      "[ episode 240 ][ timestamp 2 ] state=[-0.01412451  0.2222513  -0.02025987 -0.32823279], action=1, reward=1.0, next_state=[-0.00967949  0.41765574 -0.02682452 -0.62723528]\n",
      "[ episode 240 ][ timestamp 3 ] state=[-0.00967949  0.41765574 -0.02682452 -0.62723528], action=1, reward=1.0, next_state=[-0.00132637  0.61314162 -0.03936923 -0.92824408]\n",
      "[ episode 240 ][ timestamp 4 ] state=[-0.00132637  0.61314162 -0.03936923 -0.92824408], action=0, reward=1.0, next_state=[ 0.01093646  0.41857267 -0.05793411 -0.64818836]\n",
      "[ episode 240 ][ timestamp 5 ] state=[ 0.01093646  0.41857267 -0.05793411 -0.64818836], action=1, reward=1.0, next_state=[ 0.01930791  0.61445183 -0.07089788 -0.95853727]\n",
      "[ episode 240 ][ timestamp 6 ] state=[ 0.01930791  0.61445183 -0.07089788 -0.95853727], action=0, reward=1.0, next_state=[ 0.03159695  0.42035103 -0.09006862 -0.68894401]\n",
      "[ episode 240 ][ timestamp 7 ] state=[ 0.03159695  0.42035103 -0.09006862 -0.68894401], action=0, reward=1.0, next_state=[ 0.04000397  0.22658687 -0.1038475  -0.42592028]\n",
      "[ episode 240 ][ timestamp 8 ] state=[ 0.04000397  0.22658687 -0.1038475  -0.42592028], action=1, reward=1.0, next_state=[ 0.04453571  0.42301468 -0.11236591 -0.74945097]\n",
      "[ episode 240 ][ timestamp 9 ] state=[ 0.04453571  0.42301468 -0.11236591 -0.74945097], action=0, reward=1.0, next_state=[ 0.052996    0.22960717 -0.12735493 -0.49413538]\n",
      "[ episode 240 ][ timestamp 10 ] state=[ 0.052996    0.22960717 -0.12735493 -0.49413538], action=0, reward=1.0, next_state=[ 0.05758814  0.03648969 -0.13723763 -0.24414636]\n",
      "[ episode 240 ][ timestamp 11 ] state=[ 0.05758814  0.03648969 -0.13723763 -0.24414636], action=1, reward=1.0, next_state=[ 0.05831794  0.23327775 -0.14212056 -0.5767744 ]\n",
      "[ episode 240 ][ timestamp 12 ] state=[ 0.05831794  0.23327775 -0.14212056 -0.5767744 ], action=1, reward=1.0, next_state=[ 0.06298349  0.43007572 -0.15365605 -0.91063806]\n",
      "[ episode 240 ][ timestamp 13 ] state=[ 0.06298349  0.43007572 -0.15365605 -0.91063806], action=0, reward=1.0, next_state=[ 0.07158501  0.23732944 -0.17186881 -0.66992234]\n",
      "[ episode 240 ][ timestamp 14 ] state=[ 0.07158501  0.23732944 -0.17186881 -0.66992234], action=0, reward=1.0, next_state=[ 0.0763316   0.04496091 -0.18526726 -0.43590186]\n",
      "[ episode 240 ][ timestamp 15 ] state=[ 0.0763316   0.04496091 -0.18526726 -0.43590186], action=1, reward=1.0, next_state=[ 0.07723081  0.24215572 -0.1939853  -0.78078971]\n",
      "[ episode 240 ][ timestamp 16 ] state=[ 0.07723081  0.24215572 -0.1939853  -0.78078971], action=1, reward=-1.0, next_state=[ 0.08207393  0.43933962 -0.20960109 -1.12769259]\n",
      "[ Ended! ] Episode 240: Exploration_rate=0.3017979588795719. Score=16.\n",
      "[ Experience replay ] starts\n",
      "[ episode 241 ] state=[-0.00067885  0.01611022  0.01757745  0.04706055]\n",
      "[ episode 241 ][ timestamp 1 ] state=[-0.00067885  0.01611022  0.01757745  0.04706055], action=0, reward=1.0, next_state=[-0.00035664 -0.17925931  0.01851866  0.34523708]\n",
      "[ episode 241 ][ timestamp 2 ] state=[-0.00035664 -0.17925931  0.01851866  0.34523708], action=1, reward=1.0, next_state=[-0.00394183  0.01559438  0.0254234   0.05845083]\n",
      "[ episode 241 ][ timestamp 3 ] state=[-0.00394183  0.01559438  0.0254234   0.05845083], action=0, reward=1.0, next_state=[-0.00362994 -0.17988269  0.02659241  0.35904536]\n",
      "[ episode 241 ][ timestamp 4 ] state=[-0.00362994 -0.17988269  0.02659241  0.35904536], action=1, reward=1.0, next_state=[-0.00722759  0.01485133  0.03377332  0.07486484]\n",
      "[ episode 241 ][ timestamp 5 ] state=[-0.00722759  0.01485133  0.03377332  0.07486484], action=0, reward=1.0, next_state=[-0.00693057 -0.18073811  0.03527062  0.37800916]\n",
      "[ episode 241 ][ timestamp 6 ] state=[-0.00693057 -0.18073811  0.03527062  0.37800916], action=1, reward=1.0, next_state=[-0.01054533  0.01386564  0.0428308   0.0966525 ]\n",
      "[ episode 241 ][ timestamp 7 ] state=[-0.01054533  0.01386564  0.0428308   0.0966525 ], action=0, reward=1.0, next_state=[-0.01026802 -0.18184317  0.04476385  0.4025349 ]\n",
      "[ episode 241 ][ timestamp 8 ] state=[-0.01026802 -0.18184317  0.04476385  0.4025349 ], action=1, reward=1.0, next_state=[-0.01390488  0.01261624  0.05281455  0.12429416]\n",
      "[ episode 241 ][ timestamp 9 ] state=[-0.01390488  0.01261624  0.05281455  0.12429416], action=1, reward=1.0, next_state=[-0.01365256  0.20694333  0.05530043 -0.15126977]\n",
      "[ episode 241 ][ timestamp 10 ] state=[-0.01365256  0.20694333  0.05530043 -0.15126977], action=0, reward=1.0, next_state=[-0.00951369  0.01107493  0.05227504  0.15833375]\n",
      "[ episode 241 ][ timestamp 11 ] state=[-0.00951369  0.01107493  0.05227504  0.15833375], action=1, reward=1.0, next_state=[-0.00929219  0.205411    0.05544171 -0.11741029]\n",
      "[ episode 241 ][ timestamp 12 ] state=[-0.00929219  0.205411    0.05544171 -0.11741029], action=0, reward=1.0, next_state=[-0.00518397  0.00954031  0.05309351  0.19223582]\n",
      "[ episode 241 ][ timestamp 13 ] state=[-0.00518397  0.00954031  0.05309351  0.19223582], action=1, reward=1.0, next_state=[-0.00499316  0.20386412  0.05693822 -0.083237  ]\n",
      "[ episode 241 ][ timestamp 14 ] state=[-0.00499316  0.20386412  0.05693822 -0.083237  ], action=0, reward=1.0, next_state=[-0.00091588  0.00797416  0.05527348  0.22685256]\n",
      "[ episode 241 ][ timestamp 15 ] state=[-0.00091588  0.00797416  0.05527348  0.22685256], action=1, reward=1.0, next_state=[-0.0007564   0.20226441  0.05981053 -0.0478956 ]\n",
      "[ episode 241 ][ timestamp 16 ] state=[-0.0007564   0.20226441  0.05981053 -0.0478956 ], action=0, reward=1.0, next_state=[0.00328889 0.00633809 0.05885262 0.26304219]\n",
      "[ episode 241 ][ timestamp 17 ] state=[0.00328889 0.00633809 0.05885262 0.26304219], action=0, reward=1.0, next_state=[ 0.00341565 -0.18957238  0.06411347  0.57369181]\n",
      "[ episode 241 ][ timestamp 18 ] state=[ 0.00341565 -0.18957238  0.06411347  0.57369181], action=1, reward=1.0, next_state=[-0.0003758   0.00459486  0.0755873   0.30187579]\n",
      "[ episode 241 ][ timestamp 19 ] state=[-0.0003758   0.00459486  0.0755873   0.30187579], action=0, reward=1.0, next_state=[-2.83898860e-04 -1.91518459e-01  8.16248182e-02  6.17407310e-01]\n",
      "[ episode 241 ][ timestamp 20 ] state=[-2.83898860e-04 -1.91518459e-01  8.16248182e-02  6.17407310e-01], action=1, reward=1.0, next_state=[-0.00411427  0.00237407  0.09397296  0.3515079 ]\n",
      "[ episode 241 ][ timestamp 21 ] state=[-0.00411427  0.00237407  0.09397296  0.3515079 ], action=0, reward=1.0, next_state=[-0.00406679 -0.19394987  0.10100312  0.67228189]\n",
      "[ episode 241 ][ timestamp 22 ] state=[-0.00406679 -0.19394987  0.10100312  0.67228189], action=1, reward=1.0, next_state=[-7.94578400e-03 -3.66172656e-04  1.14448760e-01  4.13030693e-01]\n",
      "[ episode 241 ][ timestamp 23 ] state=[-7.94578400e-03 -3.66172656e-04  1.14448760e-01  4.13030693e-01], action=1, reward=1.0, next_state=[-0.00795311  0.19296323  0.12270937  0.15850829]\n",
      "[ episode 241 ][ timestamp 24 ] state=[-0.00795311  0.19296323  0.12270937  0.15850829], action=1, reward=1.0, next_state=[-0.00409384  0.3861342   0.12587954 -0.0930833 ]\n",
      "[ episode 241 ][ timestamp 25 ] state=[-0.00409384  0.3861342   0.12587954 -0.0930833 ], action=0, reward=1.0, next_state=[0.00362884 0.18945386 0.12401787 0.23651382]\n",
      "[ episode 241 ][ timestamp 26 ] state=[0.00362884 0.18945386 0.12401787 0.23651382], action=0, reward=1.0, next_state=[ 0.00741792 -0.00720157  0.12874815  0.56559926]\n",
      "[ episode 241 ][ timestamp 27 ] state=[ 0.00741792 -0.00720157  0.12874815  0.56559926], action=1, reward=1.0, next_state=[0.00727389 0.18590143 0.14006014 0.31608958]\n",
      "[ episode 241 ][ timestamp 28 ] state=[0.00727389 0.18590143 0.14006014 0.31608958], action=1, reward=1.0, next_state=[0.01099192 0.37877944 0.14638193 0.07064885]\n",
      "[ episode 241 ][ timestamp 29 ] state=[0.01099192 0.37877944 0.14638193 0.07064885], action=1, reward=1.0, next_state=[ 0.0185675   0.57153257  0.1477949  -0.17250593]\n",
      "[ episode 241 ][ timestamp 30 ] state=[ 0.0185675   0.57153257  0.1477949  -0.17250593], action=1, reward=1.0, next_state=[ 0.02999816  0.76426413  0.14434479 -0.41515789]\n",
      "[ episode 241 ][ timestamp 31 ] state=[ 0.02999816  0.76426413  0.14434479 -0.41515789], action=1, reward=1.0, next_state=[ 0.04528344  0.95707687  0.13604163 -0.65907908]\n",
      "[ episode 241 ][ timestamp 32 ] state=[ 0.04528344  0.95707687  0.13604163 -0.65907908], action=0, reward=1.0, next_state=[ 0.06442498  0.76035011  0.12286005 -0.32684241]\n",
      "[ episode 241 ][ timestamp 33 ] state=[ 0.06442498  0.76035011  0.12286005 -0.32684241], action=0, reward=1.0, next_state=[0.07963198 0.56371271 0.1163232  0.00192042]\n",
      "[ episode 241 ][ timestamp 34 ] state=[0.07963198 0.56371271 0.1163232  0.00192042], action=1, reward=1.0, next_state=[ 0.09090623  0.75699092  0.11636161 -0.25191572]\n",
      "[ episode 241 ][ timestamp 35 ] state=[ 0.09090623  0.75699092  0.11636161 -0.25191572], action=0, reward=1.0, next_state=[0.10604605 0.56041626 0.11132329 0.07508547]\n",
      "[ episode 241 ][ timestamp 36 ] state=[0.10604605 0.56041626 0.11132329 0.07508547], action=1, reward=1.0, next_state=[ 0.11725438  0.75378082  0.112825   -0.18050449]\n",
      "[ episode 241 ][ timestamp 37 ] state=[ 0.11725438  0.75378082  0.112825   -0.18050449], action=0, reward=1.0, next_state=[0.13232999 0.55724036 0.10921491 0.14553203]\n",
      "[ episode 241 ][ timestamp 38 ] state=[0.13232999 0.55724036 0.10921491 0.14553203], action=1, reward=1.0, next_state=[ 0.1434748   0.75064256  0.11212555 -0.11079744]\n",
      "[ episode 241 ][ timestamp 39 ] state=[ 0.1434748   0.75064256  0.11212555 -0.11079744], action=1, reward=1.0, next_state=[ 0.15848765  0.94399413  0.1099096  -0.36610769]\n",
      "[ episode 241 ][ timestamp 40 ] state=[ 0.15848765  0.94399413  0.1099096  -0.36610769], action=1, reward=1.0, next_state=[ 0.17736753  1.13739656  0.10258745 -0.62221245]\n",
      "[ episode 241 ][ timestamp 41 ] state=[ 0.17736753  1.13739656  0.10258745 -0.62221245], action=0, reward=1.0, next_state=[ 0.20011546  0.94100304  0.0901432  -0.29906314]\n",
      "[ episode 241 ][ timestamp 42 ] state=[ 0.20011546  0.94100304  0.0901432  -0.29906314], action=0, reward=1.0, next_state=[0.21893553 0.74471954 0.08416194 0.02063293]\n",
      "[ episode 241 ][ timestamp 43 ] state=[0.21893553 0.74471954 0.08416194 0.02063293], action=0, reward=1.0, next_state=[0.23382992 0.54849777 0.0845746  0.33863819]\n",
      "[ episode 241 ][ timestamp 44 ] state=[0.23382992 0.54849777 0.0845746  0.33863819], action=1, reward=1.0, next_state=[0.24479987 0.74232082 0.09134736 0.07377809]\n",
      "[ episode 241 ][ timestamp 45 ] state=[0.24479987 0.74232082 0.09134736 0.07377809], action=0, reward=1.0, next_state=[0.25964629 0.5460161  0.09282292 0.39382629]\n",
      "[ episode 241 ][ timestamp 46 ] state=[0.25964629 0.5460161  0.09282292 0.39382629], action=1, reward=1.0, next_state=[0.27056661 0.73970673 0.10069945 0.13179185]\n",
      "[ episode 241 ][ timestamp 47 ] state=[0.27056661 0.73970673 0.10069945 0.13179185], action=0, reward=1.0, next_state=[0.28536074 0.54329729 0.10333528 0.45446916]\n",
      "[ episode 241 ][ timestamp 48 ] state=[0.28536074 0.54329729 0.10333528 0.45446916], action=1, reward=1.0, next_state=[0.29622669 0.73681791 0.11242467 0.19606322]\n",
      "[ episode 241 ][ timestamp 49 ] state=[0.29622669 0.73681791 0.11242467 0.19606322], action=1, reward=1.0, next_state=[ 0.31096305  0.93016722  0.11634593 -0.05914656]\n",
      "[ episode 241 ][ timestamp 50 ] state=[ 0.31096305  0.93016722  0.11634593 -0.05914656], action=0, reward=1.0, next_state=[0.32956639 0.73358594 0.115163   0.26786045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 241 ][ timestamp 51 ] state=[0.32956639 0.73358594 0.115163   0.26786045], action=1, reward=1.0, next_state=[0.34423811 0.92689213 0.12052021 0.01360496]\n",
      "[ episode 241 ][ timestamp 52 ] state=[0.34423811 0.92689213 0.12052021 0.01360496], action=0, reward=1.0, next_state=[0.36277595 0.73026642 0.12079231 0.34175133]\n",
      "[ episode 241 ][ timestamp 53 ] state=[0.36277595 0.73026642 0.12079231 0.34175133], action=1, reward=1.0, next_state=[0.37738128 0.92348129 0.12762734 0.08946747]\n",
      "[ episode 241 ][ timestamp 54 ] state=[0.37738128 0.92348129 0.12762734 0.08946747], action=1, reward=1.0, next_state=[ 0.39585091  1.1165648   0.12941669 -0.16038153]\n",
      "[ episode 241 ][ timestamp 55 ] state=[ 0.39585091  1.1165648   0.12941669 -0.16038153], action=0, reward=1.0, next_state=[0.4181822  0.91985045 0.12620905 0.1701648 ]\n",
      "[ episode 241 ][ timestamp 56 ] state=[0.4181822  0.91985045 0.12620905 0.1701648 ], action=1, reward=1.0, next_state=[ 0.43657921  1.11296122  0.12961235 -0.08019037]\n",
      "[ episode 241 ][ timestamp 57 ] state=[ 0.43657921  1.11296122  0.12961235 -0.08019037], action=0, reward=1.0, next_state=[0.45883844 0.91624245 0.12800854 0.25041211]\n",
      "[ episode 241 ][ timestamp 58 ] state=[0.45883844 0.91624245 0.12800854 0.25041211], action=1, reward=1.0, next_state=[4.77163287e-01 1.10932613e+00 1.33016786e-01 6.88102929e-04]\n",
      "[ episode 241 ][ timestamp 59 ] state=[4.77163287e-01 1.10932613e+00 1.33016786e-01 6.88102929e-04], action=0, reward=1.0, next_state=[0.49934981 0.91257217 0.13303055 0.33220365]\n",
      "[ episode 241 ][ timestamp 60 ] state=[0.49934981 0.91257217 0.13303055 0.33220365], action=1, reward=1.0, next_state=[0.51760125 1.10557453 0.13967462 0.08425376]\n",
      "[ episode 241 ][ timestamp 61 ] state=[0.51760125 1.10557453 0.13967462 0.08425376], action=1, reward=1.0, next_state=[ 0.53971274  1.29844672  0.1413597  -0.16130611]\n",
      "[ episode 241 ][ timestamp 62 ] state=[ 0.53971274  1.29844672  0.1413597  -0.16130611], action=0, reward=1.0, next_state=[0.56568168 1.10161383 0.13813357 0.1724197 ]\n",
      "[ episode 241 ][ timestamp 63 ] state=[0.56568168 1.10161383 0.13813357 0.1724197 ], action=1, reward=1.0, next_state=[ 0.58771395  1.29451622  0.14158197 -0.07369548]\n",
      "[ episode 241 ][ timestamp 64 ] state=[ 0.58771395  1.29451622  0.14158197 -0.07369548], action=0, reward=1.0, next_state=[0.61360428 1.09767834 0.14010806 0.26009318]\n",
      "[ episode 241 ][ timestamp 65 ] state=[0.61360428 1.09767834 0.14010806 0.26009318], action=1, reward=1.0, next_state=[0.63555785 1.29055112 0.14530992 0.01467611]\n",
      "[ episode 241 ][ timestamp 66 ] state=[0.63555785 1.29055112 0.14530992 0.01467611], action=0, reward=1.0, next_state=[0.66136887 1.09367649 0.14560344 0.34944672]\n",
      "[ episode 241 ][ timestamp 67 ] state=[0.66136887 1.09367649 0.14560344 0.34944672], action=1, reward=1.0, next_state=[0.6832424  1.28646    0.15259238 0.10598767]\n",
      "[ episode 241 ][ timestamp 68 ] state=[0.6832424  1.28646    0.15259238 0.10598767], action=1, reward=1.0, next_state=[ 0.7089716   1.47910336  0.15471213 -0.13493143]\n",
      "[ episode 241 ][ timestamp 69 ] state=[ 0.7089716   1.47910336  0.15471213 -0.13493143], action=0, reward=1.0, next_state=[0.73855367 1.2821428  0.1520135  0.20228477]\n",
      "[ episode 241 ][ timestamp 70 ] state=[0.73855367 1.2821428  0.1520135  0.20228477], action=1, reward=1.0, next_state=[ 0.76419652  1.47480088  0.1560592  -0.03884975]\n",
      "[ episode 241 ][ timestamp 71 ] state=[ 0.76419652  1.47480088  0.1560592  -0.03884975], action=0, reward=1.0, next_state=[0.79369254 1.27782537 0.1552822  0.29871826]\n",
      "[ episode 241 ][ timestamp 72 ] state=[0.79369254 1.27782537 0.1552822  0.29871826], action=1, reward=1.0, next_state=[0.81924905 1.47043234 0.16125657 0.05875373]\n",
      "[ episode 241 ][ timestamp 73 ] state=[0.81924905 1.47043234 0.16125657 0.05875373], action=1, reward=1.0, next_state=[ 0.84865769  1.66291898  0.16243164 -0.17902611]\n",
      "[ episode 241 ][ timestamp 74 ] state=[ 0.84865769  1.66291898  0.16243164 -0.17902611], action=0, reward=1.0, next_state=[0.88191607 1.46589037 0.15885112 0.16017174]\n",
      "[ episode 241 ][ timestamp 75 ] state=[0.88191607 1.46589037 0.15885112 0.16017174], action=1, reward=1.0, next_state=[ 0.91123388  1.65842375  0.16205456 -0.07848617]\n",
      "[ episode 241 ][ timestamp 76 ] state=[ 0.91123388  1.65842375  0.16205456 -0.07848617], action=1, reward=1.0, next_state=[ 0.94440236  1.85089642  0.16048483 -0.3159767 ]\n",
      "[ episode 241 ][ timestamp 77 ] state=[ 0.94440236  1.85089642  0.16048483 -0.3159767 ], action=1, reward=1.0, next_state=[ 0.98142028  2.04341197  0.1541653  -0.55405899]\n",
      "[ episode 241 ][ timestamp 78 ] state=[ 0.98142028  2.04341197  0.1541653  -0.55405899], action=1, reward=1.0, next_state=[ 1.02228852  2.23607142  0.14308412 -0.7944755 ]\n",
      "[ episode 241 ][ timestamp 79 ] state=[ 1.02228852  2.23607142  0.14308412 -0.7944755 ], action=0, reward=1.0, next_state=[ 1.06700995  2.03930589  0.12719461 -0.46042001]\n",
      "[ episode 241 ][ timestamp 80 ] state=[ 1.06700995  2.03930589  0.12719461 -0.46042001], action=0, reward=1.0, next_state=[ 1.10779607  1.84263719  0.11798621 -0.13050563]\n",
      "[ episode 241 ][ timestamp 81 ] state=[ 1.10779607  1.84263719  0.11798621 -0.13050563], action=0, reward=1.0, next_state=[1.14464881 1.64604009 0.1153761  0.19694735]\n",
      "[ episode 241 ][ timestamp 82 ] state=[1.14464881 1.64604009 0.1153761  0.19694735], action=0, reward=1.0, next_state=[1.17756962 1.44947309 0.11931504 0.52368292]\n",
      "[ episode 241 ][ timestamp 83 ] state=[1.17756962 1.44947309 0.11931504 0.52368292], action=1, reward=1.0, next_state=[1.20655908 1.64273157 0.1297887  0.27085163]\n",
      "[ episode 241 ][ timestamp 84 ] state=[1.20655908 1.64273157 0.1297887  0.27085163], action=0, reward=1.0, next_state=[1.23941371 1.44601946 0.13520573 0.60148889]\n",
      "[ episode 241 ][ timestamp 85 ] state=[1.23941371 1.44601946 0.13520573 0.60148889], action=1, reward=1.0, next_state=[1.2683341  1.6390169  0.14723551 0.35426427]\n",
      "[ episode 241 ][ timestamp 86 ] state=[1.2683341  1.6390169  0.14723551 0.35426427], action=0, reward=1.0, next_state=[1.30111444 1.44214176 0.1543208  0.68951283]\n",
      "[ episode 241 ][ timestamp 87 ] state=[1.30111444 1.44214176 0.1543208  0.68951283], action=1, reward=1.0, next_state=[1.32995727 1.63482369 0.16811105 0.44911509]\n",
      "[ episode 241 ][ timestamp 88 ] state=[1.32995727 1.63482369 0.16811105 0.44911509], action=1, reward=1.0, next_state=[1.36265374 1.82721866 0.17709336 0.21378322]\n",
      "[ episode 241 ][ timestamp 89 ] state=[1.36265374 1.82721866 0.17709336 0.21378322], action=1, reward=1.0, next_state=[ 1.39919812  2.01942497  0.18136902 -0.01822334]\n",
      "[ episode 241 ][ timestamp 90 ] state=[ 1.39919812  2.01942497  0.18136902 -0.01822334], action=0, reward=1.0, next_state=[1.43958662 1.82222811 0.18100455 0.32575086]\n",
      "[ episode 241 ][ timestamp 91 ] state=[1.43958662 1.82222811 0.18100455 0.32575086], action=1, reward=1.0, next_state=[1.47603118 2.01437354 0.18751957 0.09516647]\n",
      "[ episode 241 ][ timestamp 92 ] state=[1.47603118 2.01437354 0.18751957 0.09516647], action=1, reward=1.0, next_state=[ 1.51631865  2.20638217  0.1894229  -0.13298929]\n",
      "[ episode 241 ][ timestamp 93 ] state=[ 1.51631865  2.20638217  0.1894229  -0.13298929], action=0, reward=1.0, next_state=[1.56044629 2.00912311 0.18676311 0.21296464]\n",
      "[ episode 241 ][ timestamp 94 ] state=[1.56044629 2.00912311 0.18676311 0.21296464], action=1, reward=1.0, next_state=[ 1.60062876  2.20115224  0.19102241 -0.01548038]\n",
      "[ episode 241 ][ timestamp 95 ] state=[ 1.60062876  2.20115224  0.19102241 -0.01548038], action=0, reward=1.0, next_state=[1.6446518  2.0038773  0.1907128  0.33086924]\n",
      "[ episode 241 ][ timestamp 96 ] state=[1.6446518  2.0038773  0.1907128  0.33086924], action=1, reward=1.0, next_state=[1.68472935 2.19584551 0.19733018 0.10386798]\n",
      "[ episode 241 ][ timestamp 97 ] state=[1.68472935 2.19584551 0.19733018 0.10386798], action=0, reward=1.0, next_state=[1.72864626 1.99852346 0.19940754 0.45174634]\n",
      "[ episode 241 ][ timestamp 98 ] state=[1.72864626 1.99852346 0.19940754 0.45174634], action=1, reward=1.0, next_state=[1.76861673 2.1903495  0.20844247 0.22794715]\n",
      "[ episode 241 ][ timestamp 99 ] state=[1.76861673 2.1903495  0.20844247 0.22794715], action=1, reward=-1.0, next_state=[1.81242372 2.38197797 0.21300141 0.00756559]\n",
      "[ Ended! ] Episode 241: Exploration_rate=0.30028896908517405. Score=99.\n",
      "[ Experience replay ] starts\n",
      "[ episode 242 ] state=[-0.01610625  0.00269935 -0.01082219 -0.02564152]\n",
      "[ episode 242 ][ timestamp 1 ] state=[-0.01610625  0.00269935 -0.01082219 -0.02564152], action=1, reward=1.0, next_state=[-0.01605226  0.19797481 -0.01133502 -0.32171923]\n",
      "[ episode 242 ][ timestamp 2 ] state=[-0.01605226  0.19797481 -0.01133502 -0.32171923], action=0, reward=1.0, next_state=[-0.01209277  0.0030161  -0.01776941 -0.03263237]\n",
      "[ episode 242 ][ timestamp 3 ] state=[-0.01209277  0.0030161  -0.01776941 -0.03263237], action=1, reward=1.0, next_state=[-0.01203245  0.1983883  -0.01842205 -0.33086834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 242 ][ timestamp 4 ] state=[-0.01203245  0.1983883  -0.01842205 -0.33086834], action=0, reward=1.0, next_state=[-0.00806468  0.00353336 -0.02503942 -0.0440513 ]\n",
      "[ episode 242 ][ timestamp 5 ] state=[-0.00806468  0.00353336 -0.02503942 -0.0440513 ], action=1, reward=1.0, next_state=[-0.00799401  0.19900525 -0.02592045 -0.34452804]\n",
      "[ episode 242 ][ timestamp 6 ] state=[-0.00799401  0.19900525 -0.02592045 -0.34452804], action=0, reward=1.0, next_state=[-0.00401391  0.00426144 -0.03281101 -0.06013021]\n",
      "[ episode 242 ][ timestamp 7 ] state=[-0.00401391  0.00426144 -0.03281101 -0.06013021], action=1, reward=1.0, next_state=[-0.00392868  0.1998381  -0.03401361 -0.362982  ]\n",
      "[ episode 242 ][ timestamp 8 ] state=[-0.00392868  0.1998381  -0.03401361 -0.362982  ], action=0, reward=1.0, next_state=[ 6.80835150e-05  5.21567260e-03 -4.12732508e-02 -8.12152939e-02]\n",
      "[ episode 242 ][ timestamp 9 ] state=[ 6.80835150e-05  5.21567260e-03 -4.12732508e-02 -8.12152939e-02], action=1, reward=1.0, next_state=[ 1.72396967e-04  2.00904227e-01 -4.28975567e-02 -3.86629039e-01]\n",
      "[ episode 242 ][ timestamp 10 ] state=[ 1.72396967e-04  2.00904227e-01 -4.28975567e-02 -3.86629039e-01], action=0, reward=1.0, next_state=[ 0.00419048  0.00641665 -0.05063014 -0.10777407]\n",
      "[ episode 242 ][ timestamp 11 ] state=[ 0.00419048  0.00641665 -0.05063014 -0.10777407], action=1, reward=1.0, next_state=[ 0.00431881  0.2022262  -0.05278562 -0.41599093]\n",
      "[ episode 242 ][ timestamp 12 ] state=[ 0.00431881  0.2022262  -0.05278562 -0.41599093], action=0, reward=1.0, next_state=[ 0.00836334  0.00789055 -0.06110544 -0.14040522]\n",
      "[ episode 242 ][ timestamp 13 ] state=[ 0.00836334  0.00789055 -0.06110544 -0.14040522], action=0, reward=1.0, next_state=[ 0.00852115 -0.18630543 -0.06391354  0.13239126]\n",
      "[ episode 242 ][ timestamp 14 ] state=[ 0.00852115 -0.18630543 -0.06391354  0.13239126], action=0, reward=1.0, next_state=[ 0.00479504 -0.3804564  -0.06126572  0.40424531]\n",
      "[ episode 242 ][ timestamp 15 ] state=[ 0.00479504 -0.3804564  -0.06126572  0.40424531], action=1, reward=1.0, next_state=[-0.00281409 -0.18452151 -0.05318081  0.09289353]\n",
      "[ episode 242 ][ timestamp 16 ] state=[-0.00281409 -0.18452151 -0.05318081  0.09289353], action=1, reward=1.0, next_state=[-0.00650452  0.01132076 -0.05132294 -0.21608235]\n",
      "[ episode 242 ][ timestamp 17 ] state=[-0.00650452  0.01132076 -0.05132294 -0.21608235], action=1, reward=1.0, next_state=[-0.0062781   0.20713742 -0.05564459 -0.52450291]\n",
      "[ episode 242 ][ timestamp 18 ] state=[-0.0062781   0.20713742 -0.05564459 -0.52450291], action=0, reward=1.0, next_state=[-0.00213535  0.01284089 -0.06613465 -0.24986027]\n",
      "[ episode 242 ][ timestamp 19 ] state=[-0.00213535  0.01284089 -0.06613465 -0.24986027], action=1, reward=1.0, next_state=[-0.00187854  0.20884189 -0.07113185 -0.56264846]\n",
      "[ episode 242 ][ timestamp 20 ] state=[-0.00187854  0.20884189 -0.07113185 -0.56264846], action=0, reward=1.0, next_state=[ 0.0022983   0.01478641 -0.08238482 -0.29319647]\n",
      "[ episode 242 ][ timestamp 21 ] state=[ 0.0022983   0.01478641 -0.08238482 -0.29319647], action=0, reward=1.0, next_state=[ 0.00259403 -0.17907021 -0.08824875 -0.02759155]\n",
      "[ episode 242 ][ timestamp 22 ] state=[ 0.00259403 -0.17907021 -0.08824875 -0.02759155], action=1, reward=1.0, next_state=[-0.00098737  0.01719918 -0.08880058 -0.34676146]\n",
      "[ episode 242 ][ timestamp 23 ] state=[-0.00098737  0.01719918 -0.08880058 -0.34676146], action=0, reward=1.0, next_state=[-0.00064339 -0.17655483 -0.09573581 -0.08334866]\n",
      "[ episode 242 ][ timestamp 24 ] state=[-0.00064339 -0.17655483 -0.09573581 -0.08334866], action=1, reward=1.0, next_state=[-0.00417449  0.01979981 -0.09740278 -0.40463526]\n",
      "[ episode 242 ][ timestamp 25 ] state=[-0.00417449  0.01979981 -0.09740278 -0.40463526], action=0, reward=1.0, next_state=[-0.00377849 -0.17381561 -0.10549549 -0.14417986]\n",
      "[ episode 242 ][ timestamp 26 ] state=[-0.00377849 -0.17381561 -0.10549549 -0.14417986], action=1, reward=1.0, next_state=[-0.0072548   0.02264657 -0.10837909 -0.46819296]\n",
      "[ episode 242 ][ timestamp 27 ] state=[-0.0072548   0.02264657 -0.10837909 -0.46819296], action=0, reward=1.0, next_state=[-0.00680187 -0.17079074 -0.11774294 -0.21154053]\n",
      "[ episode 242 ][ timestamp 28 ] state=[-0.00680187 -0.17079074 -0.11774294 -0.21154053], action=0, reward=1.0, next_state=[-0.01021769 -0.36404964 -0.12197375  0.04180422]\n",
      "[ episode 242 ][ timestamp 29 ] state=[-0.01021769 -0.36404964 -0.12197375  0.04180422], action=1, reward=1.0, next_state=[-0.01749868 -0.16740892 -0.12113767 -0.28673685]\n",
      "[ episode 242 ][ timestamp 30 ] state=[-0.01749868 -0.16740892 -0.12113767 -0.28673685], action=1, reward=1.0, next_state=[-0.02084686  0.0292136  -0.12687241 -0.61503673]\n",
      "[ episode 242 ][ timestamp 31 ] state=[-0.02084686  0.0292136  -0.12687241 -0.61503673], action=0, reward=1.0, next_state=[-0.02026259 -0.16392871 -0.13917314 -0.36485234]\n",
      "[ episode 242 ][ timestamp 32 ] state=[-0.02026259 -0.16392871 -0.13917314 -0.36485234], action=1, reward=1.0, next_state=[-0.02354116  0.03286838 -0.14647019 -0.69797869]\n",
      "[ episode 242 ][ timestamp 33 ] state=[-0.02354116  0.03286838 -0.14647019 -0.69797869], action=0, reward=1.0, next_state=[-0.02288379 -0.15995173 -0.16042976 -0.45475391]\n",
      "[ episode 242 ][ timestamp 34 ] state=[-0.02288379 -0.15995173 -0.16042976 -0.45475391], action=1, reward=1.0, next_state=[-0.02608283  0.03703172 -0.16952484 -0.79339909]\n",
      "[ episode 242 ][ timestamp 35 ] state=[-0.02608283  0.03703172 -0.16952484 -0.79339909], action=0, reward=1.0, next_state=[-0.02534219 -0.15540812 -0.18539282 -0.55847918]\n",
      "[ episode 242 ][ timestamp 36 ] state=[-0.02534219 -0.15540812 -0.18539282 -0.55847918], action=1, reward=1.0, next_state=[-0.02845036  0.04176582 -0.19656241 -0.9033657 ]\n",
      "[ episode 242 ][ timestamp 37 ] state=[-0.02845036  0.04176582 -0.19656241 -0.9033657 ], action=0, reward=-1.0, next_state=[-0.02761504 -0.15022885 -0.21462972 -0.67833728]\n",
      "[ Ended! ] Episode 242: Exploration_rate=0.2987875242397482. Score=37.\n",
      "[ Experience replay ] starts\n",
      "[ episode 243 ] state=[-0.01493549  0.04803839  0.04733765  0.04621163]\n",
      "[ episode 243 ][ timestamp 1 ] state=[-0.01493549  0.04803839  0.04733765  0.04621163], action=0, reward=1.0, next_state=[-0.01397472 -0.14772927  0.04826188  0.35344623]\n",
      "[ episode 243 ][ timestamp 2 ] state=[-0.01397472 -0.14772927  0.04826188  0.35344623], action=0, reward=1.0, next_state=[-0.0169293  -0.34350307  0.05533081  0.66094849]\n",
      "[ episode 243 ][ timestamp 3 ] state=[-0.0169293  -0.34350307  0.05533081  0.66094849], action=1, reward=1.0, next_state=[-0.02379936 -0.14919294  0.06854978  0.38618829]\n",
      "[ episode 243 ][ timestamp 4 ] state=[-0.02379936 -0.14919294  0.06854978  0.38618829], action=0, reward=1.0, next_state=[-0.02678322 -0.34521765  0.07627354  0.69967264]\n",
      "[ episode 243 ][ timestamp 5 ] state=[-0.02678322 -0.34521765  0.07627354  0.69967264], action=1, reward=1.0, next_state=[-0.03368758 -0.15123136  0.09026699  0.43194189]\n",
      "[ episode 243 ][ timestamp 6 ] state=[-0.03368758 -0.15123136  0.09026699  0.43194189], action=0, reward=1.0, next_state=[-0.0367122  -0.34750774  0.09890583  0.75166028]\n",
      "[ episode 243 ][ timestamp 7 ] state=[-0.0367122  -0.34750774  0.09890583  0.75166028], action=1, reward=1.0, next_state=[-0.04366236 -0.15387873  0.11393904  0.49166714]\n",
      "[ episode 243 ][ timestamp 8 ] state=[-0.04366236 -0.15387873  0.11393904  0.49166714], action=0, reward=1.0, next_state=[-0.04673993 -0.35040794  0.12377238  0.81797514]\n",
      "[ episode 243 ][ timestamp 9 ] state=[-0.04673993 -0.35040794  0.12377238  0.81797514], action=1, reward=1.0, next_state=[-0.05374809 -0.1571779   0.14013188  0.56664365]\n",
      "[ episode 243 ][ timestamp 10 ] state=[-0.05374809 -0.1571779   0.14013188  0.56664365], action=1, reward=1.0, next_state=[-0.05689165  0.03572895  0.15146476  0.32118388]\n",
      "[ episode 243 ][ timestamp 11 ] state=[-0.05689165  0.03572895  0.15146476  0.32118388], action=0, reward=1.0, next_state=[-0.05617707 -0.16118894  0.15788843  0.65753954]\n",
      "[ episode 243 ][ timestamp 12 ] state=[-0.05617707 -0.16118894  0.15788843  0.65753954], action=1, reward=1.0, next_state=[-0.05940085  0.03142401  0.17103922  0.41844043]\n",
      "[ episode 243 ][ timestamp 13 ] state=[-0.05940085  0.03142401  0.17103922  0.41844043], action=0, reward=1.0, next_state=[-0.05877237 -0.1656565   0.17940803  0.75978832]\n",
      "[ episode 243 ][ timestamp 14 ] state=[-0.05877237 -0.1656565   0.17940803  0.75978832], action=1, reward=1.0, next_state=[-0.0620855   0.02659988  0.1946038   0.52849593]\n",
      "[ episode 243 ][ timestamp 15 ] state=[-0.0620855   0.02659988  0.1946038   0.52849593], action=1, reward=1.0, next_state=[-0.0615535   0.2185284   0.20517372  0.3028904 ]\n",
      "[ episode 243 ][ timestamp 16 ] state=[-0.0615535   0.2185284   0.20517372  0.3028904 ], action=0, reward=-1.0, next_state=[-0.05718293  0.02116352  0.21123153  0.65262705]\n",
      "[ Ended! ] Episode 243: Exploration_rate=0.29729358661854943. Score=16.\n",
      "[ Experience replay ] starts\n",
      "[ episode 244 ] state=[-0.04246039 -0.02197169  0.04029732 -0.02889988]\n",
      "[ episode 244 ][ timestamp 1 ] state=[-0.04246039 -0.02197169  0.04029732 -0.02889988], action=1, reward=1.0, next_state=[-0.04289983  0.1725499   0.03971932 -0.30860118]\n",
      "[ episode 244 ][ timestamp 2 ] state=[-0.04289983  0.1725499   0.03971932 -0.30860118], action=0, reward=1.0, next_state=[-0.03944883 -0.02311482  0.0335473  -0.00366117]\n",
      "[ episode 244 ][ timestamp 3 ] state=[-0.03944883 -0.02311482  0.0335473  -0.00366117], action=1, reward=1.0, next_state=[-0.03991112  0.17151035  0.03347408 -0.28557361]\n",
      "[ episode 244 ][ timestamp 4 ] state=[-0.03991112  0.17151035  0.03347408 -0.28557361], action=0, reward=1.0, next_state=[-0.03648092 -0.02407262  0.0277626   0.01747603]\n",
      "[ episode 244 ][ timestamp 5 ] state=[-0.03648092 -0.02407262  0.0277626   0.01747603], action=1, reward=1.0, next_state=[-0.03696237  0.17064041  0.02811213 -0.26631981]\n",
      "[ episode 244 ][ timestamp 6 ] state=[-0.03696237  0.17064041  0.02811213 -0.26631981], action=0, reward=1.0, next_state=[-0.03354956 -0.02487125  0.02278573  0.03509568]\n",
      "[ episode 244 ][ timestamp 7 ] state=[-0.03354956 -0.02487125  0.02278573  0.03509568], action=1, reward=1.0, next_state=[-0.03404699  0.16991666  0.02348764 -0.25031191]\n",
      "[ episode 244 ][ timestamp 8 ] state=[-0.03404699  0.16991666  0.02348764 -0.25031191], action=0, reward=1.0, next_state=[-0.03064865 -0.02553269  0.0184814   0.04968598]\n",
      "[ episode 244 ][ timestamp 9 ] state=[-0.03064865 -0.02553269  0.0184814   0.04968598], action=1, reward=1.0, next_state=[-0.03115931  0.16931945  0.01947512 -0.23710909]\n",
      "[ episode 244 ][ timestamp 10 ] state=[-0.03115931  0.16931945  0.01947512 -0.23710909], action=0, reward=1.0, next_state=[-0.02777292 -0.02607524  0.01473294  0.0616527 ]\n",
      "[ episode 244 ][ timestamp 11 ] state=[-0.02777292 -0.02607524  0.01473294  0.0616527 ], action=1, reward=1.0, next_state=[-0.02829442  0.16883241  0.015966   -0.22634572]\n",
      "[ episode 244 ][ timestamp 12 ] state=[-0.02829442  0.16883241  0.015966   -0.22634572], action=0, reward=1.0, next_state=[-0.02491777 -0.02651404  0.01143908  0.07133041]\n",
      "[ episode 244 ][ timestamp 13 ] state=[-0.02491777 -0.02651404  0.01143908  0.07133041], action=1, reward=1.0, next_state=[-0.02544806  0.16844206  0.01286569 -0.2177216 ]\n",
      "[ episode 244 ][ timestamp 14 ] state=[-0.02544806  0.16844206  0.01286569 -0.2177216 ], action=0, reward=1.0, next_state=[-0.02207921 -0.02686142  0.00851126  0.07899179]\n",
      "[ episode 244 ][ timestamp 15 ] state=[-0.02207921 -0.02686142  0.00851126  0.07899179], action=1, reward=1.0, next_state=[-0.02261644  0.16813749  0.01009109 -0.2109937 ]\n",
      "[ episode 244 ][ timestamp 16 ] state=[-0.02261644  0.16813749  0.01009109 -0.2109937 ], action=0, reward=1.0, next_state=[-0.01925369 -0.02712728  0.00587122  0.08485527]\n",
      "[ episode 244 ][ timestamp 17 ] state=[-0.01925369 -0.02712728  0.00587122  0.08485527], action=1, reward=1.0, next_state=[-0.01979624  0.16791002  0.00756833 -0.20596951]\n",
      "[ episode 244 ][ timestamp 18 ] state=[-0.01979624  0.16791002  0.00756833 -0.20596951], action=0, reward=1.0, next_state=[-0.01643804 -0.02731933  0.00344894  0.08909121]\n",
      "[ episode 244 ][ timestamp 19 ] state=[-0.01643804 -0.02731933  0.00344894  0.08909121], action=1, reward=1.0, next_state=[-0.01698442  0.16775301  0.00523076 -0.20250159]\n",
      "[ episode 244 ][ timestamp 20 ] state=[-0.01698442  0.16775301  0.00523076 -0.20250159], action=0, reward=1.0, next_state=[-0.01362936 -0.02744336  0.00118073  0.09182679]\n",
      "[ episode 244 ][ timestamp 21 ] state=[-0.01362936 -0.02744336  0.00118073  0.09182679], action=1, reward=1.0, next_state=[-0.01417823  0.16766165  0.00301726 -0.20048338]\n",
      "[ episode 244 ][ timestamp 22 ] state=[-0.01417823  0.16766165  0.00301726 -0.20048338], action=0, reward=1.0, next_state=[-0.010825   -0.02750332 -0.0009924   0.09314982]\n",
      "[ episode 244 ][ timestamp 23 ] state=[-0.010825   -0.02750332 -0.0009924   0.09314982], action=1, reward=1.0, next_state=[-0.01137506  0.16763284  0.00087059 -0.19984604]\n",
      "[ episode 244 ][ timestamp 24 ] state=[-0.01137506  0.16763284  0.00087059 -0.19984604], action=0, reward=1.0, next_state=[-0.00802241 -0.02750155 -0.00312633  0.09311139]\n",
      "[ episode 244 ][ timestamp 25 ] state=[-0.00802241 -0.02750155 -0.00312633  0.09311139], action=1, reward=1.0, next_state=[-0.00857244  0.16766507 -0.0012641  -0.20055625]\n",
      "[ episode 244 ][ timestamp 26 ] state=[-0.00857244  0.16766507 -0.0012641  -0.20055625], action=0, reward=1.0, next_state=[-0.00521914 -0.02743878 -0.00527523  0.09172764]\n",
      "[ episode 244 ][ timestamp 27 ] state=[-0.00521914 -0.02743878 -0.00527523  0.09172764], action=1, reward=1.0, next_state=[-0.00576791  0.16775838 -0.00344067 -0.20261494]\n",
      "[ episode 244 ][ timestamp 28 ] state=[-0.00576791  0.16775838 -0.00344067 -0.20261494], action=0, reward=1.0, next_state=[-0.00241275 -0.02731419 -0.00749297  0.08898064]\n",
      "[ episode 244 ][ timestamp 29 ] state=[-0.00241275 -0.02731419 -0.00749297  0.08898064], action=1, reward=1.0, next_state=[-0.00295903  0.16791435 -0.00571336 -0.20605687]\n",
      "[ episode 244 ][ timestamp 30 ] state=[-0.00295903  0.16791435 -0.00571336 -0.20605687], action=0, reward=1.0, next_state=[ 0.00039926 -0.02712543 -0.0098345   0.08481831]\n",
      "[ episode 244 ][ timestamp 31 ] state=[ 0.00039926 -0.02712543 -0.0098345   0.08481831], action=1, reward=1.0, next_state=[-1.43250929e-04  1.68136100e-01 -8.13812999e-03 -2.10951118e-01]\n",
      "[ episode 244 ][ timestamp 32 ] state=[-1.43250929e-04  1.68136100e-01 -8.13812999e-03 -2.10951118e-01], action=0, reward=1.0, next_state=[ 0.00321947 -0.02686855 -0.01235715  0.07915359]\n",
      "[ episode 244 ][ timestamp 33 ] state=[ 0.00321947 -0.02686855 -0.01235715  0.07915359], action=1, reward=1.0, next_state=[ 0.0026821   0.16842834 -0.01077408 -0.2174023 ]\n",
      "[ episode 244 ][ timestamp 34 ] state=[ 0.0026821   0.16842834 -0.01077408 -0.2174023 ], action=0, reward=1.0, next_state=[ 0.00605067 -0.02653794 -0.01512213  0.07186264]\n",
      "[ episode 244 ][ timestamp 35 ] state=[ 0.00605067 -0.02653794 -0.01512213  0.07186264], action=0, reward=1.0, next_state=[ 0.00551991 -0.22143987 -0.01368487  0.35973637]\n",
      "[ episode 244 ][ timestamp 36 ] state=[ 0.00551991 -0.22143987 -0.01368487  0.35973637], action=0, reward=1.0, next_state=[ 0.00109111 -0.41636464 -0.00649015  0.64807292]\n",
      "[ episode 244 ][ timestamp 37 ] state=[ 0.00109111 -0.41636464 -0.00649015  0.64807292], action=1, reward=1.0, next_state=[-0.00723618 -0.22115288  0.00647131  0.35335335]\n",
      "[ episode 244 ][ timestamp 38 ] state=[-0.00723618 -0.22115288  0.00647131  0.35335335], action=1, reward=1.0, next_state=[-0.01165924 -0.02612354  0.01353838  0.06271802]\n",
      "[ episode 244 ][ timestamp 39 ] state=[-0.01165924 -0.02612354  0.01353838  0.06271802], action=1, reward=1.0, next_state=[-0.01218171  0.16880171  0.01479274 -0.2256629 ]\n",
      "[ episode 244 ][ timestamp 40 ] state=[-0.01218171  0.16880171  0.01479274 -0.2256629 ], action=1, reward=1.0, next_state=[-0.00880568  0.36370916  0.01027948 -0.51364318]\n",
      "[ episode 244 ][ timestamp 41 ] state=[-0.00880568  0.36370916  0.01027948 -0.51364318], action=0, reward=1.0, next_state=[-1.53149304e-03  1.68443952e-01  6.61803636e-06 -2.17738724e-01]\n",
      "[ episode 244 ][ timestamp 42 ] state=[-1.53149304e-03  1.68443952e-01  6.61803636e-06 -2.17738724e-01], action=1, reward=1.0, next_state=[ 0.00183739  0.36356581 -0.00434816 -0.51041956]\n",
      "[ episode 244 ][ timestamp 43 ] state=[ 0.00183739  0.36356581 -0.00434816 -0.51041956], action=0, reward=1.0, next_state=[ 0.0091087   0.16850538 -0.01455655 -0.21911004]\n",
      "[ episode 244 ][ timestamp 44 ] state=[ 0.0091087   0.16850538 -0.01455655 -0.21911004], action=0, reward=1.0, next_state=[ 0.01247881 -0.0264055  -0.01893875  0.06894583]\n",
      "[ episode 244 ][ timestamp 45 ] state=[ 0.01247881 -0.0264055  -0.01893875  0.06894583], action=1, reward=1.0, next_state=[ 0.0119507   0.16898278 -0.01755983 -0.22965169]\n",
      "[ episode 244 ][ timestamp 46 ] state=[ 0.0119507   0.16898278 -0.01755983 -0.22965169], action=1, reward=1.0, next_state=[ 0.01533036  0.3643512  -0.02215287 -0.52782147]\n",
      "[ episode 244 ][ timestamp 47 ] state=[ 0.01533036  0.3643512  -0.02215287 -0.52782147], action=0, reward=1.0, next_state=[ 0.02261738  0.16954783 -0.0327093  -0.24220051]\n",
      "[ episode 244 ][ timestamp 48 ] state=[ 0.02261738  0.16954783 -0.0327093  -0.24220051], action=0, reward=1.0, next_state=[ 0.02600834 -0.025092   -0.03755331  0.03998824]\n",
      "[ episode 244 ][ timestamp 49 ] state=[ 0.02600834 -0.025092   -0.03755331  0.03998824], action=1, reward=1.0, next_state=[ 0.0255065   0.17054778 -0.03675354 -0.2643026 ]\n",
      "[ episode 244 ][ timestamp 50 ] state=[ 0.0255065   0.17054778 -0.03675354 -0.2643026 ], action=0, reward=1.0, next_state=[ 0.02891745 -0.02403083 -0.04203959  0.01656509]\n",
      "[ episode 244 ][ timestamp 51 ] state=[ 0.02891745 -0.02403083 -0.04203959  0.01656509], action=0, reward=1.0, next_state=[ 0.02843684 -0.21852546 -0.04170829  0.29569327]\n",
      "[ episode 244 ][ timestamp 52 ] state=[ 0.02843684 -0.21852546 -0.04170829  0.29569327], action=1, reward=1.0, next_state=[ 0.02406633 -0.02283449 -0.03579443 -0.00984658]\n",
      "[ episode 244 ][ timestamp 53 ] state=[ 0.02406633 -0.02283449 -0.03579443 -0.00984658], action=1, reward=1.0, next_state=[ 0.02360964  0.17278203 -0.03599136 -0.31360473]\n",
      "[ episode 244 ][ timestamp 54 ] state=[ 0.02360964  0.17278203 -0.03599136 -0.31360473], action=0, reward=1.0, next_state=[ 0.02706528 -0.02180921 -0.04226345 -0.03248607]\n",
      "[ episode 244 ][ timestamp 55 ] state=[ 0.02706528 -0.02180921 -0.04226345 -0.03248607], action=1, reward=1.0, next_state=[ 0.02662909  0.17389253 -0.04291317 -0.33819831]\n",
      "[ episode 244 ][ timestamp 56 ] state=[ 0.02662909  0.17389253 -0.04291317 -0.33819831], action=0, reward=1.0, next_state=[ 0.03010694 -0.02059333 -0.04967714 -0.05935068]\n",
      "[ episode 244 ][ timestamp 57 ] state=[ 0.03010694 -0.02059333 -0.04967714 -0.05935068], action=1, reward=1.0, next_state=[ 0.02969508  0.17520439 -0.05086415 -0.36728402]\n",
      "[ episode 244 ][ timestamp 58 ] state=[ 0.02969508  0.17520439 -0.05086415 -0.36728402], action=0, reward=1.0, next_state=[ 0.03319916 -0.01915928 -0.05820983 -0.09106318]\n",
      "[ episode 244 ][ timestamp 59 ] state=[ 0.03319916 -0.01915928 -0.05820983 -0.09106318], action=1, reward=1.0, next_state=[ 0.03281598  0.17674661 -0.0600311  -0.40152834]\n",
      "[ episode 244 ][ timestamp 60 ] state=[ 0.03281598  0.17674661 -0.0600311  -0.40152834], action=0, reward=1.0, next_state=[ 0.03635091 -0.01747476 -0.06806166 -0.12835962]\n",
      "[ episode 244 ][ timestamp 61 ] state=[ 0.03635091 -0.01747476 -0.06806166 -0.12835962], action=1, reward=1.0, next_state=[ 0.03600142  0.17855285 -0.07062886 -0.44171492]\n",
      "[ episode 244 ][ timestamp 62 ] state=[ 0.03600142  0.17855285 -0.07062886 -0.44171492], action=0, reward=1.0, next_state=[ 0.03957247 -0.01550226 -0.07946315 -0.17210559]\n",
      "[ episode 244 ][ timestamp 63 ] state=[ 0.03957247 -0.01550226 -0.07946315 -0.17210559], action=0, reward=1.0, next_state=[ 0.03926243 -0.20940228 -0.08290527  0.09448906]\n",
      "[ episode 244 ][ timestamp 64 ] state=[ 0.03926243 -0.20940228 -0.08290527  0.09448906], action=1, reward=1.0, next_state=[ 0.03507438 -0.01319596 -0.08101548 -0.2231558 ]\n",
      "[ episode 244 ][ timestamp 65 ] state=[ 0.03507438 -0.01319596 -0.08101548 -0.2231558 ], action=0, reward=1.0, next_state=[ 0.03481046 -0.20707213 -0.0854786   0.0429121 ]\n",
      "[ episode 244 ][ timestamp 66 ] state=[ 0.03481046 -0.20707213 -0.0854786   0.0429121 ], action=0, reward=1.0, next_state=[ 0.03066902 -0.40087095 -0.08462036  0.30744885]\n",
      "[ episode 244 ][ timestamp 67 ] state=[ 0.03066902 -0.40087095 -0.08462036  0.30744885], action=1, reward=1.0, next_state=[ 0.0226516  -0.20465157 -0.07847138 -0.01067578]\n",
      "[ episode 244 ][ timestamp 68 ] state=[ 0.0226516  -0.20465157 -0.07847138 -0.01067578], action=1, reward=1.0, next_state=[ 0.01855857 -0.00849704 -0.0786849  -0.32704904]\n",
      "[ episode 244 ][ timestamp 69 ] state=[ 0.01855857 -0.00849704 -0.0786849  -0.32704904], action=1, reward=1.0, next_state=[ 0.01838863  0.18765183 -0.08522588 -0.64347151]\n",
      "[ episode 244 ][ timestamp 70 ] state=[ 0.01838863  0.18765183 -0.08522588 -0.64347151], action=0, reward=1.0, next_state=[ 0.02214167 -0.00618538 -0.09809531 -0.37879708]\n",
      "[ episode 244 ][ timestamp 71 ] state=[ 0.02214167 -0.00618538 -0.09809531 -0.37879708], action=1, reward=1.0, next_state=[ 0.02201796  0.19018286 -0.10567125 -0.70072718]\n",
      "[ episode 244 ][ timestamp 72 ] state=[ 0.02201796  0.19018286 -0.10567125 -0.70072718], action=0, reward=1.0, next_state=[ 0.02582162 -0.00332787 -0.11968579 -0.44308975]\n",
      "[ episode 244 ][ timestamp 73 ] state=[ 0.02582162 -0.00332787 -0.11968579 -0.44308975], action=1, reward=1.0, next_state=[ 0.02575506  0.19326626 -0.12854759 -0.77097502]\n",
      "[ episode 244 ][ timestamp 74 ] state=[ 0.02575506  0.19326626 -0.12854759 -0.77097502], action=1, reward=1.0, next_state=[ 0.02962038  0.38990074 -0.14396709 -1.10118213]\n",
      "[ episode 244 ][ timestamp 75 ] state=[ 0.02962038  0.38990074 -0.14396709 -1.10118213], action=0, reward=1.0, next_state=[ 0.0374184   0.19693598 -0.16599073 -0.85690969]\n",
      "[ episode 244 ][ timestamp 76 ] state=[ 0.0374184   0.19693598 -0.16599073 -0.85690969], action=1, reward=1.0, next_state=[ 0.04135712  0.3938832  -0.18312893 -1.19684748]\n",
      "[ episode 244 ][ timestamp 77 ] state=[ 0.04135712  0.3938832  -0.18312893 -1.19684748], action=1, reward=1.0, next_state=[ 0.04923478  0.59084074 -0.20706588 -1.54088319]\n",
      "[ episode 244 ][ timestamp 78 ] state=[ 0.04923478  0.59084074 -0.20706588 -1.54088319], action=0, reward=-1.0, next_state=[ 0.0610516   0.39872219 -0.23788354 -1.31930459]\n",
      "[ Ended! ] Episode 244: Exploration_rate=0.29580711868545667. Score=78.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 245 ] state=[ 0.00109929 -0.03127259 -0.01830105 -0.02658056]\n",
      "[ episode 245 ][ timestamp 1 ] state=[ 0.00109929 -0.03127259 -0.01830105 -0.02658056], action=0, reward=1.0, next_state=[ 0.00047384 -0.22612737 -0.01883266  0.26027246]\n",
      "[ episode 245 ][ timestamp 2 ] state=[ 0.00047384 -0.22612737 -0.01883266  0.26027246], action=0, reward=1.0, next_state=[-0.00404871 -0.42097549 -0.01362721  0.54695633]\n",
      "[ episode 245 ][ timestamp 3 ] state=[-0.00404871 -0.42097549 -0.01362721  0.54695633], action=1, reward=1.0, next_state=[-0.01246822 -0.22566476 -0.00268808  0.25001116]\n",
      "[ episode 245 ][ timestamp 4 ] state=[-0.01246822 -0.22566476 -0.00268808  0.25001116], action=0, reward=1.0, next_state=[-0.01698151 -0.42074822  0.00231214  0.541845  ]\n",
      "[ episode 245 ][ timestamp 5 ] state=[-0.01698151 -0.42074822  0.00231214  0.541845  ], action=0, reward=1.0, next_state=[-0.02539647 -0.61590259  0.01314904  0.83525554]\n",
      "[ episode 245 ][ timestamp 6 ] state=[-0.02539647 -0.61590259  0.01314904  0.83525554], action=0, reward=1.0, next_state=[-0.03771453 -0.81120168  0.02985415  1.13204455]\n",
      "[ episode 245 ][ timestamp 7 ] state=[-0.03771453 -0.81120168  0.02985415  1.13204455], action=1, reward=1.0, next_state=[-0.05393856 -0.616483    0.05249504  0.8488725 ]\n",
      "[ episode 245 ][ timestamp 8 ] state=[-0.05393856 -0.616483    0.05249504  0.8488725 ], action=0, reward=1.0, next_state=[-0.06626822 -0.81228009  0.06947249  1.15759002]\n",
      "[ episode 245 ][ timestamp 9 ] state=[-0.06626822 -0.81228009  0.06947249  1.15759002], action=1, reward=1.0, next_state=[-0.08251382 -0.61812898  0.09262429  0.88747435]\n",
      "[ episode 245 ][ timestamp 10 ] state=[-0.08251382 -0.61812898  0.09262429  0.88747435], action=1, reward=1.0, next_state=[-0.0948764  -0.42437802  0.11037378  0.62528632]\n",
      "[ episode 245 ][ timestamp 11 ] state=[-0.0948764  -0.42437802  0.11037378  0.62528632], action=1, reward=1.0, next_state=[-0.10336396 -0.23095586  0.12287951  0.36930259]\n",
      "[ episode 245 ][ timestamp 12 ] state=[-0.10336396 -0.23095586  0.12287951  0.36930259], action=1, reward=1.0, next_state=[-0.10798308 -0.03777455  0.13026556  0.11775129]\n",
      "[ episode 245 ][ timestamp 13 ] state=[-0.10798308 -0.03777455  0.13026556  0.11775129], action=0, reward=1.0, next_state=[-0.10873857 -0.234499    0.13262058  0.44852767]\n",
      "[ episode 245 ][ timestamp 14 ] state=[-0.10873857 -0.234499    0.13262058  0.44852767], action=1, reward=1.0, next_state=[-0.11342855 -0.0414778   0.14159114  0.20041457]\n",
      "[ episode 245 ][ timestamp 15 ] state=[-0.11342855 -0.0414778   0.14159114  0.20041457], action=0, reward=1.0, next_state=[-0.11425811 -0.23831099  0.14559943  0.53419856]\n",
      "[ episode 245 ][ timestamp 16 ] state=[-0.11425811 -0.23831099  0.14559943  0.53419856], action=0, reward=1.0, next_state=[-0.11902433 -0.4351481   0.1562834   0.8689853 ]\n",
      "[ episode 245 ][ timestamp 17 ] state=[-0.11902433 -0.4351481   0.1562834   0.8689853 ], action=1, reward=1.0, next_state=[-0.12772729 -0.24245807  0.1736631   0.62923335]\n",
      "[ episode 245 ][ timestamp 18 ] state=[-0.12772729 -0.24245807  0.1736631   0.62923335], action=0, reward=1.0, next_state=[-0.13257645 -0.43952356  0.18624777  0.97118603]\n",
      "[ episode 245 ][ timestamp 19 ] state=[-0.13257645 -0.43952356  0.18624777  0.97118603], action=1, reward=1.0, next_state=[-0.14136692 -0.24732316  0.20567149  0.74231213]\n",
      "[ episode 245 ][ timestamp 20 ] state=[-0.14136692 -0.24732316  0.20567149  0.74231213], action=1, reward=-1.0, next_state=[-0.14631338 -0.05554384  0.22051773  0.52074806]\n",
      "[ Ended! ] Episode 245: Exploration_rate=0.2943280830920294. Score=20.\n",
      "[ Experience replay ] starts\n",
      "[ episode 246 ] state=[-0.04219169  0.04438108 -0.00594248 -0.03747589]\n",
      "[ episode 246 ][ timestamp 1 ] state=[-0.04219169  0.04438108 -0.00594248 -0.03747589], action=0, reward=1.0, next_state=[-0.04130407 -0.15065516 -0.006692    0.25332622]\n",
      "[ episode 246 ][ timestamp 2 ] state=[-0.04130407 -0.15065516 -0.006692    0.25332622], action=0, reward=1.0, next_state=[-0.04431717 -0.34568092 -0.00162547  0.54389087]\n",
      "[ episode 246 ][ timestamp 3 ] state=[-0.04431717 -0.34568092 -0.00162547  0.54389087], action=1, reward=1.0, next_state=[-0.05123079 -0.15053616  0.00925234  0.25069624]\n",
      "[ episode 246 ][ timestamp 4 ] state=[-0.05123079 -0.15053616  0.00925234  0.25069624], action=0, reward=1.0, next_state=[-0.05424151 -0.34578901  0.01426627  0.54628312]\n",
      "[ episode 246 ][ timestamp 5 ] state=[-0.05424151 -0.34578901  0.01426627  0.54628312], action=1, reward=1.0, next_state=[-0.0611573  -0.15087038  0.02519193  0.25812907]\n",
      "[ episode 246 ][ timestamp 6 ] state=[-0.0611573  -0.15087038  0.02519193  0.25812907], action=0, reward=1.0, next_state=[-0.0641747  -0.34634275  0.03035451  0.55865024]\n",
      "[ episode 246 ][ timestamp 7 ] state=[-0.0641747  -0.34634275  0.03035451  0.55865024], action=1, reward=1.0, next_state=[-0.07110156 -0.15165974  0.04152752  0.27568311]\n",
      "[ episode 246 ][ timestamp 8 ] state=[-0.07110156 -0.15165974  0.04152752  0.27568311], action=1, reward=1.0, next_state=[-0.07413475  0.04284588  0.04704118 -0.0036182 ]\n",
      "[ episode 246 ][ timestamp 9 ] state=[-0.07413475  0.04284588  0.04704118 -0.0036182 ], action=0, reward=1.0, next_state=[-0.07327783 -0.15291804  0.04696882  0.30352785]\n",
      "[ episode 246 ][ timestamp 10 ] state=[-0.07327783 -0.15291804  0.04696882  0.30352785], action=1, reward=1.0, next_state=[-0.0763362   0.04150416  0.05303937  0.02601992]\n",
      "[ episode 246 ][ timestamp 11 ] state=[-0.0763362   0.04150416  0.05303937  0.02601992], action=1, reward=1.0, next_state=[-0.07550611  0.23582694  0.05355977 -0.24946809]\n",
      "[ episode 246 ][ timestamp 12 ] state=[-0.07550611  0.23582694  0.05355977 -0.24946809], action=1, reward=1.0, next_state=[-0.07078957  0.43014474  0.04857041 -0.52478777]\n",
      "[ episode 246 ][ timestamp 13 ] state=[-0.07078957  0.43014474  0.04857041 -0.52478777], action=0, reward=1.0, next_state=[-0.06218668  0.23437412  0.03807465 -0.21720407]\n",
      "[ episode 246 ][ timestamp 14 ] state=[-0.06218668  0.23437412  0.03807465 -0.21720407], action=0, reward=1.0, next_state=[-0.0574992   0.03872915  0.03373057  0.08724195]\n",
      "[ episode 246 ][ timestamp 15 ] state=[-0.0574992   0.03872915  0.03373057  0.08724195], action=0, reward=1.0, next_state=[-0.05672461 -0.15685965  0.03547541  0.39037318]\n",
      "[ episode 246 ][ timestamp 16 ] state=[-0.05672461 -0.15685965  0.03547541  0.39037318], action=1, reward=1.0, next_state=[-0.05986181  0.03774132  0.04328288  0.10908297]\n",
      "[ episode 246 ][ timestamp 17 ] state=[-0.05986181  0.03774132  0.04328288  0.10908297], action=0, reward=1.0, next_state=[-0.05910698 -0.15797329  0.04546453  0.41510112]\n",
      "[ episode 246 ][ timestamp 18 ] state=[-0.05910698 -0.15797329  0.04546453  0.41510112], action=1, reward=1.0, next_state=[-0.06226645  0.03647579  0.05376656  0.13709087]\n",
      "[ episode 246 ][ timestamp 19 ] state=[-0.06226645  0.03647579  0.05376656  0.13709087], action=1, reward=1.0, next_state=[-0.06153693  0.23078805  0.05650837 -0.13815658]\n",
      "[ episode 246 ][ timestamp 20 ] state=[-0.06153693  0.23078805  0.05650837 -0.13815658], action=0, reward=1.0, next_state=[-0.05692117  0.03490419  0.05374524  0.17180485]\n",
      "[ episode 246 ][ timestamp 21 ] state=[-0.05692117  0.03490419  0.05374524  0.17180485], action=1, reward=1.0, next_state=[-0.05622309  0.22921735  0.05718134 -0.10345055]\n",
      "[ episode 246 ][ timestamp 22 ] state=[-0.05622309  0.22921735  0.05718134 -0.10345055], action=0, reward=1.0, next_state=[-0.05163874  0.03332454  0.05511233  0.20671058]\n",
      "[ episode 246 ][ timestamp 23 ] state=[-0.05163874  0.03332454  0.05511233  0.20671058], action=1, reward=1.0, next_state=[-0.05097225  0.22761685  0.05924654 -0.06809058]\n",
      "[ episode 246 ][ timestamp 24 ] state=[-0.05097225  0.22761685  0.05924654 -0.06809058], action=0, reward=1.0, next_state=[-0.04641991  0.03169774  0.05788473  0.24268076]\n",
      "[ episode 246 ][ timestamp 25 ] state=[-0.04641991  0.03169774  0.05788473  0.24268076], action=0, reward=1.0, next_state=[-0.04578596 -0.16420121  0.06273834  0.55304564]\n",
      "[ episode 246 ][ timestamp 26 ] state=[-0.04578596 -0.16420121  0.06273834  0.55304564], action=1, reward=1.0, next_state=[-0.04906998  0.02998619  0.07379926  0.28077057]\n",
      "[ episode 246 ][ timestamp 27 ] state=[-0.04906998  0.02998619  0.07379926  0.28077057], action=0, reward=1.0, next_state=[-0.04847026 -0.16610664  0.07941467  0.59578648]\n",
      "[ episode 246 ][ timestamp 28 ] state=[-0.04847026 -0.16610664  0.07941467  0.59578648], action=0, reward=1.0, next_state=[-0.05179239 -0.36224493  0.0913304   0.91239006]\n",
      "[ episode 246 ][ timestamp 29 ] state=[-0.05179239 -0.36224493  0.0913304   0.91239006], action=1, reward=1.0, next_state=[-0.05903729 -0.16846939  0.1095782   0.64975197]\n",
      "[ episode 246 ][ timestamp 30 ] state=[-0.05903729 -0.16846939  0.1095782   0.64975197], action=1, reward=1.0, next_state=[-0.06240668  0.02496937  0.12257324  0.39348567]\n",
      "[ episode 246 ][ timestamp 31 ] state=[-0.06240668  0.02496937  0.12257324  0.39348567], action=1, reward=1.0, next_state=[-0.06190729  0.21815814  0.13044295  0.14182302]\n",
      "[ episode 246 ][ timestamp 32 ] state=[-0.06190729  0.21815814  0.13044295  0.14182302], action=0, reward=1.0, next_state=[-0.05754413  0.02143269  0.13327941  0.4726458 ]\n",
      "[ episode 246 ][ timestamp 33 ] state=[-0.05754413  0.02143269  0.13327941  0.4726458 ], action=1, reward=1.0, next_state=[-0.05711547  0.21444535  0.14273233  0.22476267]\n",
      "[ episode 246 ][ timestamp 34 ] state=[-0.05711547  0.21444535  0.14273233  0.22476267], action=1, reward=1.0, next_state=[-0.05282656  0.40726952  0.14722758 -0.01971138]\n",
      "[ episode 246 ][ timestamp 35 ] state=[-0.05282656  0.40726952  0.14722758 -0.01971138], action=0, reward=1.0, next_state=[-0.04468117  0.21037658  0.14683335  0.31556164]\n",
      "[ episode 246 ][ timestamp 36 ] state=[-0.04468117  0.21037658  0.14683335  0.31556164], action=1, reward=1.0, next_state=[-0.04047364  0.40313516  0.15314459  0.07254912]\n",
      "[ episode 246 ][ timestamp 37 ] state=[-0.04047364  0.40313516  0.15314459  0.07254912], action=0, reward=1.0, next_state=[-0.03241094  0.20618721  0.15459557  0.40936224]\n",
      "[ episode 246 ][ timestamp 38 ] state=[-0.03241094  0.20618721  0.15459557  0.40936224], action=1, reward=1.0, next_state=[-0.0282872   0.39881817  0.16278281  0.16913207]\n",
      "[ episode 246 ][ timestamp 39 ] state=[-0.0282872   0.39881817  0.16278281  0.16913207], action=1, reward=1.0, next_state=[-0.02031083  0.59128124  0.16616546 -0.06809896]\n",
      "[ episode 246 ][ timestamp 40 ] state=[-0.02031083  0.59128124  0.16616546 -0.06809896], action=0, reward=1.0, next_state=[-0.00848521  0.39421509  0.16480348  0.2720569 ]\n",
      "[ episode 246 ][ timestamp 41 ] state=[-0.00848521  0.39421509  0.16480348  0.2720569 ], action=1, reward=1.0, next_state=[-0.0006009   0.58664895  0.17024461  0.03555035]\n",
      "[ episode 246 ][ timestamp 42 ] state=[-0.0006009   0.58664895  0.17024461  0.03555035], action=1, reward=1.0, next_state=[ 0.01113207  0.77897252  0.17095562 -0.19895398]\n",
      "[ episode 246 ][ timestamp 43 ] state=[ 0.01113207  0.77897252  0.17095562 -0.19895398], action=0, reward=1.0, next_state=[0.02671152 0.58187028 0.16697654 0.14240602]\n",
      "[ episode 246 ][ timestamp 44 ] state=[0.02671152 0.58187028 0.16697654 0.14240602], action=1, reward=1.0, next_state=[ 0.03834893  0.77425625  0.16982466 -0.09329601]\n",
      "[ episode 246 ][ timestamp 45 ] state=[ 0.03834893  0.77425625  0.16982466 -0.09329601], action=0, reward=1.0, next_state=[0.05383406 0.5771587  0.16795874 0.24778607]\n",
      "[ episode 246 ][ timestamp 46 ] state=[0.05383406 0.5771587  0.16795874 0.24778607], action=1, reward=1.0, next_state=[0.06537723 0.76953357 0.17291446 0.01243243]\n",
      "[ episode 246 ][ timestamp 47 ] state=[0.06537723 0.76953357 0.17291446 0.01243243], action=0, reward=1.0, next_state=[0.0807679  0.57240792 0.17316311 0.35429535]\n",
      "[ episode 246 ][ timestamp 48 ] state=[0.0807679  0.57240792 0.17316311 0.35429535], action=0, reward=1.0, next_state=[0.09221606 0.37530116 0.18024902 0.69618971]\n",
      "[ episode 246 ][ timestamp 49 ] state=[0.09221606 0.37530116 0.18024902 0.69618971], action=1, reward=1.0, next_state=[0.09972208 0.56752643 0.19417281 0.46522987]\n",
      "[ episode 246 ][ timestamp 50 ] state=[0.09972208 0.56752643 0.19417281 0.46522987], action=1, reward=1.0, next_state=[0.11107261 0.75945101 0.20347741 0.23948183]\n",
      "[ episode 246 ][ timestamp 51 ] state=[0.11107261 0.75945101 0.20347741 0.23948183], action=0, reward=1.0, next_state=[0.12626163 0.56209178 0.20826705 0.58882374]\n",
      "[ episode 246 ][ timestamp 52 ] state=[0.12626163 0.56209178 0.20826705 0.58882374], action=1, reward=-1.0, next_state=[0.13750347 0.7537828  0.22004352 0.3682895 ]\n",
      "[ Ended! ] Episode 246: Exploration_rate=0.29285644267656924. Score=52.\n",
      "[ Experience replay ] starts\n",
      "[ episode 247 ] state=[ 0.02595947 -0.04071938  0.02071329 -0.01532814]\n",
      "[ episode 247 ][ timestamp 1 ] state=[ 0.02595947 -0.04071938  0.02071329 -0.01532814], action=1, reward=1.0, next_state=[ 0.02514508  0.15409949  0.02040672 -0.30140449]\n",
      "[ episode 247 ][ timestamp 2 ] state=[ 0.02514508  0.15409949  0.02040672 -0.30140449], action=0, reward=1.0, next_state=[ 0.02822707 -0.04130728  0.01437863 -0.0023562 ]\n",
      "[ episode 247 ][ timestamp 3 ] state=[ 0.02822707 -0.04130728  0.01437863 -0.0023562 ], action=1, reward=1.0, next_state=[ 0.02740093  0.15360554  0.01433151 -0.29046803]\n",
      "[ episode 247 ][ timestamp 4 ] state=[ 0.02740093  0.15360554  0.01433151 -0.29046803], action=1, reward=1.0, next_state=[ 0.03047304  0.34852023  0.00852215 -0.57859673]\n",
      "[ episode 247 ][ timestamp 5 ] state=[ 0.03047304  0.34852023  0.00852215 -0.57859673], action=1, reward=1.0, next_state=[ 0.03744344  0.54352172 -0.00304978 -0.86858285]\n",
      "[ episode 247 ][ timestamp 6 ] state=[ 0.03744344  0.54352172 -0.00304978 -0.86858285], action=1, reward=1.0, next_state=[ 0.04831388  0.73868503 -0.02042144 -1.1622231 ]\n",
      "[ episode 247 ][ timestamp 7 ] state=[ 0.04831388  0.73868503 -0.02042144 -1.1622231 ], action=1, reward=1.0, next_state=[ 0.06308758  0.9340669  -0.0436659  -1.46123828]\n",
      "[ episode 247 ][ timestamp 8 ] state=[ 0.06308758  0.9340669  -0.0436659  -1.46123828], action=0, reward=1.0, next_state=[ 0.08176892  0.73950658 -0.07289067 -1.18250968]\n",
      "[ episode 247 ][ timestamp 9 ] state=[ 0.08176892  0.73950658 -0.07289067 -1.18250968], action=0, reward=1.0, next_state=[ 0.09655905  0.54540226 -0.09654086 -0.91353722]\n",
      "[ episode 247 ][ timestamp 10 ] state=[ 0.09655905  0.54540226 -0.09654086 -0.91353722], action=1, reward=1.0, next_state=[ 0.10746709  0.74168824 -0.11481161 -1.23493413]\n",
      "[ episode 247 ][ timestamp 11 ] state=[ 0.10746709  0.74168824 -0.11481161 -1.23493413], action=1, reward=1.0, next_state=[ 0.12230086  0.93808328 -0.13951029 -1.56126771]\n",
      "[ episode 247 ][ timestamp 12 ] state=[ 0.12230086  0.93808328 -0.13951029 -1.56126771], action=1, reward=1.0, next_state=[ 0.14106252  1.13457145 -0.17073564 -1.89401951]\n",
      "[ episode 247 ][ timestamp 13 ] state=[ 0.14106252  1.13457145 -0.17073564 -1.89401951], action=1, reward=1.0, next_state=[ 0.16375395  1.33108494 -0.20861603 -2.23445656]\n",
      "[ episode 247 ][ timestamp 14 ] state=[ 0.16375395  1.33108494 -0.20861603 -2.23445656], action=1, reward=-1.0, next_state=[ 0.19037565  1.52748747 -0.25330517 -2.5835621 ]\n",
      "[ Ended! ] Episode 247: Exploration_rate=0.2913921604631864. Score=14.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 248 ] state=[-0.00591278  0.04275419  0.01792983  0.01180421]\n",
      "[ episode 248 ][ timestamp 1 ] state=[-0.00591278  0.04275419  0.01792983  0.01180421], action=0, reward=1.0, next_state=[-0.00505769 -0.15262025  0.01816592  0.31008985]\n",
      "[ episode 248 ][ timestamp 2 ] state=[-0.00505769 -0.15262025  0.01816592  0.31008985], action=1, reward=1.0, next_state=[-0.0081101   0.04223823  0.02436771  0.02319083]\n",
      "[ episode 248 ][ timestamp 3 ] state=[-0.0081101   0.04223823  0.02436771  0.02319083], action=0, reward=1.0, next_state=[-0.00726533 -0.15322454  0.02483153  0.32346136]\n",
      "[ episode 248 ][ timestamp 4 ] state=[-0.00726533 -0.15322454  0.02483153  0.32346136], action=0, reward=1.0, next_state=[-0.01032983 -0.34869112  0.03130076  0.62387055]\n",
      "[ episode 248 ][ timestamp 5 ] state=[-0.01032983 -0.34869112  0.03130076  0.62387055], action=1, reward=1.0, next_state=[-0.01730365 -0.15401984  0.04377817  0.34120758]\n",
      "[ episode 248 ][ timestamp 6 ] state=[-0.01730365 -0.15401984  0.04377817  0.34120758], action=1, reward=1.0, next_state=[-0.02038404  0.04045279  0.05060232  0.0626448 ]\n",
      "[ episode 248 ][ timestamp 7 ] state=[-0.02038404  0.04045279  0.05060232  0.0626448 ], action=0, reward=1.0, next_state=[-0.01957499 -0.15535679  0.05185522  0.37085394]\n",
      "[ episode 248 ][ timestamp 8 ] state=[-0.01957499 -0.15535679  0.05185522  0.37085394], action=1, reward=1.0, next_state=[-0.02268212  0.03899157  0.05927229  0.09496187]\n",
      "[ episode 248 ][ timestamp 9 ] state=[-0.02268212  0.03899157  0.05927229  0.09496187], action=1, reward=1.0, next_state=[-0.02190229  0.23321611  0.06117153 -0.17844748]\n",
      "[ episode 248 ][ timestamp 10 ] state=[-0.02190229  0.23321611  0.06117153 -0.17844748], action=0, reward=1.0, next_state=[-0.01723797  0.03727454  0.05760258  0.13288836]\n",
      "[ episode 248 ][ timestamp 11 ] state=[-0.01723797  0.03727454  0.05760258  0.13288836], action=1, reward=1.0, next_state=[-0.01649248  0.23152609  0.06026035 -0.1410799 ]\n",
      "[ episode 248 ][ timestamp 12 ] state=[-0.01649248  0.23152609  0.06026035 -0.1410799 ], action=0, reward=1.0, next_state=[-0.01186196  0.03559517  0.05743875  0.16998884]\n",
      "[ episode 248 ][ timestamp 13 ] state=[-0.01186196  0.03559517  0.05743875  0.16998884], action=1, reward=1.0, next_state=[-0.01115006  0.22984996  0.06083853 -0.10403509]\n",
      "[ episode 248 ][ timestamp 14 ] state=[-0.01115006  0.22984996  0.06083853 -0.10403509], action=1, reward=1.0, next_state=[-0.00655306  0.42404966  0.05875783 -0.37692022]\n",
      "[ episode 248 ][ timestamp 15 ] state=[-0.00655306  0.42404966  0.05875783 -0.37692022], action=0, reward=1.0, next_state=[ 0.00192794  0.22814454  0.05121942 -0.06630481]\n",
      "[ episode 248 ][ timestamp 16 ] state=[ 0.00192794  0.22814454  0.05121942 -0.06630481], action=1, reward=1.0, next_state=[ 0.00649083  0.42249616  0.04989333 -0.34239799]\n",
      "[ episode 248 ][ timestamp 17 ] state=[ 0.00649083  0.42249616  0.04989333 -0.34239799], action=0, reward=1.0, next_state=[ 0.01494075  0.22670119  0.04304537 -0.03440846]\n",
      "[ episode 248 ][ timestamp 18 ] state=[ 0.01494075  0.22670119  0.04304537 -0.03440846], action=0, reward=1.0, next_state=[0.01947477 0.03098924 0.0423572  0.27153897]\n",
      "[ episode 248 ][ timestamp 19 ] state=[0.01947477 0.03098924 0.0423572  0.27153897], action=1, reward=1.0, next_state=[ 0.02009456  0.22548198  0.04778798 -0.00748918]\n",
      "[ episode 248 ][ timestamp 20 ] state=[ 0.02009456  0.22548198  0.04778798 -0.00748918], action=0, reward=1.0, next_state=[0.0246042  0.02970841 0.04763819 0.29988024]\n",
      "[ episode 248 ][ timestamp 21 ] state=[0.0246042  0.02970841 0.04763819 0.29988024], action=1, reward=1.0, next_state=[0.02519837 0.22412012 0.0536358  0.02259385]\n",
      "[ episode 248 ][ timestamp 22 ] state=[0.02519837 0.22412012 0.0536358  0.02259385], action=0, reward=1.0, next_state=[0.02968077 0.02827164 0.05408767 0.33170547]\n",
      "[ episode 248 ][ timestamp 23 ] state=[0.02968077 0.02827164 0.05408767 0.33170547], action=1, reward=1.0, next_state=[0.0302462  0.22258365 0.06072178 0.05655771]\n",
      "[ episode 248 ][ timestamp 24 ] state=[0.0302462  0.22258365 0.06072178 0.05655771], action=0, reward=1.0, next_state=[0.03469788 0.02664599 0.06185294 0.36776377]\n",
      "[ episode 248 ][ timestamp 25 ] state=[0.03469788 0.02664599 0.06185294 0.36776377], action=1, reward=1.0, next_state=[0.0352308  0.220837   0.06920821 0.09520744]\n",
      "[ episode 248 ][ timestamp 26 ] state=[0.0352308  0.220837   0.06920821 0.09520744], action=0, reward=1.0, next_state=[0.03964754 0.02479488 0.07111236 0.40889764]\n",
      "[ episode 248 ][ timestamp 27 ] state=[0.03964754 0.02479488 0.07111236 0.40889764], action=1, reward=1.0, next_state=[0.04014343 0.21884032 0.07929032 0.13945455]\n",
      "[ episode 248 ][ timestamp 28 ] state=[0.04014343 0.21884032 0.07929032 0.13945455], action=1, reward=1.0, next_state=[ 0.04452024  0.41274238  0.08207941 -0.1271978 ]\n",
      "[ episode 248 ][ timestamp 29 ] state=[ 0.04452024  0.41274238  0.08207941 -0.1271978 ], action=0, reward=1.0, next_state=[0.05277509 0.21654636 0.07953545 0.1902097 ]\n",
      "[ episode 248 ][ timestamp 30 ] state=[0.05277509 0.21654636 0.07953545 0.1902097 ], action=1, reward=1.0, next_state=[ 0.05710601  0.41044571  0.08333964 -0.07636108]\n",
      "[ episode 248 ][ timestamp 31 ] state=[ 0.05710601  0.41044571  0.08333964 -0.07636108], action=0, reward=1.0, next_state=[0.06531493 0.21423403 0.08181242 0.24140844]\n",
      "[ episode 248 ][ timestamp 32 ] state=[0.06531493 0.21423403 0.08181242 0.24140844], action=0, reward=1.0, next_state=[0.06959961 0.01804451 0.08664059 0.55873443]\n",
      "[ episode 248 ][ timestamp 33 ] state=[0.06959961 0.01804451 0.08664059 0.55873443], action=1, reward=1.0, next_state=[0.0699605  0.21185029 0.09781528 0.29455667]\n",
      "[ episode 248 ][ timestamp 34 ] state=[0.0699605  0.21185029 0.09781528 0.29455667], action=1, reward=1.0, next_state=[0.07419751 0.40545152 0.10370641 0.03425484]\n",
      "[ episode 248 ][ timestamp 35 ] state=[0.07419751 0.40545152 0.10370641 0.03425484], action=0, reward=1.0, next_state=[0.08230654 0.20900707 0.10439151 0.35777343]\n",
      "[ episode 248 ][ timestamp 36 ] state=[0.08230654 0.20900707 0.10439151 0.35777343], action=1, reward=1.0, next_state=[0.08648668 0.40250203 0.11154698 0.09974641]\n",
      "[ episode 248 ][ timestamp 37 ] state=[0.08648668 0.40250203 0.11154698 0.09974641], action=1, reward=1.0, next_state=[ 0.09453672  0.59586323  0.11354191 -0.15576597]\n",
      "[ episode 248 ][ timestamp 38 ] state=[ 0.09453672  0.59586323  0.11354191 -0.15576597], action=0, reward=1.0, next_state=[0.10645398 0.39931418 0.11042659 0.17046891]\n",
      "[ episode 248 ][ timestamp 39 ] state=[0.10645398 0.39931418 0.11042659 0.17046891], action=0, reward=1.0, next_state=[0.11444027 0.20279914 0.11383597 0.49584553]\n",
      "[ episode 248 ][ timestamp 40 ] state=[0.11444027 0.20279914 0.11383597 0.49584553], action=0, reward=1.0, next_state=[0.11849625 0.00627147 0.12375288 0.82212458]\n",
      "[ episode 248 ][ timestamp 41 ] state=[0.11849625 0.00627147 0.12375288 0.82212458], action=0, reward=1.0, next_state=[ 0.11862168 -0.19030666  0.14019537  1.15102729]\n",
      "[ episode 248 ][ timestamp 42 ] state=[ 0.11862168 -0.19030666  0.14019537  1.15102729], action=1, reward=1.0, next_state=[0.11481554 0.00273574 0.16321591 0.90538725]\n",
      "[ episode 248 ][ timestamp 43 ] state=[0.11481554 0.00273574 0.16321591 0.90538725], action=1, reward=1.0, next_state=[0.11487026 0.19531617 0.18132366 0.66812845]\n",
      "[ episode 248 ][ timestamp 44 ] state=[0.11487026 0.19531617 0.18132366 0.66812845], action=1, reward=1.0, next_state=[0.11877658 0.3875155  0.19468623 0.43757338]\n",
      "[ episode 248 ][ timestamp 45 ] state=[0.11877658 0.3875155  0.19468623 0.43757338], action=1, reward=1.0, next_state=[0.12652689 0.57942598 0.2034377  0.21202276]\n",
      "[ episode 248 ][ timestamp 46 ] state=[0.12652689 0.57942598 0.2034377  0.21202276], action=1, reward=1.0, next_state=[ 0.13811541  0.77114641  0.20767815 -0.01022837]\n",
      "[ episode 248 ][ timestamp 47 ] state=[ 0.13811541  0.77114641  0.20767815 -0.01022837], action=0, reward=1.0, next_state=[0.15353834 0.57374504 0.20747358 0.34013054]\n",
      "[ episode 248 ][ timestamp 48 ] state=[0.15353834 0.57374504 0.20747358 0.34013054], action=1, reward=-1.0, next_state=[0.16501324 0.76540437 0.21427619 0.11936747]\n",
      "[ Ended! ] Episode 248: Exploration_rate=0.28993519966087045. Score=48.\n",
      "[ Experience replay ] starts\n",
      "[ episode 249 ] state=[ 0.03831255 -0.00997742 -0.01093975 -0.01371889]\n",
      "[ episode 249 ][ timestamp 1 ] state=[ 0.03831255 -0.00997742 -0.01093975 -0.01371889], action=1, reward=1.0, next_state=[ 0.038113    0.1852997  -0.01121413 -0.30983326]\n",
      "[ episode 249 ][ timestamp 2 ] state=[ 0.038113    0.1852997  -0.01121413 -0.30983326], action=1, reward=1.0, next_state=[ 0.041819    0.38057962 -0.01741079 -0.6060316 ]\n",
      "[ episode 249 ][ timestamp 3 ] state=[ 0.041819    0.38057962 -0.01741079 -0.6060316 ], action=1, reward=1.0, next_state=[ 0.04943059  0.57594064 -0.02953142 -0.90414724]\n",
      "[ episode 249 ][ timestamp 4 ] state=[ 0.04943059  0.57594064 -0.02953142 -0.90414724], action=0, reward=1.0, next_state=[ 0.0609494   0.38123084 -0.04761437 -0.62089086]\n",
      "[ episode 249 ][ timestamp 5 ] state=[ 0.0609494   0.38123084 -0.04761437 -0.62089086], action=0, reward=1.0, next_state=[ 0.06857402  0.18680505 -0.06003218 -0.34357603]\n",
      "[ episode 249 ][ timestamp 6 ] state=[ 0.06857402  0.18680505 -0.06003218 -0.34357603], action=1, reward=1.0, next_state=[ 0.07231012  0.38272737 -0.06690371 -0.65456898]\n",
      "[ episode 249 ][ timestamp 7 ] state=[ 0.07231012  0.38272737 -0.06690371 -0.65456898], action=1, reward=1.0, next_state=[ 0.07996467  0.57871392 -0.07999509 -0.96754612]\n",
      "[ episode 249 ][ timestamp 8 ] state=[ 0.07996467  0.57871392 -0.07999509 -0.96754612], action=0, reward=1.0, next_state=[ 0.09153895  0.38475197 -0.09934601 -0.70102708]\n",
      "[ episode 249 ][ timestamp 9 ] state=[ 0.09153895  0.38475197 -0.09934601 -0.70102708], action=1, reward=1.0, next_state=[ 0.09923399  0.58110045 -0.11336655 -1.02325729]\n",
      "[ episode 249 ][ timestamp 10 ] state=[ 0.09923399  0.58110045 -0.11336655 -1.02325729], action=1, reward=1.0, next_state=[ 0.110856    0.77753485 -0.13383169 -1.34927591]\n",
      "[ episode 249 ][ timestamp 11 ] state=[ 0.110856    0.77753485 -0.13383169 -1.34927591], action=0, reward=1.0, next_state=[ 0.12640669  0.58432412 -0.16081721 -1.10128054]\n",
      "[ episode 249 ][ timestamp 12 ] state=[ 0.12640669  0.58432412 -0.16081721 -1.10128054], action=1, reward=1.0, next_state=[ 0.13809317  0.78115412 -0.18284282 -1.43979267]\n",
      "[ episode 249 ][ timestamp 13 ] state=[ 0.13809317  0.78115412 -0.18284282 -1.43979267], action=0, reward=-1.0, next_state=[ 0.15371626  0.5886942  -0.21163868 -1.20937178]\n",
      "[ Ended! ] Episode 249: Exploration_rate=0.2884855236625661. Score=13.\n",
      "[ Experience replay ] starts\n",
      "[ episode 250 ] state=[-0.04366089  0.01380097 -0.04067373 -0.02860283]\n",
      "[ episode 250 ][ timestamp 1 ] state=[-0.04366089  0.01380097 -0.04067373 -0.02860283], action=1, reward=1.0, next_state=[-0.04338487  0.20948189 -0.04124579 -0.33383623]\n",
      "[ episode 250 ][ timestamp 2 ] state=[-0.04338487  0.20948189 -0.04124579 -0.33383623], action=0, reward=1.0, next_state=[-0.03919523  0.01497051 -0.04792251 -0.05444013]\n",
      "[ episode 250 ][ timestamp 3 ] state=[-0.03919523  0.01497051 -0.04792251 -0.05444013], action=1, reward=1.0, next_state=[-0.03889582  0.21074567 -0.04901132 -0.36184956]\n",
      "[ episode 250 ][ timestamp 4 ] state=[-0.03889582  0.21074567 -0.04901132 -0.36184956], action=0, reward=1.0, next_state=[-0.03468091  0.01635337 -0.05624831 -0.08501481]\n",
      "[ episode 250 ][ timestamp 5 ] state=[-0.03468091  0.01635337 -0.05624831 -0.08501481], action=1, reward=1.0, next_state=[-0.03435384  0.21223461 -0.0579486  -0.39490027]\n",
      "[ episode 250 ][ timestamp 6 ] state=[-0.03435384  0.21223461 -0.0579486  -0.39490027], action=0, reward=1.0, next_state=[-0.03010915  0.01798073 -0.06584661 -0.12103591]\n",
      "[ episode 250 ][ timestamp 7 ] state=[-0.03010915  0.01798073 -0.06584661 -0.12103591], action=1, reward=1.0, next_state=[-0.02974954  0.21398126 -0.06826733 -0.43374449]\n",
      "[ episode 250 ][ timestamp 8 ] state=[-0.02974954  0.21398126 -0.06826733 -0.43374449], action=0, reward=1.0, next_state=[-0.02546991  0.01988889 -0.07694222 -0.16333909]\n",
      "[ episode 250 ][ timestamp 9 ] state=[-0.02546991  0.01988889 -0.07694222 -0.16333909], action=1, reward=1.0, next_state=[-0.02507213  0.21602316 -0.080209   -0.47926877]\n",
      "[ episode 250 ][ timestamp 10 ] state=[-0.02507213  0.21602316 -0.080209   -0.47926877], action=0, reward=1.0, next_state=[-0.02075167  0.02211972 -0.08979438 -0.21290489]\n",
      "[ episode 250 ][ timestamp 11 ] state=[-0.02075167  0.02211972 -0.08979438 -0.21290489], action=1, reward=1.0, next_state=[-0.02030928  0.21840309 -0.09405247 -0.53250784]\n",
      "[ episode 250 ][ timestamp 12 ] state=[-0.02030928  0.21840309 -0.09405247 -0.53250784], action=0, reward=1.0, next_state=[-0.01594121  0.02472106 -0.10470263 -0.27087949]\n",
      "[ episode 250 ][ timestamp 13 ] state=[-0.01594121  0.02472106 -0.10470263 -0.27087949], action=1, reward=1.0, next_state=[-0.01544679  0.22116916 -0.11012022 -0.59466429]\n",
      "[ episode 250 ][ timestamp 14 ] state=[-0.01544679  0.22116916 -0.11012022 -0.59466429], action=0, reward=1.0, next_state=[-0.01102341  0.02774678 -0.12201351 -0.33859803]\n",
      "[ episode 250 ][ timestamp 15 ] state=[-0.01102341  0.02774678 -0.12201351 -0.33859803], action=1, reward=1.0, next_state=[-0.01046847  0.22437452 -0.12878547 -0.66712996]\n",
      "[ episode 250 ][ timestamp 16 ] state=[-0.01046847  0.22437452 -0.12878547 -0.66712996], action=1, reward=1.0, next_state=[-0.00598098  0.42102999 -0.14212807 -0.99742863]\n",
      "[ episode 250 ][ timestamp 17 ] state=[-0.00598098  0.42102999 -0.14212807 -0.99742863], action=0, reward=1.0, next_state=[ 0.00243962  0.22806484 -0.16207664 -0.75254458]\n",
      "[ episode 250 ][ timestamp 18 ] state=[ 0.00243962  0.22806484 -0.16207664 -0.75254458], action=0, reward=1.0, next_state=[ 0.00700091  0.03550454 -0.17712753 -0.51493178]\n",
      "[ episode 250 ][ timestamp 19 ] state=[ 0.00700091  0.03550454 -0.17712753 -0.51493178], action=0, reward=1.0, next_state=[ 0.007711   -0.15673879 -0.18742617 -0.28288219]\n",
      "[ episode 250 ][ timestamp 20 ] state=[ 0.007711   -0.15673879 -0.18742617 -0.28288219], action=0, reward=1.0, next_state=[ 0.00457623 -0.34876201 -0.19308381 -0.05467293]\n",
      "[ episode 250 ][ timestamp 21 ] state=[ 0.00457623 -0.34876201 -0.19308381 -0.05467293], action=1, reward=1.0, next_state=[-0.00239901 -0.15147136 -0.19417727 -0.40152415]\n",
      "[ episode 250 ][ timestamp 22 ] state=[-0.00239901 -0.15147136 -0.19417727 -0.40152415], action=0, reward=1.0, next_state=[-0.00542844 -0.3433855  -0.20220775 -0.17579301]\n",
      "[ episode 250 ][ timestamp 23 ] state=[-0.00542844 -0.3433855  -0.20220775 -0.17579301], action=1, reward=1.0, next_state=[-0.01229615 -0.1460305  -0.20572361 -0.52483881]\n",
      "[ episode 250 ][ timestamp 24 ] state=[-0.01229615 -0.1460305  -0.20572361 -0.52483881], action=1, reward=-1.0, next_state=[-0.01521676  0.05130187 -0.21622039 -0.87465279]\n",
      "[ Ended! ] Episode 250: Exploration_rate=0.28704309604425327. Score=24.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 251 ] state=[ 0.01982677  0.04921568 -0.03419026  0.03515414]\n",
      "[ episode 251 ][ timestamp 1 ] state=[ 0.01982677  0.04921568 -0.03419026  0.03515414], action=0, reward=1.0, next_state=[ 0.02081108 -0.14539972 -0.03348717  0.31685665]\n",
      "[ episode 251 ][ timestamp 2 ] state=[ 0.02081108 -0.14539972 -0.03348717  0.31685665], action=0, reward=1.0, next_state=[ 0.01790308 -0.34002909 -0.02715004  0.59879364]\n",
      "[ episode 251 ][ timestamp 3 ] state=[ 0.01790308 -0.34002909 -0.02715004  0.59879364], action=1, reward=1.0, next_state=[ 0.0111025  -0.144538   -0.01517417  0.29768394]\n",
      "[ episode 251 ][ timestamp 4 ] state=[ 0.0111025  -0.144538   -0.01517417  0.29768394], action=0, reward=1.0, next_state=[ 0.00821174 -0.33944039 -0.00922049  0.58554284]\n",
      "[ episode 251 ][ timestamp 5 ] state=[ 0.00821174 -0.33944039 -0.00922049  0.58554284], action=1, reward=1.0, next_state=[ 0.00142294 -0.14419051  0.00249037  0.28996968]\n",
      "[ episode 251 ][ timestamp 6 ] state=[ 0.00142294 -0.14419051  0.00249037  0.28996968], action=0, reward=1.0, next_state=[-0.00146087 -0.33934788  0.00828976  0.583437  ]\n",
      "[ episode 251 ][ timestamp 7 ] state=[-0.00146087 -0.33934788  0.00828976  0.583437  ], action=1, reward=1.0, next_state=[-0.00824783 -0.14434304  0.0199585   0.29337695]\n",
      "[ episode 251 ][ timestamp 8 ] state=[-0.00824783 -0.14434304  0.0199585   0.29337695], action=0, reward=1.0, next_state=[-0.01113469 -0.33974378  0.02582604  0.59228709]\n",
      "[ episode 251 ][ timestamp 9 ] state=[-0.01113469 -0.33974378  0.02582604  0.59228709], action=1, reward=1.0, next_state=[-0.01792957 -0.14499271  0.03767178  0.30784992]\n",
      "[ episode 251 ][ timestamp 10 ] state=[-0.01792957 -0.14499271  0.03767178  0.30784992], action=0, reward=1.0, next_state=[-0.02082942 -0.34063063  0.04382878  0.61217148]\n",
      "[ episode 251 ][ timestamp 11 ] state=[-0.02082942 -0.34063063  0.04382878  0.61217148], action=1, reward=1.0, next_state=[-0.02764204 -0.14614775  0.05607221  0.33360883]\n",
      "[ episode 251 ][ timestamp 12 ] state=[-0.02764204 -0.14614775  0.05607221  0.33360883], action=0, reward=1.0, next_state=[-0.03056499 -0.34202107  0.06274439  0.64343365]\n",
      "[ episode 251 ][ timestamp 13 ] state=[-0.03056499 -0.34202107  0.06274439  0.64343365], action=0, reward=1.0, next_state=[-0.03740541 -0.53795881  0.07561306  0.95519667]\n",
      "[ episode 251 ][ timestamp 14 ] state=[-0.03740541 -0.53795881  0.07561306  0.95519667], action=1, reward=1.0, next_state=[-0.04816459 -0.34393091  0.09471699  0.68719547]\n",
      "[ episode 251 ][ timestamp 15 ] state=[-0.04816459 -0.34393091  0.09471699  0.68719547], action=1, reward=1.0, next_state=[-0.05504321 -0.15024242  0.1084609   0.42577017]\n",
      "[ episode 251 ][ timestamp 16 ] state=[-0.05504321 -0.15024242  0.1084609   0.42577017], action=1, reward=1.0, next_state=[-0.05804805  0.04318951  0.11697631  0.16915224]\n",
      "[ episode 251 ][ timestamp 17 ] state=[-0.05804805  0.04318951  0.11697631  0.16915224], action=0, reward=1.0, next_state=[-0.05718426 -0.1533956   0.12035935  0.49632739]\n",
      "[ episode 251 ][ timestamp 18 ] state=[-0.05718426 -0.1533956   0.12035935  0.49632739], action=1, reward=1.0, next_state=[-0.06025218  0.03984186  0.1302859   0.24386842]\n",
      "[ episode 251 ][ timestamp 19 ] state=[-0.06025218  0.03984186  0.1302859   0.24386842], action=0, reward=1.0, next_state=[-0.05945534 -0.15687702  0.13516327  0.57464167]\n",
      "[ episode 251 ][ timestamp 20 ] state=[-0.05945534 -0.15687702  0.13516327  0.57464167], action=1, reward=1.0, next_state=[-0.06259288  0.03611701  0.1466561   0.3274081 ]\n",
      "[ episode 251 ][ timestamp 21 ] state=[-0.06259288  0.03611701  0.1466561   0.3274081 ], action=1, reward=1.0, next_state=[-0.06187054  0.22887981  0.15320426  0.08433028]\n",
      "[ episode 251 ][ timestamp 22 ] state=[-0.06187054  0.22887981  0.15320426  0.08433028], action=0, reward=1.0, next_state=[-0.05729294  0.03193157  0.15489087  0.42115846]\n",
      "[ episode 251 ][ timestamp 23 ] state=[-0.05729294  0.03193157  0.15489087  0.42115846], action=1, reward=1.0, next_state=[-0.05665431  0.22455877  0.16331404  0.18103281]\n",
      "[ episode 251 ][ timestamp 24 ] state=[-0.05665431  0.22455877  0.16331404  0.18103281], action=0, reward=1.0, next_state=[-0.05216314  0.0275224   0.16693469  0.52045585]\n",
      "[ episode 251 ][ timestamp 25 ] state=[-0.05216314  0.0275224   0.16693469  0.52045585], action=1, reward=1.0, next_state=[-0.05161269  0.21994966  0.17734381  0.2846786 ]\n",
      "[ episode 251 ][ timestamp 26 ] state=[-0.05161269  0.21994966  0.17734381  0.2846786 ], action=1, reward=1.0, next_state=[-0.0472137   0.41215748  0.18303738  0.05275503]\n",
      "[ episode 251 ][ timestamp 27 ] state=[-0.0472137   0.41215748  0.18303738  0.05275503], action=0, reward=1.0, next_state=[-0.03897055  0.21494731  0.18409248  0.39714182]\n",
      "[ episode 251 ][ timestamp 28 ] state=[-0.03897055  0.21494731  0.18409248  0.39714182], action=1, reward=1.0, next_state=[-0.0346716   0.40704552  0.19203532  0.1676814 ]\n",
      "[ episode 251 ][ timestamp 29 ] state=[-0.0346716   0.40704552  0.19203532  0.1676814 ], action=1, reward=1.0, next_state=[-0.02653069  0.59897429  0.19538895 -0.05880764]\n",
      "[ episode 251 ][ timestamp 30 ] state=[-0.02653069  0.59897429  0.19538895 -0.05880764], action=1, reward=1.0, next_state=[-0.0145512   0.79083634  0.19421279 -0.28404512]\n",
      "[ episode 251 ][ timestamp 31 ] state=[-0.0145512   0.79083634  0.19421279 -0.28404512], action=0, reward=1.0, next_state=[0.00126552 0.59355147 0.18853189 0.06305901]\n",
      "[ episode 251 ][ timestamp 32 ] state=[0.00126552 0.59355147 0.18853189 0.06305901], action=0, reward=1.0, next_state=[0.01313655 0.39629701 0.18979307 0.40879841]\n",
      "[ episode 251 ][ timestamp 33 ] state=[0.01313655 0.39629701 0.18979307 0.40879841], action=1, reward=1.0, next_state=[0.02106249 0.58829271 0.19796904 0.18144105]\n",
      "[ episode 251 ][ timestamp 34 ] state=[0.02106249 0.58829271 0.19796904 0.18144105], action=1, reward=1.0, next_state=[ 0.03282835  0.78011248  0.20159786 -0.04284522]\n",
      "[ episode 251 ][ timestamp 35 ] state=[ 0.03282835  0.78011248  0.20159786 -0.04284522], action=0, reward=1.0, next_state=[0.0484306  0.58275637 0.20074096 0.30606275]\n",
      "[ episode 251 ][ timestamp 36 ] state=[0.0484306  0.58275637 0.20074096 0.30606275], action=1, reward=1.0, next_state=[0.06008572 0.77453642 0.20686221 0.0827916 ]\n",
      "[ episode 251 ][ timestamp 37 ] state=[0.06008572 0.77453642 0.20686221 0.0827916 ], action=1, reward=1.0, next_state=[ 0.07557645  0.96618539  0.20851804 -0.1381683 ]\n",
      "[ episode 251 ][ timestamp 38 ] state=[ 0.07557645  0.96618539  0.20851804 -0.1381683 ], action=0, reward=1.0, next_state=[0.09490016 0.76878175 0.20575468 0.21238416]\n",
      "[ episode 251 ][ timestamp 39 ] state=[0.09490016 0.76878175 0.20575468 0.21238416], action=0, reward=-1.0, next_state=[0.1102758  0.57140338 0.21000236 0.56227274]\n",
      "[ Ended! ] Episode 251: Exploration_rate=0.285607880564032. Score=39.\n",
      "[ Experience replay ] starts\n",
      "[ episode 252 ] state=[ 0.04332091 -0.02265626 -0.00345676  0.04958407]\n",
      "[ episode 252 ][ timestamp 1 ] state=[ 0.04332091 -0.02265626 -0.00345676  0.04958407], action=1, reward=1.0, next_state=[ 0.04286778  0.17251508 -0.00246508 -0.24418749]\n",
      "[ episode 252 ][ timestamp 2 ] state=[ 0.04286778  0.17251508 -0.00246508 -0.24418749], action=1, reward=1.0, next_state=[ 0.04631808  0.36767216 -0.00734883 -0.53764694]\n",
      "[ episode 252 ][ timestamp 3 ] state=[ 0.04631808  0.36767216 -0.00734883 -0.53764694], action=0, reward=1.0, next_state=[ 0.05367153  0.17265429 -0.01810177 -0.24728858]\n",
      "[ episode 252 ][ timestamp 4 ] state=[ 0.05367153  0.17265429 -0.01810177 -0.24728858], action=0, reward=1.0, next_state=[ 0.05712461 -0.02220452 -0.02304754  0.03963011]\n",
      "[ episode 252 ][ timestamp 5 ] state=[ 0.05712461 -0.02220452 -0.02304754  0.03963011], action=1, reward=1.0, next_state=[ 0.05668052  0.17324022 -0.02225494 -0.26023451]\n",
      "[ episode 252 ][ timestamp 6 ] state=[ 0.05668052  0.17324022 -0.02225494 -0.26023451], action=0, reward=1.0, next_state=[ 0.06014533 -0.02155708 -0.02745963  0.02534667]\n",
      "[ episode 252 ][ timestamp 7 ] state=[ 0.06014533 -0.02155708 -0.02745963  0.02534667], action=0, reward=1.0, next_state=[ 0.05971418 -0.21627469 -0.0269527   0.30924086]\n",
      "[ episode 252 ][ timestamp 8 ] state=[ 0.05971418 -0.21627469 -0.0269527   0.30924086], action=1, reward=1.0, next_state=[ 0.05538869 -0.02077929 -0.02076788  0.00818113]\n",
      "[ episode 252 ][ timestamp 9 ] state=[ 0.05538869 -0.02077929 -0.02076788  0.00818113], action=0, reward=1.0, next_state=[ 0.0549731  -0.21559734 -0.02060426  0.29423987]\n",
      "[ episode 252 ][ timestamp 10 ] state=[ 0.0549731  -0.21559734 -0.02060426  0.29423987], action=1, reward=1.0, next_state=[ 0.05066116 -0.02018779 -0.01471946 -0.00486947]\n",
      "[ episode 252 ][ timestamp 11 ] state=[ 0.05066116 -0.02018779 -0.01471946 -0.00486947], action=0, reward=1.0, next_state=[ 0.0502574  -0.21509558 -0.01481685  0.28313319]\n",
      "[ episode 252 ][ timestamp 12 ] state=[ 0.0502574  -0.21509558 -0.01481685  0.28313319], action=1, reward=1.0, next_state=[ 0.04595549 -0.01976546 -0.00915419 -0.01418582]\n",
      "[ episode 252 ][ timestamp 13 ] state=[ 0.04595549 -0.01976546 -0.00915419 -0.01418582], action=0, reward=1.0, next_state=[ 0.04556018 -0.21475494 -0.0094379   0.27559485]\n",
      "[ episode 252 ][ timestamp 14 ] state=[ 0.04556018 -0.21475494 -0.0094379   0.27559485], action=1, reward=1.0, next_state=[ 0.04126508 -0.01949962 -0.00392601 -0.0200498 ]\n",
      "[ episode 252 ][ timestamp 15 ] state=[ 0.04126508 -0.01949962 -0.00392601 -0.0200498 ], action=0, reward=1.0, next_state=[ 0.04087509 -0.21456504 -0.004327    0.27139185]\n",
      "[ episode 252 ][ timestamp 16 ] state=[ 0.04087509 -0.21456504 -0.004327    0.27139185], action=1, reward=1.0, next_state=[ 0.03658379 -0.01938162  0.00110084 -0.02265269]\n",
      "[ episode 252 ][ timestamp 17 ] state=[ 0.03658379 -0.01938162  0.00110084 -0.02265269], action=0, reward=1.0, next_state=[ 0.03619616 -0.21451934  0.00064778  0.27037736]\n",
      "[ episode 252 ][ timestamp 18 ] state=[ 0.03619616 -0.21451934  0.00064778  0.27037736], action=1, reward=1.0, next_state=[ 0.03190577 -0.01940664  0.00605533 -0.02210118]\n",
      "[ episode 252 ][ timestamp 19 ] state=[ 0.03190577 -0.01940664  0.00605533 -0.02210118], action=0, reward=1.0, next_state=[ 0.03151764 -0.2146149   0.00561331  0.27248611]\n",
      "[ episode 252 ][ timestamp 20 ] state=[ 0.03151764 -0.2146149   0.00561331  0.27248611], action=0, reward=1.0, next_state=[ 0.02722534 -0.4098165   0.01106303  0.56693419]\n",
      "[ episode 252 ][ timestamp 21 ] state=[ 0.02722534 -0.4098165   0.01106303  0.56693419], action=1, reward=1.0, next_state=[ 0.01902901 -0.21485147  0.02240171  0.27775701]\n",
      "[ episode 252 ][ timestamp 22 ] state=[ 0.01902901 -0.21485147  0.02240171  0.27775701], action=1, reward=1.0, next_state=[ 0.01473198 -0.02005615  0.02795685 -0.0077771 ]\n",
      "[ episode 252 ][ timestamp 23 ] state=[ 0.01473198 -0.02005615  0.02795685 -0.0077771 ], action=0, reward=1.0, next_state=[ 0.01433086 -0.21556766  0.02780131  0.2935938 ]\n",
      "[ episode 252 ][ timestamp 24 ] state=[ 0.01433086 -0.21556766  0.02780131  0.2935938 ], action=1, reward=1.0, next_state=[ 0.0100195  -0.02085288  0.03367319  0.00980703]\n",
      "[ episode 252 ][ timestamp 25 ] state=[ 0.0100195  -0.02085288  0.03367319  0.00980703], action=0, reward=1.0, next_state=[ 0.00960245 -0.21644117  0.03386933  0.31292119]\n",
      "[ episode 252 ][ timestamp 26 ] state=[ 0.00960245 -0.21644117  0.03386933  0.31292119], action=1, reward=1.0, next_state=[ 0.00527362 -0.02181767  0.04012775  0.03110906]\n",
      "[ episode 252 ][ timestamp 27 ] state=[ 0.00527362 -0.02181767  0.04012775  0.03110906], action=1, reward=1.0, next_state=[ 0.00483727  0.17270654  0.04074993 -0.24864797]\n",
      "[ episode 252 ][ timestamp 28 ] state=[ 0.00483727  0.17270654  0.04074993 -0.24864797], action=0, reward=1.0, next_state=[ 0.0082914  -0.02297296  0.03577697  0.05660477]\n",
      "[ episode 252 ][ timestamp 29 ] state=[ 0.0082914  -0.02297296  0.03577697  0.05660477], action=1, reward=1.0, next_state=[ 0.00783194  0.17161823  0.03690907 -0.22457904]\n",
      "[ episode 252 ][ timestamp 30 ] state=[ 0.00783194  0.17161823  0.03690907 -0.22457904], action=0, reward=1.0, next_state=[ 0.01126431 -0.02401127  0.03241749  0.07951415]\n",
      "[ episode 252 ][ timestamp 31 ] state=[ 0.01126431 -0.02401127  0.03241749  0.07951415], action=1, reward=1.0, next_state=[ 0.01078408  0.17063133  0.03400777 -0.20276728]\n",
      "[ episode 252 ][ timestamp 32 ] state=[ 0.01078408  0.17063133  0.03400777 -0.20276728], action=0, reward=1.0, next_state=[ 0.01419671 -0.02496005  0.02995242  0.10044652]\n",
      "[ episode 252 ][ timestamp 33 ] state=[ 0.01419671 -0.02496005  0.02995242  0.10044652], action=1, reward=1.0, next_state=[ 0.01369751  0.16972011  0.03196135 -0.18263805]\n",
      "[ episode 252 ][ timestamp 34 ] state=[ 0.01369751  0.16972011  0.03196135 -0.18263805], action=0, reward=1.0, next_state=[ 0.01709191 -0.02584425  0.02830859  0.11995371]\n",
      "[ episode 252 ][ timestamp 35 ] state=[ 0.01709191 -0.02584425  0.02830859  0.11995371], action=1, reward=1.0, next_state=[ 0.01657502  0.16886092  0.03070767 -0.16366541]\n",
      "[ episode 252 ][ timestamp 36 ] state=[ 0.01657502  0.16886092  0.03070767 -0.16366541], action=0, reward=1.0, next_state=[ 0.01995224 -0.02668686  0.02743436  0.13854461]\n",
      "[ episode 252 ][ timestamp 37 ] state=[ 0.01995224 -0.02668686  0.02743436  0.13854461], action=1, reward=1.0, next_state=[ 0.0194185   0.16803163  0.03020525 -0.14535853]\n",
      "[ episode 252 ][ timestamp 38 ] state=[ 0.0194185   0.16803163  0.03020525 -0.14535853], action=0, reward=1.0, next_state=[ 0.02277914 -0.02750957  0.02729808  0.15669847]\n",
      "[ episode 252 ][ timestamp 39 ] state=[ 0.02277914 -0.02750957  0.02729808  0.15669847], action=1, reward=1.0, next_state=[ 0.02222895  0.16721112  0.03043205 -0.1272491 ]\n",
      "[ episode 252 ][ timestamp 40 ] state=[ 0.02222895  0.16721112  0.03043205 -0.1272491 ], action=0, reward=1.0, next_state=[ 0.02557317 -0.02833327  0.02788707  0.17487732]\n",
      "[ episode 252 ][ timestamp 41 ] state=[ 0.02557317 -0.02833327  0.02788707  0.17487732], action=1, reward=1.0, next_state=[ 0.0250065   0.1663787   0.03138461 -0.10887934]\n",
      "[ episode 252 ][ timestamp 42 ] state=[ 0.0250065   0.1663787   0.03138461 -0.10887934], action=1, reward=1.0, next_state=[ 0.02833408  0.36103718  0.02920703 -0.39149772]\n",
      "[ episode 252 ][ timestamp 43 ] state=[ 0.02833408  0.36103718  0.02920703 -0.39149772], action=0, reward=1.0, next_state=[ 0.03555482  0.16551316  0.02137707 -0.08975113]\n",
      "[ episode 252 ][ timestamp 44 ] state=[ 0.03555482  0.16551316  0.02137707 -0.08975113], action=0, reward=1.0, next_state=[ 0.03886508 -0.02990857  0.01958205  0.20959888]\n",
      "[ episode 252 ][ timestamp 45 ] state=[ 0.03886508 -0.02990857  0.01958205  0.20959888], action=1, reward=1.0, next_state=[ 0.03826691  0.16492799  0.02377403 -0.07684318]\n",
      "[ episode 252 ][ timestamp 46 ] state=[ 0.03826691  0.16492799  0.02377403 -0.07684318], action=0, reward=1.0, next_state=[ 0.04156547 -0.03052657  0.02223716  0.22324472]\n",
      "[ episode 252 ][ timestamp 47 ] state=[ 0.04156547 -0.03052657  0.02223716  0.22324472], action=1, reward=1.0, next_state=[ 0.04095494  0.16427061  0.02670206 -0.06234162]\n",
      "[ episode 252 ][ timestamp 48 ] state=[ 0.04095494  0.16427061  0.02670206 -0.06234162], action=1, reward=1.0, next_state=[ 0.04424035  0.35899974  0.02545523 -0.34648172]\n",
      "[ episode 252 ][ timestamp 49 ] state=[ 0.04424035  0.35899974  0.02545523 -0.34648172], action=0, reward=1.0, next_state=[ 0.05142035  0.16352513  0.01852559 -0.04588176]\n",
      "[ episode 252 ][ timestamp 50 ] state=[ 0.05142035  0.16352513  0.01852559 -0.04588176], action=0, reward=1.0, next_state=[ 0.05469085 -0.0318575   0.01760796  0.2525881 ]\n",
      "[ episode 252 ][ timestamp 51 ] state=[ 0.05469085 -0.0318575   0.01760796  0.2525881 ], action=1, reward=1.0, next_state=[ 0.0540537   0.16300865  0.02265972 -0.03448935]\n",
      "[ episode 252 ][ timestamp 52 ] state=[ 0.0540537   0.16300865  0.02265972 -0.03448935], action=0, reward=1.0, next_state=[ 0.05731387 -0.03243079  0.02196993  0.26525595]\n",
      "[ episode 252 ][ timestamp 53 ] state=[ 0.05731387 -0.03243079  0.02196993  0.26525595], action=1, reward=1.0, next_state=[ 0.05666526  0.16237081  0.02727505 -0.02041729]\n",
      "[ episode 252 ][ timestamp 54 ] state=[ 0.05666526  0.16237081  0.02727505 -0.02041729], action=0, reward=1.0, next_state=[ 0.05991267 -0.03313146  0.0268667   0.28074491]\n",
      "[ episode 252 ][ timestamp 55 ] state=[ 0.05991267 -0.03313146  0.0268667   0.28074491], action=1, reward=1.0, next_state=[ 0.05925004  0.16159715  0.0324816  -0.00334473]\n",
      "[ episode 252 ][ timestamp 56 ] state=[ 0.05925004  0.16159715  0.0324816  -0.00334473], action=0, reward=1.0, next_state=[ 0.06248199 -0.03397521  0.03241471  0.29940698]\n",
      "[ episode 252 ][ timestamp 57 ] state=[ 0.06248199 -0.03397521  0.03241471  0.29940698], action=1, reward=1.0, next_state=[0.06180248 0.16067006 0.03840285 0.0171207 ]\n",
      "[ episode 252 ][ timestamp 58 ] state=[0.06180248 0.16067006 0.03840285 0.0171207 ], action=0, reward=1.0, next_state=[ 0.06501588 -0.03498099  0.03874526  0.32166855]\n",
      "[ episode 252 ][ timestamp 59 ] state=[ 0.06501588 -0.03498099  0.03874526  0.32166855], action=1, reward=1.0, next_state=[0.06431626 0.15956841 0.04517863 0.04145173]\n",
      "[ episode 252 ][ timestamp 60 ] state=[0.06431626 0.15956841 0.04517863 0.04145173], action=0, reward=1.0, next_state=[ 0.06750763 -0.0361713   0.04600767  0.3480397 ]\n",
      "[ episode 252 ][ timestamp 61 ] state=[ 0.06750763 -0.0361713   0.04600767  0.3480397 ], action=1, reward=1.0, next_state=[0.06678421 0.15826711 0.05296846 0.0702122 ]\n",
      "[ episode 252 ][ timestamp 62 ] state=[0.06678421 0.15826711 0.05296846 0.0702122 ], action=0, reward=1.0, next_state=[ 0.06994955 -0.03757265  0.05437271  0.37912528]\n",
      "[ episode 252 ][ timestamp 63 ] state=[ 0.06994955 -0.03757265  0.05437271  0.37912528], action=1, reward=1.0, next_state=[0.0691981  0.15673668 0.06195521 0.10406972]\n",
      "[ episode 252 ][ timestamp 64 ] state=[0.0691981  0.15673668 0.06195521 0.10406972], action=1, reward=1.0, next_state=[ 0.07233283  0.35091856  0.06403661 -0.16844108]\n",
      "[ episode 252 ][ timestamp 65 ] state=[ 0.07233283  0.35091856  0.06403661 -0.16844108], action=0, reward=1.0, next_state=[0.0793512  0.15494124 0.06066778 0.14373628]\n",
      "[ episode 252 ][ timestamp 66 ] state=[0.0793512  0.15494124 0.06066778 0.14373628], action=1, reward=1.0, next_state=[ 0.08245003  0.34914424  0.06354251 -0.12920692]\n",
      "[ episode 252 ][ timestamp 67 ] state=[ 0.08245003  0.34914424  0.06354251 -0.12920692], action=0, reward=1.0, next_state=[0.08943291 0.1531723  0.06095837 0.18282667]\n",
      "[ episode 252 ][ timestamp 68 ] state=[0.08943291 0.1531723  0.06095837 0.18282667], action=1, reward=1.0, next_state=[ 0.09249636  0.34737144  0.0646149  -0.09002031]\n",
      "[ episode 252 ][ timestamp 69 ] state=[ 0.09249636  0.34737144  0.0646149  -0.09002031], action=0, reward=1.0, next_state=[0.09944378 0.15138569 0.0628145  0.22232839]\n",
      "[ episode 252 ][ timestamp 70 ] state=[0.09944378 0.15138569 0.0628145  0.22232839], action=1, reward=1.0, next_state=[ 0.1024715   0.3455562   0.06726107 -0.04989764]\n",
      "[ episode 252 ][ timestamp 71 ] state=[ 0.1024715   0.3455562   0.06726107 -0.04989764], action=0, reward=1.0, next_state=[0.10938262 0.14953748 0.06626311 0.26322543]\n",
      "[ episode 252 ][ timestamp 72 ] state=[0.10938262 0.14953748 0.06626311 0.26322543], action=1, reward=1.0, next_state=[ 0.11237337  0.34365411  0.07152762 -0.00784339]\n",
      "[ episode 252 ][ timestamp 73 ] state=[ 0.11237337  0.34365411  0.07152762 -0.00784339], action=1, reward=1.0, next_state=[ 0.11924645  0.53768124  0.07137075 -0.2771287 ]\n",
      "[ episode 252 ][ timestamp 74 ] state=[ 0.11924645  0.53768124  0.07137075 -0.2771287 ], action=1, reward=1.0, next_state=[ 0.13000008  0.73171624  0.06582818 -0.54647505]\n",
      "[ episode 252 ][ timestamp 75 ] state=[ 0.13000008  0.73171624  0.06582818 -0.54647505], action=0, reward=1.0, next_state=[ 0.1446344   0.53573416  0.05489868 -0.23379913]\n",
      "[ episode 252 ][ timestamp 76 ] state=[ 0.1446344   0.53573416  0.05489868 -0.23379913], action=0, reward=1.0, next_state=[0.15534909 0.33987254 0.0502227  0.07568279]\n",
      "[ episode 252 ][ timestamp 77 ] state=[0.15534909 0.33987254 0.0502227  0.07568279], action=1, reward=1.0, next_state=[ 0.16214654  0.53423987  0.05173635 -0.20074133]\n",
      "[ episode 252 ][ timestamp 78 ] state=[ 0.16214654  0.53423987  0.05173635 -0.20074133], action=0, reward=1.0, next_state=[0.17283134 0.33841762 0.04772153 0.10780273]\n",
      "[ episode 252 ][ timestamp 79 ] state=[0.17283134 0.33841762 0.04772153 0.10780273], action=0, reward=1.0, next_state=[0.17959969 0.14264545 0.04987758 0.41515148]\n",
      "[ episode 252 ][ timestamp 80 ] state=[0.17959969 0.14264545 0.04987758 0.41515148], action=1, reward=1.0, next_state=[0.1824526  0.3370263  0.05818061 0.13860074]\n",
      "[ episode 252 ][ timestamp 81 ] state=[0.1824526  0.3370263  0.05818061 0.13860074], action=1, reward=1.0, next_state=[ 0.18919312  0.53126877  0.06095263 -0.13517453]\n",
      "[ episode 252 ][ timestamp 82 ] state=[ 0.18919312  0.53126877  0.06095263 -0.13517453], action=1, reward=1.0, next_state=[ 0.1998185   0.72546709  0.05824913 -0.40802208]\n",
      "[ episode 252 ][ timestamp 83 ] state=[ 0.1998185   0.72546709  0.05824913 -0.40802208], action=0, reward=1.0, next_state=[ 0.21432784  0.52956969  0.05008869 -0.09755878]\n",
      "[ episode 252 ][ timestamp 84 ] state=[ 0.21432784  0.52956969  0.05008869 -0.09755878], action=0, reward=1.0, next_state=[0.22491923 0.33376697 0.04813752 0.21049686]\n",
      "[ episode 252 ][ timestamp 85 ] state=[0.22491923 0.33376697 0.04813752 0.21049686], action=1, reward=1.0, next_state=[ 0.23159457  0.52816877  0.05234745 -0.06662109]\n",
      "[ episode 252 ][ timestamp 86 ] state=[ 0.23159457  0.52816877  0.05234745 -0.06662109], action=0, reward=1.0, next_state=[0.24215795 0.33233691 0.05101503 0.24210745]\n",
      "[ episode 252 ][ timestamp 87 ] state=[0.24215795 0.33233691 0.05101503 0.24210745], action=1, reward=1.0, next_state=[ 0.24880469  0.52669444  0.05585718 -0.03405764]\n",
      "[ episode 252 ][ timestamp 88 ] state=[ 0.24880469  0.52669444  0.05585718 -0.03405764], action=0, reward=1.0, next_state=[0.25933858 0.33081781 0.05517603 0.27571254]\n",
      "[ episode 252 ][ timestamp 89 ] state=[0.25933858 0.33081781 0.05517603 0.27571254], action=1, reward=1.0, next_state=[0.26595493 0.52511091 0.06069028 0.00092992]\n",
      "[ episode 252 ][ timestamp 90 ] state=[0.26595493 0.52511091 0.06069028 0.00092992], action=1, reward=1.0, next_state=[ 0.27645715  0.71931234  0.06070888 -0.27200391]\n",
      "[ episode 252 ][ timestamp 91 ] state=[ 0.27645715  0.71931234  0.06070888 -0.27200391], action=0, reward=1.0, next_state=[0.2908434  0.52337902 0.0552688  0.03919208]\n",
      "[ episode 252 ][ timestamp 92 ] state=[0.2908434  0.52337902 0.0552688  0.03919208], action=0, reward=1.0, next_state=[0.30131098 0.32750987 0.05605264 0.34878795]\n",
      "[ episode 252 ][ timestamp 93 ] state=[0.30131098 0.32750987 0.05605264 0.34878795], action=1, reward=1.0, next_state=[0.30786117 0.52179166 0.0630284  0.07429381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 252 ][ timestamp 94 ] state=[0.30786117 0.52179166 0.0630284  0.07429381], action=1, reward=1.0, next_state=[ 0.31829701  0.71595605  0.06451428 -0.1978564 ]\n",
      "[ episode 252 ][ timestamp 95 ] state=[ 0.31829701  0.71595605  0.06451428 -0.1978564 ], action=0, reward=1.0, next_state=[0.33261613 0.5199735  0.06055715 0.11445991]\n",
      "[ episode 252 ][ timestamp 96 ] state=[0.33261613 0.5199735  0.06055715 0.11445991], action=1, reward=1.0, next_state=[ 0.3430156   0.71417782  0.06284635 -0.15851968]\n",
      "[ episode 252 ][ timestamp 97 ] state=[ 0.3430156   0.71417782  0.06284635 -0.15851968], action=0, reward=1.0, next_state=[0.35729916 0.51821503 0.05967595 0.15330887]\n",
      "[ episode 252 ][ timestamp 98 ] state=[0.35729916 0.51821503 0.05967595 0.15330887], action=1, reward=1.0, next_state=[ 0.36766346  0.71243399  0.06274213 -0.11996666]\n",
      "[ episode 252 ][ timestamp 99 ] state=[ 0.36766346  0.71243399  0.06274213 -0.11996666], action=0, reward=1.0, next_state=[0.38191214 0.51647184 0.0603428  0.19183228]\n",
      "[ episode 252 ][ timestamp 100 ] state=[0.38191214 0.51647184 0.0603428  0.19183228], action=1, reward=1.0, next_state=[ 0.39224157  0.71068097  0.06417944 -0.08122118]\n",
      "[ episode 252 ][ timestamp 101 ] state=[ 0.39224157  0.71068097  0.06417944 -0.08122118], action=0, reward=1.0, next_state=[0.40645519 0.51470052 0.06255502 0.23100008]\n",
      "[ episode 252 ][ timestamp 102 ] state=[0.40645519 0.51470052 0.06255502 0.23100008], action=1, reward=1.0, next_state=[ 0.4167492   0.70887541  0.06717502 -0.04131338]\n",
      "[ episode 252 ][ timestamp 103 ] state=[ 0.4167492   0.70887541  0.06717502 -0.04131338], action=1, reward=1.0, next_state=[ 0.43092671  0.90297298  0.06634875 -0.31206849]\n",
      "[ episode 252 ][ timestamp 104 ] state=[ 0.43092671  0.90297298  0.06634875 -0.31206849], action=0, reward=1.0, next_state=[0.44898617 0.70697163 0.06010738 0.00077889]\n",
      "[ episode 252 ][ timestamp 105 ] state=[0.44898617 0.70697163 0.06010738 0.00077889], action=0, reward=1.0, next_state=[0.4631256  0.51104146 0.06012296 0.31180433]\n",
      "[ episode 252 ][ timestamp 106 ] state=[0.4631256  0.51104146 0.06012296 0.31180433], action=1, reward=1.0, next_state=[0.47334643 0.70525762 0.06635905 0.03867196]\n",
      "[ episode 252 ][ timestamp 107 ] state=[0.47334643 0.70525762 0.06635905 0.03867196], action=1, reward=1.0, next_state=[ 0.48745158  0.89936832  0.06713249 -0.23235801]\n",
      "[ episode 252 ][ timestamp 108 ] state=[ 0.48745158  0.89936832  0.06713249 -0.23235801], action=0, reward=1.0, next_state=[0.50543895 0.70335456 0.06248533 0.08072248]\n",
      "[ episode 252 ][ timestamp 109 ] state=[0.50543895 0.70335456 0.06248533 0.08072248], action=0, reward=1.0, next_state=[0.51950604 0.50739512 0.06409978 0.39244673]\n",
      "[ episode 252 ][ timestamp 110 ] state=[0.51950604 0.50739512 0.06409978 0.39244673], action=0, reward=1.0, next_state=[0.52965394 0.31142486 0.07194871 0.70463086]\n",
      "[ episode 252 ][ timestamp 111 ] state=[0.52965394 0.31142486 0.07194871 0.70463086], action=1, reward=1.0, next_state=[0.53588244 0.50547996 0.08604133 0.43543597]\n",
      "[ episode 252 ][ timestamp 112 ] state=[0.53588244 0.50547996 0.08604133 0.43543597], action=1, reward=1.0, next_state=[0.54599204 0.69928521 0.09475005 0.17106846]\n",
      "[ episode 252 ][ timestamp 113 ] state=[0.54599204 0.69928521 0.09475005 0.17106846], action=1, reward=1.0, next_state=[ 0.55997774  0.8929323   0.09817142 -0.09028444]\n",
      "[ episode 252 ][ timestamp 114 ] state=[ 0.55997774  0.8929323   0.09817142 -0.09028444], action=0, reward=1.0, next_state=[0.57783639 0.69655028 0.09636573 0.2316863 ]\n",
      "[ episode 252 ][ timestamp 115 ] state=[0.57783639 0.69655028 0.09636573 0.2316863 ], action=0, reward=1.0, next_state=[0.5917674  0.50019289 0.10099945 0.55314355]\n",
      "[ episode 252 ][ timestamp 116 ] state=[0.5917674  0.50019289 0.10099945 0.55314355], action=1, reward=1.0, next_state=[0.60177125 0.6937623  0.11206233 0.29391249]\n",
      "[ episode 252 ][ timestamp 117 ] state=[0.60177125 0.6937623  0.11206233 0.29391249], action=1, reward=1.0, next_state=[0.6156465  0.88712302 0.11794058 0.03856807]\n",
      "[ episode 252 ][ timestamp 118 ] state=[0.6156465  0.88712302 0.11794058 0.03856807], action=0, reward=1.0, next_state=[0.63338896 0.69052462 0.11871194 0.36601125]\n",
      "[ episode 252 ][ timestamp 119 ] state=[0.63338896 0.69052462 0.11871194 0.36601125], action=0, reward=1.0, next_state=[0.64719945 0.49393336 0.12603216 0.69364211]\n",
      "[ episode 252 ][ timestamp 120 ] state=[0.64719945 0.49393336 0.12603216 0.69364211], action=1, reward=1.0, next_state=[0.65707812 0.68710258 0.139905   0.44314191]\n",
      "[ episode 252 ][ timestamp 121 ] state=[0.65707812 0.68710258 0.139905   0.44314191], action=1, reward=1.0, next_state=[0.67082017 0.87999642 0.14876784 0.19762625]\n",
      "[ episode 252 ][ timestamp 122 ] state=[0.67082017 0.87999642 0.14876784 0.19762625], action=1, reward=1.0, next_state=[ 0.6884201   1.07271202  0.15272037 -0.04467759]\n",
      "[ episode 252 ][ timestamp 123 ] state=[ 0.6884201   1.07271202  0.15272037 -0.04467759], action=0, reward=1.0, next_state=[0.70987434 0.87576753 0.15182682 0.29202622]\n",
      "[ episode 252 ][ timestamp 124 ] state=[0.70987434 0.87576753 0.15182682 0.29202622], action=1, reward=1.0, next_state=[0.72738969 1.06843547 0.15766734 0.05081465]\n",
      "[ episode 252 ][ timestamp 125 ] state=[0.72738969 1.06843547 0.15766734 0.05081465], action=1, reward=1.0, next_state=[ 0.7487584   1.26098658  0.15868363 -0.18826709]\n",
      "[ episode 252 ][ timestamp 126 ] state=[ 0.7487584   1.26098658  0.15868363 -0.18826709], action=0, reward=1.0, next_state=[0.77397813 1.06399227 0.15491829 0.14996931]\n",
      "[ episode 252 ][ timestamp 127 ] state=[0.77397813 1.06399227 0.15491829 0.14996931], action=1, reward=1.0, next_state=[ 0.79525798  1.2565957   0.15791768 -0.09011194]\n",
      "[ episode 252 ][ timestamp 128 ] state=[ 0.79525798  1.2565957   0.15791768 -0.09011194], action=0, reward=1.0, next_state=[0.82038989 1.05960417 0.15611544 0.24793365]\n",
      "[ episode 252 ][ timestamp 129 ] state=[0.82038989 1.05960417 0.15611544 0.24793365], action=1, reward=1.0, next_state=[0.84158198 1.25219206 0.16107411 0.00827672]\n",
      "[ episode 252 ][ timestamp 130 ] state=[0.84158198 1.25219206 0.16107411 0.00827672], action=0, reward=1.0, next_state=[0.86662582 1.05517063 0.16123965 0.34713465]\n",
      "[ episode 252 ][ timestamp 131 ] state=[0.86662582 1.05517063 0.16123965 0.34713465], action=0, reward=1.0, next_state=[0.88772923 0.85816654 0.16818234 0.68600711]\n",
      "[ episode 252 ][ timestamp 132 ] state=[0.88772923 0.85816654 0.16818234 0.68600711], action=1, reward=1.0, next_state=[0.90489256 1.05060405 0.18190248 0.45063643]\n",
      "[ episode 252 ][ timestamp 133 ] state=[0.90489256 1.05060405 0.18190248 0.45063643], action=1, reward=1.0, next_state=[0.92590464 1.24275016 0.19091521 0.22035739]\n",
      "[ episode 252 ][ timestamp 134 ] state=[0.92590464 1.24275016 0.19091521 0.22035739], action=1, reward=1.0, next_state=[ 0.95075964  1.4347034   0.19532236 -0.00655234]\n",
      "[ episode 252 ][ timestamp 135 ] state=[ 0.95075964  1.4347034   0.19532236 -0.00655234], action=0, reward=1.0, next_state=[0.97945371 1.23739478 0.19519131 0.34084324]\n",
      "[ episode 252 ][ timestamp 136 ] state=[0.97945371 1.23739478 0.19519131 0.34084324], action=1, reward=1.0, next_state=[1.00420161 1.42928176 0.20200818 0.11550103]\n",
      "[ episode 252 ][ timestamp 137 ] state=[1.00420161 1.42928176 0.20200818 0.11550103], action=0, reward=1.0, next_state=[1.03278724 1.23192482 0.2043182  0.46450405]\n",
      "[ episode 252 ][ timestamp 138 ] state=[1.03278724 1.23192482 0.2043182  0.46450405], action=1, reward=-1.0, next_state=[1.05742574 1.42366256 0.21360828 0.24253227]\n",
      "[ Ended! ] Episode 252: Exploration_rate=0.28417984116121187. Score=138.\n",
      "[ Experience replay ] starts\n",
      "[ episode 253 ] state=[-0.00242921  0.01143942  0.00143047  0.02567206]\n",
      "[ episode 253 ][ timestamp 1 ] state=[-0.00242921  0.01143942  0.00143047  0.02567206], action=1, reward=1.0, next_state=[-0.00220042  0.20654083  0.00194391 -0.2665592 ]\n",
      "[ episode 253 ][ timestamp 2 ] state=[-0.00220042  0.20654083  0.00194391 -0.2665592 ], action=1, reward=1.0, next_state=[ 0.00193039  0.40163498 -0.00338727 -0.55862837]\n",
      "[ episode 253 ][ timestamp 3 ] state=[ 0.00193039  0.40163498 -0.00338727 -0.55862837], action=1, reward=1.0, next_state=[ 0.00996309  0.59680432 -0.01455984 -0.85237654]\n",
      "[ episode 253 ][ timestamp 4 ] state=[ 0.00996309  0.59680432 -0.01455984 -0.85237654], action=1, reward=1.0, next_state=[ 0.02189918  0.7921217  -0.03160737 -1.149602  ]\n",
      "[ episode 253 ][ timestamp 5 ] state=[ 0.02189918  0.7921217  -0.03160737 -1.149602  ], action=1, reward=1.0, next_state=[ 0.03774161  0.98764161 -0.05459941 -1.4520264 ]\n",
      "[ episode 253 ][ timestamp 6 ] state=[ 0.03774161  0.98764161 -0.05459941 -1.4520264 ], action=0, reward=1.0, next_state=[ 0.05749445  0.79323125 -0.08363994 -1.17688968]\n",
      "[ episode 253 ][ timestamp 7 ] state=[ 0.05749445  0.79323125 -0.08363994 -1.17688968], action=0, reward=1.0, next_state=[ 0.07335907  0.59928938 -0.10717773 -0.91155533]\n",
      "[ episode 253 ][ timestamp 8 ] state=[ 0.07335907  0.59928938 -0.10717773 -0.91155533], action=0, reward=1.0, next_state=[ 0.08534486  0.40576809 -0.12540884 -0.654389  ]\n",
      "[ episode 253 ][ timestamp 9 ] state=[ 0.08534486  0.40576809 -0.12540884 -0.654389  ], action=0, reward=1.0, next_state=[ 0.09346022  0.21259477 -0.13849662 -0.40367824]\n",
      "[ episode 253 ][ timestamp 10 ] state=[ 0.09346022  0.21259477 -0.13849662 -0.40367824], action=1, reward=1.0, next_state=[ 0.09771212  0.40938128 -0.14657018 -0.73661953]\n",
      "[ episode 253 ][ timestamp 11 ] state=[ 0.09771212  0.40938128 -0.14657018 -0.73661953], action=1, reward=1.0, next_state=[ 0.10589974  0.60619082 -0.16130257 -1.07160599]\n",
      "[ episode 253 ][ timestamp 12 ] state=[ 0.10589974  0.60619082 -0.16130257 -1.07160599], action=0, reward=1.0, next_state=[ 0.11802356  0.41352596 -0.18273469 -0.83357778]\n",
      "[ episode 253 ][ timestamp 13 ] state=[ 0.11802356  0.41352596 -0.18273469 -0.83357778], action=0, reward=1.0, next_state=[ 0.12629408  0.221308   -0.19940625 -0.60347686]\n",
      "[ episode 253 ][ timestamp 14 ] state=[ 0.12629408  0.221308   -0.19940625 -0.60347686], action=0, reward=-1.0, next_state=[ 0.13072024  0.02945108 -0.21147579 -0.37963182]\n",
      "[ Ended! ] Episode 253: Exploration_rate=0.2827589419554058. Score=14.\n",
      "[ Experience replay ] starts\n",
      "[ episode 254 ] state=[0.02376596 0.02253119 0.03985849 0.02595064]\n",
      "[ episode 254 ][ timestamp 1 ] state=[0.02376596 0.02253119 0.03985849 0.02595064], action=0, reward=1.0, next_state=[ 0.02421659 -0.17313903  0.0403775   0.33093814]\n",
      "[ episode 254 ][ timestamp 2 ] state=[ 0.02421659 -0.17313903  0.0403775   0.33093814], action=0, reward=1.0, next_state=[ 0.02075381 -0.36881178  0.04699627  0.6360758 ]\n",
      "[ episode 254 ][ timestamp 3 ] state=[ 0.02075381 -0.36881178  0.04699627  0.6360758 ], action=1, reward=1.0, next_state=[ 0.01337757 -0.17437569  0.05971778  0.3585555 ]\n",
      "[ episode 254 ][ timestamp 4 ] state=[ 0.01337757 -0.17437569  0.05971778  0.3585555 ], action=1, reward=1.0, next_state=[0.00989006 0.01984872 0.06688889 0.08528481]\n",
      "[ episode 254 ][ timestamp 5 ] state=[0.00989006 0.01984872 0.06688889 0.08528481], action=1, reward=1.0, next_state=[ 0.01028703  0.21395127  0.06859459 -0.18556725]\n",
      "[ episode 254 ][ timestamp 6 ] state=[ 0.01028703  0.21395127  0.06859459 -0.18556725], action=0, reward=1.0, next_state=[0.01456606 0.01791835 0.06488324 0.12794162]\n",
      "[ episode 254 ][ timestamp 7 ] state=[0.01456606 0.01791835 0.06488324 0.12794162], action=0, reward=1.0, next_state=[ 0.01492442 -0.17807019  0.06744208  0.44036813]\n",
      "[ episode 254 ][ timestamp 8 ] state=[ 0.01492442 -0.17807019  0.06744208  0.44036813], action=1, reward=1.0, next_state=[0.01136302 0.01603572 0.07624944 0.16968412]\n",
      "[ episode 254 ][ timestamp 9 ] state=[0.01136302 0.01603572 0.07624944 0.16968412], action=0, reward=1.0, next_state=[ 0.01168373 -0.18009012  0.07964312  0.4854137 ]\n",
      "[ episode 254 ][ timestamp 10 ] state=[ 0.01168373 -0.18009012  0.07964312  0.4854137 ], action=1, reward=1.0, next_state=[0.00808193 0.01382294 0.0893514  0.21885646]\n",
      "[ episode 254 ][ timestamp 11 ] state=[0.00808193 0.01382294 0.0893514  0.21885646], action=1, reward=1.0, next_state=[ 0.00835839  0.20756156  0.09372852 -0.04435782]\n",
      "[ episode 254 ][ timestamp 12 ] state=[ 0.00835839  0.20756156  0.09372852 -0.04435782], action=0, reward=1.0, next_state=[0.01250962 0.01122928 0.09284137 0.27636381]\n",
      "[ episode 254 ][ timestamp 13 ] state=[0.01250962 0.01122928 0.09284137 0.27636381], action=0, reward=1.0, next_state=[ 0.01273421 -0.18508614  0.09836864  0.59682492]\n",
      "[ episode 254 ][ timestamp 14 ] state=[ 0.01273421 -0.18508614  0.09836864  0.59682492], action=1, reward=1.0, next_state=[0.00903248 0.00853162 0.11030514 0.33667604]\n",
      "[ episode 254 ][ timestamp 15 ] state=[0.00903248 0.00853162 0.11030514 0.33667604], action=0, reward=1.0, next_state=[ 0.00920312 -0.18797305  0.11703866  0.66200568]\n",
      "[ episode 254 ][ timestamp 16 ] state=[ 0.00920312 -0.18797305  0.11703866  0.66200568], action=1, reward=1.0, next_state=[0.00544366 0.00534273 0.13027878 0.40834664]\n",
      "[ episode 254 ][ timestamp 17 ] state=[0.00544366 0.00534273 0.13027878 0.40834664], action=1, reward=1.0, next_state=[0.00555051 0.19840009 0.13844571 0.15940833]\n",
      "[ episode 254 ][ timestamp 18 ] state=[0.00555051 0.19840009 0.13844571 0.15940833], action=0, reward=1.0, next_state=[0.00951851 0.00159558 0.14163388 0.49236361]\n",
      "[ episode 254 ][ timestamp 19 ] state=[0.00951851 0.00159558 0.14163388 0.49236361], action=1, reward=1.0, next_state=[0.00955042 0.19446552 0.15148115 0.24745688]\n",
      "[ episode 254 ][ timestamp 20 ] state=[0.00955042 0.19446552 0.15148115 0.24745688], action=1, reward=1.0, next_state=[0.01343973 0.38713608 0.15643029 0.00612588]\n",
      "[ episode 254 ][ timestamp 21 ] state=[0.01343973 0.38713608 0.15643029 0.00612588], action=0, reward=1.0, next_state=[0.02118246 0.19015694 0.1565528  0.34378999]\n",
      "[ episode 254 ][ timestamp 22 ] state=[0.02118246 0.19015694 0.1565528  0.34378999], action=1, reward=1.0, next_state=[0.02498559 0.38274562 0.1634286  0.10427858]\n",
      "[ episode 254 ][ timestamp 23 ] state=[0.02498559 0.38274562 0.1634286  0.10427858], action=1, reward=1.0, next_state=[ 0.03264051  0.57519423  0.16551418 -0.13271343]\n",
      "[ episode 254 ][ timestamp 24 ] state=[ 0.03264051  0.57519423  0.16551418 -0.13271343], action=0, reward=1.0, next_state=[0.04414439 0.37813592 0.16285991 0.20727376]\n",
      "[ episode 254 ][ timestamp 25 ] state=[0.04414439 0.37813592 0.16285991 0.20727376], action=1, reward=1.0, next_state=[ 0.05170711  0.57059987  0.16700538 -0.02993259]\n",
      "[ episode 254 ][ timestamp 26 ] state=[ 0.05170711  0.57059987  0.16700538 -0.02993259], action=0, reward=1.0, next_state=[0.06311911 0.37352571 0.16640673 0.31043746]\n",
      "[ episode 254 ][ timestamp 27 ] state=[0.06311911 0.37352571 0.16640673 0.31043746], action=1, reward=1.0, next_state=[0.07058962 0.56593427 0.17261548 0.07450954]\n",
      "[ episode 254 ][ timestamp 28 ] state=[0.07058962 0.56593427 0.17261548 0.07450954], action=1, reward=1.0, next_state=[ 0.08190831  0.75821541  0.17410567 -0.1591286 ]\n",
      "[ episode 254 ][ timestamp 29 ] state=[ 0.08190831  0.75821541  0.17410567 -0.1591286 ], action=1, reward=1.0, next_state=[ 0.09707262  0.95047272  0.1709231  -0.39222586]\n",
      "[ episode 254 ][ timestamp 30 ] state=[ 0.09707262  0.95047272  0.1709231  -0.39222586], action=0, reward=1.0, next_state=[ 0.11608207  0.75338969  0.16307858 -0.05090203]\n",
      "[ episode 254 ][ timestamp 31 ] state=[ 0.11608207  0.75338969  0.16307858 -0.05090203], action=0, reward=1.0, next_state=[0.13114986 0.55635065 0.16206054 0.28846797]\n",
      "[ episode 254 ][ timestamp 32 ] state=[0.13114986 0.55635065 0.16206054 0.28846797], action=0, reward=1.0, next_state=[0.14227688 0.3593334  0.1678299  0.62755905]\n",
      "[ episode 254 ][ timestamp 33 ] state=[0.14227688 0.3593334  0.1678299  0.62755905], action=1, reward=1.0, next_state=[0.14946354 0.55176466 0.18038108 0.39207845]\n",
      "[ episode 254 ][ timestamp 34 ] state=[0.14946354 0.55176466 0.18038108 0.39207845], action=1, reward=1.0, next_state=[0.16049884 0.74392978 0.18822265 0.16125239]\n",
      "[ episode 254 ][ timestamp 35 ] state=[0.16049884 0.74392978 0.18822265 0.16125239], action=1, reward=1.0, next_state=[ 0.17537743  0.93592853  0.1914477  -0.06664792]\n",
      "[ episode 254 ][ timestamp 36 ] state=[ 0.17537743  0.93592853  0.1914477  -0.06664792], action=1, reward=1.0, next_state=[ 0.194096    1.12786378  0.19011474 -0.29334833]\n",
      "[ episode 254 ][ timestamp 37 ] state=[ 0.194096    1.12786378  0.19011474 -0.29334833], action=0, reward=1.0, next_state=[0.21665328 0.9306118  0.18424777 0.05275631]\n",
      "[ episode 254 ][ timestamp 38 ] state=[0.21665328 0.9306118  0.18424777 0.05275631], action=1, reward=1.0, next_state=[ 0.23526552  1.12267955  0.1853029  -0.17660612]\n",
      "[ episode 254 ][ timestamp 39 ] state=[ 0.23526552  1.12267955  0.1853029  -0.17660612], action=1, reward=1.0, next_state=[ 0.25771911  1.31473298  0.18177078 -0.40558666]\n",
      "[ episode 254 ][ timestamp 40 ] state=[ 0.25771911  1.31473298  0.18177078 -0.40558666], action=0, reward=1.0, next_state=[ 0.28401377  1.11756168  0.17365904 -0.06155546]\n",
      "[ episode 254 ][ timestamp 41 ] state=[ 0.28401377  1.11756168  0.17365904 -0.06155546], action=1, reward=1.0, next_state=[ 0.306365    1.30982351  0.17242793 -0.294811  ]\n",
      "[ episode 254 ][ timestamp 42 ] state=[ 0.306365    1.30982351  0.17242793 -0.294811  ], action=0, reward=1.0, next_state=[0.33256147 1.11271653 0.16653171 0.04690814]\n",
      "[ episode 254 ][ timestamp 43 ] state=[0.33256147 1.11271653 0.16653171 0.04690814], action=1, reward=1.0, next_state=[ 0.3548158   1.30510763  0.16746988 -0.18895175]\n",
      "[ episode 254 ][ timestamp 44 ] state=[ 0.3548158   1.30510763  0.16746988 -0.18895175], action=1, reward=1.0, next_state=[ 0.38091795  1.49748717  0.16369084 -0.42447755]\n",
      "[ episode 254 ][ timestamp 45 ] state=[ 0.38091795  1.49748717  0.16369084 -0.42447755], action=1, reward=1.0, next_state=[ 0.4108677   1.68995791  0.15520129 -0.66141393]\n",
      "[ episode 254 ][ timestamp 46 ] state=[ 0.4108677   1.68995791  0.15520129 -0.66141393], action=0, reward=1.0, next_state=[ 0.44466686  1.49305595  0.14197301 -0.32416478]\n",
      "[ episode 254 ][ timestamp 47 ] state=[ 0.44466686  1.49305595  0.14197301 -0.32416478], action=0, reward=1.0, next_state=[0.47452797 1.296228   0.13548972 0.00970662]\n",
      "[ episode 254 ][ timestamp 48 ] state=[0.47452797 1.296228   0.13548972 0.00970662], action=1, reward=1.0, next_state=[ 0.50045253  1.48917295  0.13568385 -0.23734616]\n",
      "[ episode 254 ][ timestamp 49 ] state=[ 0.50045253  1.48917295  0.13568385 -0.23734616], action=1, reward=1.0, next_state=[ 0.53023599  1.6821219   0.13093693 -0.48434075]\n",
      "[ episode 254 ][ timestamp 50 ] state=[ 0.53023599  1.6821219   0.13093693 -0.48434075], action=0, reward=1.0, next_state=[ 0.56387843  1.48541875  0.12125011 -0.15342613]\n",
      "[ episode 254 ][ timestamp 51 ] state=[ 0.56387843  1.48541875  0.12125011 -0.15342613], action=1, reward=1.0, next_state=[ 0.59358681  1.67861476  0.11818159 -0.40553229]\n",
      "[ episode 254 ][ timestamp 52 ] state=[ 0.59358681  1.67861476  0.11818159 -0.40553229], action=0, reward=1.0, next_state=[ 0.6271591   1.48203253  0.11007094 -0.07805122]\n",
      "[ episode 254 ][ timestamp 53 ] state=[ 0.6271591   1.48203253  0.11007094 -0.07805122], action=0, reward=1.0, next_state=[0.65679975 1.28551888 0.10850992 0.24723095]\n",
      "[ episode 254 ][ timestamp 54 ] state=[0.65679975 1.28551888 0.10850992 0.24723095], action=1, reward=1.0, next_state=[ 0.68251013  1.4789373   0.11345454 -0.00935097]\n",
      "[ episode 254 ][ timestamp 55 ] state=[ 0.68251013  1.4789373   0.11345454 -0.00935097], action=1, reward=1.0, next_state=[ 0.71208888  1.67226478  0.11326752 -0.2641937 ]\n",
      "[ episode 254 ][ timestamp 56 ] state=[ 0.71208888  1.67226478  0.11326752 -0.2641937 ], action=1, reward=1.0, next_state=[ 0.74553417  1.86560314  0.10798364 -0.51911341]\n",
      "[ episode 254 ][ timestamp 57 ] state=[ 0.74553417  1.86560314  0.10798364 -0.51911341], action=0, reward=1.0, next_state=[ 0.78284623  1.66913987  0.09760138 -0.19444943]\n",
      "[ episode 254 ][ timestamp 58 ] state=[ 0.78284623  1.66913987  0.09760138 -0.19444943], action=0, reward=1.0, next_state=[0.81622903 1.47276708 0.09371239 0.12735714]\n",
      "[ episode 254 ][ timestamp 59 ] state=[0.81622903 1.47276708 0.09371239 0.12735714], action=1, reward=1.0, next_state=[ 0.84568437  1.66643031  0.09625953 -0.13435194]\n",
      "[ episode 254 ][ timestamp 60 ] state=[ 0.84568437  1.66643031  0.09625953 -0.13435194], action=1, reward=1.0, next_state=[ 0.87901298  1.86005109  0.09357249 -0.39518199]\n",
      "[ episode 254 ][ timestamp 61 ] state=[ 0.87901298  1.86005109  0.09357249 -0.39518199], action=0, reward=1.0, next_state=[ 0.916214    1.66373464  0.08566885 -0.07452537]\n",
      "[ episode 254 ][ timestamp 62 ] state=[ 0.916214    1.66373464  0.08566885 -0.07452537], action=1, reward=1.0, next_state=[ 0.94948869  1.85753062  0.08417834 -0.33899742]\n",
      "[ episode 254 ][ timestamp 63 ] state=[ 0.94948869  1.85753062  0.08417834 -0.33899742], action=0, reward=1.0, next_state=[ 0.98663931  1.66131805  0.0773984  -0.0210015 ]\n",
      "[ episode 254 ][ timestamp 64 ] state=[ 0.98663931  1.66131805  0.0773984  -0.0210015 ], action=1, reward=1.0, next_state=[ 1.01986567  1.85524962  0.07697837 -0.28829556]\n",
      "[ episode 254 ][ timestamp 65 ] state=[ 1.01986567  1.85524962  0.07697837 -0.28829556], action=0, reward=1.0, next_state=[1.05697066 1.65911915 0.07121245 0.02763822]\n",
      "[ episode 254 ][ timestamp 66 ] state=[1.05697066 1.65911915 0.07121245 0.02763822], action=1, reward=1.0, next_state=[ 1.09015304  1.85315143  0.07176522 -0.24175377]\n",
      "[ episode 254 ][ timestamp 67 ] state=[ 1.09015304  1.85315143  0.07176522 -0.24175377], action=0, reward=1.0, next_state=[1.12721607 1.65708164 0.06693014 0.07267475]\n",
      "[ episode 254 ][ timestamp 68 ] state=[1.12721607 1.65708164 0.06693014 0.07267475], action=1, reward=1.0, next_state=[ 1.1603577   1.8511834   0.06838364 -0.19816322]\n",
      "[ episode 254 ][ timestamp 69 ] state=[ 1.1603577   1.8511834   0.06838364 -0.19816322], action=0, reward=1.0, next_state=[1.19738137 1.65515337 0.06442037 0.11528369]\n",
      "[ episode 254 ][ timestamp 70 ] state=[1.19738137 1.65515337 0.06442037 0.11528369], action=1, reward=1.0, next_state=[ 1.23048444  1.84929596  0.06672605 -0.15639964]\n",
      "[ episode 254 ][ timestamp 71 ] state=[ 1.23048444  1.84929596  0.06672605 -0.15639964], action=0, reward=1.0, next_state=[1.26747036 1.65328525 0.06359805 0.15656504]\n",
      "[ episode 254 ][ timestamp 72 ] state=[1.26747036 1.65328525 0.06359805 0.15656504], action=1, reward=1.0, next_state=[ 1.30053606  1.8474417   0.06672936 -0.11539564]\n",
      "[ episode 254 ][ timestamp 73 ] state=[ 1.30053606  1.8474417   0.06672936 -0.11539564], action=0, reward=1.0, next_state=[1.3374849  1.65143023 0.06442144 0.19757109]\n",
      "[ episode 254 ][ timestamp 74 ] state=[1.3374849  1.65143023 0.06442144 0.19757109], action=1, reward=1.0, next_state=[ 1.3705135   1.84557441  0.06837286 -0.0741143 ]\n",
      "[ episode 254 ][ timestamp 75 ] state=[ 1.3705135   1.84557441  0.06837286 -0.0741143 ], action=0, reward=1.0, next_state=[1.40742499 1.64954227 0.06689058 0.23933284]\n",
      "[ episode 254 ][ timestamp 76 ] state=[1.40742499 1.64954227 0.06689058 0.23933284], action=1, reward=1.0, next_state=[ 1.44041584  1.84364805  0.07167724 -0.03152354]\n",
      "[ episode 254 ][ timestamp 77 ] state=[ 1.44041584  1.84364805  0.07167724 -0.03152354], action=1, reward=1.0, next_state=[ 1.4772888   2.03767282  0.07104676 -0.30075832]\n",
      "[ episode 254 ][ timestamp 78 ] state=[ 1.4772888   2.03767282  0.07104676 -0.30075832], action=0, reward=1.0, next_state=[1.51804225 1.84161393 0.0650316  0.01345827]\n",
      "[ episode 254 ][ timestamp 79 ] state=[1.51804225 1.84161393 0.0650316  0.01345827], action=1, reward=1.0, next_state=[ 1.55487453  2.03574588  0.06530076 -0.2580183 ]\n",
      "[ episode 254 ][ timestamp 80 ] state=[ 1.55487453  2.03574588  0.06530076 -0.2580183 ], action=1, reward=1.0, next_state=[ 1.59558945  2.22987774  0.0601404  -0.52941066]\n",
      "[ episode 254 ][ timestamp 81 ] state=[ 1.59558945  2.22987774  0.0601404  -0.52941066], action=0, reward=1.0, next_state=[ 1.640187    2.03396358  0.04955218 -0.2184001 ]\n",
      "[ episode 254 ][ timestamp 82 ] state=[ 1.640187    2.03396358  0.04955218 -0.2184001 ], action=0, reward=1.0, next_state=[1.68086628 1.8381696  0.04518418 0.08949277]\n",
      "[ episode 254 ][ timestamp 83 ] state=[1.68086628 1.8381696  0.04518418 0.08949277], action=0, reward=1.0, next_state=[1.71762967 1.6424301  0.04697404 0.39608199]\n",
      "[ episode 254 ][ timestamp 84 ] state=[1.71762967 1.6424301  0.04697404 0.39608199], action=1, reward=1.0, next_state=[1.75047827 1.83685518 0.05489568 0.11857135]\n",
      "[ episode 254 ][ timestamp 85 ] state=[1.75047827 1.83685518 0.05489568 0.11857135], action=0, reward=1.0, next_state=[1.78721537 1.64099142 0.0572671  0.42805564]\n",
      "[ episode 254 ][ timestamp 86 ] state=[1.78721537 1.64099142 0.0572671  0.42805564], action=1, reward=1.0, next_state=[1.8200352  1.83525754 0.06582822 0.15396149]\n",
      "[ episode 254 ][ timestamp 87 ] state=[1.8200352  1.83525754 0.06582822 0.15396149], action=0, reward=1.0, next_state=[1.85674035 1.63925782 0.06890745 0.46666382]\n",
      "[ episode 254 ][ timestamp 88 ] state=[1.85674035 1.63925782 0.06890745 0.46666382], action=1, reward=1.0, next_state=[1.88952551 1.83334197 0.07824072 0.19647125]\n",
      "[ episode 254 ][ timestamp 89 ] state=[1.88952551 1.83334197 0.07824072 0.19647125], action=0, reward=1.0, next_state=[1.92619235 1.63719313 0.08217015 0.51277372]\n",
      "[ episode 254 ][ timestamp 90 ] state=[1.92619235 1.63719313 0.08217015 0.51277372], action=0, reward=1.0, next_state=[1.95893621 1.44101579 0.09242562 0.8301777 ]\n",
      "[ episode 254 ][ timestamp 91 ] state=[1.95893621 1.44101579 0.09242562 0.8301777 ], action=0, reward=1.0, next_state=[1.98775653 1.24476024 0.10902918 1.150439  ]\n",
      "[ episode 254 ][ timestamp 92 ] state=[1.98775653 1.24476024 0.10902918 1.150439  ], action=1, reward=1.0, next_state=[2.01265173 1.43830373 0.13203796 0.8938387 ]\n",
      "[ episode 254 ][ timestamp 93 ] state=[2.01265173 1.43830373 0.13203796 0.8938387 ], action=1, reward=1.0, next_state=[2.04141781 1.63141171 0.14991473 0.64540452]\n",
      "[ episode 254 ][ timestamp 94 ] state=[2.04141781 1.63141171 0.14991473 0.64540452], action=1, reward=1.0, next_state=[2.07404604 1.82416182 0.16282282 0.40343225]\n",
      "[ episode 254 ][ timestamp 95 ] state=[2.07404604 1.82416182 0.16282282 0.40343225], action=1, reward=1.0, next_state=[2.11052928 2.01664535 0.17089147 0.16618443]\n",
      "[ episode 254 ][ timestamp 96 ] state=[2.11052928 2.01664535 0.17089147 0.16618443], action=1, reward=1.0, next_state=[ 2.15086218  2.20896152  0.17421516 -0.06808988]\n",
      "[ episode 254 ][ timestamp 97 ] state=[ 2.15086218  2.20896152  0.17421516 -0.06808988], action=0, reward=1.0, next_state=[2.19504141 2.01182549 0.17285336 0.27409862]\n",
      "[ episode 254 ][ timestamp 98 ] state=[2.19504141 2.01182549 0.17285336 0.27409862], action=1, reward=1.0, next_state=[2.23527792 2.20411394 0.17833533 0.04053035]\n",
      "[ episode 254 ][ timestamp 99 ] state=[2.23527792 2.20411394 0.17833533 0.04053035], action=0, reward=1.0, next_state=[2.2793602  2.00694256 0.17914594 0.38374994]\n",
      "[ episode 254 ][ timestamp 100 ] state=[2.2793602  2.00694256 0.17914594 0.38374994], action=1, reward=1.0, next_state=[2.31949905 2.19912901 0.18682094 0.15247146]\n",
      "[ episode 254 ][ timestamp 101 ] state=[2.31949905 2.19912901 0.18682094 0.15247146], action=1, reward=1.0, next_state=[ 2.36348163  2.39115309  0.18987036 -0.07594632]\n",
      "[ episode 254 ][ timestamp 102 ] state=[ 2.36348163  2.39115309  0.18987036 -0.07594632], action=0, reward=-1.0, next_state=[2.4113047  2.19388829 0.18835144 0.27012032]\n",
      "[ Ended! ] Episode 254: Exploration_rate=0.28134514724562876. Score=102.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 255 ] state=[ 0.03863275  0.00581702  0.0200389  -0.01618577]\n",
      "[ episode 255 ][ timestamp 1 ] state=[ 0.03863275  0.00581702  0.0200389  -0.01618577], action=1, reward=1.0, next_state=[ 0.03874909  0.20064594  0.01971518 -0.30247944]\n",
      "[ episode 255 ][ timestamp 2 ] state=[ 0.03874909  0.20064594  0.01971518 -0.30247944], action=0, reward=1.0, next_state=[ 0.04276201  0.00524863  0.01366559 -0.00364455]\n",
      "[ episode 255 ][ timestamp 3 ] state=[ 0.04276201  0.00524863  0.01366559 -0.00364455], action=1, reward=1.0, next_state=[ 0.04286698  0.20017196  0.0135927  -0.29198469]\n",
      "[ episode 255 ][ timestamp 4 ] state=[ 0.04286698  0.20017196  0.0135927  -0.29198469], action=0, reward=1.0, next_state=[0.04687042 0.00485887 0.00775301 0.00495402]\n",
      "[ episode 255 ][ timestamp 5 ] state=[0.04687042 0.00485887 0.00775301 0.00495402], action=1, reward=1.0, next_state=[ 0.0469676   0.19986878  0.00785209 -0.28527269]\n",
      "[ episode 255 ][ timestamp 6 ] state=[ 0.0469676   0.19986878  0.00785209 -0.28527269], action=0, reward=1.0, next_state=[0.05096497 0.00463572 0.00214664 0.00987636]\n",
      "[ episode 255 ][ timestamp 7 ] state=[0.05096497 0.00463572 0.00214664 0.00987636], action=1, reward=1.0, next_state=[ 0.05105769  0.19972682  0.00234416 -0.28212851]\n",
      "[ episode 255 ][ timestamp 8 ] state=[ 0.05105769  0.19972682  0.00234416 -0.28212851], action=0, reward=1.0, next_state=[ 0.05505222  0.00457151 -0.00329841  0.01129283]\n",
      "[ episode 255 ][ timestamp 9 ] state=[ 0.05505222  0.00457151 -0.00329841  0.01129283], action=0, reward=1.0, next_state=[ 0.05514365 -0.19050298 -0.00307255  0.30293325]\n",
      "[ episode 255 ][ timestamp 10 ] state=[ 0.05514365 -0.19050298 -0.00307255  0.30293325], action=1, reward=1.0, next_state=[0.05133359 0.00466263 0.00298611 0.00928289]\n",
      "[ episode 255 ][ timestamp 11 ] state=[0.05133359 0.00466263 0.00298611 0.00928289], action=1, reward=1.0, next_state=[ 0.05142685  0.19974163  0.00317177 -0.28245638]\n",
      "[ episode 255 ][ timestamp 12 ] state=[ 0.05142685  0.19974163  0.00317177 -0.28245638], action=0, reward=1.0, next_state=[ 0.05542168  0.00457458 -0.00247736  0.01122522]\n",
      "[ episode 255 ][ timestamp 13 ] state=[ 0.05542168  0.00457458 -0.00247736  0.01122522], action=1, reward=1.0, next_state=[ 0.05551317  0.19973197 -0.00225285 -0.28223832]\n",
      "[ episode 255 ][ timestamp 14 ] state=[ 0.05551317  0.19973197 -0.00225285 -0.28223832], action=0, reward=1.0, next_state=[ 0.05950781  0.00464222 -0.00789762  0.00973322]\n",
      "[ episode 255 ][ timestamp 15 ] state=[ 0.05950781  0.00464222 -0.00789762  0.00973322], action=1, reward=1.0, next_state=[ 0.05960066  0.19987654 -0.00770295 -0.285431  ]\n",
      "[ episode 255 ][ timestamp 16 ] state=[ 0.05960066  0.19987654 -0.00770295 -0.285431  ], action=0, reward=1.0, next_state=[ 0.06359819  0.00486529 -0.01341157  0.00481255]\n",
      "[ episode 255 ][ timestamp 17 ] state=[ 0.06359819  0.00486529 -0.01341157  0.00481255], action=1, reward=1.0, next_state=[ 0.06369549  0.20017699 -0.01331532 -0.29207153]\n",
      "[ episode 255 ][ timestamp 18 ] state=[ 0.06369549  0.20017699 -0.01331532 -0.29207153], action=0, reward=1.0, next_state=[ 0.06769903  0.0052474  -0.01915675 -0.00361766]\n",
      "[ episode 255 ][ timestamp 19 ] state=[ 0.06769903  0.0052474  -0.01915675 -0.00361766], action=1, reward=1.0, next_state=[ 0.06780398  0.20063877 -0.01922911 -0.30228268]\n",
      "[ episode 255 ][ timestamp 20 ] state=[ 0.06780398  0.20063877 -0.01922911 -0.30228268], action=0, reward=1.0, next_state=[ 0.07181676  0.00579608 -0.02527476 -0.01572569]\n",
      "[ episode 255 ][ timestamp 21 ] state=[ 0.07181676  0.00579608 -0.02527476 -0.01572569], action=1, reward=1.0, next_state=[ 0.07193268  0.20127122 -0.02558927 -0.31627473]\n",
      "[ episode 255 ][ timestamp 22 ] state=[ 0.07193268  0.20127122 -0.02558927 -0.31627473], action=0, reward=1.0, next_state=[ 0.0759581   0.00652293 -0.03191477 -0.03177035]\n",
      "[ episode 255 ][ timestamp 23 ] state=[ 0.0759581   0.00652293 -0.03191477 -0.03177035], action=1, reward=1.0, next_state=[ 0.07608856  0.20208767 -0.03255018 -0.33434943]\n",
      "[ episode 255 ][ timestamp 24 ] state=[ 0.07608856  0.20208767 -0.03255018 -0.33434943], action=0, reward=1.0, next_state=[ 0.08013031  0.00744374 -0.03923716 -0.05210626]\n",
      "[ episode 255 ][ timestamp 25 ] state=[ 0.08013031  0.00744374 -0.03923716 -0.05210626], action=1, reward=1.0, next_state=[ 0.08027919  0.2031057  -0.04027929 -0.35690607]\n",
      "[ episode 255 ][ timestamp 26 ] state=[ 0.08027919  0.2031057  -0.04027929 -0.35690607], action=0, reward=1.0, next_state=[ 0.0843413   0.00857886 -0.04741741 -0.07719139]\n",
      "[ episode 255 ][ timestamp 27 ] state=[ 0.0843413   0.00857886 -0.04741741 -0.07719139], action=1, reward=1.0, next_state=[ 0.08451288  0.20434737 -0.04896124 -0.38444959]\n",
      "[ episode 255 ][ timestamp 28 ] state=[ 0.08451288  0.20434737 -0.04896124 -0.38444959], action=0, reward=1.0, next_state=[ 0.08859983  0.00995349 -0.05665023 -0.10759705]\n",
      "[ episode 255 ][ timestamp 29 ] state=[ 0.08859983  0.00995349 -0.05665023 -0.10759705], action=1, reward=1.0, next_state=[ 0.0887989   0.20583956 -0.05880217 -0.41760106]\n",
      "[ episode 255 ][ timestamp 30 ] state=[ 0.0887989   0.20583956 -0.05880217 -0.41760106], action=0, reward=1.0, next_state=[ 0.09291569  0.01159806 -0.06715419 -0.14402027]\n",
      "[ episode 255 ][ timestamp 31 ] state=[ 0.09291569  0.01159806 -0.06715419 -0.14402027], action=0, reward=1.0, next_state=[ 0.09314765 -0.1825011  -0.0700346   0.12674373]\n",
      "[ episode 255 ][ timestamp 32 ] state=[ 0.09314765 -0.1825011  -0.0700346   0.12674373], action=1, reward=1.0, next_state=[ 0.08949763  0.01355063 -0.06749972 -0.18718629]\n",
      "[ episode 255 ][ timestamp 33 ] state=[ 0.08949763  0.01355063 -0.06749972 -0.18718629], action=0, reward=1.0, next_state=[ 0.08976864 -0.18054391 -0.07124345  0.08346266]\n",
      "[ episode 255 ][ timestamp 34 ] state=[ 0.08976864 -0.18054391 -0.07124345  0.08346266], action=1, reward=1.0, next_state=[ 0.08615776  0.01552314 -0.0695742  -0.23081972]\n",
      "[ episode 255 ][ timestamp 35 ] state=[ 0.08615776  0.01552314 -0.0695742  -0.23081972], action=0, reward=1.0, next_state=[ 0.08646822 -0.17853922 -0.07419059  0.03913126]\n",
      "[ episode 255 ][ timestamp 36 ] state=[ 0.08646822 -0.17853922 -0.07419059  0.03913126], action=1, reward=1.0, next_state=[ 0.08289744  0.01756389 -0.07340796 -0.27600625]\n",
      "[ episode 255 ][ timestamp 37 ] state=[ 0.08289744  0.01756389 -0.07340796 -0.27600625], action=0, reward=1.0, next_state=[ 0.08324872 -0.17643816 -0.07892809 -0.00734946]\n",
      "[ episode 255 ][ timestamp 38 ] state=[ 0.08324872 -0.17643816 -0.07892809 -0.00734946], action=1, reward=1.0, next_state=[ 0.07971995  0.01972181 -0.07907508 -0.32385415]\n",
      "[ episode 255 ][ timestamp 39 ] state=[ 0.07971995  0.01972181 -0.07907508 -0.32385415], action=0, reward=1.0, next_state=[ 0.08011439 -0.17419036 -0.08555216 -0.05711866]\n",
      "[ episode 255 ][ timestamp 40 ] state=[ 0.08011439 -0.17419036 -0.08555216 -0.05711866], action=1, reward=1.0, next_state=[ 0.07663058  0.02204745 -0.08669454 -0.37552047]\n",
      "[ episode 255 ][ timestamp 41 ] state=[ 0.07663058  0.02204745 -0.08669454 -0.37552047], action=0, reward=1.0, next_state=[ 0.07707153 -0.171743   -0.09420494 -0.11138278]\n",
      "[ episode 255 ][ timestamp 42 ] state=[ 0.07707153 -0.171743   -0.09420494 -0.11138278], action=1, reward=1.0, next_state=[ 0.07363667  0.02459375 -0.0964326  -0.43223738]\n",
      "[ episode 255 ][ timestamp 43 ] state=[ 0.07363667  0.02459375 -0.0964326  -0.43223738], action=0, reward=1.0, next_state=[ 0.07412855 -0.16903998 -0.10507735 -0.17144349]\n",
      "[ episode 255 ][ timestamp 44 ] state=[ 0.07412855 -0.16903998 -0.10507735 -0.17144349], action=1, reward=1.0, next_state=[ 0.07074775  0.02741673 -0.10850622 -0.49533912]\n",
      "[ episode 255 ][ timestamp 45 ] state=[ 0.07074775  0.02741673 -0.10850622 -0.49533912], action=0, reward=1.0, next_state=[ 0.07129608 -0.1660212  -0.118413   -0.23872691]\n",
      "[ episode 255 ][ timestamp 46 ] state=[ 0.07129608 -0.1660212  -0.118413   -0.23872691], action=0, reward=1.0, next_state=[ 0.06797566 -0.35926999 -0.12318754  0.01438428]\n",
      "[ episode 255 ][ timestamp 47 ] state=[ 0.06797566 -0.35926999 -0.12318754  0.01438428], action=1, reward=1.0, next_state=[ 0.06079026 -0.16261645 -0.12289985 -0.31448628]\n",
      "[ episode 255 ][ timestamp 48 ] state=[ 0.06079026 -0.16261645 -0.12289985 -0.31448628], action=0, reward=1.0, next_state=[ 0.05753793 -0.35579294 -0.12918958 -0.06294882]\n",
      "[ episode 255 ][ timestamp 49 ] state=[ 0.05753793 -0.35579294 -0.12918958 -0.06294882], action=0, reward=1.0, next_state=[ 0.05042207 -0.54884871 -0.13044855  0.18634546]\n",
      "[ episode 255 ][ timestamp 50 ] state=[ 0.05042207 -0.54884871 -0.13044855  0.18634546], action=0, reward=1.0, next_state=[ 0.0394451  -0.7418864  -0.12672164  0.43519863]\n",
      "[ episode 255 ][ timestamp 51 ] state=[ 0.0394451  -0.7418864  -0.12672164  0.43519863], action=0, reward=1.0, next_state=[ 0.02460737 -0.93500802 -0.11801767  0.68540172]\n",
      "[ episode 255 ][ timestamp 52 ] state=[ 0.02460737 -0.93500802 -0.11801767  0.68540172], action=1, reward=1.0, next_state=[ 0.00590721 -0.73846254 -0.10430964  0.35801754]\n",
      "[ episode 255 ][ timestamp 53 ] state=[ 0.00590721 -0.73846254 -0.10430964  0.35801754], action=1, reward=1.0, next_state=[-0.00886204 -0.5420243  -0.09714929  0.0343503 ]\n",
      "[ episode 255 ][ timestamp 54 ] state=[-0.00886204 -0.5420243  -0.09714929  0.0343503 ], action=1, reward=1.0, next_state=[-0.01970253 -0.34565314 -0.09646228 -0.28733452]\n",
      "[ episode 255 ][ timestamp 55 ] state=[-0.01970253 -0.34565314 -0.09646228 -0.28733452], action=1, reward=1.0, next_state=[-0.02661559 -0.14929737 -0.10220897 -0.60881486]\n",
      "[ episode 255 ][ timestamp 56 ] state=[-0.02661559 -0.14929737 -0.10220897 -0.60881486], action=0, reward=1.0, next_state=[-0.02960154 -0.34285314 -0.11438527 -0.34999354]\n",
      "[ episode 255 ][ timestamp 57 ] state=[-0.02960154 -0.34285314 -0.11438527 -0.34999354], action=1, reward=1.0, next_state=[-0.0364586  -0.146306   -0.12138514 -0.67644363]\n",
      "[ episode 255 ][ timestamp 58 ] state=[-0.0364586  -0.146306   -0.12138514 -0.67644363], action=0, reward=1.0, next_state=[-0.03938472 -0.3395509  -0.13491401 -0.42430882]\n",
      "[ episode 255 ][ timestamp 59 ] state=[-0.03938472 -0.3395509  -0.13491401 -0.42430882], action=1, reward=1.0, next_state=[-0.04617574 -0.14280155 -0.14340019 -0.75629552]\n",
      "[ episode 255 ][ timestamp 60 ] state=[-0.04617574 -0.14280155 -0.14340019 -0.75629552], action=0, reward=1.0, next_state=[-0.04903177 -0.33568652 -0.1585261  -0.5119531 ]\n",
      "[ episode 255 ][ timestamp 61 ] state=[-0.04903177 -0.33568652 -0.1585261  -0.5119531 ], action=0, reward=1.0, next_state=[-0.0557455  -0.52826208 -0.16876516 -0.27312351]\n",
      "[ episode 255 ][ timestamp 62 ] state=[-0.0557455  -0.52826208 -0.16876516 -0.27312351], action=1, reward=1.0, next_state=[-0.06631074 -0.33118454 -0.17422763 -0.61392174]\n",
      "[ episode 255 ][ timestamp 63 ] state=[-0.06631074 -0.33118454 -0.17422763 -0.61392174], action=0, reward=1.0, next_state=[-0.07293443 -0.52349897 -0.18650607 -0.38078153]\n",
      "[ episode 255 ][ timestamp 64 ] state=[-0.07293443 -0.52349897 -0.18650607 -0.38078153], action=1, reward=1.0, next_state=[-0.08340441 -0.32628623 -0.1941217  -0.72598602]\n",
      "[ episode 255 ][ timestamp 65 ] state=[-0.08340441 -0.32628623 -0.1941217  -0.72598602], action=0, reward=1.0, next_state=[-0.08993014 -0.51827005 -0.20864142 -0.5001332 ]\n",
      "[ episode 255 ][ timestamp 66 ] state=[-0.08993014 -0.51827005 -0.20864142 -0.5001332 ], action=0, reward=-1.0, next_state=[-0.10029554 -0.70993473 -0.21864408 -0.27976758]\n",
      "[ Ended! ] Episode 255: Exploration_rate=0.2799384215094006. Score=66.\n",
      "[ Experience replay ] starts\n",
      "[ episode 256 ] state=[ 0.00640603 -0.0373547  -0.04859114  0.0163435 ]\n",
      "[ episode 256 ][ timestamp 1 ] state=[ 0.00640603 -0.0373547  -0.04859114  0.0163435 ], action=0, reward=1.0, next_state=[ 0.00565894 -0.23174734 -0.04826427  0.29330811]\n",
      "[ episode 256 ][ timestamp 2 ] state=[ 0.00565894 -0.23174734 -0.04826427  0.29330811], action=0, reward=1.0, next_state=[ 0.00102399 -0.42614912 -0.04239811  0.57038703]\n",
      "[ episode 256 ][ timestamp 3 ] state=[ 0.00102399 -0.42614912 -0.04239811  0.57038703], action=0, reward=1.0, next_state=[-0.00749899 -0.62065164 -0.03099037  0.8494173 ]\n",
      "[ episode 256 ][ timestamp 4 ] state=[-0.00749899 -0.62065164 -0.03099037  0.8494173 ], action=1, reward=1.0, next_state=[-0.01991202 -0.42512107 -0.01400202  0.54715257]\n",
      "[ episode 256 ][ timestamp 5 ] state=[-0.01991202 -0.42512107 -0.01400202  0.54715257], action=1, reward=1.0, next_state=[-0.02841444 -0.22980523 -0.00305897  0.25009107]\n",
      "[ episode 256 ][ timestamp 6 ] state=[-0.02841444 -0.22980523 -0.00305897  0.25009107], action=0, reward=1.0, next_state=[-0.03301055 -0.42488336  0.00194285  0.54180757]\n",
      "[ episode 256 ][ timestamp 7 ] state=[-0.03301055 -0.42488336  0.00194285  0.54180757], action=1, reward=1.0, next_state=[-0.04150821 -0.22978877  0.012779    0.24973743]\n",
      "[ episode 256 ][ timestamp 8 ] state=[-0.04150821 -0.22978877  0.012779    0.24973743], action=0, reward=1.0, next_state=[-0.04610399 -0.42509086  0.01777375  0.54642357]\n",
      "[ episode 256 ][ timestamp 9 ] state=[-0.04610399 -0.42509086  0.01777375  0.54642357], action=1, reward=1.0, next_state=[-0.05460581 -0.23022308  0.02870222  0.25939328]\n",
      "[ episode 256 ][ timestamp 10 ] state=[-0.05460581 -0.23022308  0.02870222  0.25939328], action=0, reward=1.0, next_state=[-0.05921027 -0.42574277  0.03389009  0.56098932]\n",
      "[ episode 256 ][ timestamp 11 ] state=[-0.05921027 -0.42574277  0.03389009  0.56098932], action=0, reward=1.0, next_state=[-0.06772512 -0.62132355  0.04510988  0.86415381]\n",
      "[ episode 256 ][ timestamp 12 ] state=[-0.06772512 -0.62132355  0.04510988  0.86415381], action=1, reward=1.0, next_state=[-0.0801516  -0.42684375  0.06239295  0.58598867]\n",
      "[ episode 256 ][ timestamp 13 ] state=[-0.0801516  -0.42684375  0.06239295  0.58598867], action=1, reward=1.0, next_state=[-0.08868847 -0.23264864  0.07411273  0.31359443]\n",
      "[ episode 256 ][ timestamp 14 ] state=[-0.08868847 -0.23264864  0.07411273  0.31359443], action=0, reward=1.0, next_state=[-0.09334144 -0.42874382  0.08038461  0.62869895]\n",
      "[ episode 256 ][ timestamp 15 ] state=[-0.09334144 -0.42874382  0.08038461  0.62869895], action=1, reward=1.0, next_state=[-0.10191632 -0.23483026  0.09295859  0.3623755 ]\n",
      "[ episode 256 ][ timestamp 16 ] state=[-0.10191632 -0.23483026  0.09295859  0.3623755 ], action=0, reward=1.0, next_state=[-0.10661292 -0.43114205  0.1002061   0.68286228]\n",
      "[ episode 256 ][ timestamp 17 ] state=[-0.10661292 -0.43114205  0.1002061   0.68286228], action=0, reward=1.0, next_state=[-0.11523577 -0.6275022   0.11386335  1.00533629]\n",
      "[ episode 256 ][ timestamp 18 ] state=[-0.11523577 -0.6275022   0.11386335  1.00533629], action=1, reward=1.0, next_state=[-0.12778581 -0.43406981  0.13397007  0.75047008]\n",
      "[ episode 256 ][ timestamp 19 ] state=[-0.12778581 -0.43406981  0.13397007  0.75047008], action=1, reward=1.0, next_state=[-0.13646721 -0.2410249   0.14897948  0.50276688]\n",
      "[ episode 256 ][ timestamp 20 ] state=[-0.13646721 -0.2410249   0.14897948  0.50276688], action=1, reward=1.0, next_state=[-0.1412877  -0.04828216  0.15903481  0.2604934 ]\n",
      "[ episode 256 ][ timestamp 21 ] state=[-0.1412877  -0.04828216  0.15903481  0.2604934 ], action=1, reward=1.0, next_state=[-0.14225335  0.14425443  0.16424468  0.02189246]\n",
      "[ episode 256 ][ timestamp 22 ] state=[-0.14225335  0.14425443  0.16424468  0.02189246], action=0, reward=1.0, next_state=[-0.13936826 -0.05279536  0.16468253  0.36156045]\n",
      "[ episode 256 ][ timestamp 23 ] state=[-0.13936826 -0.05279536  0.16468253  0.36156045], action=1, reward=1.0, next_state=[-0.14042417  0.13964973  0.17191374  0.12499647]\n",
      "[ episode 256 ][ timestamp 24 ] state=[-0.14042417  0.13964973  0.17191374  0.12499647], action=0, reward=1.0, next_state=[-0.13763117 -0.05746473  0.17441367  0.46660378]\n",
      "[ episode 256 ][ timestamp 25 ] state=[-0.13763117 -0.05746473  0.17441367  0.46660378], action=1, reward=1.0, next_state=[-0.13878047  0.13481947  0.18374574  0.23357138]\n",
      "[ episode 256 ][ timestamp 26 ] state=[-0.13878047  0.13481947  0.18374574  0.23357138], action=0, reward=1.0, next_state=[-0.13608408 -0.06238729  0.18841717  0.5781197 ]\n",
      "[ episode 256 ][ timestamp 27 ] state=[-0.13608408 -0.06238729  0.18841717  0.5781197 ], action=1, reward=1.0, next_state=[-0.13733182  0.12966405  0.19997957  0.35020856]\n",
      "[ episode 256 ][ timestamp 28 ] state=[-0.13733182  0.12966405  0.19997957  0.35020856], action=1, reward=1.0, next_state=[-0.13473854  0.32146382  0.20698374  0.12664547]\n",
      "[ episode 256 ][ timestamp 29 ] state=[-0.13473854  0.32146382  0.20698374  0.12664547], action=1, reward=-1.0, next_state=[-0.12830926  0.51311235  0.20951665 -0.09427165]\n",
      "[ Ended! ] Episode 256: Exploration_rate=0.27853872940185365. Score=29.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 257 ] state=[-0.03337512  0.02041422 -0.01963308  0.02161838]\n",
      "[ episode 257 ][ timestamp 1 ] state=[-0.03337512  0.02041422 -0.01963308  0.02161838], action=1, reward=1.0, next_state=[-0.03296683  0.21581214 -0.01920071 -0.27719378]\n",
      "[ episode 257 ][ timestamp 2 ] state=[-0.03296683  0.21581214 -0.01920071 -0.27719378], action=0, reward=1.0, next_state=[-0.02865059  0.02096931 -0.02474459  0.00937194]\n",
      "[ episode 257 ][ timestamp 3 ] state=[-0.02865059  0.02096931 -0.02474459  0.00937194], action=0, reward=1.0, next_state=[-0.0282312  -0.17378919 -0.02455715  0.29414609]\n",
      "[ episode 257 ][ timestamp 4 ] state=[-0.0282312  -0.17378919 -0.02455715  0.29414609], action=1, reward=1.0, next_state=[-0.03170699  0.02167411 -0.01867423 -0.00617953]\n",
      "[ episode 257 ][ timestamp 5 ] state=[-0.03170699  0.02167411 -0.01867423 -0.00617953], action=1, reward=1.0, next_state=[-0.03127351  0.21705883 -0.01879782 -0.30469541]\n",
      "[ episode 257 ][ timestamp 6 ] state=[-0.03127351  0.21705883 -0.01879782 -0.30469541], action=0, reward=1.0, next_state=[-0.02693233  0.02220973 -0.02489173 -0.01799964]\n",
      "[ episode 257 ][ timestamp 7 ] state=[-0.02693233  0.02220973 -0.02489173 -0.01799964], action=1, reward=1.0, next_state=[-0.02648814  0.21767965 -0.02525172 -0.31843111]\n",
      "[ episode 257 ][ timestamp 8 ] state=[-0.02648814  0.21767965 -0.02525172 -0.31843111], action=0, reward=1.0, next_state=[-0.02213454  0.02292628 -0.03162034 -0.0338174 ]\n",
      "[ episode 257 ][ timestamp 9 ] state=[-0.02213454  0.02292628 -0.03162034 -0.0338174 ], action=1, reward=1.0, next_state=[-0.02167602  0.21848707 -0.03229669 -0.33630678]\n",
      "[ episode 257 ][ timestamp 10 ] state=[-0.02167602  0.21848707 -0.03229669 -0.33630678], action=0, reward=1.0, next_state=[-0.01730628  0.02383927 -0.03902282 -0.05398091]\n",
      "[ episode 257 ][ timestamp 11 ] state=[-0.01730628  0.02383927 -0.03902282 -0.05398091], action=1, reward=1.0, next_state=[-0.01682949  0.2194984  -0.04010244 -0.35871597]\n",
      "[ episode 257 ][ timestamp 12 ] state=[-0.01682949  0.2194984  -0.04010244 -0.35871597], action=0, reward=1.0, next_state=[-0.01243952  0.0249688  -0.04727676 -0.07894314]\n",
      "[ episode 257 ][ timestamp 13 ] state=[-0.01243952  0.0249688  -0.04727676 -0.07894314], action=0, reward=1.0, next_state=[-0.01194015 -0.16944466 -0.04885562  0.19845702]\n",
      "[ episode 257 ][ timestamp 14 ] state=[-0.01194015 -0.16944466 -0.04885562  0.19845702], action=0, reward=1.0, next_state=[-0.01532904 -0.36383501 -0.04488648  0.4753368 ]\n",
      "[ episode 257 ][ timestamp 15 ] state=[-0.01532904 -0.36383501 -0.04488648  0.4753368 ], action=0, reward=1.0, next_state=[-0.02260574 -0.55829533 -0.03537975  0.75354128]\n",
      "[ episode 257 ][ timestamp 16 ] state=[-0.02260574 -0.55829533 -0.03537975  0.75354128], action=1, reward=1.0, next_state=[-0.03377165 -0.36270391 -0.02030892  0.44993827]\n",
      "[ episode 257 ][ timestamp 17 ] state=[-0.03377165 -0.36270391 -0.02030892  0.44993827], action=1, reward=1.0, next_state=[-0.04102572 -0.16730069 -0.01131016  0.15092347]\n",
      "[ episode 257 ][ timestamp 18 ] state=[-0.04102572 -0.16730069 -0.01131016  0.15092347], action=1, reward=1.0, next_state=[-0.04437174  0.02798138 -0.00829169 -0.14530601]\n",
      "[ episode 257 ][ timestamp 19 ] state=[-0.04437174  0.02798138 -0.00829169 -0.14530601], action=0, reward=1.0, next_state=[-0.04381211 -0.16702086 -0.01119781  0.14474956]\n",
      "[ episode 257 ][ timestamp 20 ] state=[-0.04381211 -0.16702086 -0.01119781  0.14474956], action=1, reward=1.0, next_state=[-0.04715253  0.02825965 -0.00830282 -0.15144493]\n",
      "[ episode 257 ][ timestamp 21 ] state=[-0.04715253  0.02825965 -0.00830282 -0.15144493], action=1, reward=1.0, next_state=[-0.04658733  0.2234995  -0.01133172 -0.44673561]\n",
      "[ episode 257 ][ timestamp 22 ] state=[-0.04658733  0.2234995  -0.01133172 -0.44673561], action=0, reward=1.0, next_state=[-0.04211734  0.02853968 -0.02026643 -0.1576461 ]\n",
      "[ episode 257 ][ timestamp 23 ] state=[-0.04211734  0.02853968 -0.02026643 -0.1576461 ], action=0, reward=1.0, next_state=[-0.04154655 -0.16628634 -0.02341935  0.12857499]\n",
      "[ episode 257 ][ timestamp 24 ] state=[-0.04154655 -0.16628634 -0.02341935  0.12857499], action=1, reward=1.0, next_state=[-0.04487228  0.02916314 -0.02084785 -0.17140349]\n",
      "[ episode 257 ][ timestamp 25 ] state=[-0.04487228  0.02916314 -0.02084785 -0.17140349], action=0, reward=1.0, next_state=[-0.04428901 -0.16565432 -0.02427592  0.11463036]\n",
      "[ episode 257 ][ timestamp 26 ] state=[-0.04428901 -0.16565432 -0.02427592  0.11463036], action=1, reward=1.0, next_state=[-0.0476021   0.02980691 -0.02198331 -0.18561151]\n",
      "[ episode 257 ][ timestamp 27 ] state=[-0.0476021   0.02980691 -0.02198331 -0.18561151], action=1, reward=1.0, next_state=[-0.04700596  0.22523639 -0.02569554 -0.48514747]\n",
      "[ episode 257 ][ timestamp 28 ] state=[-0.04700596  0.22523639 -0.02569554 -0.48514747], action=1, reward=1.0, next_state=[-0.04250123  0.42071135 -0.03539849 -0.78581677]\n",
      "[ episode 257 ][ timestamp 29 ] state=[-0.04250123  0.42071135 -0.03539849 -0.78581677], action=0, reward=1.0, next_state=[-0.03408701  0.22609315 -0.05111483 -0.50447734]\n",
      "[ episode 257 ][ timestamp 30 ] state=[-0.03408701  0.22609315 -0.05111483 -0.50447734], action=1, reward=1.0, next_state=[-0.02956514  0.4218968  -0.06120437 -0.81282043]\n",
      "[ episode 257 ][ timestamp 31 ] state=[-0.02956514  0.4218968  -0.06120437 -0.81282043], action=0, reward=1.0, next_state=[-0.02112721  0.22766417 -0.07746078 -0.53999986]\n",
      "[ episode 257 ][ timestamp 32 ] state=[-0.02112721  0.22766417 -0.07746078 -0.53999986], action=0, reward=1.0, next_state=[-0.01657393  0.03371165 -0.08826078 -0.27269415]\n",
      "[ episode 257 ][ timestamp 33 ] state=[-0.01657393  0.03371165 -0.08826078 -0.27269415], action=0, reward=1.0, next_state=[-0.01589969 -0.16004732 -0.09371466 -0.00910199]\n",
      "[ episode 257 ][ timestamp 34 ] state=[-0.01589969 -0.16004732 -0.09371466 -0.00910199], action=0, reward=1.0, next_state=[-0.01910064 -0.35370904 -0.0938967   0.25260411]\n",
      "[ episode 257 ][ timestamp 35 ] state=[-0.01910064 -0.35370904 -0.0938967   0.25260411], action=1, reward=1.0, next_state=[-0.02617482 -0.15738049 -0.08884462 -0.06815654]\n",
      "[ episode 257 ][ timestamp 36 ] state=[-0.02617482 -0.15738049 -0.08884462 -0.06815654], action=1, reward=1.0, next_state=[-0.02932243  0.03889546 -0.09020775 -0.38749524]\n",
      "[ episode 257 ][ timestamp 37 ] state=[-0.02932243  0.03889546 -0.09020775 -0.38749524], action=1, reward=1.0, next_state=[-0.02854452  0.23517437 -0.09795766 -0.70720163]\n",
      "[ episode 257 ][ timestamp 38 ] state=[-0.02854452  0.23517437 -0.09795766 -0.70720163], action=0, reward=1.0, next_state=[-0.02384103  0.0415361  -0.11210169 -0.44689021]\n",
      "[ episode 257 ][ timestamp 39 ] state=[-0.02384103  0.0415361  -0.11210169 -0.44689021], action=0, reward=1.0, next_state=[-0.02301031 -0.15183631 -0.12103949 -0.19154115]\n",
      "[ episode 257 ][ timestamp 40 ] state=[-0.02301031 -0.15183631 -0.12103949 -0.19154115], action=1, reward=1.0, next_state=[-0.02604704  0.04479054 -0.12487032 -0.51982233]\n",
      "[ episode 257 ][ timestamp 41 ] state=[-0.02604704  0.04479054 -0.12487032 -0.51982233], action=0, reward=1.0, next_state=[-0.02515123 -0.14837273 -0.13526676 -0.26894996]\n",
      "[ episode 257 ][ timestamp 42 ] state=[-0.02515123 -0.14837273 -0.13526676 -0.26894996], action=1, reward=1.0, next_state=[-0.02811868  0.04839422 -0.14064576 -0.60105156]\n",
      "[ episode 257 ][ timestamp 43 ] state=[-0.02811868  0.04839422 -0.14064576 -0.60105156], action=0, reward=1.0, next_state=[-0.0271508  -0.1445092  -0.15266679 -0.35576729]\n",
      "[ episode 257 ][ timestamp 44 ] state=[-0.0271508  -0.1445092  -0.15266679 -0.35576729], action=1, reward=1.0, next_state=[-0.03004098  0.05241634 -0.15978214 -0.69242984]\n",
      "[ episode 257 ][ timestamp 45 ] state=[-0.03004098  0.05241634 -0.15978214 -0.69242984], action=0, reward=1.0, next_state=[-0.02899265 -0.14017061 -0.17363074 -0.4540055 ]\n",
      "[ episode 257 ][ timestamp 46 ] state=[-0.02899265 -0.14017061 -0.17363074 -0.4540055 ], action=1, reward=1.0, next_state=[-0.03179607  0.05692654 -0.18271085 -0.79599723]\n",
      "[ episode 257 ][ timestamp 47 ] state=[-0.03179607  0.05692654 -0.18271085 -0.79599723], action=0, reward=1.0, next_state=[-0.03065754 -0.13528101 -0.19863079 -0.56590352]\n",
      "[ episode 257 ][ timestamp 48 ] state=[-0.03065754 -0.13528101 -0.19863079 -0.56590352], action=0, reward=-1.0, next_state=[-0.03336316 -0.32714366 -0.20994886 -0.34178245]\n",
      "[ Ended! ] Episode 257: Exploration_rate=0.27714603575484437. Score=48.\n",
      "[ Experience replay ] starts\n",
      "[ episode 258 ] state=[0.0225866  0.04703465 0.02325517 0.03599301]\n",
      "[ episode 258 ][ timestamp 1 ] state=[0.0225866  0.04703465 0.02325517 0.03599301], action=0, reward=1.0, next_state=[ 0.02352729 -0.14841294  0.02397503  0.33592152]\n",
      "[ episode 258 ][ timestamp 2 ] state=[ 0.02352729 -0.14841294  0.02397503  0.33592152], action=0, reward=1.0, next_state=[ 0.02055903 -0.34386773  0.03069346  0.63606744]\n",
      "[ episode 258 ][ timestamp 3 ] state=[ 0.02055903 -0.34386773  0.03069346  0.63606744], action=0, reward=1.0, next_state=[ 0.01368168 -0.53940401  0.04341481  0.93825617]\n",
      "[ episode 258 ][ timestamp 4 ] state=[ 0.01368168 -0.53940401  0.04341481  0.93825617], action=0, reward=1.0, next_state=[ 0.00289359 -0.73508357  0.06217993  1.24425887]\n",
      "[ episode 258 ][ timestamp 5 ] state=[ 0.00289359 -0.73508357  0.06217993  1.24425887], action=0, reward=1.0, next_state=[-0.01180808 -0.93094578  0.08706511  1.55575353]\n",
      "[ episode 258 ][ timestamp 6 ] state=[-0.01180808 -0.93094578  0.08706511  1.55575353], action=0, reward=1.0, next_state=[-0.03042699 -1.12699627  0.11818018  1.8742802 ]\n",
      "[ episode 258 ][ timestamp 7 ] state=[-0.03042699 -1.12699627  0.11818018  1.8742802 ], action=0, reward=1.0, next_state=[-0.05296692 -1.3231937   0.15566578  2.20118773]\n",
      "[ episode 258 ][ timestamp 8 ] state=[-0.05296692 -1.3231937   0.15566578  2.20118773], action=1, reward=1.0, next_state=[-0.07943079 -1.12987547  0.19968954  1.96029777]\n",
      "[ episode 258 ][ timestamp 9 ] state=[-0.07943079 -1.12987547  0.19968954  1.96029777], action=1, reward=-1.0, next_state=[-0.1020283  -0.93735246  0.23889549  1.73557124]\n",
      "[ Ended! ] Episode 258: Exploration_rate=0.2757603055760701. Score=9.\n",
      "[ Experience replay ] starts\n",
      "[ episode 259 ] state=[ 0.00276766 -0.00293615  0.00732109  0.01373958]\n",
      "[ episode 259 ][ timestamp 1 ] state=[ 0.00276766 -0.00293615  0.00732109  0.01373958], action=1, reward=1.0, next_state=[ 0.00270894  0.19208005  0.00759588 -0.27662449]\n",
      "[ episode 259 ][ timestamp 2 ] state=[ 0.00270894  0.19208005  0.00759588 -0.27662449], action=0, reward=1.0, next_state=[ 0.00655054 -0.00314944  0.00206339  0.01844446]\n",
      "[ episode 259 ][ timestamp 3 ] state=[ 0.00655054 -0.00314944  0.00206339  0.01844446], action=1, reward=1.0, next_state=[ 0.00648755  0.19194286  0.00243228 -0.27358673]\n",
      "[ episode 259 ][ timestamp 4 ] state=[ 0.00648755  0.19194286  0.00243228 -0.27358673], action=0, reward=1.0, next_state=[ 0.0103264  -0.00321371 -0.00303946  0.01986235]\n",
      "[ episode 259 ][ timestamp 5 ] state=[ 0.0103264  -0.00321371 -0.00303946  0.01986235], action=1, reward=1.0, next_state=[ 0.01026213  0.1919517  -0.00264221 -0.27377801]\n",
      "[ episode 259 ][ timestamp 6 ] state=[ 0.01026213  0.1919517  -0.00264221 -0.27377801], action=0, reward=1.0, next_state=[ 0.01410116 -0.00313246 -0.00811777  0.01807039]\n",
      "[ episode 259 ][ timestamp 7 ] state=[ 0.01410116 -0.00313246 -0.00811777  0.01807039], action=0, reward=1.0, next_state=[ 0.01403852 -0.19813705 -0.00775636  0.30818105]\n",
      "[ episode 259 ][ timestamp 8 ] state=[ 0.01403852 -0.19813705 -0.00775636  0.30818105], action=1, reward=1.0, next_state=[ 0.01007577 -0.00290545 -0.00159274  0.01306211]\n",
      "[ episode 259 ][ timestamp 9 ] state=[ 0.01007577 -0.00290545 -0.00159274  0.01306211], action=1, reward=1.0, next_state=[ 0.01001767  0.19223931 -0.0013315  -0.28012292]\n",
      "[ episode 259 ][ timestamp 10 ] state=[ 0.01001767  0.19223931 -0.0013315  -0.28012292], action=0, reward=1.0, next_state=[ 0.01386245 -0.00286362 -0.00693396  0.01213976]\n",
      "[ episode 259 ][ timestamp 11 ] state=[ 0.01386245 -0.00286362 -0.00693396  0.01213976], action=1, reward=1.0, next_state=[ 0.01380518  0.19235708 -0.00669116 -0.28272283]\n",
      "[ episode 259 ][ timestamp 12 ] state=[ 0.01380518  0.19235708 -0.00669116 -0.28272283], action=0, reward=1.0, next_state=[ 0.01765232 -0.0026688  -0.01234562  0.00784225]\n",
      "[ episode 259 ][ timestamp 13 ] state=[ 0.01765232 -0.0026688  -0.01234562  0.00784225], action=1, reward=1.0, next_state=[ 0.01759894  0.19262801 -0.01218877 -0.28871015]\n",
      "[ episode 259 ][ timestamp 14 ] state=[ 0.01759894  0.19262801 -0.01218877 -0.28871015], action=0, reward=1.0, next_state=[ 0.02145151 -0.00231802 -0.01796297  0.00010377]\n",
      "[ episode 259 ][ timestamp 15 ] state=[ 0.02145151 -0.00231802 -0.01796297  0.00010377], action=1, reward=1.0, next_state=[ 0.02140514  0.19305688 -0.0179609  -0.29819213]\n",
      "[ episode 259 ][ timestamp 16 ] state=[ 0.02140514  0.19305688 -0.0179609  -0.29819213], action=0, reward=1.0, next_state=[ 0.02526628 -0.0018045  -0.02392474 -0.01122743]\n",
      "[ episode 259 ][ timestamp 17 ] state=[ 0.02526628 -0.0018045  -0.02392474 -0.01122743], action=1, reward=1.0, next_state=[ 0.02523019  0.19365224 -0.02414929 -0.31136185]\n",
      "[ episode 259 ][ timestamp 18 ] state=[ 0.02523019  0.19365224 -0.02414929 -0.31136185], action=0, reward=1.0, next_state=[ 0.02910324 -0.00111748 -0.03037653 -0.02639165]\n",
      "[ episode 259 ][ timestamp 19 ] state=[ 0.02910324 -0.00111748 -0.03037653 -0.02639165], action=1, reward=1.0, next_state=[ 0.02908089  0.19442663 -0.03090436 -0.32850182]\n",
      "[ episode 259 ][ timestamp 20 ] state=[ 0.02908089  0.19442663 -0.03090436 -0.32850182], action=0, reward=1.0, next_state=[ 0.03296942 -0.00024204 -0.0374744  -0.04572268]\n",
      "[ episode 259 ][ timestamp 21 ] state=[ 0.03296942 -0.00024204 -0.0374744  -0.04572268], action=1, reward=1.0, next_state=[ 0.03296458  0.19539667 -0.03838885 -0.34998962]\n",
      "[ episode 259 ][ timestamp 22 ] state=[ 0.03296458  0.19539667 -0.03838885 -0.34998962], action=0, reward=1.0, next_state=[ 0.03687251  0.00084111 -0.04538864 -0.06965484]\n",
      "[ episode 259 ][ timestamp 23 ] state=[ 0.03687251  0.00084111 -0.04538864 -0.06965484], action=1, reward=1.0, next_state=[ 0.03688933  0.19658341 -0.04678174 -0.37630558]\n",
      "[ episode 259 ][ timestamp 24 ] state=[ 0.03688933  0.19658341 -0.04678174 -0.37630558], action=0, reward=1.0, next_state=[ 0.040821    0.00215605 -0.05430785 -0.09873243]\n",
      "[ episode 259 ][ timestamp 25 ] state=[ 0.040821    0.00215605 -0.05430785 -0.09873243], action=1, reward=1.0, next_state=[ 0.04086412  0.19801258 -0.0562825  -0.40804276]\n",
      "[ episode 259 ][ timestamp 26 ] state=[ 0.04086412  0.19801258 -0.0562825  -0.40804276], action=0, reward=1.0, next_state=[ 0.04482438  0.00373195 -0.06444336 -0.13362158]\n",
      "[ episode 259 ][ timestamp 27 ] state=[ 0.04482438  0.00373195 -0.06444336 -0.13362158], action=1, reward=1.0, next_state=[ 0.04489901  0.19971496 -0.06711579 -0.44591911]\n",
      "[ episode 259 ][ timestamp 28 ] state=[ 0.04489901  0.19971496 -0.06711579 -0.44591911], action=0, reward=1.0, next_state=[ 0.04889331  0.00560353 -0.07603417 -0.17512474]\n",
      "[ episode 259 ][ timestamp 29 ] state=[ 0.04889331  0.00560353 -0.07603417 -0.17512474], action=1, reward=1.0, next_state=[ 0.04900538  0.20172665 -0.07953666 -0.49079197]\n",
      "[ episode 259 ][ timestamp 30 ] state=[ 0.04900538  0.20172665 -0.07953666 -0.49079197], action=0, reward=1.0, next_state=[ 0.05303992  0.00781147 -0.0893525  -0.22419789]\n",
      "[ episode 259 ][ timestamp 31 ] state=[ 0.05303992  0.00781147 -0.0893525  -0.22419789], action=1, reward=1.0, next_state=[ 0.05319615  0.20408932 -0.09383646 -0.54367485]\n",
      "[ episode 259 ][ timestamp 32 ] state=[ 0.05319615  0.20408932 -0.09383646 -0.54367485], action=1, reward=1.0, next_state=[ 0.05727793  0.40039601 -0.10470996 -0.86438688]\n",
      "[ episode 259 ][ timestamp 33 ] state=[ 0.05727793  0.40039601 -0.10470996 -0.86438688], action=0, reward=1.0, next_state=[ 0.06528585  0.20684333 -0.1219977  -0.60637651]\n",
      "[ episode 259 ][ timestamp 34 ] state=[ 0.06528585  0.20684333 -0.1219977  -0.60637651], action=0, reward=1.0, next_state=[ 0.06942272  0.01361942 -0.13412523 -0.35447328]\n",
      "[ episode 259 ][ timestamp 35 ] state=[ 0.06942272  0.01361942 -0.13412523 -0.35447328], action=1, reward=1.0, next_state=[ 0.06969511  0.21036811 -0.14121469 -0.68626042]\n",
      "[ episode 259 ][ timestamp 36 ] state=[ 0.06969511  0.21036811 -0.14121469 -0.68626042], action=0, reward=1.0, next_state=[ 0.07390247  0.01745952 -0.1549399  -0.44115718]\n",
      "[ episode 259 ][ timestamp 37 ] state=[ 0.07390247  0.01745952 -0.1549399  -0.44115718], action=1, reward=1.0, next_state=[ 0.07425166  0.21439567 -0.16376304 -0.77839299]\n",
      "[ episode 259 ][ timestamp 38 ] state=[ 0.07425166  0.21439567 -0.16376304 -0.77839299], action=0, reward=1.0, next_state=[ 0.07853957  0.02185864 -0.1793309  -0.54138286]\n",
      "[ episode 259 ][ timestamp 39 ] state=[ 0.07853957  0.02185864 -0.1793309  -0.54138286], action=0, reward=1.0, next_state=[ 0.07897675 -0.17034977 -0.19015856 -0.31013499]\n",
      "[ episode 259 ][ timestamp 40 ] state=[ 0.07897675 -0.17034977 -0.19015856 -0.31013499], action=1, reward=1.0, next_state=[ 0.07556975  0.02690068 -0.19636126 -0.65624759]\n",
      "[ episode 259 ][ timestamp 41 ] state=[ 0.07556975  0.02690068 -0.19636126 -0.65624759], action=0, reward=-1.0, next_state=[ 0.07610777 -0.16502441 -0.20948621 -0.43125222]\n",
      "[ Ended! ] Episode 259: Exploration_rate=0.2743815040481898. Score=41.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 260 ] state=[ 0.01822661 -0.00647644  0.01049385 -0.04673153]\n",
      "[ episode 260 ][ timestamp 1 ] state=[ 0.01822661 -0.00647644  0.01049385 -0.04673153], action=1, reward=1.0, next_state=[ 0.01809708  0.18849347  0.00955922 -0.33608517]\n",
      "[ episode 260 ][ timestamp 2 ] state=[ 0.01809708  0.18849347  0.00955922 -0.33608517], action=0, reward=1.0, next_state=[ 0.02186695 -0.00676321  0.00283752 -0.04040317]\n",
      "[ episode 260 ][ timestamp 3 ] state=[ 0.02186695 -0.00676321  0.00283752 -0.04040317], action=0, reward=1.0, next_state=[ 0.02173169 -0.20192573  0.00202946  0.25317367]\n",
      "[ episode 260 ][ timestamp 4 ] state=[ 0.02173169 -0.20192573  0.00202946  0.25317367], action=1, reward=1.0, next_state=[ 0.01769317 -0.00683282  0.00709293 -0.03886844]\n",
      "[ episode 260 ][ timestamp 5 ] state=[ 0.01769317 -0.00683282  0.00709293 -0.03886844], action=0, reward=1.0, next_state=[ 0.01755652 -0.20205576  0.00631556  0.25604391]\n",
      "[ episode 260 ][ timestamp 6 ] state=[ 0.01755652 -0.20205576  0.00631556  0.25604391], action=1, reward=1.0, next_state=[ 0.0135154  -0.00702454  0.01143644 -0.03464031]\n",
      "[ episode 260 ][ timestamp 7 ] state=[ 0.0135154  -0.00702454  0.01143644 -0.03464031], action=0, reward=1.0, next_state=[ 0.01337491 -0.20230861  0.01074363  0.26162888]\n",
      "[ episode 260 ][ timestamp 8 ] state=[ 0.01337491 -0.20230861  0.01074363  0.26162888], action=1, reward=1.0, next_state=[ 0.00932874 -0.00734166  0.01597621 -0.02764611]\n",
      "[ episode 260 ][ timestamp 9 ] state=[ 0.00932874 -0.00734166  0.01597621 -0.02764611], action=0, reward=1.0, next_state=[ 0.00918191 -0.20268903  0.01542329  0.27003437]\n",
      "[ episode 260 ][ timestamp 10 ] state=[ 0.00918191 -0.20268903  0.01542329  0.27003437], action=0, reward=1.0, next_state=[ 0.00512813 -0.39802765  0.02082398  0.5675417 ]\n",
      "[ episode 260 ][ timestamp 11 ] state=[ 0.00512813 -0.39802765  0.02082398  0.5675417 ], action=1, reward=1.0, next_state=[-0.00283243 -0.20320389  0.03217481  0.28149124]\n",
      "[ episode 260 ][ timestamp 12 ] state=[-0.00283243 -0.20320389  0.03217481  0.28149124], action=1, reward=1.0, next_state=[-0.00689651 -0.00855531  0.03780463 -0.00087276]\n",
      "[ episode 260 ][ timestamp 13 ] state=[-0.00689651 -0.00855531  0.03780463 -0.00087276], action=0, reward=1.0, next_state=[-0.00706761 -0.20419847  0.03778718  0.30349421]\n",
      "[ episode 260 ][ timestamp 14 ] state=[-0.00706761 -0.20419847  0.03778718  0.30349421], action=1, reward=1.0, next_state=[-0.01115158 -0.00963485  0.04385706  0.0229639 ]\n",
      "[ episode 260 ][ timestamp 15 ] state=[-0.01115158 -0.00963485  0.04385706  0.0229639 ], action=0, reward=1.0, next_state=[-0.01134428 -0.20535742  0.04431634  0.32915529]\n",
      "[ episode 260 ][ timestamp 16 ] state=[-0.01134428 -0.20535742  0.04431634  0.32915529], action=1, reward=1.0, next_state=[-0.01545143 -0.01089344  0.05089945  0.05077046]\n",
      "[ episode 260 ][ timestamp 17 ] state=[-0.01545143 -0.01089344  0.05089945  0.05077046], action=0, reward=1.0, next_state=[-0.01566929 -0.20670689  0.05191486  0.35906821]\n",
      "[ episode 260 ][ timestamp 18 ] state=[-0.01566929 -0.20670689  0.05191486  0.35906821], action=1, reward=1.0, next_state=[-0.01980343 -0.0123599   0.05909622  0.0831966 ]\n",
      "[ episode 260 ][ timestamp 19 ] state=[-0.01980343 -0.0123599   0.05909622  0.0831966 ], action=0, reward=1.0, next_state=[-0.02005063 -0.208277    0.06076015  0.39392341]\n",
      "[ episode 260 ][ timestamp 20 ] state=[-0.02005063 -0.208277    0.06076015  0.39392341], action=1, reward=1.0, next_state=[-0.02421617 -0.0140675   0.06863862  0.12099923]\n",
      "[ episode 260 ][ timestamp 21 ] state=[-0.02421617 -0.0140675   0.06863862  0.12099923], action=0, reward=1.0, next_state=[-0.02449752 -0.21010229  0.07105861  0.43452292]\n",
      "[ episode 260 ][ timestamp 22 ] state=[-0.02449752 -0.21010229  0.07105861  0.43452292], action=1, reward=1.0, next_state=[-0.02869957 -0.01605449  0.07974906  0.16505942]\n",
      "[ episode 260 ][ timestamp 23 ] state=[-0.02869957 -0.01605449  0.07974906  0.16505942], action=0, reward=1.0, next_state=[-0.02902066 -0.2122221   0.08305025  0.48179701]\n",
      "[ episode 260 ][ timestamp 24 ] state=[-0.02902066 -0.2122221   0.08305025  0.48179701], action=1, reward=1.0, next_state=[-0.0332651  -0.01836457  0.09268619  0.21640168]\n",
      "[ episode 260 ][ timestamp 25 ] state=[-0.0332651  -0.01836457  0.09268619  0.21640168], action=0, reward=1.0, next_state=[-0.03363239 -0.2146809   0.09701423  0.53682294]\n",
      "[ episode 260 ][ timestamp 26 ] state=[-0.03363239 -0.2146809   0.09701423  0.53682294], action=0, reward=1.0, next_state=[-0.03792601 -0.41102344  0.10775068  0.85842936]\n",
      "[ episode 260 ][ timestamp 27 ] state=[-0.03792601 -0.41102344  0.10775068  0.85842936], action=0, reward=1.0, next_state=[-0.04614648 -0.60743523  0.12491927  1.18295585]\n",
      "[ episode 260 ][ timestamp 28 ] state=[-0.04614648 -0.60743523  0.12491927  1.18295585], action=0, reward=1.0, next_state=[-0.05829518 -0.80393682  0.14857839  1.51204227]\n",
      "[ episode 260 ][ timestamp 29 ] state=[-0.05829518 -0.80393682  0.14857839  1.51204227], action=1, reward=1.0, next_state=[-0.07437392 -0.61089382  0.17881923  1.26918955]\n",
      "[ episode 260 ][ timestamp 30 ] state=[-0.07437392 -0.61089382  0.17881923  1.26918955], action=0, reward=1.0, next_state=[-0.08659179 -0.80779066  0.20420303  1.61211847]\n",
      "[ episode 260 ][ timestamp 31 ] state=[-0.08659179 -0.80779066  0.20420303  1.61211847], action=1, reward=-1.0, next_state=[-0.10274761 -0.61558072  0.23644539  1.38941323]\n",
      "[ Ended! ] Episode 260: Exploration_rate=0.2730095965279488. Score=31.\n",
      "[ Experience replay ] starts\n",
      "[ episode 261 ] state=[ 0.03795989 -0.0144076  -0.01236538 -0.01228039]\n",
      "[ episode 261 ][ timestamp 1 ] state=[ 0.03795989 -0.0144076  -0.01236538 -0.01228039], action=0, reward=1.0, next_state=[ 0.03767173 -0.20935006 -0.01261099  0.27647561]\n",
      "[ episode 261 ][ timestamp 2 ] state=[ 0.03767173 -0.20935006 -0.01261099  0.27647561], action=1, reward=1.0, next_state=[ 0.03348473 -0.01405048 -0.00708148 -0.020158  ]\n",
      "[ episode 261 ][ timestamp 3 ] state=[ 0.03348473 -0.01405048 -0.00708148 -0.020158  ], action=1, reward=1.0, next_state=[ 0.03320372  0.18117231 -0.00748464 -0.31506678]\n",
      "[ episode 261 ][ timestamp 4 ] state=[ 0.03320372  0.18117231 -0.00748464 -0.31506678], action=0, reward=1.0, next_state=[ 0.03682717 -0.01384223 -0.01378597 -0.02475362]\n",
      "[ episode 261 ][ timestamp 5 ] state=[ 0.03682717 -0.01384223 -0.01378597 -0.02475362], action=1, reward=1.0, next_state=[ 0.03655032  0.18147468 -0.01428105 -0.3217541 ]\n",
      "[ episode 261 ][ timestamp 6 ] state=[ 0.03655032  0.18147468 -0.01428105 -0.3217541 ], action=1, reward=1.0, next_state=[ 0.04017982  0.37679706 -0.02071613 -0.61890628]\n",
      "[ episode 261 ][ timestamp 7 ] state=[ 0.04017982  0.37679706 -0.02071613 -0.61890628], action=1, reward=1.0, next_state=[ 0.04771576  0.57220215 -0.03309425 -0.91804113]\n",
      "[ episode 261 ][ timestamp 8 ] state=[ 0.04771576  0.57220215 -0.03309425 -0.91804113], action=0, reward=1.0, next_state=[ 0.0591598   0.37754286 -0.05145508 -0.63594001]\n",
      "[ episode 261 ][ timestamp 9 ] state=[ 0.0591598   0.37754286 -0.05145508 -0.63594001], action=1, reward=1.0, next_state=[ 0.06671066  0.57334326 -0.06417388 -0.94437301]\n",
      "[ episode 261 ][ timestamp 10 ] state=[ 0.06671066  0.57334326 -0.06417388 -0.94437301], action=1, reward=1.0, next_state=[ 0.07817752  0.76926828 -0.08306134 -1.25650975]\n",
      "[ episode 261 ][ timestamp 11 ] state=[ 0.07817752  0.76926828 -0.08306134 -1.25650975], action=0, reward=1.0, next_state=[ 0.09356289  0.57530197 -0.10819153 -0.99095535]\n",
      "[ episode 261 ][ timestamp 12 ] state=[ 0.09356289  0.57530197 -0.10819153 -0.99095535], action=1, reward=1.0, next_state=[ 0.10506893  0.77169247 -0.12801064 -1.31556494]\n",
      "[ episode 261 ][ timestamp 13 ] state=[ 0.10506893  0.77169247 -0.12801064 -1.31556494], action=1, reward=1.0, next_state=[ 0.12050278  0.96818042 -0.15432194 -1.64541774]\n",
      "[ episode 261 ][ timestamp 14 ] state=[ 0.12050278  0.96818042 -0.15432194 -1.64541774], action=0, reward=1.0, next_state=[ 0.13986639  0.77516438 -0.18723029 -1.40452517]\n",
      "[ episode 261 ][ timestamp 15 ] state=[ 0.13986639  0.77516438 -0.18723029 -1.40452517], action=1, reward=-1.0, next_state=[ 0.15536967  0.97205179 -0.2153208  -1.74941961]\n",
      "[ Ended! ] Episode 261: Exploration_rate=0.27164454854530906. Score=15.\n",
      "[ Experience replay ] starts\n",
      "[ episode 262 ] state=[0.0415863  0.03573438 0.00589904 0.01535078]\n",
      "[ episode 262 ][ timestamp 1 ] state=[0.0415863  0.03573438 0.00589904 0.01535078], action=1, reward=1.0, next_state=[ 0.04230098  0.23077124  0.00620606 -0.27546511]\n",
      "[ episode 262 ][ timestamp 2 ] state=[ 0.04230098  0.23077124  0.00620606 -0.27546511], action=1, reward=1.0, next_state=[ 0.04691641  0.4258041   0.00069676 -0.5661842 ]\n",
      "[ episode 262 ][ timestamp 3 ] state=[ 0.04691641  0.4258041   0.00069676 -0.5661842 ], action=1, reward=1.0, next_state=[ 0.05543249  0.62091627 -0.01062693 -0.85864753]\n",
      "[ episode 262 ][ timestamp 4 ] state=[ 0.05543249  0.62091627 -0.01062693 -0.85864753], action=1, reward=1.0, next_state=[ 0.06785082  0.81618136 -0.02779988 -1.15465288]\n",
      "[ episode 262 ][ timestamp 5 ] state=[ 0.06785082  0.81618136 -0.02779988 -1.15465288], action=1, reward=1.0, next_state=[ 0.08417444  1.01165459 -0.05089293 -1.45592155]\n",
      "[ episode 262 ][ timestamp 6 ] state=[ 0.08417444  1.01165459 -0.05089293 -1.45592155], action=0, reward=1.0, next_state=[ 0.10440753  0.81719288 -0.08001136 -1.17956272]\n",
      "[ episode 262 ][ timestamp 7 ] state=[ 0.10440753  0.81719288 -0.08001136 -1.17956272], action=0, reward=1.0, next_state=[ 0.12075139  0.6231957  -0.10360262 -0.91299616]\n",
      "[ episode 262 ][ timestamp 8 ] state=[ 0.12075139  0.6231957  -0.10360262 -0.91299616], action=1, reward=1.0, next_state=[ 0.13321531  0.81955509 -0.12186254 -1.23636064]\n",
      "[ episode 262 ][ timestamp 9 ] state=[ 0.13321531  0.81955509 -0.12186254 -1.23636064], action=0, reward=1.0, next_state=[ 0.14960641  0.62619136 -0.14658976 -0.98420503]\n",
      "[ episode 262 ][ timestamp 10 ] state=[ 0.14960641  0.62619136 -0.14658976 -0.98420503], action=1, reward=1.0, next_state=[ 0.16213024  0.82294045 -0.16627386 -1.31910665]\n",
      "[ episode 262 ][ timestamp 11 ] state=[ 0.16213024  0.82294045 -0.16627386 -1.31910665], action=0, reward=1.0, next_state=[ 0.17858904  0.63026463 -0.19265599 -1.08273848]\n",
      "[ episode 262 ][ timestamp 12 ] state=[ 0.17858904  0.63026463 -0.19265599 -1.08273848], action=1, reward=-1.0, next_state=[ 0.19119434  0.82733408 -0.21431076 -1.42916486]\n",
      "[ Ended! ] Episode 262: Exploration_rate=0.2702863258025825. Score=12.\n",
      "[ Experience replay ] starts\n",
      "[ episode 263 ] state=[ 0.02190356 -0.01730752 -0.03711091 -0.0082241 ]\n",
      "[ episode 263 ][ timestamp 1 ] state=[ 0.02190356 -0.01730752 -0.03711091 -0.0082241 ], action=1, reward=1.0, next_state=[ 0.02155741  0.17832646 -0.03727539 -0.31238112]\n",
      "[ episode 263 ][ timestamp 2 ] state=[ 0.02155741  0.17832646 -0.03727539 -0.31238112], action=1, reward=1.0, next_state=[ 0.02512393  0.37395907 -0.04352301 -0.61658262]\n",
      "[ episode 263 ][ timestamp 3 ] state=[ 0.02512393  0.37395907 -0.04352301 -0.61658262], action=1, reward=1.0, next_state=[ 0.03260312  0.56966117 -0.05585466 -0.92264951]\n",
      "[ episode 263 ][ timestamp 4 ] state=[ 0.03260312  0.56966117 -0.05585466 -0.92264951], action=0, reward=1.0, next_state=[ 0.04399634  0.37533654 -0.07430765 -0.64802986]\n",
      "[ episode 263 ][ timestamp 5 ] state=[ 0.04399634  0.37533654 -0.07430765 -0.64802986], action=1, reward=1.0, next_state=[ 0.05150307  0.57141076 -0.08726825 -0.96315594]\n",
      "[ episode 263 ][ timestamp 6 ] state=[ 0.05150307  0.57141076 -0.08726825 -0.96315594], action=0, reward=1.0, next_state=[ 0.06293129  0.37756289 -0.10653137 -0.69911496]\n",
      "[ episode 263 ][ timestamp 7 ] state=[ 0.06293129  0.37756289 -0.10653137 -0.69911496], action=1, reward=1.0, next_state=[ 0.07048254  0.57398792 -0.12051367 -1.02334319]\n",
      "[ episode 263 ][ timestamp 8 ] state=[ 0.07048254  0.57398792 -0.12051367 -1.02334319], action=1, reward=1.0, next_state=[ 0.0819623   0.77049089 -0.14098053 -1.3513051 ]\n",
      "[ episode 263 ][ timestamp 9 ] state=[ 0.0819623   0.77049089 -0.14098053 -1.3513051 ], action=0, reward=1.0, next_state=[ 0.09737212  0.57739276 -0.16800663 -1.10584271]\n",
      "[ episode 263 ][ timestamp 10 ] state=[ 0.09737212  0.57739276 -0.16800663 -1.10584271], action=1, reward=1.0, next_state=[ 0.10891997  0.77427677 -0.19012349 -1.44617246]\n",
      "[ episode 263 ][ timestamp 11 ] state=[ 0.10891997  0.77427677 -0.19012349 -1.44617246], action=0, reward=-1.0, next_state=[ 0.12440551  0.58193327 -0.21904694 -1.21841615]\n",
      "[ Ended! ] Episode 263: Exploration_rate=0.2689348941735696. Score=11.\n",
      "[ Experience replay ] starts\n",
      "[ episode 264 ] state=[-0.02327585  0.04242669  0.03918376 -0.04543086]\n",
      "[ episode 264 ][ timestamp 1 ] state=[-0.02327585  0.04242669  0.03918376 -0.04543086], action=1, reward=1.0, next_state=[-0.02242732  0.2369655   0.03827514 -0.32549801]\n",
      "[ episode 264 ][ timestamp 2 ] state=[-0.02242732  0.2369655   0.03827514 -0.32549801], action=1, reward=1.0, next_state=[-0.01768801  0.43152218  0.03176518 -0.60586914]\n",
      "[ episode 264 ][ timestamp 3 ] state=[-0.01768801  0.43152218  0.03176518 -0.60586914], action=1, reward=1.0, next_state=[-0.00905757  0.62618588  0.0196478  -0.88838   ]\n",
      "[ episode 264 ][ timestamp 4 ] state=[-0.00905757  0.62618588  0.0196478  -0.88838   ], action=0, reward=1.0, next_state=[ 0.00346615  0.43080287  0.0018802  -0.58958597]\n",
      "[ episode 264 ][ timestamp 5 ] state=[ 0.00346615  0.43080287  0.0018802  -0.58958597], action=1, reward=1.0, next_state=[ 0.01208221  0.62589844 -0.00991152 -0.88167603]\n",
      "[ episode 264 ][ timestamp 6 ] state=[ 0.01208221  0.62589844 -0.00991152 -0.88167603], action=0, reward=1.0, next_state=[ 0.02460018  0.43091251 -0.02754504 -0.59212544]\n",
      "[ episode 264 ][ timestamp 7 ] state=[ 0.02460018  0.43091251 -0.02754504 -0.59212544], action=1, reward=1.0, next_state=[ 0.03321843  0.62640903 -0.03938755 -0.8933562 ]\n",
      "[ episode 264 ][ timestamp 8 ] state=[ 0.03321843  0.62640903 -0.03938755 -0.8933562 ], action=0, reward=1.0, next_state=[ 0.04574661  0.43184278 -0.05725468 -0.61331013]\n",
      "[ episode 264 ][ timestamp 9 ] state=[ 0.04574661  0.43184278 -0.05725468 -0.61331013], action=1, reward=1.0, next_state=[ 0.05438346  0.62771613 -0.06952088 -0.92346239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 264 ][ timestamp 10 ] state=[ 0.05438346  0.62771613 -0.06952088 -0.92346239], action=0, reward=1.0, next_state=[ 0.06693779  0.43359874 -0.08799013 -0.65341235]\n",
      "[ episode 264 ][ timestamp 11 ] state=[ 0.06693779  0.43359874 -0.08799013 -0.65341235], action=1, reward=1.0, next_state=[ 0.07560976  0.62982861 -0.10105837 -0.97245418]\n",
      "[ episode 264 ][ timestamp 12 ] state=[ 0.07560976  0.62982861 -0.10105837 -0.97245418], action=0, reward=1.0, next_state=[ 0.08820633  0.43619727 -0.12050746 -0.71314965]\n",
      "[ episode 264 ][ timestamp 13 ] state=[ 0.08820633  0.43619727 -0.12050746 -0.71314965], action=1, reward=1.0, next_state=[ 0.09693028  0.63276328 -0.13477045 -1.04120386]\n",
      "[ episode 264 ][ timestamp 14 ] state=[ 0.09693028  0.63276328 -0.13477045 -1.04120386], action=0, reward=1.0, next_state=[ 0.10958554  0.43966379 -0.15559453 -0.79368377]\n",
      "[ episode 264 ][ timestamp 15 ] state=[ 0.10958554  0.43966379 -0.15559453 -0.79368377], action=1, reward=1.0, next_state=[ 0.11837882  0.63654016 -0.1714682  -1.13099125]\n",
      "[ episode 264 ][ timestamp 16 ] state=[ 0.11837882  0.63654016 -0.1714682  -1.13099125], action=1, reward=1.0, next_state=[ 0.13110962  0.8334409  -0.19408803 -1.47217611]\n",
      "[ episode 264 ][ timestamp 17 ] state=[ 0.13110962  0.8334409  -0.19408803 -1.47217611], action=0, reward=-1.0, next_state=[ 0.14777844  0.64114867 -0.22353155 -1.24585779]\n",
      "[ Ended! ] Episode 264: Exploration_rate=0.26759021970270175. Score=17.\n",
      "[ Experience replay ] starts\n",
      "[ episode 265 ] state=[ 0.0254334  -0.02071791 -0.02186442 -0.03282677]\n",
      "[ episode 265 ][ timestamp 1 ] state=[ 0.0254334  -0.02071791 -0.02186442 -0.03282677], action=1, reward=1.0, next_state=[ 0.02501904  0.17471065 -0.02252095 -0.33232717]\n",
      "[ episode 265 ][ timestamp 2 ] state=[ 0.02501904  0.17471065 -0.02252095 -0.33232717], action=1, reward=1.0, next_state=[ 0.02851325  0.3701458  -0.0291675  -0.63202615]\n",
      "[ episode 265 ][ timestamp 3 ] state=[ 0.02851325  0.3701458  -0.0291675  -0.63202615], action=1, reward=1.0, next_state=[ 0.03591617  0.56566229 -0.04180802 -0.93375017]\n",
      "[ episode 265 ][ timestamp 4 ] state=[ 0.03591617  0.56566229 -0.04180802 -0.93375017], action=1, reward=1.0, next_state=[ 0.04722941  0.76132257 -0.06048302 -1.2392721 ]\n",
      "[ episode 265 ][ timestamp 5 ] state=[ 0.04722941  0.76132257 -0.06048302 -1.2392721 ], action=0, reward=1.0, next_state=[ 0.06245586  0.56702729 -0.08526847 -0.96613328]\n",
      "[ episode 265 ][ timestamp 6 ] state=[ 0.06245586  0.56702729 -0.08526847 -0.96613328], action=0, reward=1.0, next_state=[ 0.07379641  0.37314763 -0.10459113 -0.70140895]\n",
      "[ episode 265 ][ timestamp 7 ] state=[ 0.07379641  0.37314763 -0.10459113 -0.70140895], action=1, reward=1.0, next_state=[ 0.08125936  0.56955191 -0.11861931 -1.0250992 ]\n",
      "[ episode 265 ][ timestamp 8 ] state=[ 0.08125936  0.56955191 -0.11861931 -1.0250992 ], action=1, reward=1.0, next_state=[ 0.0926504   0.76603646 -0.13912129 -1.35254732]\n",
      "[ episode 265 ][ timestamp 9 ] state=[ 0.0926504   0.76603646 -0.13912129 -1.35254732], action=1, reward=1.0, next_state=[ 0.10797113  0.96260388 -0.16617224 -1.68531952]\n",
      "[ episode 265 ][ timestamp 10 ] state=[ 0.10797113  0.96260388 -0.16617224 -1.68531952], action=0, reward=1.0, next_state=[ 0.12722321  0.76974919 -0.19987863 -1.44865241]\n",
      "[ episode 265 ][ timestamp 11 ] state=[ 0.12722321  0.76974919 -0.19987863 -1.44865241], action=0, reward=-1.0, next_state=[ 0.14261819  0.5775659  -0.22885168 -1.22449064]\n",
      "[ Ended! ] Episode 265: Exploration_rate=0.2662522686041882. Score=11.\n",
      "[ Experience replay ] starts\n",
      "[ episode 266 ] state=[ 0.03780572 -0.03652694 -0.00262734  0.0240608 ]\n",
      "[ episode 266 ][ timestamp 1 ] state=[ 0.03780572 -0.03652694 -0.00262734  0.0240608 ], action=1, reward=1.0, next_state=[ 0.03707518  0.1586326  -0.00214612 -0.26944992]\n",
      "[ episode 266 ][ timestamp 2 ] state=[ 0.03707518  0.1586326  -0.00214612 -0.26944992], action=1, reward=1.0, next_state=[ 0.04024783  0.35378511 -0.00753512 -0.56280898]\n",
      "[ episode 266 ][ timestamp 3 ] state=[ 0.04024783  0.35378511 -0.00753512 -0.56280898], action=1, reward=1.0, next_state=[ 0.04732353  0.54901198 -0.0187913  -0.85785627]\n",
      "[ episode 266 ][ timestamp 4 ] state=[ 0.04732353  0.54901198 -0.0187913  -0.85785627], action=0, reward=1.0, next_state=[ 0.05830377  0.354151   -0.03594842 -0.57114073]\n",
      "[ episode 266 ][ timestamp 5 ] state=[ 0.05830377  0.354151   -0.03594842 -0.57114073], action=1, reward=1.0, next_state=[ 0.06538679  0.54975814 -0.04737124 -0.87492843]\n",
      "[ episode 266 ][ timestamp 6 ] state=[ 0.06538679  0.54975814 -0.04737124 -0.87492843], action=0, reward=1.0, next_state=[ 0.07638195  0.35531108 -0.06486981 -0.59750698]\n",
      "[ episode 266 ][ timestamp 7 ] state=[ 0.07638195  0.35531108 -0.06486981 -0.59750698], action=1, reward=1.0, next_state=[ 0.08348818  0.55127792 -0.07681995 -0.90989732]\n",
      "[ episode 266 ][ timestamp 8 ] state=[ 0.08348818  0.55127792 -0.07681995 -0.90989732], action=1, reward=1.0, next_state=[ 0.09451373  0.74735077 -0.09501789 -1.22570206]\n",
      "[ episode 266 ][ timestamp 9 ] state=[ 0.09451373  0.74735077 -0.09501789 -1.22570206], action=0, reward=1.0, next_state=[ 0.10946075  0.55357189 -0.11953193 -0.96423814]\n",
      "[ episode 266 ][ timestamp 10 ] state=[ 0.10946075  0.55357189 -0.11953193 -0.96423814], action=1, reward=1.0, next_state=[ 0.12053219  0.75007919 -0.1388167  -1.29195461]\n",
      "[ episode 266 ][ timestamp 11 ] state=[ 0.12053219  0.75007919 -0.1388167  -1.29195461], action=1, reward=1.0, next_state=[ 0.13553377  0.94666576 -0.16465579 -1.624679  ]\n",
      "[ episode 266 ][ timestamp 12 ] state=[ 0.13553377  0.94666576 -0.16465579 -1.624679  ], action=0, reward=1.0, next_state=[ 0.15446709  0.75381977 -0.19714937 -1.38751279]\n",
      "[ episode 266 ][ timestamp 13 ] state=[ 0.15446709  0.75381977 -0.19714937 -1.38751279], action=1, reward=-1.0, next_state=[ 0.16954348  0.95077562 -0.22489963 -1.73481085]\n",
      "[ Ended! ] Episode 266: Exploration_rate=0.2649210072611673. Score=13.\n",
      "[ Experience replay ] starts\n",
      "[ episode 267 ] state=[-0.01839154  0.02767723 -0.00143249 -0.03258242]\n",
      "[ episode 267 ][ timestamp 1 ] state=[-0.01839154  0.02767723 -0.00143249 -0.03258242], action=1, reward=1.0, next_state=[-0.017838    0.22281969 -0.00208414 -0.32571696]\n",
      "[ episode 267 ][ timestamp 2 ] state=[-0.017838    0.22281969 -0.00208414 -0.32571696], action=1, reward=1.0, next_state=[-0.01338161  0.41797125 -0.00859848 -0.61905641]\n",
      "[ episode 267 ][ timestamp 3 ] state=[-0.01338161  0.41797125 -0.00859848 -0.61905641], action=0, reward=1.0, next_state=[-0.00502218  0.22297045 -0.02097961 -0.32909394]\n",
      "[ episode 267 ][ timestamp 4 ] state=[-0.00502218  0.22297045 -0.02097961 -0.32909394], action=1, reward=1.0, next_state=[-5.62771793e-04  4.18384682e-01 -2.75614842e-02 -6.28318334e-01]\n",
      "[ episode 267 ][ timestamp 5 ] state=[-5.62771793e-04  4.18384682e-01 -2.75614842e-02 -6.28318334e-01], action=1, reward=1.0, next_state=[ 0.00780492  0.61388023 -0.04012785 -0.92955233]\n",
      "[ episode 267 ][ timestamp 6 ] state=[ 0.00780492  0.61388023 -0.04012785 -0.92955233], action=0, reward=1.0, next_state=[ 0.02008253  0.41932224 -0.0587189  -0.64974471]\n",
      "[ episode 267 ][ timestamp 7 ] state=[ 0.02008253  0.41932224 -0.0587189  -0.64974471], action=1, reward=1.0, next_state=[ 0.02846897  0.61521084 -0.07171379 -0.96032464]\n",
      "[ episode 267 ][ timestamp 8 ] state=[ 0.02846897  0.61521084 -0.07171379 -0.96032464], action=0, reward=1.0, next_state=[ 0.04077319  0.42112229 -0.09092028 -0.69100591]\n",
      "[ episode 267 ][ timestamp 9 ] state=[ 0.04077319  0.42112229 -0.09092028 -0.69100591], action=1, reward=1.0, next_state=[ 0.04919563  0.61738032 -0.1047404  -1.01087076]\n",
      "[ episode 267 ][ timestamp 10 ] state=[ 0.04919563  0.61738032 -0.1047404  -1.01087076], action=1, reward=1.0, next_state=[ 0.06154324  0.81373217 -0.12495782 -1.33452185]\n",
      "[ episode 267 ][ timestamp 11 ] state=[ 0.06154324  0.81373217 -0.12495782 -1.33452185], action=0, reward=1.0, next_state=[ 0.07781788  0.6203869  -0.15164825 -1.08340731]\n",
      "[ episode 267 ][ timestamp 12 ] state=[ 0.07781788  0.6203869  -0.15164825 -1.08340731], action=0, reward=1.0, next_state=[ 0.09022562  0.42755563 -0.1733164  -0.84189386]\n",
      "[ episode 267 ][ timestamp 13 ] state=[ 0.09022562  0.42755563 -0.1733164  -0.84189386], action=1, reward=1.0, next_state=[ 0.09877673  0.62456576 -0.19015428 -1.18368204]\n",
      "[ episode 267 ][ timestamp 14 ] state=[ 0.09877673  0.62456576 -0.19015428 -1.18368204], action=1, reward=-1.0, next_state=[ 0.11126805  0.82157619 -0.21382792 -1.52944008]\n",
      "[ Ended! ] Episode 267: Exploration_rate=0.26359640222486147. Score=14.\n",
      "[ Experience replay ] starts\n",
      "[ episode 268 ] state=[-0.04235456  0.04385872 -0.02321895 -0.0468943 ]\n",
      "[ episode 268 ][ timestamp 1 ] state=[-0.04235456  0.04385872 -0.02321895 -0.0468943 ], action=1, reward=1.0, next_state=[-0.04147739  0.23930579 -0.02415683 -0.34681163]\n",
      "[ episode 268 ][ timestamp 2 ] state=[-0.04147739  0.23930579 -0.02415683 -0.34681163], action=1, reward=1.0, next_state=[-0.03669127  0.43476287 -0.03109307 -0.64701314]\n",
      "[ episode 268 ][ timestamp 3 ] state=[-0.03669127  0.43476287 -0.03109307 -0.64701314], action=0, reward=1.0, next_state=[-0.02799601  0.24008762 -0.04403333 -0.3642813 ]\n",
      "[ episode 268 ][ timestamp 4 ] state=[-0.02799601  0.24008762 -0.04403333 -0.3642813 ], action=1, reward=1.0, next_state=[-0.02319426  0.43580681 -0.05131896 -0.67051713]\n",
      "[ episode 268 ][ timestamp 5 ] state=[-0.02319426  0.43580681 -0.05131896 -0.67051713], action=0, reward=1.0, next_state=[-0.01447812  0.24143448 -0.0647293  -0.39442363]\n",
      "[ episode 268 ][ timestamp 6 ] state=[-0.01447812  0.24143448 -0.0647293  -0.39442363], action=1, reward=1.0, next_state=[-0.00964944  0.43741234 -0.07261777 -0.70679191]\n",
      "[ episode 268 ][ timestamp 7 ] state=[-0.00964944  0.43741234 -0.07261777 -0.70679191], action=0, reward=1.0, next_state=[-0.00090119  0.24336756 -0.08675361 -0.43782272]\n",
      "[ episode 268 ][ timestamp 8 ] state=[-0.00090119  0.24336756 -0.08675361 -0.43782272], action=0, reward=1.0, next_state=[ 0.00396616  0.0495738  -0.09551006 -0.17369886]\n",
      "[ episode 268 ][ timestamp 9 ] state=[ 0.00396616  0.0495738  -0.09551006 -0.17369886], action=1, reward=1.0, next_state=[ 0.00495764  0.24592372 -0.09898404 -0.4949187 ]\n",
      "[ episode 268 ][ timestamp 10 ] state=[ 0.00495764  0.24592372 -0.09898404 -0.4949187 ], action=0, reward=1.0, next_state=[ 0.00987611  0.05232681 -0.10888241 -0.23499862]\n",
      "[ episode 268 ][ timestamp 11 ] state=[ 0.00987611  0.05232681 -0.10888241 -0.23499862], action=1, reward=1.0, next_state=[ 0.01092265  0.24882235 -0.11358239 -0.55994472]\n",
      "[ episode 268 ][ timestamp 12 ] state=[ 0.01092265  0.24882235 -0.11358239 -0.55994472], action=1, reward=1.0, next_state=[ 0.0158991   0.44533989 -0.12478128 -0.88614308]\n",
      "[ episode 268 ][ timestamp 13 ] state=[ 0.0158991   0.44533989 -0.12478128 -0.88614308], action=0, reward=1.0, next_state=[ 0.02480589  0.25211262 -0.14250414 -0.63514629]\n",
      "[ episode 268 ][ timestamp 14 ] state=[ 0.02480589  0.25211262 -0.14250414 -0.63514629], action=0, reward=1.0, next_state=[ 0.02984815  0.0592356  -0.15520707 -0.39051797]\n",
      "[ episode 268 ][ timestamp 15 ] state=[ 0.02984815  0.0592356  -0.15520707 -0.39051797], action=0, reward=1.0, next_state=[ 0.03103286 -0.13338224 -0.16301743 -0.15051215]\n",
      "[ episode 268 ][ timestamp 16 ] state=[ 0.03103286 -0.13338224 -0.16301743 -0.15051215], action=1, reward=1.0, next_state=[ 0.02836521  0.06365308 -0.16602767 -0.48986184]\n",
      "[ episode 268 ][ timestamp 17 ] state=[ 0.02836521  0.06365308 -0.16602767 -0.48986184], action=1, reward=1.0, next_state=[ 0.02963828  0.26068015 -0.17582491 -0.82992666]\n",
      "[ episode 268 ][ timestamp 18 ] state=[ 0.02963828  0.26068015 -0.17582491 -0.82992666], action=0, reward=1.0, next_state=[ 0.03485188  0.06834121 -0.19242344 -0.5972929 ]\n",
      "[ episode 268 ][ timestamp 19 ] state=[ 0.03485188  0.06834121 -0.19242344 -0.5972929 ], action=1, reward=1.0, next_state=[ 0.0362187   0.26556085 -0.2043693  -0.94388646]\n",
      "[ episode 268 ][ timestamp 20 ] state=[ 0.0362187   0.26556085 -0.2043693  -0.94388646], action=1, reward=-1.0, next_state=[ 0.04152992  0.4627621  -0.22324703 -1.29319964]\n",
      "[ Ended! ] Episode 268: Exploration_rate=0.26227842021373715. Score=20.\n",
      "[ Experience replay ] starts\n",
      "[ episode 269 ] state=[ 0.0456173   0.02993018  0.0205983  -0.02933782]\n",
      "[ episode 269 ][ timestamp 1 ] state=[ 0.0456173   0.02993018  0.0205983  -0.02933782], action=0, reward=1.0, next_state=[ 0.0462159  -0.16548101  0.02001154  0.26977226]\n",
      "[ episode 269 ][ timestamp 2 ] state=[ 0.0462159  -0.16548101  0.02001154  0.26977226], action=0, reward=1.0, next_state=[ 0.04290628 -0.36088273  0.02540698  0.56869916]\n",
      "[ episode 269 ][ timestamp 3 ] state=[ 0.04290628 -0.36088273  0.02540698  0.56869916], action=1, reward=1.0, next_state=[ 0.03568863 -0.16612618  0.03678097  0.28412746]\n",
      "[ episode 269 ][ timestamp 4 ] state=[ 0.03568863 -0.16612618  0.03678097  0.28412746], action=1, reward=1.0, next_state=[0.0323661  0.0284524  0.04246352 0.00326816]\n",
      "[ episode 269 ][ timestamp 5 ] state=[0.0323661  0.0284524  0.04246352 0.00326816], action=1, reward=1.0, next_state=[ 0.03293515  0.22294045  0.04252888 -0.27572041]\n",
      "[ episode 269 ][ timestamp 6 ] state=[ 0.03293515  0.22294045  0.04252888 -0.27572041], action=0, reward=1.0, next_state=[0.03739396 0.02723834 0.03701447 0.03006704]\n",
      "[ episode 269 ][ timestamp 7 ] state=[0.03739396 0.02723834 0.03701447 0.03006704], action=1, reward=1.0, next_state=[ 0.03793873  0.22181047  0.03761581 -0.25071148]\n",
      "[ episode 269 ][ timestamp 8 ] state=[ 0.03793873  0.22181047  0.03761581 -0.25071148], action=0, reward=1.0, next_state=[0.04237494 0.02617212 0.03260158 0.05359491]\n",
      "[ episode 269 ][ timestamp 9 ] state=[0.04237494 0.02617212 0.03260158 0.05359491], action=1, reward=1.0, next_state=[ 0.04289838  0.2208118   0.03367348 -0.22862631]\n",
      "[ episode 269 ][ timestamp 10 ] state=[ 0.04289838  0.2208118   0.03367348 -0.22862631], action=0, reward=1.0, next_state=[0.04731462 0.02522523 0.02910096 0.07448537]\n",
      "[ episode 269 ][ timestamp 11 ] state=[0.04731462 0.02522523 0.02910096 0.07448537], action=1, reward=1.0, next_state=[ 0.04781912  0.21991816  0.03059066 -0.20887591]\n",
      "[ episode 269 ][ timestamp 12 ] state=[ 0.04781912  0.21991816  0.03059066 -0.20887591], action=0, reward=1.0, next_state=[0.05221748 0.02437246 0.02641314 0.09329767]\n",
      "[ episode 269 ][ timestamp 13 ] state=[0.05221748 0.02437246 0.02641314 0.09329767], action=0, reward=1.0, next_state=[ 0.05270493 -0.17111792  0.0282791   0.39419551]\n",
      "[ episode 269 ][ timestamp 14 ] state=[ 0.05270493 -0.17111792  0.0282791   0.39419551], action=1, reward=1.0, next_state=[0.04928257 0.02359158 0.03616301 0.11056098]\n",
      "[ episode 269 ][ timestamp 15 ] state=[0.04928257 0.02359158 0.03616301 0.11056098], action=0, reward=1.0, next_state=[ 0.04975441 -0.17202941  0.03837423  0.41443022]\n",
      "[ episode 269 ][ timestamp 16 ] state=[ 0.04975441 -0.17202941  0.03837423  0.41443022], action=1, reward=1.0, next_state=[0.04631382 0.02252822 0.04666283 0.13408789]\n",
      "[ episode 269 ][ timestamp 17 ] state=[0.04631382 0.02252822 0.04666283 0.13408789], action=1, reward=1.0, next_state=[ 0.04676438  0.21695179  0.04934459 -0.14351612]\n",
      "[ episode 269 ][ timestamp 18 ] state=[ 0.04676438  0.21695179  0.04934459 -0.14351612], action=0, reward=1.0, next_state=[0.05110342 0.02115916 0.04647427 0.16431677]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 269 ][ timestamp 19 ] state=[0.05110342 0.02115916 0.04647427 0.16431677], action=1, reward=1.0, next_state=[ 0.0515266   0.21558608  0.0497606  -0.1133502 ]\n",
      "[ episode 269 ][ timestamp 20 ] state=[ 0.0515266   0.21558608  0.0497606  -0.1133502 ], action=1, reward=1.0, next_state=[ 0.05583832  0.409961    0.0474936  -0.38992811]\n",
      "[ episode 269 ][ timestamp 21 ] state=[ 0.05583832  0.409961    0.0474936  -0.38992811], action=0, reward=1.0, next_state=[ 0.06403754  0.21419828  0.03969504 -0.08265727]\n",
      "[ episode 269 ][ timestamp 22 ] state=[ 0.06403754  0.21419828  0.03969504 -0.08265727], action=1, reward=1.0, next_state=[ 0.06832151  0.40872939  0.03804189 -0.3625568 ]\n",
      "[ episode 269 ][ timestamp 23 ] state=[ 0.06832151  0.40872939  0.03804189 -0.3625568 ], action=0, reward=1.0, next_state=[ 0.0764961   0.21308797  0.03079076 -0.05812538]\n",
      "[ episode 269 ][ timestamp 24 ] state=[ 0.0764961   0.21308797  0.03079076 -0.05812538], action=1, reward=1.0, next_state=[ 0.08075785  0.40775522  0.02962825 -0.34093679]\n",
      "[ episode 269 ][ timestamp 25 ] state=[ 0.08075785  0.40775522  0.02962825 -0.34093679], action=0, reward=1.0, next_state=[ 0.08891296  0.21222452  0.02280951 -0.03906003]\n",
      "[ episode 269 ][ timestamp 26 ] state=[ 0.08891296  0.21222452  0.02280951 -0.03906003], action=1, reward=1.0, next_state=[ 0.09315745  0.40701208  0.02202831 -0.32445995]\n",
      "[ episode 269 ][ timestamp 27 ] state=[ 0.09315745  0.40701208  0.02202831 -0.32445995], action=0, reward=1.0, next_state=[ 0.10129769  0.21158351  0.01553911 -0.02491242]\n",
      "[ episode 269 ][ timestamp 28 ] state=[ 0.10129769  0.21158351  0.01553911 -0.02491242], action=1, reward=1.0, next_state=[ 0.10552936  0.40647921  0.01504086 -0.31265236]\n",
      "[ episode 269 ][ timestamp 29 ] state=[ 0.10552936  0.40647921  0.01504086 -0.31265236], action=0, reward=1.0, next_state=[ 0.11365895  0.21114625  0.00878782 -0.01526422]\n",
      "[ episode 269 ][ timestamp 30 ] state=[ 0.11365895  0.21114625  0.00878782 -0.01526422], action=1, reward=1.0, next_state=[ 0.11788187  0.40614108  0.00848253 -0.30516158]\n",
      "[ episode 269 ][ timestamp 31 ] state=[ 0.11788187  0.40614108  0.00848253 -0.30516158], action=0, reward=1.0, next_state=[ 0.12600469  0.21089928  0.0023793  -0.00981559]\n",
      "[ episode 269 ][ timestamp 32 ] state=[ 0.12600469  0.21089928  0.0023793  -0.00981559], action=1, reward=1.0, next_state=[ 0.13022268  0.40598703  0.00218299 -0.30174687]\n",
      "[ episode 269 ][ timestamp 33 ] state=[ 0.13022268  0.40598703  0.00218299 -0.30174687], action=0, reward=1.0, next_state=[ 0.13834242  0.21083404 -0.00385195 -0.00837627]\n",
      "[ episode 269 ][ timestamp 34 ] state=[ 0.13834242  0.21083404 -0.00385195 -0.00837627], action=1, reward=1.0, next_state=[ 0.1425591   0.40601102 -0.00401947 -0.30227204]\n",
      "[ episode 269 ][ timestamp 35 ] state=[ 0.1425591   0.40601102 -0.00401947 -0.30227204], action=0, reward=1.0, next_state=[ 0.15067932  0.21094658 -0.01006491 -0.01085948]\n",
      "[ episode 269 ][ timestamp 36 ] state=[ 0.15067932  0.21094658 -0.01006491 -0.01085948], action=1, reward=1.0, next_state=[ 0.15489825  0.40621142 -0.0102821  -0.30670093]\n",
      "[ episode 269 ][ timestamp 37 ] state=[ 0.15489825  0.40621142 -0.0102821  -0.30670093], action=0, reward=1.0, next_state=[ 0.16302248  0.21123748 -0.01641612 -0.01727837]\n",
      "[ episode 269 ][ timestamp 38 ] state=[ 0.16302248  0.21123748 -0.01641612 -0.01727837], action=1, reward=1.0, next_state=[ 0.16724723  0.40659097 -0.01676169 -0.31509524]\n",
      "[ episode 269 ][ timestamp 39 ] state=[ 0.16724723  0.40659097 -0.01676169 -0.31509524], action=0, reward=1.0, next_state=[ 0.17537905  0.21171174 -0.0230636  -0.02774517]\n",
      "[ episode 269 ][ timestamp 40 ] state=[ 0.17537905  0.21171174 -0.0230636  -0.02774517], action=1, reward=1.0, next_state=[ 0.17961328  0.40715672 -0.0236185  -0.32761476]\n",
      "[ episode 269 ][ timestamp 41 ] state=[ 0.17961328  0.40715672 -0.0236185  -0.32761476], action=0, reward=1.0, next_state=[ 0.18775642  0.21237884 -0.03017079 -0.04247262]\n",
      "[ episode 269 ][ timestamp 42 ] state=[ 0.18775642  0.21237884 -0.03017079 -0.04247262], action=0, reward=1.0, next_state=[ 0.19200399  0.01770223 -0.03102025  0.24054053]\n",
      "[ episode 269 ][ timestamp 43 ] state=[ 0.19200399  0.01770223 -0.03102025  0.24054053], action=1, reward=1.0, next_state=[ 0.19235804  0.21325326 -0.02620944 -0.06176338]\n",
      "[ episode 269 ][ timestamp 44 ] state=[ 0.19235804  0.21325326 -0.02620944 -0.06176338], action=1, reward=1.0, next_state=[ 0.1966231   0.408741   -0.0274447  -0.36259898]\n",
      "[ episode 269 ][ timestamp 45 ] state=[ 0.1966231   0.408741   -0.0274447  -0.36259898], action=0, reward=1.0, next_state=[ 0.20479792  0.21401965 -0.03469668 -0.07869469]\n",
      "[ episode 269 ][ timestamp 46 ] state=[ 0.20479792  0.21401965 -0.03469668 -0.07869469], action=1, reward=1.0, next_state=[ 0.20907832  0.40962137 -0.03627058 -0.38211945]\n",
      "[ episode 269 ][ timestamp 47 ] state=[ 0.20907832  0.40962137 -0.03627058 -0.38211945], action=0, reward=1.0, next_state=[ 0.21727074  0.2150327  -0.04391297 -0.10108962]\n",
      "[ episode 269 ][ timestamp 48 ] state=[ 0.21727074  0.2150327  -0.04391297 -0.10108962], action=0, reward=1.0, next_state=[ 0.2215714   0.0205667  -0.04593476  0.17742191]\n",
      "[ episode 269 ][ timestamp 49 ] state=[ 0.2215714   0.0205667  -0.04593476  0.17742191], action=1, reward=1.0, next_state=[ 0.22198273  0.21631488 -0.04238632 -0.12939072]\n",
      "[ episode 269 ][ timestamp 50 ] state=[ 0.22198273  0.21631488 -0.04238632 -0.12939072], action=1, reward=1.0, next_state=[ 0.22630903  0.41201758 -0.04497413 -0.43513896]\n",
      "[ episode 269 ][ timestamp 51 ] state=[ 0.22630903  0.41201758 -0.04497413 -0.43513896], action=0, reward=1.0, next_state=[ 0.23454938  0.21756021 -0.05367691 -0.15696579]\n",
      "[ episode 269 ][ timestamp 52 ] state=[ 0.23454938  0.21756021 -0.05367691 -0.15696579], action=0, reward=1.0, next_state=[ 0.23890059  0.02324623 -0.05681623  0.11831197]\n",
      "[ episode 269 ][ timestamp 53 ] state=[ 0.23890059  0.02324623 -0.05681623  0.11831197], action=1, reward=1.0, next_state=[ 0.23936551  0.21913426 -0.05444999 -0.19174094]\n",
      "[ episode 269 ][ timestamp 54 ] state=[ 0.23936551  0.21913426 -0.05444999 -0.19174094], action=1, reward=1.0, next_state=[ 0.2437482   0.41499116 -0.05828481 -0.50109128]\n",
      "[ episode 269 ][ timestamp 55 ] state=[ 0.2437482   0.41499116 -0.05828481 -0.50109128], action=1, reward=1.0, next_state=[ 0.25204802  0.6108842  -0.06830663 -0.81155791]\n",
      "[ episode 269 ][ timestamp 56 ] state=[ 0.25204802  0.6108842  -0.06830663 -0.81155791], action=0, reward=1.0, next_state=[ 0.2642657   0.41676114 -0.08453779 -0.54111889]\n",
      "[ episode 269 ][ timestamp 57 ] state=[ 0.2642657   0.41676114 -0.08453779 -0.54111889], action=0, reward=1.0, next_state=[ 0.27260093  0.22292281 -0.09536017 -0.27622428]\n",
      "[ episode 269 ][ timestamp 58 ] state=[ 0.27260093  0.22292281 -0.09536017 -0.27622428], action=1, reward=1.0, next_state=[ 0.27705938  0.41926676 -0.10088466 -0.59739552]\n",
      "[ episode 269 ][ timestamp 59 ] state=[ 0.27705938  0.41926676 -0.10088466 -0.59739552], action=0, reward=1.0, next_state=[ 0.28544472  0.22569045 -0.11283257 -0.33811723]\n",
      "[ episode 269 ][ timestamp 60 ] state=[ 0.28544472  0.22569045 -0.11283257 -0.33811723], action=1, reward=1.0, next_state=[ 0.28995853  0.42222202 -0.11959491 -0.66414245]\n",
      "[ episode 269 ][ timestamp 61 ] state=[ 0.28995853  0.42222202 -0.11959491 -0.66414245], action=0, reward=1.0, next_state=[ 0.29840297  0.22894889 -0.13287776 -0.41138072]\n",
      "[ episode 269 ][ timestamp 62 ] state=[ 0.29840297  0.22894889 -0.13287776 -0.41138072], action=0, reward=1.0, next_state=[ 0.30298194  0.03593627 -0.14110537 -0.16336518]\n",
      "[ episode 269 ][ timestamp 63 ] state=[ 0.30298194  0.03593627 -0.14110537 -0.16336518], action=1, reward=1.0, next_state=[ 0.30370067  0.23276661 -0.14437268 -0.49702372]\n",
      "[ episode 269 ][ timestamp 64 ] state=[ 0.30370067  0.23276661 -0.14437268 -0.49702372], action=0, reward=1.0, next_state=[ 0.308356    0.03994389 -0.15431315 -0.253097  ]\n",
      "[ episode 269 ][ timestamp 65 ] state=[ 0.308356    0.03994389 -0.15431315 -0.253097  ], action=1, reward=1.0, next_state=[ 0.30915488  0.2368939  -0.15937509 -0.59019981]\n",
      "[ episode 269 ][ timestamp 66 ] state=[ 0.30915488  0.2368939  -0.15937509 -0.59019981], action=0, reward=1.0, next_state=[ 0.31389276  0.04432007 -0.17117909 -0.35165806]\n",
      "[ episode 269 ][ timestamp 67 ] state=[ 0.31389276  0.04432007 -0.17117909 -0.35165806], action=0, reward=1.0, next_state=[ 0.31477916 -0.1480068  -0.17821225 -0.11746539]\n",
      "[ episode 269 ][ timestamp 68 ] state=[ 0.31477916 -0.1480068  -0.17821225 -0.11746539], action=1, reward=1.0, next_state=[ 0.31181902  0.04916145 -0.18056156 -0.4606512 ]\n",
      "[ episode 269 ][ timestamp 69 ] state=[ 0.31181902  0.04916145 -0.18056156 -0.4606512 ], action=1, reward=1.0, next_state=[ 0.31280225  0.24631459 -0.18977458 -0.80437036]\n",
      "[ episode 269 ][ timestamp 70 ] state=[ 0.31280225  0.24631459 -0.18977458 -0.80437036], action=0, reward=1.0, next_state=[ 0.31772854  0.05423047 -0.20586199 -0.5768764 ]\n",
      "[ episode 269 ][ timestamp 71 ] state=[ 0.31772854  0.05423047 -0.20586199 -0.5768764 ], action=0, reward=-1.0, next_state=[ 0.31881315 -0.137502   -0.21739952 -0.35544714]\n",
      "[ Ended! ] Episode 269: Exploration_rate=0.2609670281126685. Score=71.\n",
      "[ Experience replay ] starts\n",
      "[ episode 270 ] state=[-0.04793079 -0.01765611  0.02067728 -0.00844789]\n",
      "[ episode 270 ][ timestamp 1 ] state=[-0.04793079 -0.01765611  0.02067728 -0.00844789], action=0, reward=1.0, next_state=[-0.04828391 -0.2130684   0.02050832  0.29068658]\n",
      "[ episode 270 ][ timestamp 2 ] state=[-0.04828391 -0.2130684   0.02050832  0.29068658], action=0, reward=1.0, next_state=[-0.05254528 -0.40847669  0.02632205  0.5897664 ]\n",
      "[ episode 270 ][ timestamp 3 ] state=[-0.05254528 -0.40847669  0.02632205  0.5897664 ], action=1, reward=1.0, next_state=[-0.06071481 -0.213733    0.03811738  0.30548984]\n",
      "[ episode 270 ][ timestamp 4 ] state=[-0.06071481 -0.213733    0.03811738  0.30548984], action=0, reward=1.0, next_state=[-0.06498947 -0.40937682  0.04422718  0.6099462 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 270 ][ timestamp 5 ] state=[-0.06498947 -0.40937682  0.04422718  0.6099462 ], action=1, reward=1.0, next_state=[-0.07317701 -0.2149001   0.0564261   0.33151492]\n",
      "[ episode 270 ][ timestamp 6 ] state=[-0.07317701 -0.2149001   0.0564261   0.33151492], action=1, reward=1.0, next_state=[-0.07747501 -0.02062483  0.0630564   0.05714628]\n",
      "[ episode 270 ][ timestamp 7 ] state=[-0.07747501 -0.02062483  0.0630564   0.05714628], action=0, reward=1.0, next_state=[-0.07788751 -0.21659156  0.06419932  0.36903849]\n",
      "[ episode 270 ][ timestamp 8 ] state=[-0.07788751 -0.21659156  0.06419932  0.36903849], action=0, reward=1.0, next_state=[-0.08221934 -0.41256416  0.07158009  0.68125344]\n",
      "[ episode 270 ][ timestamp 9 ] state=[-0.08221934 -0.41256416  0.07158009  0.68125344], action=0, reward=1.0, next_state=[-0.09047062 -0.60860341  0.08520516  0.99558589]\n",
      "[ episode 270 ][ timestamp 10 ] state=[-0.09047062 -0.60860341  0.08520516  0.99558589], action=1, reward=1.0, next_state=[-0.10264269 -0.41471797  0.10511688  0.73083281]\n",
      "[ episode 270 ][ timestamp 11 ] state=[-0.10264269 -0.41471797  0.10511688  0.73083281], action=0, reward=1.0, next_state=[-0.11093705 -0.61112349  0.11973354  1.05466242]\n",
      "[ episode 270 ][ timestamp 12 ] state=[-0.11093705 -0.61112349  0.11973354  1.05466242], action=0, reward=1.0, next_state=[-0.12315952 -0.80761148  0.14082679  1.38240188]\n",
      "[ episode 270 ][ timestamp 13 ] state=[-0.12315952 -0.80761148  0.14082679  1.38240188], action=1, reward=1.0, next_state=[-0.13931175 -0.61449928  0.16847482  1.13686757]\n",
      "[ episode 270 ][ timestamp 14 ] state=[-0.13931175 -0.61449928  0.16847482  1.13686757], action=0, reward=1.0, next_state=[-0.15160173 -0.81137548  0.19121217  1.47729833]\n",
      "[ episode 270 ][ timestamp 15 ] state=[-0.15160173 -0.81137548  0.19121217  1.47729833], action=0, reward=-1.0, next_state=[-0.16782924 -1.00824839  0.22075814  1.82309999]\n",
      "[ Ended! ] Episode 270: Exploration_rate=0.25966219297210513. Score=15.\n",
      "[ Experience replay ] starts\n",
      "[ episode 271 ] state=[-0.0458072   0.02028931 -0.02819912  0.03360974]\n",
      "[ episode 271 ][ timestamp 1 ] state=[-0.0458072   0.02028931 -0.02819912  0.03360974], action=1, reward=1.0, next_state=[-0.04540142  0.21580406 -0.02752693 -0.26783524]\n",
      "[ episode 271 ][ timestamp 2 ] state=[-0.04540142  0.21580406 -0.02752693 -0.26783524], action=1, reward=1.0, next_state=[-0.04108534  0.41130783 -0.03288363 -0.56907168]\n",
      "[ episode 271 ][ timestamp 3 ] state=[-0.04108534  0.41130783 -0.03288363 -0.56907168], action=0, reward=1.0, next_state=[-0.03285918  0.21666214 -0.04426507 -0.28692704]\n",
      "[ episode 271 ][ timestamp 4 ] state=[-0.03285918  0.21666214 -0.04426507 -0.28692704], action=0, reward=1.0, next_state=[-0.02852594  0.0221985  -0.05000361 -0.00852698]\n",
      "[ episode 271 ][ timestamp 5 ] state=[-0.02852594  0.0221985  -0.05000361 -0.00852698], action=1, reward=1.0, next_state=[-0.02808197  0.21800058 -0.05017415 -0.31655794]\n",
      "[ episode 271 ][ timestamp 6 ] state=[-0.02808197  0.21800058 -0.05017415 -0.31655794], action=1, reward=1.0, next_state=[-0.02372196  0.41379996 -0.05650531 -0.6246324 ]\n",
      "[ episode 271 ][ timestamp 7 ] state=[-0.02372196  0.41379996 -0.05650531 -0.6246324 ], action=0, reward=1.0, next_state=[-0.01544596  0.2195105  -0.06899795 -0.35026706]\n",
      "[ episode 271 ][ timestamp 8 ] state=[-0.01544596  0.2195105  -0.06899795 -0.35026706], action=0, reward=1.0, next_state=[-0.01105575  0.02543418 -0.0760033  -0.08011458]\n",
      "[ episode 271 ][ timestamp 9 ] state=[-0.01105575  0.02543418 -0.0760033  -0.08011458], action=1, reward=1.0, next_state=[-0.01054706  0.22155873 -0.07760559 -0.39577559]\n",
      "[ episode 271 ][ timestamp 10 ] state=[-0.01054706  0.22155873 -0.07760559 -0.39577559], action=0, reward=1.0, next_state=[-0.00611589  0.02761874 -0.0855211  -0.12853433]\n",
      "[ episode 271 ][ timestamp 11 ] state=[-0.00611589  0.02761874 -0.0855211  -0.12853433], action=1, reward=1.0, next_state=[-0.00556351  0.22385508 -0.08809179 -0.44692563]\n",
      "[ episode 271 ][ timestamp 12 ] state=[-0.00556351  0.22385508 -0.08809179 -0.44692563], action=0, reward=1.0, next_state=[-0.00108641  0.03008256 -0.0970303  -0.18325941]\n",
      "[ episode 271 ][ timestamp 13 ] state=[-0.00108641  0.03008256 -0.0970303  -0.18325941], action=1, reward=1.0, next_state=[-4.84761553e-04  2.26449328e-01 -1.00695486e-01 -5.04906231e-01]\n",
      "[ episode 271 ][ timestamp 14 ] state=[-4.84761553e-04  2.26449328e-01 -1.00695486e-01 -5.04906231e-01], action=1, reward=1.0, next_state=[ 0.00404423  0.42283544 -0.11079361 -0.82754768]\n",
      "[ episode 271 ][ timestamp 15 ] state=[ 0.00404423  0.42283544 -0.11079361 -0.82754768], action=0, reward=1.0, next_state=[ 0.01250093  0.22938863 -0.12734456 -0.57166332]\n",
      "[ episode 271 ][ timestamp 16 ] state=[ 0.01250093  0.22938863 -0.12734456 -0.57166332], action=0, reward=1.0, next_state=[ 0.01708871  0.03626074 -0.13877783 -0.32165542]\n",
      "[ episode 271 ][ timestamp 17 ] state=[ 0.01708871  0.03626074 -0.13877783 -0.32165542], action=1, reward=1.0, next_state=[ 0.01781392  0.233058   -0.14521094 -0.65468307]\n",
      "[ episode 271 ][ timestamp 18 ] state=[ 0.01781392  0.233058   -0.14521094 -0.65468307], action=0, reward=1.0, next_state=[ 0.02247508  0.04022436 -0.1583046  -0.411019  ]\n",
      "[ episode 271 ][ timestamp 19 ] state=[ 0.02247508  0.04022436 -0.1583046  -0.411019  ], action=1, reward=1.0, next_state=[ 0.02327957  0.23719474 -0.16652498 -0.74912758]\n",
      "[ episode 271 ][ timestamp 20 ] state=[ 0.02327957  0.23719474 -0.16652498 -0.74912758], action=0, reward=1.0, next_state=[ 0.02802346  0.0447133  -0.18150753 -0.51313178]\n",
      "[ episode 271 ][ timestamp 21 ] state=[ 0.02802346  0.0447133  -0.18150753 -0.51313178], action=0, reward=1.0, next_state=[ 0.02891773 -0.14745057 -0.19177017 -0.28269177]\n",
      "[ episode 271 ][ timestamp 22 ] state=[ 0.02891773 -0.14745057 -0.19177017 -0.28269177], action=1, reward=1.0, next_state=[ 0.02596872  0.0498155  -0.197424   -0.62920206]\n",
      "[ episode 271 ][ timestamp 23 ] state=[ 0.02596872  0.0498155  -0.197424   -0.62920206], action=1, reward=-1.0, next_state=[ 0.02696503  0.24706478 -0.21000804 -0.97699499]\n",
      "[ Ended! ] Episode 271: Exploration_rate=0.2583638820072446. Score=23.\n",
      "[ Experience replay ] starts\n",
      "[ episode 272 ] state=[ 0.0316299  -0.03945546 -0.03299657  0.02389158]\n",
      "[ episode 272 ][ timestamp 1 ] state=[ 0.0316299  -0.03945546 -0.03299657  0.02389158], action=1, reward=1.0, next_state=[ 0.03084079  0.15612377 -0.03251874 -0.27901681]\n",
      "[ episode 272 ][ timestamp 2 ] state=[ 0.03084079  0.15612377 -0.03251874 -0.27901681], action=0, reward=1.0, next_state=[ 0.03396326 -0.03851956 -0.03809907  0.003235  ]\n",
      "[ episode 272 ][ timestamp 3 ] state=[ 0.03396326 -0.03851956 -0.03809907  0.003235  ], action=1, reward=1.0, next_state=[ 0.03319287  0.15712749 -0.03803437 -0.30122103]\n",
      "[ episode 272 ][ timestamp 4 ] state=[ 0.03319287  0.15712749 -0.03803437 -0.30122103], action=0, reward=1.0, next_state=[ 0.03633542 -0.0374323  -0.0440588  -0.02077181]\n",
      "[ episode 272 ][ timestamp 5 ] state=[ 0.03633542 -0.0374323  -0.0440588  -0.02077181], action=1, reward=1.0, next_state=[ 0.03558678  0.1582929  -0.04447423 -0.3270238 ]\n",
      "[ episode 272 ][ timestamp 6 ] state=[ 0.03558678  0.1582929  -0.04447423 -0.3270238 ], action=0, reward=1.0, next_state=[ 0.03875263 -0.03616858 -0.05101471 -0.04869113]\n",
      "[ episode 272 ][ timestamp 7 ] state=[ 0.03875263 -0.03616858 -0.05101471 -0.04869113], action=1, reward=1.0, next_state=[ 0.03802926  0.15964635 -0.05198853 -0.35702323]\n",
      "[ episode 272 ][ timestamp 8 ] state=[ 0.03802926  0.15964635 -0.05198853 -0.35702323], action=0, reward=1.0, next_state=[ 0.04122219 -0.03469941 -0.05912899 -0.08117619]\n",
      "[ episode 272 ][ timestamp 9 ] state=[ 0.04122219 -0.03469941 -0.05912899 -0.08117619], action=1, reward=1.0, next_state=[ 0.0405282   0.16121812 -0.06075252 -0.3919127 ]\n",
      "[ episode 272 ][ timestamp 10 ] state=[ 0.0405282   0.16121812 -0.06075252 -0.3919127 ], action=0, reward=1.0, next_state=[ 0.04375256 -0.03299141 -0.06859077 -0.11898611]\n",
      "[ episode 272 ][ timestamp 11 ] state=[ 0.04375256 -0.03299141 -0.06859077 -0.11898611], action=1, reward=1.0, next_state=[ 0.04309274  0.16304283 -0.07097049 -0.4324959 ]\n",
      "[ episode 272 ][ timestamp 12 ] state=[ 0.04309274  0.16304283 -0.07097049 -0.4324959 ], action=0, reward=1.0, next_state=[ 0.04635359 -0.03100626 -0.07962041 -0.16300281]\n",
      "[ episode 272 ][ timestamp 13 ] state=[ 0.04635359 -0.03100626 -0.07962041 -0.16300281], action=1, reward=1.0, next_state=[ 0.04573347  0.16515988 -0.08288047 -0.47970351]\n",
      "[ episode 272 ][ timestamp 14 ] state=[ 0.04573347  0.16515988 -0.08288047 -0.47970351], action=0, reward=1.0, next_state=[ 0.04903666 -0.02870025 -0.09247454 -0.21425046]\n",
      "[ episode 272 ][ timestamp 15 ] state=[ 0.04903666 -0.02870025 -0.09247454 -0.21425046], action=1, reward=1.0, next_state=[ 0.04846266  0.16761375 -0.09675955 -0.53461205]\n",
      "[ episode 272 ][ timestamp 16 ] state=[ 0.04846266  0.16761375 -0.09675955 -0.53461205], action=0, reward=1.0, next_state=[ 0.05181493 -0.02602387 -0.10745179 -0.27391718]\n",
      "[ episode 272 ][ timestamp 17 ] state=[ 0.05181493 -0.02602387 -0.10745179 -0.27391718], action=1, reward=1.0, next_state=[ 0.05129446  0.17045406 -0.11293013 -0.5984644 ]\n",
      "[ episode 272 ][ timestamp 18 ] state=[ 0.05129446  0.17045406 -0.11293013 -0.5984644 ], action=1, reward=1.0, next_state=[ 0.05470354  0.36695989 -0.12489942 -0.92447651]\n",
      "[ episode 272 ][ timestamp 19 ] state=[ 0.05470354  0.36695989 -0.12489942 -0.92447651], action=0, reward=1.0, next_state=[ 0.06204274  0.17372616 -0.14338895 -0.67350883]\n",
      "[ episode 272 ][ timestamp 20 ] state=[ 0.06204274  0.17372616 -0.14338895 -0.67350883], action=0, reward=1.0, next_state=[ 0.06551726 -0.01914252 -0.15685913 -0.42918684]\n",
      "[ episode 272 ][ timestamp 21 ] state=[ 0.06551726 -0.01914252 -0.15685913 -0.42918684], action=1, reward=1.0, next_state=[ 0.06513441  0.17781254 -0.16544286 -0.76692005]\n",
      "[ episode 272 ][ timestamp 22 ] state=[ 0.06513441  0.17781254 -0.16544286 -0.76692005], action=0, reward=1.0, next_state=[ 0.06869066 -0.01469224 -0.18078127 -0.53052433]\n",
      "[ episode 272 ][ timestamp 23 ] state=[ 0.06869066 -0.01469224 -0.18078127 -0.53052433], action=1, reward=1.0, next_state=[ 0.06839681  0.18245056 -0.19139175 -0.87428009]\n",
      "[ episode 272 ][ timestamp 24 ] state=[ 0.06839681  0.18245056 -0.19139175 -0.87428009], action=0, reward=1.0, next_state=[ 0.07204583 -0.00962638 -0.20887735 -0.64735181]\n",
      "[ episode 272 ][ timestamp 25 ] state=[ 0.07204583 -0.00962638 -0.20887735 -0.64735181], action=1, reward=-1.0, next_state=[ 0.0718533   0.18769919 -0.22182439 -0.99787102]\n",
      "[ Ended! ] Episode 272: Exploration_rate=0.2570720625972084. Score=25.\n",
      "[ Experience replay ] starts\n",
      "[ episode 273 ] state=[-0.02752263 -0.04833874 -0.00116412 -0.00384441]\n",
      "[ episode 273 ][ timestamp 1 ] state=[-0.02752263 -0.04833874 -0.00116412 -0.00384441], action=1, reward=1.0, next_state=[-0.02848941  0.14679988 -0.00124101 -0.2968944 ]\n",
      "[ episode 273 ][ timestamp 2 ] state=[-0.02848941  0.14679988 -0.00124101 -0.2968944 ], action=0, reward=1.0, next_state=[-0.02555341 -0.04830435 -0.00717889 -0.00460312]\n",
      "[ episode 273 ][ timestamp 3 ] state=[-0.02555341 -0.04830435 -0.00717889 -0.00460312], action=1, reward=1.0, next_state=[-0.0265195   0.14691981 -0.00727096 -0.2995424 ]\n",
      "[ episode 273 ][ timestamp 4 ] state=[-0.0265195   0.14691981 -0.00727096 -0.2995424 ], action=0, reward=1.0, next_state=[-0.0235811  -0.04809775 -0.0132618  -0.00916144]\n",
      "[ episode 273 ][ timestamp 5 ] state=[-0.0235811  -0.04809775 -0.0132618  -0.00916144], action=1, reward=1.0, next_state=[-0.02454306  0.14721186 -0.01344503 -0.30599894]\n",
      "[ episode 273 ][ timestamp 6 ] state=[-0.02454306  0.14721186 -0.01344503 -0.30599894], action=0, reward=1.0, next_state=[-0.02159882 -0.04771594 -0.01956501 -0.01758639]\n",
      "[ episode 273 ][ timestamp 7 ] state=[-0.02159882 -0.04771594 -0.01956501 -0.01758639], action=1, reward=1.0, next_state=[-0.02255314  0.14768105 -0.01991674 -0.31637753]\n",
      "[ episode 273 ][ timestamp 8 ] state=[-0.02255314  0.14768105 -0.01991674 -0.31637753], action=0, reward=1.0, next_state=[-0.01959952 -0.04715163 -0.02624429 -0.0300416 ]\n",
      "[ episode 273 ][ timestamp 9 ] state=[-0.01959952 -0.04715163 -0.02624429 -0.0300416 ], action=1, reward=1.0, next_state=[-0.02054255  0.14833665 -0.02684512 -0.33088799]\n",
      "[ episode 273 ][ timestamp 10 ] state=[-0.02054255  0.14833665 -0.02684512 -0.33088799], action=0, reward=1.0, next_state=[-0.01757582 -0.04639308 -0.03346288 -0.04679014]\n",
      "[ episode 273 ][ timestamp 11 ] state=[-0.01757582 -0.04639308 -0.03346288 -0.04679014], action=1, reward=1.0, next_state=[-0.01850368  0.14919232 -0.03439868 -0.34984026]\n",
      "[ episode 273 ][ timestamp 12 ] state=[-0.01850368  0.14919232 -0.03439868 -0.34984026], action=0, reward=1.0, next_state=[-0.01551983 -0.04542395 -0.04139549 -0.06819977]\n",
      "[ episode 273 ][ timestamp 13 ] state=[-0.01551983 -0.04542395 -0.04139549 -0.06819977], action=1, reward=1.0, next_state=[-0.01642831  0.15026628 -0.04275949 -0.37365045]\n",
      "[ episode 273 ][ timestamp 14 ] state=[-0.01642831  0.15026628 -0.04275949 -0.37365045], action=0, reward=1.0, next_state=[-0.01342299 -0.044223   -0.05023249 -0.09475064]\n",
      "[ episode 273 ][ timestamp 15 ] state=[-0.01342299 -0.044223   -0.05023249 -0.09475064], action=1, reward=1.0, next_state=[-0.01430745  0.15158159 -0.05212751 -0.40284919]\n",
      "[ episode 273 ][ timestamp 16 ] state=[-0.01430745  0.15158159 -0.05212751 -0.40284919], action=0, reward=1.0, next_state=[-0.01127581 -0.04276378 -0.06018449 -0.12704567]\n",
      "[ episode 273 ][ timestamp 17 ] state=[-0.01127581 -0.04276378 -0.06018449 -0.12704567], action=1, reward=1.0, next_state=[-0.01213109  0.15316641 -0.0627254  -0.4380924 ]\n",
      "[ episode 273 ][ timestamp 18 ] state=[-0.01213109  0.15316641 -0.0627254  -0.4380924 ], action=0, reward=1.0, next_state=[-0.00906776 -0.04101423 -0.07148725 -0.16582343]\n",
      "[ episode 273 ][ timestamp 19 ] state=[-0.00906776 -0.04101423 -0.07148725 -0.16582343], action=1, reward=1.0, next_state=[-0.00988805  0.15505435 -0.07480372 -0.48017449]\n",
      "[ episode 273 ][ timestamp 20 ] state=[-0.00988805  0.15505435 -0.07480372 -0.48017449], action=0, reward=1.0, next_state=[-0.00678696 -0.03893633 -0.08440721 -0.21197399]\n",
      "[ episode 273 ][ timestamp 21 ] state=[-0.00678696 -0.03893633 -0.08440721 -0.21197399], action=1, reward=1.0, next_state=[-0.00756568  0.15728465 -0.08864669 -0.53004386]\n",
      "[ episode 273 ][ timestamp 22 ] state=[-0.00756568  0.15728465 -0.08864669 -0.53004386], action=0, reward=1.0, next_state=[-0.00441999 -0.03648576 -0.09924757 -0.26655751]\n",
      "[ episode 273 ][ timestamp 23 ] state=[-0.00441999 -0.03648576 -0.09924757 -0.26655751], action=0, reward=1.0, next_state=[-0.00514971 -0.2300615  -0.10457872 -0.0067537 ]\n",
      "[ episode 273 ][ timestamp 24 ] state=[-0.00514971 -0.2300615  -0.10457872 -0.0067537 ], action=1, reward=1.0, next_state=[-0.00975094 -0.03360729 -0.10471379 -0.33051519]\n",
      "[ episode 273 ][ timestamp 25 ] state=[-0.00975094 -0.03360729 -0.10471379 -0.33051519], action=1, reward=1.0, next_state=[-0.01042308  0.16283728 -0.1113241  -0.65429764]\n",
      "[ episode 273 ][ timestamp 26 ] state=[-0.01042308  0.16283728 -0.1113241  -0.65429764], action=0, reward=1.0, next_state=[-0.00716634 -0.03057301 -0.12441005 -0.39863977]\n",
      "[ episode 273 ][ timestamp 27 ] state=[-0.00716634 -0.03057301 -0.12441005 -0.39863977], action=1, reward=1.0, next_state=[-0.0077778   0.16607401 -0.13238284 -0.72781276]\n",
      "[ episode 273 ][ timestamp 28 ] state=[-0.0077778   0.16607401 -0.13238284 -0.72781276], action=0, reward=1.0, next_state=[-0.00445632 -0.02699357 -0.1469391  -0.47955233]\n",
      "[ episode 273 ][ timestamp 29 ] state=[-0.00445632 -0.02699357 -0.1469391  -0.47955233], action=0, reward=1.0, next_state=[-0.00499619 -0.21976888 -0.15653015 -0.23655023]\n",
      "[ episode 273 ][ timestamp 30 ] state=[-0.00499619 -0.21976888 -0.15653015 -0.23655023], action=1, reward=1.0, next_state=[-0.00939157 -0.02279731 -0.16126115 -0.57422753]\n",
      "[ episode 273 ][ timestamp 31 ] state=[-0.00939157 -0.02279731 -0.16126115 -0.57422753], action=0, reward=1.0, next_state=[-0.00984751 -0.21533488 -0.1727457  -0.33637383]\n",
      "[ episode 273 ][ timestamp 32 ] state=[-0.00984751 -0.21533488 -0.1727457  -0.33637383], action=1, reward=1.0, next_state=[-0.01415421 -0.01822959 -0.17947318 -0.67816638]\n",
      "[ episode 273 ][ timestamp 33 ] state=[-0.01415421 -0.01822959 -0.17947318 -0.67816638], action=0, reward=1.0, next_state=[-0.0145188  -0.21046441 -0.19303651 -0.44692799]\n",
      "[ episode 273 ][ timestamp 34 ] state=[-0.0145188  -0.21046441 -0.19303651 -0.44692799], action=0, reward=1.0, next_state=[-0.01872809 -0.40240671 -0.20197506 -0.22076311]\n",
      "[ episode 273 ][ timestamp 35 ] state=[-0.01872809 -0.40240671 -0.20197506 -0.22076311], action=1, reward=1.0, next_state=[-0.02677622 -0.20505692 -0.20639033 -0.56974804]\n",
      "[ episode 273 ][ timestamp 36 ] state=[-0.02677622 -0.20505692 -0.20639033 -0.56974804], action=0, reward=-1.0, next_state=[-0.03087736 -0.39677805 -0.21778529 -0.34851858]\n",
      "[ Ended! ] Episode 273: Exploration_rate=0.25578670228422234. Score=36.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 274 ] state=[-0.03945875  0.01359409  0.03689304 -0.01022964]\n",
      "[ episode 274 ][ timestamp 1 ] state=[-0.03945875  0.01359409  0.03689304 -0.01022964], action=0, reward=1.0, next_state=[-0.03918687 -0.182037    0.03668845  0.29386142]\n",
      "[ episode 274 ][ timestamp 2 ] state=[-0.03918687 -0.182037    0.03668845  0.29386142], action=1, reward=1.0, next_state=[-0.04282761  0.01254319  0.04256568  0.01297153]\n",
      "[ episode 274 ][ timestamp 3 ] state=[-0.04282761  0.01254319  0.04256568  0.01297153], action=1, reward=1.0, next_state=[-0.04257674  0.20702966  0.04282511 -0.26598341]\n",
      "[ episode 274 ][ timestamp 4 ] state=[-0.04257674  0.20702966  0.04282511 -0.26598341], action=0, reward=1.0, next_state=[-0.03843615  0.01132349  0.03750544  0.03989344]\n",
      "[ episode 274 ][ timestamp 5 ] state=[-0.03843615  0.01132349  0.03750544  0.03989344], action=1, reward=1.0, next_state=[-0.03820968  0.2058881   0.03830331 -0.24072423]\n",
      "[ episode 274 ][ timestamp 6 ] state=[-0.03820968  0.2058881   0.03830331 -0.24072423], action=0, reward=1.0, next_state=[-0.03409192  0.01024052  0.03348883  0.06379031]\n",
      "[ episode 274 ][ timestamp 7 ] state=[-0.03409192  0.01024052  0.03348883  0.06379031], action=0, reward=1.0, next_state=[-0.03388711 -0.18534518  0.03476463  0.36684823]\n",
      "[ episode 274 ][ timestamp 8 ] state=[-0.03388711 -0.18534518  0.03476463  0.36684823], action=1, reward=1.0, next_state=[-0.03759401  0.00926596  0.0421016   0.08532665]\n",
      "[ episode 274 ][ timestamp 9 ] state=[-0.03759401  0.00926596  0.0421016   0.08532665], action=0, reward=1.0, next_state=[-0.03740869 -0.18643341  0.04380813  0.39098979]\n",
      "[ episode 274 ][ timestamp 10 ] state=[-0.03740869 -0.18643341  0.04380813  0.39098979], action=1, reward=1.0, next_state=[-0.04113736  0.00804031  0.05162793  0.11243455]\n",
      "[ episode 274 ][ timestamp 11 ] state=[-0.04113736  0.00804031  0.05162793  0.11243455], action=1, reward=1.0, next_state=[-0.04097656  0.20238593  0.05387662 -0.16352357]\n",
      "[ episode 274 ][ timestamp 12 ] state=[-0.04097656  0.20238593  0.05387662 -0.16352357], action=1, reward=1.0, next_state=[-0.03692884  0.39669688  0.05060615 -0.43873502]\n",
      "[ episode 274 ][ timestamp 13 ] state=[-0.03692884  0.39669688  0.05060615 -0.43873502], action=0, reward=1.0, next_state=[-0.0289949   0.20089656  0.04183145 -0.13053868]\n",
      "[ episode 274 ][ timestamp 14 ] state=[-0.0289949   0.20089656  0.04183145 -0.13053868], action=1, reward=1.0, next_state=[-0.02497697  0.39539509  0.03922067 -0.4097364 ]\n",
      "[ episode 274 ][ timestamp 15 ] state=[-0.02497697  0.39539509  0.03922067 -0.4097364 ], action=0, reward=1.0, next_state=[-0.01706907  0.19973967  0.03102594 -0.10495104]\n",
      "[ episode 274 ][ timestamp 16 ] state=[-0.01706907  0.19973967  0.03102594 -0.10495104], action=0, reward=1.0, next_state=[-0.01307427  0.00418715  0.02892692  0.19735673]\n",
      "[ episode 274 ][ timestamp 17 ] state=[-0.01307427  0.00418715  0.02892692  0.19735673], action=1, reward=1.0, next_state=[-0.01299053  0.19888366  0.03287406 -0.08606252]\n",
      "[ episode 274 ][ timestamp 18 ] state=[-0.01299053  0.19888366  0.03287406 -0.08606252], action=0, reward=1.0, next_state=[-0.00901286  0.00330628  0.03115281  0.21680827]\n",
      "[ episode 274 ][ timestamp 19 ] state=[-0.00901286  0.00330628  0.03115281  0.21680827], action=1, reward=1.0, next_state=[-0.00894673  0.19796935  0.03548897 -0.06588722]\n",
      "[ episode 274 ][ timestamp 20 ] state=[-0.00894673  0.19796935  0.03548897 -0.06588722], action=0, reward=1.0, next_state=[-0.00498734  0.00235703  0.03417123  0.23777807]\n",
      "[ episode 274 ][ timestamp 21 ] state=[-0.00498734  0.00235703  0.03417123  0.23777807], action=1, reward=1.0, next_state=[-0.0049402   0.19697457  0.03892679 -0.04393342]\n",
      "[ episode 274 ][ timestamp 22 ] state=[-0.0049402   0.19697457  0.03892679 -0.04393342], action=0, reward=1.0, next_state=[-0.00100071  0.00131667  0.03804812  0.26077268]\n",
      "[ episode 274 ][ timestamp 23 ] state=[-0.00100071  0.00131667  0.03804812  0.26077268], action=1, reward=1.0, next_state=[-0.00097438  0.1958754   0.04326357 -0.01967076]\n",
      "[ episode 274 ][ timestamp 24 ] state=[-0.00097438  0.1958754   0.04326357 -0.01967076], action=0, reward=1.0, next_state=[2.94312891e-03 1.60566826e-04 4.28701589e-02 2.86342316e-01]\n",
      "[ episode 274 ][ timestamp 25 ] state=[2.94312891e-03 1.60566826e-04 4.28701589e-02 2.86342316e-01], action=1, reward=1.0, next_state=[0.00294634 0.19464574 0.04859701 0.00748256]\n",
      "[ episode 274 ][ timestamp 26 ] state=[0.00294634 0.19464574 0.04859701 0.00748256], action=1, reward=1.0, next_state=[ 0.00683926  0.38903827  0.04874666 -0.2694801 ]\n",
      "[ episode 274 ][ timestamp 27 ] state=[ 0.00683926  0.38903827  0.04874666 -0.2694801 ], action=0, reward=1.0, next_state=[0.01462002 0.1932558  0.04335705 0.03817061]\n",
      "[ episode 274 ][ timestamp 28 ] state=[0.01462002 0.1932558  0.04335705 0.03817061], action=0, reward=1.0, next_state=[ 0.01848514 -0.00246021  0.04412047  0.34421171]\n",
      "[ episode 274 ][ timestamp 29 ] state=[ 0.01848514 -0.00246021  0.04412047  0.34421171], action=1, reward=1.0, next_state=[0.01843593 0.19200722 0.0510047  0.06576164]\n",
      "[ episode 274 ][ timestamp 30 ] state=[0.01843593 0.19200722 0.0510047  0.06576164], action=0, reward=1.0, next_state=[ 0.02227608 -0.00380748  0.05231993  0.37409061]\n",
      "[ episode 274 ][ timestamp 31 ] state=[ 0.02227608 -0.00380748  0.05231993  0.37409061], action=1, reward=1.0, next_state=[0.02219993 0.19053374 0.05980175 0.09835271]\n",
      "[ episode 274 ][ timestamp 32 ] state=[0.02219993 0.19053374 0.05980175 0.09835271], action=0, reward=1.0, next_state=[ 0.0260106  -0.00539204  0.0617688   0.40928726]\n",
      "[ episode 274 ][ timestamp 33 ] state=[ 0.0260106  -0.00539204  0.0617688   0.40928726], action=1, reward=1.0, next_state=[0.02590276 0.18880225 0.06995455 0.13669983]\n",
      "[ episode 274 ][ timestamp 34 ] state=[0.02590276 0.18880225 0.06995455 0.13669983], action=0, reward=1.0, next_state=[ 0.02967881 -0.00724832  0.07268854  0.45060629]\n",
      "[ episode 274 ][ timestamp 35 ] state=[ 0.02967881 -0.00724832  0.07268854  0.45060629], action=1, reward=1.0, next_state=[0.02953384 0.18677435 0.08170067 0.18169243]\n",
      "[ episode 274 ][ timestamp 36 ] state=[0.02953384 0.18677435 0.08170067 0.18169243], action=1, reward=1.0, next_state=[ 0.03326933  0.38063795  0.08533452 -0.0841397 ]\n",
      "[ episode 274 ][ timestamp 37 ] state=[ 0.03326933  0.38063795  0.08533452 -0.0841397 ], action=1, reward=1.0, next_state=[ 0.04088209  0.57443959  0.08365172 -0.34872645]\n",
      "[ episode 274 ][ timestamp 38 ] state=[ 0.04088209  0.57443959  0.08365172 -0.34872645], action=0, reward=1.0, next_state=[ 0.05237088  0.37823368  0.07667719 -0.03088178]\n",
      "[ episode 274 ][ timestamp 39 ] state=[ 0.05237088  0.37823368  0.07667719 -0.03088178], action=0, reward=1.0, next_state=[0.05993555 0.18210065 0.07605956 0.28497434]\n",
      "[ episode 274 ][ timestamp 40 ] state=[0.05993555 0.18210065 0.07605956 0.28497434], action=0, reward=1.0, next_state=[ 0.06357756 -0.01401903  0.08175904  0.6006433 ]\n",
      "[ episode 274 ][ timestamp 41 ] state=[ 0.06357756 -0.01401903  0.08175904  0.6006433 ], action=1, reward=1.0, next_state=[0.06329718 0.17986972 0.09377191 0.33479207]\n",
      "[ episode 274 ][ timestamp 42 ] state=[0.06329718 0.17986972 0.09377191 0.33479207], action=1, reward=1.0, next_state=[0.06689458 0.37354071 0.10046775 0.07309044]\n",
      "[ episode 274 ][ timestamp 43 ] state=[0.06689458 0.37354071 0.10046775 0.07309044], action=1, reward=1.0, next_state=[ 0.07436539  0.56708954  0.10192956 -0.18628095]\n",
      "[ episode 274 ][ timestamp 44 ] state=[ 0.07436539  0.56708954  0.10192956 -0.18628095], action=0, reward=1.0, next_state=[0.08570718 0.37066812 0.09820394 0.13673736]\n",
      "[ episode 274 ][ timestamp 45 ] state=[0.08570718 0.37066812 0.09820394 0.13673736], action=1, reward=1.0, next_state=[ 0.09312055  0.56425634  0.10093869 -0.12342029]\n",
      "[ episode 274 ][ timestamp 46 ] state=[ 0.09312055  0.56425634  0.10093869 -0.12342029], action=0, reward=1.0, next_state=[0.10440567 0.36784403 0.09847028 0.19932419]\n",
      "[ episode 274 ][ timestamp 47 ] state=[0.10440567 0.36784403 0.09847028 0.19932419], action=1, reward=1.0, next_state=[ 0.11176255  0.56142979  0.10245677 -0.06074428]\n",
      "[ episode 274 ][ timestamp 48 ] state=[ 0.11176255  0.56142979  0.10245677 -0.06074428], action=1, reward=1.0, next_state=[ 0.12299115  0.75494487  0.10124188 -0.31942506]\n",
      "[ episode 274 ][ timestamp 49 ] state=[ 0.12299115  0.75494487  0.10124188 -0.31942506], action=0, reward=1.0, next_state=[0.13809005 0.55853772 0.09485338 0.00339137]\n",
      "[ episode 274 ][ timestamp 50 ] state=[0.13809005 0.55853772 0.09485338 0.00339137], action=0, reward=1.0, next_state=[0.1492608  0.36219245 0.09492121 0.32443045]\n",
      "[ episode 274 ][ timestamp 51 ] state=[0.1492608  0.36219245 0.09492121 0.32443045], action=1, reward=1.0, next_state=[0.15650465 0.55584371 0.10140982 0.06312614]\n",
      "[ episode 274 ][ timestamp 52 ] state=[0.15650465 0.55584371 0.10140982 0.06312614], action=1, reward=1.0, next_state=[ 0.16762152  0.7493765   0.10267234 -0.19591822]\n",
      "[ episode 274 ][ timestamp 53 ] state=[ 0.16762152  0.7493765   0.10267234 -0.19591822], action=0, reward=1.0, next_state=[0.18260905 0.55294723 0.09875397 0.12730671]\n",
      "[ episode 274 ][ timestamp 54 ] state=[0.18260905 0.55294723 0.09875397 0.12730671], action=1, reward=1.0, next_state=[ 0.193668    0.74652596  0.10130011 -0.13266016]\n",
      "[ episode 274 ][ timestamp 55 ] state=[ 0.193668    0.74652596  0.10130011 -0.13266016], action=0, reward=1.0, next_state=[0.20859852 0.55010985 0.09864691 0.19018495]\n",
      "[ episode 274 ][ timestamp 56 ] state=[0.20859852 0.55010985 0.09864691 0.19018495], action=1, reward=1.0, next_state=[ 0.21960071  0.74369231  0.1024506  -0.06982186]\n",
      "[ episode 274 ][ timestamp 57 ] state=[ 0.21960071  0.74369231  0.1024506  -0.06982186], action=1, reward=1.0, next_state=[ 0.23447456  0.93720761  0.10105417 -0.32850496]\n",
      "[ episode 274 ][ timestamp 58 ] state=[ 0.23447456  0.93720761  0.10105417 -0.32850496], action=0, reward=1.0, next_state=[ 0.25321871  0.74080312  0.09448407 -0.00574181]\n",
      "[ episode 274 ][ timestamp 59 ] state=[ 0.25321871  0.74080312  0.09448407 -0.00574181], action=0, reward=1.0, next_state=[0.26803478 0.54446205 0.09436923 0.31519318]\n",
      "[ episode 274 ][ timestamp 60 ] state=[0.26803478 0.54446205 0.09436923 0.31519318], action=0, reward=1.0, next_state=[0.27892402 0.34813142 0.1006731  0.63608217]\n",
      "[ episode 274 ][ timestamp 61 ] state=[0.27892402 0.34813142 0.1006731  0.63608217], action=0, reward=1.0, next_state=[0.28588664 0.15176021 0.11339474 0.95869549]\n",
      "[ episode 274 ][ timestamp 62 ] state=[0.28588664 0.15176021 0.11339474 0.95869549], action=1, reward=1.0, next_state=[0.28892185 0.34519007 0.13256865 0.70368075]\n",
      "[ episode 274 ][ timestamp 63 ] state=[0.28892185 0.34519007 0.13256865 0.70368075], action=1, reward=1.0, next_state=[0.29582565 0.53825003 0.14664226 0.45549289]\n",
      "[ episode 274 ][ timestamp 64 ] state=[0.29582565 0.53825003 0.14664226 0.45549289], action=0, reward=1.0, next_state=[0.30659065 0.34139215 0.15575212 0.79056896]\n",
      "[ episode 274 ][ timestamp 65 ] state=[0.30659065 0.34139215 0.15575212 0.79056896], action=1, reward=1.0, next_state=[0.31341849 0.53407167 0.1715635  0.55065443]\n",
      "[ episode 274 ][ timestamp 66 ] state=[0.31341849 0.53407167 0.1715635  0.55065443], action=1, reward=1.0, next_state=[0.32409993 0.72642143 0.18257659 0.3165582 ]\n",
      "[ episode 274 ][ timestamp 67 ] state=[0.32409993 0.72642143 0.18257659 0.3165582 ], action=1, reward=1.0, next_state=[0.33862836 0.91853712 0.18890775 0.08655416]\n",
      "[ episode 274 ][ timestamp 68 ] state=[0.33862836 0.91853712 0.18890775 0.08655416], action=1, reward=1.0, next_state=[ 0.3569991   1.11051985  0.19063884 -0.14108773]\n",
      "[ episode 274 ][ timestamp 69 ] state=[ 0.3569991   1.11051985  0.19063884 -0.14108773], action=0, reward=1.0, next_state=[0.3792095  0.91325157 0.18781708 0.20516289]\n",
      "[ episode 274 ][ timestamp 70 ] state=[0.3792095  0.91325157 0.18781708 0.20516289], action=1, reward=1.0, next_state=[ 0.39747453  1.10526072  0.19192034 -0.02289171]\n",
      "[ episode 274 ][ timestamp 71 ] state=[ 0.39747453  1.10526072  0.19192034 -0.02289171], action=1, reward=1.0, next_state=[ 0.41957974  1.29718649  0.19146251 -0.24941584]\n",
      "[ episode 274 ][ timestamp 72 ] state=[ 0.41957974  1.29718649  0.19146251 -0.24941584], action=1, reward=1.0, next_state=[ 0.44552347  1.48913216  0.18647419 -0.47612651]\n",
      "[ episode 274 ][ timestamp 73 ] state=[ 0.44552347  1.48913216  0.18647419 -0.47612651], action=1, reward=1.0, next_state=[ 0.47530611  1.68119932  0.17695166 -0.7047265 ]\n",
      "[ episode 274 ][ timestamp 74 ] state=[ 0.47530611  1.68119932  0.17695166 -0.7047265 ], action=0, reward=1.0, next_state=[ 0.5089301   1.48412439  0.16285713 -0.36197741]\n",
      "[ episode 274 ][ timestamp 75 ] state=[ 0.5089301   1.48412439  0.16285713 -0.36197741], action=0, reward=1.0, next_state=[ 0.53861259  1.28710761  0.15561758 -0.02269398]\n",
      "[ episode 274 ][ timestamp 76 ] state=[ 0.53861259  1.28710761  0.15561758 -0.02269398], action=1, reward=1.0, next_state=[ 0.56435474  1.4796952   0.1551637  -0.26251739]\n",
      "[ episode 274 ][ timestamp 77 ] state=[ 0.56435474  1.4796952   0.1551637  -0.26251739], action=0, reward=1.0, next_state=[0.59394864 1.2827381  0.14991335 0.07480426]\n",
      "[ episode 274 ][ timestamp 78 ] state=[0.59394864 1.2827381  0.14991335 0.07480426], action=1, reward=1.0, next_state=[ 0.61960341  1.47542846  0.15140944 -0.16707983]\n",
      "[ episode 274 ][ timestamp 79 ] state=[ 0.61960341  1.47542846  0.15140944 -0.16707983], action=1, reward=1.0, next_state=[ 0.64911198  1.6680954   0.14806784 -0.40842943]\n",
      "[ episode 274 ][ timestamp 80 ] state=[ 0.64911198  1.6680954   0.14806784 -0.40842943], action=0, reward=1.0, next_state=[ 0.68247388  1.47121838  0.13989925 -0.0729722 ]\n",
      "[ episode 274 ][ timestamp 81 ] state=[ 0.68247388  1.47121838  0.13989925 -0.0729722 ], action=1, reward=1.0, next_state=[ 0.71189825  1.66408636  0.13843981 -0.31845137]\n",
      "[ episode 274 ][ timestamp 82 ] state=[ 0.71189825  1.66408636  0.13843981 -0.31845137], action=0, reward=1.0, next_state=[0.74517998 1.46729212 0.13207078 0.01448716]\n",
      "[ episode 274 ][ timestamp 83 ] state=[0.74517998 1.46729212 0.13207078 0.01448716], action=1, reward=1.0, next_state=[ 0.77452582  1.66029706  0.13236052 -0.23378299]\n",
      "[ episode 274 ][ timestamp 84 ] state=[ 0.77452582  1.66029706  0.13236052 -0.23378299], action=1, reward=1.0, next_state=[ 0.80773176  1.85330392  0.12768487 -0.4819605 ]\n",
      "[ episode 274 ][ timestamp 85 ] state=[ 0.80773176  1.85330392  0.12768487 -0.4819605 ], action=0, reward=1.0, next_state=[ 0.84479784  1.65663282  0.11804566 -0.15191797]\n",
      "[ episode 274 ][ timestamp 86 ] state=[ 0.84479784  1.65663282  0.11804566 -0.15191797], action=1, reward=1.0, next_state=[ 0.8779305   1.84988403  0.1150073  -0.40515256]\n",
      "[ episode 274 ][ timestamp 87 ] state=[ 0.8779305   1.84988403  0.1150073  -0.40515256], action=0, reward=1.0, next_state=[ 0.91492818  1.65333498  0.10690424 -0.07853894]\n",
      "[ episode 274 ][ timestamp 88 ] state=[ 0.91492818  1.65333498  0.10690424 -0.07853894], action=1, reward=1.0, next_state=[ 0.94799488  1.84677492  0.10533347 -0.33567236]\n",
      "[ episode 274 ][ timestamp 89 ] state=[ 0.94799488  1.84677492  0.10533347 -0.33567236], action=0, reward=1.0, next_state=[ 0.98493038  1.65032393  0.09862002 -0.01171831]\n",
      "[ episode 274 ][ timestamp 90 ] state=[ 0.98493038  1.65032393  0.09862002 -0.01171831], action=1, reward=1.0, next_state=[ 1.01793685  1.84390338  0.09838565 -0.27172926]\n",
      "[ episode 274 ][ timestamp 91 ] state=[ 1.01793685  1.84390338  0.09838565 -0.27172926], action=0, reward=1.0, next_state=[1.05481492 1.64752524 0.09295107 0.05029217]\n",
      "[ episode 274 ][ timestamp 92 ] state=[1.05481492 1.64752524 0.09295107 0.05029217], action=1, reward=1.0, next_state=[ 1.08776543  1.84119994  0.09395691 -0.21167751]\n",
      "[ episode 274 ][ timestamp 93 ] state=[ 1.08776543  1.84119994  0.09395691 -0.21167751], action=0, reward=1.0, next_state=[1.12458943 1.64486898 0.08972336 0.10910271]\n",
      "[ episode 274 ][ timestamp 94 ] state=[1.12458943 1.64486898 0.08972336 0.10910271], action=0, reward=1.0, next_state=[1.15748681 1.44858351 0.09190541 0.42868988]\n",
      "[ episode 274 ][ timestamp 95 ] state=[1.15748681 1.44858351 0.09190541 0.42868988], action=1, reward=1.0, next_state=[1.18645848 1.64229188 0.10047921 0.16633576]\n",
      "[ episode 274 ][ timestamp 96 ] state=[1.18645848 1.64229188 0.10047921 0.16633576], action=1, reward=1.0, next_state=[ 1.21930431  1.8358427   0.10380593 -0.09303492]\n",
      "[ episode 274 ][ timestamp 97 ] state=[ 1.21930431  1.8358427   0.10380593 -0.09303492], action=0, reward=1.0, next_state=[1.25602117 1.6393979  0.10194523 0.23051025]\n",
      "[ episode 274 ][ timestamp 98 ] state=[1.25602117 1.6393979  0.10194523 0.23051025], action=1, reward=1.0, next_state=[ 1.28880912  1.83292653  0.10655543 -0.02835551]\n",
      "[ episode 274 ][ timestamp 99 ] state=[ 1.28880912  1.83292653  0.10655543 -0.02835551], action=1, reward=1.0, next_state=[ 1.32546766  2.02637184  0.10598832 -0.28560969]\n",
      "[ episode 274 ][ timestamp 100 ] state=[ 1.32546766  2.02637184  0.10598832 -0.28560969], action=0, reward=1.0, next_state=[1.36599509 1.82991052 0.10027613 0.03853088]\n",
      "[ episode 274 ][ timestamp 101 ] state=[1.36599509 1.82991052 0.10027613 0.03853088], action=1, reward=1.0, next_state=[ 1.4025933   2.0234622   0.10104675 -0.2209064 ]\n",
      "[ episode 274 ][ timestamp 102 ] state=[ 1.4025933   2.0234622   0.10104675 -0.2209064 ], action=0, reward=1.0, next_state=[1.44306255 1.82705198 0.09662862 0.10186335]\n",
      "[ episode 274 ][ timestamp 103 ] state=[1.44306255 1.82705198 0.09662862 0.10186335], action=1, reward=1.0, next_state=[ 1.47960359  2.02066586  0.09866589 -0.15883805]\n",
      "[ episode 274 ][ timestamp 104 ] state=[ 1.47960359  2.02066586  0.09866589 -0.15883805], action=0, reward=1.0, next_state=[1.5200169  1.82427992 0.09548913 0.1632689 ]\n",
      "[ episode 274 ][ timestamp 105 ] state=[1.5200169  1.82427992 0.09548913 0.1632689 ], action=1, reward=1.0, next_state=[ 1.5565025   2.0179144   0.0987545  -0.09782849]\n",
      "[ episode 274 ][ timestamp 106 ] state=[ 1.5565025   2.0179144   0.0987545  -0.09782849], action=1, reward=1.0, next_state=[ 1.59686079  2.2114925   0.09679793 -0.35779422]\n",
      "[ episode 274 ][ timestamp 107 ] state=[ 1.59686079  2.2114925   0.09679793 -0.35779422], action=0, reward=1.0, next_state=[ 1.64109064  2.01513725  0.08964205 -0.03622597]\n",
      "[ episode 274 ][ timestamp 108 ] state=[ 1.64109064  2.01513725  0.08964205 -0.03622597], action=1, reward=1.0, next_state=[ 1.68139338  2.20886699  0.08891753 -0.29933431]\n",
      "[ episode 274 ][ timestamp 109 ] state=[ 1.68139338  2.20886699  0.08891753 -0.29933431], action=0, reward=1.0, next_state=[1.72557072 2.01259755 0.08293084 0.02001411]\n",
      "[ episode 274 ][ timestamp 110 ] state=[1.72557072 2.01259755 0.08293084 0.02001411], action=1, reward=1.0, next_state=[ 1.76582268  2.2064383   0.08333113 -0.24539401]\n",
      "[ episode 274 ][ timestamp 111 ] state=[ 1.76582268  2.2064383   0.08333113 -0.24539401], action=0, reward=1.0, next_state=[1.80995144 2.01023114 0.07842325 0.07236647]\n",
      "[ episode 274 ][ timestamp 112 ] state=[1.80995144 2.01023114 0.07842325 0.07236647], action=0, reward=1.0, next_state=[1.85015606 1.81407758 0.07987058 0.38872529]\n",
      "[ episode 274 ][ timestamp 113 ] state=[1.85015606 1.81407758 0.07987058 0.38872529], action=1, reward=1.0, next_state=[1.88643762 2.00798038 0.08764508 0.12225532]\n",
      "[ episode 274 ][ timestamp 114 ] state=[1.88643762 2.00798038 0.08764508 0.12225532], action=1, reward=1.0, next_state=[ 1.92659722  2.20174445  0.09009019 -0.1415405 ]\n",
      "[ episode 274 ][ timestamp 115 ] state=[ 1.92659722  2.20174445  0.09009019 -0.1415405 ], action=0, reward=1.0, next_state=[1.97063211 2.00545546 0.08725938 0.17814964]\n",
      "[ episode 274 ][ timestamp 116 ] state=[1.97063211 2.00545546 0.08725938 0.17814964], action=1, reward=1.0, next_state=[ 2.01074122  2.19922734  0.09082237 -0.08578061]\n",
      "[ episode 274 ][ timestamp 117 ] state=[ 2.01074122  2.19922734  0.09082237 -0.08578061], action=0, reward=1.0, next_state=[2.05472577 2.00292882 0.08910676 0.23411868]\n",
      "[ episode 274 ][ timestamp 118 ] state=[2.05472577 2.00292882 0.08910676 0.23411868], action=1, reward=1.0, next_state=[ 2.09478434  2.1966721   0.09378913 -0.02918052]\n",
      "[ episode 274 ][ timestamp 119 ] state=[ 2.09478434  2.1966721   0.09378913 -0.02918052], action=0, reward=1.0, next_state=[2.13871779 2.00033903 0.09320552 0.29155837]\n",
      "[ episode 274 ][ timestamp 120 ] state=[2.13871779 2.00033903 0.09320552 0.29155837], action=1, reward=1.0, next_state=[2.17872457 2.19401697 0.09903669 0.02966521]\n",
      "[ episode 274 ][ timestamp 121 ] state=[2.17872457 2.19401697 0.09903669 0.02966521], action=1, reward=1.0, next_state=[ 2.22260491  2.38758947  0.09962999 -0.23020154]\n",
      "[ episode 274 ][ timestamp 122 ] state=[ 2.22260491  2.38758947  0.09962999 -0.23020154], action=0, reward=1.0, next_state=[2.2703567  2.19119543 0.09502596 0.09217144]\n",
      "[ episode 274 ][ timestamp 123 ] state=[2.2703567  2.19119543 0.09502596 0.09217144], action=1, reward=1.0, next_state=[ 2.3141806   2.38483599  0.09686939 -0.16908335]\n",
      "[ episode 274 ][ timestamp 124 ] state=[ 2.3141806   2.38483599  0.09686939 -0.16908335], action=0, reward=1.0, next_state=[2.36187732 2.18847056 0.09348772 0.15251897]\n",
      "[ episode 274 ][ timestamp 125 ] state=[2.36187732 2.18847056 0.09348772 0.15251897], action=1, reward=-1.0, next_state=[ 2.40564674  2.38213819  0.0965381  -0.10926854]\n",
      "[ Ended! ] Episode 274: Exploration_rate=0.25450776877280124. Score=125.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 275 ] state=[ 0.0299835  -0.01159394  0.0491974  -0.0016413 ]\n",
      "[ episode 275 ][ timestamp 1 ] state=[ 0.0299835  -0.01159394  0.0491974  -0.0016413 ], action=1, reward=1.0, next_state=[ 0.02975162  0.18278919  0.04916457 -0.278405  ]\n",
      "[ episode 275 ][ timestamp 2 ] state=[ 0.02975162  0.18278919  0.04916457 -0.278405  ], action=1, reward=1.0, next_state=[ 0.0334074   0.37717654  0.04359647 -0.55518515]\n",
      "[ episode 275 ][ timestamp 3 ] state=[ 0.0334074   0.37717654  0.04359647 -0.55518515], action=1, reward=1.0, next_state=[ 0.04095093  0.57166013  0.03249277 -0.83382003]\n",
      "[ episode 275 ][ timestamp 4 ] state=[ 0.04095093  0.57166013  0.03249277 -0.83382003], action=1, reward=1.0, next_state=[ 0.05238413  0.76632341  0.01581637 -1.11610964]\n",
      "[ episode 275 ][ timestamp 5 ] state=[ 0.05238413  0.76632341  0.01581637 -1.11610964], action=0, reward=1.0, next_state=[ 0.0677106   0.57099747 -0.00650582 -0.81850755]\n",
      "[ episode 275 ][ timestamp 6 ] state=[ 0.0677106   0.57099747 -0.00650582 -0.81850755], action=1, reward=1.0, next_state=[ 0.07913055  0.76620786 -0.02287598 -1.11322964]\n",
      "[ episode 275 ][ timestamp 7 ] state=[ 0.07913055  0.76620786 -0.02287598 -1.11322964], action=0, reward=1.0, next_state=[ 0.09445471  0.57139367 -0.04514057 -0.82780977]\n",
      "[ episode 275 ][ timestamp 8 ] state=[ 0.09445471  0.57139367 -0.04514057 -0.82780977], action=0, reward=1.0, next_state=[ 0.10588258  0.37691703 -0.06169676 -0.54965879]\n",
      "[ episode 275 ][ timestamp 9 ] state=[ 0.10588258  0.37691703 -0.06169676 -0.54965879], action=0, reward=1.0, next_state=[ 0.11342092  0.18271351 -0.07268994 -0.27703509]\n",
      "[ episode 275 ][ timestamp 10 ] state=[ 0.11342092  0.18271351 -0.07268994 -0.27703509], action=1, reward=1.0, next_state=[ 0.11707519  0.37879316 -0.07823064 -0.5917299 ]\n",
      "[ episode 275 ][ timestamp 11 ] state=[ 0.11707519  0.37879316 -0.07823064 -0.5917299 ], action=1, reward=1.0, next_state=[ 0.12465106  0.57491813 -0.09006524 -0.90799396]\n",
      "[ episode 275 ][ timestamp 12 ] state=[ 0.12465106  0.57491813 -0.09006524 -0.90799396], action=0, reward=1.0, next_state=[ 0.13614942  0.38112325 -0.10822512 -0.64492324]\n",
      "[ episode 275 ][ timestamp 13 ] state=[ 0.13614942  0.38112325 -0.10822512 -0.64492324], action=1, reward=1.0, next_state=[ 0.14377188  0.57757368 -0.12112358 -0.96963095]\n",
      "[ episode 275 ][ timestamp 14 ] state=[ 0.14377188  0.57757368 -0.12112358 -0.96963095], action=0, reward=1.0, next_state=[ 0.15532336  0.38426754 -0.1405162  -0.71731946]\n",
      "[ episode 275 ][ timestamp 15 ] state=[ 0.15532336  0.38426754 -0.1405162  -0.71731946], action=1, reward=1.0, next_state=[ 0.16300871  0.58102557 -0.15486259 -1.05072354]\n",
      "[ episode 275 ][ timestamp 16 ] state=[ 0.16300871  0.58102557 -0.15486259 -1.05072354], action=0, reward=1.0, next_state=[ 0.17462922  0.38825868 -0.17587706 -0.81038137]\n",
      "[ episode 275 ][ timestamp 17 ] state=[ 0.17462922  0.38825868 -0.17587706 -0.81038137], action=0, reward=1.0, next_state=[ 0.18239439  0.19592611 -0.19208469 -0.57777475]\n",
      "[ episode 275 ][ timestamp 18 ] state=[ 0.18239439  0.19592611 -0.19208469 -0.57777475], action=1, reward=1.0, next_state=[ 0.18631292  0.39314747 -0.20364018 -0.92429222]\n",
      "[ episode 275 ][ timestamp 19 ] state=[ 0.18631292  0.39314747 -0.20364018 -0.92429222], action=0, reward=-1.0, next_state=[ 0.19417586  0.2012716  -0.22212603 -0.70188282]\n",
      "[ Ended! ] Episode 275: Exploration_rate=0.2532352299289372. Score=19.\n",
      "[ Experience replay ] starts\n",
      "[ episode 276 ] state=[-0.0166539   0.04490049 -0.0113331   0.02243085]\n",
      "[ episode 276 ][ timestamp 1 ] state=[-0.0166539   0.04490049 -0.0113331   0.02243085], action=0, reward=1.0, next_state=[-0.01575589 -0.15005712 -0.01088448  0.31151661]\n",
      "[ episode 276 ][ timestamp 2 ] state=[-0.01575589 -0.15005712 -0.01088448  0.31151661], action=1, reward=1.0, next_state=[-0.01875703  0.0452182  -0.00465415  0.01542102]\n",
      "[ episode 276 ][ timestamp 3 ] state=[-0.01875703  0.0452182  -0.00465415  0.01542102], action=1, reward=1.0, next_state=[-0.01785267  0.24040658 -0.00434573 -0.27872671]\n",
      "[ episode 276 ][ timestamp 4 ] state=[-0.01785267  0.24040658 -0.00434573 -0.27872671], action=0, reward=1.0, next_state=[-0.01304453  0.0453469  -0.00992026  0.01258242]\n",
      "[ episode 276 ][ timestamp 5 ] state=[-0.01304453  0.0453469  -0.00992026  0.01258242], action=0, reward=1.0, next_state=[-0.0121376  -0.14963139 -0.00966862  0.30211895]\n",
      "[ episode 276 ][ timestamp 6 ] state=[-0.0121376  -0.14963139 -0.00966862  0.30211895], action=1, reward=1.0, next_state=[-0.01513022  0.04562702 -0.00362624  0.0064025 ]\n",
      "[ episode 276 ][ timestamp 7 ] state=[-0.01513022  0.04562702 -0.00362624  0.0064025 ], action=0, reward=1.0, next_state=[-0.01421768 -0.14944274 -0.00349819  0.2979391 ]\n",
      "[ episode 276 ][ timestamp 8 ] state=[-0.01421768 -0.14944274 -0.00349819  0.2979391 ], action=1, reward=1.0, next_state=[-0.01720654  0.0457289   0.00246059  0.00415496]\n",
      "[ episode 276 ][ timestamp 9 ] state=[-0.01720654  0.0457289   0.00246059  0.00415496], action=0, reward=1.0, next_state=[-0.01629196 -0.14942825  0.00254369  0.29761322]\n",
      "[ episode 276 ][ timestamp 10 ] state=[-0.01629196 -0.14942825  0.00254369  0.29761322], action=1, reward=1.0, next_state=[-0.01928052  0.04565735  0.00849596  0.00573361]\n",
      "[ episode 276 ][ timestamp 11 ] state=[-0.01928052  0.04565735  0.00849596  0.00573361], action=1, reward=1.0, next_state=[-0.01836738  0.24065643  0.00861063 -0.28425667]\n",
      "[ episode 276 ][ timestamp 12 ] state=[-0.01836738  0.24065643  0.00861063 -0.28425667], action=1, reward=1.0, next_state=[-0.01355425  0.43565452  0.0029255  -0.57421147]\n",
      "[ episode 276 ][ timestamp 13 ] state=[-0.01355425  0.43565452  0.0029255  -0.57421147], action=0, reward=1.0, next_state=[-0.00484116  0.24049168 -0.00855873 -0.28060836]\n",
      "[ episode 276 ][ timestamp 14 ] state=[-0.00484116  0.24049168 -0.00855873 -0.28060836], action=0, reward=1.0, next_state=[-3.13254009e-05  4.54928507e-02 -1.41708997e-02  9.36292632e-03]\n",
      "[ episode 276 ][ timestamp 15 ] state=[-3.13254009e-05  4.54928507e-02 -1.41708997e-02  9.36292632e-03], action=1, reward=1.0, next_state=[ 0.00087853  0.24081514 -0.01398364 -0.28775719]\n",
      "[ episode 276 ][ timestamp 16 ] state=[ 0.00087853  0.24081514 -0.01398364 -0.28775719], action=0, reward=1.0, next_state=[ 0.00569483  0.04589536 -0.01973878  0.00048283]\n",
      "[ episode 276 ][ timestamp 17 ] state=[ 0.00569483  0.04589536 -0.01973878  0.00048283], action=0, reward=1.0, next_state=[ 0.00661274 -0.14893803 -0.01972913  0.28687315]\n",
      "[ episode 276 ][ timestamp 18 ] state=[ 0.00661274 -0.14893803 -0.01972913  0.28687315], action=1, reward=1.0, next_state=[ 0.00363398  0.04645965 -0.01399167 -0.01196631]\n",
      "[ episode 276 ][ timestamp 19 ] state=[ 0.00363398  0.04645965 -0.01399167 -0.01196631], action=0, reward=1.0, next_state=[ 0.00456317 -0.14845888 -0.01423099  0.27626945]\n",
      "[ episode 276 ][ timestamp 20 ] state=[ 0.00456317 -0.14845888 -0.01423099  0.27626945], action=1, reward=1.0, next_state=[ 0.001594    0.04686319 -0.0087056  -0.02086775]\n",
      "[ episode 276 ][ timestamp 21 ] state=[ 0.001594    0.04686319 -0.0087056  -0.02086775], action=0, reward=1.0, next_state=[ 0.00253126 -0.14813284 -0.00912296  0.2690558 ]\n",
      "[ episode 276 ][ timestamp 22 ] state=[ 0.00253126 -0.14813284 -0.00912296  0.2690558 ], action=1, reward=1.0, next_state=[-0.0004314   0.0471181  -0.00374184 -0.02649055]\n",
      "[ episode 276 ][ timestamp 23 ] state=[-0.0004314   0.0471181  -0.00374184 -0.02649055], action=0, reward=1.0, next_state=[ 0.00051097 -0.14794999 -0.00427165  0.26500944]\n",
      "[ episode 276 ][ timestamp 24 ] state=[ 0.00051097 -0.14794999 -0.00427165  0.26500944], action=1, reward=1.0, next_state=[-0.00244803  0.04723267  0.00102854 -0.02901774]\n",
      "[ episode 276 ][ timestamp 25 ] state=[-0.00244803  0.04723267  0.00102854 -0.02901774], action=0, reward=1.0, next_state=[-0.00150338 -0.14790401  0.00044818  0.26398953]\n",
      "[ episode 276 ][ timestamp 26 ] state=[-0.00150338 -0.14790401  0.00044818  0.26398953], action=1, reward=1.0, next_state=[-0.00446146  0.04721154  0.00572797 -0.02855201]\n",
      "[ episode 276 ][ timestamp 27 ] state=[-0.00446146  0.04721154  0.00572797 -0.02855201], action=0, reward=1.0, next_state=[-0.00351723 -0.14799209  0.00515693  0.26593264]\n",
      "[ episode 276 ][ timestamp 28 ] state=[-0.00351723 -0.14799209  0.00515693  0.26593264], action=1, reward=1.0, next_state=[-0.00647707  0.04705588  0.01047558 -0.02511929]\n",
      "[ episode 276 ][ timestamp 29 ] state=[-0.00647707  0.04705588  0.01047558 -0.02511929], action=0, reward=1.0, next_state=[-0.00553595 -0.14821472  0.0099732   0.2708503 ]\n",
      "[ episode 276 ][ timestamp 30 ] state=[-0.00553595 -0.14821472  0.0099732   0.2708503 ], action=1, reward=1.0, next_state=[-0.00850025  0.04676351  0.0153902  -0.01867042]\n",
      "[ episode 276 ][ timestamp 31 ] state=[-0.00850025  0.04676351  0.0153902  -0.01867042], action=0, reward=1.0, next_state=[-0.00756498 -0.14857574  0.0150168   0.27882829]\n",
      "[ episode 276 ][ timestamp 32 ] state=[-0.00756498 -0.14857574  0.0150168   0.27882829], action=1, reward=1.0, next_state=[-0.01053649  0.04632881  0.02059336 -0.00908079]\n",
      "[ episode 276 ][ timestamp 33 ] state=[-0.01053649  0.04632881  0.02059336 -0.00908079], action=0, reward=1.0, next_state=[-0.00960992 -0.14908233  0.02041175  0.29002779]\n",
      "[ episode 276 ][ timestamp 34 ] state=[-0.00960992 -0.14908233  0.02041175  0.29002779], action=1, reward=1.0, next_state=[-0.01259156  0.0457427   0.0262123   0.00385175]\n",
      "[ episode 276 ][ timestamp 35 ] state=[-0.01259156  0.0457427   0.0262123   0.00385175], action=0, reward=1.0, next_state=[-0.01167671 -0.14974518  0.02628934  0.30468837]\n",
      "[ episode 276 ][ timestamp 36 ] state=[-0.01167671 -0.14974518  0.02628934  0.30468837], action=1, reward=1.0, next_state=[-0.01467161  0.04499246  0.0323831   0.02041103]\n",
      "[ episode 276 ][ timestamp 37 ] state=[-0.01467161  0.04499246  0.0323831   0.02041103], action=0, reward=1.0, next_state=[-0.01377176 -0.15057858  0.03279133  0.32313275]\n",
      "[ episode 276 ][ timestamp 38 ] state=[-0.01377176 -0.15057858  0.03279133  0.32313275], action=1, reward=1.0, next_state=[-0.01678334  0.04406147  0.03925398  0.04096856]\n",
      "[ episode 276 ][ timestamp 39 ] state=[-0.01678334  0.04406147  0.03925398  0.04096856], action=0, reward=1.0, next_state=[-0.01590211 -0.15160075  0.04007335  0.3457735 ]\n",
      "[ episode 276 ][ timestamp 40 ] state=[-0.01590211 -0.15160075  0.04007335  0.3457735 ], action=1, reward=1.0, next_state=[-0.01893412  0.04292893  0.04698882  0.06599165]\n",
      "[ episode 276 ][ timestamp 41 ] state=[-0.01893412  0.04292893  0.04698882  0.06599165], action=0, reward=1.0, next_state=[-0.01807554 -0.15283411  0.04830865  0.37312173]\n",
      "[ episode 276 ][ timestamp 42 ] state=[-0.01807554 -0.15283411  0.04830865  0.37312173], action=1, reward=1.0, next_state=[-0.02113223  0.04156949  0.05577109  0.09605375]\n",
      "[ episode 276 ][ timestamp 43 ] state=[-0.02113223  0.04156949  0.05577109  0.09605375], action=0, reward=1.0, next_state=[-0.02030084 -0.15430561  0.05769216  0.40579778]\n",
      "[ episode 276 ][ timestamp 44 ] state=[-0.02030084 -0.15430561  0.05769216  0.40579778], action=1, reward=1.0, next_state=[-0.02338695  0.03995279  0.06580812  0.13184705]\n",
      "[ episode 276 ][ timestamp 45 ] state=[-0.02338695  0.03995279  0.06580812  0.13184705], action=0, reward=1.0, next_state=[-0.02258789 -0.15604709  0.06844506  0.44454411]\n",
      "[ episode 276 ][ timestamp 46 ] state=[-0.02258789 -0.15604709  0.06844506  0.44454411], action=1, reward=1.0, next_state=[-0.02570883  0.03804308  0.07733594  0.17419768]\n",
      "[ episode 276 ][ timestamp 47 ] state=[-0.02570883  0.03804308  0.07733594  0.17419768], action=1, reward=1.0, next_state=[-0.02494797  0.23197793  0.0808199  -0.09312099]\n",
      "[ episode 276 ][ timestamp 48 ] state=[-0.02494797  0.23197793  0.0808199  -0.09312099], action=1, reward=1.0, next_state=[-0.02030841  0.42585407  0.07895748 -0.35925075]\n",
      "[ episode 276 ][ timestamp 49 ] state=[-0.02030841  0.42585407  0.07895748 -0.35925075], action=0, reward=1.0, next_state=[-0.01179133  0.22970367  0.07177246 -0.04275244]\n",
      "[ episode 276 ][ timestamp 50 ] state=[-0.01179133  0.22970367  0.07177246 -0.04275244], action=0, reward=1.0, next_state=[-0.00719726  0.03362984  0.07091741  0.27168411]\n",
      "[ episode 276 ][ timestamp 51 ] state=[-0.00719726  0.03362984  0.07091741  0.27168411], action=0, reward=1.0, next_state=[-0.00652466 -0.16242864  0.0763511   0.58586485]\n",
      "[ episode 276 ][ timestamp 52 ] state=[-0.00652466 -0.16242864  0.0763511   0.58586485], action=0, reward=1.0, next_state=[-0.00977323 -0.3585323   0.08806839  0.90158879]\n",
      "[ episode 276 ][ timestamp 53 ] state=[-0.00977323 -0.3585323   0.08806839  0.90158879], action=1, reward=1.0, next_state=[-0.01694388 -0.16470682  0.10610017  0.63783598]\n",
      "[ episode 276 ][ timestamp 54 ] state=[-0.01694388 -0.16470682  0.10610017  0.63783598], action=1, reward=1.0, next_state=[-0.02023802  0.02878814  0.11885689  0.38036063]\n",
      "[ episode 276 ][ timestamp 55 ] state=[-0.02023802  0.02878814  0.11885689  0.38036063], action=0, reward=1.0, next_state=[-0.01966225 -0.16780339  0.1264641   0.70802915]\n",
      "[ episode 276 ][ timestamp 56 ] state=[-0.01966225 -0.16780339  0.1264641   0.70802915], action=1, reward=1.0, next_state=[-0.02301832  0.025361    0.14062468  0.45767788]\n",
      "[ episode 276 ][ timestamp 57 ] state=[-0.02301832  0.025361    0.14062468  0.45767788], action=0, reward=1.0, next_state=[-0.0225111  -0.17143969  0.14977824  0.79117242]\n",
      "[ episode 276 ][ timestamp 58 ] state=[-0.0225111  -0.17143969  0.14977824  0.79117242], action=1, reward=1.0, next_state=[-0.0259399   0.02134324  0.16560169  0.54910591]\n",
      "[ episode 276 ][ timestamp 59 ] state=[-0.0259399   0.02134324  0.16560169  0.54910591], action=1, reward=1.0, next_state=[-0.02551303  0.21379922  0.17658381  0.31283599]\n",
      "[ episode 276 ][ timestamp 60 ] state=[-0.02551303  0.21379922  0.17658381  0.31283599], action=0, reward=1.0, next_state=[-0.02123705  0.01665903  0.18284053  0.6555941 ]\n",
      "[ episode 276 ][ timestamp 61 ] state=[-0.02123705  0.01665903  0.18284053  0.6555941 ], action=1, reward=1.0, next_state=[-0.02090387  0.20882821  0.19595241  0.42560128]\n",
      "[ episode 276 ][ timestamp 62 ] state=[-0.02090387  0.20882821  0.19595241  0.42560128], action=1, reward=1.0, next_state=[-0.0167273   0.40071347  0.20446443  0.20052369]\n",
      "[ episode 276 ][ timestamp 63 ] state=[-0.0167273   0.40071347  0.20446443  0.20052369], action=0, reward=1.0, next_state=[-0.00871303  0.20334359  0.20847491  0.55010623]\n",
      "[ episode 276 ][ timestamp 64 ] state=[-0.00871303  0.20334359  0.20847491  0.55010623], action=1, reward=-1.0, next_state=[-0.00464616  0.3950219   0.21947703  0.32966278]\n",
      "[ Ended! ] Episode 276: Exploration_rate=0.2519690537792925. Score=64.\n",
      "[ Experience replay ] starts\n",
      "[ episode 277 ] state=[ 0.04830466 -0.00226298  0.01032555 -0.00675793]\n",
      "[ episode 277 ][ timestamp 1 ] state=[ 0.04830466 -0.00226298  0.01032555 -0.00675793], action=1, reward=1.0, next_state=[ 0.0482594   0.19270938  0.01019039 -0.29616522]\n",
      "[ episode 277 ][ timestamp 2 ] state=[ 0.0482594   0.19270938  0.01019039 -0.29616522], action=1, reward=1.0, next_state=[ 0.05211359  0.38768458  0.00426709 -0.58561692]\n",
      "[ episode 277 ][ timestamp 3 ] state=[ 0.05211359  0.38768458  0.00426709 -0.58561692], action=0, reward=1.0, next_state=[ 0.05986728  0.19250312 -0.00744525 -0.29159288]\n",
      "[ episode 277 ][ timestamp 4 ] state=[ 0.05986728  0.19250312 -0.00744525 -0.29159288], action=1, reward=1.0, next_state=[ 0.06371734  0.38773044 -0.01327711 -0.58661461]\n",
      "[ episode 277 ][ timestamp 5 ] state=[ 0.06371734  0.38773044 -0.01327711 -0.58661461], action=0, reward=1.0, next_state=[ 0.07147195  0.19279693 -0.0250094  -0.29814349]\n",
      "[ episode 277 ][ timestamp 6 ] state=[ 0.07147195  0.19279693 -0.0250094  -0.29814349], action=1, reward=1.0, next_state=[ 0.07532789  0.38826629 -0.03097227 -0.59860783]\n",
      "[ episode 277 ][ timestamp 7 ] state=[ 0.07532789  0.38826629 -0.03097227 -0.59860783], action=0, reward=1.0, next_state=[ 0.08309321  0.19359108 -0.04294442 -0.31583945]\n",
      "[ episode 277 ][ timestamp 8 ] state=[ 0.08309321  0.19359108 -0.04294442 -0.31583945], action=1, reward=1.0, next_state=[ 0.08696503  0.38929759 -0.04926121 -0.62175034]\n",
      "[ episode 277 ][ timestamp 9 ] state=[ 0.08696503  0.38929759 -0.04926121 -0.62175034], action=0, reward=1.0, next_state=[ 0.09475099  0.19489689 -0.06169622 -0.34497998]\n",
      "[ episode 277 ][ timestamp 10 ] state=[ 0.09475099  0.19489689 -0.06169622 -0.34497998], action=1, reward=1.0, next_state=[ 0.09864892  0.39083975 -0.06859582 -0.65646224]\n",
      "[ episode 277 ][ timestamp 11 ] state=[ 0.09864892  0.39083975 -0.06859582 -0.65646224], action=0, reward=1.0, next_state=[ 0.10646572  0.19673639 -0.08172506 -0.38614329]\n",
      "[ episode 277 ][ timestamp 12 ] state=[ 0.10646572  0.19673639 -0.08172506 -0.38614329], action=1, reward=1.0, next_state=[ 0.11040045  0.39291765 -0.08944793 -0.70343344]\n",
      "[ episode 277 ][ timestamp 13 ] state=[ 0.11040045  0.39291765 -0.08944793 -0.70343344], action=0, reward=1.0, next_state=[ 0.1182588   0.1991417  -0.1035166  -0.44019417]\n",
      "[ episode 277 ][ timestamp 14 ] state=[ 0.1182588   0.1991417  -0.1035166  -0.44019417], action=0, reward=1.0, next_state=[ 0.12224163  0.0056254  -0.11232048 -0.18185313]\n",
      "[ episode 277 ][ timestamp 15 ] state=[ 0.12224163  0.0056254  -0.11232048 -0.18185313], action=1, reward=1.0, next_state=[ 0.12235414  0.2021604  -0.11595755 -0.50775082]\n",
      "[ episode 277 ][ timestamp 16 ] state=[ 0.12235414  0.2021604  -0.11595755 -0.50775082], action=1, reward=1.0, next_state=[ 0.12639735  0.39870887 -0.12611256 -0.8346088 ]\n",
      "[ episode 277 ][ timestamp 17 ] state=[ 0.12639735  0.39870887 -0.12611256 -0.8346088 ], action=0, reward=1.0, next_state=[ 0.13437153  0.2055146  -0.14280474 -0.58409771]\n",
      "[ episode 277 ][ timestamp 18 ] state=[ 0.13437153  0.2055146  -0.14280474 -0.58409771], action=1, reward=1.0, next_state=[ 0.13848182  0.4023178  -0.15448669 -0.91813958]\n",
      "[ episode 277 ][ timestamp 19 ] state=[ 0.13848182  0.4023178  -0.15448669 -0.91813958], action=0, reward=1.0, next_state=[ 0.14652818  0.20958369 -0.17284948 -0.67772006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 277 ][ timestamp 20 ] state=[ 0.14652818  0.20958369 -0.17284948 -0.67772006], action=0, reward=1.0, next_state=[ 0.15071985  0.01723084 -0.18640388 -0.44405532]\n",
      "[ episode 277 ][ timestamp 21 ] state=[ 0.15071985  0.01723084 -0.18640388 -0.44405532], action=1, reward=1.0, next_state=[ 0.15106447  0.21443335 -0.19528499 -0.78922083]\n",
      "[ episode 277 ][ timestamp 22 ] state=[ 0.15106447  0.21443335 -0.19528499 -0.78922083], action=0, reward=-1.0, next_state=[ 0.15535313  0.02245243 -0.21106941 -0.56377265]\n",
      "[ Ended! ] Episode 277: Exploration_rate=0.2507092085103961. Score=22.\n",
      "[ Experience replay ] starts\n",
      "[ episode 278 ] state=[ 0.02486152  0.01258941  0.01766504 -0.00914656]\n",
      "[ episode 278 ][ timestamp 1 ] state=[ 0.02486152  0.01258941  0.01766504 -0.00914656], action=0, reward=1.0, next_state=[ 0.02511331 -0.18278137  0.01748211  0.28905713]\n",
      "[ episode 278 ][ timestamp 2 ] state=[ 0.02511331 -0.18278137  0.01748211  0.28905713], action=0, reward=1.0, next_state=[ 0.02145768 -0.37814819  0.02326326  0.58720207]\n",
      "[ episode 278 ][ timestamp 3 ] state=[ 0.02145768 -0.37814819  0.02326326  0.58720207], action=0, reward=1.0, next_state=[ 0.01389472 -0.57358809  0.0350073   0.88712138]\n",
      "[ episode 278 ][ timestamp 4 ] state=[ 0.01389472 -0.57358809  0.0350073   0.88712138], action=0, reward=1.0, next_state=[ 0.00242296 -0.76916729  0.05274972  1.19060046]\n",
      "[ episode 278 ][ timestamp 5 ] state=[ 0.00242296 -0.76916729  0.05274972  1.19060046], action=1, reward=1.0, next_state=[-0.01296039 -0.57476707  0.07656173  0.91490696]\n",
      "[ episode 278 ][ timestamp 6 ] state=[-0.01296039 -0.57476707  0.07656173  0.91490696], action=1, reward=1.0, next_state=[-0.02445573 -0.38075942  0.09485987  0.64723516]\n",
      "[ episode 278 ][ timestamp 7 ] state=[-0.02445573 -0.38075942  0.09485987  0.64723516], action=0, reward=1.0, next_state=[-0.03207092 -0.57706608  0.10780458  0.9682183 ]\n",
      "[ episode 278 ][ timestamp 8 ] state=[-0.03207092 -0.57706608  0.10780458  0.9682183 ], action=1, reward=1.0, next_state=[-0.04361224 -0.38354378  0.12716894  0.71125322]\n",
      "[ episode 278 ][ timestamp 9 ] state=[-0.04361224 -0.38354378  0.12716894  0.71125322], action=0, reward=1.0, next_state=[-0.05128312 -0.5801759   0.14139401  1.04110665]\n",
      "[ episode 278 ][ timestamp 10 ] state=[-0.05128312 -0.5801759   0.14139401  1.04110665], action=1, reward=1.0, next_state=[-0.06288663 -0.38718624  0.16221614  0.79594253]\n",
      "[ episode 278 ][ timestamp 11 ] state=[-0.06288663 -0.38718624  0.16221614  0.79594253], action=0, reward=1.0, next_state=[-0.07063036 -0.58411833  0.17813499  1.13494527]\n",
      "[ episode 278 ][ timestamp 12 ] state=[-0.07063036 -0.58411833  0.17813499  1.13494527], action=0, reward=1.0, next_state=[-0.08231273 -0.78106615  0.2008339   1.47778736]\n",
      "[ episode 278 ][ timestamp 13 ] state=[-0.08231273 -0.78106615  0.2008339   1.47778736], action=0, reward=-1.0, next_state=[-0.09793405 -0.97799309  0.23038964  1.82588963]\n",
      "[ Ended! ] Episode 278: Exploration_rate=0.2494556624678441. Score=13.\n",
      "[ Experience replay ] starts\n",
      "[ episode 279 ] state=[ 0.01219859  0.03773277 -0.02692827  0.03049198]\n",
      "[ episode 279 ][ timestamp 1 ] state=[ 0.01219859  0.03773277 -0.02692827  0.03049198], action=0, reward=1.0, next_state=[ 0.01295325 -0.15699287 -0.02631843  0.31455859]\n",
      "[ episode 279 ][ timestamp 2 ] state=[ 0.01295325 -0.15699287 -0.02631843  0.31455859], action=1, reward=1.0, next_state=[ 0.00981339  0.0384939  -0.02002726  0.01369325]\n",
      "[ episode 279 ][ timestamp 3 ] state=[ 0.00981339  0.0384939  -0.02002726  0.01369325], action=0, reward=1.0, next_state=[ 0.01058327 -0.15633519 -0.0197534   0.29999067]\n",
      "[ episode 279 ][ timestamp 4 ] state=[ 0.01058327 -0.15633519 -0.0197534   0.29999067], action=1, reward=1.0, next_state=[ 0.00745657  0.03906267 -0.01375358  0.00114394]\n",
      "[ episode 279 ][ timestamp 5 ] state=[ 0.00745657  0.03906267 -0.01375358  0.00114394], action=0, reward=1.0, next_state=[ 0.00823782 -0.15585937 -0.0137307   0.28945591]\n",
      "[ episode 279 ][ timestamp 6 ] state=[ 0.00823782 -0.15585937 -0.0137307   0.28945591], action=1, reward=1.0, next_state=[ 0.00512063  0.03945566 -0.00794159 -0.00752572]\n",
      "[ episode 279 ][ timestamp 7 ] state=[ 0.00512063  0.03945566 -0.00794159 -0.00752572], action=0, reward=1.0, next_state=[ 0.00590975 -0.1555515  -0.0080921   0.282641  ]\n",
      "[ episode 279 ][ timestamp 8 ] state=[ 0.00590975 -0.1555515  -0.0080921   0.282641  ], action=0, reward=1.0, next_state=[ 0.00279872 -0.3505571  -0.00243928  0.57276077]\n",
      "[ episode 279 ][ timestamp 9 ] state=[ 0.00279872 -0.3505571  -0.00243928  0.57276077], action=1, reward=1.0, next_state=[-0.00421243 -0.15540103  0.00901594  0.27931039]\n",
      "[ episode 279 ][ timestamp 10 ] state=[-0.00421243 -0.15540103  0.00901594  0.27931039], action=0, reward=1.0, next_state=[-0.00732045 -0.35065043  0.01460214  0.57482324]\n",
      "[ episode 279 ][ timestamp 11 ] state=[-0.00732045 -0.35065043  0.01460214  0.57482324], action=1, reward=1.0, next_state=[-0.01433346 -0.1557362   0.02609861  0.28677594]\n",
      "[ episode 279 ][ timestamp 12 ] state=[-0.01433346 -0.1557362   0.02609861  0.28677594], action=0, reward=1.0, next_state=[-0.01744818 -0.35122044  0.03183413  0.58757456]\n",
      "[ episode 279 ][ timestamp 13 ] state=[-0.01744818 -0.35122044  0.03183413  0.58757456], action=1, reward=1.0, next_state=[-0.02447259 -0.15655844  0.04358562  0.30508715]\n",
      "[ episode 279 ][ timestamp 14 ] state=[-0.02447259 -0.15655844  0.04358562  0.30508715], action=0, reward=1.0, next_state=[-0.02760376 -0.35227353  0.04968736  0.6111911 ]\n",
      "[ episode 279 ][ timestamp 15 ] state=[-0.02760376 -0.35227353  0.04968736  0.6111911 ], action=1, reward=1.0, next_state=[-0.03464923 -0.15787999  0.06191118  0.33456272]\n",
      "[ episode 279 ][ timestamp 16 ] state=[-0.03464923 -0.15787999  0.06191118  0.33456272], action=1, reward=1.0, next_state=[-0.03780683  0.0363087   0.06860244  0.06202802]\n",
      "[ episode 279 ][ timestamp 17 ] state=[-0.03780683  0.0363087   0.06860244  0.06202802], action=0, reward=1.0, next_state=[-0.03708065 -0.15972637  0.069843    0.37554225]\n",
      "[ episode 279 ][ timestamp 18 ] state=[-0.03708065 -0.15972637  0.069843    0.37554225], action=1, reward=1.0, next_state=[-0.04027518  0.03433763  0.07735384  0.1056731 ]\n",
      "[ episode 279 ][ timestamp 19 ] state=[-0.04027518  0.03433763  0.07735384  0.1056731 ], action=0, reward=1.0, next_state=[-0.03958843 -0.16180272  0.0794673   0.4217232 ]\n",
      "[ episode 279 ][ timestamp 20 ] state=[-0.03958843 -0.16180272  0.0794673   0.4217232 ], action=1, reward=1.0, next_state=[-0.04282448  0.03210871  0.08790177  0.1551128 ]\n",
      "[ episode 279 ][ timestamp 21 ] state=[-0.04282448  0.03210871  0.08790177  0.1551128 ], action=0, reward=1.0, next_state=[-0.04218231 -0.16415466  0.09100403  0.47418109]\n",
      "[ episode 279 ][ timestamp 22 ] state=[-0.04218231 -0.16415466  0.09100403  0.47418109], action=1, reward=1.0, next_state=[-0.0454654   0.02957221  0.10048765  0.21151151]\n",
      "[ episode 279 ][ timestamp 23 ] state=[-0.0454654   0.02957221  0.10048765  0.21151151], action=0, reward=1.0, next_state=[-0.04487396 -0.16683219  0.10471788  0.5341256 ]\n",
      "[ episode 279 ][ timestamp 24 ] state=[-0.04487396 -0.16683219  0.10471788  0.5341256 ], action=1, reward=1.0, next_state=[-0.0482106   0.0266733   0.11540039  0.27618819]\n",
      "[ episode 279 ][ timestamp 25 ] state=[-0.0482106   0.0266733   0.11540039  0.27618819], action=0, reward=1.0, next_state=[-0.04767713 -0.16988974  0.12092415  0.60292414]\n",
      "[ episode 279 ][ timestamp 26 ] state=[-0.04767713 -0.16988974  0.12092415  0.60292414], action=1, reward=1.0, next_state=[-0.05107493  0.02335186  0.13298264  0.35064355]\n",
      "[ episode 279 ][ timestamp 27 ] state=[-0.05107493  0.02335186  0.13298264  0.35064355], action=0, reward=1.0, next_state=[-0.05060789 -0.17338587  0.13999551  0.68212636]\n",
      "[ episode 279 ][ timestamp 28 ] state=[-0.05060789 -0.17338587  0.13999551  0.68212636], action=1, reward=1.0, next_state=[-0.05407561  0.01954295  0.15363803  0.43658875]\n",
      "[ episode 279 ][ timestamp 29 ] state=[-0.05407561  0.01954295  0.15363803  0.43658875], action=0, reward=1.0, next_state=[-0.05368475 -0.17738197  0.16236981  0.77348881]\n",
      "[ episode 279 ][ timestamp 30 ] state=[-0.05368475 -0.17738197  0.16236981  0.77348881], action=1, reward=1.0, next_state=[-0.05723239  0.01517822  0.17783959  0.5359749 ]\n",
      "[ episode 279 ][ timestamp 31 ] state=[-0.05723239  0.01517822  0.17783959  0.5359749 ], action=0, reward=1.0, next_state=[-0.05692883 -0.18193982  0.18855908  0.87899826]\n",
      "[ episode 279 ][ timestamp 32 ] state=[-0.05692883 -0.18193982  0.18855908  0.87899826], action=1, reward=1.0, next_state=[-0.06056762  0.01018892  0.20613905  0.65102173]\n",
      "[ episode 279 ][ timestamp 33 ] state=[-0.06056762  0.01018892  0.20613905  0.65102173], action=1, reward=-1.0, next_state=[-0.06036384  0.20193442  0.21915948  0.42966937]\n",
      "[ Ended! ] Episode 279: Exploration_rate=0.24820838415550486. Score=33.\n",
      "[ Experience replay ] starts\n",
      "[ episode 280 ] state=[-0.02392808  0.00211522  0.04313152  0.01873915]\n",
      "[ episode 280 ][ timestamp 1 ] state=[-0.02392808  0.00211522  0.04313152  0.01873915], action=1, reward=1.0, next_state=[-0.02388578  0.19659293  0.0435063  -0.26002938]\n",
      "[ episode 280 ][ timestamp 2 ] state=[-0.02388578  0.19659293  0.0435063  -0.26002938], action=1, reward=1.0, next_state=[-0.01995392  0.39106767  0.03830571 -0.53867865]\n",
      "[ episode 280 ][ timestamp 3 ] state=[-0.01995392  0.39106767  0.03830571 -0.53867865], action=1, reward=1.0, next_state=[-0.01213256  0.58563077  0.02753214 -0.81905007]\n",
      "[ episode 280 ][ timestamp 4 ] state=[-0.01213256  0.58563077  0.02753214 -0.81905007], action=1, reward=1.0, next_state=[-4.19947842e-04  7.80365284e-01  1.11511359e-02 -1.10294772e+00]\n",
      "[ episode 280 ][ timestamp 5 ] state=[-4.19947842e-04  7.80365284e-01  1.11511359e-02 -1.10294772e+00], action=0, reward=1.0, next_state=[ 0.01518736  0.58509843 -0.01090782 -0.80678729]\n",
      "[ episode 280 ][ timestamp 6 ] state=[ 0.01518736  0.58509843 -0.01090782 -0.80678729], action=0, reward=1.0, next_state=[ 0.02688933  0.39012767 -0.02704356 -0.51755538]\n",
      "[ episode 280 ][ timestamp 7 ] state=[ 0.02688933  0.39012767 -0.02704356 -0.51755538], action=1, reward=1.0, next_state=[ 0.03469188  0.58561976 -0.03739467 -0.81863612]\n",
      "[ episode 280 ][ timestamp 8 ] state=[ 0.03469188  0.58561976 -0.03739467 -0.81863612], action=0, reward=1.0, next_state=[ 0.04640428  0.39102906 -0.05376739 -0.5379456 ]\n",
      "[ episode 280 ][ timestamp 9 ] state=[ 0.04640428  0.39102906 -0.05376739 -0.5379456 ], action=1, reward=1.0, next_state=[ 0.05422486  0.58686407 -0.06452631 -0.84707361]\n",
      "[ episode 280 ][ timestamp 10 ] state=[ 0.05422486  0.58686407 -0.06452631 -0.84707361], action=0, reward=1.0, next_state=[ 0.06596214  0.39267889 -0.08146778 -0.57535959]\n",
      "[ episode 280 ][ timestamp 11 ] state=[ 0.06596214  0.39267889 -0.08146778 -0.57535959], action=0, reward=1.0, next_state=[ 0.07381572  0.19878781 -0.09297497 -0.30941263]\n",
      "[ episode 280 ][ timestamp 12 ] state=[ 0.07381572  0.19878781 -0.09297497 -0.30941263], action=1, reward=1.0, next_state=[ 0.07779147  0.395103   -0.09916322 -0.62990884]\n",
      "[ episode 280 ][ timestamp 13 ] state=[ 0.07779147  0.395103   -0.09916322 -0.62990884], action=0, reward=1.0, next_state=[ 0.08569353  0.20149442 -0.1117614  -0.37002889]\n",
      "[ episode 280 ][ timestamp 14 ] state=[ 0.08569353  0.20149442 -0.1117614  -0.37002889], action=1, reward=1.0, next_state=[ 0.08972342  0.39801212 -0.11916198 -0.69575588]\n",
      "[ episode 280 ][ timestamp 15 ] state=[ 0.08972342  0.39801212 -0.11916198 -0.69575588], action=1, reward=1.0, next_state=[ 0.09768366  0.5945675  -0.1330771  -1.02344894]\n",
      "[ episode 280 ][ timestamp 16 ] state=[ 0.09768366  0.5945675  -0.1330771  -1.02344894], action=0, reward=1.0, next_state=[ 0.10957501  0.4014448  -0.15354607 -0.77533548]\n",
      "[ episode 280 ][ timestamp 17 ] state=[ 0.10957501  0.4014448  -0.15354607 -0.77533548], action=1, reward=1.0, next_state=[ 0.11760391  0.59830773 -0.16905278 -1.1121211 ]\n",
      "[ episode 280 ][ timestamp 18 ] state=[ 0.11760391  0.59830773 -0.16905278 -1.1121211 ], action=0, reward=1.0, next_state=[ 0.12957006  0.40576012 -0.19129521 -0.87688207]\n",
      "[ episode 280 ][ timestamp 19 ] state=[ 0.12957006  0.40576012 -0.19129521 -0.87688207], action=1, reward=1.0, next_state=[ 0.13768527  0.60289501 -0.20883285 -1.22308885]\n",
      "[ episode 280 ][ timestamp 20 ] state=[ 0.13768527  0.60289501 -0.20883285 -1.22308885], action=0, reward=-1.0, next_state=[ 0.14974317  0.41098269 -0.23329462 -1.00242629]\n",
      "[ Ended! ] Episode 280: Exploration_rate=0.24696734223472733. Score=20.\n",
      "[ Experience replay ] starts\n",
      "[ episode 281 ] state=[ 0.02962943  0.02891227  0.01148868 -0.02581117]\n",
      "[ episode 281 ][ timestamp 1 ] state=[ 0.02962943  0.02891227  0.01148868 -0.02581117], action=1, reward=1.0, next_state=[ 0.03020767  0.2238676   0.01097245 -0.31484726]\n",
      "[ episode 281 ][ timestamp 2 ] state=[ 0.03020767  0.2238676   0.01097245 -0.31484726], action=1, reward=1.0, next_state=[ 0.03468503  0.41883155  0.00467551 -0.60404974]\n",
      "[ episode 281 ][ timestamp 3 ] state=[ 0.03468503  0.41883155  0.00467551 -0.60404974], action=0, reward=1.0, next_state=[ 0.04306166  0.22364452 -0.00740549 -0.30989781]\n",
      "[ episode 281 ][ timestamp 4 ] state=[ 0.04306166  0.22364452 -0.00740549 -0.30989781], action=1, reward=1.0, next_state=[ 0.04753455  0.41887119 -0.01360344 -0.60490698]\n",
      "[ episode 281 ][ timestamp 5 ] state=[ 0.04753455  0.41887119 -0.01360344 -0.60490698], action=1, reward=1.0, next_state=[ 0.05591197  0.61418071 -0.02570158 -0.90184344]\n",
      "[ episode 281 ][ timestamp 6 ] state=[ 0.05591197  0.61418071 -0.02570158 -0.90184344], action=0, reward=1.0, next_state=[ 0.06819559  0.41941622 -0.04373845 -0.61734862]\n",
      "[ episode 281 ][ timestamp 7 ] state=[ 0.06819559  0.41941622 -0.04373845 -0.61734862], action=0, reward=1.0, next_state=[ 0.07658391  0.22493169 -0.05608542 -0.33875583]\n",
      "[ episode 281 ][ timestamp 8 ] state=[ 0.07658391  0.22493169 -0.05608542 -0.33875583], action=1, reward=1.0, next_state=[ 0.08108254  0.42080499 -0.06286054 -0.64858428]\n",
      "[ episode 281 ][ timestamp 9 ] state=[ 0.08108254  0.42080499 -0.06286054 -0.64858428], action=1, reward=1.0, next_state=[ 0.08949864  0.61674372 -0.07583223 -0.96038072]\n",
      "[ episode 281 ][ timestamp 10 ] state=[ 0.08949864  0.61674372 -0.07583223 -0.96038072], action=0, reward=1.0, next_state=[ 0.10183352  0.42271846 -0.09503984 -0.69245255]\n",
      "[ episode 281 ][ timestamp 11 ] state=[ 0.10183352  0.42271846 -0.09503984 -0.69245255], action=0, reward=1.0, next_state=[ 0.11028789  0.22903456 -0.10888889 -0.43113748]\n",
      "[ episode 281 ][ timestamp 12 ] state=[ 0.11028789  0.22903456 -0.10888889 -0.43113748], action=1, reward=1.0, next_state=[ 0.11486858  0.42551633 -0.11751164 -0.75606473]\n",
      "[ episode 281 ][ timestamp 13 ] state=[ 0.11486858  0.42551633 -0.11751164 -0.75606473], action=0, reward=1.0, next_state=[ 0.1233789   0.23219325 -0.13263294 -0.50254897]\n",
      "[ episode 281 ][ timestamp 14 ] state=[ 0.1233789   0.23219325 -0.13263294 -0.50254897], action=1, reward=1.0, next_state=[ 0.12802277  0.42891081 -0.14268392 -0.83391355]\n",
      "[ episode 281 ][ timestamp 15 ] state=[ 0.12802277  0.42891081 -0.14268392 -0.83391355], action=1, reward=1.0, next_state=[ 0.13660099  0.62566381 -0.15936219 -1.16785079]\n",
      "[ episode 281 ][ timestamp 16 ] state=[ 0.13660099  0.62566381 -0.15936219 -1.16785079], action=0, reward=1.0, next_state=[ 0.14911426  0.43293281 -0.1827192  -0.92907195]\n",
      "[ episode 281 ][ timestamp 17 ] state=[ 0.14911426  0.43293281 -0.1827192  -0.92907195], action=0, reward=1.0, next_state=[ 0.15777292  0.24068481 -0.20130064 -0.69892143]\n",
      "[ episode 281 ][ timestamp 18 ] state=[ 0.15777292  0.24068481 -0.20130064 -0.69892143], action=1, reward=-1.0, next_state=[ 0.16258661  0.43794404 -0.21527907 -1.047619  ]\n",
      "[ Ended! ] Episode 281: Exploration_rate=0.2457325055235537. Score=18.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 282 ] state=[ 0.02009807  0.00879812 -0.03012464  0.02945343]\n",
      "[ episode 282 ][ timestamp 1 ] state=[ 0.02009807  0.00879812 -0.03012464  0.02945343], action=1, reward=1.0, next_state=[ 0.02027404  0.20433883 -0.02953558 -0.27257987]\n",
      "[ episode 282 ][ timestamp 2 ] state=[ 0.02027404  0.20433883 -0.02953558 -0.27257987], action=1, reward=1.0, next_state=[ 0.02436081  0.3998695  -0.03498717 -0.57443016]\n",
      "[ episode 282 ][ timestamp 3 ] state=[ 0.02436081  0.3998695  -0.03498717 -0.57443016], action=0, reward=1.0, next_state=[ 0.0323582   0.20525507 -0.04647578 -0.29297129]\n",
      "[ episode 282 ][ timestamp 4 ] state=[ 0.0323582   0.20525507 -0.04647578 -0.29297129], action=1, reward=1.0, next_state=[ 0.0364633   0.40100779 -0.0523352  -0.59994227]\n",
      "[ episode 282 ][ timestamp 5 ] state=[ 0.0364633   0.40100779 -0.0523352  -0.59994227], action=0, reward=1.0, next_state=[ 0.04448346  0.20665558 -0.06433405 -0.32419263]\n",
      "[ episode 282 ][ timestamp 6 ] state=[ 0.04448346  0.20665558 -0.06433405 -0.32419263], action=0, reward=1.0, next_state=[ 0.04861657  0.01250585 -0.0708179  -0.05247167]\n",
      "[ episode 282 ][ timestamp 7 ] state=[ 0.04861657  0.01250585 -0.0708179  -0.05247167], action=1, reward=1.0, next_state=[ 0.04886669  0.20856803 -0.07186733 -0.36663083]\n",
      "[ episode 282 ][ timestamp 8 ] state=[ 0.04886669  0.20856803 -0.07186733 -0.36663083], action=1, reward=1.0, next_state=[ 0.05303805  0.40463373 -0.07919995 -0.68108103]\n",
      "[ episode 282 ][ timestamp 9 ] state=[ 0.05303805  0.40463373 -0.07919995 -0.68108103], action=0, reward=1.0, next_state=[ 0.06113072  0.2106959  -0.09282157 -0.41434663]\n",
      "[ episode 282 ][ timestamp 10 ] state=[ 0.06113072  0.2106959  -0.09282157 -0.41434663], action=1, reward=1.0, next_state=[ 0.06534464  0.40700249 -0.1011085  -0.73478929]\n",
      "[ episode 282 ][ timestamp 11 ] state=[ 0.06534464  0.40700249 -0.1011085  -0.73478929], action=0, reward=1.0, next_state=[ 0.07348469  0.21341187 -0.11580429 -0.47556167]\n",
      "[ episode 282 ][ timestamp 12 ] state=[ 0.07348469  0.21341187 -0.11580429 -0.47556167], action=1, reward=1.0, next_state=[ 0.07775293  0.40996231 -0.12531552 -0.80238306]\n",
      "[ episode 282 ][ timestamp 13 ] state=[ 0.07775293  0.40996231 -0.12531552 -0.80238306], action=0, reward=1.0, next_state=[ 0.08595217  0.21676114 -0.14136318 -0.55160026]\n",
      "[ episode 282 ][ timestamp 14 ] state=[ 0.08595217  0.21676114 -0.14136318 -0.55160026], action=1, reward=1.0, next_state=[ 0.0902874   0.41355588 -0.15239519 -0.88527028]\n",
      "[ episode 282 ][ timestamp 15 ] state=[ 0.0902874   0.41355588 -0.15239519 -0.88527028], action=0, reward=1.0, next_state=[ 0.09855852  0.22079479 -0.17010059 -0.64411066]\n",
      "[ episode 282 ][ timestamp 16 ] state=[ 0.09855852  0.22079479 -0.17010059 -0.64411066], action=1, reward=1.0, next_state=[ 0.10297441  0.41782779 -0.18298281 -0.98516347]\n",
      "[ episode 282 ][ timestamp 17 ] state=[ 0.10297441  0.41782779 -0.18298281 -0.98516347], action=1, reward=1.0, next_state=[ 0.11133097  0.61486613 -0.20268608 -1.32928398]\n",
      "[ episode 282 ][ timestamp 18 ] state=[ 0.11133097  0.61486613 -0.20268608 -1.32928398], action=0, reward=-1.0, next_state=[ 0.12362829  0.42279451 -0.22927176 -1.10625681]\n",
      "[ Ended! ] Episode 282: Exploration_rate=0.24450384299593592. Score=18.\n",
      "[ Experience replay ] starts\n",
      "[ episode 283 ] state=[ 0.04109828 -0.03567806  0.01142942 -0.00190164]\n",
      "[ episode 283 ][ timestamp 1 ] state=[ 0.04109828 -0.03567806  0.01142942 -0.00190164], action=1, reward=1.0, next_state=[ 0.04038472  0.15927813  0.01139139 -0.29095664]\n",
      "[ episode 283 ][ timestamp 2 ] state=[ 0.04038472  0.15927813  0.01139139 -0.29095664], action=0, reward=1.0, next_state=[ 0.04357028 -0.03600438  0.00557226  0.00529712]\n",
      "[ episode 283 ][ timestamp 3 ] state=[ 0.04357028 -0.03600438  0.00557226  0.00529712], action=1, reward=1.0, next_state=[ 0.0428502   0.15903721  0.0056782  -0.2856225 ]\n",
      "[ episode 283 ][ timestamp 4 ] state=[ 0.0428502   0.15903721  0.0056782  -0.2856225 ], action=0, reward=1.0, next_state=[ 4.60309402e-02 -3.61652590e-02 -3.42497633e-05  8.84587125e-03]\n",
      "[ episode 283 ][ timestamp 5 ] state=[ 4.60309402e-02 -3.61652590e-02 -3.42497633e-05  8.84587125e-03], action=1, reward=1.0, next_state=[ 4.53076350e-02  1.58957183e-01  1.42667662e-04 -2.83847862e-01]\n",
      "[ episode 283 ][ timestamp 6 ] state=[ 4.53076350e-02  1.58957183e-01  1.42667662e-04 -2.83847862e-01], action=1, reward=1.0, next_state=[ 0.04848678  0.3540771  -0.00553429 -0.57648579]\n",
      "[ episode 283 ][ timestamp 7 ] state=[ 0.04848678  0.3540771  -0.00553429 -0.57648579], action=0, reward=1.0, next_state=[ 0.05556832  0.15903316 -0.01706401 -0.28555143]\n",
      "[ episode 283 ][ timestamp 8 ] state=[ 0.05556832  0.15903316 -0.01706401 -0.28555143], action=1, reward=1.0, next_state=[ 0.05874898  0.35439427 -0.02277503 -0.583567  ]\n",
      "[ episode 283 ][ timestamp 9 ] state=[ 0.05874898  0.35439427 -0.02277503 -0.583567  ], action=0, reward=1.0, next_state=[ 0.06583687  0.15959866 -0.03444637 -0.29814465]\n",
      "[ episode 283 ][ timestamp 10 ] state=[ 0.06583687  0.15959866 -0.03444637 -0.29814465], action=0, reward=1.0, next_state=[ 0.06902884 -0.03501577 -0.04040927 -0.01652141]\n",
      "[ episode 283 ][ timestamp 11 ] state=[ 0.06902884 -0.03501577 -0.04040927 -0.01652141], action=1, reward=1.0, next_state=[ 0.06832853  0.1606617  -0.0407397  -0.3216751 ]\n",
      "[ episode 283 ][ timestamp 12 ] state=[ 0.06832853  0.1606617  -0.0407397  -0.3216751 ], action=0, reward=1.0, next_state=[ 0.07154176 -0.03385713 -0.0471732  -0.04211311]\n",
      "[ episode 283 ][ timestamp 13 ] state=[ 0.07154176 -0.03385713 -0.0471732  -0.04211311], action=1, reward=1.0, next_state=[ 0.07086462  0.16190841 -0.04801546 -0.34929854]\n",
      "[ episode 283 ][ timestamp 14 ] state=[ 0.07086462  0.16190841 -0.04801546 -0.34929854], action=0, reward=1.0, next_state=[ 0.07410279 -0.03249893 -0.05500143 -0.07213473]\n",
      "[ episode 283 ][ timestamp 15 ] state=[ 0.07410279 -0.03249893 -0.05500143 -0.07213473], action=1, reward=1.0, next_state=[ 0.07345281  0.16336664 -0.05644412 -0.38165108]\n",
      "[ episode 283 ][ timestamp 16 ] state=[ 0.07345281  0.16336664 -0.05644412 -0.38165108], action=1, reward=1.0, next_state=[ 0.07672014  0.35924274 -0.06407715 -0.69158307]\n",
      "[ episode 283 ][ timestamp 17 ] state=[ 0.07672014  0.35924274 -0.06407715 -0.69158307], action=0, reward=1.0, next_state=[ 0.083905    0.16506561 -0.07790881 -0.41974092]\n",
      "[ episode 283 ][ timestamp 18 ] state=[ 0.083905    0.16506561 -0.07790881 -0.41974092], action=0, reward=1.0, next_state=[ 0.08720631 -0.02887095 -0.08630363 -0.15260052]\n",
      "[ episode 283 ][ timestamp 19 ] state=[ 0.08720631 -0.02887095 -0.08630363 -0.15260052], action=1, reward=1.0, next_state=[ 0.08662889  0.16737394 -0.08935564 -0.47121404]\n",
      "[ episode 283 ][ timestamp 20 ] state=[ 0.08662889  0.16737394 -0.08935564 -0.47121404], action=1, reward=1.0, next_state=[ 0.08997637  0.36363688 -0.09877992 -0.79066957]\n",
      "[ episode 283 ][ timestamp 21 ] state=[ 0.08997637  0.36363688 -0.09877992 -0.79066957], action=1, reward=1.0, next_state=[ 0.09724911  0.55996642 -0.11459331 -1.11272237]\n",
      "[ episode 283 ][ timestamp 22 ] state=[ 0.09724911  0.55996642 -0.11459331 -1.11272237], action=0, reward=1.0, next_state=[ 0.10844843  0.3665205  -0.13684776 -0.85807334]\n",
      "[ episode 283 ][ timestamp 23 ] state=[ 0.10844843  0.3665205  -0.13684776 -0.85807334], action=0, reward=1.0, next_state=[ 0.11577884  0.17350158 -0.15400922 -0.61135956]\n",
      "[ episode 283 ][ timestamp 24 ] state=[ 0.11577884  0.17350158 -0.15400922 -0.61135956], action=0, reward=1.0, next_state=[ 0.11924888 -0.0191706  -0.16623641 -0.37087193]\n",
      "[ episode 283 ][ timestamp 25 ] state=[ 0.11924888 -0.0191706  -0.16623641 -0.37087193], action=1, reward=1.0, next_state=[ 0.11886546  0.17787476 -0.17365385 -0.71101414]\n",
      "[ episode 283 ][ timestamp 26 ] state=[ 0.11886546  0.17787476 -0.17365385 -0.71101414], action=1, reward=1.0, next_state=[ 0.12242296  0.37492175 -0.18787414 -1.05293726]\n",
      "[ episode 283 ][ timestamp 27 ] state=[ 0.12242296  0.37492175 -0.18787414 -1.05293726], action=0, reward=1.0, next_state=[ 0.12992139  0.1827198  -0.20893288 -0.82461809]\n",
      "[ episode 283 ][ timestamp 28 ] state=[ 0.12992139  0.1827198  -0.20893288 -0.82461809], action=1, reward=-1.0, next_state=[ 0.13357579  0.37999311 -0.22542524 -1.17507318]\n",
      "[ Ended! ] Episode 283: Exploration_rate=0.24328132378095624. Score=28.\n",
      "[ Experience replay ] starts\n",
      "[ episode 284 ] state=[-0.04173717 -0.04654033  0.03300641  0.03538015]\n",
      "[ episode 284 ][ timestamp 1 ] state=[-0.04173717 -0.04654033  0.03300641  0.03538015], action=1, reward=1.0, next_state=[-0.04266798  0.14809313  0.03371401 -0.24670892]\n",
      "[ episode 284 ][ timestamp 2 ] state=[-0.04266798  0.14809313  0.03371401 -0.24670892], action=0, reward=1.0, next_state=[-0.03970611 -0.0474937   0.02877983  0.05641466]\n",
      "[ episode 284 ][ timestamp 3 ] state=[-0.03970611 -0.0474937   0.02877983  0.05641466], action=0, reward=1.0, next_state=[-0.04065599 -0.24301623  0.02990813  0.3580371 ]\n",
      "[ episode 284 ][ timestamp 4 ] state=[-0.04065599 -0.24301623  0.02990813  0.3580371 ], action=1, reward=1.0, next_state=[-0.04551631 -0.04833195  0.03706887  0.07493296]\n",
      "[ episode 284 ][ timestamp 5 ] state=[-0.04551631 -0.04833195  0.03706887  0.07493296], action=1, reward=1.0, next_state=[-0.04648295  0.14623952  0.03856753 -0.20582799]\n",
      "[ episode 284 ][ timestamp 6 ] state=[-0.04648295  0.14623952  0.03856753 -0.20582799], action=0, reward=1.0, next_state=[-0.04355816 -0.04941212  0.03445097  0.09876727]\n",
      "[ episode 284 ][ timestamp 7 ] state=[-0.04355816 -0.04941212  0.03445097  0.09876727], action=1, reward=1.0, next_state=[-0.0445464   0.14519957  0.03642631 -0.18285048]\n",
      "[ episode 284 ][ timestamp 8 ] state=[-0.0445464   0.14519957  0.03642631 -0.18285048], action=0, reward=1.0, next_state=[-0.04164241 -0.05042415  0.0327693   0.12109742]\n",
      "[ episode 284 ][ timestamp 9 ] state=[-0.04164241 -0.05042415  0.0327693   0.12109742], action=1, reward=1.0, next_state=[-0.0426509   0.14421336  0.03519125 -0.16106965]\n",
      "[ episode 284 ][ timestamp 10 ] state=[-0.0426509   0.14421336  0.03519125 -0.16106965], action=0, reward=1.0, next_state=[-0.03976663 -0.05139426  0.03196986  0.14250421]\n",
      "[ episode 284 ][ timestamp 11 ] state=[-0.03976663 -0.05139426  0.03196986  0.14250421], action=1, reward=1.0, next_state=[-0.04079451  0.14325559  0.03481994 -0.13992384]\n",
      "[ episode 284 ][ timestamp 12 ] state=[-0.04079451  0.14325559  0.03481994 -0.13992384], action=0, reward=1.0, next_state=[-0.0379294  -0.05234732  0.03202147  0.16353767]\n",
      "[ episode 284 ][ timestamp 13 ] state=[-0.0379294  -0.05234732  0.03202147  0.16353767], action=1, reward=1.0, next_state=[-0.03897635  0.14230195  0.03529222 -0.11887385]\n",
      "[ episode 284 ][ timestamp 14 ] state=[-0.03897635  0.14230195  0.03529222 -0.11887385], action=0, reward=1.0, next_state=[-0.03613031 -0.05330742  0.03291474  0.18473125]\n",
      "[ episode 284 ][ timestamp 15 ] state=[-0.03613031 -0.05330742  0.03291474  0.18473125], action=0, reward=1.0, next_state=[-0.03719646 -0.24888448  0.03660937  0.48761313]\n",
      "[ episode 284 ][ timestamp 16 ] state=[-0.03719646 -0.24888448  0.03660937  0.48761313], action=1, reward=1.0, next_state=[-0.04217415 -0.05429767  0.04636163  0.20668925]\n",
      "[ episode 284 ][ timestamp 17 ] state=[-0.04217415 -0.05429767  0.04636163  0.20668925], action=1, reward=1.0, next_state=[-0.0432601   0.14013171  0.05049541 -0.07101602]\n",
      "[ episode 284 ][ timestamp 18 ] state=[-0.0432601   0.14013171  0.05049541 -0.07101602], action=0, reward=1.0, next_state=[-0.04045747 -0.05567644  0.04907509  0.23716118]\n",
      "[ episode 284 ][ timestamp 19 ] state=[-0.04045747 -0.05567644  0.04907509  0.23716118], action=1, reward=1.0, next_state=[-0.041571    0.1387113   0.05381832 -0.03964709]\n",
      "[ episode 284 ][ timestamp 20 ] state=[-0.041571    0.1387113   0.05381832 -0.03964709], action=0, reward=1.0, next_state=[-0.03879677 -0.05713945  0.05302538  0.26951863]\n",
      "[ episode 284 ][ timestamp 21 ] state=[-0.03879677 -0.05713945  0.05302538  0.26951863], action=1, reward=1.0, next_state=[-0.03993956  0.13718728  0.05841575 -0.00597962]\n",
      "[ episode 284 ][ timestamp 22 ] state=[-0.03993956  0.13718728  0.05841575 -0.00597962], action=0, reward=1.0, next_state=[-0.03719581 -0.05872167  0.05829616  0.30454703]\n",
      "[ episode 284 ][ timestamp 23 ] state=[-0.03719581 -0.05872167  0.05829616  0.30454703], action=1, reward=1.0, next_state=[-0.03837025  0.13552314  0.0643871   0.03080412]\n",
      "[ episode 284 ][ timestamp 24 ] state=[-0.03837025  0.13552314  0.0643871   0.03080412], action=0, reward=1.0, next_state=[-0.03565978 -0.06046023  0.06500318  0.34308677]\n",
      "[ episode 284 ][ timestamp 25 ] state=[-0.03565978 -0.06046023  0.06500318  0.34308677], action=1, reward=1.0, next_state=[-0.03686899  0.13367962  0.07186491  0.07158949]\n",
      "[ episode 284 ][ timestamp 26 ] state=[-0.03686899  0.13367962  0.07186491  0.07158949], action=0, reward=1.0, next_state=[-0.0341954  -0.06239511  0.0732967   0.38605253]\n",
      "[ episode 284 ][ timestamp 27 ] state=[-0.0341954  -0.06239511  0.0732967   0.38605253], action=1, reward=1.0, next_state=[-0.0354433   0.13161394  0.08101775  0.11735027]\n",
      "[ episode 284 ][ timestamp 28 ] state=[-0.0354433   0.13161394  0.08101775  0.11735027], action=1, reward=1.0, next_state=[-0.03281102  0.32548724  0.08336476 -0.1487126 ]\n",
      "[ episode 284 ][ timestamp 29 ] state=[-0.03281102  0.32548724  0.08336476 -0.1487126 ], action=0, reward=1.0, next_state=[-0.02630127  0.12927658  0.08039051  0.16906212]\n",
      "[ episode 284 ][ timestamp 30 ] state=[-0.02630127  0.12927658  0.08039051  0.16906212], action=0, reward=1.0, next_state=[-0.02371574 -0.06689852  0.08377175  0.48598379]\n",
      "[ episode 284 ][ timestamp 31 ] state=[-0.02371574 -0.06689852  0.08377175  0.48598379], action=1, reward=1.0, next_state=[-0.02505371  0.12694761  0.09349143  0.22083437]\n",
      "[ episode 284 ][ timestamp 32 ] state=[-0.02505371  0.12694761  0.09349143  0.22083437], action=1, reward=1.0, next_state=[-0.02251476  0.3206175   0.09790811 -0.04095533]\n",
      "[ episode 284 ][ timestamp 33 ] state=[-0.02251476  0.3206175   0.09790811 -0.04095533], action=0, reward=1.0, next_state=[-0.01610241  0.12423782  0.09708901  0.28094246]\n",
      "[ episode 284 ][ timestamp 34 ] state=[-0.01610241  0.12423782  0.09708901  0.28094246], action=1, reward=1.0, next_state=[-0.01361765  0.31785045  0.10270786  0.02039057]\n",
      "[ episode 284 ][ timestamp 35 ] state=[-0.01361765  0.31785045  0.10270786  0.02039057], action=0, reward=1.0, next_state=[-0.00726065  0.12141699  0.10311567  0.34363106]\n",
      "[ episode 284 ][ timestamp 36 ] state=[-0.00726065  0.12141699  0.10311567  0.34363106], action=1, reward=1.0, next_state=[-0.00483231  0.3149324   0.10998829  0.0851621 ]\n",
      "[ episode 284 ][ timestamp 37 ] state=[-0.00483231  0.3149324   0.10998829  0.0851621 ], action=0, reward=1.0, next_state=[0.00146634 0.11841977 0.11169153 0.41042126]\n",
      "[ episode 284 ][ timestamp 38 ] state=[0.00146634 0.11841977 0.11169153 0.41042126], action=1, reward=1.0, next_state=[0.00383474 0.31179573 0.11989996 0.1549338 ]\n",
      "[ episode 284 ][ timestamp 39 ] state=[0.00383474 0.31179573 0.11989996 0.1549338 ], action=1, reward=1.0, next_state=[ 0.01007065  0.5050151   0.12299863 -0.09764827]\n",
      "[ episode 284 ][ timestamp 40 ] state=[ 0.01007065  0.5050151   0.12299863 -0.09764827], action=0, reward=1.0, next_state=[0.02017095 0.30836464 0.12104567 0.23116943]\n",
      "[ episode 284 ][ timestamp 41 ] state=[0.02017095 0.30836464 0.12104567 0.23116943], action=0, reward=1.0, next_state=[0.02633825 0.11173969 0.12566906 0.55944935]\n",
      "[ episode 284 ][ timestamp 42 ] state=[0.02633825 0.11173969 0.12566906 0.55944935], action=1, reward=1.0, next_state=[0.02857304 0.30489452 0.13685804 0.30885146]\n",
      "[ episode 284 ][ timestamp 43 ] state=[0.02857304 0.30489452 0.13685804 0.30885146], action=1, reward=1.0, next_state=[0.03467093 0.49782807 0.14303507 0.06226793]\n",
      "[ episode 284 ][ timestamp 44 ] state=[0.03467093 0.49782807 0.14303507 0.06226793], action=1, reward=1.0, next_state=[ 0.04462749  0.69064042  0.14428043 -0.182088  ]\n",
      "[ episode 284 ][ timestamp 45 ] state=[ 0.04462749  0.69064042  0.14428043 -0.182088  ], action=0, reward=1.0, next_state=[0.0584403  0.4937803  0.14063867 0.15240543]\n",
      "[ episode 284 ][ timestamp 46 ] state=[0.0584403  0.4937803  0.14063867 0.15240543], action=1, reward=1.0, next_state=[ 0.06831591  0.68663767  0.14368678 -0.09281282]\n",
      "[ episode 284 ][ timestamp 47 ] state=[ 0.06831591  0.68663767  0.14368678 -0.09281282], action=0, reward=1.0, next_state=[0.08204866 0.48977984 0.14183052 0.24152965]\n",
      "[ episode 284 ][ timestamp 48 ] state=[0.08204866 0.48977984 0.14183052 0.24152965], action=1, reward=1.0, next_state=[ 0.09184426  0.68262101  0.14666112 -0.00326908]\n",
      "[ episode 284 ][ timestamp 49 ] state=[ 0.09184426  0.68262101  0.14666112 -0.00326908], action=1, reward=1.0, next_state=[ 0.10549668  0.87536846  0.14659573 -0.24632245]\n",
      "[ episode 284 ][ timestamp 50 ] state=[ 0.10549668  0.87536846  0.14659573 -0.24632245], action=0, reward=1.0, next_state=[0.12300405 0.67849013 0.14166928 0.08877244]\n",
      "[ episode 284 ][ timestamp 51 ] state=[0.12300405 0.67849013 0.14166928 0.08877244], action=1, reward=1.0, next_state=[ 0.13657385  0.87132719  0.14344473 -0.15607372]\n",
      "[ episode 284 ][ timestamp 52 ] state=[ 0.13657385  0.87132719  0.14344473 -0.15607372], action=0, reward=1.0, next_state=[0.15400039 0.67447389 0.14032326 0.17820182]\n",
      "[ episode 284 ][ timestamp 53 ] state=[0.15400039 0.67447389 0.14032326 0.17820182], action=1, reward=1.0, next_state=[ 0.16748987  0.86733799  0.1438873  -0.06713101]\n",
      "[ episode 284 ][ timestamp 54 ] state=[ 0.16748987  0.86733799  0.1438873  -0.06713101], action=0, reward=1.0, next_state=[0.18483663 0.67047765 0.14254468 0.26726503]\n",
      "[ episode 284 ][ timestamp 55 ] state=[0.18483663 0.67047765 0.14254468 0.26726503], action=1, reward=1.0, next_state=[0.19824618 0.86330803 0.14788998 0.02271944]\n",
      "[ episode 284 ][ timestamp 56 ] state=[0.19824618 0.86330803 0.14788998 0.02271944], action=1, reward=1.0, next_state=[ 0.21551234  1.0560337   0.14834436 -0.2198921 ]\n",
      "[ episode 284 ][ timestamp 57 ] state=[ 0.21551234  1.0560337   0.14834436 -0.2198921 ], action=0, reward=1.0, next_state=[0.23663302 0.85913704 0.14394652 0.11566261]\n",
      "[ episode 284 ][ timestamp 58 ] state=[0.23663302 0.85913704 0.14394652 0.11566261], action=1, reward=1.0, next_state=[ 0.25381576  1.05193457  0.14625978 -0.1283684 ]\n",
      "[ episode 284 ][ timestamp 59 ] state=[ 0.25381576  1.05193457  0.14625978 -0.1283684 ], action=0, reward=1.0, next_state=[0.27485445 0.85505315 0.14369241 0.20664783]\n",
      "[ episode 284 ][ timestamp 60 ] state=[0.27485445 0.85505315 0.14369241 0.20664783], action=1, reward=1.0, next_state=[ 0.29195551  1.04785926  0.14782536 -0.03748042]\n",
      "[ episode 284 ][ timestamp 61 ] state=[ 0.29195551  1.04785926  0.14782536 -0.03748042], action=1, reward=1.0, next_state=[ 0.3129127   1.24058621  0.14707576 -0.28011539]\n",
      "[ episode 284 ][ timestamp 62 ] state=[ 0.3129127   1.24058621  0.14707576 -0.28011539], action=0, reward=1.0, next_state=[0.33772442 1.04370586 0.14147345 0.05510136]\n",
      "[ episode 284 ][ timestamp 63 ] state=[0.33772442 1.04370586 0.14147345 0.05510136], action=1, reward=1.0, next_state=[ 0.35859854  1.23654572  0.14257547 -0.18981395]\n",
      "[ episode 284 ][ timestamp 64 ] state=[ 0.35859854  1.23654572  0.14257547 -0.18981395], action=0, reward=1.0, next_state=[0.38332945 1.03970243 0.1387792  0.14423036]\n",
      "[ episode 284 ][ timestamp 65 ] state=[0.38332945 1.03970243 0.1387792  0.14423036], action=0, reward=1.0, next_state=[0.4041235  0.84289402 0.1416638  0.47727492]\n",
      "[ episode 284 ][ timestamp 66 ] state=[0.4041235  0.84289402 0.1416638  0.47727492], action=1, reward=1.0, next_state=[0.42098138 1.03576143 0.1512093  0.23238188]\n",
      "[ episode 284 ][ timestamp 67 ] state=[0.42098138 1.03576143 0.1512093  0.23238188], action=0, reward=1.0, next_state=[0.44169661 0.8388387  0.15585694 0.56868186]\n",
      "[ episode 284 ][ timestamp 68 ] state=[0.44169661 0.8388387  0.15585694 0.56868186], action=1, reward=1.0, next_state=[0.45847339 1.03147081 0.16723058 0.32887272]\n",
      "[ episode 284 ][ timestamp 69 ] state=[0.45847339 1.03147081 0.16723058 0.32887272], action=1, reward=1.0, next_state=[0.4791028  1.22386642 0.17380803 0.09324228]\n",
      "[ episode 284 ][ timestamp 70 ] state=[0.4791028  1.22386642 0.17380803 0.09324228], action=1, reward=1.0, next_state=[ 0.50358013  1.41612636  0.17567288 -0.1399599 ]\n",
      "[ episode 284 ][ timestamp 71 ] state=[ 0.50358013  1.41612636  0.17567288 -0.1399599 ], action=1, reward=1.0, next_state=[ 0.53190266  1.60835413  0.17287368 -0.37248116]\n",
      "[ episode 284 ][ timestamp 72 ] state=[ 0.53190266  1.60835413  0.17287368 -0.37248116], action=1, reward=1.0, next_state=[ 0.56406974  1.80065287  0.16542406 -0.60605773]\n",
      "[ episode 284 ][ timestamp 73 ] state=[ 0.56406974  1.80065287  0.16542406 -0.60605773], action=0, reward=1.0, next_state=[ 0.6000828   1.60365143  0.1533029  -0.26617641]\n",
      "[ episode 284 ][ timestamp 74 ] state=[ 0.6000828   1.60365143  0.1533029  -0.26617641], action=0, reward=1.0, next_state=[0.63215583 1.40671175 0.14797937 0.07066329]\n",
      "[ episode 284 ][ timestamp 75 ] state=[0.63215583 1.40671175 0.14797937 0.07066329], action=1, reward=1.0, next_state=[ 0.66029006  1.59943648  0.14939264 -0.17191704]\n",
      "[ episode 284 ][ timestamp 76 ] state=[ 0.66029006  1.59943648  0.14939264 -0.17191704], action=0, reward=1.0, next_state=[0.69227879 1.40252722 0.1459543  0.16391523]\n",
      "[ episode 284 ][ timestamp 77 ] state=[0.69227879 1.40252722 0.1459543  0.16391523], action=0, reward=1.0, next_state=[0.72032934 1.20565019 0.1492326  0.49884922]\n",
      "[ episode 284 ][ timestamp 78 ] state=[0.72032934 1.20565019 0.1492326  0.49884922], action=1, reward=1.0, next_state=[0.74444234 1.39838791 0.15920959 0.25666765]\n",
      "[ episode 284 ][ timestamp 79 ] state=[0.74444234 1.39838791 0.15920959 0.25666765], action=1, reward=1.0, next_state=[0.7724101  1.59092105 0.16434294 0.01813053]\n",
      "[ episode 284 ][ timestamp 80 ] state=[0.7724101  1.59092105 0.16434294 0.01813053], action=1, reward=1.0, next_state=[ 0.80422852  1.78335147  0.16470555 -0.21852628]\n",
      "[ episode 284 ][ timestamp 81 ] state=[ 0.80422852  1.78335147  0.16470555 -0.21852628], action=0, reward=1.0, next_state=[0.83989555 1.58630513 0.16033502 0.12124798]\n",
      "[ episode 284 ][ timestamp 82 ] state=[0.83989555 1.58630513 0.16033502 0.12124798], action=0, reward=1.0, next_state=[0.87162165 1.38929266 0.16275998 0.45991311]\n",
      "[ episode 284 ][ timestamp 83 ] state=[0.87162165 1.38929266 0.16275998 0.45991311], action=1, reward=1.0, next_state=[0.8994075  1.581785   0.17195825 0.22263106]\n",
      "[ episode 284 ][ timestamp 84 ] state=[0.8994075  1.581785   0.17195825 0.22263106], action=1, reward=1.0, next_state=[ 0.9310432   1.77408544  0.17641087 -0.01125846]\n",
      "[ episode 284 ][ timestamp 85 ] state=[ 0.9310432   1.77408544  0.17641087 -0.01125846], action=1, reward=1.0, next_state=[ 0.96652491  1.96629642  0.1761857  -0.24350404]\n",
      "[ episode 284 ][ timestamp 86 ] state=[ 0.96652491  1.96629642  0.1761857  -0.24350404], action=0, reward=1.0, next_state=[1.00585084 1.76915298 0.17131562 0.09916431]\n",
      "[ episode 284 ][ timestamp 87 ] state=[1.00585084 1.76915298 0.17131562 0.09916431], action=1, reward=1.0, next_state=[ 1.0412339   1.96145849  0.1732989  -0.13495053]\n",
      "[ episode 284 ][ timestamp 88 ] state=[ 1.0412339   1.96145849  0.1732989  -0.13495053], action=1, reward=1.0, next_state=[ 1.08046307  2.1537293   0.17059989 -0.36834156]\n",
      "[ episode 284 ][ timestamp 89 ] state=[ 1.08046307  2.1537293   0.17059989 -0.36834156], action=1, reward=1.0, next_state=[ 1.12353766  2.34606863  0.16323306 -0.60274886]\n",
      "[ episode 284 ][ timestamp 90 ] state=[ 1.12353766  2.34606863  0.16323306 -0.60274886], action=0, reward=1.0, next_state=[ 1.17045903  2.14908529  0.15117808 -0.2634239 ]\n",
      "[ episode 284 ][ timestamp 91 ] state=[ 1.17045903  2.14908529  0.15117808 -0.2634239 ], action=0, reward=1.0, next_state=[1.21344074 1.95216511 0.14590961 0.0728646 ]\n",
      "[ episode 284 ][ timestamp 92 ] state=[1.21344074 1.95216511 0.14590961 0.0728646 ], action=1, reward=1.0, next_state=[ 1.25248404  2.14492667  0.1473669  -0.17045996]\n",
      "[ episode 284 ][ timestamp 93 ] state=[ 1.25248404  2.14492667  0.1473669  -0.17045996], action=0, reward=1.0, next_state=[1.29538257 1.9480365  0.1439577  0.16484341]\n",
      "[ episode 284 ][ timestamp 94 ] state=[1.29538257 1.9480365  0.1439577  0.16484341], action=1, reward=1.0, next_state=[ 1.3343433   2.14083576  0.14725457 -0.07918645]\n",
      "[ episode 284 ][ timestamp 95 ] state=[ 1.3343433   2.14083576  0.14725457 -0.07918645], action=0, reward=1.0, next_state=[1.37716002 1.9439434  0.14567084 0.25609237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 284 ][ timestamp 96 ] state=[1.37716002 1.9439434  0.14567084 0.25609237], action=1, reward=1.0, next_state=[1.41603888 2.13671773 0.15079269 0.01266938]\n",
      "[ episode 284 ][ timestamp 97 ] state=[1.41603888 2.13671773 0.15079269 0.01266938], action=1, reward=1.0, next_state=[ 1.45877324  2.3293916   0.15104607 -0.22889659]\n",
      "[ episode 284 ][ timestamp 98 ] state=[ 1.45877324  2.3293916   0.15104607 -0.22889659], action=0, reward=1.0, next_state=[1.50536107 2.13247016 0.14646814 0.10736128]\n",
      "[ episode 284 ][ timestamp 99 ] state=[1.50536107 2.13247016 0.14646814 0.10736128], action=1, reward=1.0, next_state=[ 1.54801047  2.32522269  0.14861537 -0.13576389]\n",
      "[ episode 284 ][ timestamp 100 ] state=[ 1.54801047  2.32522269  0.14861537 -0.13576389], action=0, reward=1.0, next_state=[1.59451493 2.12831916 0.14590009 0.19986798]\n",
      "[ episode 284 ][ timestamp 101 ] state=[1.59451493 2.12831916 0.14590009 0.19986798], action=1, reward=1.0, next_state=[ 1.63708131  2.3210858   0.14989745 -0.04346729]\n",
      "[ episode 284 ][ timestamp 102 ] state=[ 1.63708131  2.3210858   0.14989745 -0.04346729], action=1, reward=1.0, next_state=[ 1.68350303  2.5137759   0.1490281  -0.28535632]\n",
      "[ episode 284 ][ timestamp 103 ] state=[ 1.68350303  2.5137759   0.1490281  -0.28535632], action=1, reward=1.0, next_state=[ 1.73377855  2.70649301  0.14332098 -0.52757555]\n",
      "[ episode 284 ][ timestamp 104 ] state=[ 1.73377855  2.70649301  0.14332098 -0.52757555], action=0, reward=1.0, next_state=[ 1.78790841  2.50967623  0.13276947 -0.19338504]\n",
      "[ episode 284 ][ timestamp 105 ] state=[ 1.78790841  2.50967623  0.13276947 -0.19338504], action=0, reward=1.0, next_state=[1.83810193 2.31292959 0.12890177 0.13805725]\n",
      "[ episode 284 ][ timestamp 106 ] state=[1.83810193 2.31292959 0.12890177 0.13805725], action=1, reward=1.0, next_state=[ 1.88436052  2.50599227  0.13166291 -0.11134195]\n",
      "[ episode 284 ][ timestamp 107 ] state=[ 1.88436052  2.50599227  0.13166291 -0.11134195], action=0, reward=1.0, next_state=[1.93448037 2.30925353 0.12943607 0.21980915]\n",
      "[ episode 284 ][ timestamp 108 ] state=[1.93448037 2.30925353 0.12943607 0.21980915], action=1, reward=1.0, next_state=[ 1.98066544  2.50231058  0.13383225 -0.02940596]\n",
      "[ episode 284 ][ timestamp 109 ] state=[ 1.98066544  2.50231058  0.13383225 -0.02940596], action=0, reward=1.0, next_state=[2.03071165 2.30554856 0.13324413 0.30232717]\n",
      "[ episode 284 ][ timestamp 110 ] state=[2.03071165 2.30554856 0.13324413 0.30232717], action=1, reward=1.0, next_state=[2.07682262 2.49854474 0.13929068 0.0544569 ]\n",
      "[ episode 284 ][ timestamp 111 ] state=[2.07682262 2.49854474 0.13929068 0.0544569 ], action=1, reward=1.0, next_state=[ 2.12679351  2.69142315  0.14037982 -0.19123942]\n",
      "[ episode 284 ][ timestamp 112 ] state=[ 2.12679351  2.69142315  0.14037982 -0.19123942], action=0, reward=1.0, next_state=[2.18062198 2.49460123 0.13655503 0.14222548]\n",
      "[ episode 284 ][ timestamp 113 ] state=[2.18062198 2.49460123 0.13655503 0.14222548], action=1, reward=1.0, next_state=[ 2.230514    2.68753011  0.13939954 -0.10445132]\n",
      "[ episode 284 ][ timestamp 114 ] state=[ 2.230514    2.68753011  0.13939954 -0.10445132], action=1, reward=1.0, next_state=[ 2.2842646   2.88040767  0.13731051 -0.35011032]\n",
      "[ episode 284 ][ timestamp 115 ] state=[ 2.2842646   2.88040767  0.13731051 -0.35011032], action=1, reward=1.0, next_state=[ 2.34187276  3.07333693  0.1303083  -0.59653779]\n",
      "[ episode 284 ][ timestamp 116 ] state=[ 2.34187276  3.07333693  0.1303083  -0.59653779], action=0, reward=-1.0, next_state=[ 2.4033395   2.87665535  0.11837755 -0.26581435]\n",
      "[ Ended! ] Episode 284: Exploration_rate=0.24206491716205145. Score=116.\n",
      "[ Experience replay ] starts\n",
      "[ episode 285 ] state=[0.03600277 0.01561071 0.03360163 0.03455977]\n",
      "[ episode 285 ][ timestamp 1 ] state=[0.03600277 0.01561071 0.03360163 0.03455977], action=1, reward=1.0, next_state=[ 0.03631498  0.21023509  0.03429282 -0.24733499]\n",
      "[ episode 285 ][ timestamp 2 ] state=[ 0.03631498  0.21023509  0.03429282 -0.24733499], action=0, reward=1.0, next_state=[0.04051969 0.01464059 0.02934612 0.05596438]\n",
      "[ episode 285 ][ timestamp 3 ] state=[0.04051969 0.01464059 0.02934612 0.05596438], action=1, reward=1.0, next_state=[ 0.0408125   0.20932974  0.03046541 -0.22731708]\n",
      "[ episode 285 ][ timestamp 4 ] state=[ 0.0408125   0.20932974  0.03046541 -0.22731708], action=1, reward=1.0, next_state=[ 0.04499909  0.40400336  0.02591907 -0.51023657]\n",
      "[ episode 285 ][ timestamp 5 ] state=[ 0.04499909  0.40400336  0.02591907 -0.51023657], action=1, reward=1.0, next_state=[ 0.05307916  0.59875077  0.01571434 -0.79464021]\n",
      "[ episode 285 ][ timestamp 6 ] state=[ 0.05307916  0.59875077  0.01571434 -0.79464021], action=0, reward=1.0, next_state=[ 6.50541752e-02  4.03416703e-01 -1.78467635e-04 -4.97055458e-01]\n",
      "[ episode 285 ][ timestamp 7 ] state=[ 6.50541752e-02  4.03416703e-01 -1.78467635e-04 -4.97055458e-01], action=1, reward=1.0, next_state=[ 0.07312251  0.59854117 -0.01011958 -0.78979462]\n",
      "[ episode 285 ][ timestamp 8 ] state=[ 0.07312251  0.59854117 -0.01011958 -0.78979462], action=0, reward=1.0, next_state=[ 0.08509333  0.40355964 -0.02591547 -0.50031241]\n",
      "[ episode 285 ][ timestamp 9 ] state=[ 0.08509333  0.40355964 -0.02591547 -0.50031241], action=1, reward=1.0, next_state=[ 0.09316453  0.59903716 -0.03592172 -0.80104852]\n",
      "[ episode 285 ][ timestamp 10 ] state=[ 0.09316453  0.59903716 -0.03592172 -0.80104852], action=1, reward=1.0, next_state=[ 0.10514527  0.79463289 -0.05194269 -1.10481157]\n",
      "[ episode 285 ][ timestamp 11 ] state=[ 0.10514527  0.79463289 -0.05194269 -1.10481157], action=0, reward=1.0, next_state=[ 0.12103793  0.60023106 -0.07403892 -0.82886638]\n",
      "[ episode 285 ][ timestamp 12 ] state=[ 0.12103793  0.60023106 -0.07403892 -0.82886638], action=1, reward=1.0, next_state=[ 0.13304255  0.79628288 -0.09061625 -1.14388602]\n",
      "[ episode 285 ][ timestamp 13 ] state=[ 0.13304255  0.79628288 -0.09061625 -1.14388602], action=0, reward=1.0, next_state=[ 0.14896821  0.60245402 -0.11349397 -0.88094033]\n",
      "[ episode 285 ][ timestamp 14 ] state=[ 0.14896821  0.60245402 -0.11349397 -0.88094033], action=0, reward=1.0, next_state=[ 0.16101729  0.40904156 -0.13111277 -0.62598377]\n",
      "[ episode 285 ][ timestamp 15 ] state=[ 0.16101729  0.40904156 -0.13111277 -0.62598377], action=1, reward=1.0, next_state=[ 0.16919812  0.60572642 -0.14363245 -0.95691566]\n",
      "[ episode 285 ][ timestamp 16 ] state=[ 0.16919812  0.60572642 -0.14363245 -0.95691566], action=0, reward=1.0, next_state=[ 0.18131265  0.41279752 -0.16277076 -0.7125852 ]\n",
      "[ episode 285 ][ timestamp 17 ] state=[ 0.18131265  0.41279752 -0.16277076 -0.7125852 ], action=0, reward=1.0, next_state=[ 0.1895686   0.22025862 -0.17702247 -0.47523787]\n",
      "[ episode 285 ][ timestamp 18 ] state=[ 0.1895686   0.22025862 -0.17702247 -0.47523787], action=1, reward=1.0, next_state=[ 0.19397377  0.41738053 -0.18652722 -0.81807314]\n",
      "[ episode 285 ][ timestamp 19 ] state=[ 0.19397377  0.41738053 -0.18652722 -0.81807314], action=0, reward=1.0, next_state=[ 0.20232138  0.22523447 -0.20288869 -0.589375  ]\n",
      "[ episode 285 ][ timestamp 20 ] state=[ 0.20232138  0.22523447 -0.20288869 -0.589375  ], action=1, reward=-1.0, next_state=[ 0.20682607  0.42253251 -0.21467619 -0.93849266]\n",
      "[ Ended! ] Episode 285: Exploration_rate=0.2408545925762412. Score=20.\n",
      "[ Experience replay ] starts\n",
      "[ episode 286 ] state=[-0.01789238  0.02631775  0.01686206  0.02487838]\n",
      "[ episode 286 ][ timestamp 1 ] state=[-0.01789238  0.02631775  0.01686206  0.02487838], action=0, reward=1.0, next_state=[-0.01736603 -0.16904191  0.01735963  0.32283342]\n",
      "[ episode 286 ][ timestamp 2 ] state=[-0.01736603 -0.16904191  0.01735963  0.32283342], action=0, reward=1.0, next_state=[-0.02074687 -0.3644067   0.0238163   0.62093993]\n",
      "[ episode 286 ][ timestamp 3 ] state=[-0.02074687 -0.3644067   0.0238163   0.62093993], action=1, reward=1.0, next_state=[-0.028035   -0.1696253   0.0362351   0.33585202]\n",
      "[ episode 286 ][ timestamp 4 ] state=[-0.028035   -0.1696253   0.0362351   0.33585202], action=1, reward=1.0, next_state=[-0.03142751  0.02496274  0.04295214  0.05481234]\n",
      "[ episode 286 ][ timestamp 5 ] state=[-0.03142751  0.02496274  0.04295214  0.05481234], action=1, reward=1.0, next_state=[-0.03092825  0.21944334  0.04404839 -0.22401546]\n",
      "[ episode 286 ][ timestamp 6 ] state=[-0.03092825  0.21944334  0.04404839 -0.22401546], action=1, reward=1.0, next_state=[-0.02653938  0.41390896  0.03956808 -0.5024849 ]\n",
      "[ episode 286 ][ timestamp 7 ] state=[-0.02653938  0.41390896  0.03956808 -0.5024849 ], action=0, reward=1.0, next_state=[-0.0182612   0.21825228  0.02951838 -0.19759962]\n",
      "[ episode 286 ][ timestamp 8 ] state=[-0.0182612   0.21825228  0.02951838 -0.19759962], action=1, reward=1.0, next_state=[-0.01389616  0.41293986  0.02556639 -0.48082662]\n",
      "[ episode 286 ][ timestamp 9 ] state=[-0.01389616  0.41293986  0.02556639 -0.48082662], action=0, reward=1.0, next_state=[-0.00563736  0.21746652  0.01594985 -0.18019674]\n",
      "[ episode 286 ][ timestamp 10 ] state=[-0.00563736  0.21746652  0.01594985 -0.18019674], action=0, reward=1.0, next_state=[-0.00128803  0.02212     0.01234592  0.11747482]\n",
      "[ episode 286 ][ timestamp 11 ] state=[-0.00128803  0.02212     0.01234592  0.11747482], action=1, reward=1.0, next_state=[-0.00084563  0.21706291  0.01469542 -0.17128764]\n",
      "[ episode 286 ][ timestamp 12 ] state=[-0.00084563  0.21706291  0.01469542 -0.17128764], action=1, reward=1.0, next_state=[ 0.00349563  0.41197147  0.01126966 -0.45929863]\n",
      "[ episode 286 ][ timestamp 13 ] state=[ 0.00349563  0.41197147  0.01126966 -0.45929863], action=0, reward=1.0, next_state=[ 0.01173506  0.21669205  0.00208369 -0.16308488]\n",
      "[ episode 286 ][ timestamp 14 ] state=[ 0.01173506  0.21669205  0.00208369 -0.16308488], action=1, reward=1.0, next_state=[ 0.0160689   0.41178411 -0.00117801 -0.45510973]\n",
      "[ episode 286 ][ timestamp 15 ] state=[ 0.0160689   0.41178411 -0.00117801 -0.45510973], action=0, reward=1.0, next_state=[ 0.02430458  0.21667883 -0.0102802  -0.16279836]\n",
      "[ episode 286 ][ timestamp 16 ] state=[ 0.02430458  0.21667883 -0.0102802  -0.16279836], action=1, reward=1.0, next_state=[ 0.02863816  0.41194643 -0.01353617 -0.4587066 ]\n",
      "[ episode 286 ][ timestamp 17 ] state=[ 0.02863816  0.41194643 -0.01353617 -0.4587066 ], action=0, reward=1.0, next_state=[ 0.03687708  0.21701842 -0.0227103  -0.17032088]\n",
      "[ episode 286 ][ timestamp 18 ] state=[ 0.03687708  0.21701842 -0.0227103  -0.17032088], action=1, reward=1.0, next_state=[ 0.04121745  0.41245794 -0.02611672 -0.47008082]\n",
      "[ episode 286 ][ timestamp 19 ] state=[ 0.04121745  0.41245794 -0.02611672 -0.47008082], action=0, reward=1.0, next_state=[ 0.04946661  0.21771446 -0.03551834 -0.18574266]\n",
      "[ episode 286 ][ timestamp 20 ] state=[ 0.04946661  0.21771446 -0.03551834 -0.18574266], action=1, reward=1.0, next_state=[ 0.0538209   0.41332612 -0.03923319 -0.48941529]\n",
      "[ episode 286 ][ timestamp 21 ] state=[ 0.0538209   0.41332612 -0.03923319 -0.48941529], action=0, reward=1.0, next_state=[ 0.06208742  0.21877899 -0.04902149 -0.20935075]\n",
      "[ episode 286 ][ timestamp 22 ] state=[ 0.06208742  0.21877899 -0.04902149 -0.20935075], action=1, reward=1.0, next_state=[ 0.066463    0.41456636 -0.05320851 -0.51708555]\n",
      "[ episode 286 ][ timestamp 23 ] state=[ 0.066463    0.41456636 -0.05320851 -0.51708555], action=0, reward=1.0, next_state=[ 0.07475433  0.2202324  -0.06355022 -0.24163308]\n",
      "[ episode 286 ][ timestamp 24 ] state=[ 0.07475433  0.2202324  -0.06355022 -0.24163308], action=1, reward=1.0, next_state=[ 0.07915898  0.41620186 -0.06838288 -0.55366507]\n",
      "[ episode 286 ][ timestamp 25 ] state=[ 0.07915898  0.41620186 -0.06838288 -0.55366507], action=1, reward=1.0, next_state=[ 0.08748302  0.61221406 -0.07945618 -0.8670851 ]\n",
      "[ episode 286 ][ timestamp 26 ] state=[ 0.08748302  0.61221406 -0.07945618 -0.8670851 ], action=0, reward=1.0, next_state=[ 0.0997273   0.41825803 -0.09679789 -0.60040449]\n",
      "[ episode 286 ][ timestamp 27 ] state=[ 0.0997273   0.41825803 -0.09679789 -0.60040449], action=0, reward=1.0, next_state=[ 0.10809246  0.224614   -0.10880598 -0.33971236]\n",
      "[ episode 286 ][ timestamp 28 ] state=[ 0.10809246  0.224614   -0.10880598 -0.33971236], action=1, reward=1.0, next_state=[ 0.11258474  0.42110234 -0.11560022 -0.66462783]\n",
      "[ episode 286 ][ timestamp 29 ] state=[ 0.11258474  0.42110234 -0.11560022 -0.66462783], action=0, reward=1.0, next_state=[ 0.12100678  0.22776204 -0.12889278 -0.4104638 ]\n",
      "[ episode 286 ][ timestamp 30 ] state=[ 0.12100678  0.22776204 -0.12889278 -0.4104638 ], action=1, reward=1.0, next_state=[ 0.12556203  0.42445319 -0.13710206 -0.74084278]\n",
      "[ episode 286 ][ timestamp 31 ] state=[ 0.12556203  0.42445319 -0.13710206 -0.74084278], action=0, reward=1.0, next_state=[ 0.13405109  0.23146353 -0.15191891 -0.49425659]\n",
      "[ episode 286 ][ timestamp 32 ] state=[ 0.13405109  0.23146353 -0.15191891 -0.49425659], action=1, reward=1.0, next_state=[ 0.13868036  0.42836482 -0.16180404 -0.83069937]\n",
      "[ episode 286 ][ timestamp 33 ] state=[ 0.13868036  0.42836482 -0.16180404 -0.83069937], action=0, reward=1.0, next_state=[ 0.14724766  0.23578034 -0.17841803 -0.59295898]\n",
      "[ episode 286 ][ timestamp 34 ] state=[ 0.14724766  0.23578034 -0.17841803 -0.59295898], action=1, reward=1.0, next_state=[ 0.15196326  0.43289197 -0.19027721 -0.93610994]\n",
      "[ episode 286 ][ timestamp 35 ] state=[ 0.15196326  0.43289197 -0.19027721 -0.93610994], action=0, reward=1.0, next_state=[ 0.1606211   0.24077462 -0.20899941 -0.70873952]\n",
      "[ episode 286 ][ timestamp 36 ] state=[ 0.1606211   0.24077462 -0.20899941 -0.70873952], action=1, reward=-1.0, next_state=[ 0.16543659  0.43808423 -0.2231742  -1.05926291]\n",
      "[ Ended! ] Episode 286: Exploration_rate=0.23965031961336. Score=36.\n",
      "[ Experience replay ] starts\n",
      "[ episode 287 ] state=[ 0.00681531 -0.01760415 -0.04047668  0.00628327]\n",
      "[ episode 287 ][ timestamp 1 ] state=[ 0.00681531 -0.01760415 -0.04047668  0.00628327], action=0, reward=1.0, next_state=[ 0.00646322 -0.21212294 -0.04035102  0.28592557]\n",
      "[ episode 287 ][ timestamp 2 ] state=[ 0.00646322 -0.21212294 -0.04035102  0.28592557], action=1, reward=1.0, next_state=[ 0.00222076 -0.01644944 -0.0346325  -0.01920575]\n",
      "[ episode 287 ][ timestamp 3 ] state=[ 0.00222076 -0.01644944 -0.0346325  -0.01920575], action=0, reward=1.0, next_state=[ 0.00189177 -0.21105805 -0.03501662  0.2623522 ]\n",
      "[ episode 287 ][ timestamp 4 ] state=[ 0.00189177 -0.21105805 -0.03501662  0.2623522 ], action=1, reward=1.0, next_state=[-0.00232939 -0.01545421 -0.02976957 -0.04116647]\n",
      "[ episode 287 ][ timestamp 5 ] state=[-0.00232939 -0.01545421 -0.02976957 -0.04116647], action=0, reward=1.0, next_state=[-0.00263847 -0.2101369  -0.0305929   0.24197721]\n",
      "[ episode 287 ][ timestamp 6 ] state=[-0.00263847 -0.2101369  -0.0305929   0.24197721], action=1, reward=1.0, next_state=[-0.00684121 -0.01459161 -0.02575336 -0.06019638]\n",
      "[ episode 287 ][ timestamp 7 ] state=[-0.00684121 -0.01459161 -0.02575336 -0.06019638], action=0, reward=1.0, next_state=[-0.00713304 -0.20933503 -0.02695729  0.22425123]\n",
      "[ episode 287 ][ timestamp 8 ] state=[-0.00713304 -0.20933503 -0.02695729  0.22425123], action=1, reward=1.0, next_state=[-0.01131974 -0.01383837 -0.02247226 -0.0768117 ]\n",
      "[ episode 287 ][ timestamp 9 ] state=[-0.01131974 -0.01383837 -0.02247226 -0.0768117 ], action=0, reward=1.0, next_state=[-0.01159651 -0.20863108 -0.0240085   0.2086973 ]\n",
      "[ episode 287 ][ timestamp 10 ] state=[-0.01159651 -0.20863108 -0.0240085   0.2086973 ], action=1, reward=1.0, next_state=[-0.01576913 -0.01317421 -0.01983455 -0.09146134]\n",
      "[ episode 287 ][ timestamp 11 ] state=[-0.01576913 -0.01317421 -0.01983455 -0.09146134], action=0, reward=1.0, next_state=[-0.01603261 -0.20800633 -0.02166378  0.19489839]\n",
      "[ episode 287 ][ timestamp 12 ] state=[-0.01603261 -0.20800633 -0.02166378  0.19489839], action=1, reward=1.0, next_state=[-0.02019274 -0.0125813  -0.01776581 -0.10453903]\n",
      "[ episode 287 ][ timestamp 13 ] state=[-0.02019274 -0.0125813  -0.01776581 -0.10453903], action=0, reward=1.0, next_state=[-0.02044437 -0.2074442  -0.01985659  0.18248633]\n",
      "[ episode 287 ][ timestamp 14 ] state=[-0.02044437 -0.2074442  -0.01985659  0.18248633], action=1, reward=1.0, next_state=[-0.02459325 -0.01204384 -0.01620686 -0.11639389]\n",
      "[ episode 287 ][ timestamp 15 ] state=[-0.02459325 -0.01204384 -0.01620686 -0.11639389], action=0, reward=1.0, next_state=[-0.02483413 -0.20692987 -0.01853474  0.17113215]\n",
      "[ episode 287 ][ timestamp 16 ] state=[-0.02483413 -0.20692987 -0.01853474  0.17113215], action=1, reward=1.0, next_state=[-0.02897273 -0.0115476  -0.0151121  -0.12733981]\n",
      "[ episode 287 ][ timestamp 17 ] state=[-0.02897273 -0.0115476  -0.0151121  -0.12733981], action=0, reward=1.0, next_state=[-0.02920368 -0.20644984 -0.01765889  0.16053738]\n",
      "[ episode 287 ][ timestamp 18 ] state=[-0.02920368 -0.20644984 -0.01765889  0.16053738], action=1, reward=1.0, next_state=[-0.03333267 -0.01107959 -0.01444815 -0.13766375]\n",
      "[ episode 287 ][ timestamp 19 ] state=[-0.03333267 -0.01107959 -0.01444815 -0.13766375], action=0, reward=1.0, next_state=[-0.03355427 -0.20599165 -0.01720142  0.15042622]\n",
      "[ episode 287 ][ timestamp 20 ] state=[-0.03355427 -0.20599165 -0.01720142  0.15042622], action=1, reward=1.0, next_state=[-0.0376741  -0.01062766 -0.0141929  -0.14763338]\n",
      "[ episode 287 ][ timestamp 21 ] state=[-0.0376741  -0.01062766 -0.0141929  -0.14763338], action=0, reward=1.0, next_state=[-0.03788665 -0.20554352 -0.01714557  0.14053839]\n",
      "[ episode 287 ][ timestamp 22 ] state=[-0.03788665 -0.20554352 -0.01714557  0.14053839], action=1, reward=1.0, next_state=[-0.04199752 -0.01018026 -0.0143348  -0.15750398]\n",
      "[ episode 287 ][ timestamp 23 ] state=[-0.04199752 -0.01018026 -0.0143348  -0.15750398], action=0, reward=1.0, next_state=[-0.04220113 -0.20509407 -0.01748488  0.13062242]\n",
      "[ episode 287 ][ timestamp 24 ] state=[-0.04220113 -0.20509407 -0.01748488  0.13062242], action=1, reward=1.0, next_state=[-0.04630301 -0.00972608 -0.01487243 -0.16752508]\n",
      "[ episode 287 ][ timestamp 25 ] state=[-0.04630301 -0.00972608 -0.01487243 -0.16752508], action=0, reward=1.0, next_state=[-0.04649753 -0.20463202 -0.01822293  0.12042917]\n",
      "[ episode 287 ][ timestamp 26 ] state=[-0.04649753 -0.20463202 -0.01822293  0.12042917], action=1, reward=1.0, next_state=[-0.05059017 -0.00925379 -0.01581435 -0.17794676]\n",
      "[ episode 287 ][ timestamp 27 ] state=[-0.05059017 -0.00925379 -0.01581435 -0.17794676], action=0, reward=1.0, next_state=[-0.05077525 -0.2041459  -0.01937328  0.10970562]\n",
      "[ episode 287 ][ timestamp 28 ] state=[-0.05077525 -0.2041459  -0.01937328  0.10970562], action=1, reward=1.0, next_state=[-0.05485816 -0.00875177 -0.01717917 -0.18902596]\n",
      "[ episode 287 ][ timestamp 29 ] state=[-0.05485816 -0.00875177 -0.01717917 -0.18902596], action=0, reward=1.0, next_state=[-0.0550332  -0.20362378 -0.02095969  0.09818851]\n",
      "[ episode 287 ][ timestamp 30 ] state=[-0.0550332  -0.20362378 -0.02095969  0.09818851], action=1, reward=1.0, next_state=[-0.05910568 -0.00820781 -0.01899592 -0.20103278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 287 ][ timestamp 31 ] state=[-0.05910568 -0.00820781 -0.01899592 -0.20103278], action=0, reward=1.0, next_state=[-0.05926983 -0.203053   -0.02301657  0.08559782]\n",
      "[ episode 287 ][ timestamp 32 ] state=[-0.05926983 -0.203053   -0.02301657  0.08559782], action=1, reward=1.0, next_state=[-0.06333089 -0.00760881 -0.02130462 -0.21425709]\n",
      "[ episode 287 ][ timestamp 33 ] state=[-0.06333089 -0.00760881 -0.02130462 -0.21425709], action=0, reward=1.0, next_state=[-0.06348307 -0.2024198  -0.02558976  0.07163   ]\n",
      "[ episode 287 ][ timestamp 34 ] state=[-0.06348307 -0.2024198  -0.02558976  0.07163   ], action=1, reward=1.0, next_state=[-0.06753146 -0.00694051 -0.02415716 -0.22901551]\n",
      "[ episode 287 ][ timestamp 35 ] state=[-0.06753146 -0.00694051 -0.02415716 -0.22901551], action=0, reward=1.0, next_state=[-0.06767027 -0.20170907 -0.02873747  0.05595057]\n",
      "[ episode 287 ][ timestamp 36 ] state=[-0.06767027 -0.20170907 -0.02873747  0.05595057], action=0, reward=1.0, next_state=[-0.07170446 -0.39640743 -0.02761846  0.33942989]\n",
      "[ episode 287 ][ timestamp 37 ] state=[-0.07170446 -0.39640743 -0.02761846  0.33942989], action=1, reward=1.0, next_state=[-0.0796326  -0.20090361 -0.02082986  0.03816719]\n",
      "[ episode 287 ][ timestamp 38 ] state=[-0.0796326  -0.20090361 -0.02082986  0.03816719], action=1, reward=1.0, next_state=[-0.08365068 -0.00548924 -0.02006652 -0.2610143 ]\n",
      "[ episode 287 ][ timestamp 39 ] state=[-0.08365068 -0.00548924 -0.02006652 -0.2610143 ], action=0, reward=1.0, next_state=[-0.08376046 -0.20031908 -0.0252868   0.02527246]\n",
      "[ episode 287 ][ timestamp 40 ] state=[-0.08376046 -0.20031908 -0.0252868   0.02527246], action=1, reward=1.0, next_state=[-0.08776684 -0.0048438  -0.02478135 -0.27528026]\n",
      "[ episode 287 ][ timestamp 41 ] state=[-0.08776684 -0.0048438  -0.02478135 -0.27528026], action=0, reward=1.0, next_state=[-0.08786372 -0.19960357 -0.03028696  0.00948474]\n",
      "[ episode 287 ][ timestamp 42 ] state=[-0.08786372 -0.19960357 -0.03028696  0.00948474], action=1, reward=1.0, next_state=[-0.09185579 -0.00406065 -0.03009726 -0.29259813]\n",
      "[ episode 287 ][ timestamp 43 ] state=[-0.09185579 -0.00406065 -0.03009726 -0.29259813], action=0, reward=1.0, next_state=[-0.091937   -0.19874084 -0.03594923 -0.00955737]\n",
      "[ episode 287 ][ timestamp 44 ] state=[-0.091937   -0.19874084 -0.03594923 -0.00955737], action=0, reward=1.0, next_state=[-0.09591182 -0.39332928 -0.03614037  0.27156991]\n",
      "[ episode 287 ][ timestamp 45 ] state=[-0.09591182 -0.39332928 -0.03614037  0.27156991], action=1, reward=1.0, next_state=[-0.10377841 -0.19771076 -0.03070898 -0.03228922]\n",
      "[ episode 287 ][ timestamp 46 ] state=[-0.10377841 -0.19771076 -0.03070898 -0.03228922], action=1, reward=1.0, next_state=[-0.10773262 -0.0021622  -0.03135476 -0.33450079]\n",
      "[ episode 287 ][ timestamp 47 ] state=[-0.10773262 -0.0021622  -0.03135476 -0.33450079], action=0, reward=1.0, next_state=[-0.10777586 -0.19682419 -0.03804478 -0.0518681 ]\n",
      "[ episode 287 ][ timestamp 48 ] state=[-0.10777586 -0.19682419 -0.03804478 -0.0518681 ], action=1, reward=1.0, next_state=[-0.11171235 -0.00117796 -0.03908214 -0.35630756]\n",
      "[ episode 287 ][ timestamp 49 ] state=[-0.11171235 -0.00117796 -0.03908214 -0.35630756], action=0, reward=1.0, next_state=[-0.11173591 -0.19572309 -0.04620829 -0.07619991]\n",
      "[ episode 287 ][ timestamp 50 ] state=[-0.11173591 -0.19572309 -0.04620829 -0.07619991], action=1, reward=1.0, next_state=[-1.15650370e-01  2.97851597e-05 -4.77322874e-02 -3.83096213e-01]\n",
      "[ episode 287 ][ timestamp 51 ] state=[-1.15650370e-01  2.97851597e-05 -4.77322874e-02 -3.83096213e-01], action=0, reward=1.0, next_state=[-0.11564977 -0.1943831  -0.05539421 -0.10583699]\n",
      "[ episode 287 ][ timestamp 52 ] state=[-0.11564977 -0.1943831  -0.05539421 -0.10583699], action=0, reward=1.0, next_state=[-0.11953744 -0.38866927 -0.05751095  0.16886767]\n",
      "[ episode 287 ][ timestamp 53 ] state=[-0.11953744 -0.38866927 -0.05751095  0.16886767], action=1, reward=1.0, next_state=[-0.12731082 -0.1927733  -0.0541336  -0.14138937]\n",
      "[ episode 287 ][ timestamp 54 ] state=[-0.12731082 -0.1927733  -0.0541336  -0.14138937], action=1, reward=1.0, next_state=[-0.13116629  0.00308048 -0.05696139 -0.4506472 ]\n",
      "[ episode 287 ][ timestamp 55 ] state=[-0.13116629  0.00308048 -0.05696139 -0.4506472 ], action=1, reward=1.0, next_state=[-0.13110468  0.19895984 -0.06597433 -0.7607273 ]\n",
      "[ episode 287 ][ timestamp 56 ] state=[-0.13110468  0.19895984 -0.06597433 -0.7607273 ], action=0, reward=1.0, next_state=[-0.12712548  0.00480584 -0.08118888 -0.48951226]\n",
      "[ episode 287 ][ timestamp 57 ] state=[-0.12712548  0.00480584 -0.08118888 -0.48951226], action=1, reward=1.0, next_state=[-0.12702936  0.20097368 -0.09097912 -0.80663807]\n",
      "[ episode 287 ][ timestamp 58 ] state=[-0.12702936  0.20097368 -0.09097912 -0.80663807], action=0, reward=1.0, next_state=[-0.12300989  0.00720867 -0.10711188 -0.54390358]\n",
      "[ episode 287 ][ timestamp 59 ] state=[-0.12300989  0.00720867 -0.10711188 -0.54390358], action=1, reward=1.0, next_state=[-0.12286572  0.2036599  -0.11798995 -0.86832234]\n",
      "[ episode 287 ][ timestamp 60 ] state=[-0.12286572  0.2036599  -0.11798995 -0.86832234], action=0, reward=1.0, next_state=[-0.11879252  0.01032388 -0.1353564  -0.61494324]\n",
      "[ episode 287 ][ timestamp 61 ] state=[-0.11879252  0.01032388 -0.1353564  -0.61494324], action=1, reward=1.0, next_state=[-0.11858604  0.2070515  -0.14765526 -0.94700894]\n",
      "[ episode 287 ][ timestamp 62 ] state=[-0.11858604  0.2070515  -0.14765526 -0.94700894], action=1, reward=1.0, next_state=[-0.11444501  0.40382005 -0.16659544 -1.28220322]\n",
      "[ episode 287 ][ timestamp 63 ] state=[-0.11444501  0.40382005 -0.16659544 -1.28220322], action=0, reward=1.0, next_state=[-0.10636861  0.21116502 -0.19223951 -1.04597443]\n",
      "[ episode 287 ][ timestamp 64 ] state=[-0.10636861  0.21116502 -0.19223951 -1.04597443], action=0, reward=-1.0, next_state=[-0.10214531  0.01904186 -0.213159   -0.81926934]\n",
      "[ Ended! ] Episode 287: Exploration_rate=0.2384520680152932. Score=64.\n",
      "[ Experience replay ] starts\n",
      "[ episode 288 ] state=[-0.04095196  0.02241386  0.01124837  0.02435368]\n",
      "[ episode 288 ][ timestamp 1 ] state=[-0.04095196  0.02241386  0.01124837  0.02435368], action=0, reward=1.0, next_state=[-0.04050368 -0.17286758  0.01173544  0.32056427]\n",
      "[ episode 288 ][ timestamp 2 ] state=[-0.04050368 -0.17286758  0.01173544  0.32056427], action=0, reward=1.0, next_state=[-0.04396103 -0.36815468  0.01814673  0.61692488]\n",
      "[ episode 288 ][ timestamp 3 ] state=[-0.04396103 -0.36815468  0.01814673  0.61692488], action=0, reward=1.0, next_state=[-0.05132413 -0.56352538  0.03048523  0.91526752]\n",
      "[ episode 288 ][ timestamp 4 ] state=[-0.05132413 -0.56352538  0.03048523  0.91526752], action=1, reward=1.0, next_state=[-0.06259464 -0.36882868  0.04879058  0.63231945]\n",
      "[ episode 288 ][ timestamp 5 ] state=[-0.06259464 -0.36882868  0.04879058  0.63231945], action=0, reward=1.0, next_state=[-0.06997121 -0.56459616  0.06143697  0.93995994]\n",
      "[ episode 288 ][ timestamp 6 ] state=[-0.06997121 -0.56459616  0.06143697  0.93995994], action=1, reward=1.0, next_state=[-0.08126313 -0.37035374  0.08023617  0.66719713]\n",
      "[ episode 288 ][ timestamp 7 ] state=[-0.08126313 -0.37035374  0.08023617  0.66719713], action=1, reward=1.0, next_state=[-0.08867021 -0.1764339   0.09358011  0.40081732]\n",
      "[ episode 288 ][ timestamp 8 ] state=[-0.08867021 -0.1764339   0.09358011  0.40081732], action=0, reward=1.0, next_state=[-0.09219889 -0.37275004  0.10159645  0.72147548]\n",
      "[ episode 288 ][ timestamp 9 ] state=[-0.09219889 -0.37275004  0.10159645  0.72147548], action=0, reward=1.0, next_state=[-0.09965389 -0.56911972  0.11602596  1.04432914]\n",
      "[ episode 288 ][ timestamp 10 ] state=[-0.09965389 -0.56911972  0.11602596  1.04432914], action=1, reward=1.0, next_state=[-0.11103628 -0.37571334  0.13691255  0.79020526]\n",
      "[ episode 288 ][ timestamp 11 ] state=[-0.11103628 -0.37571334  0.13691255  0.79020526], action=1, reward=1.0, next_state=[-0.11855055 -0.18271039  0.15271665  0.54353664]\n",
      "[ episode 288 ][ timestamp 12 ] state=[-0.11855055 -0.18271039  0.15271665  0.54353664], action=0, reward=1.0, next_state=[-0.12220475 -0.37961137  0.16358739  0.88017503]\n",
      "[ episode 288 ][ timestamp 13 ] state=[-0.12220475 -0.37961137  0.16358739  0.88017503], action=1, reward=1.0, next_state=[-0.12979698 -0.18704449  0.18119089  0.64306151]\n",
      "[ episode 288 ][ timestamp 14 ] state=[-0.12979698 -0.18704449  0.18119089  0.64306151], action=0, reward=1.0, next_state=[-0.13353787 -0.38416742  0.19405212  0.9868846 ]\n",
      "[ episode 288 ][ timestamp 15 ] state=[-0.13353787 -0.38416742  0.19405212  0.9868846 ], action=1, reward=-1.0, next_state=[-0.14122122 -0.19209849  0.21378981  0.76088259]\n",
      "[ Ended! ] Episode 288: Exploration_rate=0.23725980767521673. Score=15.\n",
      "[ Experience replay ] starts\n",
      "[ episode 289 ] state=[-0.0232944  -0.01329455 -0.03407638  0.0224213 ]\n",
      "[ episode 289 ][ timestamp 1 ] state=[-0.0232944  -0.01329455 -0.03407638  0.0224213 ], action=1, reward=1.0, next_state=[-0.0235603   0.1822991  -0.03362795 -0.28081536]\n",
      "[ episode 289 ][ timestamp 2 ] state=[-0.0235603   0.1822991  -0.03362795 -0.28081536], action=1, reward=1.0, next_state=[-0.01991431  0.37788419 -0.03924426 -0.5839119 ]\n",
      "[ episode 289 ][ timestamp 3 ] state=[-0.01991431  0.37788419 -0.03924426 -0.5839119 ], action=0, reward=1.0, next_state=[-0.01235663  0.18333335 -0.0509225  -0.30384517]\n",
      "[ episode 289 ][ timestamp 4 ] state=[-0.01235663  0.18333335 -0.0509225  -0.30384517], action=1, reward=1.0, next_state=[-0.00868996  0.37914263 -0.0569994  -0.61214312]\n",
      "[ episode 289 ][ timestamp 5 ] state=[-0.00868996  0.37914263 -0.0569994  -0.61214312], action=0, reward=1.0, next_state=[-0.00110711  0.18486168 -0.06924226 -0.33794371]\n",
      "[ episode 289 ][ timestamp 6 ] state=[-0.00110711  0.18486168 -0.06924226 -0.33794371], action=1, reward=1.0, next_state=[ 0.00259012  0.38089712 -0.07600114 -0.6516332 ]\n",
      "[ episode 289 ][ timestamp 7 ] state=[ 0.00259012  0.38089712 -0.07600114 -0.6516332 ], action=0, reward=1.0, next_state=[ 0.01020807  0.18691132 -0.0890338  -0.3838173 ]\n",
      "[ episode 289 ][ timestamp 8 ] state=[ 0.01020807  0.18691132 -0.0890338  -0.3838173 ], action=1, reward=1.0, next_state=[ 0.01394629  0.38317709 -0.09671015 -0.70319124]\n",
      "[ episode 289 ][ timestamp 9 ] state=[ 0.01394629  0.38317709 -0.09671015 -0.70319124], action=1, reward=1.0, next_state=[ 0.02160983  0.57949686 -0.11077397 -1.02468334]\n",
      "[ episode 289 ][ timestamp 10 ] state=[ 0.02160983  0.57949686 -0.11077397 -1.02468334], action=0, reward=1.0, next_state=[ 0.03319977  0.38601038 -0.13126764 -0.76873347]\n",
      "[ episode 289 ][ timestamp 11 ] state=[ 0.03319977  0.38601038 -0.13126764 -0.76873347], action=1, reward=1.0, next_state=[ 0.04091998  0.58267135 -0.14664231 -1.099669  ]\n",
      "[ episode 289 ][ timestamp 12 ] state=[ 0.04091998  0.58267135 -0.14664231 -1.099669  ], action=1, reward=1.0, next_state=[ 0.05257341  0.77938665 -0.16863569 -1.4345335 ]\n",
      "[ episode 289 ][ timestamp 13 ] state=[ 0.05257341  0.77938665 -0.16863569 -1.4345335 ], action=1, reward=1.0, next_state=[ 0.06816114  0.97613899 -0.19732636 -1.77481974]\n",
      "[ episode 289 ][ timestamp 14 ] state=[ 0.06816114  0.97613899 -0.19732636 -1.77481974], action=0, reward=-1.0, next_state=[ 0.08768392  0.78371277 -0.23282275 -1.54941987]\n",
      "[ Ended! ] Episode 289: Exploration_rate=0.23607350863684065. Score=14.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 290 ] state=[0.0332761  0.01614172 0.01836639 0.0319052 ]\n",
      "[ episode 290 ][ timestamp 1 ] state=[0.0332761  0.01614172 0.01836639 0.0319052 ], action=1, reward=1.0, next_state=[ 0.03359893  0.21099554  0.01900449 -0.25492681]\n",
      "[ episode 290 ][ timestamp 2 ] state=[ 0.03359893  0.21099554  0.01900449 -0.25492681], action=1, reward=1.0, next_state=[ 0.03781884  0.40584106  0.01390596 -0.54155533]\n",
      "[ episode 290 ][ timestamp 3 ] state=[ 0.03781884  0.40584106  0.01390596 -0.54155533], action=0, reward=1.0, next_state=[ 0.04593567  0.21052645  0.00307485 -0.24452351]\n",
      "[ episode 290 ][ timestamp 4 ] state=[ 0.04593567  0.21052645  0.00307485 -0.24452351], action=1, reward=1.0, next_state=[ 0.05014619  0.40560435 -0.00181562 -0.53623497]\n",
      "[ episode 290 ][ timestamp 5 ] state=[ 0.05014619  0.40560435 -0.00181562 -0.53623497], action=0, reward=1.0, next_state=[ 0.05825828  0.21050797 -0.01254032 -0.24412468]\n",
      "[ episode 290 ][ timestamp 6 ] state=[ 0.05825828  0.21050797 -0.01254032 -0.24412468], action=1, reward=1.0, next_state=[ 0.06246844  0.40580677 -0.01742281 -0.54073661]\n",
      "[ episode 290 ][ timestamp 7 ] state=[ 0.06246844  0.40580677 -0.01742281 -0.54073661], action=0, reward=1.0, next_state=[ 0.07058458  0.210934   -0.02823754 -0.25359386]\n",
      "[ episode 290 ][ timestamp 8 ] state=[ 0.07058458  0.210934   -0.02823754 -0.25359386], action=1, reward=1.0, next_state=[ 0.07480326  0.40644753 -0.03330942 -0.55504798]\n",
      "[ episode 290 ][ timestamp 9 ] state=[ 0.07480326  0.40644753 -0.03330942 -0.55504798], action=0, reward=1.0, next_state=[ 0.08293221  0.21180871 -0.04441038 -0.27304287]\n",
      "[ episode 290 ][ timestamp 10 ] state=[ 0.08293221  0.21180871 -0.04441038 -0.27304287], action=1, reward=1.0, next_state=[ 0.08716838  0.40753528 -0.04987124 -0.5793956 ]\n",
      "[ episode 290 ][ timestamp 11 ] state=[ 0.08716838  0.40753528 -0.04987124 -0.5793956 ], action=0, reward=1.0, next_state=[ 0.09531909  0.21314639 -0.06145915 -0.30283087]\n",
      "[ episode 290 ][ timestamp 12 ] state=[ 0.09531909  0.21314639 -0.06145915 -0.30283087], action=1, reward=1.0, next_state=[ 0.09958201  0.40908796 -0.06751577 -0.61424592]\n",
      "[ episode 290 ][ timestamp 13 ] state=[ 0.09958201  0.40908796 -0.06751577 -0.61424592], action=0, reward=1.0, next_state=[ 0.10776377  0.21497116 -0.07980069 -0.34356867]\n",
      "[ episode 290 ][ timestamp 14 ] state=[ 0.10776377  0.21497116 -0.07980069 -0.34356867], action=1, reward=1.0, next_state=[ 0.1120632   0.41113232 -0.08667206 -0.66031054]\n",
      "[ episode 290 ][ timestamp 15 ] state=[ 0.1120632   0.41113232 -0.08667206 -0.66031054], action=0, reward=1.0, next_state=[ 0.12028584  0.2173166  -0.09987827 -0.39612793]\n",
      "[ episode 290 ][ timestamp 16 ] state=[ 0.12028584  0.2173166  -0.09987827 -0.39612793], action=0, reward=1.0, next_state=[ 0.12463218  0.02374311 -0.10780083 -0.13653017]\n",
      "[ episode 290 ][ timestamp 17 ] state=[ 0.12463218  0.02374311 -0.10780083 -0.13653017], action=1, reward=1.0, next_state=[ 0.12510704  0.22023074 -0.11053143 -0.46118283]\n",
      "[ episode 290 ][ timestamp 18 ] state=[ 0.12510704  0.22023074 -0.11053143 -0.46118283], action=1, reward=1.0, next_state=[ 0.12951165  0.41672717 -0.11975509 -0.78655894]\n",
      "[ episode 290 ][ timestamp 19 ] state=[ 0.12951165  0.41672717 -0.11975509 -0.78655894], action=0, reward=1.0, next_state=[ 0.1378462   0.22343606 -0.13548627 -0.53382273]\n",
      "[ episode 290 ][ timestamp 20 ] state=[ 0.1378462   0.22343606 -0.13548627 -0.53382273], action=1, reward=1.0, next_state=[ 0.14231492  0.4201772  -0.14616272 -0.86594117]\n",
      "[ episode 290 ][ timestamp 21 ] state=[ 0.14231492  0.4201772  -0.14616272 -0.86594117], action=0, reward=1.0, next_state=[ 0.15071846  0.2273145  -0.16348154 -0.62255079]\n",
      "[ episode 290 ][ timestamp 22 ] state=[ 0.15071846  0.2273145  -0.16348154 -0.62255079], action=1, reward=1.0, next_state=[ 0.15526475  0.42429613 -0.17593256 -0.96193336]\n",
      "[ episode 290 ][ timestamp 23 ] state=[ 0.15526475  0.42429613 -0.17593256 -0.96193336], action=1, reward=1.0, next_state=[ 0.16375067  0.62128989 -0.19517123 -1.3043205 ]\n",
      "[ episode 290 ][ timestamp 24 ] state=[ 0.16375067  0.62128989 -0.19517123 -1.3043205 ], action=0, reward=-1.0, next_state=[ 0.17617647  0.42910346 -0.22125764 -1.07853073]\n",
      "[ Ended! ] Episode 290: Exploration_rate=0.23489314109365644. Score=24.\n",
      "[ Experience replay ] starts\n",
      "[ episode 291 ] state=[0.04948893 0.01331399 0.04851025 0.0095579 ]\n",
      "[ episode 291 ][ timestamp 1 ] state=[0.04948893 0.01331399 0.04851025 0.0095579 ], action=0, reward=1.0, next_state=[ 0.04975521 -0.18246889  0.04870141  0.31714316]\n",
      "[ episode 291 ][ timestamp 2 ] state=[ 0.04975521 -0.18246889  0.04870141  0.31714316], action=1, reward=1.0, next_state=[0.04610584 0.01192679 0.05504427 0.04020795]\n",
      "[ episode 291 ][ timestamp 3 ] state=[0.04610584 0.01192679 0.05504427 0.04020795], action=0, reward=1.0, next_state=[ 0.04634437 -0.18393952  0.05584843  0.34973728]\n",
      "[ episode 291 ][ timestamp 4 ] state=[ 0.04634437 -0.18393952  0.05584843  0.34973728], action=1, reward=1.0, next_state=[0.04266558 0.01034551 0.06284317 0.075175  ]\n",
      "[ episode 291 ][ timestamp 5 ] state=[0.04266558 0.01034551 0.06284317 0.075175  ], action=0, reward=1.0, next_state=[ 0.04287249 -0.18561843  0.06434667  0.3870044 ]\n",
      "[ episode 291 ][ timestamp 6 ] state=[ 0.04287249 -0.18561843  0.06434667  0.3870044 ], action=1, reward=1.0, next_state=[0.03916012 0.0085339  0.07208676 0.11528349]\n",
      "[ episode 291 ][ timestamp 7 ] state=[0.03916012 0.0085339  0.07208676 0.11528349], action=0, reward=1.0, next_state=[ 0.0393308  -0.18754295  0.07439243  0.42981006]\n",
      "[ episode 291 ][ timestamp 8 ] state=[ 0.0393308  -0.18754295  0.07439243  0.42981006], action=1, reward=1.0, next_state=[0.03557994 0.00645102 0.08298863 0.16147515]\n",
      "[ episode 291 ][ timestamp 9 ] state=[0.03557994 0.00645102 0.08298863 0.16147515], action=0, reward=1.0, next_state=[ 0.03570896 -0.18975489  0.08621814  0.47914179]\n",
      "[ episode 291 ][ timestamp 10 ] state=[ 0.03570896 -0.18975489  0.08621814  0.47914179], action=1, reward=1.0, next_state=[0.03191386 0.00405082 0.09580097 0.2148298 ]\n",
      "[ episode 291 ][ timestamp 11 ] state=[0.03191386 0.00405082 0.09580097 0.2148298 ], action=0, reward=1.0, next_state=[ 0.03199488 -0.19230091  0.10009757  0.53612928]\n",
      "[ episode 291 ][ timestamp 12 ] state=[ 0.03199488 -0.19230091  0.10009757  0.53612928], action=1, reward=1.0, next_state=[0.02814886 0.00128164 0.11082015 0.27658851]\n",
      "[ episode 291 ][ timestamp 13 ] state=[0.02814886 0.00128164 0.11082015 0.27658851], action=0, reward=1.0, next_state=[ 0.0281745  -0.19523253  0.11635192  0.60206604]\n",
      "[ episode 291 ][ timestamp 14 ] state=[ 0.0281745  -0.19523253  0.11635192  0.60206604], action=0, reward=1.0, next_state=[ 0.02426985 -0.39177325  0.12839325  0.92901417]\n",
      "[ episode 291 ][ timestamp 15 ] state=[ 0.02426985 -0.39177325  0.12839325  0.92901417], action=1, reward=1.0, next_state=[ 0.01643438 -0.1985964   0.14697353  0.67927796]\n",
      "[ episode 291 ][ timestamp 16 ] state=[ 0.01643438 -0.1985964   0.14697353  0.67927796], action=1, reward=1.0, next_state=[ 0.01246245 -0.00578867  0.16055909  0.43623924]\n",
      "[ episode 291 ][ timestamp 17 ] state=[ 0.01246245 -0.00578867  0.16055909  0.43623924], action=1, reward=1.0, next_state=[0.01234668 0.18673962 0.16928387 0.19816304]\n",
      "[ episode 291 ][ timestamp 18 ] state=[0.01234668 0.18673962 0.16928387 0.19816304], action=1, reward=1.0, next_state=[ 0.01608147  0.37908688  0.17324713 -0.03670154]\n",
      "[ episode 291 ][ timestamp 19 ] state=[ 0.01608147  0.37908688  0.17324713 -0.03670154], action=0, reward=1.0, next_state=[0.02366321 0.18195857 0.1725131  0.30524471]\n",
      "[ episode 291 ][ timestamp 20 ] state=[0.02366321 0.18195857 0.1725131  0.30524471], action=1, reward=1.0, next_state=[0.02730238 0.37425622 0.178618   0.07154745]\n",
      "[ episode 291 ][ timestamp 21 ] state=[0.02730238 0.37425622 0.178618   0.07154745], action=0, reward=1.0, next_state=[0.0347875  0.17708307 0.18004895 0.41483659]\n",
      "[ episode 291 ][ timestamp 22 ] state=[0.0347875  0.17708307 0.18004895 0.41483659], action=1, reward=1.0, next_state=[0.03832917 0.36925743 0.18834568 0.18388367]\n",
      "[ episode 291 ][ timestamp 23 ] state=[0.03832917 0.36925743 0.18834568 0.18388367], action=1, reward=1.0, next_state=[ 0.04571431  0.56125534  0.19202335 -0.04397323]\n",
      "[ episode 291 ][ timestamp 24 ] state=[ 0.04571431  0.56125534  0.19202335 -0.04397323], action=0, reward=1.0, next_state=[0.05693942 0.36397281 0.19114389 0.30262007]\n",
      "[ episode 291 ][ timestamp 25 ] state=[0.05693942 0.36397281 0.19114389 0.30262007], action=1, reward=1.0, next_state=[0.06421888 0.55592979 0.19719629 0.07578334]\n",
      "[ episode 291 ][ timestamp 26 ] state=[0.06421888 0.55592979 0.19719629 0.07578334], action=0, reward=1.0, next_state=[0.07533747 0.35860781 0.19871195 0.42363078]\n",
      "[ episode 291 ][ timestamp 27 ] state=[0.07533747 0.35860781 0.19871195 0.42363078], action=1, reward=1.0, next_state=[0.08250963 0.55044193 0.20718457 0.19957966]\n",
      "[ episode 291 ][ timestamp 28 ] state=[0.08250963 0.55044193 0.20718457 0.19957966], action=1, reward=-1.0, next_state=[ 0.09351847  0.74209146  0.21117616 -0.02126928]\n",
      "[ Ended! ] Episode 291: Exploration_rate=0.23371867538818816. Score=28.\n",
      "[ Experience replay ] starts\n",
      "[ episode 292 ] state=[-0.0092259  -0.02391969 -0.00314329  0.03793566]\n",
      "[ episode 292 ][ timestamp 1 ] state=[-0.0092259  -0.02391969 -0.00314329  0.03793566], action=1, reward=1.0, next_state=[-0.0097043   0.1712472  -0.00238458 -0.25573735]\n",
      "[ episode 292 ][ timestamp 2 ] state=[-0.0097043   0.1712472  -0.00238458 -0.25573735], action=1, reward=1.0, next_state=[-0.00627935  0.36640311 -0.00749933 -0.54917145]\n",
      "[ episode 292 ][ timestamp 3 ] state=[-0.00627935  0.36640311 -0.00749933 -0.54917145], action=1, reward=1.0, next_state=[ 0.00104871  0.5616296  -0.01848275 -0.84420773]\n",
      "[ episode 292 ][ timestamp 4 ] state=[ 0.00104871  0.5616296  -0.01848275 -0.84420773], action=1, reward=1.0, next_state=[ 0.0122813   0.75699883 -0.03536691 -1.14264514]\n",
      "[ episode 292 ][ timestamp 5 ] state=[ 0.0122813   0.75699883 -0.03536691 -1.14264514], action=0, reward=1.0, next_state=[ 0.02742128  0.56235643 -0.05821981 -0.86125983]\n",
      "[ episode 292 ][ timestamp 6 ] state=[ 0.02742128  0.56235643 -0.05821981 -0.86125983], action=1, reward=1.0, next_state=[ 0.03866841  0.75822082 -0.07544501 -1.1716656 ]\n",
      "[ episode 292 ][ timestamp 7 ] state=[ 0.03866841  0.75822082 -0.07544501 -1.1716656 ], action=0, reward=1.0, next_state=[ 0.05383282  0.56415649 -0.09887832 -0.90355695]\n",
      "[ episode 292 ][ timestamp 8 ] state=[ 0.05383282  0.56415649 -0.09887832 -0.90355695], action=0, reward=1.0, next_state=[ 0.06511595  0.37050283 -0.11694946 -0.64351818]\n",
      "[ episode 292 ][ timestamp 9 ] state=[ 0.06511595  0.37050283 -0.11694946 -0.64351818], action=1, reward=1.0, next_state=[ 0.07252601  0.56704382 -0.12981982 -0.97062069]\n",
      "[ episode 292 ][ timestamp 10 ] state=[ 0.07252601  0.56704382 -0.12981982 -0.97062069], action=1, reward=1.0, next_state=[ 0.08386688  0.76364665 -0.14923224 -1.30110331]\n",
      "[ episode 292 ][ timestamp 11 ] state=[ 0.08386688  0.76364665 -0.14923224 -1.30110331], action=1, reward=1.0, next_state=[ 0.09913982  0.96031352 -0.1752543  -1.63653644]\n",
      "[ episode 292 ][ timestamp 12 ] state=[ 0.09913982  0.96031352 -0.1752543  -1.63653644], action=1, reward=1.0, next_state=[ 0.11834609  1.15700461 -0.20798503 -1.97831519]\n",
      "[ episode 292 ][ timestamp 13 ] state=[ 0.11834609  1.15700461 -0.20798503 -1.97831519], action=1, reward=-1.0, next_state=[ 0.14148618  1.35362225 -0.24755134 -2.32759338]\n",
      "[ Ended! ] Episode 292: Exploration_rate=0.23255008201124722. Score=13.\n",
      "[ Experience replay ] starts\n",
      "[ episode 293 ] state=[-0.02741579  0.03369335  0.04789875 -0.03204784]\n",
      "[ episode 293 ][ timestamp 1 ] state=[-0.02741579  0.03369335  0.04789875 -0.03204784], action=1, reward=1.0, next_state=[-0.02674192  0.22809685  0.0472578  -0.30924179]\n",
      "[ episode 293 ][ timestamp 2 ] state=[-0.02674192  0.22809685  0.0472578  -0.30924179], action=1, reward=1.0, next_state=[-0.02217998  0.42251473  0.04107296 -0.5866544 ]\n",
      "[ episode 293 ][ timestamp 3 ] state=[-0.02217998  0.42251473  0.04107296 -0.5866544 ], action=1, reward=1.0, next_state=[-0.01372969  0.61703808  0.02933987 -0.86612129]\n",
      "[ episode 293 ][ timestamp 4 ] state=[-0.01372969  0.61703808  0.02933987 -0.86612129], action=0, reward=1.0, next_state=[-0.00138893  0.42152938  0.01201745 -0.56435977]\n",
      "[ episode 293 ][ timestamp 5 ] state=[-0.00138893  0.42152938  0.01201745 -0.56435977], action=1, reward=1.0, next_state=[ 7.04166242e-03  6.16480671e-01  7.30251231e-04 -8.53232548e-01]\n",
      "[ episode 293 ][ timestamp 6 ] state=[ 7.04166242e-03  6.16480671e-01  7.30251231e-04 -8.53232548e-01], action=0, reward=1.0, next_state=[ 0.01937128  0.42134877 -0.0163344  -0.56032009]\n",
      "[ episode 293 ][ timestamp 7 ] state=[ 0.01937128  0.42134877 -0.0163344  -0.56032009], action=1, reward=1.0, next_state=[ 0.02779825  0.61669613 -0.0275408  -0.85810412]\n",
      "[ episode 293 ][ timestamp 8 ] state=[ 0.02779825  0.61669613 -0.0275408  -0.85810412], action=0, reward=1.0, next_state=[ 0.04013217  0.42195997 -0.04470288 -0.57420664]\n",
      "[ episode 293 ][ timestamp 9 ] state=[ 0.04013217  0.42195997 -0.04470288 -0.57420664], action=1, reward=1.0, next_state=[ 0.04857137  0.6176792  -0.05618702 -0.88063047]\n",
      "[ episode 293 ][ timestamp 10 ] state=[ 0.04857137  0.6176792  -0.05618702 -0.88063047], action=0, reward=1.0, next_state=[ 0.06092496  0.42336371 -0.07379963 -0.60612749]\n",
      "[ episode 293 ][ timestamp 11 ] state=[ 0.06092496  0.42336371 -0.07379963 -0.60612749], action=0, reward=1.0, next_state=[ 0.06939223  0.22934706 -0.08592218 -0.33757208]\n",
      "[ episode 293 ][ timestamp 12 ] state=[ 0.06939223  0.22934706 -0.08592218 -0.33757208], action=0, reward=1.0, next_state=[ 0.07397917  0.0355462  -0.09267362 -0.07317324]\n",
      "[ episode 293 ][ timestamp 13 ] state=[ 0.07397917  0.0355462  -0.09267362 -0.07317324], action=1, reward=1.0, next_state=[ 0.0746901   0.23186613 -0.09413708 -0.39359654]\n",
      "[ episode 293 ][ timestamp 14 ] state=[ 0.0746901   0.23186613 -0.09413708 -0.39359654], action=0, reward=1.0, next_state=[ 0.07932742  0.03819728 -0.10200901 -0.13201495]\n",
      "[ episode 293 ][ timestamp 15 ] state=[ 0.07932742  0.03819728 -0.10200901 -0.13201495], action=1, reward=1.0, next_state=[ 0.08009136  0.23462129 -0.10464931 -0.45505799]\n",
      "[ episode 293 ][ timestamp 16 ] state=[ 0.08009136  0.23462129 -0.10464931 -0.45505799], action=1, reward=1.0, next_state=[ 0.08478379  0.43105519 -0.11375047 -0.77880766]\n",
      "[ episode 293 ][ timestamp 17 ] state=[ 0.08478379  0.43105519 -0.11375047 -0.77880766], action=1, reward=1.0, next_state=[ 0.09340489  0.62754207 -0.12932662 -1.1050038 ]\n",
      "[ episode 293 ][ timestamp 18 ] state=[ 0.09340489  0.62754207 -0.12932662 -1.1050038 ], action=1, reward=1.0, next_state=[ 0.10595574  0.82410529 -0.1514267  -1.43530251]\n",
      "[ episode 293 ][ timestamp 19 ] state=[ 0.10595574  0.82410529 -0.1514267  -1.43530251], action=0, reward=1.0, next_state=[ 0.12243784  0.63114008 -0.18013275 -1.19351639]\n",
      "[ episode 293 ][ timestamp 20 ] state=[ 0.12243784  0.63114008 -0.18013275 -1.19351639], action=1, reward=1.0, next_state=[ 0.13506064  0.82807835 -0.20400308 -1.53681717]\n",
      "[ episode 293 ][ timestamp 21 ] state=[ 0.13506064  0.82807835 -0.20400308 -1.53681717], action=1, reward=-1.0, next_state=[ 0.15162221  1.02498735 -0.23473942 -1.8856176 ]\n",
      "[ Ended! ] Episode 293: Exploration_rate=0.231387331601191. Score=21.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 294 ] state=[-0.02007218 -0.04114938 -0.04451245  0.03196513]\n",
      "[ episode 294 ][ timestamp 1 ] state=[-0.02007218 -0.04114938 -0.04451245  0.03196513], action=1, reward=1.0, next_state=[-0.02089517  0.1545817  -0.04387315 -0.27442302]\n",
      "[ episode 294 ][ timestamp 2 ] state=[-0.02089517  0.1545817  -0.04387315 -0.27442302], action=1, reward=1.0, next_state=[-0.01780353  0.35030128 -0.04936161 -0.58061445]\n",
      "[ episode 294 ][ timestamp 3 ] state=[-0.01780353  0.35030128 -0.04936161 -0.58061445], action=1, reward=1.0, next_state=[-0.01079751  0.5460789  -0.0609739  -0.8884296 ]\n",
      "[ episode 294 ][ timestamp 4 ] state=[-0.01079751  0.5460789  -0.0609739  -0.8884296 ], action=0, reward=1.0, next_state=[ 1.24070028e-04  3.51835087e-01 -7.87424883e-02 -6.15520556e-01]\n",
      "[ episode 294 ][ timestamp 5 ] state=[ 1.24070028e-04  3.51835087e-01 -7.87424883e-02 -6.15520556e-01], action=1, reward=1.0, next_state=[ 0.00716077  0.54796379 -0.0910529  -0.9319284 ]\n",
      "[ episode 294 ][ timestamp 6 ] state=[ 0.00716077  0.54796379 -0.0910529  -0.9319284 ], action=0, reward=1.0, next_state=[ 0.01812005  0.35418065 -0.10969147 -0.66919038]\n",
      "[ episode 294 ][ timestamp 7 ] state=[ 0.01812005  0.35418065 -0.10969147 -0.66919038], action=1, reward=1.0, next_state=[ 0.02520366  0.55064305 -0.12307527 -0.9942975 ]\n",
      "[ episode 294 ][ timestamp 8 ] state=[ 0.02520366  0.55064305 -0.12307527 -0.9942975 ], action=1, reward=1.0, next_state=[ 0.03621652  0.74717717 -0.14296122 -1.32296159]\n",
      "[ episode 294 ][ timestamp 9 ] state=[ 0.03621652  0.74717717 -0.14296122 -1.32296159], action=0, reward=1.0, next_state=[ 0.05116006  0.55412113 -0.16942046 -1.07821932]\n",
      "[ episode 294 ][ timestamp 10 ] state=[ 0.05116006  0.55412113 -0.16942046 -1.07821932], action=1, reward=1.0, next_state=[ 0.06224249  0.75102575 -0.19098484 -1.41891919]\n",
      "[ episode 294 ][ timestamp 11 ] state=[ 0.06224249  0.75102575 -0.19098484 -1.41891919], action=0, reward=-1.0, next_state=[ 0.077263    0.55871088 -0.21936323 -1.19150078]\n",
      "[ Ended! ] Episode 294: Exploration_rate=0.23023039494318503. Score=11.\n",
      "[ Experience replay ] starts\n",
      "[ episode 295 ] state=[-0.01394372  0.019515   -0.02194852 -0.01020139]\n",
      "[ episode 295 ][ timestamp 1 ] state=[-0.01394372  0.019515   -0.02194852 -0.01020139], action=1, reward=1.0, next_state=[-0.01355342  0.21494474 -0.02215255 -0.30972774]\n",
      "[ episode 295 ][ timestamp 2 ] state=[-0.01355342  0.21494474 -0.02215255 -0.30972774], action=1, reward=1.0, next_state=[-0.00925452  0.41037519 -0.0283471  -0.60931381]\n",
      "[ episode 295 ][ timestamp 3 ] state=[-0.00925452  0.41037519 -0.0283471  -0.60931381], action=0, reward=1.0, next_state=[-0.00104702  0.21566074 -0.04053338 -0.32569241]\n",
      "[ episode 295 ][ timestamp 4 ] state=[-0.00104702  0.21566074 -0.04053338 -0.32569241], action=1, reward=1.0, next_state=[ 0.0032662   0.41133566 -0.04704723 -0.63087726]\n",
      "[ episode 295 ][ timestamp 5 ] state=[ 0.0032662   0.41133566 -0.04704723 -0.63087726], action=0, reward=1.0, next_state=[ 0.01149291  0.21690065 -0.05966477 -0.35337423]\n",
      "[ episode 295 ][ timestamp 6 ] state=[ 0.01149291  0.21690065 -0.05966477 -0.35337423], action=0, reward=1.0, next_state=[ 0.01583092  0.02267561 -0.06673226 -0.08008613]\n",
      "[ episode 295 ][ timestamp 7 ] state=[ 0.01583092  0.02267561 -0.06673226 -0.08008613], action=1, reward=1.0, next_state=[ 0.01628443  0.21868757 -0.06833398 -0.39305438]\n",
      "[ episode 295 ][ timestamp 8 ] state=[ 0.01628443  0.21868757 -0.06833398 -0.39305438], action=1, reward=1.0, next_state=[ 0.02065819  0.41470932 -0.07619507 -0.70647533]\n",
      "[ episode 295 ][ timestamp 9 ] state=[ 0.02065819  0.41470932 -0.07619507 -0.70647533], action=0, reward=1.0, next_state=[ 0.02895237  0.22072108 -0.09032457 -0.43871691]\n",
      "[ episode 295 ][ timestamp 10 ] state=[ 0.02895237  0.22072108 -0.09032457 -0.43871691], action=1, reward=1.0, next_state=[ 0.03336679  0.41699759 -0.09909891 -0.75845083]\n",
      "[ episode 295 ][ timestamp 11 ] state=[ 0.03336679  0.41699759 -0.09909891 -0.75845083], action=1, reward=1.0, next_state=[ 0.04170674  0.61333533 -0.11426793 -1.08059992]\n",
      "[ episode 295 ][ timestamp 12 ] state=[ 0.04170674  0.61333533 -0.11426793 -1.08059992], action=0, reward=1.0, next_state=[ 0.05397345  0.41989204 -0.13587993 -0.825849  ]\n",
      "[ episode 295 ][ timestamp 13 ] state=[ 0.05397345  0.41989204 -0.13587993 -0.825849  ], action=1, reward=1.0, next_state=[ 0.06237129  0.61658458 -0.15239691 -1.15799418]\n",
      "[ episode 295 ][ timestamp 14 ] state=[ 0.06237129  0.61658458 -0.15239691 -1.15799418], action=0, reward=1.0, next_state=[ 0.07470298  0.42374112 -0.17555679 -0.91671302]\n",
      "[ episode 295 ][ timestamp 15 ] state=[ 0.07470298  0.42374112 -0.17555679 -0.91671302], action=1, reward=1.0, next_state=[ 0.08317781  0.62074648 -0.19389105 -1.25902792]\n",
      "[ episode 295 ][ timestamp 16 ] state=[ 0.08317781  0.62074648 -0.19389105 -1.25902792], action=0, reward=-1.0, next_state=[ 0.09559274  0.42856013 -0.21907161 -1.03279767]\n",
      "[ Ended! ] Episode 295: Exploration_rate=0.2290792429684691. Score=16.\n",
      "[ Experience replay ] starts\n",
      "[ episode 296 ] state=[-0.04743947 -0.02583275 -0.035825   -0.00274793]\n",
      "[ episode 296 ][ timestamp 1 ] state=[-0.04743947 -0.02583275 -0.035825   -0.00274793], action=1, reward=1.0, next_state=[-0.04795613  0.16978418 -0.03587995 -0.30651535]\n",
      "[ episode 296 ][ timestamp 2 ] state=[-0.04795613  0.16978418 -0.03587995 -0.30651535], action=0, reward=1.0, next_state=[-0.04456044 -0.02480861 -0.04201026 -0.02536048]\n",
      "[ episode 296 ][ timestamp 3 ] state=[-0.04456044 -0.02480861 -0.04201026 -0.02536048], action=1, reward=1.0, next_state=[-0.04505662  0.17088984 -0.04251747 -0.33099653]\n",
      "[ episode 296 ][ timestamp 4 ] state=[-0.04505662  0.17088984 -0.04251747 -0.33099653], action=1, reward=1.0, next_state=[-0.04163882  0.36659041 -0.0491374  -0.63677846]\n",
      "[ episode 296 ][ timestamp 5 ] state=[-0.04163882  0.36659041 -0.0491374  -0.63677846], action=1, reward=1.0, next_state=[-0.03430701  0.56236194 -0.06187297 -0.9445219 ]\n",
      "[ episode 296 ][ timestamp 6 ] state=[-0.03430701  0.56236194 -0.06187297 -0.9445219 ], action=0, reward=1.0, next_state=[-0.02305977  0.36812559 -0.08076341 -0.67190394]\n",
      "[ episode 296 ][ timestamp 7 ] state=[-0.02305977  0.36812559 -0.08076341 -0.67190394], action=1, reward=1.0, next_state=[-0.01569726  0.56427183 -0.09420149 -0.98888291]\n",
      "[ episode 296 ][ timestamp 8 ] state=[-0.01569726  0.56427183 -0.09420149 -0.98888291], action=0, reward=1.0, next_state=[-0.00441182  0.37052858 -0.11397915 -0.72721082]\n",
      "[ episode 296 ][ timestamp 9 ] state=[-0.00441182  0.37052858 -0.11397915 -0.72721082], action=1, reward=1.0, next_state=[ 0.00299875  0.56702638 -0.12852336 -1.05348239]\n",
      "[ episode 296 ][ timestamp 10 ] state=[ 0.00299875  0.56702638 -0.12852336 -1.05348239], action=1, reward=1.0, next_state=[ 0.01433928  0.76359626 -0.14959301 -1.38358724]\n",
      "[ episode 296 ][ timestamp 11 ] state=[ 0.01433928  0.76359626 -0.14959301 -1.38358724], action=0, reward=1.0, next_state=[ 0.0296112   0.57062311 -0.17726475 -1.14117675]\n",
      "[ episode 296 ][ timestamp 12 ] state=[ 0.0296112   0.57062311 -0.17726475 -1.14117675], action=1, reward=1.0, next_state=[ 0.04102366  0.76756225 -0.20008829 -1.48379965]\n",
      "[ episode 296 ][ timestamp 13 ] state=[ 0.04102366  0.76756225 -0.20008829 -1.48379965], action=0, reward=-1.0, next_state=[ 0.05637491  0.57536252 -0.22976428 -1.25968613]\n",
      "[ Ended! ] Episode 296: Exploration_rate=0.22793384675362674. Score=13.\n",
      "[ Experience replay ] starts\n",
      "[ episode 297 ] state=[ 0.03936613 -0.00907758  0.0471985  -0.02660083]\n",
      "[ episode 297 ][ timestamp 1 ] state=[ 0.03936613 -0.00907758  0.0471985  -0.02660083], action=1, reward=1.0, next_state=[ 0.03918458  0.18533685  0.04666649 -0.3040265 ]\n",
      "[ episode 297 ][ timestamp 2 ] state=[ 0.03918458  0.18533685  0.04666649 -0.3040265 ], action=1, reward=1.0, next_state=[ 0.04289131  0.37976375  0.04058596 -0.58163439]\n",
      "[ episode 297 ][ timestamp 3 ] state=[ 0.04289131  0.37976375  0.04058596 -0.58163439], action=0, reward=1.0, next_state=[ 0.05048659  0.18409734  0.02895327 -0.27644747]\n",
      "[ episode 297 ][ timestamp 4 ] state=[ 0.05048659  0.18409734  0.02895327 -0.27644747], action=0, reward=1.0, next_state=[ 0.05416854 -0.01142546  0.02342432  0.02522489]\n",
      "[ episode 297 ][ timestamp 5 ] state=[ 0.05416854 -0.01142546  0.02342432  0.02522489], action=1, reward=1.0, next_state=[ 0.05394003  0.18335287  0.02392882 -0.25997634]\n",
      "[ episode 297 ][ timestamp 6 ] state=[ 0.05394003  0.18335287  0.02392882 -0.25997634], action=1, reward=1.0, next_state=[ 0.05760708  0.3781252   0.01872929 -0.54501678]\n",
      "[ episode 297 ][ timestamp 7 ] state=[ 0.05760708  0.3781252   0.01872929 -0.54501678], action=0, reward=1.0, next_state=[ 0.06516959  0.18274514  0.00782896 -0.24649202]\n",
      "[ episode 297 ][ timestamp 8 ] state=[ 0.06516959  0.18274514  0.00782896 -0.24649202], action=1, reward=1.0, next_state=[ 0.06882449  0.37775441  0.00289912 -0.53669526]\n",
      "[ episode 297 ][ timestamp 9 ] state=[ 0.06882449  0.37775441  0.00289912 -0.53669526], action=0, reward=1.0, next_state=[ 0.07637958  0.18259182 -0.00783479 -0.24310026]\n",
      "[ episode 297 ][ timestamp 10 ] state=[ 0.07637958  0.18259182 -0.00783479 -0.24310026], action=1, reward=1.0, next_state=[ 0.08003141  0.3778248  -0.0126968  -0.53824415]\n",
      "[ episode 297 ][ timestamp 11 ] state=[ 0.08003141  0.3778248  -0.0126968  -0.53824415], action=0, reward=1.0, next_state=[ 0.08758791  0.18288363 -0.02346168 -0.24958873]\n",
      "[ episode 297 ][ timestamp 12 ] state=[ 0.08758791  0.18288363 -0.02346168 -0.24958873], action=1, reward=1.0, next_state=[ 0.09124558  0.37833263 -0.02845345 -0.54957865]\n",
      "[ episode 297 ][ timestamp 13 ] state=[ 0.09124558  0.37833263 -0.02845345 -0.54957865], action=0, reward=1.0, next_state=[ 0.09881224  0.18362168 -0.03944503 -0.26599462]\n",
      "[ episode 297 ][ timestamp 14 ] state=[ 0.09881224  0.18362168 -0.03944503 -0.26599462], action=1, reward=1.0, next_state=[ 0.10248467  0.37928375 -0.04476492 -0.57085327]\n",
      "[ episode 297 ][ timestamp 15 ] state=[ 0.10248467  0.37928375 -0.04476492 -0.57085327], action=0, reward=1.0, next_state=[ 0.11007034  0.18481721 -0.05618198 -0.29260216]\n",
      "[ episode 297 ][ timestamp 16 ] state=[ 0.11007034  0.18481721 -0.05618198 -0.29260216], action=1, reward=1.0, next_state=[ 0.11376669  0.38069331 -0.06203403 -0.60246156]\n",
      "[ episode 297 ][ timestamp 17 ] state=[ 0.11376669  0.38069331 -0.06203403 -0.60246156], action=1, reward=1.0, next_state=[ 0.12138055  0.57662559 -0.07408326 -0.91402097]\n",
      "[ episode 297 ][ timestamp 18 ] state=[ 0.12138055  0.57662559 -0.07408326 -0.91402097], action=0, reward=1.0, next_state=[ 0.13291307  0.38257968 -0.09236368 -0.64551104]\n",
      "[ episode 297 ][ timestamp 19 ] state=[ 0.13291307  0.38257968 -0.09236368 -0.64551104], action=1, reward=1.0, next_state=[ 0.14056466  0.57885907 -0.1052739  -0.96579149]\n",
      "[ episode 297 ][ timestamp 20 ] state=[ 0.14056466  0.57885907 -0.1052739  -0.96579149], action=0, reward=1.0, next_state=[ 0.15214184  0.38529653 -0.12458973 -0.70794847]\n",
      "[ episode 297 ][ timestamp 21 ] state=[ 0.15214184  0.38529653 -0.12458973 -0.70794847], action=1, reward=1.0, next_state=[ 0.15984777  0.58190393 -0.1387487  -1.03710833]\n",
      "[ episode 297 ][ timestamp 22 ] state=[ 0.15984777  0.58190393 -0.1387487  -1.03710833], action=1, reward=1.0, next_state=[ 0.17148585  0.77856992 -0.15949086 -1.36993367]\n",
      "[ episode 297 ][ timestamp 23 ] state=[ 0.17148585  0.77856992 -0.15949086 -1.36993367], action=0, reward=1.0, next_state=[ 0.18705725  0.58576176 -0.18688954 -1.13108381]\n",
      "[ episode 297 ][ timestamp 24 ] state=[ 0.18705725  0.58576176 -0.18688954 -1.13108381], action=1, reward=-1.0, next_state=[ 0.19877248  0.78277243 -0.20951121 -1.47608021]\n",
      "[ Ended! ] Episode 297: Exploration_rate=0.22679417751985861. Score=24.\n",
      "[ Experience replay ] starts\n",
      "[ episode 298 ] state=[ 0.02947255 -0.00350513  0.02216218 -0.04434541]\n",
      "[ episode 298 ][ timestamp 1 ] state=[ 0.02947255 -0.00350513  0.02216218 -0.04434541], action=1, reward=1.0, next_state=[ 0.02940244  0.19129213  0.02127527 -0.3299544 ]\n",
      "[ episode 298 ][ timestamp 2 ] state=[ 0.02940244  0.19129213  0.02127527 -0.3299544 ], action=0, reward=1.0, next_state=[ 0.03322829 -0.00412611  0.01467618 -0.03063891]\n",
      "[ episode 298 ][ timestamp 3 ] state=[ 0.03322829 -0.00412611  0.01467618 -0.03063891], action=1, reward=1.0, next_state=[ 0.03314577  0.19078233  0.0140634  -0.31865545]\n",
      "[ episode 298 ][ timestamp 4 ] state=[ 0.03314577  0.19078233  0.0140634  -0.31865545], action=0, reward=1.0, next_state=[ 0.03696141 -0.00453706  0.00769029 -0.02157083]\n",
      "[ episode 298 ][ timestamp 5 ] state=[ 0.03696141 -0.00453706  0.00769029 -0.02157083], action=1, reward=1.0, next_state=[ 0.03687067  0.19047376  0.00725888 -0.31181749]\n",
      "[ episode 298 ][ timestamp 6 ] state=[ 0.03687067  0.19047376  0.00725888 -0.31181749], action=0, reward=1.0, next_state=[ 0.04068015 -0.00475085  0.00102253 -0.0168542 ]\n",
      "[ episode 298 ][ timestamp 7 ] state=[ 0.04068015 -0.00475085  0.00102253 -0.0168542 ], action=0, reward=1.0, next_state=[ 0.04058513 -0.19988745  0.00068544  0.27615117]\n",
      "[ episode 298 ][ timestamp 8 ] state=[ 0.04058513 -0.19988745  0.00068544  0.27615117], action=1, reward=1.0, next_state=[ 0.03658738 -0.00477528  0.00620847 -0.01631549]\n",
      "[ episode 298 ][ timestamp 9 ] state=[ 0.03658738 -0.00477528  0.00620847 -0.01631549], action=1, reward=1.0, next_state=[ 0.03649187  0.19025708  0.00588216 -0.30703312]\n",
      "[ episode 298 ][ timestamp 10 ] state=[ 0.03649187  0.19025708  0.00588216 -0.30703312], action=0, reward=1.0, next_state=[ 0.04029702 -0.00494819 -0.00025851 -0.01250093]\n",
      "[ episode 298 ][ timestamp 11 ] state=[ 0.04029702 -0.00494819 -0.00025851 -0.01250093], action=1, reward=1.0, next_state=[ 0.04019805  0.19017747 -0.00050852 -0.30526541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 298 ][ timestamp 12 ] state=[ 0.04019805  0.19017747 -0.00050852 -0.30526541], action=1, reward=1.0, next_state=[ 0.0440016   0.38530666 -0.00661383 -0.59810867]\n",
      "[ episode 298 ][ timestamp 13 ] state=[ 0.0440016   0.38530666 -0.00661383 -0.59810867], action=0, reward=1.0, next_state=[ 0.05170774  0.19027788 -0.01857601 -0.30751634]\n",
      "[ episode 298 ][ timestamp 14 ] state=[ 0.05170774  0.19027788 -0.01857601 -0.30751634], action=1, reward=1.0, next_state=[ 0.05551329  0.38565953 -0.02472633 -0.60599929]\n",
      "[ episode 298 ][ timestamp 15 ] state=[ 0.05551329  0.38565953 -0.02472633 -0.60599929], action=1, reward=1.0, next_state=[ 0.06322648  0.58111835 -0.03684632 -0.90636669]\n",
      "[ episode 298 ][ timestamp 16 ] state=[ 0.06322648  0.58111835 -0.03684632 -0.90636669], action=0, reward=1.0, next_state=[ 0.07484885  0.38651415 -0.05497365 -0.62548889]\n",
      "[ episode 298 ][ timestamp 17 ] state=[ 0.07484885  0.38651415 -0.05497365 -0.62548889], action=1, reward=1.0, next_state=[ 0.08257913  0.58235868 -0.06748343 -0.93496602]\n",
      "[ episode 298 ][ timestamp 18 ] state=[ 0.08257913  0.58235868 -0.06748343 -0.93496602], action=0, reward=1.0, next_state=[ 0.09422631  0.38820869 -0.08618275 -0.66422898]\n",
      "[ episode 298 ][ timestamp 19 ] state=[ 0.09422631  0.38820869 -0.08618275 -0.66422898], action=0, reward=1.0, next_state=[ 0.10199048  0.19438464 -0.09946733 -0.39987833]\n",
      "[ episode 298 ][ timestamp 20 ] state=[ 0.10199048  0.19438464 -0.09946733 -0.39987833], action=0, reward=1.0, next_state=[ 0.10587817  0.00080399 -0.1074649  -0.1401378 ]\n",
      "[ episode 298 ][ timestamp 21 ] state=[ 0.10587817  0.00080399 -0.1074649  -0.1401378 ], action=1, reward=1.0, next_state=[ 0.10589425  0.19728786 -0.11026765 -0.46469728]\n",
      "[ episode 298 ][ timestamp 22 ] state=[ 0.10589425  0.19728786 -0.11026765 -0.46469728], action=1, reward=1.0, next_state=[ 0.10984001  0.39378113 -0.1195616  -0.79000017]\n",
      "[ episode 298 ][ timestamp 23 ] state=[ 0.10984001  0.39378113 -0.1195616  -0.79000017], action=1, reward=1.0, next_state=[ 0.11771563  0.59032425 -0.1353616  -1.1177776 ]\n",
      "[ episode 298 ][ timestamp 24 ] state=[ 0.11771563  0.59032425 -0.1353616  -1.1177776 ], action=0, reward=1.0, next_state=[ 0.12952212  0.39721278 -0.15771715 -0.870435  ]\n",
      "[ episode 298 ][ timestamp 25 ] state=[ 0.12952212  0.39721278 -0.15771715 -0.870435  ], action=1, reward=1.0, next_state=[ 0.13746637  0.59408784 -0.17512585 -1.20825912]\n",
      "[ episode 298 ][ timestamp 26 ] state=[ 0.13746637  0.59408784 -0.17512585 -1.20825912], action=1, reward=1.0, next_state=[ 0.14934813  0.7909848  -0.19929104 -1.55031138]\n",
      "[ episode 298 ][ timestamp 27 ] state=[ 0.14934813  0.7909848  -0.19929104 -1.55031138], action=1, reward=-1.0, next_state=[ 0.16516783  0.98786097 -0.23029726 -1.89798502]\n",
      "[ Ended! ] Episode 298: Exploration_rate=0.22566020663225933. Score=27.\n",
      "[ Experience replay ] starts\n",
      "[ episode 299 ] state=[-0.03608013 -0.02063794  0.04196844 -0.00438427]\n",
      "[ episode 299 ][ timestamp 1 ] state=[-0.03608013 -0.02063794  0.04196844 -0.00438427], action=1, reward=1.0, next_state=[-0.03649289  0.17385778  0.04188076 -0.28353585]\n",
      "[ episode 299 ][ timestamp 2 ] state=[-0.03649289  0.17385778  0.04188076 -0.28353585], action=0, reward=1.0, next_state=[-0.03301574 -0.02183572  0.03621004  0.02205634]\n",
      "[ episode 299 ][ timestamp 3 ] state=[-0.03301574 -0.02183572  0.03621004  0.02205634], action=1, reward=1.0, next_state=[-0.03345245  0.17274874  0.03665117 -0.2589856 ]\n",
      "[ episode 299 ][ timestamp 4 ] state=[-0.03345245  0.17274874  0.03665117 -0.2589856 ], action=1, reward=1.0, next_state=[-0.02999748  0.36732881  0.03147146 -0.53988666]\n",
      "[ episode 299 ][ timestamp 5 ] state=[-0.02999748  0.36732881  0.03147146 -0.53988666], action=0, reward=1.0, next_state=[-0.0226509   0.17177893  0.02067372 -0.237456  ]\n",
      "[ episode 299 ][ timestamp 6 ] state=[-0.0226509   0.17177893  0.02067372 -0.237456  ], action=0, reward=1.0, next_state=[-0.01921532 -0.02363218  0.0159246   0.06167567]\n",
      "[ episode 299 ][ timestamp 7 ] state=[-0.01921532 -0.02363218  0.0159246   0.06167567], action=1, reward=1.0, next_state=[-0.01968797  0.17125787  0.01715812 -0.2259407 ]\n",
      "[ episode 299 ][ timestamp 8 ] state=[-0.01968797  0.17125787  0.01715812 -0.2259407 ], action=0, reward=1.0, next_state=[-0.01626281 -0.02410505  0.0126393   0.07210477]\n",
      "[ episode 299 ][ timestamp 9 ] state=[-0.01626281 -0.02410505  0.0126393   0.07210477], action=1, reward=1.0, next_state=[-0.01674491  0.17083344  0.0140814  -0.21656375]\n",
      "[ episode 299 ][ timestamp 10 ] state=[-0.01674491  0.17083344  0.0140814  -0.21656375], action=0, reward=1.0, next_state=[-0.01332824 -0.02448695  0.00975012  0.08052759]\n",
      "[ episode 299 ][ timestamp 11 ] state=[-0.01332824 -0.02448695  0.00975012  0.08052759], action=1, reward=1.0, next_state=[-0.01381798  0.17049388  0.01136067 -0.20906327]\n",
      "[ episode 299 ][ timestamp 12 ] state=[-0.01381798  0.17049388  0.01136067 -0.20906327], action=0, reward=1.0, next_state=[-0.0104081  -0.02478865  0.00717941  0.0871816 ]\n",
      "[ episode 299 ][ timestamp 13 ] state=[-0.0104081  -0.02478865  0.00717941  0.0871816 ], action=1, reward=1.0, next_state=[-0.01090387  0.17022966  0.00892304 -0.2032276 ]\n",
      "[ episode 299 ][ timestamp 14 ] state=[-0.01090387  0.17022966  0.00892304 -0.2032276 ], action=0, reward=1.0, next_state=[-0.00749928 -0.02501876  0.00485849  0.0922567 ]\n",
      "[ episode 299 ][ timestamp 15 ] state=[-0.00749928 -0.02501876  0.00485849  0.0922567 ], action=1, reward=1.0, next_state=[-0.00799966  0.17003322  0.00670362 -0.19888942]\n",
      "[ episode 299 ][ timestamp 16 ] state=[-0.00799966  0.17003322  0.00670362 -0.19888942], action=0, reward=1.0, next_state=[-0.00459899 -0.02518397  0.00272583  0.09590063]\n",
      "[ episode 299 ][ timestamp 17 ] state=[-0.00459899 -0.02518397  0.00272583  0.09590063], action=0, reward=1.0, next_state=[-0.00510267 -0.22034488  0.00464385  0.38944231]\n",
      "[ episode 299 ][ timestamp 18 ] state=[-0.00510267 -0.22034488  0.00464385  0.38944231], action=0, reward=1.0, next_state=[-0.00950957 -0.41553244  0.01243269  0.68358577]\n",
      "[ episode 299 ][ timestamp 19 ] state=[-0.00950957 -0.41553244  0.01243269  0.68358577], action=1, reward=1.0, next_state=[-0.01782022 -0.22058531  0.02610441  0.39484279]\n",
      "[ episode 299 ][ timestamp 20 ] state=[-0.01782022 -0.22058531  0.02610441  0.39484279], action=1, reward=1.0, next_state=[-0.02223192 -0.0258433   0.03400126  0.11050313]\n",
      "[ episode 299 ][ timestamp 21 ] state=[-0.02223192 -0.0258433   0.03400126  0.11050313], action=1, reward=1.0, next_state=[-0.02274879  0.16877535  0.03621133 -0.17126167]\n",
      "[ episode 299 ][ timestamp 22 ] state=[-0.02274879  0.16877535  0.03621133 -0.17126167], action=0, reward=1.0, next_state=[-0.01937328 -0.02684568  0.03278609  0.13262131]\n",
      "[ episode 299 ][ timestamp 23 ] state=[-0.01937328 -0.02684568  0.03278609  0.13262131], action=1, reward=1.0, next_state=[-0.0199102   0.16779167  0.03543852 -0.14954042]\n",
      "[ episode 299 ][ timestamp 24 ] state=[-0.0199102   0.16779167  0.03543852 -0.14954042], action=0, reward=1.0, next_state=[-0.01655436 -0.02781935  0.03244771  0.15410863]\n",
      "[ episode 299 ][ timestamp 25 ] state=[-0.01655436 -0.02781935  0.03244771  0.15410863], action=1, reward=1.0, next_state=[-0.01711075  0.16682334  0.03552988 -0.12816377]\n",
      "[ episode 299 ][ timestamp 26 ] state=[-0.01711075  0.16682334  0.03552988 -0.12816377], action=0, reward=1.0, next_state=[-0.01377428 -0.02878911  0.03296661  0.1755133 ]\n",
      "[ episode 299 ][ timestamp 27 ] state=[-0.01377428 -0.02878911  0.03296661  0.1755133 ], action=1, reward=1.0, next_state=[-0.01435007  0.16584592  0.03647687 -0.10659018]\n",
      "[ episode 299 ][ timestamp 28 ] state=[-0.01435007  0.16584592  0.03647687 -0.10659018], action=0, reward=1.0, next_state=[-0.01103315 -0.02977926  0.03434507  0.19737422]\n",
      "[ episode 299 ][ timestamp 29 ] state=[-0.01103315 -0.02977926  0.03434507  0.19737422], action=1, reward=1.0, next_state=[-0.01162873  0.16483503  0.03829256 -0.0842796 ]\n",
      "[ episode 299 ][ timestamp 30 ] state=[-0.01162873  0.16483503  0.03829256 -0.0842796 ], action=0, reward=1.0, next_state=[-0.00833203 -0.03081431  0.03660696  0.22023453]\n",
      "[ episode 299 ][ timestamp 31 ] state=[-0.00833203 -0.03081431  0.03660696  0.22023453], action=1, reward=1.0, next_state=[-0.00894832  0.16376578  0.04101165 -0.06068001]\n",
      "[ episode 299 ][ timestamp 32 ] state=[-0.00894832  0.16376578  0.04101165 -0.06068001], action=0, reward=1.0, next_state=[-0.005673   -0.03191946  0.03979805  0.24465508]\n",
      "[ episode 299 ][ timestamp 33 ] state=[-0.005673   -0.03191946  0.03979805  0.24465508], action=1, reward=1.0, next_state=[-0.00631139  0.16261212  0.04469116 -0.0352137 ]\n",
      "[ episode 299 ][ timestamp 34 ] state=[-0.00631139  0.16261212  0.04469116 -0.0352137 ], action=0, reward=1.0, next_state=[-0.00305915 -0.03312128  0.04398688  0.27122807]\n",
      "[ episode 299 ][ timestamp 35 ] state=[-0.00305915 -0.03312128  0.04398688  0.27122807], action=1, reward=1.0, next_state=[-0.00372157  0.16134629  0.04941144 -0.00726316]\n",
      "[ episode 299 ][ timestamp 36 ] state=[-0.00372157  0.16134629  0.04941144 -0.00726316], action=0, reward=1.0, next_state=[-0.00049465 -0.03444818  0.04926618  0.30059116]\n",
      "[ episode 299 ][ timestamp 37 ] state=[-0.00049465 -0.03444818  0.04926618  0.30059116], action=1, reward=1.0, next_state=[-0.00118361  0.15993821  0.055278    0.02384375]\n",
      "[ episode 299 ][ timestamp 38 ] state=[-0.00118361  0.15993821  0.055278    0.02384375], action=0, reward=1.0, next_state=[ 0.00201515 -0.03593112  0.05575488  0.33344243]\n",
      "[ episode 299 ][ timestamp 39 ] state=[ 0.00201515 -0.03593112  0.05575488  0.33344243], action=1, reward=1.0, next_state=[0.00129653 0.15835478 0.06242373 0.05884988]\n",
      "[ episode 299 ][ timestamp 40 ] state=[0.00129653 0.15835478 0.06242373 0.05884988], action=0, reward=1.0, next_state=[ 0.00446363 -0.03760408  0.06360072  0.37055632]\n",
      "[ episode 299 ][ timestamp 41 ] state=[ 0.00446363 -0.03760408  0.06360072  0.37055632], action=1, reward=1.0, next_state=[0.00371154 0.15655932 0.07101185 0.09858607]\n",
      "[ episode 299 ][ timestamp 42 ] state=[0.00371154 0.15655932 0.07101185 0.09858607], action=0, reward=1.0, next_state=[ 0.00684273 -0.03950473  0.07298357  0.41280089]\n",
      "[ episode 299 ][ timestamp 43 ] state=[ 0.00684273 -0.03950473  0.07298357  0.41280089], action=1, reward=1.0, next_state=[0.00605264 0.15451088 0.08123959 0.14399033]\n",
      "[ episode 299 ][ timestamp 44 ] state=[0.00605264 0.15451088 0.08123959 0.14399033], action=0, reward=1.0, next_state=[ 0.00914285 -0.04167487  0.0841194   0.46115657]\n",
      "[ episode 299 ][ timestamp 45 ] state=[ 0.00914285 -0.04167487  0.0841194   0.46115657], action=0, reward=1.0, next_state=[ 0.00830936 -0.23787875  0.09334253  0.77912369]\n",
      "[ episode 299 ][ timestamp 46 ] state=[ 0.00830936 -0.23787875  0.09334253  0.77912369], action=1, reward=1.0, next_state=[ 0.00355178 -0.04415565  0.108925    0.5172069 ]\n",
      "[ episode 299 ][ timestamp 47 ] state=[ 0.00355178 -0.04415565  0.108925    0.5172069 ], action=1, reward=1.0, next_state=[0.00266867 0.14927759 0.11926914 0.26073726]\n",
      "[ episode 299 ][ timestamp 48 ] state=[0.00266867 0.14927759 0.11926914 0.26073726], action=0, reward=1.0, next_state=[ 0.00565422 -0.04732712  0.12448388  0.58853132]\n",
      "[ episode 299 ][ timestamp 49 ] state=[ 0.00565422 -0.04732712  0.12448388  0.58853132], action=1, reward=1.0, next_state=[0.00470768 0.14585198 0.13625451 0.33750874]\n",
      "[ episode 299 ][ timestamp 50 ] state=[0.00470768 0.14585198 0.13625451 0.33750874], action=1, reward=1.0, next_state=[0.00762472 0.33879855 0.14300469 0.09070631]\n",
      "[ episode 299 ][ timestamp 51 ] state=[0.00762472 0.33879855 0.14300469 0.09070631], action=1, reward=1.0, next_state=[ 0.01440069  0.53161204  0.14481881 -0.15366141]\n",
      "[ episode 299 ][ timestamp 52 ] state=[ 0.01440069  0.53161204  0.14481881 -0.15366141], action=0, reward=1.0, next_state=[0.02503293 0.33474541 0.14174558 0.18097541]\n",
      "[ episode 299 ][ timestamp 53 ] state=[0.02503293 0.33474541 0.14174558 0.18097541], action=1, reward=1.0, next_state=[ 0.03172784  0.52758455  0.14536509 -0.0638485 ]\n",
      "[ episode 299 ][ timestamp 54 ] state=[ 0.03172784  0.52758455  0.14536509 -0.0638485 ], action=0, reward=1.0, next_state=[0.04227953 0.33070994 0.14408812 0.27093577]\n",
      "[ episode 299 ][ timestamp 55 ] state=[0.04227953 0.33070994 0.14408812 0.27093577], action=1, reward=1.0, next_state=[0.04889373 0.52351335 0.14950684 0.02694309]\n",
      "[ episode 299 ][ timestamp 56 ] state=[0.04889373 0.52351335 0.14950684 0.02694309], action=0, reward=1.0, next_state=[0.05936399 0.32659883 0.1500457  0.36281134]\n",
      "[ episode 299 ][ timestamp 57 ] state=[0.05936399 0.32659883 0.1500457  0.36281134], action=1, reward=1.0, next_state=[0.06589597 0.51930518 0.15730193 0.12094771]\n",
      "[ episode 299 ][ timestamp 58 ] state=[0.06589597 0.51930518 0.15730193 0.12094771], action=1, reward=1.0, next_state=[ 0.07628207  0.71186469  0.15972088 -0.11826913]\n",
      "[ episode 299 ][ timestamp 59 ] state=[ 0.07628207  0.71186469  0.15972088 -0.11826913], action=0, reward=1.0, next_state=[0.09051937 0.51485766 0.1573555  0.22023861]\n",
      "[ episode 299 ][ timestamp 60 ] state=[0.09051937 0.51485766 0.1573555  0.22023861], action=1, reward=1.0, next_state=[ 0.10081652  0.70742138  0.16176027 -0.01896648]\n",
      "[ episode 299 ][ timestamp 61 ] state=[ 0.10081652  0.70742138  0.16176027 -0.01896648], action=0, reward=1.0, next_state=[0.11496495 0.51039381 0.16138094 0.32006705]\n",
      "[ episode 299 ][ timestamp 62 ] state=[0.11496495 0.51039381 0.16138094 0.32006705], action=0, reward=1.0, next_state=[0.12517282 0.31338564 0.16778228 0.65897984]\n",
      "[ episode 299 ][ timestamp 63 ] state=[0.12517282 0.31338564 0.16778228 0.65897984], action=1, reward=1.0, next_state=[0.13144054 0.50582432 0.18096188 0.42347217]\n",
      "[ episode 299 ][ timestamp 64 ] state=[0.13144054 0.50582432 0.18096188 0.42347217], action=1, reward=1.0, next_state=[0.14155702 0.69798335 0.18943132 0.19285315]\n",
      "[ episode 299 ][ timestamp 65 ] state=[0.14155702 0.69798335 0.18943132 0.19285315], action=1, reward=1.0, next_state=[ 0.15551669  0.88996187  0.19328838 -0.03460298]\n",
      "[ episode 299 ][ timestamp 66 ] state=[ 0.15551669  0.88996187  0.19328838 -0.03460298], action=0, reward=1.0, next_state=[0.17331593 0.69266928 0.19259633 0.31229849]\n",
      "[ episode 299 ][ timestamp 67 ] state=[0.17331593 0.69266928 0.19259633 0.31229849], action=1, reward=1.0, next_state=[0.18716931 0.88460061 0.19884229 0.08599845]\n",
      "[ episode 299 ][ timestamp 68 ] state=[0.18716931 0.88460061 0.19884229 0.08599845], action=1, reward=1.0, next_state=[ 0.20486133  1.07639927  0.20056226 -0.13795554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 299 ][ timestamp 69 ] state=[ 0.20486133  1.07639927  0.20056226 -0.13795554], action=0, reward=1.0, next_state=[0.22638931 0.87905428 0.19780315 0.21069897]\n",
      "[ episode 299 ][ timestamp 70 ] state=[0.22638931 0.87905428 0.19780315 0.21069897], action=1, reward=1.0, next_state=[ 0.2439704   1.07087932  0.20201713 -0.01365226]\n",
      "[ episode 299 ][ timestamp 71 ] state=[ 0.2439704   1.07087932  0.20201713 -0.01365226], action=1, reward=1.0, next_state=[ 0.26538798  1.26261746  0.20174409 -0.23642076]\n",
      "[ episode 299 ][ timestamp 72 ] state=[ 0.26538798  1.26261746  0.20174409 -0.23642076], action=0, reward=1.0, next_state=[0.29064033 1.06527079 0.19701567 0.11250678]\n",
      "[ episode 299 ][ timestamp 73 ] state=[0.29064033 1.06527079 0.19701567 0.11250678], action=1, reward=1.0, next_state=[ 0.31194575  1.25710439  0.19926581 -0.1121285 ]\n",
      "[ episode 299 ][ timestamp 74 ] state=[ 0.31194575  1.25710439  0.19926581 -0.1121285 ], action=0, reward=1.0, next_state=[0.33708784 1.05976794 0.19702324 0.2362161 ]\n",
      "[ episode 299 ][ timestamp 75 ] state=[0.33708784 1.05976794 0.19702324 0.2362161 ], action=1, reward=1.0, next_state=[0.3582832  1.25160962 0.20174756 0.01157155]\n",
      "[ episode 299 ][ timestamp 76 ] state=[0.3582832  1.25160962 0.20174756 0.01157155], action=0, reward=1.0, next_state=[0.38331539 1.05425206 0.20197899 0.36051589]\n",
      "[ episode 299 ][ timestamp 77 ] state=[0.38331539 1.05425206 0.20197899 0.36051589], action=0, reward=1.0, next_state=[0.40440043 0.85691809 0.20918931 0.70947848]\n",
      "[ episode 299 ][ timestamp 78 ] state=[0.40440043 0.85691809 0.20918931 0.70947848], action=1, reward=-1.0, next_state=[0.42153879 1.04862377 0.22337888 0.48924293]\n",
      "[ Ended! ] Episode 299: Exploration_rate=0.22453190559909803. Score=78.\n",
      "[ Experience replay ] starts\n",
      "[ episode 300 ] state=[ 0.01905837  0.0176583  -0.01354051  0.01228794]\n",
      "[ episode 300 ][ timestamp 1 ] state=[ 0.01905837  0.0176583  -0.01354051  0.01228794], action=1, reward=1.0, next_state=[ 0.01941154  0.21297179 -0.01329475 -0.28463624]\n",
      "[ episode 300 ][ timestamp 2 ] state=[ 0.01941154  0.21297179 -0.01329475 -0.28463624], action=1, reward=1.0, next_state=[ 0.02367097  0.40828081 -0.01898748 -0.58148242]\n",
      "[ episode 300 ][ timestamp 3 ] state=[ 0.02367097  0.40828081 -0.01898748 -0.58148242], action=1, reward=1.0, next_state=[ 0.03183659  0.60366359 -0.03061713 -0.88008574]\n",
      "[ episode 300 ][ timestamp 4 ] state=[ 0.03183659  0.60366359 -0.03061713 -0.88008574], action=0, reward=1.0, next_state=[ 0.04390986  0.40897068 -0.04821884 -0.59718326]\n",
      "[ episode 300 ][ timestamp 5 ] state=[ 0.04390986  0.40897068 -0.04821884 -0.59718326], action=0, reward=1.0, next_state=[ 0.05208928  0.21455546 -0.06016251 -0.32007024]\n",
      "[ episode 300 ][ timestamp 6 ] state=[ 0.05208928  0.21455546 -0.06016251 -0.32007024], action=1, reward=1.0, next_state=[ 0.05638039  0.41048031 -0.06656391 -0.63110293]\n",
      "[ episode 300 ][ timestamp 7 ] state=[ 0.05638039  0.41048031 -0.06656391 -0.63110293], action=1, reward=1.0, next_state=[ 0.06458999  0.60646478 -0.07918597 -0.94398394]\n",
      "[ episode 300 ][ timestamp 8 ] state=[ 0.06458999  0.60646478 -0.07918597 -0.94398394], action=0, reward=1.0, next_state=[ 0.07671929  0.41249377 -0.09806565 -0.67719551]\n",
      "[ episode 300 ][ timestamp 9 ] state=[ 0.07671929  0.41249377 -0.09806565 -0.67719551], action=1, reward=1.0, next_state=[ 0.08496916  0.60883161 -0.11160956 -0.9990724 ]\n",
      "[ episode 300 ][ timestamp 10 ] state=[ 0.08496916  0.60883161 -0.11160956 -0.9990724 ], action=1, reward=1.0, next_state=[ 0.0971458   0.80525419 -0.13159101 -1.32461823]\n",
      "[ episode 300 ][ timestamp 11 ] state=[ 0.0971458   0.80525419 -0.13159101 -1.32461823], action=0, reward=1.0, next_state=[ 0.11325088  0.61201656 -0.15808337 -1.07584396]\n",
      "[ episode 300 ][ timestamp 12 ] state=[ 0.11325088  0.61201656 -0.15808337 -1.07584396], action=0, reward=1.0, next_state=[ 0.12549121  0.41929588 -0.17960025 -0.83665072]\n",
      "[ episode 300 ][ timestamp 13 ] state=[ 0.12549121  0.41929588 -0.17960025 -0.83665072], action=1, reward=1.0, next_state=[ 0.13387713  0.61635641 -0.19633327 -1.18000605]\n",
      "[ episode 300 ][ timestamp 14 ] state=[ 0.13387713  0.61635641 -0.19633327 -1.18000605], action=1, reward=-1.0, next_state=[ 0.14620426  0.81340859 -0.21993339 -1.52725766]\n",
      "[ Ended! ] Episode 300: Exploration_rate=0.22340924607110255. Score=14.\n",
      "[ Experience replay ] starts\n",
      "[ episode 301 ] state=[-0.00580059 -0.03888155  0.01423531 -0.02519646]\n",
      "[ episode 301 ][ timestamp 1 ] state=[-0.00580059 -0.03888155  0.01423531 -0.02519646], action=1, reward=1.0, next_state=[-0.00657822  0.15603339  0.01373139 -0.31335421]\n",
      "[ episode 301 ][ timestamp 2 ] state=[-0.00657822  0.15603339  0.01373139 -0.31335421], action=1, reward=1.0, next_state=[-0.00345755  0.35095707  0.0074643  -0.60167525]\n",
      "[ episode 301 ][ timestamp 3 ] state=[-0.00345755  0.35095707  0.0074643  -0.60167525], action=0, reward=1.0, next_state=[ 0.00356159  0.1557315  -0.0045692  -0.30665058]\n",
      "[ episode 301 ][ timestamp 4 ] state=[ 0.00356159  0.1557315  -0.0045692  -0.30665058], action=1, reward=1.0, next_state=[ 0.00667622  0.35091826 -0.01070222 -0.60077101]\n",
      "[ episode 301 ][ timestamp 5 ] state=[ 0.00667622  0.35091826 -0.01070222 -0.60077101], action=1, reward=1.0, next_state=[ 0.01369459  0.54618828 -0.02271764 -0.89680566]\n",
      "[ episode 301 ][ timestamp 6 ] state=[ 0.01369459  0.54618828 -0.02271764 -0.89680566], action=0, reward=1.0, next_state=[ 0.02461835  0.35138156 -0.04065375 -0.61134938]\n",
      "[ episode 301 ][ timestamp 7 ] state=[ 0.02461835  0.35138156 -0.04065375 -0.61134938], action=0, reward=1.0, next_state=[ 0.03164598  0.15685069 -0.05288074 -0.33174308]\n",
      "[ episode 301 ][ timestamp 8 ] state=[ 0.03164598  0.15685069 -0.05288074 -0.33174308], action=1, reward=1.0, next_state=[ 0.034783    0.35268391 -0.0595156  -0.64062198]\n",
      "[ episode 301 ][ timestamp 9 ] state=[ 0.034783    0.35268391 -0.0595156  -0.64062198], action=0, reward=1.0, next_state=[ 0.04183667  0.15843995 -0.07232804 -0.36725918]\n",
      "[ episode 301 ][ timestamp 10 ] state=[ 0.04183667  0.15843995 -0.07232804 -0.36725918], action=1, reward=1.0, next_state=[ 0.04500547  0.35451115 -0.07967322 -0.68184293]\n",
      "[ episode 301 ][ timestamp 11 ] state=[ 0.04500547  0.35451115 -0.07967322 -0.68184293], action=1, reward=1.0, next_state=[ 0.0520957   0.5506439  -0.09331008 -0.99850793]\n",
      "[ episode 301 ][ timestamp 12 ] state=[ 0.0520957   0.5506439  -0.09331008 -0.99850793], action=0, reward=1.0, next_state=[ 0.06310857  0.35688484 -0.11328024 -0.73652707]\n",
      "[ episode 301 ][ timestamp 13 ] state=[ 0.06310857  0.35688484 -0.11328024 -0.73652707], action=0, reward=1.0, next_state=[ 0.07024627  0.16349462 -0.12801078 -0.4815342 ]\n",
      "[ episode 301 ][ timestamp 14 ] state=[ 0.07024627  0.16349462 -0.12801078 -0.4815342 ], action=1, reward=1.0, next_state=[ 0.07351616  0.36016903 -0.13764146 -0.81166444]\n",
      "[ episode 301 ][ timestamp 15 ] state=[ 0.07351616  0.36016903 -0.13764146 -0.81166444], action=0, reward=1.0, next_state=[ 0.08071954  0.16717389 -0.15387475 -0.56524858]\n",
      "[ episode 301 ][ timestamp 16 ] state=[ 0.08071954  0.16717389 -0.15387475 -0.56524858], action=1, reward=1.0, next_state=[ 0.08406302  0.36408172 -0.16517972 -0.90218139]\n",
      "[ episode 301 ][ timestamp 17 ] state=[ 0.08406302  0.36408172 -0.16517972 -0.90218139], action=1, reward=1.0, next_state=[ 0.09134466  0.56100958 -0.18322335 -1.24189485]\n",
      "[ episode 301 ][ timestamp 18 ] state=[ 0.09134466  0.56100958 -0.18322335 -1.24189485], action=0, reward=1.0, next_state=[ 0.10256485  0.36865    -0.20806125 -1.01175196]\n",
      "[ episode 301 ][ timestamp 19 ] state=[ 0.10256485  0.36865    -0.20806125 -1.01175196], action=1, reward=-1.0, next_state=[ 0.10993785  0.56584852 -0.22829629 -1.36189997]\n",
      "[ Ended! ] Episode 301: Exploration_rate=0.22229219984074702. Score=19.\n",
      "[ Experience replay ] starts\n",
      "[ episode 302 ] state=[-0.02556469 -0.03206176  0.03584896  0.03421253]\n",
      "[ episode 302 ][ timestamp 1 ] state=[-0.02556469 -0.03206176  0.03584896  0.03421253], action=1, reward=1.0, next_state=[-0.02620593  0.16252825  0.03653322 -0.24694762]\n",
      "[ episode 302 ][ timestamp 2 ] state=[-0.02620593  0.16252825  0.03653322 -0.24694762], action=0, reward=1.0, next_state=[-0.02295536 -0.0330959   0.03159426  0.0570312 ]\n",
      "[ episode 302 ][ timestamp 3 ] state=[-0.02295536 -0.0330959   0.03159426  0.0570312 ], action=1, reward=1.0, next_state=[-0.02361728  0.16155913  0.03273489 -0.22551847]\n",
      "[ episode 302 ][ timestamp 4 ] state=[-0.02361728  0.16155913  0.03273489 -0.22551847], action=0, reward=1.0, next_state=[-0.0203861  -0.034015    0.02822452  0.0773079 ]\n",
      "[ episode 302 ][ timestamp 5 ] state=[-0.0203861  -0.034015    0.02822452  0.0773079 ], action=1, reward=1.0, next_state=[-0.0210664   0.16069121  0.02977068 -0.20633818]\n",
      "[ episode 302 ][ timestamp 6 ] state=[-0.0210664   0.16069121  0.02977068 -0.20633818], action=0, reward=1.0, next_state=[-0.01785257 -0.03484354  0.02564391  0.09558525]\n",
      "[ episode 302 ][ timestamp 7 ] state=[-0.01785257 -0.03484354  0.02564391  0.09558525], action=0, reward=1.0, next_state=[-0.01854945 -0.23032346  0.02755562  0.39624722]\n",
      "[ episode 302 ][ timestamp 8 ] state=[-0.01854945 -0.23032346  0.02755562  0.39624722], action=1, reward=1.0, next_state=[-0.02315591 -0.0356031   0.03548056  0.11237788]\n",
      "[ episode 302 ][ timestamp 9 ] state=[-0.02315591 -0.0356031   0.03548056  0.11237788], action=1, reward=1.0, next_state=[-0.02386798  0.15899296  0.03772812 -0.1689034 ]\n",
      "[ episode 302 ][ timestamp 10 ] state=[-0.02386798  0.15899296  0.03772812 -0.1689034 ], action=0, reward=1.0, next_state=[-0.02068812 -0.03664814  0.03435005  0.13543885]\n",
      "[ episode 302 ][ timestamp 11 ] state=[-0.02068812 -0.03664814  0.03435005  0.13543885], action=1, reward=1.0, next_state=[-0.02142108  0.15796539  0.03705883 -0.1462123 ]\n",
      "[ episode 302 ][ timestamp 12 ] state=[-0.02142108  0.15796539  0.03705883 -0.1462123 ], action=0, reward=1.0, next_state=[-0.01826177 -0.03766713  0.03413458  0.15792779]\n",
      "[ episode 302 ][ timestamp 13 ] state=[-0.01826177 -0.03766713  0.03413458  0.15792779], action=1, reward=1.0, next_state=[-0.01901512  0.15694991  0.03729314 -0.12379409]\n",
      "[ episode 302 ][ timestamp 14 ] state=[-0.01901512  0.15694991  0.03729314 -0.12379409], action=0, reward=1.0, next_state=[-0.01587612 -0.03868593  0.03481726  0.18041727]\n",
      "[ episode 302 ][ timestamp 15 ] state=[-0.01587612 -0.03868593  0.03481726  0.18041727], action=1, reward=1.0, next_state=[-0.01664984  0.15592094  0.0384256  -0.10108192]\n",
      "[ episode 302 ][ timestamp 16 ] state=[-0.01664984  0.15592094  0.0384256  -0.10108192], action=0, reward=1.0, next_state=[-0.01353142 -0.03973003  0.03640396  0.20347226]\n",
      "[ episode 302 ][ timestamp 17 ] state=[-0.01353142 -0.03973003  0.03640396  0.20347226], action=1, reward=1.0, next_state=[-0.01432602  0.1548529   0.04047341 -0.07750837]\n",
      "[ episode 302 ][ timestamp 18 ] state=[-0.01432602  0.1548529   0.04047341 -0.07750837], action=0, reward=1.0, next_state=[-0.01122896 -0.04082519  0.03892324  0.22766433]\n",
      "[ episode 302 ][ timestamp 19 ] state=[-0.01122896 -0.04082519  0.03892324  0.22766433], action=0, reward=1.0, next_state=[-0.01204546 -0.23648114  0.04347653  0.53236652]\n",
      "[ episode 302 ][ timestamp 20 ] state=[-0.01204546 -0.23648114  0.04347653  0.53236652], action=1, reward=1.0, next_state=[-0.01677509 -0.04199679  0.05412386  0.25369373]\n",
      "[ episode 302 ][ timestamp 21 ] state=[-0.01677509 -0.04199679  0.05412386  0.25369373], action=1, reward=1.0, next_state=[-0.01761502  0.15231225  0.05919773 -0.02143838]\n",
      "[ episode 302 ][ timestamp 22 ] state=[-0.01761502  0.15231225  0.05919773 -0.02143838], action=0, reward=1.0, next_state=[-0.01456878 -0.0436065   0.05876896  0.28931893]\n",
      "[ episode 302 ][ timestamp 23 ] state=[-0.01456878 -0.0436065   0.05876896  0.28931893], action=0, reward=1.0, next_state=[-0.01544091 -0.23951509  0.06455534  0.59994263]\n",
      "[ episode 302 ][ timestamp 24 ] state=[-0.01544091 -0.23951509  0.06455534  0.59994263], action=1, reward=1.0, next_state=[-0.02023121 -0.04535286  0.0765542   0.32827202]\n",
      "[ episode 302 ][ timestamp 25 ] state=[-0.02023121 -0.04535286  0.0765542   0.32827202], action=1, reward=1.0, next_state=[-0.02113827  0.14860051  0.08311964  0.06067901]\n",
      "[ episode 302 ][ timestamp 26 ] state=[-0.02113827  0.14860051  0.08311964  0.06067901], action=0, reward=1.0, next_state=[-0.01816626 -0.04760876  0.08433322  0.37838585]\n",
      "[ episode 302 ][ timestamp 27 ] state=[-0.01816626 -0.04760876  0.08433322  0.37838585], action=1, reward=1.0, next_state=[-0.01911843  0.1462206   0.09190093  0.1134397 ]\n",
      "[ episode 302 ][ timestamp 28 ] state=[-0.01911843  0.1462206   0.09190093  0.1134397 ], action=1, reward=1.0, next_state=[-0.01619402  0.33991375  0.09416973 -0.14889312]\n",
      "[ episode 302 ][ timestamp 29 ] state=[-0.01619402  0.33991375  0.09416973 -0.14889312], action=0, reward=1.0, next_state=[-0.00939574  0.14357829  0.09119187  0.17195021]\n",
      "[ episode 302 ][ timestamp 30 ] state=[-0.00939574  0.14357829  0.09119187  0.17195021], action=0, reward=1.0, next_state=[-0.00652418 -0.0527225   0.09463087  0.49195119]\n",
      "[ episode 302 ][ timestamp 31 ] state=[-0.00652418 -0.0527225   0.09463087  0.49195119], action=1, reward=1.0, next_state=[-0.00757863  0.14094618  0.10446989  0.2305279 ]\n",
      "[ episode 302 ][ timestamp 32 ] state=[-0.00757863  0.14094618  0.10446989  0.2305279 ], action=1, reward=1.0, next_state=[-0.0047597   0.33443221  0.10908045 -0.0274605 ]\n",
      "[ episode 302 ][ timestamp 33 ] state=[-0.0047597   0.33443221  0.10908045 -0.0274605 ], action=0, reward=1.0, next_state=[0.00192894 0.13792875 0.10853124 0.29754894]\n",
      "[ episode 302 ][ timestamp 34 ] state=[0.00192894 0.13792875 0.10853124 0.29754894], action=0, reward=1.0, next_state=[ 0.00468751 -0.05855946  0.11448222  0.62239271]\n",
      "[ episode 302 ][ timestamp 35 ] state=[ 0.00468751 -0.05855946  0.11448222  0.62239271], action=1, reward=1.0, next_state=[0.00351633 0.13479351 0.12693007 0.36784607]\n",
      "[ episode 302 ][ timestamp 36 ] state=[0.00351633 0.13479351 0.12693007 0.36784607], action=1, reward=1.0, next_state=[0.0062122  0.32790486 0.134287   0.11772667]\n",
      "[ episode 302 ][ timestamp 37 ] state=[0.0062122  0.32790486 0.134287   0.11772667], action=0, reward=1.0, next_state=[0.01277029 0.13113999 0.13664153 0.4495786 ]\n",
      "[ episode 302 ][ timestamp 38 ] state=[0.01277029 0.13113999 0.13664153 0.4495786 ], action=1, reward=1.0, next_state=[0.01539309 0.3240915  0.1456331  0.20289679]\n",
      "[ episode 302 ][ timestamp 39 ] state=[0.01539309 0.3240915  0.1456331  0.20289679], action=1, reward=1.0, next_state=[ 0.02187492  0.51686305  0.14969104 -0.04053463]\n",
      "[ episode 302 ][ timestamp 40 ] state=[ 0.02187492  0.51686305  0.14969104 -0.04053463], action=0, reward=1.0, next_state=[0.03221218 0.31994692 0.14888034 0.29538144]\n",
      "[ episode 302 ][ timestamp 41 ] state=[0.03221218 0.31994692 0.14888034 0.29538144], action=1, reward=1.0, next_state=[0.03861112 0.51266749 0.15478797 0.05310777]\n",
      "[ episode 302 ][ timestamp 42 ] state=[0.03861112 0.51266749 0.15478797 0.05310777], action=0, reward=1.0, next_state=[0.04886447 0.31570392 0.15585013 0.39034702]\n",
      "[ episode 302 ][ timestamp 43 ] state=[0.04886447 0.31570392 0.15585013 0.39034702], action=1, reward=1.0, next_state=[0.05517855 0.50831029 0.16365707 0.15057373]\n",
      "[ episode 302 ][ timestamp 44 ] state=[0.05517855 0.50831029 0.16365707 0.15057373], action=1, reward=1.0, next_state=[ 0.06534476  0.70075665  0.16666854 -0.08633794]\n",
      "[ episode 302 ][ timestamp 45 ] state=[ 0.06534476  0.70075665  0.16666854 -0.08633794], action=0, reward=1.0, next_state=[0.07935989 0.50368652 0.16494179 0.25394505]\n",
      "[ episode 302 ][ timestamp 46 ] state=[0.07935989 0.50368652 0.16494179 0.25394505], action=1, reward=1.0, next_state=[0.08943362 0.69611636 0.17002069 0.01749109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 302 ][ timestamp 47 ] state=[0.08943362 0.69611636 0.17002069 0.01749109], action=0, reward=1.0, next_state=[0.10335595 0.49901573 0.17037051 0.35862473]\n",
      "[ episode 302 ][ timestamp 48 ] state=[0.10335595 0.49901573 0.17037051 0.35862473], action=1, reward=1.0, next_state=[0.11333626 0.69135803 0.177543   0.12413535]\n",
      "[ episode 302 ][ timestamp 49 ] state=[0.11333626 0.69135803 0.177543   0.12413535], action=1, reward=1.0, next_state=[ 0.12716342  0.88355093  0.18002571 -0.10769843]\n",
      "[ episode 302 ][ timestamp 50 ] state=[ 0.12716342  0.88355093  0.18002571 -0.10769843], action=0, reward=1.0, next_state=[0.14483444 0.68636717 0.17787174 0.23593934]\n",
      "[ episode 302 ][ timestamp 51 ] state=[0.14483444 0.68636717 0.17787174 0.23593934], action=1, reward=1.0, next_state=[0.15856178 0.878561   0.18259053 0.00421609]\n",
      "[ episode 302 ][ timestamp 52 ] state=[0.15856178 0.878561   0.18259053 0.00421609], action=0, reward=1.0, next_state=[0.176133   0.68135401 0.18267485 0.34849303]\n",
      "[ episode 302 ][ timestamp 53 ] state=[0.176133   0.68135401 0.18267485 0.34849303], action=1, reward=1.0, next_state=[0.18976008 0.87347165 0.18964471 0.11851965]\n",
      "[ episode 302 ][ timestamp 54 ] state=[0.18976008 0.87347165 0.18964471 0.11851965], action=1, reward=1.0, next_state=[ 0.20722952  1.06544199  0.1920151  -0.10885124]\n",
      "[ episode 302 ][ timestamp 55 ] state=[ 0.20722952  1.06544199  0.1920151  -0.10885124], action=1, reward=1.0, next_state=[ 0.22853836  1.25736811  0.18983808 -0.33534332]\n",
      "[ episode 302 ][ timestamp 56 ] state=[ 0.22853836  1.25736811  0.18983808 -0.33534332], action=0, reward=1.0, next_state=[0.25368572 1.06012316 0.18313121 0.01068656]\n",
      "[ episode 302 ][ timestamp 57 ] state=[0.25368572 1.06012316 0.18313121 0.01068656], action=0, reward=1.0, next_state=[0.27488818 0.86291174 0.18334494 0.35509725]\n",
      "[ episode 302 ][ timestamp 58 ] state=[0.27488818 0.86291174 0.18334494 0.35509725], action=1, reward=1.0, next_state=[0.29214642 1.05501798 0.19044689 0.12536957]\n",
      "[ episode 302 ][ timestamp 59 ] state=[0.29214642 1.05501798 0.19044689 0.12536957], action=1, reward=1.0, next_state=[ 0.31324678  1.24697381  0.19295428 -0.10170473]\n",
      "[ episode 302 ][ timestamp 60 ] state=[ 0.31324678  1.24697381  0.19295428 -0.10170473], action=0, reward=1.0, next_state=[0.33818625 1.04968554 0.19092018 0.24511295]\n",
      "[ episode 302 ][ timestamp 61 ] state=[0.33818625 1.04968554 0.19092018 0.24511295], action=1, reward=1.0, next_state=[0.35917996 1.24164082 0.19582244 0.01820193]\n",
      "[ episode 302 ][ timestamp 62 ] state=[0.35917996 1.24164082 0.19582244 0.01820193], action=0, reward=1.0, next_state=[0.38401278 1.04432838 0.19618648 0.36571858]\n",
      "[ episode 302 ][ timestamp 63 ] state=[0.38401278 1.04432838 0.19618648 0.36571858], action=1, reward=1.0, next_state=[0.40489935 1.23620034 0.20350085 0.14074117]\n",
      "[ episode 302 ][ timestamp 64 ] state=[0.40489935 1.23620034 0.20350085 0.14074117], action=1, reward=1.0, next_state=[ 0.42962335  1.42791465  0.20631568 -0.08147912]\n",
      "[ episode 302 ][ timestamp 65 ] state=[ 0.42962335  1.42791465  0.20631568 -0.08147912], action=0, reward=1.0, next_state=[0.45818165 1.23052453 0.2046861  0.26855419]\n",
      "[ episode 302 ][ timestamp 66 ] state=[0.45818165 1.23052453 0.2046861  0.26855419], action=1, reward=-1.0, next_state=[0.48279214 1.42222707 0.21005718 0.04676149]\n",
      "[ Ended! ] Episode 302: Exploration_rate=0.2211807388415433. Score=66.\n",
      "[ Experience replay ] starts\n",
      "[ episode 303 ] state=[0.00334792 0.02398538 0.0325281  0.00448266]\n",
      "[ episode 303 ][ timestamp 1 ] state=[0.00334792 0.02398538 0.0325281  0.00448266], action=1, reward=1.0, next_state=[ 0.00382762  0.21862609  0.03261775 -0.2777624 ]\n",
      "[ episode 303 ][ timestamp 2 ] state=[ 0.00382762  0.21862609  0.03261775 -0.2777624 ], action=1, reward=1.0, next_state=[ 0.00820015  0.4132679   0.0270625  -0.55998189]\n",
      "[ episode 303 ][ timestamp 3 ] state=[ 0.00820015  0.4132679   0.0270625  -0.55998189], action=1, reward=1.0, next_state=[ 0.0164655   0.60799977  0.01586287 -0.84401733]\n",
      "[ episode 303 ][ timestamp 4 ] state=[ 0.0164655   0.60799977  0.01586287 -0.84401733], action=1, reward=1.0, next_state=[ 2.86255001e-02  8.02901697e-01 -1.01748157e-03 -1.13166996e+00]\n",
      "[ episode 303 ][ timestamp 5 ] state=[ 2.86255001e-02  8.02901697e-01 -1.01748157e-03 -1.13166996e+00], action=0, reward=1.0, next_state=[ 0.04468353  0.60779308 -0.02365088 -0.83930632]\n",
      "[ episode 303 ][ timestamp 6 ] state=[ 0.04468353  0.60779308 -0.02365088 -0.83930632], action=0, reward=1.0, next_state=[ 0.0568394   0.41300191 -0.04043701 -0.55415399]\n",
      "[ episode 303 ][ timestamp 7 ] state=[ 0.0568394   0.41300191 -0.04043701 -0.55415399], action=1, reward=1.0, next_state=[ 0.06509943  0.60866765 -0.05152009 -0.85929791]\n",
      "[ episode 303 ][ timestamp 8 ] state=[ 0.06509943  0.60866765 -0.05152009 -0.85929791], action=1, reward=1.0, next_state=[ 0.07727279  0.80445208 -0.06870605 -1.16772509]\n",
      "[ episode 303 ][ timestamp 9 ] state=[ 0.07727279  0.80445208 -0.06870605 -1.16772509], action=0, reward=1.0, next_state=[ 0.09336183  0.610288   -0.09206055 -0.89734982]\n",
      "[ episode 303 ][ timestamp 10 ] state=[ 0.09336183  0.610288   -0.09206055 -0.89734982], action=0, reward=1.0, next_state=[ 0.10556759  0.41652648 -0.11000754 -0.63496588]\n",
      "[ episode 303 ][ timestamp 11 ] state=[ 0.10556759  0.41652648 -0.11000754 -0.63496588], action=0, reward=1.0, next_state=[ 0.11389812  0.22309691 -0.12270686 -0.37885238]\n",
      "[ episode 303 ][ timestamp 12 ] state=[ 0.11389812  0.22309691 -0.12270686 -0.37885238], action=1, reward=1.0, next_state=[ 0.11836006  0.41972836 -0.13028391 -0.70756719]\n",
      "[ episode 303 ][ timestamp 13 ] state=[ 0.11836006  0.41972836 -0.13028391 -0.70756719], action=1, reward=1.0, next_state=[ 0.12675462  0.61639137 -0.14443525 -1.03825684]\n",
      "[ episode 303 ][ timestamp 14 ] state=[ 0.12675462  0.61639137 -0.14443525 -1.03825684], action=0, reward=1.0, next_state=[ 0.13908245  0.42345324 -0.16520039 -0.79417961]\n",
      "[ episode 303 ][ timestamp 15 ] state=[ 0.13908245  0.42345324 -0.16520039 -0.79417961], action=1, reward=1.0, next_state=[ 0.14755152  0.6204106  -0.18108398 -1.13394172]\n",
      "[ episode 303 ][ timestamp 16 ] state=[ 0.14755152  0.6204106  -0.18108398 -1.13394172], action=0, reward=1.0, next_state=[ 0.15995973  0.42805983 -0.20376282 -0.90308144]\n",
      "[ episode 303 ][ timestamp 17 ] state=[ 0.15995973  0.42805983 -0.20376282 -0.90308144], action=1, reward=-1.0, next_state=[ 0.16852092  0.62527193 -0.22182444 -1.25227232]\n",
      "[ Ended! ] Episode 303: Exploration_rate=0.22007483514733558. Score=17.\n",
      "[ Experience replay ] starts\n",
      "[ episode 304 ] state=[-0.00343493  0.03122857  0.01465782  0.04991634]\n",
      "[ episode 304 ][ timestamp 1 ] state=[-0.00343493  0.03122857  0.01465782  0.04991634], action=1, reward=1.0, next_state=[-0.00281036  0.2261373   0.01565614 -0.23810612]\n",
      "[ episode 304 ][ timestamp 2 ] state=[-0.00281036  0.2261373   0.01565614 -0.23810612], action=0, reward=1.0, next_state=[0.00171238 0.03079523 0.01089402 0.05947381]\n",
      "[ episode 304 ][ timestamp 3 ] state=[0.00171238 0.03079523 0.01089402 0.05947381], action=1, reward=1.0, next_state=[ 0.00232829  0.2257593   0.0120835  -0.22975217]\n",
      "[ episode 304 ][ timestamp 4 ] state=[ 0.00232829  0.2257593   0.0120835  -0.22975217], action=0, reward=1.0, next_state=[0.00684347 0.03046678 0.00748846 0.06671769]\n",
      "[ episode 304 ][ timestamp 5 ] state=[0.00684347 0.03046678 0.00748846 0.06671769], action=1, reward=1.0, next_state=[ 0.00745281  0.22548057  0.00882281 -0.22359321]\n",
      "[ episode 304 ][ timestamp 6 ] state=[ 0.00745281  0.22548057  0.00882281 -0.22359321], action=0, reward=1.0, next_state=[0.01196242 0.03023364 0.00435094 0.07185966]\n",
      "[ episode 304 ][ timestamp 7 ] state=[0.01196242 0.03023364 0.00435094 0.07185966], action=1, reward=1.0, next_state=[ 0.01256709  0.22529294  0.00578814 -0.21944735]\n",
      "[ episode 304 ][ timestamp 8 ] state=[ 0.01256709  0.22529294  0.00578814 -0.21944735], action=0, reward=1.0, next_state=[0.01707295 0.03008873 0.00139919 0.07505576]\n",
      "[ episode 304 ][ timestamp 9 ] state=[0.01707295 0.03008873 0.00139919 0.07505576], action=1, reward=1.0, next_state=[ 0.01767473  0.2251906   0.00290031 -0.21718539]\n",
      "[ episode 304 ][ timestamp 10 ] state=[ 0.01767473  0.2251906   0.00290031 -0.21718539], action=0, reward=1.0, next_state=[ 0.02217854  0.0300273  -0.0014434   0.07641101]\n",
      "[ episode 304 ][ timestamp 11 ] state=[ 0.02217854  0.0300273  -0.0014434   0.07641101], action=1, reward=1.0, next_state=[ 2.27790854e-02  2.25169918e-01  8.48188419e-05 -2.16726965e-01]\n",
      "[ episode 304 ][ timestamp 12 ] state=[ 2.27790854e-02  2.25169918e-01  8.48188419e-05 -2.16726965e-01], action=0, reward=1.0, next_state=[ 0.02728248  0.03004675 -0.00424972  0.07598272]\n",
      "[ episode 304 ][ timestamp 13 ] state=[ 0.02728248  0.03004675 -0.00424972  0.07598272], action=1, reward=1.0, next_state=[ 0.02788342  0.22522937 -0.00273007 -0.21803798]\n",
      "[ episode 304 ][ timestamp 14 ] state=[ 0.02788342  0.22522937 -0.00273007 -0.21803798], action=0, reward=1.0, next_state=[ 0.03238801  0.03014655 -0.00709083  0.07378252]\n",
      "[ episode 304 ][ timestamp 15 ] state=[ 0.03238801  0.03014655 -0.00709083  0.07378252], action=1, reward=1.0, next_state=[ 0.03299094  0.22536944 -0.00561518 -0.22112913]\n",
      "[ episode 304 ][ timestamp 16 ] state=[ 0.03299094  0.22536944 -0.00561518 -0.22112913], action=0, reward=1.0, next_state=[ 0.03749833  0.0303282  -0.01003776  0.06977727]\n",
      "[ episode 304 ][ timestamp 17 ] state=[ 0.03749833  0.0303282  -0.01003776  0.06977727], action=1, reward=1.0, next_state=[ 0.03810489  0.22559261 -0.00864221 -0.22605565]\n",
      "[ episode 304 ][ timestamp 18 ] state=[ 0.03810489  0.22559261 -0.00864221 -0.22605565], action=0, reward=1.0, next_state=[ 0.04261674  0.03059523 -0.01316333  0.06388872]\n",
      "[ episode 304 ][ timestamp 19 ] state=[ 0.04261674  0.03059523 -0.01316333  0.06388872], action=0, reward=1.0, next_state=[ 0.04322865 -0.16433555 -0.01188555  0.35238965]\n",
      "[ episode 304 ][ timestamp 20 ] state=[ 0.04322865 -0.16433555 -0.01188555  0.35238965], action=1, reward=1.0, next_state=[ 0.03994194  0.03095339 -0.00483776  0.05598267]\n",
      "[ episode 304 ][ timestamp 21 ] state=[ 0.03994194  0.03095339 -0.00483776  0.05598267], action=1, reward=1.0, next_state=[ 0.040561    0.22614437 -0.0037181  -0.23822267]\n",
      "[ episode 304 ][ timestamp 22 ] state=[ 0.040561    0.22614437 -0.0037181  -0.23822267], action=0, reward=1.0, next_state=[ 0.04508389  0.03107573 -0.00848256  0.05328514]\n",
      "[ episode 304 ][ timestamp 23 ] state=[ 0.04508389  0.03107573 -0.00848256  0.05328514], action=1, reward=1.0, next_state=[ 0.04570541  0.22631828 -0.00741686 -0.24206199]\n",
      "[ episode 304 ][ timestamp 24 ] state=[ 0.04570541  0.22631828 -0.00741686 -0.24206199], action=0, reward=1.0, next_state=[ 0.05023177  0.03130305 -0.0122581   0.04827227]\n",
      "[ episode 304 ][ timestamp 25 ] state=[ 0.05023177  0.03130305 -0.0122581   0.04827227], action=1, reward=1.0, next_state=[ 0.05085783  0.22659861 -0.01129265 -0.24825285]\n",
      "[ episode 304 ][ timestamp 26 ] state=[ 0.05085783  0.22659861 -0.01129265 -0.24825285], action=0, reward=1.0, next_state=[ 0.0553898   0.03163974 -0.01625771  0.04084685]\n",
      "[ episode 304 ][ timestamp 27 ] state=[ 0.0553898   0.03163974 -0.01625771  0.04084685], action=1, reward=1.0, next_state=[ 0.0560226   0.226991   -0.01544077 -0.25692088]\n",
      "[ episode 304 ][ timestamp 28 ] state=[ 0.0560226   0.226991   -0.01544077 -0.25692088], action=1, reward=1.0, next_state=[ 0.06056242  0.42232996 -0.02057919 -0.5544338 ]\n",
      "[ episode 304 ][ timestamp 29 ] state=[ 0.06056242  0.42232996 -0.02057919 -0.5544338 ], action=0, reward=1.0, next_state=[ 0.06900902  0.22750293 -0.03166786 -0.26830498]\n",
      "[ episode 304 ][ timestamp 30 ] state=[ 0.06900902  0.22750293 -0.03166786 -0.26830498], action=0, reward=1.0, next_state=[ 0.07355908  0.03284689 -0.03703396  0.01422388]\n",
      "[ episode 304 ][ timestamp 31 ] state=[ 0.07355908  0.03284689 -0.03703396  0.01422388], action=0, reward=1.0, next_state=[ 0.07421601 -0.16172492 -0.03674949  0.29499597]\n",
      "[ episode 304 ][ timestamp 32 ] state=[ 0.07421601 -0.16172492 -0.03674949  0.29499597], action=1, reward=1.0, next_state=[ 0.07098152  0.03390116 -0.03084957 -0.00904693]\n",
      "[ episode 304 ][ timestamp 33 ] state=[ 0.07098152  0.03390116 -0.03084957 -0.00904693], action=1, reward=1.0, next_state=[ 0.07165954  0.22945164 -0.0310305  -0.31130142]\n",
      "[ episode 304 ][ timestamp 34 ] state=[ 0.07165954  0.22945164 -0.0310305  -0.31130142], action=0, reward=1.0, next_state=[ 0.07624857  0.03478521 -0.03725653 -0.02856384]\n",
      "[ episode 304 ][ timestamp 35 ] state=[ 0.07624857  0.03478521 -0.03725653 -0.02856384], action=1, reward=1.0, next_state=[ 0.07694428  0.23042109 -0.03782781 -0.33276491]\n",
      "[ episode 304 ][ timestamp 36 ] state=[ 0.07694428  0.23042109 -0.03782781 -0.33276491], action=0, reward=1.0, next_state=[ 0.0815527   0.0358574  -0.04448311 -0.05224689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 304 ][ timestamp 37 ] state=[ 0.0815527   0.0358574  -0.04448311 -0.05224689], action=1, reward=1.0, next_state=[ 0.08226985  0.23158802 -0.04552805 -0.35862612]\n",
      "[ episode 304 ][ timestamp 38 ] state=[ 0.08226985  0.23158802 -0.04552805 -0.35862612], action=0, reward=1.0, next_state=[ 0.08690161  0.03714187 -0.05270057 -0.08063974]\n",
      "[ episode 304 ][ timestamp 39 ] state=[ 0.08690161  0.03714187 -0.05270057 -0.08063974], action=1, reward=1.0, next_state=[ 0.08764444  0.23297813 -0.05431336 -0.38947309]\n",
      "[ episode 304 ][ timestamp 40 ] state=[ 0.08764444  0.23297813 -0.05431336 -0.38947309], action=0, reward=1.0, next_state=[ 0.09230401  0.03866745 -0.06210282 -0.11439715]\n",
      "[ episode 304 ][ timestamp 41 ] state=[ 0.09230401  0.03866745 -0.06210282 -0.11439715], action=1, reward=1.0, next_state=[ 0.09307736  0.23462174 -0.06439077 -0.42600845]\n",
      "[ episode 304 ][ timestamp 42 ] state=[ 0.09307736  0.23462174 -0.06439077 -0.42600845], action=1, reward=1.0, next_state=[ 0.09776979  0.43059383 -0.07291094 -0.7382752 ]\n",
      "[ episode 304 ][ timestamp 43 ] state=[ 0.09776979  0.43059383 -0.07291094 -0.7382752 ], action=1, reward=1.0, next_state=[ 0.10638167  0.62664287 -0.08767644 -1.05298429]\n",
      "[ episode 304 ][ timestamp 44 ] state=[ 0.10638167  0.62664287 -0.08767644 -1.05298429], action=1, reward=1.0, next_state=[ 0.11891452  0.82281102 -0.10873613 -1.37185011]\n",
      "[ episode 304 ][ timestamp 45 ] state=[ 0.11891452  0.82281102 -0.10873613 -1.37185011], action=0, reward=1.0, next_state=[ 0.13537074  0.62920382 -0.13617313 -1.11505993]\n",
      "[ episode 304 ][ timestamp 46 ] state=[ 0.13537074  0.62920382 -0.13617313 -1.11505993], action=0, reward=1.0, next_state=[ 0.14795482  0.43610645 -0.15847433 -0.86800649]\n",
      "[ episode 304 ][ timestamp 47 ] state=[ 0.14795482  0.43610645 -0.15847433 -0.86800649], action=0, reward=1.0, next_state=[ 0.15667695  0.24345446 -0.17583446 -0.6290463 ]\n",
      "[ episode 304 ][ timestamp 48 ] state=[ 0.15667695  0.24345446 -0.17583446 -0.6290463 ], action=1, reward=1.0, next_state=[ 0.16154604  0.44053766 -0.18841538 -0.9715422 ]\n",
      "[ episode 304 ][ timestamp 49 ] state=[ 0.16154604  0.44053766 -0.18841538 -0.9715422 ], action=0, reward=1.0, next_state=[ 0.17035679  0.24837516 -0.20784623 -0.74346666]\n",
      "[ episode 304 ][ timestamp 50 ] state=[ 0.17035679  0.24837516 -0.20784623 -0.74346666], action=1, reward=-1.0, next_state=[ 0.1753243   0.44566684 -0.22271556 -1.09370267]\n",
      "[ Ended! ] Episode 304: Exploration_rate=0.2189744609715989. Score=50.\n",
      "[ Experience replay ] starts\n",
      "[ episode 305 ] state=[-0.02092663 -0.04115671  0.03433351  0.03770665]\n",
      "[ episode 305 ][ timestamp 1 ] state=[-0.02092663 -0.04115671  0.03433351  0.03770665], action=0, reward=1.0, next_state=[-0.02174977 -0.23675376  0.03508765  0.34102138]\n",
      "[ episode 305 ][ timestamp 2 ] state=[-0.02174977 -0.23675376  0.03508765  0.34102138], action=1, reward=1.0, next_state=[-0.02648484 -0.04214815  0.04190807  0.05960628]\n",
      "[ episode 305 ][ timestamp 3 ] state=[-0.02648484 -0.04214815  0.04190807  0.05960628], action=0, reward=1.0, next_state=[-0.0273278  -0.23784514  0.0431002   0.3652114 ]\n",
      "[ episode 305 ][ timestamp 4 ] state=[-0.0273278  -0.23784514  0.0431002   0.3652114 ], action=1, reward=1.0, next_state=[-0.03208471 -0.04336135  0.05040443  0.08642417]\n",
      "[ episode 305 ][ timestamp 5 ] state=[-0.03208471 -0.04336135  0.05040443  0.08642417], action=0, reward=1.0, next_state=[-0.03295193 -0.23916822  0.05213291  0.39457407]\n",
      "[ episode 305 ][ timestamp 6 ] state=[-0.03295193 -0.23916822  0.05213291  0.39457407], action=1, reward=1.0, next_state=[-0.0377353  -0.04482327  0.06002439  0.11877285]\n",
      "[ episode 305 ][ timestamp 7 ] state=[-0.0377353  -0.04482327  0.06002439  0.11877285], action=0, reward=1.0, next_state=[-0.03863176 -0.24075158  0.06239985  0.42977261]\n",
      "[ episode 305 ][ timestamp 8 ] state=[-0.03863176 -0.24075158  0.06239985  0.42977261], action=1, reward=1.0, next_state=[-0.0434468  -0.04656623  0.0709953   0.15739513]\n",
      "[ episode 305 ][ timestamp 9 ] state=[-0.0434468  -0.04656623  0.0709953   0.15739513], action=0, reward=1.0, next_state=[-0.04437812 -0.24262904  0.0741432   0.47160358]\n",
      "[ episode 305 ][ timestamp 10 ] state=[-0.04437812 -0.24262904  0.0741432   0.47160358], action=1, reward=1.0, next_state=[-0.0492307  -0.04862833  0.08357528  0.20318014]\n",
      "[ episode 305 ][ timestamp 11 ] state=[-0.0492307  -0.04862833  0.08357528  0.20318014], action=0, reward=1.0, next_state=[-0.05020327 -0.24483989  0.08763888  0.52101274]\n",
      "[ episode 305 ][ timestamp 12 ] state=[-0.05020327 -0.24483989  0.08763888  0.52101274], action=1, reward=1.0, next_state=[-0.05510007 -0.05105383  0.09805913  0.25718208]\n",
      "[ episode 305 ][ timestamp 13 ] state=[-0.05510007 -0.05105383  0.09805913  0.25718208], action=0, reward=1.0, next_state=[-0.05612114 -0.24742906  0.10320277  0.57911306]\n",
      "[ episode 305 ][ timestamp 14 ] state=[-0.05612114 -0.24742906  0.10320277  0.57911306], action=1, reward=1.0, next_state=[-0.06106972 -0.05389328  0.11478504  0.3206418 ]\n",
      "[ episode 305 ][ timestamp 15 ] state=[-0.06106972 -0.05389328  0.11478504  0.3206418 ], action=0, reward=1.0, next_state=[-0.06214759 -0.25044685  0.12119787  0.64720474]\n",
      "[ episode 305 ][ timestamp 16 ] state=[-0.06214759 -0.25044685  0.12119787  0.64720474], action=1, reward=1.0, next_state=[-0.06715653 -0.05720336  0.13414197  0.39501081]\n",
      "[ episode 305 ][ timestamp 17 ] state=[-0.06715653 -0.05720336  0.13414197  0.39501081], action=0, reward=1.0, next_state=[-0.06830059 -0.25394826  0.14204218  0.72679652]\n",
      "[ episode 305 ][ timestamp 18 ] state=[-0.06830059 -0.25394826  0.14204218  0.72679652], action=1, reward=1.0, next_state=[-0.07337956 -0.06104602  0.15657811  0.48197738]\n",
      "[ episode 305 ][ timestamp 19 ] state=[-0.07337956 -0.06104602  0.15657811  0.48197738], action=1, reward=1.0, next_state=[-0.07460048  0.13155953  0.16621766  0.24244945]\n",
      "[ episode 305 ][ timestamp 20 ] state=[-0.07460048  0.13155953  0.16621766  0.24244945], action=1, reward=1.0, next_state=[-0.07196929  0.32396544  0.17106665  0.00646159]\n",
      "[ episode 305 ][ timestamp 21 ] state=[-0.07196929  0.32396544  0.17106665  0.00646159], action=1, reward=1.0, next_state=[-0.06548998  0.51627385  0.17119588 -0.2277419 ]\n",
      "[ episode 305 ][ timestamp 22 ] state=[-0.06548998  0.51627385  0.17119588 -0.2277419 ], action=0, reward=1.0, next_state=[-0.0551645   0.31917158  0.16664104  0.11367566]\n",
      "[ episode 305 ][ timestamp 23 ] state=[-0.0551645   0.31917158  0.16664104  0.11367566], action=1, reward=1.0, next_state=[-0.04878107  0.51156243  0.16891456 -0.12214694]\n",
      "[ episode 305 ][ timestamp 24 ] state=[-0.04878107  0.51156243  0.16891456 -0.12214694], action=1, reward=1.0, next_state=[-0.03854982  0.7039124   0.16647162 -0.35714049]\n",
      "[ episode 305 ][ timestamp 25 ] state=[-0.03854982  0.7039124   0.16647162 -0.35714049], action=0, reward=1.0, next_state=[-0.02447157  0.50686338  0.15932881 -0.01693618]\n",
      "[ episode 305 ][ timestamp 26 ] state=[-0.02447157  0.50686338  0.15932881 -0.01693618], action=0, reward=1.0, next_state=[-0.01433431  0.30985779  0.15899008  0.32147403]\n",
      "[ episode 305 ][ timestamp 27 ] state=[-0.01433431  0.30985779  0.15899008  0.32147403], action=1, reward=1.0, next_state=[-0.00813715  0.50240065  0.16541956  0.08284876]\n",
      "[ episode 305 ][ timestamp 28 ] state=[-0.00813715  0.50240065  0.16541956  0.08284876], action=0, reward=1.0, next_state=[0.00191086 0.30534145 0.16707654 0.42281444]\n",
      "[ episode 305 ][ timestamp 29 ] state=[0.00191086 0.30534145 0.16707654 0.42281444], action=1, reward=1.0, next_state=[0.00801769 0.49775127 0.17553283 0.18711092]\n",
      "[ episode 305 ][ timestamp 30 ] state=[0.00801769 0.49775127 0.17553283 0.18711092], action=1, reward=1.0, next_state=[ 0.01797272  0.68998422  0.17927505 -0.04546558]\n",
      "[ episode 305 ][ timestamp 31 ] state=[ 0.01797272  0.68998422  0.17927505 -0.04546558], action=0, reward=1.0, next_state=[0.0317724  0.49280505 0.17836574 0.29798794]\n",
      "[ episode 305 ][ timestamp 32 ] state=[0.0317724  0.49280505 0.17836574 0.29798794], action=1, reward=1.0, next_state=[0.0416285  0.68499559 0.18432549 0.06643772]\n",
      "[ episode 305 ][ timestamp 33 ] state=[0.0416285  0.68499559 0.18432549 0.06643772], action=1, reward=1.0, next_state=[ 0.05532841  0.8770622   0.18565425 -0.16289647]\n",
      "[ episode 305 ][ timestamp 34 ] state=[ 0.05532841  0.8770622   0.18565425 -0.16289647], action=0, reward=1.0, next_state=[0.07286966 0.67983495 0.18239632 0.18212992]\n",
      "[ episode 305 ][ timestamp 35 ] state=[0.07286966 0.67983495 0.18239632 0.18212992], action=1, reward=1.0, next_state=[ 0.08646636  0.8719421   0.18603892 -0.04792307]\n",
      "[ episode 305 ][ timestamp 36 ] state=[ 0.08646636  0.8719421   0.18603892 -0.04792307], action=0, reward=1.0, next_state=[0.1039052  0.67470738 0.18508046 0.29720444]\n",
      "[ episode 305 ][ timestamp 37 ] state=[0.1039052  0.67470738 0.18508046 0.29720444], action=1, reward=1.0, next_state=[0.11739935 0.86677513 0.19102455 0.0681267 ]\n",
      "[ episode 305 ][ timestamp 38 ] state=[0.11739935 0.86677513 0.19102455 0.0681267 ], action=0, reward=1.0, next_state=[0.13473485 0.66950098 0.19238708 0.41447565]\n",
      "[ episode 305 ][ timestamp 39 ] state=[0.13473485 0.66950098 0.19238708 0.41447565], action=1, reward=1.0, next_state=[0.14812487 0.86144998 0.20067659 0.18807769]\n",
      "[ episode 305 ][ timestamp 40 ] state=[0.14812487 0.86144998 0.20067659 0.18807769], action=1, reward=1.0, next_state=[ 0.16535387  1.05321993  0.20443815 -0.03520082]\n",
      "[ episode 305 ][ timestamp 41 ] state=[ 0.16535387  1.05321993  0.20443815 -0.03520082], action=1, reward=1.0, next_state=[ 0.18641827  1.24491309  0.20373413 -0.2570656 ]\n",
      "[ episode 305 ][ timestamp 42 ] state=[ 0.18641827  1.24491309  0.20373413 -0.2570656 ], action=0, reward=1.0, next_state=[0.21131653 1.04755368 0.19859282 0.09233513]\n",
      "[ episode 305 ][ timestamp 43 ] state=[0.21131653 1.04755368 0.19859282 0.09233513], action=0, reward=1.0, next_state=[0.2322676  0.85022156 0.20043952 0.44051876]\n",
      "[ episode 305 ][ timestamp 44 ] state=[0.2322676  0.85022156 0.20043952 0.44051876], action=1, reward=1.0, next_state=[0.24927203 1.04202665 0.2092499  0.2171067 ]\n",
      "[ episode 305 ][ timestamp 45 ] state=[0.24927203 1.04202665 0.2092499  0.2171067 ], action=1, reward=-1.0, next_state=[ 0.27011257  1.23363906  0.21359203 -0.00297094]\n",
      "[ Ended! ] Episode 305: Exploration_rate=0.2178795886667409. Score=45.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 306 ] state=[ 0.04060167 -0.04990466 -0.04858994  0.04503864]\n",
      "[ episode 306 ][ timestamp 1 ] state=[ 0.04060167 -0.04990466 -0.04858994  0.04503864], action=1, reward=1.0, next_state=[ 0.03960358  0.14587915 -0.04768917 -0.26257029]\n",
      "[ episode 306 ][ timestamp 2 ] state=[ 0.03960358  0.14587915 -0.04768917 -0.26257029], action=1, reward=1.0, next_state=[ 0.04252116  0.34164824 -0.05294057 -0.56990536]\n",
      "[ episode 306 ][ timestamp 3 ] state=[ 0.04252116  0.34164824 -0.05294057 -0.56990536], action=1, reward=1.0, next_state=[ 0.04935413  0.53747113 -0.06433868 -0.87878543]\n",
      "[ episode 306 ][ timestamp 4 ] state=[ 0.04935413  0.53747113 -0.06433868 -0.87878543], action=1, reward=1.0, next_state=[ 0.06010355  0.73340554 -0.08191439 -1.19098148]\n",
      "[ episode 306 ][ timestamp 5 ] state=[ 0.06010355  0.73340554 -0.08191439 -1.19098148], action=0, reward=1.0, next_state=[ 0.07477166  0.53943491 -0.10573402 -0.92505704]\n",
      "[ episode 306 ][ timestamp 6 ] state=[ 0.07477166  0.53943491 -0.10573402 -0.92505704], action=1, reward=1.0, next_state=[ 0.08556036  0.73581381 -0.12423516 -1.24900825]\n",
      "[ episode 306 ][ timestamp 7 ] state=[ 0.08556036  0.73581381 -0.12423516 -1.24900825], action=0, reward=1.0, next_state=[ 0.10027664  0.54248392 -0.14921532 -0.99767973]\n",
      "[ episode 306 ][ timestamp 8 ] state=[ 0.10027664  0.54248392 -0.14921532 -0.99767973], action=0, reward=1.0, next_state=[ 0.11112631  0.3496378  -0.16916892 -0.75533159]\n",
      "[ episode 306 ][ timestamp 9 ] state=[ 0.11112631  0.3496378  -0.16916892 -0.75533159], action=1, reward=1.0, next_state=[ 0.11811907  0.54663754 -0.18427555 -1.09611174]\n",
      "[ episode 306 ][ timestamp 10 ] state=[ 0.11811907  0.54663754 -0.18427555 -1.09611174], action=1, reward=1.0, next_state=[ 0.12905182  0.74364409 -0.20619778 -1.44048926]\n",
      "[ episode 306 ][ timestamp 11 ] state=[ 0.12905182  0.74364409 -0.20619778 -1.44048926], action=0, reward=-1.0, next_state=[ 0.1439247   0.55157085 -0.23500757 -1.21867608]\n",
      "[ Ended! ] Episode 306: Exploration_rate=0.2167901907234072. Score=11.\n",
      "[ Experience replay ] starts\n",
      "[ episode 307 ] state=[-0.04436083  0.02073563  0.00548693 -0.03211743]\n",
      "[ episode 307 ][ timestamp 1 ] state=[-0.04436083  0.02073563  0.00548693 -0.03211743], action=1, reward=1.0, next_state=[-0.04394612  0.21577847  0.00484458 -0.32306413]\n",
      "[ episode 307 ][ timestamp 2 ] state=[-0.04394612  0.21577847  0.00484458 -0.32306413], action=1, reward=1.0, next_state=[-0.03963055  0.4108311  -0.0016167  -0.61421535]\n",
      "[ episode 307 ][ timestamp 3 ] state=[-0.03963055  0.4108311  -0.0016167  -0.61421535], action=0, reward=1.0, next_state=[-0.03141393  0.21573178 -0.01390101 -0.32204206]\n",
      "[ episode 307 ][ timestamp 4 ] state=[-0.03141393  0.21573178 -0.01390101 -0.32204206], action=1, reward=1.0, next_state=[-0.02709929  0.41104889 -0.02034185 -0.61907619]\n",
      "[ episode 307 ][ timestamp 5 ] state=[-0.02709929  0.41104889 -0.02034185 -0.61907619], action=0, reward=1.0, next_state=[-0.01887831  0.21621689 -0.03272337 -0.33286873]\n",
      "[ episode 307 ][ timestamp 6 ] state=[-0.01887831  0.21621689 -0.03272337 -0.33286873], action=1, reward=1.0, next_state=[-0.01455398  0.41178895 -0.03938075 -0.63568873]\n",
      "[ episode 307 ][ timestamp 7 ] state=[-0.01455398  0.41178895 -0.03938075 -0.63568873], action=0, reward=1.0, next_state=[-0.0063182   0.21723774 -0.05209452 -0.35566312]\n",
      "[ episode 307 ][ timestamp 8 ] state=[-0.0063182   0.21723774 -0.05209452 -0.35566312], action=1, reward=1.0, next_state=[-0.00197344  0.41306017 -0.05920778 -0.66430715]\n",
      "[ episode 307 ][ timestamp 9 ] state=[-0.00197344  0.41306017 -0.05920778 -0.66430715], action=0, reward=1.0, next_state=[ 0.00628776  0.21880966 -0.07249393 -0.39083886]\n",
      "[ episode 307 ][ timestamp 10 ] state=[ 0.00628776  0.21880966 -0.07249393 -0.39083886], action=1, reward=1.0, next_state=[ 0.01066395  0.41488158 -0.0803107  -0.70546881]\n",
      "[ episode 307 ][ timestamp 11 ] state=[ 0.01066395  0.41488158 -0.0803107  -0.70546881], action=1, reward=1.0, next_state=[ 0.01896159  0.61101904 -0.09442008 -1.0223127 ]\n",
      "[ episode 307 ][ timestamp 12 ] state=[ 0.01896159  0.61101904 -0.09442008 -1.0223127 ], action=0, reward=1.0, next_state=[ 0.03118197  0.41727305 -0.11486633 -0.7607065 ]\n",
      "[ episode 307 ][ timestamp 13 ] state=[ 0.03118197  0.41727305 -0.11486633 -0.7607065 ], action=1, reward=1.0, next_state=[ 0.03952743  0.61377432 -0.13008046 -1.0872125 ]\n",
      "[ episode 307 ][ timestamp 14 ] state=[ 0.03952743  0.61377432 -0.13008046 -1.0872125 ], action=0, reward=1.0, next_state=[ 0.05180291  0.4205852  -0.15182471 -0.83801296]\n",
      "[ episode 307 ][ timestamp 15 ] state=[ 0.05180291  0.4205852  -0.15182471 -0.83801296], action=1, reward=1.0, next_state=[ 0.06021462  0.61741817 -0.16858497 -1.17433127]\n",
      "[ episode 307 ][ timestamp 16 ] state=[ 0.06021462  0.61741817 -0.16858497 -1.17433127], action=1, reward=1.0, next_state=[ 0.07256298  0.81428105 -0.1920716  -1.5147688 ]\n",
      "[ episode 307 ][ timestamp 17 ] state=[ 0.07256298  0.81428105 -0.1920716  -1.5147688 ], action=0, reward=-1.0, next_state=[ 0.0888486   0.62193198 -0.22236698 -1.28767337]\n",
      "[ Ended! ] Episode 307: Exploration_rate=0.21570623976979014. Score=17.\n",
      "[ Experience replay ] starts\n",
      "[ episode 308 ] state=[ 0.00198691  0.02381976 -0.0159085   0.03802183]\n",
      "[ episode 308 ][ timestamp 1 ] state=[ 0.00198691  0.02381976 -0.0159085   0.03802183], action=1, reward=1.0, next_state=[ 0.00246331  0.21916618 -0.01514806 -0.25963763]\n",
      "[ episode 308 ][ timestamp 2 ] state=[ 0.00246331  0.21916618 -0.01514806 -0.25963763], action=0, reward=1.0, next_state=[ 0.00684663  0.02426372 -0.02034082  0.02822916]\n",
      "[ episode 308 ][ timestamp 3 ] state=[ 0.00684663  0.02426372 -0.02034082  0.02822916], action=1, reward=1.0, next_state=[ 0.00733191  0.21967138 -0.01977623 -0.27080148]\n",
      "[ episode 308 ][ timestamp 4 ] state=[ 0.00733191  0.21967138 -0.01977623 -0.27080148], action=0, reward=1.0, next_state=[ 0.01172533  0.02483713 -0.02519226  0.01557891]\n",
      "[ episode 308 ][ timestamp 5 ] state=[ 0.01172533  0.02483713 -0.02519226  0.01557891], action=1, reward=1.0, next_state=[ 0.01222208  0.22031114 -0.02488069 -0.28494481]\n",
      "[ episode 308 ][ timestamp 6 ] state=[ 0.01222208  0.22031114 -0.02488069 -0.28494481], action=0, reward=1.0, next_state=[ 0.0166283   0.02555272 -0.03057958 -0.00021176]\n",
      "[ episode 308 ][ timestamp 7 ] state=[ 0.0166283   0.02555272 -0.03057958 -0.00021176], action=0, reward=1.0, next_state=[ 0.01713935 -0.16911763 -0.03058382  0.28266825]\n",
      "[ episode 308 ][ timestamp 8 ] state=[ 0.01713935 -0.16911763 -0.03058382  0.28266825], action=1, reward=1.0, next_state=[ 0.013757    0.0264269  -0.02493045 -0.01950162]\n",
      "[ episode 308 ][ timestamp 9 ] state=[ 0.013757    0.0264269  -0.02493045 -0.01950162], action=1, reward=1.0, next_state=[ 0.01428554  0.22189734 -0.02532048 -0.31994497]\n",
      "[ episode 308 ][ timestamp 10 ] state=[ 0.01428554  0.22189734 -0.02532048 -0.31994497], action=0, reward=1.0, next_state=[ 0.01872348  0.02714497 -0.03171938 -0.03535348]\n",
      "[ episode 308 ][ timestamp 11 ] state=[ 0.01872348  0.02714497 -0.03171938 -0.03535348], action=1, reward=1.0, next_state=[ 0.01926638  0.22270709 -0.03242645 -0.33787304]\n",
      "[ episode 308 ][ timestamp 12 ] state=[ 0.01926638  0.22270709 -0.03242645 -0.33787304], action=0, reward=1.0, next_state=[ 0.02372053  0.02806122 -0.03918391 -0.05558942]\n",
      "[ episode 308 ][ timestamp 13 ] state=[ 0.02372053  0.02806122 -0.03918391 -0.05558942], action=1, reward=1.0, next_state=[ 0.02428175  0.22372246 -0.0402957  -0.36037312]\n",
      "[ episode 308 ][ timestamp 14 ] state=[ 0.02428175  0.22372246 -0.0402957  -0.36037312], action=0, reward=1.0, next_state=[ 0.0287562   0.02919578 -0.04750317 -0.08066369]\n",
      "[ episode 308 ][ timestamp 15 ] state=[ 0.0287562   0.02919578 -0.04750317 -0.08066369], action=1, reward=1.0, next_state=[ 0.02934012  0.22496537 -0.04911644 -0.3879475 ]\n",
      "[ episode 308 ][ timestamp 16 ] state=[ 0.02934012  0.22496537 -0.04911644 -0.3879475 ], action=0, reward=1.0, next_state=[ 0.03383942  0.03057376 -0.05687539 -0.11114616]\n",
      "[ episode 308 ][ timestamp 17 ] state=[ 0.03383942  0.03057376 -0.05687539 -0.11114616], action=1, reward=1.0, next_state=[ 0.0344509   0.22646263 -0.05909831 -0.4212167 ]\n",
      "[ episode 308 ][ timestamp 18 ] state=[ 0.0344509   0.22646263 -0.05909831 -0.4212167 ], action=1, reward=1.0, next_state=[ 0.03898015  0.42236994 -0.06752265 -0.73192942]\n",
      "[ episode 308 ][ timestamp 19 ] state=[ 0.03898015  0.42236994 -0.06752265 -0.73192942], action=0, reward=1.0, next_state=[ 0.04742755  0.22824282 -0.08216123 -0.46123888]\n",
      "[ episode 308 ][ timestamp 20 ] state=[ 0.04742755  0.22824282 -0.08216123 -0.46123888], action=1, reward=1.0, next_state=[ 0.05199241  0.42442407 -0.09138601 -0.77864632]\n",
      "[ episode 308 ][ timestamp 21 ] state=[ 0.05199241  0.42442407 -0.09138601 -0.77864632], action=0, reward=1.0, next_state=[ 0.06048089  0.23066953 -0.10695894 -0.51605737]\n",
      "[ episode 308 ][ timestamp 22 ] state=[ 0.06048089  0.23066953 -0.10695894 -0.51605737], action=1, reward=1.0, next_state=[ 0.06509428  0.4271222  -0.11728009 -0.84043838]\n",
      "[ episode 308 ][ timestamp 23 ] state=[ 0.06509428  0.4271222  -0.11728009 -0.84043838], action=1, reward=1.0, next_state=[ 0.07363672  0.62363323 -0.13408885 -1.16758141]\n",
      "[ episode 308 ][ timestamp 24 ] state=[ 0.07363672  0.62363323 -0.13408885 -1.16758141], action=1, reward=1.0, next_state=[ 0.08610939  0.82022036 -0.15744048 -1.49911922]\n",
      "[ episode 308 ][ timestamp 25 ] state=[ 0.08610939  0.82022036 -0.15744048 -1.49911922], action=0, reward=1.0, next_state=[ 0.10251379  0.62732242 -0.18742287 -1.25944751]\n",
      "[ episode 308 ][ timestamp 26 ] state=[ 0.10251379  0.62732242 -0.18742287 -1.25944751], action=1, reward=-1.0, next_state=[ 0.11506024  0.8242811  -0.21261182 -1.60449201]\n",
      "[ Ended! ] Episode 308: Exploration_rate=0.21462770857094118. Score=26.\n",
      "[ Experience replay ] starts\n",
      "[ episode 309 ] state=[-0.04423704  0.01798281  0.00018785 -0.00372284]\n",
      "[ episode 309 ][ timestamp 1 ] state=[-0.04423704  0.01798281  0.00018785 -0.00372284], action=0, reward=1.0, next_state=[-4.38773794e-02 -1.77141830e-01  1.13393862e-04  2.89019346e-01]\n",
      "[ episode 309 ][ timestamp 2 ] state=[-4.38773794e-02 -1.77141830e-01  1.13393862e-04  2.89019346e-01], action=0, reward=1.0, next_state=[-0.04742022 -0.3722654   0.00589378  0.58173803]\n",
      "[ episode 309 ][ timestamp 3 ] state=[-0.04742022 -0.3722654   0.00589378  0.58173803], action=1, reward=1.0, next_state=[-0.05486552 -0.17722652  0.01752854  0.29091756]\n",
      "[ episode 309 ][ timestamp 4 ] state=[-0.05486552 -0.17722652  0.01752854  0.29091756], action=0, reward=1.0, next_state=[-0.05841005 -0.37259397  0.02334689  0.58907684]\n",
      "[ episode 309 ][ timestamp 5 ] state=[-0.05841005 -0.37259397  0.02334689  0.58907684], action=1, reward=1.0, next_state=[-0.06586193 -0.17780659  0.03512843  0.30383876]\n",
      "[ episode 309 ][ timestamp 6 ] state=[-0.06586193 -0.17780659  0.03512843  0.30383876], action=0, reward=1.0, next_state=[-0.06941807 -0.3734111   0.0412052   0.60739015]\n",
      "[ episode 309 ][ timestamp 7 ] state=[-0.06941807 -0.3734111   0.0412052   0.60739015], action=1, reward=1.0, next_state=[-0.07688629 -0.17888875  0.05335301  0.32796519]\n",
      "[ episode 309 ][ timestamp 8 ] state=[-0.07688629 -0.17888875  0.05335301  0.32796519], action=0, reward=1.0, next_state=[-0.08046406 -0.37472806  0.05991231  0.6369845 ]\n",
      "[ episode 309 ][ timestamp 9 ] state=[-0.08046406 -0.37472806  0.05991231  0.6369845 ], action=1, reward=1.0, next_state=[-0.08795862 -0.18049054  0.072652    0.36375466]\n",
      "[ episode 309 ][ timestamp 10 ] state=[-0.08795862 -0.18049054  0.072652    0.36375466], action=0, reward=1.0, next_state=[-0.09156843 -0.3765658   0.07992709  0.67843258]\n",
      "[ episode 309 ][ timestamp 11 ] state=[-0.09156843 -0.3765658   0.07992709  0.67843258], action=1, reward=1.0, next_state=[-0.09909975 -0.18263984  0.09349575  0.41194586]\n",
      "[ episode 309 ][ timestamp 12 ] state=[-0.09909975 -0.18263984  0.09349575  0.41194586], action=1, reward=1.0, next_state=[-0.10275255  0.01104098  0.10173466  0.15014121]\n",
      "[ episode 309 ][ timestamp 13 ] state=[-0.10275255  0.01104098  0.10173466  0.15014121], action=1, reward=1.0, next_state=[-0.10253173  0.20457013  0.10473749 -0.10879312]\n",
      "[ episode 309 ][ timestamp 14 ] state=[-0.10253173  0.20457013  0.10473749 -0.10879312], action=0, reward=1.0, next_state=[-0.09844032  0.00811537  0.10256163  0.21501072]\n",
      "[ episode 309 ][ timestamp 15 ] state=[-0.09844032  0.00811537  0.10256163  0.21501072], action=0, reward=1.0, next_state=[-0.09827802 -0.1883119   0.10686184  0.53820362]\n",
      "[ episode 309 ][ timestamp 16 ] state=[-0.09827802 -0.1883119   0.10686184  0.53820362], action=1, reward=1.0, next_state=[-0.10204426  0.00515824  0.11762591  0.28101145]\n",
      "[ episode 309 ][ timestamp 17 ] state=[-0.10204426  0.00515824  0.11762591  0.28101145], action=0, reward=1.0, next_state=[-0.10194109 -0.19142798  0.12324614  0.6083555 ]\n",
      "[ episode 309 ][ timestamp 18 ] state=[-0.10194109 -0.19142798  0.12324614  0.6083555 ], action=1, reward=1.0, next_state=[-0.10576965  0.00177502  0.13541325  0.35689192]\n",
      "[ episode 309 ][ timestamp 19 ] state=[-0.10576965  0.00177502  0.13541325  0.35689192], action=0, reward=1.0, next_state=[-0.10573415 -0.19498613  0.14255109  0.68902176]\n",
      "[ episode 309 ][ timestamp 20 ] state=[-0.10573415 -0.19498613  0.14255109  0.68902176], action=1, reward=1.0, next_state=[-0.10963387 -0.00210005  0.15633152  0.44439559]\n",
      "[ episode 309 ][ timestamp 21 ] state=[-0.10963387 -0.00210005  0.15633152  0.44439559], action=1, reward=1.0, next_state=[-0.10967587  0.1905046   0.16521944  0.20478629]\n",
      "[ episode 309 ][ timestamp 22 ] state=[-0.10967587  0.1905046   0.16521944  0.20478629], action=0, reward=1.0, next_state=[-0.10586578 -0.00654721  0.16931516  0.54469273]\n",
      "[ episode 309 ][ timestamp 23 ] state=[-0.10586578 -0.00654721  0.16931516  0.54469273], action=1, reward=1.0, next_state=[-0.10599673  0.18584171  0.18020902  0.30977713]\n",
      "[ episode 309 ][ timestamp 24 ] state=[-0.10599673  0.18584171  0.18020902  0.30977713], action=0, reward=1.0, next_state=[-0.10227989 -0.01132885  0.18640456  0.65343874]\n",
      "[ episode 309 ][ timestamp 25 ] state=[-0.10227989 -0.01132885  0.18640456  0.65343874], action=1, reward=1.0, next_state=[-0.10250647  0.1807757   0.19947333  0.4247598 ]\n",
      "[ episode 309 ][ timestamp 26 ] state=[-0.10250647  0.1807757   0.19947333  0.4247598 ], action=1, reward=1.0, next_state=[-0.09889095  0.37259597  0.20796853  0.2009918 ]\n",
      "[ episode 309 ][ timestamp 27 ] state=[-0.09889095  0.37259597  0.20796853  0.2009918 ], action=1, reward=-1.0, next_state=[-0.09143904  0.56423097  0.21198837 -0.01956383]\n",
      "[ Ended! ] Episode 309: Exploration_rate=0.21355457002808648. Score=27.\n",
      "[ Experience replay ] starts\n",
      "[ episode 310 ] state=[ 0.03724037 -0.02649702  0.02218003  0.03375365]\n",
      "[ episode 310 ][ timestamp 1 ] state=[ 0.03724037 -0.02649702  0.02218003  0.03375365], action=1, reward=1.0, next_state=[ 0.03671043  0.16829996  0.0228551  -0.25184955]\n",
      "[ episode 310 ][ timestamp 2 ] state=[ 0.03671043  0.16829996  0.0228551  -0.25184955], action=0, reward=1.0, next_state=[ 0.04007643 -0.02714077  0.01781811  0.0479538 ]\n",
      "[ episode 310 ][ timestamp 3 ] state=[ 0.04007643 -0.02714077  0.01781811  0.0479538 ], action=1, reward=1.0, next_state=[ 0.03953361  0.16772121  0.01877718 -0.23905453]\n",
      "[ episode 310 ][ timestamp 4 ] state=[ 0.03953361  0.16772121  0.01877718 -0.23905453], action=0, reward=1.0, next_state=[ 0.04288804 -0.02766389  0.01399609  0.05949161]\n",
      "[ episode 310 ][ timestamp 5 ] state=[ 0.04288804 -0.02766389  0.01399609  0.05949161], action=1, reward=1.0, next_state=[ 0.04233476  0.16725462  0.01518593 -0.2287428 ]\n",
      "[ episode 310 ][ timestamp 6 ] state=[ 0.04233476  0.16725462  0.01518593 -0.2287428 ], action=0, reward=1.0, next_state=[ 0.04567985 -0.02808101  0.01061107  0.06869136]\n",
      "[ episode 310 ][ timestamp 7 ] state=[ 0.04567985 -0.02808101  0.01061107  0.06869136], action=1, reward=1.0, next_state=[ 0.04511823  0.16688721  0.0119849  -0.22062492]\n",
      "[ episode 310 ][ timestamp 8 ] state=[ 0.04511823  0.16688721  0.0119849  -0.22062492], action=0, reward=1.0, next_state=[ 0.04845598 -0.02840398  0.0075724   0.07581431]\n",
      "[ episode 310 ][ timestamp 9 ] state=[ 0.04845598 -0.02840398  0.0075724   0.07581431], action=0, reward=1.0, next_state=[ 0.0478879  -0.22363366  0.00908868  0.3708767 ]\n",
      "[ episode 310 ][ timestamp 10 ] state=[ 0.0478879  -0.22363366  0.00908868  0.3708767 ], action=1, reward=1.0, next_state=[ 0.04341522 -0.02864201  0.01650622  0.08107334]\n",
      "[ episode 310 ][ timestamp 11 ] state=[ 0.04341522 -0.02864201  0.01650622  0.08107334], action=1, reward=1.0, next_state=[ 0.04284238  0.16623948  0.01812769 -0.20635647]\n",
      "[ episode 310 ][ timestamp 12 ] state=[ 0.04284238  0.16623948  0.01812769 -0.20635647], action=0, reward=1.0, next_state=[ 0.04616717 -0.02913694  0.01400056  0.09198926]\n",
      "[ episode 310 ][ timestamp 13 ] state=[ 0.04616717 -0.02913694  0.01400056  0.09198926], action=1, reward=1.0, next_state=[ 0.04558443  0.16578157  0.01584034 -0.19624382]\n",
      "[ episode 310 ][ timestamp 14 ] state=[ 0.04558443  0.16578157  0.01584034 -0.19624382], action=0, reward=1.0, next_state=[ 0.04890006 -0.02956334  0.01191547  0.10139364]\n",
      "[ episode 310 ][ timestamp 15 ] state=[ 0.04890006 -0.02956334  0.01191547  0.10139364], action=0, reward=1.0, next_state=[ 0.0483088  -0.22485401  0.01394334  0.39781192]\n",
      "[ episode 310 ][ timestamp 16 ] state=[ 0.0483088  -0.22485401  0.01394334  0.39781192], action=1, reward=1.0, next_state=[ 0.04381172 -0.02993262  0.02189958  0.10955747]\n",
      "[ episode 310 ][ timestamp 17 ] state=[ 0.04381172 -0.02993262  0.02189958  0.10955747], action=1, reward=1.0, next_state=[ 0.04321307  0.16486878  0.02409073 -0.17613661]\n",
      "[ episode 310 ][ timestamp 18 ] state=[ 0.04321307  0.16486878  0.02409073 -0.17613661], action=1, reward=1.0, next_state=[ 0.04651044  0.35963783  0.02056799 -0.46112342]\n",
      "[ episode 310 ][ timestamp 19 ] state=[ 0.04651044  0.35963783  0.02056799 -0.46112342], action=0, reward=1.0, next_state=[ 0.0537032   0.1642313   0.01134553 -0.16202906]\n",
      "[ episode 310 ][ timestamp 20 ] state=[ 0.0537032   0.1642313   0.01134553 -0.16202906], action=1, reward=1.0, next_state=[ 0.05698782  0.35918901  0.00810494 -0.45111129]\n",
      "[ episode 310 ][ timestamp 21 ] state=[ 0.05698782  0.35918901  0.00810494 -0.45111129], action=0, reward=1.0, next_state=[ 0.0641716   0.16395337 -0.00091728 -0.15588463]\n",
      "[ episode 310 ][ timestamp 22 ] state=[ 0.0641716   0.16395337 -0.00091728 -0.15588463], action=1, reward=1.0, next_state=[ 0.06745067  0.35908845 -0.00403497 -0.44885679]\n",
      "[ episode 310 ][ timestamp 23 ] state=[ 0.06745067  0.35908845 -0.00403497 -0.44885679], action=1, reward=1.0, next_state=[ 0.07463244  0.55426724 -0.01301211 -0.74280888]\n",
      "[ episode 310 ][ timestamp 24 ] state=[ 0.07463244  0.55426724 -0.01301211 -0.74280888], action=0, reward=1.0, next_state=[ 0.08571778  0.35932729 -0.02786829 -0.45424916]\n",
      "[ episode 310 ][ timestamp 25 ] state=[ 0.08571778  0.35932729 -0.02786829 -0.45424916], action=0, reward=1.0, next_state=[ 0.09290433  0.16461026 -0.03695327 -0.17047924]\n",
      "[ episode 310 ][ timestamp 26 ] state=[ 0.09290433  0.16461026 -0.03695327 -0.17047924], action=1, reward=1.0, next_state=[ 0.09619654  0.3602411  -0.04036286 -0.47458696]\n",
      "[ episode 310 ][ timestamp 27 ] state=[ 0.09619654  0.3602411  -0.04036286 -0.47458696], action=0, reward=1.0, next_state=[ 0.10340136  0.1657117  -0.0498546  -0.19489397]\n",
      "[ episode 310 ][ timestamp 28 ] state=[ 0.10340136  0.1657117  -0.0498546  -0.19489397], action=1, reward=1.0, next_state=[ 0.10671559  0.36151003 -0.05375247 -0.50287773]\n",
      "[ episode 310 ][ timestamp 29 ] state=[ 0.10671559  0.36151003 -0.05375247 -0.50287773], action=0, reward=1.0, next_state=[ 0.11394579  0.16718527 -0.06381003 -0.22760721]\n",
      "[ episode 310 ][ timestamp 30 ] state=[ 0.11394579  0.16718527 -0.06381003 -0.22760721], action=1, reward=1.0, next_state=[ 0.1172895   0.36315834 -0.06836217 -0.53971597]\n",
      "[ episode 310 ][ timestamp 31 ] state=[ 0.1172895   0.36315834 -0.06836217 -0.53971597], action=0, reward=1.0, next_state=[ 0.12455266  0.1690606  -0.07915649 -0.26933226]\n",
      "[ episode 310 ][ timestamp 32 ] state=[ 0.12455266  0.1690606  -0.07915649 -0.26933226], action=1, reward=1.0, next_state=[ 0.12793388  0.3652177  -0.08454314 -0.58589429]\n",
      "[ episode 310 ][ timestamp 33 ] state=[ 0.12793388  0.3652177  -0.08454314 -0.58589429], action=0, reward=1.0, next_state=[ 0.13523823  0.17137531 -0.09626102 -0.32099529]\n",
      "[ episode 310 ][ timestamp 34 ] state=[ 0.13523823  0.17137531 -0.09626102 -0.32099529], action=1, reward=1.0, next_state=[ 0.13866574  0.36772689 -0.10268093 -0.6424162 ]\n",
      "[ episode 310 ][ timestamp 35 ] state=[ 0.13866574  0.36772689 -0.10268093 -0.6424162 ], action=0, reward=1.0, next_state=[ 0.14602027  0.17417473 -0.11552925 -0.38375231]\n",
      "[ episode 310 ][ timestamp 36 ] state=[ 0.14602027  0.17417473 -0.11552925 -0.38375231], action=1, reward=1.0, next_state=[ 0.14950377  0.37073117 -0.1232043  -0.71051167]\n",
      "[ episode 310 ][ timestamp 37 ] state=[ 0.14950377  0.37073117 -0.1232043  -0.71051167], action=0, reward=1.0, next_state=[ 0.15691839  0.17751132 -0.13741453 -0.45900931]\n",
      "[ episode 310 ][ timestamp 38 ] state=[ 0.15691839  0.17751132 -0.13741453 -0.45900931], action=0, reward=1.0, next_state=[ 0.16046862 -0.01542788 -0.14659472 -0.21260149]\n",
      "[ episode 310 ][ timestamp 39 ] state=[ 0.16046862 -0.01542788 -0.14659472 -0.21260149], action=1, reward=1.0, next_state=[ 0.16016006  0.18145265 -0.15084675 -0.54769939]\n",
      "[ episode 310 ][ timestamp 40 ] state=[ 0.16016006  0.18145265 -0.15084675 -0.54769939], action=1, reward=1.0, next_state=[ 0.16378911  0.37833599 -0.16180074 -0.88385168]\n",
      "[ episode 310 ][ timestamp 41 ] state=[ 0.16378911  0.37833599 -0.16180074 -0.88385168], action=1, reward=1.0, next_state=[ 0.17135583  0.57524143 -0.17947777 -1.22271424]\n",
      "[ episode 310 ][ timestamp 42 ] state=[ 0.17135583  0.57524143 -0.17947777 -1.22271424], action=1, reward=1.0, next_state=[ 0.18286066  0.77216288 -0.20393206 -1.56583533]\n",
      "[ episode 310 ][ timestamp 43 ] state=[ 0.18286066  0.77216288 -0.20393206 -1.56583533], action=1, reward=-1.0, next_state=[ 0.19830392  0.96905377 -0.23524876 -1.91459297]\n",
      "[ Ended! ] Episode 310: Exploration_rate=0.21248679717794605. Score=43.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 311 ] state=[ 0.02062566 -0.02638916  0.01126309 -0.00673929]\n",
      "[ episode 311 ][ timestamp 1 ] state=[ 0.02062566 -0.02638916  0.01126309 -0.00673929], action=0, reward=1.0, next_state=[ 0.02009788 -0.22167082  0.0111283   0.28947589]\n",
      "[ episode 311 ][ timestamp 2 ] state=[ 0.02009788 -0.22167082  0.0111283   0.28947589], action=1, reward=1.0, next_state=[ 0.01566446 -0.0267093   0.01691782  0.00032338]\n",
      "[ episode 311 ][ timestamp 3 ] state=[ 0.01566446 -0.0267093   0.01691782  0.00032338], action=0, reward=1.0, next_state=[ 0.01513027 -0.22206975  0.01692429  0.29829571]\n",
      "[ episode 311 ][ timestamp 4 ] state=[ 0.01513027 -0.22206975  0.01692429  0.29829571], action=1, reward=1.0, next_state=[ 0.01068888 -0.02719308  0.0228902   0.01099808]\n",
      "[ episode 311 ][ timestamp 5 ] state=[ 0.01068888 -0.02719308  0.0228902   0.01099808], action=0, reward=1.0, next_state=[ 0.01014502 -0.2226357   0.02311016  0.31081434]\n",
      "[ episode 311 ][ timestamp 6 ] state=[ 0.01014502 -0.2226357   0.02311016  0.31081434], action=1, reward=1.0, next_state=[ 0.0056923  -0.0278505   0.02932645  0.02550834]\n",
      "[ episode 311 ][ timestamp 7 ] state=[ 0.0056923  -0.0278505   0.02932645  0.02550834], action=0, reward=1.0, next_state=[ 0.00513529 -0.22338047  0.02983662  0.32729793]\n",
      "[ episode 311 ][ timestamp 8 ] state=[ 0.00513529 -0.22338047  0.02983662  0.32729793], action=1, reward=1.0, next_state=[ 0.00066768 -0.02869573  0.03638257  0.04417145]\n",
      "[ episode 311 ][ timestamp 9 ] state=[ 0.00066768 -0.02869573  0.03638257  0.04417145], action=0, reward=1.0, next_state=[ 9.37702978e-05 -2.24319992e-01  3.72660039e-02  3.48107774e-01]\n",
      "[ episode 311 ][ timestamp 10 ] state=[ 9.37702978e-05 -2.24319992e-01  3.72660039e-02  3.48107774e-01], action=1, reward=1.0, next_state=[-0.00439263 -0.02974735  0.04422816  0.06740512]\n",
      "[ episode 311 ][ timestamp 11 ] state=[-0.00439263 -0.02974735  0.04422816  0.06740512], action=0, reward=1.0, next_state=[-0.00498758 -0.22547458  0.04557626  0.3737077 ]\n",
      "[ episode 311 ][ timestamp 12 ] state=[-0.00498758 -0.22547458  0.04557626  0.3737077 ], action=1, reward=1.0, next_state=[-0.00949707 -0.03102868  0.05305042  0.09573651]\n",
      "[ episode 311 ][ timestamp 13 ] state=[-0.00949707 -0.03102868  0.05305042  0.09573651], action=1, reward=1.0, next_state=[-0.01011764  0.16329437  0.05496515 -0.17974849]\n",
      "[ episode 311 ][ timestamp 14 ] state=[-0.01011764  0.16329437  0.05496515 -0.17974849], action=0, reward=1.0, next_state=[-0.00685175 -0.03256929  0.05137018  0.12975492]\n",
      "[ episode 311 ][ timestamp 15 ] state=[-0.00685175 -0.03256929  0.05137018  0.12975492], action=0, reward=1.0, next_state=[-0.00750314 -0.22838805  0.05396527  0.43819178]\n",
      "[ episode 311 ][ timestamp 16 ] state=[-0.00750314 -0.22838805  0.05396527  0.43819178], action=1, reward=1.0, next_state=[-0.0120709  -0.0340698   0.06272911  0.16299682]\n",
      "[ episode 311 ][ timestamp 17 ] state=[-0.0120709  -0.0340698   0.06272911  0.16299682], action=0, reward=1.0, next_state=[-0.0127523  -0.23003105  0.06598905  0.47479083]\n",
      "[ episode 311 ][ timestamp 18 ] state=[-0.0127523  -0.23003105  0.06598905  0.47479083], action=1, reward=1.0, next_state=[-0.01735292 -0.03589999  0.07548486  0.20361473]\n",
      "[ episode 311 ][ timestamp 19 ] state=[-0.01735292 -0.03589999  0.07548486  0.20361473], action=0, reward=1.0, next_state=[-0.01807092 -0.23201574  0.07955716  0.51912214]\n",
      "[ episode 311 ][ timestamp 20 ] state=[-0.01807092 -0.23201574  0.07955716  0.51912214], action=1, reward=1.0, next_state=[-0.02271123 -0.03809867  0.0899396   0.25253171]\n",
      "[ episode 311 ][ timestamp 21 ] state=[-0.02271123 -0.03809867  0.0899396   0.25253171], action=0, reward=1.0, next_state=[-0.02347321 -0.23438209  0.09499023  0.57217343]\n",
      "[ episode 311 ][ timestamp 22 ] state=[-0.02347321 -0.23438209  0.09499023  0.57217343], action=1, reward=1.0, next_state=[-0.02816085 -0.04071144  0.1064337   0.31086226]\n",
      "[ episode 311 ][ timestamp 23 ] state=[-0.02816085 -0.04071144  0.1064337   0.31086226], action=1, reward=1.0, next_state=[-0.02897508  0.15274586  0.11265095  0.05355085]\n",
      "[ episode 311 ][ timestamp 24 ] state=[-0.02897508  0.15274586  0.11265095  0.05355085], action=0, reward=1.0, next_state=[-0.02592016 -0.04379599  0.11372197  0.37954437]\n",
      "[ episode 311 ][ timestamp 25 ] state=[-0.02592016 -0.04379599  0.11372197  0.37954437], action=1, reward=1.0, next_state=[-0.02679608  0.14954282  0.12131285  0.12477165]\n",
      "[ episode 311 ][ timestamp 26 ] state=[-0.02679608  0.14954282  0.12131285  0.12477165], action=0, reward=1.0, next_state=[-0.02380522 -0.04708937  0.12380829  0.45313081]\n",
      "[ episode 311 ][ timestamp 27 ] state=[-0.02380522 -0.04708937  0.12380829  0.45313081], action=1, reward=1.0, next_state=[-0.02474701  0.14608426  0.1328709   0.20189505]\n",
      "[ episode 311 ][ timestamp 28 ] state=[-0.02474701  0.14608426  0.1328709   0.20189505], action=0, reward=1.0, next_state=[-0.02182533 -0.05066297  0.1369088   0.53336379]\n",
      "[ episode 311 ][ timestamp 29 ] state=[-0.02182533 -0.05066297  0.1369088   0.53336379], action=1, reward=1.0, next_state=[-0.02283858  0.14229483  0.14757608  0.28676101]\n",
      "[ episode 311 ][ timestamp 30 ] state=[-0.02283858  0.14229483  0.14757608  0.28676101], action=1, reward=1.0, next_state=[-0.01999269  0.3350378   0.1533113   0.04401918]\n",
      "[ episode 311 ][ timestamp 31 ] state=[-0.01999269  0.3350378   0.1533113   0.04401918], action=0, reward=1.0, next_state=[-0.01329193  0.13808779  0.15419168  0.38087626]\n",
      "[ episode 311 ][ timestamp 32 ] state=[-0.01329193  0.13808779  0.15419168  0.38087626], action=1, reward=1.0, next_state=[-0.01053018  0.33072256  0.16180921  0.14050517]\n",
      "[ episode 311 ][ timestamp 33 ] state=[-0.01053018  0.33072256  0.16180921  0.14050517], action=1, reward=1.0, next_state=[-0.00391572  0.52320179  0.16461931 -0.09707767]\n",
      "[ episode 311 ][ timestamp 34 ] state=[-0.00391572  0.52320179  0.16461931 -0.09707767], action=0, reward=1.0, next_state=[0.00654831 0.3261501  0.16267776 0.24268367]\n",
      "[ episode 311 ][ timestamp 35 ] state=[0.00654831 0.3261501  0.16267776 0.24268367], action=1, reward=1.0, next_state=[0.01307131 0.51861984 0.16753143 0.00540738]\n",
      "[ episode 311 ][ timestamp 36 ] state=[0.01307131 0.51861984 0.16753143 0.00540738], action=0, reward=1.0, next_state=[0.02344371 0.32154092 0.16763958 0.3459111 ]\n",
      "[ episode 311 ][ timestamp 37 ] state=[0.02344371 0.32154092 0.16763958 0.3459111 ], action=1, reward=1.0, next_state=[0.02987453 0.51393102 0.1745578  0.11042703]\n",
      "[ episode 311 ][ timestamp 38 ] state=[0.02987453 0.51393102 0.1745578  0.11042703], action=0, reward=1.0, next_state=[0.04015315 0.31679334 0.17676634 0.45269958]\n",
      "[ episode 311 ][ timestamp 39 ] state=[0.04015315 0.31679334 0.17676634 0.45269958], action=1, reward=1.0, next_state=[0.04648902 0.50903288 0.18582033 0.22053273]\n",
      "[ episode 311 ][ timestamp 40 ] state=[0.04648902 0.50903288 0.18582033 0.22053273], action=1, reward=1.0, next_state=[ 0.05666967  0.70107993  0.19023099 -0.0082614 ]\n",
      "[ episode 311 ][ timestamp 41 ] state=[ 0.05666967  0.70107993  0.19023099 -0.0082614 ], action=0, reward=1.0, next_state=[0.07069127 0.50381122 0.19006576 0.33789495]\n",
      "[ episode 311 ][ timestamp 42 ] state=[0.07069127 0.50381122 0.19006576 0.33789495], action=1, reward=1.0, next_state=[0.0807675  0.69579219 0.19682366 0.11065285]\n",
      "[ episode 311 ][ timestamp 43 ] state=[0.0807675  0.69579219 0.19682366 0.11065285], action=1, reward=1.0, next_state=[ 0.09468334  0.88762928  0.19903672 -0.11405374]\n",
      "[ episode 311 ][ timestamp 44 ] state=[ 0.09468334  0.88762928  0.19903672 -0.11405374], action=0, reward=1.0, next_state=[0.11243593 0.69029465 0.19675564 0.23423558]\n",
      "[ episode 311 ][ timestamp 45 ] state=[0.11243593 0.69029465 0.19675564 0.23423558], action=1, reward=1.0, next_state=[0.12624182 0.88214111 0.20144035 0.00949177]\n",
      "[ episode 311 ][ timestamp 46 ] state=[0.12624182 0.88214111 0.20144035 0.00949177], action=1, reward=1.0, next_state=[ 0.14388464  1.07388998  0.20163019 -0.21349187]\n",
      "[ episode 311 ][ timestamp 47 ] state=[ 0.14388464  1.07388998  0.20163019 -0.21349187], action=1, reward=1.0, next_state=[ 0.16536244  1.26564418  0.19736035 -0.43641773]\n",
      "[ episode 311 ][ timestamp 48 ] state=[ 0.16536244  1.26564418  0.19736035 -0.43641773], action=0, reward=1.0, next_state=[ 0.19067532  1.06835618  0.188632   -0.08858249]\n",
      "[ episode 311 ][ timestamp 49 ] state=[ 0.19067532  1.06835618  0.188632   -0.08858249], action=0, reward=1.0, next_state=[0.21204245 0.87110162 0.18686035 0.2571804 ]\n",
      "[ episode 311 ][ timestamp 50 ] state=[0.21204245 0.87110162 0.18686035 0.2571804 ], action=1, reward=1.0, next_state=[0.22946448 1.06313272 0.19200395 0.02876576]\n",
      "[ episode 311 ][ timestamp 51 ] state=[0.22946448 1.06313272 0.19200395 0.02876576], action=1, reward=1.0, next_state=[ 0.25072713  1.25505701  0.19257927 -0.19772745]\n",
      "[ episode 311 ][ timestamp 52 ] state=[ 0.25072713  1.25505701  0.19257927 -0.19772745], action=1, reward=1.0, next_state=[ 0.27582827  1.44697777  0.18862472 -0.42401779]\n",
      "[ episode 311 ][ timestamp 53 ] state=[ 0.27582827  1.44697777  0.18862472 -0.42401779], action=0, reward=1.0, next_state=[ 0.30476783  1.24975465  0.18014436 -0.0783029 ]\n",
      "[ episode 311 ][ timestamp 54 ] state=[ 0.30476783  1.24975465  0.18014436 -0.0783029 ], action=0, reward=1.0, next_state=[0.32976292 1.05256895 0.17857831 0.26536576]\n",
      "[ episode 311 ][ timestamp 55 ] state=[0.32976292 1.05256895 0.17857831 0.26536576], action=1, reward=1.0, next_state=[0.3508143  1.24475243 0.18388562 0.03389833]\n",
      "[ episode 311 ][ timestamp 56 ] state=[0.3508143  1.24475243 0.18388562 0.03389833], action=1, reward=1.0, next_state=[ 0.37570935  1.43682655  0.18456359 -0.19559727]\n",
      "[ episode 311 ][ timestamp 57 ] state=[ 0.37570935  1.43682655  0.18456359 -0.19559727], action=0, reward=1.0, next_state=[0.40444588 1.23961024 0.18065164 0.14915721]\n",
      "[ episode 311 ][ timestamp 58 ] state=[0.40444588 1.23961024 0.18065164 0.14915721], action=1, reward=1.0, next_state=[ 0.42923809  1.43174744  0.18363479 -0.0815354 ]\n",
      "[ episode 311 ][ timestamp 59 ] state=[ 0.42923809  1.43174744  0.18363479 -0.0815354 ], action=1, reward=1.0, next_state=[ 0.45787304  1.62382715  0.18200408 -0.31112495]\n",
      "[ episode 311 ][ timestamp 60 ] state=[ 0.45787304  1.62382715  0.18200408 -0.31112495], action=0, reward=1.0, next_state=[0.49034958 1.42664204 0.17578158 0.03298161]\n",
      "[ episode 311 ][ timestamp 61 ] state=[0.49034958 1.42664204 0.17578158 0.03298161], action=1, reward=1.0, next_state=[ 0.51888242  1.61886468  0.17644121 -0.19949512]\n",
      "[ episode 311 ][ timestamp 62 ] state=[ 0.51888242  1.61886468  0.17644121 -0.19949512], action=0, reward=1.0, next_state=[0.55125971 1.42171575 0.17245131 0.14324203]\n",
      "[ episode 311 ][ timestamp 63 ] state=[0.55125971 1.42171575 0.17245131 0.14324203], action=1, reward=1.0, next_state=[ 0.57969403  1.61400239  0.17531615 -0.09045991]\n",
      "[ episode 311 ][ timestamp 64 ] state=[ 0.57969403  1.61400239  0.17531615 -0.09045991], action=0, reward=1.0, next_state=[0.61197408 1.41685759 0.17350695 0.25200369]\n",
      "[ episode 311 ][ timestamp 65 ] state=[0.61197408 1.41685759 0.17350695 0.25200369], action=1, reward=1.0, next_state=[0.64031123 1.60913222 0.17854703 0.01867761]\n",
      "[ episode 311 ][ timestamp 66 ] state=[0.64031123 1.60913222 0.17854703 0.01867761], action=0, reward=1.0, next_state=[0.67249387 1.41195884 0.17892058 0.36195028]\n",
      "[ episode 311 ][ timestamp 67 ] state=[0.67249387 1.41195884 0.17892058 0.36195028], action=1, reward=1.0, next_state=[0.70073305 1.60414658 0.18615958 0.13059315]\n",
      "[ episode 311 ][ timestamp 68 ] state=[0.70073305 1.60414658 0.18615958 0.13059315], action=1, reward=1.0, next_state=[ 0.73281598  1.79618171  0.18877145 -0.09806731]\n",
      "[ episode 311 ][ timestamp 69 ] state=[ 0.73281598  1.79618171  0.18877145 -0.09806731], action=0, reward=1.0, next_state=[0.76873961 1.59892636 0.1868101  0.24772928]\n",
      "[ episode 311 ][ timestamp 70 ] state=[0.76873961 1.59892636 0.1868101  0.24772928], action=1, reward=1.0, next_state=[0.80071814 1.79095753 0.19176469 0.01929734]\n",
      "[ episode 311 ][ timestamp 71 ] state=[0.80071814 1.79095753 0.19176469 0.01929734], action=1, reward=1.0, next_state=[ 0.83653729  1.98288615  0.19215063 -0.20728445]\n",
      "[ episode 311 ][ timestamp 72 ] state=[ 0.83653729  1.98288615  0.19215063 -0.20728445], action=0, reward=1.0, next_state=[0.87619501 1.78561025 0.18800494 0.13932864]\n",
      "[ episode 311 ][ timestamp 73 ] state=[0.87619501 1.78561025 0.18800494 0.13932864], action=1, reward=1.0, next_state=[ 0.91190722  1.97761182  0.19079152 -0.08865042]\n",
      "[ episode 311 ][ timestamp 74 ] state=[ 0.91190722  1.97761182  0.19079152 -0.08865042], action=1, reward=1.0, next_state=[ 0.95145946  2.16955983  0.18901851 -0.31559492]\n",
      "[ episode 311 ][ timestamp 75 ] state=[ 0.95145946  2.16955983  0.18901851 -0.31559492], action=0, reward=1.0, next_state=[0.99485065 1.97231897 0.18270661 0.03023795]\n",
      "[ episode 311 ][ timestamp 76 ] state=[0.99485065 1.97231897 0.18270661 0.03023795], action=1, reward=1.0, next_state=[ 1.03429703  2.16441472  0.18331137 -0.19969229]\n",
      "[ episode 311 ][ timestamp 77 ] state=[ 1.03429703  2.16441472  0.18331137 -0.19969229], action=0, reward=1.0, next_state=[1.07758533 1.96720888 0.17931752 0.14475255]\n",
      "[ episode 311 ][ timestamp 78 ] state=[1.07758533 1.96720888 0.17931752 0.14475255], action=0, reward=1.0, next_state=[1.1169295  1.77003263 0.18221258 0.4882118 ]\n",
      "[ episode 311 ][ timestamp 79 ] state=[1.1169295  1.77003263 0.18221258 0.4882118 ], action=1, reward=1.0, next_state=[1.15233016 1.96217933 0.19197681 0.25803774]\n",
      "[ episode 311 ][ timestamp 80 ] state=[1.15233016 1.96217933 0.19197681 0.25803774], action=0, reward=1.0, next_state=[1.19157374 1.76490917 0.19713757 0.60460203]\n",
      "[ episode 311 ][ timestamp 81 ] state=[1.19157374 1.76490917 0.19713757 0.60460203], action=1, reward=1.0, next_state=[1.22687193 1.95680776 0.20922961 0.37991317]\n",
      "[ episode 311 ][ timestamp 82 ] state=[1.22687193 1.95680776 0.20922961 0.37991317], action=1, reward=-1.0, next_state=[1.26600808 2.14844018 0.21682787 0.15979912]\n",
      "[ Ended! ] Episode 311: Exploration_rate=0.21142436319205632. Score=82.\n",
      "[ Experience replay ] starts\n",
      "[ episode 312 ] state=[-0.01924703  0.00376477  0.00718515 -0.02135947]\n",
      "[ episode 312 ][ timestamp 1 ] state=[-0.01924703  0.00376477  0.00718515 -0.02135947], action=1, reward=1.0, next_state=[-0.01917174  0.19878295  0.00675796 -0.31176677]\n",
      "[ episode 312 ][ timestamp 2 ] state=[-0.01917174  0.19878295  0.00675796 -0.31176677], action=1, reward=1.0, next_state=[-1.51960787e-02  3.93807972e-01  5.22623636e-04 -6.02310803e-01]\n",
      "[ episode 312 ][ timestamp 3 ] state=[-1.51960787e-02  3.93807972e-01  5.22623636e-04 -6.02310803e-01], action=1, reward=1.0, next_state=[-0.00731992  0.58892261 -0.01152359 -0.89482907]\n",
      "[ episode 312 ][ timestamp 4 ] state=[-0.00731992  0.58892261 -0.01152359 -0.89482907], action=1, reward=1.0, next_state=[ 0.00445853  0.78419891 -0.02942017 -1.19111193]\n",
      "[ episode 312 ][ timestamp 5 ] state=[ 0.00445853  0.78419891 -0.02942017 -1.19111193], action=1, reward=1.0, next_state=[ 0.02014251  0.97968945 -0.05324241 -1.49286914]\n",
      "[ episode 312 ][ timestamp 6 ] state=[ 0.02014251  0.97968945 -0.05324241 -1.49286914], action=0, reward=1.0, next_state=[ 0.0397363   0.78525422 -0.0830998  -1.21727544]\n",
      "[ episode 312 ][ timestamp 7 ] state=[ 0.0397363   0.78525422 -0.0830998  -1.21727544], action=0, reward=1.0, next_state=[ 0.05544138  0.59129635 -0.1074453  -0.95174583]\n",
      "[ episode 312 ][ timestamp 8 ] state=[ 0.05544138  0.59129635 -0.1074453  -0.95174583], action=1, reward=1.0, next_state=[ 0.06726731  0.78768736 -0.12648022 -1.27616173]\n",
      "[ episode 312 ][ timestamp 9 ] state=[ 0.06726731  0.78768736 -0.12648022 -1.27616173], action=0, reward=1.0, next_state=[ 0.08302106  0.59438467 -0.15200346 -1.02560997]\n",
      "[ episode 312 ][ timestamp 10 ] state=[ 0.08302106  0.59438467 -0.15200346 -1.02560997], action=0, reward=1.0, next_state=[ 0.09490875  0.40157732 -0.17251565 -0.78425075]\n",
      "[ episode 312 ][ timestamp 11 ] state=[ 0.09490875  0.40157732 -0.17251565 -0.78425075], action=1, reward=1.0, next_state=[ 0.1029403   0.59859679 -0.18820067 -1.12586153]\n",
      "[ episode 312 ][ timestamp 12 ] state=[ 0.1029403   0.59859679 -0.18820067 -1.12586153], action=1, reward=-1.0, next_state=[ 0.11491223  0.79561855 -0.2107179  -1.47118073]\n",
      "[ Ended! ] Episode 312: Exploration_rate=0.21036724137609603. Score=12.\n",
      "[ Experience replay ] starts\n",
      "[ episode 313 ] state=[-0.02286923 -0.0171292  -0.00720667  0.03386126]\n",
      "[ episode 313 ][ timestamp 1 ] state=[-0.02286923 -0.0171292  -0.00720667  0.03386126], action=1, reward=1.0, next_state=[-0.02321182  0.17809535 -0.00652945 -0.2610867 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 313 ][ timestamp 2 ] state=[-0.02321182  0.17809535 -0.00652945 -0.2610867 ], action=1, reward=1.0, next_state=[-0.01964991  0.3733099  -0.01175118 -0.55582193]\n",
      "[ episode 313 ][ timestamp 3 ] state=[-0.01964991  0.3733099  -0.01175118 -0.55582193], action=0, reward=1.0, next_state=[-0.01218371  0.17835489 -0.02286762 -0.26686437]\n",
      "[ episode 313 ][ timestamp 4 ] state=[-0.01218371  0.17835489 -0.02286762 -0.26686437], action=1, reward=1.0, next_state=[-0.00861661  0.37379561 -0.02820491 -0.56667131]\n",
      "[ episode 313 ][ timestamp 5 ] state=[-0.00861661  0.37379561 -0.02820491 -0.56667131], action=1, reward=1.0, next_state=[-0.0011407   0.56930164 -0.03953833 -0.86810485]\n",
      "[ episode 313 ][ timestamp 6 ] state=[-0.0011407   0.56930164 -0.03953833 -0.86810485], action=0, reward=1.0, next_state=[ 0.01024533  0.37473932 -0.05690043 -0.58811069]\n",
      "[ episode 313 ][ timestamp 7 ] state=[ 0.01024533  0.37473932 -0.05690043 -0.58811069], action=1, reward=1.0, next_state=[ 0.01774012  0.57061    -0.06866264 -0.89816092]\n",
      "[ episode 313 ][ timestamp 8 ] state=[ 0.01774012  0.57061    -0.06866264 -0.89816092], action=0, reward=1.0, next_state=[ 0.02915232  0.37648255 -0.08662586 -0.62782685]\n",
      "[ episode 313 ][ timestamp 9 ] state=[ 0.02915232  0.37648255 -0.08662586 -0.62782685], action=1, reward=1.0, next_state=[ 0.03668197  0.57269989 -0.0991824  -0.94648541]\n",
      "[ episode 313 ][ timestamp 10 ] state=[ 0.03668197  0.57269989 -0.0991824  -0.94648541], action=0, reward=1.0, next_state=[ 0.04813597  0.37904345 -0.11811211 -0.68654018]\n",
      "[ episode 313 ][ timestamp 11 ] state=[ 0.04813597  0.37904345 -0.11811211 -0.68654018], action=1, reward=1.0, next_state=[ 0.05571684  0.57558971 -0.13184291 -1.0139498 ]\n",
      "[ episode 313 ][ timestamp 12 ] state=[ 0.05571684  0.57558971 -0.13184291 -1.0139498 ], action=0, reward=1.0, next_state=[ 0.06722863  0.38244912 -0.15212191 -0.76540284]\n",
      "[ episode 313 ][ timestamp 13 ] state=[ 0.06722863  0.38244912 -0.15212191 -0.76540284], action=1, reward=1.0, next_state=[ 0.07487761  0.57930185 -0.16742996 -1.10182353]\n",
      "[ episode 313 ][ timestamp 14 ] state=[ 0.07487761  0.57930185 -0.16742996 -1.10182353], action=0, reward=1.0, next_state=[ 0.08646365  0.3867304  -0.18946643 -0.8660004 ]\n",
      "[ episode 313 ][ timestamp 15 ] state=[ 0.08646365  0.3867304  -0.18946643 -0.8660004 ], action=1, reward=1.0, next_state=[ 0.09419826  0.58385561 -0.20678644 -1.2117673 ]\n",
      "[ episode 313 ][ timestamp 16 ] state=[ 0.09419826  0.58385561 -0.20678644 -1.2117673 ], action=1, reward=-1.0, next_state=[ 0.10587537  0.78095727 -0.23102179 -1.56148401]\n",
      "[ Ended! ] Episode 313: Exploration_rate=0.20931540516921554. Score=16.\n",
      "[ Experience replay ] starts\n",
      "[ episode 314 ] state=[ 0.00424191 -0.00838576 -0.03558675 -0.00115524]\n",
      "[ episode 314 ][ timestamp 1 ] state=[ 0.00424191 -0.00838576 -0.03558675 -0.00115524], action=1, reward=1.0, next_state=[ 0.0040742   0.18722801 -0.03560985 -0.30485041]\n",
      "[ episode 314 ][ timestamp 2 ] state=[ 0.0040742   0.18722801 -0.03560985 -0.30485041], action=0, reward=1.0, next_state=[ 0.00781876 -0.00736886 -0.04170686 -0.02360725]\n",
      "[ episode 314 ][ timestamp 3 ] state=[ 0.00781876 -0.00736886 -0.04170686 -0.02360725], action=1, reward=1.0, next_state=[ 0.00767138  0.18832562 -0.04217901 -0.32915197]\n",
      "[ episode 314 ][ timestamp 4 ] state=[ 0.00767138  0.18832562 -0.04217901 -0.32915197], action=0, reward=1.0, next_state=[ 0.01143789 -0.00617129 -0.04876205 -0.05006303]\n",
      "[ episode 314 ][ timestamp 5 ] state=[ 0.01143789 -0.00617129 -0.04876205 -0.05006303], action=1, reward=1.0, next_state=[ 0.01131447  0.18961472 -0.04976331 -0.35772332]\n",
      "[ episode 314 ][ timestamp 6 ] state=[ 0.01131447  0.18961472 -0.04976331 -0.35772332], action=0, reward=1.0, next_state=[ 0.01510676 -0.00476575 -0.05691777 -0.08113794]\n",
      "[ episode 314 ][ timestamp 7 ] state=[ 0.01510676 -0.00476575 -0.05691777 -0.08113794], action=1, reward=1.0, next_state=[ 0.01501145  0.19112397 -0.05854053 -0.39122149]\n",
      "[ episode 314 ][ timestamp 8 ] state=[ 0.01501145  0.19112397 -0.05854053 -0.39122149], action=0, reward=1.0, next_state=[ 0.01883393 -0.00312042 -0.06636496 -0.1175551 ]\n",
      "[ episode 314 ][ timestamp 9 ] state=[ 0.01883393 -0.00312042 -0.06636496 -0.1175551 ], action=1, reward=1.0, next_state=[ 0.01877152  0.19288654 -0.06871606 -0.4304153 ]\n",
      "[ episode 314 ][ timestamp 10 ] state=[ 0.01877152  0.19288654 -0.06871606 -0.4304153 ], action=0, reward=1.0, next_state=[ 0.02262925 -0.00119848 -0.07732437 -0.16016146]\n",
      "[ episode 314 ][ timestamp 11 ] state=[ 0.02262925 -0.00119848 -0.07732437 -0.16016146], action=0, reward=1.0, next_state=[ 0.02260528 -0.19513316 -0.0805276   0.10716061]\n",
      "[ episode 314 ][ timestamp 12 ] state=[ 0.02260528 -0.19513316 -0.0805276   0.10716061], action=1, reward=1.0, next_state=[ 0.01870262  0.0010449  -0.07838439 -0.20980242]\n",
      "[ episode 314 ][ timestamp 13 ] state=[ 0.01870262  0.0010449  -0.07838439 -0.20980242], action=1, reward=1.0, next_state=[ 0.01872351  0.19719504 -0.08258043 -0.52614562]\n",
      "[ episode 314 ][ timestamp 14 ] state=[ 0.01872351  0.19719504 -0.08258043 -0.52614562], action=0, reward=1.0, next_state=[ 0.02266742  0.00332628 -0.09310335 -0.26058455]\n",
      "[ episode 314 ][ timestamp 15 ] state=[ 0.02266742  0.00332628 -0.09310335 -0.26058455], action=1, reward=1.0, next_state=[ 0.02273394  0.19964545 -0.09831504 -0.58112078]\n",
      "[ episode 314 ][ timestamp 16 ] state=[ 0.02273394  0.19964545 -0.09831504 -0.58112078], action=1, reward=1.0, next_state=[ 0.02672685  0.39599755 -0.10993745 -0.90308472]\n",
      "[ episode 314 ][ timestamp 17 ] state=[ 0.02672685  0.39599755 -0.10993745 -0.90308472], action=1, reward=1.0, next_state=[ 0.0346468   0.59242318 -0.12799915 -1.22820097]\n",
      "[ episode 314 ][ timestamp 18 ] state=[ 0.0346468   0.59242318 -0.12799915 -1.22820097], action=0, reward=1.0, next_state=[ 0.04649526  0.3991595  -0.15256317 -0.97820608]\n",
      "[ episode 314 ][ timestamp 19 ] state=[ 0.04649526  0.3991595  -0.15256317 -0.97820608], action=0, reward=1.0, next_state=[ 0.05447845  0.20637565 -0.17212729 -0.73706892]\n",
      "[ episode 314 ][ timestamp 20 ] state=[ 0.05447845  0.20637565 -0.17212729 -0.73706892], action=0, reward=1.0, next_state=[ 0.05860597  0.01399596 -0.18686867 -0.50311958]\n",
      "[ episode 314 ][ timestamp 21 ] state=[ 0.05860597  0.01399596 -0.18686867 -0.50311958], action=1, reward=1.0, next_state=[ 0.05888589  0.21119203 -0.19693106 -0.84838434]\n",
      "[ episode 314 ][ timestamp 22 ] state=[ 0.05888589  0.21119203 -0.19693106 -0.84838434], action=1, reward=-1.0, next_state=[ 0.06310973  0.40837605 -0.21389875 -1.19596775]\n",
      "[ Ended! ] Episode 314: Exploration_rate=0.20826882814336947. Score=22.\n",
      "[ Experience replay ] starts\n",
      "[ episode 315 ] state=[-0.02220633 -0.01104927 -0.02139373 -0.0118338 ]\n",
      "[ episode 315 ][ timestamp 1 ] state=[-0.02220633 -0.01104927 -0.02139373 -0.0118338 ], action=1, reward=1.0, next_state=[-0.02242732  0.18437286 -0.02163041 -0.31118919]\n",
      "[ episode 315 ][ timestamp 2 ] state=[-0.02242732  0.18437286 -0.02163041 -0.31118919], action=0, reward=1.0, next_state=[-0.01873986 -0.01043435 -0.02785419 -0.02540558]\n",
      "[ episode 315 ][ timestamp 3 ] state=[-0.01873986 -0.01043435 -0.02785419 -0.02540558], action=0, reward=1.0, next_state=[-0.01894855 -0.20514601 -0.0283623   0.25836054]\n",
      "[ episode 315 ][ timestamp 4 ] state=[-0.01894855 -0.20514601 -0.0283623   0.25836054], action=1, reward=1.0, next_state=[-0.02305147 -0.00963087 -0.02319509 -0.04313162]\n",
      "[ episode 315 ][ timestamp 5 ] state=[-0.02305147 -0.00963087 -0.02319509 -0.04313162], action=0, reward=1.0, next_state=[-0.02324408 -0.20441266 -0.02405773  0.24214373]\n",
      "[ episode 315 ][ timestamp 6 ] state=[-0.02324408 -0.20441266 -0.02405773  0.24214373], action=1, reward=1.0, next_state=[-0.02733234 -0.00895547 -0.01921485 -0.0580295 ]\n",
      "[ episode 315 ][ timestamp 7 ] state=[-0.02733234 -0.00895547 -0.01921485 -0.0580295 ], action=0, reward=1.0, next_state=[-0.02751145 -0.20379672 -0.02037544  0.22852961]\n",
      "[ episode 315 ][ timestamp 8 ] state=[-0.02751145 -0.20379672 -0.02037544  0.22852961], action=1, reward=1.0, next_state=[-0.03158738 -0.00838961 -0.01580485 -0.07051018]\n",
      "[ episode 315 ][ timestamp 9 ] state=[-0.03158738 -0.00838961 -0.01580485 -0.07051018], action=0, reward=1.0, next_state=[-0.03175517 -0.20328145 -0.01721505  0.21714463]\n",
      "[ episode 315 ][ timestamp 10 ] state=[-0.03175517 -0.20328145 -0.01721505  0.21714463], action=1, reward=1.0, next_state=[-0.0358208  -0.00791769 -0.01287216 -0.08091857]\n",
      "[ episode 315 ][ timestamp 11 ] state=[-0.0358208  -0.00791769 -0.01287216 -0.08091857], action=0, reward=1.0, next_state=[-0.03597916 -0.20285277 -0.01449053  0.20767553]\n",
      "[ episode 315 ][ timestamp 12 ] state=[-0.03597916 -0.20285277 -0.01449053  0.20767553], action=1, reward=1.0, next_state=[-0.04003621 -0.00752665 -0.01033702 -0.08954297]\n",
      "[ episode 315 ][ timestamp 13 ] state=[-0.04003621 -0.00752665 -0.01033702 -0.08954297], action=0, reward=1.0, next_state=[-0.04018675 -0.20249892 -0.01212788  0.19986078]\n",
      "[ episode 315 ][ timestamp 14 ] state=[-0.04018675 -0.20249892 -0.01212788  0.19986078], action=1, reward=1.0, next_state=[-0.04423672 -0.00720563 -0.00813066 -0.09662312]\n",
      "[ episode 315 ][ timestamp 15 ] state=[-0.04423672 -0.00720563 -0.00813066 -0.09662312], action=0, reward=1.0, next_state=[-0.04438084 -0.20221011 -0.01006313  0.19348354]\n",
      "[ episode 315 ][ timestamp 16 ] state=[-0.04438084 -0.20221011 -0.01006313  0.19348354], action=1, reward=1.0, next_state=[-0.04842504 -0.00694566 -0.00619346 -0.10235681]\n",
      "[ episode 315 ][ timestamp 17 ] state=[-0.04842504 -0.00694566 -0.00619346 -0.10235681], action=0, reward=1.0, next_state=[-0.04856395 -0.20197831 -0.00824059  0.18836569]\n",
      "[ episode 315 ][ timestamp 18 ] state=[-0.04856395 -0.20197831 -0.00824059  0.18836569], action=1, reward=1.0, next_state=[-0.05260352 -0.00673943 -0.00447328 -0.10690539]\n",
      "[ episode 315 ][ timestamp 19 ] state=[-0.05260352 -0.00673943 -0.00447328 -0.10690539], action=0, reward=1.0, next_state=[-0.05273831 -0.201797   -0.00661139  0.18436289]\n",
      "[ episode 315 ][ timestamp 20 ] state=[-0.05273831 -0.201797   -0.00661139  0.18436289], action=1, reward=1.0, next_state=[-0.05677425 -0.00658108 -0.00292413 -0.11039833]\n",
      "[ episode 315 ][ timestamp 21 ] state=[-0.05677425 -0.00658108 -0.00292413 -0.11039833], action=0, reward=1.0, next_state=[-0.05690587 -0.201661   -0.00513209  0.18136062]\n",
      "[ episode 315 ][ timestamp 22 ] state=[-0.05690587 -0.201661   -0.00513209  0.18136062], action=1, reward=1.0, next_state=[-0.06093909 -0.00646599 -0.00150488 -0.11293687]\n",
      "[ episode 315 ][ timestamp 23 ] state=[-0.06093909 -0.00646599 -0.00150488 -0.11293687], action=0, reward=1.0, next_state=[-0.06106841 -0.20156635 -0.00376362  0.1792709 ]\n",
      "[ episode 315 ][ timestamp 24 ] state=[-0.06106841 -0.20156635 -0.00376362  0.1792709 ], action=1, reward=1.0, next_state=[-0.06509973 -0.00639074 -0.0001782  -0.11459694]\n",
      "[ episode 315 ][ timestamp 25 ] state=[-0.06509973 -0.00639074 -0.0001782  -0.11459694], action=0, reward=1.0, next_state=[-0.06522755 -0.20151014 -0.00247014  0.17802976]\n",
      "[ episode 315 ][ timestamp 26 ] state=[-0.06522755 -0.20151014 -0.00247014  0.17802976], action=1, reward=1.0, next_state=[-0.06925775 -0.00635293  0.00109045 -0.11543139]\n",
      "[ episode 315 ][ timestamp 27 ] state=[-0.06925775 -0.00635293  0.00109045 -0.11543139], action=0, reward=1.0, next_state=[-0.06938481 -0.20149048 -0.00121817  0.17759537]\n",
      "[ episode 315 ][ timestamp 28 ] state=[-0.06938481 -0.20149048 -0.00121817  0.17759537], action=0, reward=1.0, next_state=[-0.07341462 -0.39659498  0.00233373  0.46989376]\n",
      "[ episode 315 ][ timestamp 29 ] state=[-0.07341462 -0.39659498  0.00233373  0.46989376], action=0, reward=1.0, next_state=[-0.08134652 -0.59174982  0.01173161  0.76331134]\n",
      "[ episode 315 ][ timestamp 30 ] state=[-0.08134652 -0.59174982  0.01173161  0.76331134], action=1, reward=1.0, next_state=[-0.09318152 -0.3967914   0.02699784  0.47434284]\n",
      "[ episode 315 ][ timestamp 31 ] state=[-0.09318152 -0.3967914   0.02699784  0.47434284], action=1, reward=1.0, next_state=[-0.10111734 -0.2020609   0.03648469  0.19028995]\n",
      "[ episode 315 ][ timestamp 32 ] state=[-0.10111734 -0.2020609   0.03648469  0.19028995], action=1, reward=1.0, next_state=[-0.10515856 -0.00747939  0.04029049 -0.09066397]\n",
      "[ episode 315 ][ timestamp 33 ] state=[-0.10515856 -0.00747939  0.04029049 -0.09066397], action=0, reward=1.0, next_state=[-0.10530815 -0.20315499  0.03847721  0.21445343]\n",
      "[ episode 315 ][ timestamp 34 ] state=[-0.10530815 -0.20315499  0.03847721  0.21445343], action=1, reward=1.0, next_state=[-0.10937125 -0.00860365  0.04276628 -0.06584807]\n",
      "[ episode 315 ][ timestamp 35 ] state=[-0.10937125 -0.00860365  0.04276628 -0.06584807], action=0, reward=1.0, next_state=[-0.10954332 -0.20431183  0.04144932  0.24001524]\n",
      "[ episode 315 ][ timestamp 36 ] state=[-0.10954332 -0.20431183  0.04144932  0.24001524], action=1, reward=1.0, next_state=[-0.11362956 -0.00980575  0.04624962 -0.03931068]\n",
      "[ episode 315 ][ timestamp 37 ] state=[-0.11362956 -0.00980575  0.04624962 -0.03931068], action=0, reward=1.0, next_state=[-0.11382567 -0.20555936  0.04546341  0.26759829]\n",
      "[ episode 315 ][ timestamp 38 ] state=[-0.11382567 -0.20555936  0.04546341  0.26759829], action=1, reward=1.0, next_state=[-0.11793686 -0.01111473  0.05081538 -0.01040564]\n",
      "[ episode 315 ][ timestamp 39 ] state=[-0.11793686 -0.01111473  0.05081538 -0.01040564], action=0, reward=1.0, next_state=[-0.11815916 -0.20692722  0.05060726  0.29786726]\n",
      "[ episode 315 ][ timestamp 40 ] state=[-0.11815916 -0.20692722  0.05060726  0.29786726], action=1, reward=1.0, next_state=[-0.1222977  -0.01256183  0.05656461  0.02156461]\n",
      "[ episode 315 ][ timestamp 41 ] state=[-0.1222977  -0.01256183  0.05656461  0.02156461], action=0, reward=1.0, next_state=[-0.12254894 -0.20844744  0.0569959   0.33154421]\n",
      "[ episode 315 ][ timestamp 42 ] state=[-0.12254894 -0.20844744  0.0569959   0.33154421], action=1, reward=1.0, next_state=[-0.12671789 -0.01418114  0.06362679  0.05736567]\n",
      "[ episode 315 ][ timestamp 43 ] state=[-0.12671789 -0.01418114  0.06362679  0.05736567], action=0, reward=1.0, next_state=[-0.12700151 -0.21015495  0.0647741   0.36942522]\n",
      "[ episode 315 ][ timestamp 44 ] state=[-0.12700151 -0.21015495  0.0647741   0.36942522], action=1, reward=1.0, next_state=[-0.13120461 -0.01601026  0.0721626   0.09784917]\n",
      "[ episode 315 ][ timestamp 45 ] state=[-0.13120461 -0.01601026  0.0721626   0.09784917], action=0, reward=1.0, next_state=[-0.13152481 -0.21208828  0.07411959  0.41239813]\n",
      "[ episode 315 ][ timestamp 46 ] state=[-0.13152481 -0.21208828  0.07411959  0.41239813], action=1, reward=1.0, next_state=[-0.13576658 -0.01809098  0.08236755  0.14397235]\n",
      "[ episode 315 ][ timestamp 47 ] state=[-0.13576658 -0.01809098  0.08236755  0.14397235], action=0, reward=1.0, next_state=[-0.1361284  -0.21429003  0.085247    0.46146185]\n",
      "[ episode 315 ][ timestamp 48 ] state=[-0.1361284  -0.21429003  0.085247    0.46146185], action=1, reward=1.0, next_state=[-0.1404142  -0.02046985  0.09447623  0.19681959]\n",
      "[ episode 315 ][ timestamp 49 ] state=[-0.1404142  -0.02046985  0.09447623  0.19681959], action=1, reward=1.0, next_state=[-0.1408236   0.17318269  0.09841262 -0.0646291 ]\n",
      "[ episode 315 ][ timestamp 50 ] state=[-0.1408236   0.17318269  0.09841262 -0.0646291 ], action=0, reward=1.0, next_state=[-0.13735994 -0.02320243  0.09712004  0.25740986]\n",
      "[ episode 315 ][ timestamp 51 ] state=[-0.13735994 -0.02320243  0.09712004  0.25740986], action=1, reward=1.0, next_state=[-0.13782399  0.17040849  0.10226824 -0.00312952]\n",
      "[ episode 315 ][ timestamp 52 ] state=[-0.13782399  0.17040849  0.10226824 -0.00312952], action=0, reward=1.0, next_state=[-0.13441582 -0.02602014  0.10220565  0.31998844]\n",
      "[ episode 315 ][ timestamp 53 ] state=[-0.13441582 -0.02602014  0.10220565  0.31998844], action=0, reward=1.0, next_state=[-0.13493622 -0.22243788  0.10860542  0.64307373]\n",
      "[ episode 315 ][ timestamp 54 ] state=[-0.13493622 -0.22243788  0.10860542  0.64307373], action=1, reward=1.0, next_state=[-0.13938498 -0.02898384  0.12146689  0.38646962]\n",
      "[ episode 315 ][ timestamp 55 ] state=[-0.13938498 -0.02898384  0.12146689  0.38646962], action=0, reward=1.0, next_state=[-0.13996466 -0.22560185  0.12919629  0.7148471 ]\n",
      "[ episode 315 ][ timestamp 56 ] state=[-0.13996466 -0.22560185  0.12919629  0.7148471 ], action=1, reward=1.0, next_state=[-0.1444767  -0.03248253  0.14349323  0.46546052]\n",
      "[ episode 315 ][ timestamp 57 ] state=[-0.1444767  -0.03248253  0.14349323  0.46546052], action=1, reward=1.0, next_state=[-0.14512635  0.16035134  0.15280244  0.22122487]\n",
      "[ episode 315 ][ timestamp 58 ] state=[-0.14512635  0.16035134  0.15280244  0.22122487], action=1, reward=1.0, next_state=[-0.14191932  0.35299653  0.15722694 -0.01962668]\n",
      "[ episode 315 ][ timestamp 59 ] state=[-0.14191932  0.35299653  0.15722694 -0.01962668], action=0, reward=1.0, next_state=[-0.13485939  0.15601013  0.1568344   0.31824276]\n",
      "[ episode 315 ][ timestamp 60 ] state=[-0.13485939  0.15601013  0.1568344   0.31824276], action=1, reward=1.0, next_state=[-0.13173919  0.34859121  0.16319926  0.07883708]\n",
      "[ episode 315 ][ timestamp 61 ] state=[-0.13173919  0.34859121  0.16319926  0.07883708], action=0, reward=1.0, next_state=[-0.12476736  0.15155167  0.164776    0.41823704]\n",
      "[ episode 315 ][ timestamp 62 ] state=[-0.12476736  0.15155167  0.164776    0.41823704], action=1, reward=1.0, next_state=[-0.12173633  0.34400214  0.17314074  0.18169664]\n",
      "[ episode 315 ][ timestamp 63 ] state=[-0.12173633  0.34400214  0.17314074  0.18169664], action=1, reward=1.0, next_state=[-0.11485629  0.53627832  0.17677467 -0.05175599]\n",
      "[ episode 315 ][ timestamp 64 ] state=[-0.11485629  0.53627832  0.17677467 -0.05175599], action=0, reward=1.0, next_state=[-0.10413072  0.33912023  0.17573955  0.29107386]\n",
      "[ episode 315 ][ timestamp 65 ] state=[-0.10413072  0.33912023  0.17573955  0.29107386], action=1, reward=1.0, next_state=[-0.09734831  0.53135788  0.18156103  0.05856069]\n",
      "[ episode 315 ][ timestamp 66 ] state=[-0.09734831  0.53135788  0.18156103  0.05856069], action=1, reward=1.0, next_state=[-0.08672116  0.72347511  0.18273224 -0.17179225]\n",
      "[ episode 315 ][ timestamp 67 ] state=[-0.08672116  0.72347511  0.18273224 -0.17179225], action=0, reward=1.0, next_state=[-0.07225166  0.52627217  0.1792964   0.17251207]\n",
      "[ episode 315 ][ timestamp 68 ] state=[-0.07225166  0.52627217  0.1792964   0.17251207], action=0, reward=1.0, next_state=[-0.06172621  0.32909763  0.18274664  0.51596381]\n",
      "[ episode 315 ][ timestamp 69 ] state=[-0.06172621  0.32909763  0.18274664  0.51596381], action=0, reward=1.0, next_state=[-0.05514426  0.13193643  0.19306592  0.86020994]\n",
      "[ episode 315 ][ timestamp 70 ] state=[-0.05514426  0.13193643  0.19306592  0.86020994], action=0, reward=-1.0, next_state=[-0.05250553 -0.06521649  0.21027012  1.20685427]\n",
      "[ Ended! ] Episode 315: Exploration_rate=0.20722748400265262. Score=70.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 316 ] state=[ 0.00389902 -0.00023103  0.00446962 -0.0037901 ]\n",
      "[ episode 316 ][ timestamp 1 ] state=[ 0.00389902 -0.00023103  0.00446962 -0.0037901 ], action=1, reward=1.0, next_state=[ 0.0038944   0.19482654  0.00439382 -0.29505946]\n",
      "[ episode 316 ][ timestamp 2 ] state=[ 0.0038944   0.19482654  0.00439382 -0.29505946], action=1, reward=1.0, next_state=[ 0.00779093  0.38988558 -0.00150737 -0.58635341]\n",
      "[ episode 316 ][ timestamp 3 ] state=[ 0.00779093  0.38988558 -0.00150737 -0.58635341], action=0, reward=1.0, next_state=[ 0.01558864  0.19478477 -0.01323444 -0.2941457 ]\n",
      "[ episode 316 ][ timestamp 4 ] state=[ 0.01558864  0.19478477 -0.01323444 -0.2941457 ], action=0, reward=1.0, next_state=[ 0.01948434 -0.00014602 -0.01911735 -0.00566593]\n",
      "[ episode 316 ][ timestamp 5 ] state=[ 0.01948434 -0.00014602 -0.01911735 -0.00566593], action=1, reward=1.0, next_state=[ 0.01948141  0.19524481 -0.01923067 -0.30431878]\n",
      "[ episode 316 ][ timestamp 6 ] state=[ 0.01948141  0.19524481 -0.01923067 -0.30431878], action=0, reward=1.0, next_state=[ 0.02338631  0.00040212 -0.02531705 -0.01776225]\n",
      "[ episode 316 ][ timestamp 7 ] state=[ 0.02338631  0.00040212 -0.02531705 -0.01776225], action=1, reward=1.0, next_state=[ 0.02339435  0.19587783 -0.02567229 -0.31832426]\n",
      "[ episode 316 ][ timestamp 8 ] state=[ 0.02339435  0.19587783 -0.02567229 -0.31832426], action=0, reward=1.0, next_state=[ 0.02731191  0.00113074 -0.03203878 -0.03384672]\n",
      "[ episode 316 ][ timestamp 9 ] state=[ 0.02731191  0.00113074 -0.03203878 -0.03384672], action=1, reward=1.0, next_state=[ 0.02733452  0.19669714 -0.03271571 -0.33646357]\n",
      "[ episode 316 ][ timestamp 10 ] state=[ 0.02733452  0.19669714 -0.03271571 -0.33646357], action=1, reward=1.0, next_state=[ 0.03126847  0.39226903 -0.03944498 -0.63928112]\n",
      "[ episode 316 ][ timestamp 11 ] state=[ 0.03126847  0.39226903 -0.03944498 -0.63928112], action=0, reward=1.0, next_state=[ 0.03911385  0.1977186  -0.0522306  -0.35927629]\n",
      "[ episode 316 ][ timestamp 12 ] state=[ 0.03911385  0.1977186  -0.0522306  -0.35927629], action=0, reward=1.0, next_state=[ 0.04306822  0.00337654 -0.05941613 -0.08350955]\n",
      "[ episode 316 ][ timestamp 13 ] state=[ 0.04306822  0.00337654 -0.05941613 -0.08350955], action=1, reward=1.0, next_state=[ 0.04313575  0.19929764 -0.06108632 -0.39433069]\n",
      "[ episode 316 ][ timestamp 14 ] state=[ 0.04313575  0.19929764 -0.06108632 -0.39433069], action=0, reward=1.0, next_state=[ 0.0471217   0.00509328 -0.06897293 -0.1215157 ]\n",
      "[ episode 316 ][ timestamp 15 ] state=[ 0.0471217   0.00509328 -0.06897293 -0.1215157 ], action=1, reward=1.0, next_state=[ 0.04722357  0.20113215 -0.07140325 -0.43513678]\n",
      "[ episode 316 ][ timestamp 16 ] state=[ 0.04722357  0.20113215 -0.07140325 -0.43513678], action=1, reward=1.0, next_state=[ 0.05124621  0.39718845 -0.08010598 -0.7494466 ]\n",
      "[ episode 316 ][ timestamp 17 ] state=[ 0.05124621  0.39718845 -0.08010598 -0.7494466 ], action=0, reward=1.0, next_state=[ 0.05918998  0.20325744 -0.09509492 -0.4830089 ]\n",
      "[ episode 316 ][ timestamp 18 ] state=[ 0.05918998  0.20325744 -0.09509492 -0.4830089 ], action=0, reward=1.0, next_state=[ 0.06325513  0.00959723 -0.10475509 -0.22174684]\n",
      "[ episode 316 ][ timestamp 19 ] state=[ 0.06325513  0.00959723 -0.10475509 -0.22174684], action=0, reward=1.0, next_state=[ 0.06344708 -0.18388357 -0.10919003  0.03614173]\n",
      "[ episode 316 ][ timestamp 20 ] state=[ 0.06344708 -0.18388357 -0.10919003  0.03614173], action=1, reward=1.0, next_state=[ 0.0597694   0.01262102 -0.1084672  -0.28889791]\n",
      "[ episode 316 ][ timestamp 21 ] state=[ 0.0597694   0.01262102 -0.1084672  -0.28889791], action=0, reward=1.0, next_state=[ 0.06002182 -0.18080048 -0.11424515 -0.03229756]\n",
      "[ episode 316 ][ timestamp 22 ] state=[ 0.06002182 -0.18080048 -0.11424515 -0.03229756], action=1, reward=1.0, next_state=[ 0.05640581  0.01575867 -0.11489111 -0.35872934]\n",
      "[ episode 316 ][ timestamp 23 ] state=[ 0.05640581  0.01575867 -0.11489111 -0.35872934], action=0, reward=1.0, next_state=[ 0.05672099 -0.17755853 -0.12206569 -0.10436899]\n",
      "[ episode 316 ][ timestamp 24 ] state=[ 0.05672099 -0.17755853 -0.12206569 -0.10436899], action=1, reward=1.0, next_state=[ 0.05316982  0.01908207 -0.12415307 -0.4329334 ]\n",
      "[ episode 316 ][ timestamp 25 ] state=[ 0.05316982  0.01908207 -0.12415307 -0.4329334 ], action=0, reward=1.0, next_state=[ 0.05355146 -0.17408349 -0.13281174 -0.1818226 ]\n",
      "[ episode 316 ][ timestamp 26 ] state=[ 0.05355146 -0.17408349 -0.13281174 -0.1818226 ], action=1, reward=1.0, next_state=[ 0.05006979  0.02266414 -0.13644819 -0.51327701]\n",
      "[ episode 316 ][ timestamp 27 ] state=[ 0.05006979  0.02266414 -0.13644819 -0.51327701], action=0, reward=1.0, next_state=[ 0.05052307 -0.17029885 -0.14671373 -0.2665142 ]\n",
      "[ episode 316 ][ timestamp 28 ] state=[ 0.05052307 -0.17029885 -0.14671373 -0.2665142 ], action=1, reward=1.0, next_state=[ 0.04711709  0.02657913 -0.15204402 -0.60163779]\n",
      "[ episode 316 ][ timestamp 29 ] state=[ 0.04711709  0.02657913 -0.15204402 -0.60163779], action=0, reward=1.0, next_state=[ 0.04764868 -0.16612577 -0.16407677 -0.36044406]\n",
      "[ episode 316 ][ timestamp 30 ] state=[ 0.04764868 -0.16612577 -0.16407677 -0.36044406], action=1, reward=1.0, next_state=[ 0.04432616  0.03090193 -0.17128565 -0.70003878]\n",
      "[ episode 316 ][ timestamp 31 ] state=[ 0.04432616  0.03090193 -0.17128565 -0.70003878], action=0, reward=1.0, next_state=[ 0.0449442  -0.16148381 -0.18528643 -0.4657952 ]\n",
      "[ episode 316 ][ timestamp 32 ] state=[ 0.0449442  -0.16148381 -0.18528643 -0.4657952 ], action=1, reward=1.0, next_state=[ 0.04171452  0.03570632 -0.19460233 -0.81068064]\n",
      "[ episode 316 ][ timestamp 33 ] state=[ 0.04171452  0.03570632 -0.19460233 -0.81068064], action=0, reward=-1.0, next_state=[ 0.04242865 -0.15629332 -0.21081595 -0.58496993]\n",
      "[ Ended! ] Episode 316: Exploration_rate=0.20619134658263935. Score=33.\n",
      "[ Experience replay ] starts\n",
      "[ episode 317 ] state=[-0.04418393 -0.00964337  0.0233631   0.01293565]\n",
      "[ episode 317 ][ timestamp 1 ] state=[-0.04418393 -0.00964337  0.0233631   0.01293565], action=1, reward=1.0, next_state=[-0.0443768   0.18513587  0.02362181 -0.27228534]\n",
      "[ episode 317 ][ timestamp 2 ] state=[-0.0443768   0.18513587  0.02362181 -0.27228534], action=1, reward=1.0, next_state=[-0.04067408  0.37991293  0.01817611 -0.55742526]\n",
      "[ episode 317 ][ timestamp 3 ] state=[-0.04067408  0.37991293  0.01817611 -0.55742526], action=1, reward=1.0, next_state=[-0.03307583  0.57477507  0.0070276  -0.8443267 ]\n",
      "[ episode 317 ][ timestamp 4 ] state=[-0.03307583  0.57477507  0.0070276  -0.8443267 ], action=0, reward=1.0, next_state=[-0.02158032  0.37955793 -0.00985893 -0.54944212]\n",
      "[ episode 317 ][ timestamp 5 ] state=[-0.02158032  0.37955793 -0.00985893 -0.54944212], action=0, reward=1.0, next_state=[-0.01398917  0.18457584 -0.02084777 -0.25988169]\n",
      "[ episode 317 ][ timestamp 6 ] state=[-0.01398917  0.18457584 -0.02084777 -0.25988169], action=0, reward=1.0, next_state=[-0.01029765 -0.01024239 -0.02604541  0.02615335]\n",
      "[ episode 317 ][ timestamp 7 ] state=[-0.01029765 -0.01024239 -0.02604541  0.02615335], action=1, reward=1.0, next_state=[-0.0105025   0.18524321 -0.02552234 -0.27463207]\n",
      "[ episode 317 ][ timestamp 8 ] state=[-0.0105025   0.18524321 -0.02552234 -0.27463207], action=0, reward=1.0, next_state=[-0.00679763 -0.00950547 -0.03101498  0.00989306]\n",
      "[ episode 317 ][ timestamp 9 ] state=[-0.00679763 -0.00950547 -0.03101498  0.00989306], action=0, reward=1.0, next_state=[-0.00698774 -0.20416922 -0.03081712  0.2926313 ]\n",
      "[ episode 317 ][ timestamp 10 ] state=[-0.00698774 -0.20416922 -0.03081712  0.2926313 ], action=1, reward=1.0, next_state=[-0.01107113 -0.00862174 -0.0249645  -0.00960944]\n",
      "[ episode 317 ][ timestamp 11 ] state=[-0.01107113 -0.00862174 -0.0249645  -0.00960944], action=0, reward=1.0, next_state=[-0.01124356 -0.20337694 -0.02515668  0.27509352]\n",
      "[ episode 317 ][ timestamp 12 ] state=[-0.01124356 -0.20337694 -0.02515668  0.27509352], action=1, reward=1.0, next_state=[-0.0153111  -0.00790526 -0.01965481 -0.0254165 ]\n",
      "[ episode 317 ][ timestamp 13 ] state=[-0.0153111  -0.00790526 -0.01965481 -0.0254165 ], action=0, reward=1.0, next_state=[-0.01546921 -0.20273991 -0.02016314  0.26100089]\n",
      "[ episode 317 ][ timestamp 14 ] state=[-0.01546921 -0.20273991 -0.02016314  0.26100089], action=1, reward=1.0, next_state=[-0.019524   -0.00733602 -0.01494313 -0.03797293]\n",
      "[ episode 317 ][ timestamp 15 ] state=[-0.019524   -0.00733602 -0.01494313 -0.03797293], action=0, reward=1.0, next_state=[-0.01967072 -0.20224053 -0.01570258  0.24995808]\n",
      "[ episode 317 ][ timestamp 16 ] state=[-0.01967072 -0.20224053 -0.01570258  0.24995808], action=1, reward=1.0, next_state=[-0.02371553 -0.0068979  -0.01070342 -0.04763611]\n",
      "[ episode 317 ][ timestamp 17 ] state=[-0.02371553 -0.0068979  -0.01070342 -0.04763611], action=0, reward=1.0, next_state=[-0.02385349 -0.20186475 -0.01165615  0.24165066]\n",
      "[ episode 317 ][ timestamp 18 ] state=[-0.02385349 -0.20186475 -0.01165615  0.24165066], action=1, reward=1.0, next_state=[-0.02789079 -0.00657826 -0.00682313 -0.05468601]\n",
      "[ episode 317 ][ timestamp 19 ] state=[-0.02789079 -0.00657826 -0.00682313 -0.05468601], action=0, reward=1.0, next_state=[-0.02802235 -0.20160171 -0.00791685  0.23583638]\n",
      "[ episode 317 ][ timestamp 20 ] state=[-0.02802235 -0.20160171 -0.00791685  0.23583638], action=1, reward=1.0, next_state=[-0.03205439 -0.00636755 -0.00320012 -0.05933321]\n",
      "[ episode 317 ][ timestamp 21 ] state=[-0.03205439 -0.00636755 -0.00320012 -0.05933321], action=0, reward=1.0, next_state=[-0.03218174 -0.20144347 -0.00438679  0.23233833]\n",
      "[ episode 317 ][ timestamp 22 ] state=[-0.03218174 -0.20144347 -0.00438679  0.23233833], action=1, reward=1.0, next_state=[-0.03621061 -0.00625912  0.00025998 -0.0617251 ]\n",
      "[ episode 317 ][ timestamp 23 ] state=[-0.03621061 -0.00625912  0.00025998 -0.0617251 ], action=1, reward=1.0, next_state=[-0.03633579  0.18885911 -0.00097452 -0.35432599]\n",
      "[ episode 317 ][ timestamp 24 ] state=[-0.03633579  0.18885911 -0.00097452 -0.35432599], action=0, reward=1.0, next_state=[-0.03255861 -0.00624897 -0.00806104 -0.06195051]\n",
      "[ episode 317 ][ timestamp 25 ] state=[-0.03255861 -0.00624897 -0.00806104 -0.06195051], action=0, reward=1.0, next_state=[-0.03268359 -0.20125443 -0.00930005  0.22817824]\n",
      "[ episode 317 ][ timestamp 26 ] state=[-0.03268359 -0.20125443 -0.00930005  0.22817824], action=1, reward=1.0, next_state=[-0.03670868 -0.00600081 -0.00473649 -0.06742369]\n",
      "[ episode 317 ][ timestamp 27 ] state=[-0.03670868 -0.00600081 -0.00473649 -0.06742369], action=0, reward=1.0, next_state=[-0.03682869 -0.20105454 -0.00608496  0.22376109]\n",
      "[ episode 317 ][ timestamp 28 ] state=[-0.03682869 -0.20105454 -0.00608496  0.22376109], action=1, reward=1.0, next_state=[-0.04084978 -0.00584615 -0.00160974 -0.07083504]\n",
      "[ episode 317 ][ timestamp 29 ] state=[-0.04084978 -0.00584615 -0.00160974 -0.07083504], action=0, reward=1.0, next_state=[-0.04096671 -0.20094499 -0.00302644  0.22133957]\n",
      "[ episode 317 ][ timestamp 30 ] state=[-0.04096671 -0.20094499 -0.00302644  0.22133957], action=1, reward=1.0, next_state=[-0.04498561 -0.00577991  0.00140035 -0.07229648]\n",
      "[ episode 317 ][ timestamp 31 ] state=[-0.04498561 -0.00577991  0.00140035 -0.07229648], action=0, reward=1.0, next_state=[-4.51012040e-02 -2.00921905e-01 -4.55801761e-05  2.20827935e-01]\n",
      "[ episode 317 ][ timestamp 32 ] state=[-4.51012040e-02 -2.00921905e-01 -4.55801761e-05  2.20827935e-01], action=1, reward=1.0, next_state=[-0.04911964 -0.0057993   0.00437098 -0.07186937]\n",
      "[ episode 317 ][ timestamp 33 ] state=[-0.04911964 -0.0057993   0.00437098 -0.07186937], action=0, reward=1.0, next_state=[-0.04923563 -0.20098364  0.00293359  0.22218941]\n",
      "[ episode 317 ][ timestamp 34 ] state=[-0.04923563 -0.20098364  0.00293359  0.22218941], action=1, reward=1.0, next_state=[-0.0532553  -0.00590375  0.00737738 -0.0695667 ]\n",
      "[ episode 317 ][ timestamp 35 ] state=[-0.0532553  -0.00590375  0.00737738 -0.0695667 ], action=0, reward=1.0, next_state=[-0.05337338 -0.20113068  0.00598605  0.22543466]\n",
      "[ episode 317 ][ timestamp 36 ] state=[-0.05337338 -0.20113068  0.00598605  0.22543466], action=1, reward=1.0, next_state=[-0.05739599 -0.00609479  0.01049474 -0.06535404]\n",
      "[ episode 317 ][ timestamp 37 ] state=[-0.05739599 -0.00609479  0.01049474 -0.06535404], action=0, reward=1.0, next_state=[-0.05751789 -0.20136563  0.00918766  0.23062147]\n",
      "[ episode 317 ][ timestamp 38 ] state=[-0.05751789 -0.20136563  0.00918766  0.23062147], action=1, reward=1.0, next_state=[-0.0615452  -0.00637616  0.01380009 -0.05914925]\n",
      "[ episode 317 ][ timestamp 39 ] state=[-0.0615452  -0.00637616  0.01380009 -0.05914925], action=0, reward=1.0, next_state=[-0.06167272 -0.20169323  0.0126171   0.23785556]\n",
      "[ episode 317 ][ timestamp 40 ] state=[-0.06167272 -0.20169323  0.0126171   0.23785556], action=1, reward=1.0, next_state=[-0.06570659 -0.00675378  0.01737421 -0.05082101]\n",
      "[ episode 317 ][ timestamp 41 ] state=[-0.06570659 -0.00675378  0.01737421 -0.05082101], action=0, reward=1.0, next_state=[-0.06584166 -0.2021205   0.01635779  0.24729259]\n",
      "[ episode 317 ][ timestamp 42 ] state=[-0.06584166 -0.2021205   0.01635779  0.24729259], action=1, reward=1.0, next_state=[-0.06988407 -0.00723594  0.02130364 -0.04018616]\n",
      "[ episode 317 ][ timestamp 43 ] state=[-0.06988407 -0.00723594  0.02130364 -0.04018616], action=0, reward=1.0, next_state=[-0.07002879 -0.2026568   0.02049992  0.25914141]\n",
      "[ episode 317 ][ timestamp 44 ] state=[-0.07002879 -0.2026568   0.02049992  0.25914141], action=1, reward=1.0, next_state=[-0.07408193 -0.00783341  0.02568275 -0.02700571]\n",
      "[ episode 317 ][ timestamp 45 ] state=[-0.07408193 -0.00783341  0.02568275 -0.02700571], action=0, reward=1.0, next_state=[-0.07423859 -0.20331408  0.02514264  0.27366849]\n",
      "[ episode 317 ][ timestamp 46 ] state=[-0.07423859 -0.20331408  0.02514264  0.27366849], action=1, reward=1.0, next_state=[-0.07830488 -0.00855973  0.03061601 -0.01097955]\n",
      "[ episode 317 ][ timestamp 47 ] state=[-0.07830488 -0.00855973  0.03061601 -0.01097955], action=0, reward=1.0, next_state=[-0.07847607 -0.20410707  0.03039641  0.29120371]\n",
      "[ episode 317 ][ timestamp 48 ] state=[-0.07847607 -0.20410707  0.03039641  0.29120371], action=1, reward=1.0, next_state=[-0.08255821 -0.00943142  0.03622049  0.0082603 ]\n",
      "[ episode 317 ][ timestamp 49 ] state=[-0.08255821 -0.00943142  0.03622049  0.0082603 ], action=0, reward=1.0, next_state=[-0.08274684 -0.2050536   0.03638569  0.3121476 ]\n",
      "[ episode 317 ][ timestamp 50 ] state=[-0.08274684 -0.2050536   0.03638569  0.3121476 ], action=1, reward=1.0, next_state=[-0.08684791 -0.0104684   0.04262865  0.03115802]\n",
      "[ episode 317 ][ timestamp 51 ] state=[-0.08684791 -0.0104684   0.04262865  0.03115802], action=0, reward=1.0, next_state=[-0.08705728 -0.20617492  0.04325181  0.33698014]\n",
      "[ episode 317 ][ timestamp 52 ] state=[-0.08705728 -0.20617492  0.04325181  0.33698014], action=1, reward=1.0, next_state=[-0.09118078 -0.0116943   0.04999141  0.0582441 ]\n",
      "[ episode 317 ][ timestamp 53 ] state=[-0.09118078 -0.0116943   0.04999141  0.0582441 ], action=0, reward=1.0, next_state=[-0.09141466 -0.20749607  0.05115629  0.36627118]\n",
      "[ episode 317 ][ timestamp 54 ] state=[-0.09141466 -0.20749607  0.05115629  0.36627118], action=1, reward=1.0, next_state=[-0.09556459 -0.01313699  0.05848172  0.09014734]\n",
      "[ episode 317 ][ timestamp 55 ] state=[-0.09556459 -0.01313699  0.05848172  0.09014734], action=1, reward=1.0, next_state=[-0.09582733  0.18110007  0.06028466 -0.18352634]\n",
      "[ episode 317 ][ timestamp 56 ] state=[-0.09582733  0.18110007  0.06028466 -0.18352634], action=0, reward=1.0, next_state=[-0.09220532 -0.01483034  0.05661414  0.12754835]\n",
      "[ episode 317 ][ timestamp 57 ] state=[-0.09220532 -0.01483034  0.05661414  0.12754835], action=1, reward=1.0, next_state=[-0.09250193  0.1794368   0.0591651  -0.14674982]\n",
      "[ episode 317 ][ timestamp 58 ] state=[-0.09250193  0.1794368   0.0591651  -0.14674982], action=0, reward=1.0, next_state=[-0.0889132  -0.01648032  0.05623011  0.16399604]\n",
      "[ episode 317 ][ timestamp 59 ] state=[-0.0889132  -0.01648032  0.05623011  0.16399604], action=1, reward=1.0, next_state=[-0.0892428   0.17779349  0.05951003 -0.11043115]\n",
      "[ episode 317 ][ timestamp 60 ] state=[-0.0892428   0.17779349  0.05951003 -0.11043115], action=1, reward=1.0, next_state=[-0.08568693  0.37201443  0.0573014  -0.38376122]\n",
      "[ episode 317 ][ timestamp 61 ] state=[-0.08568693  0.37201443  0.0573014  -0.38376122], action=0, reward=1.0, next_state=[-0.07824664  0.17612773  0.04962618 -0.07357604]\n",
      "[ episode 317 ][ timestamp 62 ] state=[-0.07824664  0.17612773  0.04962618 -0.07357604], action=0, reward=1.0, next_state=[-0.07472409 -0.01966925  0.04815466  0.23434197]\n",
      "[ episode 317 ][ timestamp 63 ] state=[-0.07472409 -0.01966925  0.04815466  0.23434197], action=1, reward=1.0, next_state=[-0.07511747  0.17473278  0.0528415  -0.04277105]\n",
      "[ episode 317 ][ timestamp 64 ] state=[-0.07511747  0.17473278  0.0528415  -0.04277105], action=0, reward=1.0, next_state=[-0.07162282 -0.02110552  0.05198608  0.26610454]\n",
      "[ episode 317 ][ timestamp 65 ] state=[-0.07162282 -0.02110552  0.05198608  0.26610454], action=1, reward=1.0, next_state=[-0.07204493  0.17323741  0.05730817 -0.009739  ]\n",
      "[ episode 317 ][ timestamp 66 ] state=[-0.07204493  0.17323741  0.05730817 -0.009739  ], action=0, reward=1.0, next_state=[-0.06858018 -0.02265759  0.05711339  0.3004605 ]\n",
      "[ episode 317 ][ timestamp 67 ] state=[-0.06858018 -0.02265759  0.05711339  0.3004605 ], action=1, reward=1.0, next_state=[-0.06903333  0.17160576  0.0631226   0.02632281]\n",
      "[ episode 317 ][ timestamp 68 ] state=[-0.06903333  0.17160576  0.0631226   0.02632281], action=0, reward=1.0, next_state=[-0.06560122 -0.02436196  0.06364905  0.33823468]\n",
      "[ episode 317 ][ timestamp 69 ] state=[-0.06560122 -0.02436196  0.06364905  0.33823468], action=1, reward=1.0, next_state=[-0.06608846  0.16979926  0.07041375  0.06628279]\n",
      "[ episode 317 ][ timestamp 70 ] state=[-0.06608846  0.16979926  0.07041375  0.06628279], action=0, reward=1.0, next_state=[-0.06269247 -0.02625788  0.0717394   0.38032429]\n",
      "[ episode 317 ][ timestamp 71 ] state=[-0.06269247 -0.02625788  0.0717394   0.38032429], action=1, reward=1.0, next_state=[-0.06321763  0.16777592  0.07934589  0.11109551]\n",
      "[ episode 317 ][ timestamp 72 ] state=[-0.06321763  0.16777592  0.07934589  0.11109551], action=0, reward=1.0, next_state=[-0.05986211 -0.02838807  0.0815678   0.42771895]\n",
      "[ episode 317 ][ timestamp 73 ] state=[-0.05986211 -0.02838807  0.0815678   0.42771895], action=0, reward=1.0, next_state=[-0.06042987 -0.22456481  0.09012218  0.74496003]\n",
      "[ episode 317 ][ timestamp 74 ] state=[-0.06042987 -0.22456481  0.09012218  0.74496003], action=1, reward=1.0, next_state=[-0.06492117 -0.03079447  0.10502138  0.48194414]\n",
      "[ episode 317 ][ timestamp 75 ] state=[-0.06492117 -0.03079447  0.10502138  0.48194414], action=1, reward=1.0, next_state=[-0.06553706  0.16270054  0.11466026  0.22412033]\n",
      "[ episode 317 ][ timestamp 76 ] state=[-0.06553706  0.16270054  0.11466026  0.22412033], action=1, reward=1.0, next_state=[-0.06228305  0.35601293  0.11914267 -0.03030794]\n",
      "[ episode 317 ][ timestamp 77 ] state=[-0.06228305  0.35601293  0.11914267 -0.03030794], action=0, reward=1.0, next_state=[-0.05516279  0.15940178  0.11853651  0.29746323]\n",
      "[ episode 317 ][ timestamp 78 ] state=[-0.05516279  0.15940178  0.11853651  0.29746323], action=1, reward=1.0, next_state=[-0.05197475  0.35265208  0.12448577  0.04439009]\n",
      "[ episode 317 ][ timestamp 79 ] state=[-0.05197475  0.35265208  0.12448577  0.04439009], action=0, reward=1.0, next_state=[-0.04492171  0.1559853   0.12537358  0.37361181]\n",
      "[ episode 317 ][ timestamp 80 ] state=[-0.04492171  0.1559853   0.12537358  0.37361181], action=1, reward=1.0, next_state=[-0.041802    0.34912404  0.13284581  0.12294094]\n",
      "[ episode 317 ][ timestamp 81 ] state=[-0.041802    0.34912404  0.13284581  0.12294094], action=1, reward=1.0, next_state=[-0.03481952  0.54211733  0.13530463 -0.12505639]\n",
      "[ episode 317 ][ timestamp 82 ] state=[-0.03481952  0.54211733  0.13530463 -0.12505639], action=0, reward=1.0, next_state=[-0.02397718  0.34534255  0.1328035   0.20706638]\n",
      "[ episode 317 ][ timestamp 83 ] state=[-0.02397718  0.34534255  0.1328035   0.20706638], action=1, reward=1.0, next_state=[-0.01707033  0.53834016  0.13694483 -0.04095133]\n",
      "[ episode 317 ][ timestamp 84 ] state=[-0.01707033  0.53834016  0.13694483 -0.04095133], action=0, reward=1.0, next_state=[-0.00630352  0.34154727  0.1361258   0.29161041]\n",
      "[ episode 317 ][ timestamp 85 ] state=[-0.00630352  0.34154727  0.1361258   0.29161041], action=1, reward=1.0, next_state=[5.27422212e-04 5.34492264e-01 1.41958013e-01 4.47677641e-02]\n",
      "[ episode 317 ][ timestamp 86 ] state=[5.27422212e-04 5.34492264e-01 1.41958013e-01 4.47677641e-02], action=0, reward=1.0, next_state=[0.01121727 0.33765025 0.14285337 0.37865631]\n",
      "[ episode 317 ][ timestamp 87 ] state=[0.01121727 0.33765025 0.14285337 0.37865631], action=1, reward=1.0, next_state=[0.01797027 0.53048516 0.15042649 0.13420652]\n",
      "[ episode 317 ][ timestamp 88 ] state=[0.01797027 0.53048516 0.15042649 0.13420652], action=1, reward=1.0, next_state=[ 0.02857998  0.72316818  0.15311062 -0.10749534]\n",
      "[ episode 317 ][ timestamp 89 ] state=[ 0.02857998  0.72316818  0.15311062 -0.10749534], action=1, reward=1.0, next_state=[ 0.04304334  0.91580232  0.15096072 -0.34822738]\n",
      "[ episode 317 ][ timestamp 90 ] state=[ 0.04304334  0.91580232  0.15096072 -0.34822738], action=0, reward=1.0, next_state=[ 0.06135939  0.71889177  0.14399617 -0.01200666]\n",
      "[ episode 317 ][ timestamp 91 ] state=[ 0.06135939  0.71889177  0.14399617 -0.01200666], action=1, reward=1.0, next_state=[ 0.07573722  0.91168656  0.14375604 -0.25601712]\n",
      "[ episode 317 ][ timestamp 92 ] state=[ 0.07573722  0.91168656  0.14375604 -0.25601712], action=0, reward=1.0, next_state=[0.09397095 0.714836   0.13863569 0.07833176]\n",
      "[ episode 317 ][ timestamp 93 ] state=[0.09397095 0.714836   0.13863569 0.07833176], action=1, reward=1.0, next_state=[ 0.10826767  0.90772639  0.14020233 -0.16759931]\n",
      "[ episode 317 ][ timestamp 94 ] state=[ 0.10826767  0.90772639  0.14020233 -0.16759931], action=0, reward=1.0, next_state=[0.1264222  0.71090504 0.13685034 0.16582039]\n",
      "[ episode 317 ][ timestamp 95 ] state=[0.1264222  0.71090504 0.13685034 0.16582039], action=1, reward=1.0, next_state=[ 0.1406403   0.90382971  0.14016675 -0.08075248]\n",
      "[ episode 317 ][ timestamp 96 ] state=[ 0.1406403   0.90382971  0.14016675 -0.08075248], action=0, reward=1.0, next_state=[0.1587169  0.70700577 0.1385517  0.25266218]\n",
      "[ episode 317 ][ timestamp 97 ] state=[0.1587169  0.70700577 0.1385517  0.25266218], action=1, reward=1.0, next_state=[0.17285701 0.8999054  0.14360495 0.00668956]\n",
      "[ episode 317 ][ timestamp 98 ] state=[0.17285701 0.8999054  0.14360495 0.00668956], action=0, reward=1.0, next_state=[0.19085512 0.70304716 0.14373874 0.34101227]\n",
      "[ episode 317 ][ timestamp 99 ] state=[0.19085512 0.70304716 0.14373874 0.34101227], action=1, reward=1.0, next_state=[0.20491606 0.89586273 0.15055898 0.0968854 ]\n",
      "[ episode 317 ][ timestamp 100 ] state=[0.20491606 0.89586273 0.15055898 0.0968854 ], action=1, reward=1.0, next_state=[ 0.22283332  1.08854212  0.15249669 -0.14476683]\n",
      "[ episode 317 ][ timestamp 101 ] state=[ 0.22283332  1.08854212  0.15249669 -0.14476683], action=0, reward=1.0, next_state=[0.24460416 0.89160252 0.14960135 0.19187477]\n",
      "[ episode 317 ][ timestamp 102 ] state=[0.24460416 0.89160252 0.14960135 0.19187477], action=1, reward=1.0, next_state=[ 0.26243621  1.08430297  0.15343885 -0.05012846]\n",
      "[ episode 317 ][ timestamp 103 ] state=[ 0.26243621  1.08430297  0.15343885 -0.05012846], action=0, reward=1.0, next_state=[0.28412227 0.88735187 0.15243628 0.28676155]\n",
      "[ episode 317 ][ timestamp 104 ] state=[0.28412227 0.88735187 0.15243628 0.28676155], action=1, reward=1.0, next_state=[0.30186931 1.08000852 0.15817151 0.04577053]\n",
      "[ episode 317 ][ timestamp 105 ] state=[0.30186931 1.08000852 0.15817151 0.04577053], action=1, reward=1.0, next_state=[ 0.32346948  1.27255049  0.15908692 -0.19312839]\n",
      "[ episode 317 ][ timestamp 106 ] state=[ 0.32346948  1.27255049  0.15908692 -0.19312839], action=1, reward=1.0, next_state=[ 0.34892049  1.46508142  0.15522435 -0.43170346]\n",
      "[ episode 317 ][ timestamp 107 ] state=[ 0.34892049  1.46508142  0.15522435 -0.43170346], action=0, reward=1.0, next_state=[ 0.37822211  1.26814145  0.14659028 -0.09439234]\n",
      "[ episode 317 ][ timestamp 108 ] state=[ 0.37822211  1.26814145  0.14659028 -0.09439234], action=1, reward=1.0, next_state=[ 0.40358494  1.46089144  0.14470244 -0.33747305]\n",
      "[ episode 317 ][ timestamp 109 ] state=[ 0.40358494  1.46089144  0.14470244 -0.33747305], action=0, reward=1.0, next_state=[ 0.43280277  1.26403861  0.13795298 -0.00288561]\n",
      "[ episode 317 ][ timestamp 110 ] state=[ 0.43280277  1.26403861  0.13795298 -0.00288561], action=0, reward=1.0, next_state=[0.45808354 1.06723554 0.13789526 0.32994408]\n",
      "[ episode 317 ][ timestamp 111 ] state=[0.45808354 1.06723554 0.13789526 0.32994408], action=1, reward=1.0, next_state=[0.47942826 1.26015272 0.14449415 0.08372805]\n",
      "[ episode 317 ][ timestamp 112 ] state=[0.47942826 1.26015272 0.14449415 0.08372805], action=1, reward=1.0, next_state=[ 0.50463131  1.45293965  0.14616871 -0.16010515]\n",
      "[ episode 317 ][ timestamp 113 ] state=[ 0.50463131  1.45293965  0.14616871 -0.16010515], action=0, reward=1.0, next_state=[0.5336901 1.2560604 0.1429666 0.1748853]\n",
      "[ episode 317 ][ timestamp 114 ] state=[0.5336901 1.2560604 0.1429666 0.1748853], action=1, reward=1.0, next_state=[ 0.55881131  1.44887766  0.14646431 -0.06950068]\n",
      "[ episode 317 ][ timestamp 115 ] state=[ 0.55881131  1.44887766  0.14646431 -0.06950068], action=0, reward=1.0, next_state=[0.58778886 1.25199263 0.1450743  0.26557159]\n",
      "[ episode 317 ][ timestamp 116 ] state=[0.58778886 1.25199263 0.1450743  0.26557159], action=1, reward=1.0, next_state=[0.61282872 1.44477821 0.15038573 0.02193338]\n",
      "[ episode 317 ][ timestamp 117 ] state=[0.61282872 1.44477821 0.15038573 0.02193338], action=0, reward=1.0, next_state=[0.64172428 1.24785537 0.1508244  0.35803068]\n",
      "[ episode 317 ][ timestamp 118 ] state=[0.64172428 1.24785537 0.1508244  0.35803068], action=1, reward=1.0, next_state=[0.66668139 1.44054742 0.15798501 0.11644835]\n",
      "[ episode 317 ][ timestamp 119 ] state=[0.66668139 1.44054742 0.15798501 0.11644835], action=1, reward=1.0, next_state=[ 0.69549234  1.6330945   0.16031398 -0.12252077]\n",
      "[ episode 317 ][ timestamp 120 ] state=[ 0.69549234  1.6330945   0.16031398 -0.12252077], action=0, reward=1.0, next_state=[0.72815423 1.43608226 0.15786356 0.21613889]\n",
      "[ episode 317 ][ timestamp 121 ] state=[0.72815423 1.43608226 0.15786356 0.21613889], action=1, reward=1.0, next_state=[ 0.75687587  1.6286366   0.16218634 -0.02288175]\n",
      "[ episode 317 ][ timestamp 122 ] state=[ 0.75687587  1.6286366   0.16218634 -0.02288175], action=0, reward=1.0, next_state=[0.7894486  1.43160523 0.1617287  0.31626073]\n",
      "[ episode 317 ][ timestamp 123 ] state=[0.7894486  1.43160523 0.1617287  0.31626073], action=1, reward=1.0, next_state=[0.81808071 1.6240985  0.16805392 0.07863   ]\n",
      "[ episode 317 ][ timestamp 124 ] state=[0.81808071 1.6240985  0.16805392 0.07863   ], action=0, reward=1.0, next_state=[0.85056268 1.42701599 0.16962652 0.41926468]\n",
      "[ episode 317 ][ timestamp 125 ] state=[0.85056268 1.42701599 0.16962652 0.41926468], action=1, reward=1.0, next_state=[0.879103   1.61937947 0.17801181 0.18449207]\n",
      "[ episode 317 ][ timestamp 126 ] state=[0.879103   1.61937947 0.17801181 0.18449207], action=1, reward=1.0, next_state=[ 0.91149059  1.81156702  0.18170165 -0.04717423]\n",
      "[ episode 317 ][ timestamp 127 ] state=[ 0.91149059  1.81156702  0.18170165 -0.04717423], action=0, reward=1.0, next_state=[0.94772193 1.61436774 0.18075817 0.29688196]\n",
      "[ episode 317 ][ timestamp 128 ] state=[0.94772193 1.61436774 0.18075817 0.29688196], action=1, reward=1.0, next_state=[0.98000928 1.80651452 0.18669581 0.06621155]\n",
      "[ episode 317 ][ timestamp 129 ] state=[0.98000928 1.80651452 0.18669581 0.06621155], action=1, reward=1.0, next_state=[ 1.01613957  1.99853748  0.18802004 -0.16224744]\n",
      "[ episode 317 ][ timestamp 130 ] state=[ 1.01613957  1.99853748  0.18802004 -0.16224744], action=0, reward=1.0, next_state=[1.05611032 1.80129119 0.18477509 0.18336045]\n",
      "[ episode 317 ][ timestamp 131 ] state=[1.05611032 1.80129119 0.18477509 0.18336045], action=1, reward=1.0, next_state=[ 1.09213615  1.99335475  0.1884423  -0.04581553]\n",
      "[ episode 317 ][ timestamp 132 ] state=[ 1.09213615  1.99335475  0.1884423  -0.04581553], action=1, reward=1.0, next_state=[ 1.13200324  2.18534509  0.18752599 -0.27362819]\n",
      "[ episode 317 ][ timestamp 133 ] state=[ 1.13200324  2.18534509  0.18752599 -0.27362819], action=0, reward=1.0, next_state=[1.17571014 1.98811156 0.18205342 0.07184549]\n",
      "[ episode 317 ][ timestamp 134 ] state=[1.17571014 1.98811156 0.18205342 0.07184549], action=0, reward=1.0, next_state=[1.21547237 1.79090989 0.18349033 0.41598809]\n",
      "[ episode 317 ][ timestamp 135 ] state=[1.21547237 1.79090989 0.18349033 0.41598809], action=1, reward=1.0, next_state=[1.25129057 1.98302181 0.1918101  0.18630169]\n",
      "[ episode 317 ][ timestamp 136 ] state=[1.25129057 1.98302181 0.1918101  0.18630169], action=1, reward=1.0, next_state=[ 1.29095101  2.17495596  0.19553613 -0.04027265]\n",
      "[ episode 317 ][ timestamp 137 ] state=[ 1.29095101  2.17495596  0.19553613 -0.04027265], action=1, reward=1.0, next_state=[ 1.33445013  2.36681494  0.19473068 -0.26545492]\n",
      "[ episode 317 ][ timestamp 138 ] state=[ 1.33445013  2.36681494  0.19473068 -0.26545492], action=0, reward=1.0, next_state=[1.38178643 2.16952416 0.18942158 0.08177768]\n",
      "[ episode 317 ][ timestamp 139 ] state=[1.38178643 2.16952416 0.18942158 0.08177768], action=1, reward=1.0, next_state=[ 1.42517691  2.36149727  0.19105713 -0.14567382]\n",
      "[ episode 317 ][ timestamp 140 ] state=[ 1.42517691  2.36149727  0.19105713 -0.14567382], action=1, reward=1.0, next_state=[ 1.47240685  2.55344284  0.18814366 -0.37252354]\n",
      "[ episode 317 ][ timestamp 141 ] state=[ 1.47240685  2.55344284  0.18814366 -0.37252354], action=0, reward=1.0, next_state=[ 1.52347571  2.35621602  0.18069318 -0.02691549]\n",
      "[ episode 317 ][ timestamp 142 ] state=[ 1.52347571  2.35621602  0.18069318 -0.02691549], action=1, reward=1.0, next_state=[ 1.57060003  2.54834869  0.18015487 -0.25758725]\n",
      "[ episode 317 ][ timestamp 143 ] state=[ 1.57060003  2.54834869  0.18015487 -0.25758725], action=0, reward=1.0, next_state=[1.62156701 2.35117341 0.17500313 0.08606853]\n",
      "[ episode 317 ][ timestamp 144 ] state=[1.62156701 2.35117341 0.17500313 0.08606853], action=1, reward=1.0, next_state=[ 1.66859047  2.54341134  0.1767245  -0.1466953 ]\n",
      "[ episode 317 ][ timestamp 145 ] state=[ 1.66859047  2.54341134  0.1767245  -0.1466953 ], action=1, reward=1.0, next_state=[ 1.7194587   2.73562024  0.17379059 -0.37883116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 317 ][ timestamp 146 ] state=[ 1.7194587   2.73562024  0.17379059 -0.37883116], action=1, reward=1.0, next_state=[ 1.77417111  2.92790319  0.16621397 -0.61207325]\n",
      "[ episode 317 ][ timestamp 147 ] state=[ 1.77417111  2.92790319  0.16621397 -0.61207325], action=1, reward=1.0, next_state=[ 1.83272917  3.12036005  0.15397251 -0.84813772]\n",
      "[ episode 317 ][ timestamp 148 ] state=[ 1.83272917  3.12036005  0.15397251 -0.84813772], action=1, reward=1.0, next_state=[ 1.89513637  3.31308449  0.13700975 -1.08871512]\n",
      "[ episode 317 ][ timestamp 149 ] state=[ 1.89513637  3.31308449  0.13700975 -1.08871512], action=1, reward=1.0, next_state=[ 1.96139806  3.50616042  0.11523545 -1.33546003]\n",
      "[ episode 317 ][ timestamp 150 ] state=[ 1.96139806  3.50616042  0.11523545 -1.33546003], action=1, reward=1.0, next_state=[ 2.03152127  3.69965719  0.08852625 -1.58997593]\n",
      "[ episode 317 ][ timestamp 151 ] state=[ 2.03152127  3.69965719  0.08852625 -1.58997593], action=1, reward=1.0, next_state=[ 2.10551441  3.89362329  0.05672673 -1.85379301]\n",
      "[ episode 317 ][ timestamp 152 ] state=[ 2.10551441  3.89362329  0.05672673 -1.85379301], action=1, reward=1.0, next_state=[ 2.18338688  4.08807779  0.01965087 -2.12833687]\n",
      "[ episode 317 ][ timestamp 153 ] state=[ 2.18338688  4.08807779  0.01965087 -2.12833687], action=1, reward=1.0, next_state=[ 2.26514843  4.28299932 -0.02291587 -2.41488573]\n",
      "[ episode 317 ][ timestamp 154 ] state=[ 2.26514843  4.28299932 -0.02291587 -2.41488573], action=1, reward=1.0, next_state=[ 2.35080842  4.47831194 -0.07121358 -2.71451441]\n",
      "[ episode 317 ][ timestamp 155 ] state=[ 2.35080842  4.47831194 -0.07121358 -2.71451441], action=1, reward=-1.0, next_state=[ 2.44037466  4.67386779 -0.12550387 -3.0280238 ]\n",
      "[ Ended! ] Episode 317: Exploration_rate=0.20516038984972615. Score=155.\n",
      "[ Experience replay ] starts\n",
      "[ episode 318 ] state=[ 0.03686101 -0.04620754 -0.04448619 -0.04701657]\n",
      "[ episode 318 ][ timestamp 1 ] state=[ 0.03686101 -0.04620754 -0.04448619 -0.04701657], action=1, reward=1.0, next_state=[ 0.03593686  0.14952314 -0.04542652 -0.35339675]\n",
      "[ episode 318 ][ timestamp 2 ] state=[ 0.03593686  0.14952314 -0.04542652 -0.35339675], action=1, reward=1.0, next_state=[ 0.03892732  0.34526061 -0.05249446 -0.66005088]\n",
      "[ episode 318 ][ timestamp 3 ] state=[ 0.03892732  0.34526061 -0.05249446 -0.66005088], action=1, reward=1.0, next_state=[ 0.04583253  0.54107228 -0.06569547 -0.96879006]\n",
      "[ episode 318 ][ timestamp 4 ] state=[ 0.04583253  0.54107228 -0.06569547 -0.96879006], action=0, reward=1.0, next_state=[ 0.05665398  0.34689091 -0.08507128 -0.69744691]\n",
      "[ episode 318 ][ timestamp 5 ] state=[ 0.05665398  0.34689091 -0.08507128 -0.69744691], action=0, reward=1.0, next_state=[ 0.0635918   0.15304521 -0.09902021 -0.43271069]\n",
      "[ episode 318 ][ timestamp 6 ] state=[ 0.0635918   0.15304521 -0.09902021 -0.43271069], action=0, reward=1.0, next_state=[ 0.0666527  -0.04054553 -0.10767443 -0.17281142]\n",
      "[ episode 318 ][ timestamp 7 ] state=[ 0.0666527  -0.04054553 -0.10767443 -0.17281142], action=1, reward=1.0, next_state=[ 0.06584179  0.15593955 -0.11113066 -0.49742733]\n",
      "[ episode 318 ][ timestamp 8 ] state=[ 0.06584179  0.15593955 -0.11113066 -0.49742733], action=1, reward=1.0, next_state=[ 0.06896058  0.35243858 -0.1210792  -0.82296288]\n",
      "[ episode 318 ][ timestamp 9 ] state=[ 0.06896058  0.35243858 -0.1210792  -0.82296288], action=1, reward=1.0, next_state=[ 0.07600935  0.54899044 -0.13753846 -1.15114257]\n",
      "[ episode 318 ][ timestamp 10 ] state=[ 0.07600935  0.54899044 -0.13753846 -1.15114257], action=0, reward=1.0, next_state=[ 0.08698916  0.35590454 -0.16056131 -0.90455777]\n",
      "[ episode 318 ][ timestamp 11 ] state=[ 0.08698916  0.35590454 -0.16056131 -0.90455777], action=0, reward=1.0, next_state=[ 0.09410725  0.16327852 -0.17865247 -0.66633763]\n",
      "[ episode 318 ][ timestamp 12 ] state=[ 0.09410725  0.16327852 -0.17865247 -0.66633763], action=1, reward=1.0, next_state=[ 0.09737282  0.36037605 -0.19197922 -1.0095233 ]\n",
      "[ episode 318 ][ timestamp 13 ] state=[ 0.09737282  0.36037605 -0.19197922 -1.0095233 ], action=0, reward=-1.0, next_state=[ 0.10458034  0.16826225 -0.21216969 -0.78274255]\n",
      "[ Ended! ] Episode 318: Exploration_rate=0.2041345879004775. Score=13.\n",
      "[ Experience replay ] starts\n",
      "[ episode 319 ] state=[ 0.01752308 -0.00386657 -0.02201242  0.0198224 ]\n",
      "[ episode 319 ][ timestamp 1 ] state=[ 0.01752308 -0.00386657 -0.02201242  0.0198224 ], action=1, reward=1.0, next_state=[ 0.01744575  0.19156403 -0.02161598 -0.27972362]\n",
      "[ episode 319 ][ timestamp 2 ] state=[ 0.01744575  0.19156403 -0.02161598 -0.27972362], action=1, reward=1.0, next_state=[ 0.02127703  0.38698756 -0.02721045 -0.57914503]\n",
      "[ episode 319 ][ timestamp 3 ] state=[ 0.02127703  0.38698756 -0.02721045 -0.57914503], action=0, reward=1.0, next_state=[ 0.02901678  0.1922573  -0.03879335 -0.29515665]\n",
      "[ episode 319 ][ timestamp 4 ] state=[ 0.02901678  0.1922573  -0.03879335 -0.29515665], action=0, reward=1.0, next_state=[ 0.03286192 -0.00229074 -0.04469648 -0.01495653]\n",
      "[ episode 319 ][ timestamp 5 ] state=[ 0.03286192 -0.00229074 -0.04469648 -0.01495653], action=1, reward=1.0, next_state=[ 0.03281611  0.19344277 -0.04499561 -0.32139996]\n",
      "[ episode 319 ][ timestamp 6 ] state=[ 0.03281611  0.19344277 -0.04499561 -0.32139996], action=0, reward=1.0, next_state=[ 0.03668496 -0.0010105  -0.05142361 -0.04323953]\n",
      "[ episode 319 ][ timestamp 7 ] state=[ 0.03668496 -0.0010105  -0.05142361 -0.04323953], action=1, reward=1.0, next_state=[ 0.03666475  0.19480969 -0.0522884  -0.35169341]\n",
      "[ episode 319 ][ timestamp 8 ] state=[ 0.03666475  0.19480969 -0.0522884  -0.35169341], action=0, reward=1.0, next_state=[ 0.04056095  0.0004688  -0.05932227 -0.07594628]\n",
      "[ episode 319 ][ timestamp 9 ] state=[ 0.04056095  0.0004688  -0.05932227 -0.07594628], action=1, reward=1.0, next_state=[ 0.04057032  0.19638881 -0.0608412  -0.38673986]\n",
      "[ episode 319 ][ timestamp 10 ] state=[ 0.04057032  0.19638881 -0.0608412  -0.38673986], action=0, reward=1.0, next_state=[ 0.0444981   0.00218092 -0.06857599 -0.11384331]\n",
      "[ episode 319 ][ timestamp 11 ] state=[ 0.0444981   0.00218092 -0.06857599 -0.11384331], action=1, reward=1.0, next_state=[ 0.04454172  0.19821505 -0.07085286 -0.42734891]\n",
      "[ episode 319 ][ timestamp 12 ] state=[ 0.04454172  0.19821505 -0.07085286 -0.42734891], action=0, reward=1.0, next_state=[ 0.04850602  0.00416439 -0.07939984 -0.15781654]\n",
      "[ episode 319 ][ timestamp 13 ] state=[ 0.04850602  0.00416439 -0.07939984 -0.15781654], action=1, reward=1.0, next_state=[ 0.04858931  0.20032804 -0.08255617 -0.47445403]\n",
      "[ episode 319 ][ timestamp 14 ] state=[ 0.04858931  0.20032804 -0.08255617 -0.47445403], action=0, reward=1.0, next_state=[ 0.05259587  0.00646305 -0.09204525 -0.2088909 ]\n",
      "[ episode 319 ][ timestamp 15 ] state=[ 0.05259587  0.00646305 -0.09204525 -0.2088909 ], action=1, reward=1.0, next_state=[ 0.05272513  0.20277235 -0.09622307 -0.52913144]\n",
      "[ episode 319 ][ timestamp 16 ] state=[ 0.05272513  0.20277235 -0.09622307 -0.52913144], action=0, reward=1.0, next_state=[ 0.05678058  0.00912642 -0.1068057  -0.26825217]\n",
      "[ episode 319 ][ timestamp 17 ] state=[ 0.05678058  0.00912642 -0.1068057  -0.26825217], action=1, reward=1.0, next_state=[ 0.05696311  0.20559764 -0.11217074 -0.59262088]\n",
      "[ episode 319 ][ timestamp 18 ] state=[ 0.05696311  0.20559764 -0.11217074 -0.59262088], action=0, reward=1.0, next_state=[ 0.06107506  0.01220987 -0.12402316 -0.33727133]\n",
      "[ episode 319 ][ timestamp 19 ] state=[ 0.06107506  0.01220987 -0.12402316 -0.33727133], action=0, reward=1.0, next_state=[ 0.06131926 -0.18094903 -0.13076858 -0.08612787]\n",
      "[ episode 319 ][ timestamp 20 ] state=[ 0.06131926 -0.18094903 -0.13076858 -0.08612787], action=1, reward=1.0, next_state=[ 0.05770027  0.01578132 -0.13249114 -0.41704035]\n",
      "[ episode 319 ][ timestamp 21 ] state=[ 0.05770027  0.01578132 -0.13249114 -0.41704035], action=1, reward=1.0, next_state=[ 0.0580159   0.21250761 -0.14083195 -0.74838212]\n",
      "[ episode 319 ][ timestamp 22 ] state=[ 0.0580159   0.21250761 -0.14083195 -0.74838212], action=0, reward=1.0, next_state=[ 0.06226605  0.01958023 -0.15579959 -0.50312401]\n",
      "[ episode 319 ][ timestamp 23 ] state=[ 0.06226605  0.01958023 -0.15579959 -0.50312401], action=1, reward=1.0, next_state=[ 0.06265766  0.21651532 -0.16586207 -0.84056865]\n",
      "[ episode 319 ][ timestamp 24 ] state=[ 0.06265766  0.21651532 -0.16586207 -0.84056865], action=0, reward=1.0, next_state=[ 0.06698796  0.02399886 -0.18267344 -0.60429717]\n",
      "[ episode 319 ][ timestamp 25 ] state=[ 0.06698796  0.02399886 -0.18267344 -0.60429717], action=1, reward=1.0, next_state=[ 0.06746794  0.22114197 -0.19475939 -0.94849941]\n",
      "[ episode 319 ][ timestamp 26 ] state=[ 0.06746794  0.22114197 -0.19475939 -0.94849941], action=0, reward=-1.0, next_state=[ 0.07189078  0.0290995  -0.21372938 -0.72277971]\n",
      "[ Ended! ] Episode 319: Exploration_rate=0.2031139149609751. Score=26.\n",
      "[ Experience replay ] starts\n",
      "[ episode 320 ] state=[-0.01602912 -0.04912929  0.03934842  0.00280794]\n",
      "[ episode 320 ][ timestamp 1 ] state=[-0.01602912 -0.04912929  0.03934842  0.00280794], action=0, reward=1.0, next_state=[-0.0170117  -0.24479282  0.03940458  0.3076415 ]\n",
      "[ episode 320 ][ timestamp 2 ] state=[-0.0170117  -0.24479282  0.03940458  0.3076415 ], action=0, reward=1.0, next_state=[-0.02190756 -0.44045345  0.04555741  0.61248657]\n",
      "[ episode 320 ][ timestamp 3 ] state=[-0.02190756 -0.44045345  0.04555741  0.61248657], action=1, reward=1.0, next_state=[-0.03071663 -0.24599679  0.05780714  0.33449347]\n",
      "[ episode 320 ][ timestamp 4 ] state=[-0.03071663 -0.24599679  0.05780714  0.33449347], action=1, reward=1.0, next_state=[-0.03563656 -0.05174317  0.06449701  0.06058559]\n",
      "[ episode 320 ][ timestamp 5 ] state=[-0.03563656 -0.05174317  0.06449701  0.06058559], action=0, reward=1.0, next_state=[-0.03667143 -0.24772774  0.06570872  0.37290018]\n",
      "[ episode 320 ][ timestamp 6 ] state=[-0.03667143 -0.24772774  0.06570872  0.37290018], action=1, reward=1.0, next_state=[-0.04162598 -0.05359777  0.07316672  0.1016381 ]\n",
      "[ episode 320 ][ timestamp 7 ] state=[-0.04162598 -0.05359777  0.07316672  0.1016381 ], action=0, reward=1.0, next_state=[-0.04269794 -0.24968789  0.07519948  0.41647815]\n",
      "[ episode 320 ][ timestamp 8 ] state=[-0.04269794 -0.24968789  0.07519948  0.41647815], action=1, reward=1.0, next_state=[-0.04769169 -0.05570775  0.08352905  0.14841808]\n",
      "[ episode 320 ][ timestamp 9 ] state=[-0.04769169 -0.05570775  0.08352905  0.14841808], action=0, reward=1.0, next_state=[-0.04880585 -0.25192033  0.08649741  0.4662398 ]\n",
      "[ episode 320 ][ timestamp 10 ] state=[-0.04880585 -0.25192033  0.08649741  0.4662398 ], action=1, reward=1.0, next_state=[-0.05384426 -0.05812021  0.0958222   0.20202496]\n",
      "[ episode 320 ][ timestamp 11 ] state=[-0.05384426 -0.05812021  0.0958222   0.20202496], action=0, reward=1.0, next_state=[-0.05500666 -0.25447267  0.0998627   0.52333116]\n",
      "[ episode 320 ][ timestamp 12 ] state=[-0.05500666 -0.25447267  0.0998627   0.52333116], action=1, reward=1.0, next_state=[-0.06009611 -0.06088755  0.11032933  0.26371103]\n",
      "[ episode 320 ][ timestamp 13 ] state=[-0.06009611 -0.06088755  0.11032933  0.26371103], action=0, reward=1.0, next_state=[-0.06131387 -0.25739719  0.11560355  0.58905435]\n",
      "[ episode 320 ][ timestamp 14 ] state=[-0.06131387 -0.25739719  0.11560355  0.58905435], action=0, reward=1.0, next_state=[-0.06646181 -0.45393195  0.12738463  0.91580058]\n",
      "[ episode 320 ][ timestamp 15 ] state=[-0.06646181 -0.45393195  0.12738463  0.91580058], action=1, reward=1.0, next_state=[-0.07554045 -0.26074138  0.14570065  0.66571258]\n",
      "[ episode 320 ][ timestamp 16 ] state=[-0.07554045 -0.26074138  0.14570065  0.66571258], action=1, reward=1.0, next_state=[-0.08075528 -0.06791418  0.1590149   0.42222104]\n",
      "[ episode 320 ][ timestamp 17 ] state=[-0.08075528 -0.06791418  0.1590149   0.42222104], action=1, reward=1.0, next_state=[-0.08211356  0.12463979  0.16745932  0.18358766]\n",
      "[ episode 320 ][ timestamp 18 ] state=[-0.08211356  0.12463979  0.16745932  0.18358766], action=0, reward=1.0, next_state=[-0.07962076 -0.07243304  0.17113107  0.52406501]\n",
      "[ episode 320 ][ timestamp 19 ] state=[-0.07962076 -0.07243304  0.17113107  0.52406501], action=1, reward=1.0, next_state=[-0.08106942  0.11991973  0.18161237  0.28981778]\n",
      "[ episode 320 ][ timestamp 20 ] state=[-0.08106942  0.11991973  0.18161237  0.28981778], action=0, reward=1.0, next_state=[-0.07867103 -0.07726444  0.18740873  0.63383063]\n",
      "[ episode 320 ][ timestamp 21 ] state=[-0.07867103 -0.07726444  0.18740873  0.63383063], action=1, reward=1.0, next_state=[-0.08021632  0.11481744  0.20008534  0.40552897]\n",
      "[ episode 320 ][ timestamp 22 ] state=[-0.08021632  0.11481744  0.20008534  0.40552897], action=1, reward=1.0, next_state=[-0.07791997  0.30662333  0.20819592  0.1819934 ]\n",
      "[ episode 320 ][ timestamp 23 ] state=[-0.07791997  0.30662333  0.20819592  0.1819934 ], action=1, reward=-1.0, next_state=[-0.0717875   0.49825262  0.21183579 -0.03847493]\n",
      "[ Ended! ] Episode 320: Exploration_rate=0.20209834538617025. Score=23.\n",
      "[ Experience replay ] starts\n",
      "[ episode 321 ] state=[-0.03937171  0.02073997  0.0167814   0.00615652]\n",
      "[ episode 321 ][ timestamp 1 ] state=[-0.03937171  0.02073997  0.0167814   0.00615652], action=0, reward=1.0, next_state=[-0.03895691 -0.17461858  0.01690453  0.30408659]\n",
      "[ episode 321 ][ timestamp 2 ] state=[-0.03895691 -0.17461858  0.01690453  0.30408659], action=1, reward=1.0, next_state=[-0.04244928  0.02025843  0.02298626  0.01678253]\n",
      "[ episode 321 ][ timestamp 3 ] state=[-0.04244928  0.02025843  0.02298626  0.01678253], action=0, reward=1.0, next_state=[-0.04204411 -0.1751855   0.02332191  0.31662834]\n",
      "[ episode 321 ][ timestamp 4 ] state=[-0.04204411 -0.1751855   0.02332191  0.31662834], action=1, reward=1.0, next_state=[-0.04554782  0.01959663  0.02965448  0.03139062]\n",
      "[ episode 321 ][ timestamp 5 ] state=[-0.04554782  0.01959663  0.02965448  0.03139062], action=0, reward=1.0, next_state=[-0.04515589 -0.17593775  0.03028229  0.33328038]\n",
      "[ episode 321 ][ timestamp 6 ] state=[-0.04515589 -0.17593775  0.03028229  0.33328038], action=1, reward=1.0, next_state=[-0.04867464  0.01874039  0.0369479   0.05029868]\n",
      "[ episode 321 ][ timestamp 7 ] state=[-0.04867464  0.01874039  0.0369479   0.05029868], action=0, reward=1.0, next_state=[-0.04829983 -0.17689134  0.03795387  0.35440621]\n",
      "[ episode 321 ][ timestamp 8 ] state=[-0.04829983 -0.17689134  0.03795387  0.35440621], action=1, reward=1.0, next_state=[-0.05183766  0.01767097  0.04504199  0.07392867]\n",
      "[ episode 321 ][ timestamp 9 ] state=[-0.05183766  0.01767097  0.04504199  0.07392867], action=1, reward=1.0, next_state=[-0.05148424  0.21211922  0.04652057 -0.20421001]\n",
      "[ episode 321 ][ timestamp 10 ] state=[-0.05148424  0.21211922  0.04652057 -0.20421001], action=0, reward=1.0, next_state=[-0.04724186  0.01636393  0.04243637  0.10277736]\n",
      "[ episode 321 ][ timestamp 11 ] state=[-0.04724186  0.01636393  0.04243637  0.10277736], action=0, reward=1.0, next_state=[-0.04691458 -0.17933968  0.04449192  0.40854104]\n",
      "[ episode 321 ][ timestamp 12 ] state=[-0.04691458 -0.17933968  0.04449192  0.40854104], action=1, reward=1.0, next_state=[-0.05050137  0.01512413  0.05266274  0.1302103 ]\n",
      "[ episode 321 ][ timestamp 13 ] state=[-0.05050137  0.01512413  0.05266274  0.1302103 ], action=0, reward=1.0, next_state=[-0.05019889 -0.18071112  0.05526694  0.43903161]\n",
      "[ episode 321 ][ timestamp 14 ] state=[-0.05019889 -0.18071112  0.05526694  0.43903161], action=1, reward=1.0, next_state=[-0.05381311  0.01358685  0.06404757  0.16426986]\n",
      "[ episode 321 ][ timestamp 15 ] state=[-0.05381311  0.01358685  0.06404757  0.16426986], action=0, reward=1.0, next_state=[-0.05354138 -0.1823907   0.06733297  0.47645057]\n",
      "[ episode 321 ][ timestamp 16 ] state=[-0.05354138 -0.1823907   0.06733297  0.47645057], action=1, reward=1.0, next_state=[-0.05718919  0.01171912  0.07686198  0.20572656]\n",
      "[ episode 321 ][ timestamp 17 ] state=[-0.05718919  0.01171912  0.07686198  0.20572656], action=0, reward=1.0, next_state=[-0.05695481 -0.18441302  0.08097651  0.52163136]\n",
      "[ episode 321 ][ timestamp 18 ] state=[-0.05695481 -0.18441302  0.08097651  0.52163136], action=1, reward=1.0, next_state=[-0.06064307  0.00948132  0.09140914  0.25552395]\n",
      "[ episode 321 ][ timestamp 19 ] state=[-0.06064307  0.00948132  0.09140914  0.25552395], action=0, reward=1.0, next_state=[-0.06045344 -0.18681878  0.09651962  0.57558169]\n",
      "[ episode 321 ][ timestamp 20 ] state=[-0.06045344 -0.18681878  0.09651962  0.57558169], action=1, reward=1.0, next_state=[-0.06418982  0.00682707  0.10803125  0.31479759]\n",
      "[ episode 321 ][ timestamp 21 ] state=[-0.06418982  0.00682707  0.10803125  0.31479759], action=1, reward=1.0, next_state=[-0.06405328  0.20025761  0.11432721  0.0580427 ]\n",
      "[ episode 321 ][ timestamp 22 ] state=[-0.06405328  0.20025761  0.11432721  0.0580427 ], action=0, reward=1.0, next_state=[-0.06004812  0.00369783  0.11548806  0.3844966 ]\n",
      "[ episode 321 ][ timestamp 23 ] state=[-0.06004812  0.00369783  0.11548806  0.3844966 ], action=1, reward=1.0, next_state=[-0.05997417  0.19700701  0.12317799  0.13034245]\n",
      "[ episode 321 ][ timestamp 24 ] state=[-0.05997417  0.19700701  0.12317799  0.13034245], action=0, reward=1.0, next_state=[-5.60340262e-02  3.55577288e-04  1.25784841e-01  4.59207424e-01]\n",
      "[ episode 321 ][ timestamp 25 ] state=[-5.60340262e-02  3.55577288e-04  1.25784841e-01  4.59207424e-01], action=1, reward=1.0, next_state=[-0.05602691  0.19349595  0.13496899  0.20866903]\n",
      "[ episode 321 ][ timestamp 26 ] state=[-0.05602691  0.19349595  0.13496899  0.20866903], action=1, reward=1.0, next_state=[-0.052157    0.38645574  0.13914237 -0.03857784]\n",
      "[ episode 321 ][ timestamp 27 ] state=[-0.052157    0.38645574  0.13914237 -0.03857784], action=0, reward=1.0, next_state=[-0.04442788  0.18964116  0.13837081  0.29456679]\n",
      "[ episode 321 ][ timestamp 28 ] state=[-0.04442788  0.18964116  0.13837081  0.29456679], action=1, reward=1.0, next_state=[-0.04063506  0.38254705  0.14426215  0.04852497]\n",
      "[ episode 321 ][ timestamp 29 ] state=[-0.04063506  0.38254705  0.14426215  0.04852497], action=0, reward=1.0, next_state=[-0.03298412  0.18568279  0.14523265  0.38302   ]\n",
      "[ episode 321 ][ timestamp 30 ] state=[-0.03298412  0.18568279  0.14523265  0.38302   ], action=1, reward=1.0, next_state=[-0.02927046  0.37847631  0.15289305  0.13942269]\n",
      "[ episode 321 ][ timestamp 31 ] state=[-0.02927046  0.37847631  0.15289305  0.13942269], action=0, reward=1.0, next_state=[-0.02170093  0.1815328   0.1556815   0.47616746]\n",
      "[ episode 321 ][ timestamp 32 ] state=[-0.02170093  0.1815328   0.1556815   0.47616746], action=1, reward=1.0, next_state=[-0.01807028  0.3741534   0.16520485  0.23631656]\n",
      "[ episode 321 ][ timestamp 33 ] state=[-0.01807028  0.3741534   0.16520485  0.23631656], action=1, reward=1.0, next_state=[-1.05872108e-02  5.66577116e-01  1.69931183e-01 -3.95589221e-05]\n",
      "[ episode 321 ][ timestamp 34 ] state=[-1.05872108e-02  5.66577116e-01  1.69931183e-01 -3.95589221e-05], action=0, reward=1.0, next_state=[0.00074433 0.36947721 0.16993039 0.34107155]\n",
      "[ episode 321 ][ timestamp 35 ] state=[0.00074433 0.36947721 0.16993039 0.34107155], action=1, reward=1.0, next_state=[0.00813388 0.56182544 0.17675182 0.10642437]\n",
      "[ episode 321 ][ timestamp 36 ] state=[0.00813388 0.56182544 0.17675182 0.10642437], action=1, reward=1.0, next_state=[ 0.01937038  0.7540321   0.17888031 -0.12569888]\n",
      "[ episode 321 ][ timestamp 37 ] state=[ 0.01937038  0.7540321   0.17888031 -0.12569888], action=0, reward=1.0, next_state=[0.03445103 0.55685861 0.17636633 0.21765286]\n",
      "[ episode 321 ][ timestamp 38 ] state=[0.03445103 0.55685861 0.17636633 0.21765286], action=1, reward=1.0, next_state=[ 0.0455882   0.74907847  0.18071939 -0.01462097]\n",
      "[ episode 321 ][ timestamp 39 ] state=[ 0.0455882   0.74907847  0.18071939 -0.01462097], action=0, reward=1.0, next_state=[0.06056977 0.55188696 0.18042697 0.32919206]\n",
      "[ episode 321 ][ timestamp 40 ] state=[0.06056977 0.55188696 0.18042697 0.32919206], action=1, reward=1.0, next_state=[0.07160751 0.74404332 0.18701081 0.09839457]\n",
      "[ episode 321 ][ timestamp 41 ] state=[0.07160751 0.74404332 0.18701081 0.09839457], action=1, reward=1.0, next_state=[ 0.08648837  0.93606144  0.1889787  -0.12994942]\n",
      "[ episode 321 ][ timestamp 42 ] state=[ 0.08648837  0.93606144  0.1889787  -0.12994942], action=0, reward=1.0, next_state=[0.1052096  0.73880577 0.18637971 0.21589598]\n",
      "[ episode 321 ][ timestamp 43 ] state=[0.1052096  0.73880577 0.18637971 0.21589598], action=1, reward=1.0, next_state=[ 0.11998572  0.93084218  0.19069763 -0.01269102]\n",
      "[ episode 321 ][ timestamp 44 ] state=[ 0.11998572  0.93084218  0.19069763 -0.01269102], action=0, reward=1.0, next_state=[0.13860256 0.73356979 0.19044381 0.33357933]\n",
      "[ episode 321 ][ timestamp 45 ] state=[0.13860256 0.73356979 0.19044381 0.33357933], action=1, reward=1.0, next_state=[0.15327396 0.92554328 0.1971154  0.10647797]\n",
      "[ episode 321 ][ timestamp 46 ] state=[0.15327396 0.92554328 0.1971154  0.10647797], action=1, reward=1.0, next_state=[ 0.17178482  1.11737478  0.19924496 -0.11811985]\n",
      "[ episode 321 ][ timestamp 47 ] state=[ 0.17178482  1.11737478  0.19924496 -0.11811985], action=0, reward=1.0, next_state=[0.19413232 0.92003875 0.19688256 0.23021934]\n",
      "[ episode 321 ][ timestamp 48 ] state=[0.19413232 0.92003875 0.19688256 0.23021934], action=1, reward=1.0, next_state=[0.21253309 1.11188251 0.20148695 0.00552326]\n",
      "[ episode 321 ][ timestamp 49 ] state=[0.21253309 1.11188251 0.20148695 0.00552326], action=0, reward=1.0, next_state=[0.23477074 0.91452687 0.20159741 0.35440514]\n",
      "[ episode 321 ][ timestamp 50 ] state=[0.23477074 0.91452687 0.20159741 0.35440514], action=1, reward=1.0, next_state=[0.25306128 1.10629726 0.20868552 0.13144415]\n",
      "[ episode 321 ][ timestamp 51 ] state=[0.25306128 1.10629726 0.20868552 0.13144415], action=1, reward=-1.0, next_state=[ 0.27518723  1.2979142   0.2113144  -0.08883611]\n",
      "[ Ended! ] Episode 321: Exploration_rate=0.2010878536592394. Score=51.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 322 ] state=[ 0.03135268 -0.00976926 -0.03672819  0.02619545]\n",
      "[ episode 322 ][ timestamp 1 ] state=[ 0.03135268 -0.00976926 -0.03672819  0.02619545], action=1, reward=1.0, next_state=[ 0.03115729  0.18585963 -0.03620429 -0.27784564]\n",
      "[ episode 322 ][ timestamp 2 ] state=[ 0.03115729  0.18585963 -0.03620429 -0.27784564], action=1, reward=1.0, next_state=[ 0.03487449  0.38147887 -0.0417612  -0.58172395]\n",
      "[ episode 322 ][ timestamp 3 ] state=[ 0.03487449  0.38147887 -0.0417612  -0.58172395], action=0, reward=1.0, next_state=[ 0.04250406  0.18696616 -0.05339568 -0.3024835 ]\n",
      "[ episode 322 ][ timestamp 4 ] state=[ 0.04250406  0.18696616 -0.05339568 -0.3024835 ], action=0, reward=1.0, next_state=[ 0.04624339 -0.00735574 -0.05944535 -0.02710695]\n",
      "[ episode 322 ][ timestamp 5 ] state=[ 0.04624339 -0.00735574 -0.05944535 -0.02710695], action=1, reward=1.0, next_state=[ 0.04609627  0.1885661  -0.05998749 -0.33793724]\n",
      "[ episode 322 ][ timestamp 6 ] state=[ 0.04609627  0.1885661  -0.05998749 -0.33793724], action=0, reward=1.0, next_state=[ 0.04986759 -0.00565321 -0.06674623 -0.06475804]\n",
      "[ episode 322 ][ timestamp 7 ] state=[ 0.04986759 -0.00565321 -0.06674623 -0.06475804], action=1, reward=1.0, next_state=[ 0.04975453  0.19035907 -0.06804139 -0.37773059]\n",
      "[ episode 322 ][ timestamp 8 ] state=[ 0.04975453  0.19035907 -0.06804139 -0.37773059], action=0, reward=1.0, next_state=[ 0.05356171 -0.00373389 -0.075596   -0.10725356]\n",
      "[ episode 322 ][ timestamp 9 ] state=[ 0.05356171 -0.00373389 -0.075596   -0.10725356], action=1, reward=1.0, next_state=[ 0.05348703  0.19238539 -0.07774108 -0.42279636]\n",
      "[ episode 322 ][ timestamp 10 ] state=[ 0.05348703  0.19238539 -0.07774108 -0.42279636], action=0, reward=1.0, next_state=[ 0.05733474 -0.00155408 -0.086197   -0.15559866]\n",
      "[ episode 322 ][ timestamp 11 ] state=[ 0.05733474 -0.00155408 -0.086197   -0.15559866], action=1, reward=1.0, next_state=[ 0.05730366  0.19468949 -0.08930898 -0.47418168]\n",
      "[ episode 322 ][ timestamp 12 ] state=[ 0.05730366  0.19468949 -0.08930898 -0.47418168], action=0, reward=1.0, next_state=[ 0.06119745  0.00093477 -0.09879261 -0.21092984]\n",
      "[ episode 322 ][ timestamp 13 ] state=[ 0.06119745  0.00093477 -0.09879261 -0.21092984], action=1, reward=1.0, next_state=[ 0.06121615  0.19732028 -0.10301121 -0.53306954]\n",
      "[ episode 322 ][ timestamp 14 ] state=[ 0.06121615  0.19732028 -0.10301121 -0.53306954], action=0, reward=1.0, next_state=[ 0.06516255  0.00378646 -0.1136726  -0.27453944]\n",
      "[ episode 322 ][ timestamp 15 ] state=[ 0.06516255  0.00378646 -0.1136726  -0.27453944], action=1, reward=1.0, next_state=[ 0.06523828  0.20033127 -0.11916339 -0.60080178]\n",
      "[ episode 322 ][ timestamp 16 ] state=[ 0.06523828  0.20033127 -0.11916339 -0.60080178], action=1, reward=1.0, next_state=[ 0.06924491  0.39690092 -0.13117942 -0.92851647]\n",
      "[ episode 322 ][ timestamp 17 ] state=[ 0.06924491  0.39690092 -0.13117942 -0.92851647], action=0, reward=1.0, next_state=[ 0.07718292  0.20377053 -0.14974975 -0.67976609]\n",
      "[ episode 322 ][ timestamp 18 ] state=[ 0.07718292  0.20377053 -0.14974975 -0.67976609], action=0, reward=1.0, next_state=[ 0.08125833  0.01101092 -0.16334507 -0.43772467]\n",
      "[ episode 322 ][ timestamp 19 ] state=[ 0.08125833  0.01101092 -0.16334507 -0.43772467], action=1, reward=1.0, next_state=[ 0.08147855  0.20802242 -0.17209957 -0.7771184 ]\n",
      "[ episode 322 ][ timestamp 20 ] state=[ 0.08147855  0.20802242 -0.17209957 -0.7771184 ], action=1, reward=1.0, next_state=[ 0.085639    0.4050404  -0.18764193 -1.11862756]\n",
      "[ episode 322 ][ timestamp 21 ] state=[ 0.085639    0.4050404  -0.18764193 -1.11862756], action=1, reward=-1.0, next_state=[ 0.09373981  0.6020613  -0.21001449 -1.46381496]\n",
      "[ Ended! ] Episode 322: Exploration_rate=0.2000824143909432. Score=21.\n",
      "[ Experience replay ] starts\n",
      "[ episode 323 ] state=[-0.01506724 -0.02456203 -0.00174991 -0.01151574]\n",
      "[ episode 323 ][ timestamp 1 ] state=[-0.01506724 -0.02456203 -0.00174991 -0.01151574], action=0, reward=1.0, next_state=[-0.01555848 -0.21965884 -0.00198022  0.28061456]\n",
      "[ episode 323 ][ timestamp 2 ] state=[-0.01555848 -0.21965884 -0.00198022  0.28061456], action=1, reward=1.0, next_state=[-0.01995166 -0.0245087   0.00363207 -0.01269227]\n",
      "[ episode 323 ][ timestamp 3 ] state=[-0.01995166 -0.0245087   0.00363207 -0.01269227], action=0, reward=1.0, next_state=[-0.02044183 -0.21968255  0.00337822  0.28113441]\n",
      "[ episode 323 ][ timestamp 4 ] state=[-0.02044183 -0.21968255  0.00337822  0.28113441], action=1, reward=1.0, next_state=[-0.02483549 -0.02460895  0.00900091 -0.01048113]\n",
      "[ episode 323 ][ timestamp 5 ] state=[-0.02483549 -0.02460895  0.00900091 -0.01048113], action=0, reward=1.0, next_state=[-0.02532766 -0.21985882  0.00879129  0.28502805]\n",
      "[ episode 323 ][ timestamp 6 ] state=[-0.02532766 -0.21985882  0.00879129  0.28502805], action=1, reward=1.0, next_state=[-0.02972484 -0.02486335  0.01449185 -0.00486925]\n",
      "[ episode 323 ][ timestamp 7 ] state=[-0.02972484 -0.02486335  0.01449185 -0.00486925], action=0, reward=1.0, next_state=[-0.03022211 -0.2201901   0.01439446  0.29235057]\n",
      "[ episode 323 ][ timestamp 8 ] state=[-0.03022211 -0.2201901   0.01439446  0.29235057], action=1, reward=1.0, next_state=[-0.03462591 -0.02527632  0.02024148  0.004242  ]\n",
      "[ episode 323 ][ timestamp 9 ] state=[-0.03462591 -0.02527632  0.02024148  0.004242  ], action=0, reward=1.0, next_state=[-0.03513144 -0.22068262  0.02032632  0.30324201]\n",
      "[ episode 323 ][ timestamp 10 ] state=[-0.03513144 -0.22068262  0.02032632  0.30324201], action=1, reward=1.0, next_state=[-0.03954509 -0.02585617  0.02639116  0.01703822]\n",
      "[ episode 323 ][ timestamp 11 ] state=[-0.03954509 -0.02585617  0.02639116  0.01703822], action=0, reward=1.0, next_state=[-0.04006221 -0.22134646  0.02673192  0.31792965]\n",
      "[ episode 323 ][ timestamp 12 ] state=[-0.04006221 -0.22134646  0.02673192  0.31792965], action=1, reward=1.0, next_state=[-0.04448914 -0.02661525  0.03309051  0.03379543]\n",
      "[ episode 323 ][ timestamp 13 ] state=[-0.04448914 -0.02661525  0.03309051  0.03379543], action=1, reward=1.0, next_state=[-0.04502145  0.16801693  0.03376642 -0.24826618]\n",
      "[ episode 323 ][ timestamp 14 ] state=[-0.04502145  0.16801693  0.03376642 -0.24826618], action=0, reward=1.0, next_state=[-0.04166111 -0.02757057  0.0288011   0.05487328]\n",
      "[ episode 323 ][ timestamp 15 ] state=[-0.04166111 -0.02757057  0.0288011   0.05487328], action=1, reward=1.0, next_state=[-0.04221252  0.16712683  0.02989856 -0.22858535]\n",
      "[ episode 323 ][ timestamp 16 ] state=[-0.04221252  0.16712683  0.02989856 -0.22858535], action=0, reward=1.0, next_state=[-0.03886998 -0.02840934  0.02532686  0.07337669]\n",
      "[ episode 323 ][ timestamp 17 ] state=[-0.03886998 -0.02840934  0.02532686  0.07337669], action=1, reward=1.0, next_state=[-0.03943817  0.16634053  0.02679439 -0.21120914]\n",
      "[ episode 323 ][ timestamp 18 ] state=[-0.03943817  0.16634053  0.02679439 -0.21120914], action=0, reward=1.0, next_state=[-0.03611136 -0.02915407  0.02257021  0.08980412]\n",
      "[ episode 323 ][ timestamp 19 ] state=[-0.03611136 -0.02915407  0.02257021  0.08980412], action=1, reward=1.0, next_state=[-0.03669444  0.16563722  0.02436629 -0.19567332]\n",
      "[ episode 323 ][ timestamp 20 ] state=[-0.03669444  0.16563722  0.02436629 -0.19567332], action=0, reward=1.0, next_state=[-0.0333817  -0.02982464  0.02045282  0.10459542]\n",
      "[ episode 323 ][ timestamp 21 ] state=[-0.0333817  -0.02982464  0.02045282  0.10459542], action=1, reward=1.0, next_state=[-0.03397819  0.16499832  0.02254473 -0.18156519]\n",
      "[ episode 323 ][ timestamp 22 ] state=[-0.03397819  0.16499832  0.02254473 -0.18156519], action=0, reward=1.0, next_state=[-0.03067822 -0.03043885  0.01891343  0.11814366]\n",
      "[ episode 323 ][ timestamp 23 ] state=[-0.03067822 -0.03043885  0.01891343  0.11814366], action=1, reward=1.0, next_state=[-0.031287    0.16440708  0.0212763  -0.16851274]\n",
      "[ episode 323 ][ timestamp 24 ] state=[-0.031287    0.16440708  0.0212763  -0.16851274], action=0, reward=1.0, next_state=[-0.02799886 -0.03101286  0.01790605  0.13080557]\n",
      "[ episode 323 ][ timestamp 25 ] state=[-0.02799886 -0.03101286  0.01790605  0.13080557], action=1, reward=1.0, next_state=[-0.02861911  0.16384808  0.02052216 -0.15617488]\n",
      "[ episode 323 ][ timestamp 26 ] state=[-0.02861911  0.16384808  0.02052216 -0.15617488], action=0, reward=1.0, next_state=[-0.02534215 -0.0315616   0.01739866  0.14291101]\n",
      "[ episode 323 ][ timestamp 27 ] state=[-0.02534215 -0.0315616   0.01739866  0.14291101], action=1, reward=1.0, next_state=[-0.02597339  0.16330691  0.02025688 -0.14423257]\n",
      "[ episode 323 ][ timestamp 28 ] state=[-0.02597339  0.16330691  0.02025688 -0.14423257], action=0, reward=1.0, next_state=[-0.02270725 -0.0320992   0.01737223  0.15477158]\n",
      "[ episode 323 ][ timestamp 29 ] state=[-0.02270725 -0.0320992   0.01737223  0.15477158], action=1, reward=1.0, next_state=[-0.02334923  0.16276977  0.02046766 -0.13238058]\n",
      "[ episode 323 ][ timestamp 30 ] state=[-0.02334923  0.16276977  0.02046766 -0.13238058], action=0, reward=1.0, next_state=[-0.02009384 -0.0326393   0.01782005  0.1666887 ]\n",
      "[ episode 323 ][ timestamp 31 ] state=[-0.02009384 -0.0326393   0.01782005  0.1666887 ], action=1, reward=1.0, next_state=[-0.02074662  0.16222309  0.02115382 -0.12031967]\n",
      "[ episode 323 ][ timestamp 32 ] state=[-0.02074662  0.16222309  0.02115382 -0.12031967], action=0, reward=1.0, next_state=[-0.01750216 -0.03319545  0.01874743  0.17896132]\n",
      "[ episode 323 ][ timestamp 33 ] state=[-0.01750216 -0.03319545  0.01874743  0.17896132], action=1, reward=1.0, next_state=[-0.01816607  0.16165328  0.02232666 -0.10774899]\n",
      "[ episode 323 ][ timestamp 34 ] state=[-0.01816607  0.16165328  0.02232666 -0.10774899], action=0, reward=1.0, next_state=[-0.014933   -0.03378138  0.02017168  0.19189344]\n",
      "[ episode 323 ][ timestamp 35 ] state=[-0.014933   -0.03378138  0.02017168  0.19189344], action=1, reward=1.0, next_state=[-0.01560863  0.16104628  0.02400955 -0.09435853]\n",
      "[ episode 323 ][ timestamp 36 ] state=[-0.01560863  0.16104628  0.02400955 -0.09435853], action=0, reward=1.0, next_state=[-0.01238771 -0.03441142  0.02212238  0.20580164]\n",
      "[ episode 323 ][ timestamp 37 ] state=[-0.01238771 -0.03441142  0.02212238  0.20580164], action=1, reward=1.0, next_state=[-0.01307593  0.16038731  0.02623841 -0.0798215 ]\n",
      "[ episode 323 ][ timestamp 38 ] state=[-0.01307593  0.16038731  0.02623841 -0.0798215 ], action=0, reward=1.0, next_state=[-0.00986819 -0.03510076  0.02464198  0.22102287]\n",
      "[ episode 323 ][ timestamp 39 ] state=[-0.00986819 -0.03510076  0.02464198  0.22102287], action=1, reward=1.0, next_state=[-0.0105702   0.15966045  0.02906244 -0.06378625]\n",
      "[ episode 323 ][ timestamp 40 ] state=[-0.0105702   0.15966045  0.02906244 -0.06378625], action=0, reward=1.0, next_state=[-0.00737699 -0.03586586  0.02778671  0.23792253]\n",
      "[ episode 323 ][ timestamp 41 ] state=[-0.00737699 -0.03586586  0.02778671  0.23792253], action=1, reward=1.0, next_state=[-0.00809431  0.15884833  0.03254516 -0.04586776]\n",
      "[ episode 323 ][ timestamp 42 ] state=[-0.00809431  0.15884833  0.03254516 -0.04586776], action=0, reward=1.0, next_state=[-0.00491734 -0.03672482  0.03162781  0.2569032 ]\n",
      "[ episode 323 ][ timestamp 43 ] state=[-0.00491734 -0.03672482  0.03162781  0.2569032 ], action=1, reward=1.0, next_state=[-0.00565184  0.15793164  0.03676587 -0.02563844]\n",
      "[ episode 323 ][ timestamp 44 ] state=[-0.00565184  0.15793164  0.03676587 -0.02563844], action=1, reward=1.0, next_state=[-0.00249321  0.35250757  0.0362531  -0.30649837]\n",
      "[ episode 323 ][ timestamp 45 ] state=[-0.00249321  0.35250757  0.0362531  -0.30649837], action=0, reward=1.0, next_state=[ 0.00455694  0.15688828  0.03012313 -0.00260616]\n",
      "[ episode 323 ][ timestamp 46 ] state=[ 0.00455694  0.15688828  0.03012313 -0.00260616], action=1, reward=1.0, next_state=[ 0.00769471  0.35156556  0.03007101 -0.28563474]\n",
      "[ episode 323 ][ timestamp 47 ] state=[ 0.00769471  0.35156556  0.03007101 -0.28563474], action=0, reward=1.0, next_state=[0.01472602 0.15602794 0.02435832 0.01637864]\n",
      "[ episode 323 ][ timestamp 48 ] state=[0.01472602 0.15602794 0.02435832 0.01637864], action=1, reward=1.0, next_state=[ 0.01784658  0.35079224  0.02468589 -0.26852052]\n",
      "[ episode 323 ][ timestamp 49 ] state=[ 0.01784658  0.35079224  0.02468589 -0.26852052], action=0, reward=1.0, next_state=[0.02486242 0.15532685 0.01931548 0.03184514]\n",
      "[ episode 323 ][ timestamp 50 ] state=[0.02486242 0.15532685 0.01931548 0.03184514], action=0, reward=1.0, next_state=[ 0.02796896 -0.04006669  0.01995238  0.33055918]\n",
      "[ episode 323 ][ timestamp 51 ] state=[ 0.02796896 -0.04006669  0.01995238  0.33055918], action=1, reward=1.0, next_state=[0.02716763 0.15476565 0.02656356 0.04423446]\n",
      "[ episode 323 ][ timestamp 52 ] state=[0.02716763 0.15476565 0.02656356 0.04423446], action=1, reward=1.0, next_state=[ 0.03026294  0.34949681  0.02744825 -0.23995047]\n",
      "[ episode 323 ][ timestamp 53 ] state=[ 0.03026294  0.34949681  0.02744825 -0.23995047], action=0, reward=1.0, next_state=[0.03725288 0.15399373 0.02264924 0.06126247]\n",
      "[ episode 323 ][ timestamp 54 ] state=[0.03725288 0.15399373 0.02264924 0.06126247], action=1, reward=1.0, next_state=[ 0.04033275  0.34878374  0.02387449 -0.2241893 ]\n",
      "[ episode 323 ][ timestamp 55 ] state=[ 0.04033275  0.34878374  0.02387449 -0.2241893 ], action=0, reward=1.0, next_state=[0.04730843 0.15332885 0.01939071 0.07592792]\n",
      "[ episode 323 ][ timestamp 56 ] state=[0.04730843 0.15332885 0.01939071 0.07592792], action=1, reward=1.0, next_state=[ 0.050375    0.34816752  0.02090927 -0.21057464]\n",
      "[ episode 323 ][ timestamp 57 ] state=[ 0.050375    0.34816752  0.02090927 -0.21057464], action=0, reward=1.0, next_state=[0.05733835 0.15275295 0.01669777 0.08863003]\n",
      "[ episode 323 ][ timestamp 58 ] state=[0.05733835 0.15275295 0.01669777 0.08863003], action=1, reward=1.0, next_state=[ 0.06039341  0.34763162  0.01847037 -0.19873832]\n",
      "[ episode 323 ][ timestamp 59 ] state=[ 0.06039341  0.34763162  0.01847037 -0.19873832], action=0, reward=1.0, next_state=[0.06734604 0.15225043 0.01449561 0.09971346]\n",
      "[ episode 323 ][ timestamp 60 ] state=[0.06734604 0.15225043 0.01449561 0.09971346], action=1, reward=1.0, next_state=[ 0.07039105  0.34716167  0.01648988 -0.18836112]\n",
      "[ episode 323 ][ timestamp 61 ] state=[ 0.07039105  0.34716167  0.01648988 -0.18836112], action=0, reward=1.0, next_state=[0.07733429 0.15180773 0.01272265 0.10947776]\n",
      "[ episode 323 ][ timestamp 62 ] state=[0.07733429 0.15180773 0.01272265 0.10947776], action=1, reward=1.0, next_state=[ 0.08037044  0.34674507  0.01491221 -0.17916424]\n",
      "[ episode 323 ][ timestamp 63 ] state=[ 0.08037044  0.34674507  0.01491221 -0.17916424], action=1, reward=1.0, next_state=[ 0.08730534  0.54165049  0.01132892 -0.46710583]\n",
      "[ episode 323 ][ timestamp 64 ] state=[ 0.08730534  0.54165049  0.01132892 -0.46710583], action=0, reward=1.0, next_state=[ 0.09813835  0.34637033  0.00198681 -0.17087375]\n",
      "[ episode 323 ][ timestamp 65 ] state=[ 0.09813835  0.34637033  0.00198681 -0.17087375], action=0, reward=1.0, next_state=[ 0.10506576  0.15121999 -0.00143067  0.12243529]\n",
      "[ episode 323 ][ timestamp 66 ] state=[ 0.10506576  0.15121999 -0.00143067  0.12243529], action=1, reward=1.0, next_state=[ 0.10809016  0.34636241  0.00101804 -0.17069865]\n",
      "[ episode 323 ][ timestamp 67 ] state=[ 0.10809016  0.34636241  0.00101804 -0.17069865], action=1, reward=1.0, next_state=[ 0.11501741  0.54146978 -0.00239593 -0.46306025]\n",
      "[ episode 323 ][ timestamp 68 ] state=[ 0.11501741  0.54146978 -0.00239593 -0.46306025], action=0, reward=1.0, next_state=[ 0.1258468   0.34638177 -0.01165714 -0.17113348]\n",
      "[ episode 323 ][ timestamp 69 ] state=[ 0.1258468   0.34638177 -0.01165714 -0.17113348], action=0, reward=1.0, next_state=[ 0.13277444  0.15142859 -0.01507981  0.1178493 ]\n",
      "[ episode 323 ][ timestamp 70 ] state=[ 0.13277444  0.15142859 -0.01507981  0.1178493 ], action=1, reward=1.0, next_state=[ 0.13580301  0.34676332 -0.01272282 -0.17955277]\n",
      "[ episode 323 ][ timestamp 71 ] state=[ 0.13580301  0.34676332 -0.01272282 -0.17955277], action=0, reward=1.0, next_state=[ 0.14273828  0.15182572 -0.01631388  0.10908955]\n",
      "[ episode 323 ][ timestamp 72 ] state=[ 0.14273828  0.15182572 -0.01631388  0.10908955], action=1, reward=1.0, next_state=[ 0.14577479  0.3471776  -0.01413209 -0.18869535]\n",
      "[ episode 323 ][ timestamp 73 ] state=[ 0.14577479  0.3471776  -0.01413209 -0.18869535], action=0, reward=1.0, next_state=[ 0.15271834  0.15226065 -0.01790599  0.09949618]\n",
      "[ episode 323 ][ timestamp 74 ] state=[ 0.15271834  0.15226065 -0.01790599  0.09949618], action=1, reward=1.0, next_state=[ 0.15576356  0.34763459 -0.01591607 -0.19878183]\n",
      "[ episode 323 ][ timestamp 75 ] state=[ 0.15576356  0.34763459 -0.01591607 -0.19878183], action=0, reward=1.0, next_state=[ 0.16271625  0.15274386 -0.01989171  0.08883811]\n",
      "[ episode 323 ][ timestamp 76 ] state=[ 0.16271625  0.15274386 -0.01989171  0.08883811], action=0, reward=1.0, next_state=[ 0.16577112 -0.0420874  -0.01811495  0.37517942]\n",
      "[ episode 323 ][ timestamp 77 ] state=[ 0.16577112 -0.0420874  -0.01811495  0.37517942], action=1, reward=1.0, next_state=[ 0.16492938  0.15328711 -0.01061136  0.07684023]\n",
      "[ episode 323 ][ timestamp 78 ] state=[ 0.16492938  0.15328711 -0.01061136  0.07684023], action=0, reward=1.0, next_state=[ 0.16799512 -0.04168113 -0.00907455  0.36615644]\n",
      "[ episode 323 ][ timestamp 79 ] state=[ 0.16799512 -0.04168113 -0.00907455  0.36615644], action=1, reward=1.0, next_state=[ 0.1671615   0.1535686  -0.00175142  0.07062603]\n",
      "[ episode 323 ][ timestamp 80 ] state=[ 0.1671615   0.1535686  -0.00175142  0.07062603], action=1, reward=1.0, next_state=[ 1.70232868e-01  3.48715614e-01 -3.38902750e-04 -2.22608964e-01]\n",
      "[ episode 323 ][ timestamp 81 ] state=[ 1.70232868e-01  3.48715614e-01 -3.38902750e-04 -2.22608964e-01], action=0, reward=1.0, next_state=[ 0.17720718  0.15359851 -0.00479108  0.06996704]\n",
      "[ episode 323 ][ timestamp 82 ] state=[ 0.17720718  0.15359851 -0.00479108  0.06996704], action=0, reward=1.0, next_state=[ 0.18027915 -0.04145443 -0.00339174  0.36113451]\n",
      "[ episode 323 ][ timestamp 83 ] state=[ 0.18027915 -0.04145443 -0.00339174  0.36113451], action=1, reward=1.0, next_state=[0.17945006 0.15371557 0.00383095 0.06738403]\n",
      "[ episode 323 ][ timestamp 84 ] state=[0.17945006 0.15371557 0.00383095 0.06738403], action=1, reward=1.0, next_state=[ 0.18252437  0.34878239  0.00517863 -0.22408775]\n",
      "[ episode 323 ][ timestamp 85 ] state=[ 0.18252437  0.34878239  0.00517863 -0.22408775], action=0, reward=1.0, next_state=[0.18950002 0.15358681 0.00069687 0.07022421]\n",
      "[ episode 323 ][ timestamp 86 ] state=[0.18950002 0.15358681 0.00069687 0.07022421], action=1, reward=1.0, next_state=[ 0.19257176  0.34869876  0.00210136 -0.22223877]\n",
      "[ episode 323 ][ timestamp 87 ] state=[ 0.19257176  0.34869876  0.00210136 -0.22223877], action=0, reward=1.0, next_state=[ 0.19954573  0.15354684 -0.00234342  0.07110627]\n",
      "[ episode 323 ][ timestamp 88 ] state=[ 0.19954573  0.15354684 -0.00234342  0.07110627], action=1, reward=1.0, next_state=[ 0.20261667  0.3487023  -0.00092129 -0.2223151 ]\n",
      "[ episode 323 ][ timestamp 89 ] state=[ 0.20261667  0.3487023  -0.00092129 -0.2223151 ], action=0, reward=1.0, next_state=[ 0.20959072  0.15359353 -0.00536759  0.07007707]\n",
      "[ episode 323 ][ timestamp 90 ] state=[ 0.20959072  0.15359353 -0.00536759  0.07007707], action=0, reward=1.0, next_state=[ 0.21266259 -0.04145105 -0.00396605  0.36106168]\n",
      "[ episode 323 ][ timestamp 91 ] state=[ 0.21266259 -0.04145105 -0.00396605  0.36106168], action=0, reward=1.0, next_state=[ 0.21183356 -0.23651641  0.00325518  0.65249139]\n",
      "[ episode 323 ][ timestamp 92 ] state=[ 0.21183356 -0.23651641  0.00325518  0.65249139], action=1, reward=1.0, next_state=[ 0.20710324 -0.04143994  0.01630501  0.36083526]\n",
      "[ episode 323 ][ timestamp 93 ] state=[ 0.20710324 -0.04143994  0.01630501  0.36083526], action=1, reward=1.0, next_state=[0.20627444 0.1534465  0.02352171 0.07333792]\n",
      "[ episode 323 ][ timestamp 94 ] state=[0.20627444 0.1534465  0.02352171 0.07333792], action=1, reward=1.0, next_state=[ 0.20934337  0.34822348  0.02498847 -0.21183198]\n",
      "[ episode 323 ][ timestamp 95 ] state=[ 0.20934337  0.34822348  0.02498847 -0.21183198], action=0, reward=1.0, next_state=[0.21630784 0.15275333 0.02075183 0.08862756]\n",
      "[ episode 323 ][ timestamp 96 ] state=[0.21630784 0.15275333 0.02075183 0.08862756], action=1, reward=1.0, next_state=[ 0.2193629   0.34757177  0.02252438 -0.19743659]\n",
      "[ episode 323 ][ timestamp 97 ] state=[ 0.2193629   0.34757177  0.02252438 -0.19743659], action=0, reward=1.0, next_state=[0.22631434 0.15213501 0.01857565 0.1022658 ]\n",
      "[ episode 323 ][ timestamp 98 ] state=[0.22631434 0.15213501 0.01857565 0.1022658 ], action=1, reward=1.0, next_state=[ 0.22935704  0.34698589  0.02062097 -0.18449917]\n",
      "[ episode 323 ][ timestamp 99 ] state=[ 0.22935704  0.34698589  0.02062097 -0.18449917], action=0, reward=1.0, next_state=[0.23629676 0.15157505 0.01693099 0.11461691]\n",
      "[ episode 323 ][ timestamp 100 ] state=[0.23629676 0.15157505 0.01693099 0.11461691], action=1, reward=1.0, next_state=[ 0.23932826  0.34645036  0.01922332 -0.17267669]\n",
      "[ episode 323 ][ timestamp 101 ] state=[ 0.23932826  0.34645036  0.01922332 -0.17267669], action=0, reward=1.0, next_state=[0.24625727 0.15105863 0.01576979 0.12600806]\n",
      "[ episode 323 ][ timestamp 102 ] state=[0.24625727 0.15105863 0.01576979 0.12600806], action=1, reward=1.0, next_state=[ 0.24927844  0.34595116  0.01828995 -0.16165825]\n",
      "[ episode 323 ][ timestamp 103 ] state=[ 0.24927844  0.34595116  0.01828995 -0.16165825], action=0, reward=1.0, next_state=[0.25619746 0.15057221 0.01505679 0.1367381 ]\n",
      "[ episode 323 ][ timestamp 104 ] state=[0.25619746 0.15057221 0.01505679 0.1367381 ], action=1, reward=1.0, next_state=[ 0.25920891  0.3454753   0.01779155 -0.15115687]\n",
      "[ episode 323 ][ timestamp 105 ] state=[ 0.25920891  0.3454753   0.01779155 -0.15115687], action=0, reward=1.0, next_state=[0.26611841 0.15010316 0.01476841 0.14708539]\n",
      "[ episode 323 ][ timestamp 106 ] state=[0.26611841 0.15010316 0.01476841 0.14708539], action=1, reward=1.0, next_state=[ 0.26912047  0.34501055  0.01771012 -0.14090205]\n",
      "[ episode 323 ][ timestamp 107 ] state=[ 0.26912047  0.34501055  0.01771012 -0.14090205], action=0, reward=1.0, next_state=[0.27602069 0.14963948 0.01489208 0.15731509]\n",
      "[ episode 323 ][ timestamp 108 ] state=[0.27602069 0.14963948 0.01489208 0.15731509], action=1, reward=1.0, next_state=[ 0.27901348  0.34454509  0.01803838 -0.13063279]\n",
      "[ episode 323 ][ timestamp 109 ] state=[ 0.27901348  0.34454509  0.01803838 -0.13063279], action=0, reward=1.0, next_state=[0.28590438 0.14916945 0.01542572 0.16768599]\n",
      "[ episode 323 ][ timestamp 110 ] state=[0.28590438 0.14916945 0.01542572 0.16768599], action=1, reward=1.0, next_state=[ 0.28888777  0.34406724  0.01877944 -0.12009093]\n",
      "[ episode 323 ][ timestamp 111 ] state=[ 0.28888777  0.34406724  0.01877944 -0.12009093], action=0, reward=1.0, next_state=[0.29576911 0.14868133 0.01637762 0.17845708]\n",
      "[ episode 323 ][ timestamp 112 ] state=[0.29576911 0.14868133 0.01637762 0.17845708], action=1, reward=1.0, next_state=[ 0.29874274  0.34356513  0.01994677 -0.1090146 ]\n",
      "[ episode 323 ][ timestamp 113 ] state=[ 0.29874274  0.34356513  0.01994677 -0.1090146 ], action=0, reward=1.0, next_state=[0.30561404 0.14816311 0.01776647 0.18989408]\n",
      "[ episode 323 ][ timestamp 114 ] state=[0.30561404 0.14816311 0.01776647 0.18989408], action=1, reward=1.0, next_state=[ 0.3085773   0.34302644  0.02156436 -0.09713172]\n",
      "[ episode 323 ][ timestamp 115 ] state=[ 0.3085773   0.34302644  0.02156436 -0.09713172], action=0, reward=1.0, next_state=[0.31543783 0.14760217 0.01962172 0.20227597]\n",
      "[ episode 323 ][ timestamp 116 ] state=[0.31543783 0.14760217 0.01962172 0.20227597], action=1, reward=1.0, next_state=[ 0.31838987  0.34243808  0.02366724 -0.08415323]\n",
      "[ episode 323 ][ timestamp 117 ] state=[ 0.31838987  0.34243808  0.02366724 -0.08415323], action=0, reward=1.0, next_state=[0.32523864 0.14698501 0.02198418 0.2159018 ]\n",
      "[ episode 323 ][ timestamp 118 ] state=[0.32523864 0.14698501 0.02198418 0.2159018 ], action=1, reward=1.0, next_state=[ 0.32817834  0.34178589  0.02630221 -0.06976609]\n",
      "[ episode 323 ][ timestamp 119 ] state=[ 0.32817834  0.34178589  0.02630221 -0.06976609], action=0, reward=1.0, next_state=[0.33501405 0.14629692 0.02490689 0.2310979 ]\n",
      "[ episode 323 ][ timestamp 120 ] state=[0.33501405 0.14629692 0.02490689 0.2310979 ], action=1, reward=1.0, next_state=[ 0.33793999  0.34105427  0.02952885 -0.05362565]\n",
      "[ episode 323 ][ timestamp 121 ] state=[ 0.33793999  0.34105427  0.02952885 -0.05362565], action=0, reward=1.0, next_state=[0.34476108 0.14552163 0.02845634 0.24822566]\n",
      "[ episode 323 ][ timestamp 122 ] state=[0.34476108 0.14552163 0.02845634 0.24822566], action=0, reward=1.0, next_state=[ 0.34767151 -0.04999491  0.03342085  0.54974678]\n",
      "[ episode 323 ][ timestamp 123 ] state=[ 0.34767151 -0.04999491  0.03342085  0.54974678], action=1, reward=1.0, next_state=[0.34667161 0.14464205 0.04441578 0.26777828]\n",
      "[ episode 323 ][ timestamp 124 ] state=[0.34667161 0.14464205 0.04441578 0.26777828], action=1, reward=1.0, next_state=[ 0.34956445  0.3391029   0.04977135 -0.01057138]\n",
      "[ episode 323 ][ timestamp 125 ] state=[ 0.34956445  0.3391029   0.04977135 -0.01057138], action=1, reward=1.0, next_state=[ 0.35634651  0.53347704  0.04955992 -0.2871448 ]\n",
      "[ episode 323 ][ timestamp 126 ] state=[ 0.35634651  0.53347704  0.04955992 -0.2871448 ], action=0, reward=1.0, next_state=[0.36701605 0.33768463 0.04381703 0.02074785]\n",
      "[ episode 323 ][ timestamp 127 ] state=[0.36701605 0.33768463 0.04381703 0.02074785], action=1, reward=1.0, next_state=[ 0.37376974  0.53215171  0.04423198 -0.25779469]\n",
      "[ episode 323 ][ timestamp 128 ] state=[ 0.37376974  0.53215171  0.04423198 -0.25779469], action=0, reward=1.0, next_state=[0.38441278 0.3364271  0.03907609 0.04850503]\n",
      "[ episode 323 ][ timestamp 129 ] state=[0.38441278 0.3364271  0.03907609 0.04850503], action=1, reward=1.0, next_state=[ 0.39114132  0.53096758  0.04004619 -0.23159748]\n",
      "[ episode 323 ][ timestamp 130 ] state=[ 0.39114132  0.53096758  0.04004619 -0.23159748], action=0, reward=1.0, next_state=[0.40176067 0.33529696 0.03541424 0.07344356]\n",
      "[ episode 323 ][ timestamp 131 ] state=[0.40176067 0.33529696 0.03541424 0.07344356], action=1, reward=1.0, next_state=[ 0.40846661  0.52989378  0.03688311 -0.20785903]\n",
      "[ episode 323 ][ timestamp 132 ] state=[ 0.40846661  0.52989378  0.03688311 -0.20785903], action=0, reward=1.0, next_state=[0.41906449 0.33426437 0.03272593 0.09622669]\n",
      "[ episode 323 ][ timestamp 133 ] state=[0.41906449 0.33426437 0.03272593 0.09622669], action=1, reward=1.0, next_state=[ 0.42574977  0.52890237  0.03465047 -0.18595427]\n",
      "[ episode 323 ][ timestamp 134 ] state=[ 0.42574977  0.52890237  0.03465047 -0.18595427], action=0, reward=1.0, next_state=[0.43632782 0.33330222 0.03093138 0.11745503]\n",
      "[ episode 323 ][ timestamp 135 ] state=[0.43632782 0.33330222 0.03093138 0.11745503], action=1, reward=1.0, next_state=[ 0.44299387  0.52796765  0.03328048 -0.16531106]\n",
      "[ episode 323 ][ timestamp 136 ] state=[ 0.44299387  0.52796765  0.03328048 -0.16531106], action=0, reward=1.0, next_state=[0.45355322 0.33238549 0.02997426 0.13768238]\n",
      "[ episode 323 ][ timestamp 137 ] state=[0.45355322 0.33238549 0.02997426 0.13768238], action=1, reward=1.0, next_state=[ 0.46020093  0.52706558  0.03272791 -0.14539548]\n",
      "[ episode 323 ][ timestamp 138 ] state=[ 0.46020093  0.52706558  0.03272791 -0.14539548], action=0, reward=1.0, next_state=[0.47074224 0.3314906  0.02982    0.15743019]\n",
      "[ episode 323 ][ timestamp 139 ] state=[0.47074224 0.3314906  0.02982    0.15743019], action=1, reward=1.0, next_state=[ 0.47737205  0.5261732   0.0329686  -0.1256981 ]\n",
      "[ episode 323 ][ timestamp 140 ] state=[ 0.47737205  0.5261732   0.0329686  -0.1256981 ], action=0, reward=1.0, next_state=[0.48789552 0.33059483 0.03045464 0.17720104]\n",
      "[ episode 323 ][ timestamp 141 ] state=[0.48789552 0.33059483 0.03045464 0.17720104], action=1, reward=1.0, next_state=[ 0.49450741  0.52526801  0.03399866 -0.10572104]\n",
      "[ episode 323 ][ timestamp 142 ] state=[ 0.49450741  0.52526801  0.03399866 -0.10572104], action=1, reward=1.0, next_state=[ 0.50501277  0.71988666  0.03188424 -0.38748664]\n",
      "[ episode 323 ][ timestamp 143 ] state=[ 0.50501277  0.71988666  0.03188424 -0.38748664], action=1, reward=1.0, next_state=[ 0.51941051  0.91454185  0.02413451 -0.66994864]\n",
      "[ episode 323 ][ timestamp 144 ] state=[ 0.51941051  0.91454185  0.02413451 -0.66994864], action=0, reward=1.0, next_state=[ 0.53770134  0.71909281  0.01073553 -0.3697656 ]\n",
      "[ episode 323 ][ timestamp 145 ] state=[ 0.53770134  0.71909281  0.01073553 -0.3697656 ], action=1, reward=1.0, next_state=[ 0.5520832   0.91406059  0.00334022 -0.65904424]\n",
      "[ episode 323 ][ timestamp 146 ] state=[ 0.5520832   0.91406059  0.00334022 -0.65904424], action=0, reward=1.0, next_state=[ 0.57036441  0.71889231 -0.00984066 -0.36531143]\n",
      "[ episode 323 ][ timestamp 147 ] state=[ 0.57036441  0.71889231 -0.00984066 -0.36531143], action=1, reward=1.0, next_state=[ 0.58474226  0.91415272 -0.01714689 -0.66108097]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 323 ][ timestamp 148 ] state=[ 0.58474226  0.91415272 -0.01714689 -0.66108097], action=0, reward=1.0, next_state=[ 0.60302531  0.71927351 -0.03036851 -0.37384607]\n",
      "[ episode 323 ][ timestamp 149 ] state=[ 0.60302531  0.71927351 -0.03036851 -0.37384607], action=0, reward=1.0, next_state=[ 0.61741078  0.52459582 -0.03784543 -0.09089114]\n",
      "[ episode 323 ][ timestamp 150 ] state=[ 0.61741078  0.52459582 -0.03784543 -0.09089114], action=0, reward=1.0, next_state=[ 0.6279027   0.33003618 -0.03966326  0.18961544]\n",
      "[ episode 323 ][ timestamp 151 ] state=[ 0.6279027   0.33003618 -0.03966326  0.18961544], action=1, reward=1.0, next_state=[ 0.63450342  0.52570246 -0.03587095 -0.11531109]\n",
      "[ episode 323 ][ timestamp 152 ] state=[ 0.63450342  0.52570246 -0.03587095 -0.11531109], action=0, reward=1.0, next_state=[ 0.64501747  0.33111236 -0.03817717  0.1658425 ]\n",
      "[ episode 323 ][ timestamp 153 ] state=[ 0.64501747  0.33111236 -0.03817717  0.1658425 ], action=0, reward=1.0, next_state=[ 0.65163972  0.13655711 -0.03486032  0.44624137]\n",
      "[ episode 323 ][ timestamp 154 ] state=[ 0.65163972  0.13655711 -0.03486032  0.44624137], action=1, reward=1.0, next_state=[ 0.65437086  0.33215445 -0.02593549  0.14277676]\n",
      "[ episode 323 ][ timestamp 155 ] state=[ 0.65437086  0.33215445 -0.02593549  0.14277676], action=0, reward=1.0, next_state=[ 0.66101395  0.13741335 -0.02307996  0.42716599]\n",
      "[ episode 323 ][ timestamp 156 ] state=[ 0.66101395  0.13741335 -0.02307996  0.42716599], action=1, reward=1.0, next_state=[ 0.66376222  0.33285446 -0.01453664  0.1272975 ]\n",
      "[ episode 323 ][ timestamp 157 ] state=[ 0.66376222  0.33285446 -0.01453664  0.1272975 ], action=0, reward=1.0, next_state=[ 0.67041931  0.13794374 -0.01199069  0.41535907]\n",
      "[ episode 323 ][ timestamp 158 ] state=[ 0.67041931  0.13794374 -0.01199069  0.41535907], action=1, reward=1.0, next_state=[ 0.67317818  0.33323356 -0.00368351  0.11892021]\n",
      "[ episode 323 ][ timestamp 159 ] state=[ 0.67317818  0.33323356 -0.00368351  0.11892021], action=0, reward=1.0, next_state=[ 0.67984285  0.13816458 -0.0013051   0.41043875]\n",
      "[ episode 323 ][ timestamp 160 ] state=[ 0.67984285  0.13816458 -0.0013051   0.41043875], action=1, reward=1.0, next_state=[0.68260614 0.33330501 0.00690367 0.11734466]\n",
      "[ episode 323 ][ timestamp 161 ] state=[0.68260614 0.33330501 0.00690367 0.11734466], action=0, reward=1.0, next_state=[0.68927224 0.13808483 0.00925057 0.41219762]\n",
      "[ episode 323 ][ timestamp 162 ] state=[0.68927224 0.13808483 0.00925057 0.41219762], action=1, reward=1.0, next_state=[0.69203394 0.33307443 0.01749452 0.12244535]\n",
      "[ episode 323 ][ timestamp 163 ] state=[0.69203394 0.33307443 0.01749452 0.12244535], action=1, reward=1.0, next_state=[ 0.69869543  0.52794143  0.01994343 -0.16466729]\n",
      "[ episode 323 ][ timestamp 164 ] state=[ 0.69869543  0.52794143  0.01994343 -0.16466729], action=0, reward=1.0, next_state=[0.70925426 0.33253975 0.01665008 0.13423992]\n",
      "[ episode 323 ][ timestamp 165 ] state=[0.70925426 0.33253975 0.01665008 0.13423992], action=1, reward=1.0, next_state=[ 0.71590505  0.5274193   0.01933488 -0.15314399]\n",
      "[ episode 323 ][ timestamp 166 ] state=[ 0.71590505  0.5274193   0.01933488 -0.15314399], action=0, reward=1.0, next_state=[0.72645344 0.33202591 0.016272   0.14557541]\n",
      "[ episode 323 ][ timestamp 167 ] state=[0.72645344 0.33202591 0.016272   0.14557541], action=0, reward=1.0, next_state=[0.73309396 0.13667476 0.01918351 0.44334711]\n",
      "[ episode 323 ][ timestamp 168 ] state=[0.73309396 0.13667476 0.01918351 0.44334711], action=1, reward=1.0, next_state=[0.73582745 0.33152009 0.02805045 0.15677249]\n",
      "[ episode 323 ][ timestamp 169 ] state=[0.73582745 0.33152009 0.02805045 0.15677249], action=1, reward=1.0, next_state=[ 0.74245785  0.52622943  0.0311859  -0.12693088]\n",
      "[ episode 323 ][ timestamp 170 ] state=[ 0.74245785  0.52622943  0.0311859  -0.12693088], action=0, reward=1.0, next_state=[0.75298244 0.33067492 0.02864728 0.17542543]\n",
      "[ episode 323 ][ timestamp 171 ] state=[0.75298244 0.33067492 0.02864728 0.17542543], action=1, reward=1.0, next_state=[ 0.75959594  0.52537543  0.03215579 -0.10808435]\n",
      "[ episode 323 ][ timestamp 172 ] state=[ 0.75959594  0.52537543  0.03215579 -0.10808435], action=0, reward=1.0, next_state=[0.77010345 0.32980779 0.0299941  0.19456763]\n",
      "[ episode 323 ][ timestamp 173 ] state=[0.77010345 0.32980779 0.0299941  0.19456763], action=1, reward=1.0, next_state=[ 0.77669961  0.52448814  0.03388546 -0.0885046 ]\n",
      "[ episode 323 ][ timestamp 174 ] state=[ 0.77669961  0.52448814  0.03388546 -0.0885046 ], action=0, reward=1.0, next_state=[0.78718937 0.32889728 0.03211536 0.21467369]\n",
      "[ episode 323 ][ timestamp 175 ] state=[0.78718937 0.32889728 0.03211536 0.21467369], action=1, reward=1.0, next_state=[ 0.79376731  0.52354572  0.03640884 -0.06770813]\n",
      "[ episode 323 ][ timestamp 176 ] state=[ 0.79376731  0.52354572  0.03640884 -0.06770813], action=0, reward=1.0, next_state=[0.80423823 0.3279212  0.03505468 0.23623601]\n",
      "[ episode 323 ][ timestamp 177 ] state=[0.80423823 0.3279212  0.03505468 0.23623601], action=1, reward=1.0, next_state=[ 0.81079665  0.52252525  0.0397794  -0.04518676]\n",
      "[ episode 323 ][ timestamp 178 ] state=[ 0.81079665  0.52252525  0.0397794  -0.04518676], action=0, reward=1.0, next_state=[0.82124716 0.32685613 0.03887566 0.25977679]\n",
      "[ episode 323 ][ timestamp 179 ] state=[0.82124716 0.32685613 0.03887566 0.25977679], action=1, reward=1.0, next_state=[ 0.82778428  0.52140216  0.0440712  -0.0203952 ]\n",
      "[ episode 323 ][ timestamp 180 ] state=[ 0.82778428  0.52140216  0.0440712  -0.0203952 ], action=0, reward=1.0, next_state=[0.83821232 0.3256768  0.04366329 0.28586051]\n",
      "[ episode 323 ][ timestamp 181 ] state=[0.83821232 0.3256768  0.04366329 0.28586051], action=1, reward=1.0, next_state=[0.84472586 0.52014972 0.0493805  0.00726208]\n",
      "[ episode 323 ][ timestamp 182 ] state=[0.84472586 0.52014972 0.0493805  0.00726208], action=0, reward=1.0, next_state=[0.85512885 0.32435564 0.04952574 0.31510717]\n",
      "[ episode 323 ][ timestamp 183 ] state=[0.85512885 0.32435564 0.04952574 0.31510717], action=0, reward=1.0, next_state=[0.86161597 0.12856448 0.05582789 0.62298842]\n",
      "[ episode 323 ][ timestamp 184 ] state=[0.86161597 0.12856448 0.05582789 0.62298842], action=1, reward=1.0, next_state=[0.86418726 0.32286431 0.06828766 0.34839763]\n",
      "[ episode 323 ][ timestamp 185 ] state=[0.86418726 0.32286431 0.06828766 0.34839763], action=1, reward=1.0, next_state=[0.87064454 0.51695191 0.07525561 0.07800573]\n",
      "[ episode 323 ][ timestamp 186 ] state=[0.87064454 0.51695191 0.07525561 0.07800573], action=1, reward=1.0, next_state=[ 0.88098358  0.71091888  0.07681572 -0.19001695]\n",
      "[ episode 323 ][ timestamp 187 ] state=[ 0.88098358  0.71091888  0.07681572 -0.19001695], action=0, reward=1.0, next_state=[0.89520196 0.51478682 0.07301538 0.1258752 ]\n",
      "[ episode 323 ][ timestamp 188 ] state=[0.89520196 0.51478682 0.07301538 0.1258752 ], action=1, reward=1.0, next_state=[ 0.90549769  0.70879093  0.07553289 -0.14290813]\n",
      "[ episode 323 ][ timestamp 189 ] state=[ 0.90549769  0.70879093  0.07553289 -0.14290813], action=1, reward=1.0, next_state=[ 0.91967351  0.90275443  0.07267473 -0.41083827]\n",
      "[ episode 323 ][ timestamp 190 ] state=[ 0.91967351  0.90275443  0.07267473 -0.41083827], action=0, reward=1.0, next_state=[ 0.9377286   0.70668148  0.06445796 -0.09615762]\n",
      "[ episode 323 ][ timestamp 191 ] state=[ 0.9377286   0.70668148  0.06445796 -0.09615762], action=0, reward=1.0, next_state=[0.95186223 0.51069774 0.06253481 0.216145  ]\n",
      "[ episode 323 ][ timestamp 192 ] state=[0.95186223 0.51069774 0.06253481 0.216145  ], action=1, reward=1.0, next_state=[ 0.96207619  0.70487255  0.06685771 -0.05617463]\n",
      "[ episode 323 ][ timestamp 193 ] state=[ 0.96207619  0.70487255  0.06685771 -0.05617463], action=0, reward=1.0, next_state=[0.97617364 0.50885884 0.06573422 0.25683058]\n",
      "[ episode 323 ][ timestamp 194 ] state=[0.97617364 0.50885884 0.06573422 0.25683058], action=0, reward=1.0, next_state=[0.98635081 0.31286298 0.07087083 0.56950136]\n",
      "[ episode 323 ][ timestamp 195 ] state=[0.98635081 0.31286298 0.07087083 0.56950136], action=1, reward=1.0, next_state=[0.99260807 0.50692315 0.08226085 0.29996042]\n",
      "[ episode 323 ][ timestamp 196 ] state=[0.99260807 0.50692315 0.08226085 0.29996042], action=1, reward=1.0, next_state=[1.00274654 0.70078211 0.08826006 0.0343127 ]\n",
      "[ episode 323 ][ timestamp 197 ] state=[1.00274654 0.70078211 0.08826006 0.0343127 ], action=1, reward=1.0, next_state=[ 1.01676218  0.8945348   0.08894632 -0.22927031]\n",
      "[ episode 323 ][ timestamp 198 ] state=[ 1.01676218  0.8945348   0.08894632 -0.22927031], action=0, reward=1.0, next_state=[1.03465287 0.69826182 0.08436091 0.09009108]\n",
      "[ episode 323 ][ timestamp 199 ] state=[1.03465287 0.69826182 0.08436091 0.09009108], action=1, reward=1.0, next_state=[ 1.04861811  0.89207963  0.08616273 -0.17482902]\n",
      "[ episode 323 ][ timestamp 200 ] state=[ 1.04861811  0.89207963  0.08616273 -0.17482902], action=1, reward=1.0, next_state=[ 1.0664597   1.08586954  0.08266615 -0.43913503]\n",
      "[ episode 323 ][ timestamp 201 ] state=[ 1.0664597   1.08586954  0.08266615 -0.43913503], action=1, reward=1.0, next_state=[ 1.08817709  1.27973015  0.07388345 -0.70465675]\n",
      "[ episode 323 ][ timestamp 202 ] state=[ 1.08817709  1.27973015  0.07388345 -0.70465675], action=0, reward=1.0, next_state=[ 1.1137717   1.08366638  0.05979032 -0.38966145]\n",
      "[ episode 323 ][ timestamp 203 ] state=[ 1.1137717   1.08366638  0.05979032 -0.38966145], action=0, reward=1.0, next_state=[ 1.13544502  0.88774903  0.05199709 -0.07874267]\n",
      "[ episode 323 ][ timestamp 204 ] state=[ 1.13544502  0.88774903  0.05199709 -0.07874267], action=1, reward=1.0, next_state=[ 1.15320001  1.08208851  0.05042223 -0.35457765]\n",
      "[ episode 323 ][ timestamp 205 ] state=[ 1.15320001  1.08208851  0.05042223 -0.35457765], action=1, reward=1.0, next_state=[ 1.17484178  1.27645862  0.04333068 -0.6309444 ]\n",
      "[ episode 323 ][ timestamp 206 ] state=[ 1.17484178  1.27645862  0.04333068 -0.6309444 ], action=0, reward=1.0, next_state=[ 1.20037095  1.08075971  0.03071179 -0.32493634]\n",
      "[ episode 323 ][ timestamp 207 ] state=[ 1.20037095  1.08075971  0.03071179 -0.32493634], action=0, reward=1.0, next_state=[ 1.22198614  0.88521424  0.02421307 -0.0227286 ]\n",
      "[ episode 323 ][ timestamp 208 ] state=[ 1.22198614  0.88521424  0.02421307 -0.0227286 ], action=0, reward=1.0, next_state=[1.23969043 0.68975357 0.02375849 0.27749441]\n",
      "[ episode 323 ][ timestamp 209 ] state=[1.23969043 0.68975357 0.02375849 0.27749441], action=0, reward=1.0, next_state=[1.2534855  0.49430087 0.02930838 0.57757505]\n",
      "[ episode 323 ][ timestamp 210 ] state=[1.2534855  0.49430087 0.02930838 0.57757505], action=1, reward=1.0, next_state=[1.26337152 0.68900004 0.04085988 0.29426715]\n",
      "[ episode 323 ][ timestamp 211 ] state=[1.26337152 0.68900004 0.04085988 0.29426715], action=0, reward=1.0, next_state=[1.27715152 0.49332009 0.04674523 0.59955155]\n",
      "[ episode 323 ][ timestamp 212 ] state=[1.27715152 0.49332009 0.04674523 0.59955155], action=0, reward=1.0, next_state=[1.28701792 0.29757638 0.05873626 0.90658447]\n",
      "[ episode 323 ][ timestamp 213 ] state=[1.28701792 0.29757638 0.05873626 0.90658447], action=1, reward=1.0, next_state=[1.29296945 0.49185599 0.07686795 0.63292614]\n",
      "[ episode 323 ][ timestamp 214 ] state=[1.29296945 0.49185599 0.07686795 0.63292614], action=0, reward=1.0, next_state=[1.30280657 0.29575061 0.08952647 0.94879253]\n",
      "[ episode 323 ][ timestamp 215 ] state=[1.30280657 0.29575061 0.08952647 0.94879253], action=0, reward=1.0, next_state=[1.30872158 0.09954485 0.10850232 1.26820815]\n",
      "[ episode 323 ][ timestamp 216 ] state=[1.30872158 0.09954485 0.10850232 1.26820815], action=1, reward=1.0, next_state=[1.31071247 0.29312672 0.13386648 1.01138004]\n",
      "[ episode 323 ][ timestamp 217 ] state=[1.31071247 0.29312672 0.13386648 1.01138004], action=1, reward=1.0, next_state=[1.31657501 0.48623317 0.15409408 0.76355117]\n",
      "[ episode 323 ][ timestamp 218 ] state=[1.31657501 0.48623317 0.15409408 0.76355117], action=1, reward=1.0, next_state=[1.32629967 0.67893514 0.16936511 0.52304779]\n",
      "[ episode 323 ][ timestamp 219 ] state=[1.32629967 0.67893514 0.16936511 0.52304779], action=0, reward=1.0, next_state=[1.33987838 0.48188506 0.17982606 0.86394945]\n",
      "[ episode 323 ][ timestamp 220 ] state=[1.33987838 0.48188506 0.17982606 0.86394945], action=0, reward=1.0, next_state=[1.34951608 0.28483088 0.19710505 1.2073488 ]\n",
      "[ episode 323 ][ timestamp 221 ] state=[1.34951608 0.28483088 0.19710505 1.2073488 ], action=1, reward=-1.0, next_state=[1.35521269 0.47693814 0.22125203 0.9823418 ]\n",
      "[ Ended! ] Episode 323: Exploration_rate=0.19908200231898848. Score=221.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 324 ] state=[-0.0292733   0.02375754  0.0196457   0.00230873]\n",
      "[ episode 324 ][ timestamp 1 ] state=[-0.0292733   0.02375754  0.0196457   0.00230873], action=1, reward=1.0, next_state=[-0.02879815  0.21859231  0.01969187 -0.28411157]\n",
      "[ episode 324 ][ timestamp 2 ] state=[-0.02879815  0.21859231  0.01969187 -0.28411157], action=1, reward=1.0, next_state=[-0.0244263   0.41342795  0.01400964 -0.57051933]\n",
      "[ episode 324 ][ timestamp 3 ] state=[-0.0244263   0.41342795  0.01400964 -0.57051933], action=1, reward=1.0, next_state=[-0.01615774  0.60835066  0.00259925 -0.858756  ]\n",
      "[ episode 324 ][ timestamp 4 ] state=[-0.01615774  0.60835066  0.00259925 -0.858756  ], action=1, reward=1.0, next_state=[-0.00399073  0.8034371  -0.01457587 -1.1506205 ]\n",
      "[ episode 324 ][ timestamp 5 ] state=[-0.00399073  0.8034371  -0.01457587 -1.1506205 ], action=1, reward=1.0, next_state=[ 0.01207801  0.9987462  -0.03758828 -1.44783818]\n",
      "[ episode 324 ][ timestamp 6 ] state=[ 0.01207801  0.9987462  -0.03758828 -1.44783818], action=0, reward=1.0, next_state=[ 0.03205293  0.80410607 -0.06654504 -1.16713257]\n",
      "[ episode 324 ][ timestamp 7 ] state=[ 0.03205293  0.80410607 -0.06654504 -1.16713257], action=0, reward=1.0, next_state=[ 0.04813506  0.60991012 -0.08988769 -0.89603316]\n",
      "[ episode 324 ][ timestamp 8 ] state=[ 0.04813506  0.60991012 -0.08988769 -0.89603316], action=0, reward=1.0, next_state=[ 0.06033326  0.41611431 -0.10780836 -0.63290444]\n",
      "[ episode 324 ][ timestamp 9 ] state=[ 0.06033326  0.41611431 -0.10780836 -0.63290444], action=0, reward=1.0, next_state=[ 0.06865554  0.22264838 -0.12046644 -0.37602464]\n",
      "[ episode 324 ][ timestamp 10 ] state=[ 0.06865554  0.22264838 -0.12046644 -0.37602464], action=1, reward=1.0, next_state=[ 0.07310851  0.41925699 -0.12798694 -0.70413176]\n",
      "[ episode 324 ][ timestamp 11 ] state=[ 0.07310851  0.41925699 -0.12798694 -0.70413176], action=0, reward=1.0, next_state=[ 0.08149365  0.22611907 -0.14206957 -0.45431996]\n",
      "[ episode 324 ][ timestamp 12 ] state=[ 0.08149365  0.22611907 -0.14206957 -0.45431996], action=0, reward=1.0, next_state=[ 0.08601603  0.0332617  -0.15115597 -0.20957652]\n",
      "[ episode 324 ][ timestamp 13 ] state=[ 0.08601603  0.0332617  -0.15115597 -0.20957652], action=1, reward=1.0, next_state=[ 0.08668127  0.2301854  -0.1553475  -0.54586482]\n",
      "[ episode 324 ][ timestamp 14 ] state=[ 0.08668127  0.2301854  -0.1553475  -0.54586482], action=0, reward=1.0, next_state=[ 0.09128498  0.03754815 -0.1662648  -0.30587728]\n",
      "[ episode 324 ][ timestamp 15 ] state=[ 0.09128498  0.03754815 -0.1662648  -0.30587728], action=1, reward=1.0, next_state=[ 0.09203594  0.23460084 -0.17238234 -0.64603719]\n",
      "[ episode 324 ][ timestamp 16 ] state=[ 0.09203594  0.23460084 -0.17238234 -0.64603719], action=1, reward=1.0, next_state=[ 0.09672796  0.43165226 -0.18530309 -0.98766331]\n",
      "[ episode 324 ][ timestamp 17 ] state=[ 0.09672796  0.43165226 -0.18530309 -0.98766331], action=1, reward=1.0, next_state=[ 0.105361    0.62870651 -0.20505635 -1.33235234]\n",
      "[ episode 324 ][ timestamp 18 ] state=[ 0.105361    0.62870651 -0.20505635 -1.33235234], action=0, reward=-1.0, next_state=[ 0.11793513  0.43667356 -0.2317034  -1.11020266]\n",
      "[ Ended! ] Episode 324: Exploration_rate=0.19808659230739353. Score=18.\n",
      "[ Experience replay ] starts\n",
      "[ episode 325 ] state=[-0.00673737  0.01092694  0.01341354 -0.02547977]\n",
      "[ episode 325 ][ timestamp 1 ] state=[-0.00673737  0.01092694  0.01341354 -0.02547977], action=0, reward=1.0, next_state=[-0.00651883 -0.18438478  0.01290394  0.27140492]\n",
      "[ episode 325 ][ timestamp 2 ] state=[-0.00651883 -0.18438478  0.01290394  0.27140492], action=1, reward=1.0, next_state=[-0.01020653  0.01055068  0.01833204 -0.01718028]\n",
      "[ episode 325 ][ timestamp 3 ] state=[-0.01020653  0.01055068  0.01833204 -0.01718028], action=0, reward=1.0, next_state=[-0.00999551 -0.18482931  0.01798844  0.28122978]\n",
      "[ episode 325 ][ timestamp 4 ] state=[-0.00999551 -0.18482931  0.01798844  0.28122978], action=1, reward=1.0, next_state=[-0.0136921   0.01003149  0.02361303 -0.00572581]\n",
      "[ episode 325 ][ timestamp 5 ] state=[-0.0136921   0.01003149  0.02361303 -0.00572581], action=0, reward=1.0, next_state=[-0.01349147 -0.185421    0.02349852  0.29431279]\n",
      "[ episode 325 ][ timestamp 6 ] state=[-0.01349147 -0.185421    0.02349852  0.29431279], action=1, reward=1.0, next_state=[-0.01719989  0.00935819  0.02938477  0.00913259]\n",
      "[ episode 325 ][ timestamp 7 ] state=[-0.01719989  0.00935819  0.02938477  0.00913259], action=0, reward=1.0, next_state=[-0.01701273 -0.18617259  0.02956742  0.31094002]\n",
      "[ episode 325 ][ timestamp 8 ] state=[-0.01701273 -0.18617259  0.02956742  0.31094002], action=1, reward=1.0, next_state=[-0.02073618  0.0085159   0.03578622  0.02772648]\n",
      "[ episode 325 ][ timestamp 9 ] state=[-0.02073618  0.0085159   0.03578622  0.02772648], action=1, reward=1.0, next_state=[-0.02056586  0.20310687  0.03634075 -0.25345418]\n",
      "[ episode 325 ][ timestamp 10 ] state=[-0.02056586  0.20310687  0.03634075 -0.25345418], action=0, reward=1.0, next_state=[-0.01650372  0.00748537  0.03127167  0.05046616]\n",
      "[ episode 325 ][ timestamp 11 ] state=[-0.01650372  0.00748537  0.03127167  0.05046616], action=0, reward=1.0, next_state=[-0.01635401 -0.18807071  0.03228099  0.35284923]\n",
      "[ episode 325 ][ timestamp 12 ] state=[-0.01635401 -0.18807071  0.03228099  0.35284923], action=1, reward=1.0, next_state=[-0.02011543  0.00657769  0.03933798  0.07051771]\n",
      "[ episode 325 ][ timestamp 13 ] state=[-0.02011543  0.00657769  0.03933798  0.07051771], action=1, reward=1.0, next_state=[-0.01998388  0.20111424  0.04074833 -0.20949897]\n",
      "[ episode 325 ][ timestamp 14 ] state=[-0.01998388  0.20111424  0.04074833 -0.20949897], action=0, reward=1.0, next_state=[-0.01596159  0.00543405  0.03655835  0.09575435]\n",
      "[ episode 325 ][ timestamp 15 ] state=[-0.01596159  0.00543405  0.03655835  0.09575435], action=1, reward=1.0, next_state=[-0.01585291  0.20001347  0.03847344 -0.185174  ]\n",
      "[ episode 325 ][ timestamp 16 ] state=[-0.01585291  0.20001347  0.03847344 -0.185174  ], action=0, reward=1.0, next_state=[-0.01185264  0.00436277  0.03476996  0.11939328]\n",
      "[ episode 325 ][ timestamp 17 ] state=[-0.01185264  0.00436277  0.03476996  0.11939328], action=1, reward=1.0, next_state=[-0.01176538  0.19896974  0.03715783 -0.16212044]\n",
      "[ episode 325 ][ timestamp 18 ] state=[-0.01176538  0.19896974  0.03715783 -0.16212044], action=0, reward=1.0, next_state=[-0.00778599  0.00333609  0.03391542  0.14204937]\n",
      "[ episode 325 ][ timestamp 19 ] state=[-0.00778599  0.00333609  0.03391542  0.14204937], action=1, reward=1.0, next_state=[-0.00771927  0.19795631  0.0367564  -0.13974386]\n",
      "[ episode 325 ][ timestamp 20 ] state=[-0.00771927  0.19795631  0.0367564  -0.13974386], action=0, reward=1.0, next_state=[-0.00376014  0.00232773  0.03396153  0.16430476]\n",
      "[ episode 325 ][ timestamp 21 ] state=[-0.00376014  0.00232773  0.03396153  0.16430476], action=1, reward=1.0, next_state=[-0.00371359  0.19694747  0.03724762 -0.11747375]\n",
      "[ episode 325 ][ timestamp 22 ] state=[-0.00371359  0.19694747  0.03724762 -0.11747375], action=0, reward=1.0, next_state=[0.00022536 0.00131218 0.03489815 0.18672392]\n",
      "[ episode 325 ][ timestamp 23 ] state=[0.00022536 0.00131218 0.03489815 0.18672392], action=1, reward=1.0, next_state=[ 0.00025161  0.1959179   0.03863263 -0.09474895]\n",
      "[ episode 325 ][ timestamp 24 ] state=[ 0.00025161  0.1959179   0.03863263 -0.09474895], action=0, reward=1.0, next_state=[0.00416996 0.00026414 0.03673765 0.20986787]\n",
      "[ episode 325 ][ timestamp 25 ] state=[0.00416996 0.00026414 0.03673765 0.20986787], action=1, reward=1.0, next_state=[ 0.00417525  0.19484207  0.040935   -0.07100364]\n",
      "[ episode 325 ][ timestamp 26 ] state=[ 0.00417525  0.19484207  0.040935   -0.07100364], action=0, reward=1.0, next_state=[ 0.00807209 -0.00084211  0.03951493  0.23430827]\n",
      "[ episode 325 ][ timestamp 27 ] state=[ 0.00807209 -0.00084211  0.03951493  0.23430827], action=1, reward=1.0, next_state=[ 0.00805525  0.19369363  0.0442011  -0.04565319]\n",
      "[ episode 325 ][ timestamp 28 ] state=[ 0.00805525  0.19369363  0.0442011  -0.04565319], action=0, reward=1.0, next_state=[ 0.01192912 -0.00203336  0.04328803  0.26064142]\n",
      "[ episode 325 ][ timestamp 29 ] state=[ 0.01192912 -0.00203336  0.04328803  0.26064142], action=1, reward=1.0, next_state=[ 0.01188845  0.19244478  0.04850086 -0.01807979]\n",
      "[ episode 325 ][ timestamp 30 ] state=[ 0.01188845  0.19244478  0.04850086 -0.01807979], action=0, reward=1.0, next_state=[ 0.01573735 -0.00333797  0.04813927  0.28950265]\n",
      "[ episode 325 ][ timestamp 31 ] state=[ 0.01573735 -0.00333797  0.04813927  0.28950265], action=1, reward=1.0, next_state=[0.01567059 0.19106566 0.05392932 0.0123825 ]\n",
      "[ episode 325 ][ timestamp 32 ] state=[0.01567059 0.19106566 0.05392932 0.0123825 ], action=0, reward=1.0, next_state=[ 0.0194919  -0.00478658  0.05417697  0.32158128]\n",
      "[ episode 325 ][ timestamp 33 ] state=[ 0.0194919  -0.00478658  0.05417697  0.32158128], action=1, reward=1.0, next_state=[0.01939617 0.18952369 0.06060859 0.04646376]\n",
      "[ episode 325 ][ timestamp 34 ] state=[0.01939617 0.18952369 0.06060859 0.04646376], action=0, reward=1.0, next_state=[ 0.02318664 -0.00641262  0.06153787  0.3576366 ]\n",
      "[ episode 325 ][ timestamp 35 ] state=[ 0.02318664 -0.00641262  0.06153787  0.3576366 ], action=1, reward=1.0, next_state=[0.02305839 0.18778295 0.0686906  0.08497534]\n",
      "[ episode 325 ][ timestamp 36 ] state=[0.02305839 0.18778295 0.0686906  0.08497534], action=0, reward=1.0, next_state=[ 0.02681405 -0.00825298  0.07039011  0.39851492]\n",
      "[ episode 325 ][ timestamp 37 ] state=[ 0.02681405 -0.00825298  0.07039011  0.39851492], action=1, reward=1.0, next_state=[0.02664899 0.18580347 0.07836041 0.12882869]\n",
      "[ episode 325 ][ timestamp 38 ] state=[0.02664899 0.18580347 0.07836041 0.12882869], action=0, reward=1.0, next_state=[ 0.03036506 -0.01034847  0.08093698  0.44516813]\n",
      "[ episode 325 ][ timestamp 39 ] state=[ 0.03036506 -0.01034847  0.08093698  0.44516813], action=1, reward=1.0, next_state=[0.03015809 0.18354068 0.08984034 0.17905597]\n",
      "[ episode 325 ][ timestamp 40 ] state=[0.03015809 0.18354068 0.08984034 0.17905597], action=0, reward=1.0, next_state=[ 0.0338289  -0.01274437  0.09342146  0.49867369]\n",
      "[ episode 325 ][ timestamp 41 ] state=[ 0.0338289  -0.01274437  0.09342146  0.49867369], action=1, reward=1.0, next_state=[0.03357402 0.18094486 0.10339494 0.23683272]\n",
      "[ episode 325 ][ timestamp 42 ] state=[0.03357402 0.18094486 0.10339494 0.23683272], action=1, reward=1.0, next_state=[ 0.03719291  0.37444935  0.10813159 -0.02152991]\n",
      "[ episode 325 ][ timestamp 43 ] state=[ 0.03719291  0.37444935  0.10813159 -0.02152991], action=0, reward=1.0, next_state=[0.0446819  0.17795619 0.10770099 0.30321715]\n",
      "[ episode 325 ][ timestamp 44 ] state=[0.0446819  0.17795619 0.10770099 0.30321715], action=1, reward=1.0, next_state=[0.04824102 0.37139157 0.11376533 0.04634819]\n",
      "[ episode 325 ][ timestamp 45 ] state=[0.04824102 0.37139157 0.11376533 0.04634819], action=0, reward=1.0, next_state=[0.05566886 0.17483767 0.1146923  0.37264807]\n",
      "[ episode 325 ][ timestamp 46 ] state=[0.05566886 0.17483767 0.1146923  0.37264807], action=0, reward=1.0, next_state=[ 0.05916561 -0.02171089  0.12214526  0.6991796 ]\n",
      "[ episode 325 ][ timestamp 47 ] state=[ 0.05916561 -0.02171089  0.12214526  0.6991796 ], action=1, reward=1.0, next_state=[0.05873139 0.1715249  0.13612885 0.44730694]\n",
      "[ episode 325 ][ timestamp 48 ] state=[0.05873139 0.1715249  0.13612885 0.44730694], action=1, reward=1.0, next_state=[0.06216189 0.36448506 0.14507499 0.20044277]\n",
      "[ episode 325 ][ timestamp 49 ] state=[0.06216189 0.36448506 0.14507499 0.20044277], action=1, reward=1.0, next_state=[ 0.06945159  0.55726635  0.14908385 -0.04318885]\n",
      "[ episode 325 ][ timestamp 50 ] state=[ 0.06945159  0.55726635  0.14908385 -0.04318885], action=0, reward=1.0, next_state=[0.08059692 0.360356   0.14822007 0.29256883]\n",
      "[ episode 325 ][ timestamp 51 ] state=[0.08059692 0.360356   0.14822007 0.29256883], action=1, reward=1.0, next_state=[0.08780404 0.55308803 0.15407145 0.05005791]\n",
      "[ episode 325 ][ timestamp 52 ] state=[0.08780404 0.55308803 0.15407145 0.05005791], action=0, reward=1.0, next_state=[0.0988658  0.35613105 0.1550726  0.3871118 ]\n",
      "[ episode 325 ][ timestamp 53 ] state=[0.0988658  0.35613105 0.1550726  0.3871118 ], action=1, reward=1.0, next_state=[0.10598842 0.54875087 0.16281484 0.14705796]\n",
      "[ episode 325 ][ timestamp 54 ] state=[0.10598842 0.54875087 0.16281484 0.14705796], action=0, reward=1.0, next_state=[0.11696344 0.3517172  0.165756   0.48635615]\n",
      "[ episode 325 ][ timestamp 55 ] state=[0.11696344 0.3517172  0.165756   0.48635615], action=1, reward=1.0, next_state=[0.12399778 0.54416    0.17548312 0.25015783]\n",
      "[ episode 325 ][ timestamp 56 ] state=[0.12399778 0.54416    0.17548312 0.25015783], action=1, reward=1.0, next_state=[0.13488098 0.73639854 0.18048628 0.01755619]\n",
      "[ episode 325 ][ timestamp 57 ] state=[0.13488098 0.73639854 0.18048628 0.01755619], action=0, reward=1.0, next_state=[0.14960895 0.53920898 0.1808374  0.3613113 ]\n",
      "[ episode 325 ][ timestamp 58 ] state=[0.14960895 0.53920898 0.1808374  0.3613113 ], action=1, reward=1.0, next_state=[0.16039313 0.73136174 0.18806363 0.13065909]\n",
      "[ episode 325 ][ timestamp 59 ] state=[0.16039313 0.73136174 0.18806363 0.13065909], action=1, reward=1.0, next_state=[ 0.17502037  0.9233618   0.19067681 -0.09729765]\n",
      "[ episode 325 ][ timestamp 60 ] state=[ 0.17502037  0.9233618   0.19067681 -0.09729765], action=0, reward=1.0, next_state=[0.1934876  0.72609128 0.18873086 0.24896508]\n",
      "[ episode 325 ][ timestamp 61 ] state=[0.1934876  0.72609128 0.18873086 0.24896508], action=1, reward=1.0, next_state=[0.20800943 0.91808724 0.19371016 0.02124308]\n",
      "[ episode 325 ][ timestamp 62 ] state=[0.20800943 0.91808724 0.19371016 0.02124308], action=1, reward=1.0, next_state=[ 0.22637117  1.10997985  0.19413502 -0.20461704]\n",
      "[ episode 325 ][ timestamp 63 ] state=[ 0.22637117  1.10997985  0.19413502 -0.20461704], action=0, reward=1.0, next_state=[0.24857077 0.9126883  0.19004268 0.14247894]\n",
      "[ episode 325 ][ timestamp 64 ] state=[0.24857077 0.9126883  0.19004268 0.14247894], action=1, reward=1.0, next_state=[ 0.26682453  1.10465243  0.19289226 -0.0847463 ]\n",
      "[ episode 325 ][ timestamp 65 ] state=[ 0.26682453  1.10465243  0.19289226 -0.0847463 ], action=0, reward=1.0, next_state=[0.28891758 0.90736407 0.19119733 0.26205716]\n",
      "[ episode 325 ][ timestamp 66 ] state=[0.28891758 0.90736407 0.19119733 0.26205716], action=1, reward=1.0, next_state=[0.30706486 1.09931582 0.19643848 0.03524647]\n",
      "[ episode 325 ][ timestamp 67 ] state=[0.30706486 1.09931582 0.19643848 0.03524647], action=1, reward=1.0, next_state=[ 0.32905118  1.29115797  0.19714341 -0.18960027]\n",
      "[ episode 325 ][ timestamp 68 ] state=[ 0.32905118  1.29115797  0.19714341 -0.18960027], action=0, reward=1.0, next_state=[0.35487434 1.09384216 0.1933514  0.15822592]\n",
      "[ episode 325 ][ timestamp 69 ] state=[0.35487434 1.09384216 0.1933514  0.15822592], action=0, reward=1.0, next_state=[0.37675118 0.89655354 0.19651592 0.50513615]\n",
      "[ episode 325 ][ timestamp 70 ] state=[0.37675118 0.89655354 0.19651592 0.50513615], action=1, reward=1.0, next_state=[0.39468225 1.08844248 0.20661864 0.28024725]\n",
      "[ episode 325 ][ timestamp 71 ] state=[0.39468225 1.08844248 0.20661864 0.28024725], action=0, reward=-1.0, next_state=[0.4164511  0.89106449 0.21222359 0.63033153]\n",
      "[ Ended! ] Episode 325: Exploration_rate=0.19709615934585656. Score=71.\n",
      "[ Experience replay ] starts\n",
      "[ episode 326 ] state=[-0.04753123 -0.04799182  0.02721661  0.00690436]\n",
      "[ episode 326 ][ timestamp 1 ] state=[-0.04753123 -0.04799182  0.02721661  0.00690436], action=1, reward=1.0, next_state=[-0.04849107  0.14672945  0.0273547  -0.27706867]\n",
      "[ episode 326 ][ timestamp 2 ] state=[-0.04849107  0.14672945  0.0273547  -0.27706867], action=1, reward=1.0, next_state=[-0.04555648  0.34145068  0.02181332 -0.56099997]\n",
      "[ episode 326 ][ timestamp 3 ] state=[-0.04555648  0.34145068  0.02181332 -0.56099997], action=1, reward=1.0, next_state=[-0.03872747  0.53625981  0.01059333 -0.84673154]\n",
      "[ episode 326 ][ timestamp 4 ] state=[-0.03872747  0.53625981  0.01059333 -0.84673154], action=1, reward=1.0, next_state=[-0.02800227  0.73123565 -0.00634131 -1.13606452]\n",
      "[ episode 326 ][ timestamp 5 ] state=[-0.02800227  0.73123565 -0.00634131 -1.13606452], action=1, reward=1.0, next_state=[-0.01337756  0.92643999 -0.0290626  -1.43072946]\n",
      "[ episode 326 ][ timestamp 6 ] state=[-0.01337756  0.92643999 -0.0290626  -1.43072946], action=1, reward=1.0, next_state=[ 0.00515124  1.1219084  -0.05767719 -1.73235146]\n",
      "[ episode 326 ][ timestamp 7 ] state=[ 0.00515124  1.1219084  -0.05767719 -1.73235146], action=0, reward=1.0, next_state=[ 0.02758941  0.92749028 -0.09232421 -1.45815692]\n",
      "[ episode 326 ][ timestamp 8 ] state=[ 0.02758941  0.92749028 -0.09232421 -1.45815692], action=1, reward=1.0, next_state=[ 0.04613922  1.12361557 -0.12148735 -1.77819672]\n",
      "[ episode 326 ][ timestamp 9 ] state=[ 0.04613922  1.12361557 -0.12148735 -1.77819672], action=1, reward=1.0, next_state=[ 0.06861153  1.31987797 -0.15705129 -2.10604998]\n",
      "[ episode 326 ][ timestamp 10 ] state=[ 0.06861153  1.31987797 -0.15705129 -2.10604998], action=1, reward=1.0, next_state=[ 0.09500909  1.51618722 -0.19917229 -2.44287331]\n",
      "[ episode 326 ][ timestamp 11 ] state=[ 0.09500909  1.51618722 -0.19917229 -2.44287331], action=1, reward=-1.0, next_state=[ 0.12533283  1.71237672 -0.24802975 -2.78951004]\n",
      "[ Ended! ] Episode 326: Exploration_rate=0.19611067854912728. Score=11.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 327 ] state=[ 0.03010453 -0.02380792  0.0339224  -0.03144814]\n",
      "[ episode 327 ][ timestamp 1 ] state=[ 0.03010453 -0.02380792  0.0339224  -0.03144814], action=1, reward=1.0, next_state=[ 0.02962837  0.17081156  0.03329344 -0.31323813]\n",
      "[ episode 327 ][ timestamp 2 ] state=[ 0.02962837  0.17081156  0.03329344 -0.31323813], action=1, reward=1.0, next_state=[ 0.03304461  0.36544379  0.02702868 -0.59523823]\n",
      "[ episode 327 ][ timestamp 3 ] state=[ 0.03304461  0.36544379  0.02702868 -0.59523823], action=1, reward=1.0, next_state=[ 0.04035348  0.56017724  0.01512391 -0.87928624]\n",
      "[ episode 327 ][ timestamp 4 ] state=[ 0.04035348  0.56017724  0.01512391 -0.87928624], action=0, reward=1.0, next_state=[ 0.05155703  0.36485309 -0.00246181 -0.58188728]\n",
      "[ episode 327 ][ timestamp 5 ] state=[ 0.05155703  0.36485309 -0.00246181 -0.58188728], action=0, reward=1.0, next_state=[ 0.05885409  0.16976572 -0.01409956 -0.28998088]\n",
      "[ episode 327 ][ timestamp 6 ] state=[ 0.05885409  0.16976572 -0.01409956 -0.28998088], action=1, reward=1.0, next_state=[ 0.0622494   0.36508586 -0.01989917 -0.58707709]\n",
      "[ episode 327 ][ timestamp 7 ] state=[ 0.0622494   0.36508586 -0.01989917 -0.58707709], action=1, reward=1.0, next_state=[ 0.06955112  0.56048077 -0.03164072 -0.8859614 ]\n",
      "[ episode 327 ][ timestamp 8 ] state=[ 0.06955112  0.56048077 -0.03164072 -0.8859614 ], action=1, reward=1.0, next_state=[ 0.08076073  0.75601764 -0.04935994 -1.18842073]\n",
      "[ episode 327 ][ timestamp 9 ] state=[ 0.08076073  0.75601764 -0.04935994 -1.18842073], action=0, reward=1.0, next_state=[ 0.09588109  0.56156909 -0.07312836 -0.91160907]\n",
      "[ episode 327 ][ timestamp 10 ] state=[ 0.09588109  0.56156909 -0.07312836 -0.91160907], action=1, reward=1.0, next_state=[ 0.10711247  0.75760027 -0.09136054 -1.22635052]\n",
      "[ episode 327 ][ timestamp 11 ] state=[ 0.10711247  0.75760027 -0.09136054 -1.22635052], action=1, reward=1.0, next_state=[ 0.12226447  0.95377184 -0.11588755 -1.54620334]\n",
      "[ episode 327 ][ timestamp 12 ] state=[ 0.12226447  0.95377184 -0.11588755 -1.54620334], action=1, reward=1.0, next_state=[ 0.14133991  1.15007921 -0.14681162 -1.87268404]\n",
      "[ episode 327 ][ timestamp 13 ] state=[ 0.14133991  1.15007921 -0.14681162 -1.87268404], action=1, reward=1.0, next_state=[ 0.1643415   1.34646852 -0.1842653  -2.20710676]\n",
      "[ episode 327 ][ timestamp 14 ] state=[ 0.1643415   1.34646852 -0.1842653  -2.20710676], action=1, reward=-1.0, next_state=[ 0.19127087  1.5428206  -0.22840743 -2.55051679]\n",
      "[ Ended! ] Episode 327: Exploration_rate=0.19513012515638165. Score=14.\n",
      "[ Experience replay ] starts\n",
      "[ episode 328 ] state=[-0.03172456  0.02514922  0.01974657 -0.03318515]\n",
      "[ episode 328 ][ timestamp 1 ] state=[-0.03172456  0.02514922  0.01974657 -0.03318515], action=0, reward=1.0, next_state=[-0.03122158 -0.17025025  0.01908287  0.26566204]\n",
      "[ episode 328 ][ timestamp 2 ] state=[-0.03122158 -0.17025025  0.01908287  0.26566204], action=1, reward=1.0, next_state=[-0.03462658  0.02459421  0.02439611 -0.02094142]\n",
      "[ episode 328 ][ timestamp 3 ] state=[-0.03462658  0.02459421  0.02439611 -0.02094142], action=1, reward=1.0, next_state=[-0.0341347   0.21935795  0.02397728 -0.30582836]\n",
      "[ episode 328 ][ timestamp 4 ] state=[-0.0341347   0.21935795  0.02397728 -0.30582836], action=1, reward=1.0, next_state=[-0.02974754  0.41413016  0.01786071 -0.59085405]\n",
      "[ episode 328 ][ timestamp 5 ] state=[-0.02974754  0.41413016  0.01786071 -0.59085405], action=0, reward=1.0, next_state=[-0.02146494  0.21876276  0.00604363 -0.29259891]\n",
      "[ episode 328 ][ timestamp 6 ] state=[-0.02146494  0.21876276  0.00604363 -0.29259891], action=1, reward=1.0, next_state=[-1.70896817e-02  4.13798023e-01  1.91655446e-04 -5.83369645e-01]\n",
      "[ episode 328 ][ timestamp 7 ] state=[-1.70896817e-02  4.13798023e-01  1.91655446e-04 -5.83369645e-01], action=1, reward=1.0, next_state=[-0.00881372  0.60891729 -0.01147574 -0.87599219]\n",
      "[ episode 328 ][ timestamp 8 ] state=[-0.00881372  0.60891729 -0.01147574 -0.87599219], action=1, reward=1.0, next_state=[ 0.00336462  0.80419333 -0.02899558 -1.17226076]\n",
      "[ episode 328 ][ timestamp 9 ] state=[ 0.00336462  0.80419333 -0.02899558 -1.17226076], action=1, reward=1.0, next_state=[ 0.01944849  0.99968    -0.0524408  -1.47389101]\n",
      "[ episode 328 ][ timestamp 10 ] state=[ 0.01944849  0.99968    -0.0524408  -1.47389101], action=1, reward=1.0, next_state=[ 0.03944209  1.19540221 -0.08191862 -1.78248127]\n",
      "[ episode 328 ][ timestamp 11 ] state=[ 0.03944209  1.19540221 -0.08191862 -1.78248127], action=1, reward=1.0, next_state=[ 0.06335014  1.39134411 -0.11756824 -2.09946563]\n",
      "[ episode 328 ][ timestamp 12 ] state=[ 0.06335014  1.39134411 -0.11756824 -2.09946563], action=1, reward=1.0, next_state=[ 0.09117702  1.58743484 -0.15955755 -2.42605675]\n",
      "[ episode 328 ][ timestamp 13 ] state=[ 0.09117702  1.58743484 -0.15955755 -2.42605675], action=1, reward=1.0, next_state=[ 0.12292571  1.78353191 -0.20807869 -2.76317715]\n",
      "[ episode 328 ][ timestamp 14 ] state=[ 0.12292571  1.78353191 -0.20807869 -2.76317715], action=1, reward=-1.0, next_state=[ 0.15859635  1.97940219 -0.26334223 -3.11137973]\n",
      "[ Ended! ] Episode 328: Exploration_rate=0.19415447453059972. Score=14.\n",
      "[ Experience replay ] starts\n",
      "[ episode 329 ] state=[ 0.04532742 -0.0054446  -0.01980548  0.01699338]\n",
      "[ episode 329 ][ timestamp 1 ] state=[ 0.04532742 -0.0054446  -0.01980548  0.01699338], action=0, reward=1.0, next_state=[ 0.04521853 -0.200277   -0.01946561  0.30336223]\n",
      "[ episode 329 ][ timestamp 2 ] state=[ 0.04521853 -0.200277   -0.01946561  0.30336223], action=1, reward=1.0, next_state=[ 0.04121299 -0.00488311 -0.01339837  0.00460441]\n",
      "[ episode 329 ][ timestamp 3 ] state=[ 0.04121299 -0.00488311 -0.01339837  0.00460441], action=0, reward=1.0, next_state=[ 0.04111533 -0.19981038 -0.01330628  0.29303005]\n",
      "[ episode 329 ][ timestamp 4 ] state=[ 0.04111533 -0.19981038 -0.01330628  0.29303005], action=1, reward=1.0, next_state=[ 0.03711912 -0.00450126 -0.00744568 -0.00381962]\n",
      "[ episode 329 ][ timestamp 5 ] state=[ 0.03711912 -0.00450126 -0.00744568 -0.00381962], action=0, reward=1.0, next_state=[ 0.0370291  -0.19951564 -0.00752207  0.28650484]\n",
      "[ episode 329 ][ timestamp 6 ] state=[ 0.0370291  -0.19951564 -0.00752207  0.28650484], action=1, reward=1.0, next_state=[ 0.03303878 -0.00428723 -0.00179198 -0.00854097]\n",
      "[ episode 329 ][ timestamp 7 ] state=[ 0.03303878 -0.00428723 -0.00179198 -0.00854097], action=0, reward=1.0, next_state=[ 0.03295304 -0.19938344 -0.0019628   0.28357603]\n",
      "[ episode 329 ][ timestamp 8 ] state=[ 0.03295304 -0.19938344 -0.0019628   0.28357603], action=1, reward=1.0, next_state=[ 0.02896537 -0.00423354  0.00370872 -0.0097253 ]\n",
      "[ episode 329 ][ timestamp 9 ] state=[ 0.02896537 -0.00423354  0.00370872 -0.0097253 ], action=1, reward=1.0, next_state=[ 0.0288807   0.19083502  0.00351422 -0.30123578]\n",
      "[ episode 329 ][ timestamp 10 ] state=[ 0.0288807   0.19083502  0.00351422 -0.30123578], action=0, reward=1.0, next_state=[ 0.0326974  -0.00433684 -0.0025105  -0.00744662]\n",
      "[ episode 329 ][ timestamp 11 ] state=[ 0.0326974  -0.00433684 -0.0025105  -0.00744662], action=0, reward=1.0, next_state=[ 0.03261066 -0.1994227  -0.00265943  0.28444316]\n",
      "[ episode 329 ][ timestamp 12 ] state=[ 0.03261066 -0.1994227  -0.00265943  0.28444316], action=1, reward=1.0, next_state=[ 0.02862221 -0.00426292  0.00302943 -0.00907734]\n",
      "[ episode 329 ][ timestamp 13 ] state=[ 0.02862221 -0.00426292  0.00302943 -0.00907734], action=0, reward=1.0, next_state=[ 0.02853695 -0.19942818  0.00284789  0.28455986]\n",
      "[ episode 329 ][ timestamp 14 ] state=[ 0.02853695 -0.19942818  0.00284789  0.28455986], action=1, reward=1.0, next_state=[ 0.02454839 -0.00434696  0.00853908 -0.0072235 ]\n",
      "[ episode 329 ][ timestamp 15 ] state=[ 0.02454839 -0.00434696  0.00853908 -0.0072235 ], action=1, reward=1.0, next_state=[ 0.02446145  0.19065149  0.00839461 -0.29720006]\n",
      "[ episode 329 ][ timestamp 16 ] state=[ 0.02446145  0.19065149  0.00839461 -0.29720006], action=0, reward=1.0, next_state=[ 0.02827448 -0.00458912  0.00245061 -0.00188148]\n",
      "[ episode 329 ][ timestamp 17 ] state=[ 0.02827448 -0.00458912  0.00245061 -0.00188148], action=0, reward=1.0, next_state=[ 0.0281827  -0.19974613  0.00241298  0.29157364]\n",
      "[ episode 329 ][ timestamp 18 ] state=[ 0.0281827  -0.19974613  0.00241298  0.29157364], action=1, reward=1.0, next_state=[ 0.02418777 -0.00465866  0.00824446 -0.00034729]\n",
      "[ episode 329 ][ timestamp 19 ] state=[ 0.02418777 -0.00465866  0.00824446 -0.00034729], action=0, reward=1.0, next_state=[ 0.0240946  -0.19989788  0.00823751  0.29492542]\n",
      "[ episode 329 ][ timestamp 20 ] state=[ 0.0240946  -0.19989788  0.00823751  0.29492542], action=1, reward=1.0, next_state=[ 0.02009664 -0.00489433  0.01413602  0.00485182]\n",
      "[ episode 329 ][ timestamp 21 ] state=[ 0.02009664 -0.00489433  0.01413602  0.00485182], action=0, reward=1.0, next_state=[ 0.01999876 -0.20021613  0.01423306  0.3019611 ]\n",
      "[ episode 329 ][ timestamp 22 ] state=[ 0.01999876 -0.20021613  0.01423306  0.3019611 ], action=1, reward=1.0, next_state=[ 0.01599443 -0.00529989  0.02027228  0.01380074]\n",
      "[ episode 329 ][ timestamp 23 ] state=[ 0.01599443 -0.00529989  0.02027228  0.01380074], action=0, reward=1.0, next_state=[ 0.01588843 -0.20070662  0.02054829  0.31281025]\n",
      "[ episode 329 ][ timestamp 24 ] state=[ 0.01588843 -0.20070662  0.02054829  0.31281025], action=1, reward=1.0, next_state=[ 0.0118743  -0.00588334  0.0268045   0.02667779]\n",
      "[ episode 329 ][ timestamp 25 ] state=[ 0.0118743  -0.00588334  0.0268045   0.02667779], action=0, reward=1.0, next_state=[ 0.01175664 -0.20137922  0.02733805  0.32769586]\n",
      "[ episode 329 ][ timestamp 26 ] state=[ 0.01175664 -0.20137922  0.02733805  0.32769586], action=1, reward=1.0, next_state=[ 0.00772905 -0.00665693  0.03389197  0.04375794]\n",
      "[ episode 329 ][ timestamp 27 ] state=[ 0.00772905 -0.00665693  0.03389197  0.04375794], action=0, reward=1.0, next_state=[ 0.00759591 -0.20224807  0.03476713  0.3469385 ]\n",
      "[ episode 329 ][ timestamp 28 ] state=[ 0.00759591 -0.20224807  0.03476713  0.3469385 ], action=1, reward=1.0, next_state=[ 0.00355095 -0.00763745  0.0417059   0.06541846]\n",
      "[ episode 329 ][ timestamp 29 ] state=[ 0.00355095 -0.00763745  0.0417059   0.06541846], action=0, reward=1.0, next_state=[ 0.0033982  -0.20333177  0.04301427  0.37096266]\n",
      "[ episode 329 ][ timestamp 30 ] state=[ 0.0033982  -0.20333177  0.04301427  0.37096266], action=1, reward=1.0, next_state=[-0.00066843 -0.00884649  0.05043352  0.09214687]\n",
      "[ episode 329 ][ timestamp 31 ] state=[-0.00066843 -0.00884649  0.05043352  0.09214687], action=0, reward=1.0, next_state=[-0.00084536 -0.20465367  0.05227646  0.40030537]\n",
      "[ episode 329 ][ timestamp 32 ] state=[-0.00084536 -0.20465367  0.05227646  0.40030537], action=1, reward=1.0, next_state=[-0.00493844 -0.01031073  0.06028257  0.12455148]\n",
      "[ episode 329 ][ timestamp 33 ] state=[-0.00493844 -0.01031073  0.06028257  0.12455148], action=0, reward=1.0, next_state=[-0.00514465 -0.20624219  0.0627736   0.43562715]\n",
      "[ episode 329 ][ timestamp 34 ] state=[-0.00514465 -0.20624219  0.0627736   0.43562715], action=1, reward=1.0, next_state=[-0.00926949 -0.01206244  0.07148614  0.16337454]\n",
      "[ episode 329 ][ timestamp 35 ] state=[-0.00926949 -0.01206244  0.07148614  0.16337454], action=1, reward=1.0, next_state=[-0.00951074  0.18196722  0.07475363 -0.10592758]\n",
      "[ episode 329 ][ timestamp 36 ] state=[-0.00951074  0.18196722  0.07475363 -0.10592758], action=0, reward=1.0, next_state=[-0.0058714  -0.01414196  0.07263508  0.20937178]\n",
      "[ episode 329 ][ timestamp 37 ] state=[-0.0058714  -0.01414196  0.07263508  0.20937178], action=1, reward=1.0, next_state=[-0.00615424  0.17987029  0.07682251 -0.05954332]\n",
      "[ episode 329 ][ timestamp 38 ] state=[-0.00615424  0.17987029  0.07682251 -0.05954332], action=0, reward=1.0, next_state=[-0.00255683 -0.01626428  0.07563165  0.25635443]\n",
      "[ episode 329 ][ timestamp 39 ] state=[-0.00255683 -0.01626428  0.07563165  0.25635443], action=1, reward=1.0, next_state=[-0.00288212  0.17770095  0.08075874 -0.01154717]\n",
      "[ episode 329 ][ timestamp 40 ] state=[-0.00288212  0.17770095  0.08075874 -0.01154717], action=1, reward=1.0, next_state=[ 0.0006719   0.37157742  0.08052779 -0.27769678]\n",
      "[ episode 329 ][ timestamp 41 ] state=[ 0.0006719   0.37157742  0.08052779 -0.27769678], action=0, reward=1.0, next_state=[0.00810345 0.1754045  0.07497386 0.03925861]\n",
      "[ episode 329 ][ timestamp 42 ] state=[0.00810345 0.1754045  0.07497386 0.03925861], action=0, reward=1.0, next_state=[ 0.01161154 -0.02070803  0.07575903  0.3546227 ]\n",
      "[ episode 329 ][ timestamp 43 ] state=[ 0.01161154 -0.02070803  0.07575903  0.3546227 ], action=0, reward=1.0, next_state=[ 0.01119738 -0.21682084  0.08285148  0.67019999]\n",
      "[ episode 329 ][ timestamp 44 ] state=[ 0.01119738 -0.21682084  0.08285148  0.67019999], action=1, reward=1.0, next_state=[ 0.00686096 -0.02294256  0.09625548  0.40471062]\n",
      "[ episode 329 ][ timestamp 45 ] state=[ 0.00686096 -0.02294256  0.09625548  0.40471062], action=1, reward=1.0, next_state=[0.00640211 0.17069195 0.1043497  0.14385879]\n",
      "[ episode 329 ][ timestamp 46 ] state=[0.00640211 0.17069195 0.1043497  0.14385879], action=0, reward=1.0, next_state=[ 0.00981595 -0.02575764  0.10722687  0.46755345]\n",
      "[ episode 329 ][ timestamp 47 ] state=[ 0.00981595 -0.02575764  0.10722687  0.46755345], action=1, reward=1.0, next_state=[0.0093008  0.16769898 0.11657794 0.21049946]\n",
      "[ episode 329 ][ timestamp 48 ] state=[0.0093008  0.16769898 0.11657794 0.21049946], action=1, reward=1.0, next_state=[ 0.01265478  0.36097784  0.12078793 -0.04325466]\n",
      "[ episode 329 ][ timestamp 49 ] state=[ 0.01265478  0.36097784  0.12078793 -0.04325466], action=1, reward=1.0, next_state=[ 0.01987433  0.55417928  0.11992284 -0.29551997]\n",
      "[ episode 329 ][ timestamp 50 ] state=[ 0.01987433  0.55417928  0.11992284 -0.29551997], action=0, reward=1.0, next_state=[0.03095792 0.35757    0.11401244 0.03244871]\n",
      "[ episode 329 ][ timestamp 51 ] state=[0.03095792 0.35757    0.11401244 0.03244871], action=1, reward=1.0, next_state=[ 0.03810932  0.55088802  0.11466141 -0.22219859]\n",
      "[ episode 329 ][ timestamp 52 ] state=[ 0.03810932  0.55088802  0.11466141 -0.22219859], action=1, reward=1.0, next_state=[ 0.04912708  0.74420029  0.11021744 -0.47662631]\n",
      "[ episode 329 ][ timestamp 53 ] state=[ 0.04912708  0.74420029  0.11021744 -0.47662631], action=1, reward=1.0, next_state=[ 0.06401109  0.93760749  0.10068491 -0.73263842]\n",
      "[ episode 329 ][ timestamp 54 ] state=[ 0.06401109  0.93760749  0.10068491 -0.73263842], action=1, reward=1.0, next_state=[ 0.08276323  1.13120473  0.08603215 -0.99201221]\n",
      "[ episode 329 ][ timestamp 55 ] state=[ 0.08276323  1.13120473  0.08603215 -0.99201221], action=1, reward=1.0, next_state=[ 0.10538733  1.3250767   0.0661919  -1.25648234]\n",
      "[ episode 329 ][ timestamp 56 ] state=[ 0.10538733  1.3250767   0.0661919  -1.25648234], action=1, reward=1.0, next_state=[ 0.13188886  1.51929183  0.04106225 -1.52772088]\n",
      "[ episode 329 ][ timestamp 57 ] state=[ 0.13188886  1.51929183  0.04106225 -1.52772088], action=1, reward=1.0, next_state=[ 0.1622747   1.71389503  0.01050784 -1.80731071]\n",
      "[ episode 329 ][ timestamp 58 ] state=[ 0.1622747   1.71389503  0.01050784 -1.80731071], action=1, reward=1.0, next_state=[ 0.1965526   1.9088982  -0.02563838 -2.09671007]\n",
      "[ episode 329 ][ timestamp 59 ] state=[ 0.1965526   1.9088982  -0.02563838 -2.09671007], action=1, reward=1.0, next_state=[ 0.23473056  2.10426834 -0.06757258 -2.39720582]\n",
      "[ episode 329 ][ timestamp 60 ] state=[ 0.23473056  2.10426834 -0.06757258 -2.39720582], action=1, reward=1.0, next_state=[ 0.27681593  2.29991262 -0.1155167  -2.70985372]\n",
      "[ episode 329 ][ timestamp 61 ] state=[ 0.27681593  2.29991262 -0.1155167  -2.70985372], action=1, reward=1.0, next_state=[ 0.32281418  2.49566052 -0.16971377 -3.03540512]\n",
      "[ episode 329 ][ timestamp 62 ] state=[ 0.32281418  2.49566052 -0.16971377 -3.03540512], action=1, reward=-1.0, next_state=[ 0.37272739  2.69124346 -0.23042187 -3.37422133]\n",
      "[ Ended! ] Episode 329: Exploration_rate=0.19318370215794672. Score=62.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 330 ] state=[-0.01677044  0.03027792 -0.04351233  0.02625351]\n",
      "[ episode 330 ][ timestamp 1 ] state=[-0.01677044  0.03027792 -0.04351233  0.02625351], action=0, reward=1.0, next_state=[-0.01616489 -0.16419389 -0.04298726  0.30489653]\n",
      "[ episode 330 ][ timestamp 2 ] state=[-0.01616489 -0.16419389 -0.04298726  0.30489653], action=0, reward=1.0, next_state=[-0.01944876 -0.35867772 -0.03688933  0.58371841]\n",
      "[ episode 330 ][ timestamp 3 ] state=[-0.01944876 -0.35867772 -0.03688933  0.58371841], action=1, reward=1.0, next_state=[-0.02662232 -0.16305893 -0.02521497  0.27964685]\n",
      "[ episode 330 ][ timestamp 4 ] state=[-0.02662232 -0.16305893 -0.02521497  0.27964685], action=0, reward=1.0, next_state=[-0.0298835  -0.35781228 -0.01962203  0.5642716 ]\n",
      "[ episode 330 ][ timestamp 5 ] state=[-0.0298835  -0.35781228 -0.01962203  0.5642716 ], action=0, reward=1.0, next_state=[-0.03703974 -0.5526535  -0.0083366   0.85070866]\n",
      "[ episode 330 ][ timestamp 6 ] state=[-0.03703974 -0.5526535  -0.0083366   0.85070866], action=1, reward=1.0, next_state=[-0.04809281 -0.35741888  0.00867758  0.55541597]\n",
      "[ episode 330 ][ timestamp 7 ] state=[-0.04809281 -0.35741888  0.00867758  0.55541597], action=0, reward=1.0, next_state=[-0.05524119 -0.55266158  0.0197859   0.85082018]\n",
      "[ episode 330 ][ timestamp 8 ] state=[-0.05524119 -0.55266158  0.0197859   0.85082018], action=1, reward=1.0, next_state=[-0.06629442 -0.35781493  0.0368023   0.56442407]\n",
      "[ episode 330 ][ timestamp 9 ] state=[-0.06629442 -0.35781493  0.0368023   0.56442407], action=1, reward=1.0, next_state=[-0.07345072 -0.16322814  0.04809078  0.28355896]\n",
      "[ episode 330 ][ timestamp 10 ] state=[-0.07345072 -0.16322814  0.04809078  0.28355896], action=0, reward=1.0, next_state=[-0.07671528 -0.35900184  0.05376196  0.59101325]\n",
      "[ episode 330 ][ timestamp 11 ] state=[-0.07671528 -0.35900184  0.05376196  0.59101325], action=0, reward=1.0, next_state=[-0.08389532 -0.55483365  0.06558223  0.90013494]\n",
      "[ episode 330 ][ timestamp 12 ] state=[-0.08389532 -0.55483365  0.06558223  0.90013494], action=1, reward=1.0, next_state=[-0.09499199 -0.36065876  0.08358492  0.62876611]\n",
      "[ episode 330 ][ timestamp 13 ] state=[-0.09499199 -0.36065876  0.08358492  0.62876611], action=1, reward=1.0, next_state=[-0.10220517 -0.16679664  0.09616025  0.3635335 ]\n",
      "[ episode 330 ][ timestamp 14 ] state=[-0.10220517 -0.16679664  0.09616025  0.3635335 ], action=0, reward=1.0, next_state=[-0.1055411  -0.36314436  0.10343092  0.68492201]\n",
      "[ episode 330 ][ timestamp 15 ] state=[-0.1055411  -0.36314436  0.10343092  0.68492201], action=1, reward=1.0, next_state=[-0.11280399 -0.16959892  0.11712936  0.42650986]\n",
      "[ episode 330 ][ timestamp 16 ] state=[-0.11280399 -0.16959892  0.11712936  0.42650986], action=1, reward=1.0, next_state=[-0.11619597  0.02368615  0.12565955  0.17292614]\n",
      "[ episode 330 ][ timestamp 17 ] state=[-0.11619597  0.02368615  0.12565955  0.17292614], action=0, reward=1.0, next_state=[-0.11572224 -0.17298933  0.12911808  0.50246001]\n",
      "[ episode 330 ][ timestamp 18 ] state=[-0.11572224 -0.17298933  0.12911808  0.50246001], action=0, reward=1.0, next_state=[-0.11918203 -0.36967221  0.13916728  0.83288381]\n",
      "[ episode 330 ][ timestamp 19 ] state=[-0.11918203 -0.36967221  0.13916728  0.83288381], action=1, reward=1.0, next_state=[-0.12657547 -0.17669827  0.15582495  0.5870047 ]\n",
      "[ episode 330 ][ timestamp 20 ] state=[-0.12657547 -0.17669827  0.15582495  0.5870047 ], action=1, reward=1.0, next_state=[-0.13010944  0.0159376   0.16756505  0.34717927]\n",
      "[ episode 330 ][ timestamp 21 ] state=[-0.13010944  0.0159376   0.16756505  0.34717927], action=0, reward=1.0, next_state=[-0.12979069 -0.18112205  0.17450863  0.68766257]\n",
      "[ episode 330 ][ timestamp 22 ] state=[-0.12979069 -0.18112205  0.17450863  0.68766257], action=1, reward=1.0, next_state=[-0.13341313  0.01120357  0.18826188  0.45460124]\n",
      "[ episode 330 ][ timestamp 23 ] state=[-0.13341313  0.01120357  0.18826188  0.45460124], action=1, reward=1.0, next_state=[-0.13318906  0.2032345   0.19735391  0.22666695]\n",
      "[ episode 330 ][ timestamp 24 ] state=[-0.13318906  0.2032345   0.19735391  0.22666695], action=0, reward=1.0, next_state=[-0.12912437  0.00592001  0.20188725  0.57453966]\n",
      "[ episode 330 ][ timestamp 25 ] state=[-0.12912437  0.00592001  0.20188725  0.57453966], action=1, reward=-1.0, next_state=[-0.12900597  0.19772492  0.21337804  0.35162813]\n",
      "[ Ended! ] Episode 330: Exploration_rate=0.192217783647157. Score=25.\n",
      "[ Experience replay ] starts\n",
      "[ episode 331 ] state=[-0.00241476 -0.03619608  0.01384377  0.04563215]\n",
      "[ episode 331 ][ timestamp 1 ] state=[-0.00241476 -0.03619608  0.01384377  0.04563215], action=0, reward=1.0, next_state=[-0.00313868 -0.23151378  0.01475641  0.34265056]\n",
      "[ episode 331 ][ timestamp 2 ] state=[-0.00313868 -0.23151378  0.01475641  0.34265056], action=0, reward=1.0, next_state=[-0.00776895 -0.42684253  0.02160942  0.63995001]\n",
      "[ episode 331 ][ timestamp 3 ] state=[-0.00776895 -0.42684253  0.02160942  0.63995001], action=1, reward=1.0, next_state=[-0.01630581 -0.23202841  0.03440842  0.35414974]\n",
      "[ episode 331 ][ timestamp 4 ] state=[-0.01630581 -0.23202841  0.03440842  0.35414974], action=0, reward=1.0, next_state=[-0.02094637 -0.4276223   0.04149142  0.65748098]\n",
      "[ episode 331 ][ timestamp 5 ] state=[-0.02094637 -0.4276223   0.04149142  0.65748098], action=1, reward=1.0, next_state=[-0.02949882 -0.23310171  0.05464104  0.37814619]\n",
      "[ episode 331 ][ timestamp 6 ] state=[-0.02949882 -0.23310171  0.05464104  0.37814619], action=1, reward=1.0, next_state=[-0.03416085 -0.03879661  0.06220396  0.10318002]\n",
      "[ episode 331 ][ timestamp 7 ] state=[-0.03416085 -0.03879661  0.06220396  0.10318002], action=0, reward=1.0, next_state=[-0.03493679 -0.23475231  0.06426756  0.41482125]\n",
      "[ episode 331 ][ timestamp 8 ] state=[-0.03493679 -0.23475231  0.06426756  0.41482125], action=0, reward=1.0, next_state=[-0.03963183 -0.43072349  0.07256399  0.72705282]\n",
      "[ episode 331 ][ timestamp 9 ] state=[-0.03963183 -0.43072349  0.07256399  0.72705282], action=1, reward=1.0, next_state=[-0.0482463  -0.2366758   0.08710504  0.45806238]\n",
      "[ episode 331 ][ timestamp 10 ] state=[-0.0482463  -0.2366758   0.08710504  0.45806238], action=0, reward=1.0, next_state=[-0.05297982 -0.43291418  0.09626629  0.77688047]\n",
      "[ episode 331 ][ timestamp 11 ] state=[-0.05297982 -0.43291418  0.09626629  0.77688047], action=1, reward=1.0, next_state=[-0.0616381  -0.23923864  0.1118039   0.51597084]\n",
      "[ episode 331 ][ timestamp 12 ] state=[-0.0616381  -0.23923864  0.1118039   0.51597084], action=0, reward=1.0, next_state=[-0.06642287 -0.43574274  0.12212332  0.84168857]\n",
      "[ episode 331 ][ timestamp 13 ] state=[-0.06642287 -0.43574274  0.12212332  0.84168857], action=0, reward=1.0, next_state=[-0.07513773 -0.63230112  0.13895709  1.17014533]\n",
      "[ episode 331 ][ timestamp 14 ] state=[-0.07513773 -0.63230112  0.13895709  1.17014533], action=0, reward=1.0, next_state=[-0.08778375 -0.82892931  0.16235999  1.5029667 ]\n",
      "[ episode 331 ][ timestamp 15 ] state=[-0.08778375 -0.82892931  0.16235999  1.5029667 ], action=1, reward=1.0, next_state=[-0.10436234 -0.63610756  0.19241933  1.2650623 ]\n",
      "[ episode 331 ][ timestamp 16 ] state=[-0.10436234 -0.63610756  0.19241933  1.2650623 ], action=1, reward=-1.0, next_state=[-0.11708449 -0.44389333  0.21772057  1.03828492]\n",
      "[ Ended! ] Episode 331: Exploration_rate=0.1912566947289212. Score=16.\n",
      "[ Experience replay ] starts\n",
      "[ episode 332 ] state=[-0.01650812 -0.03061145 -0.03207702  0.01840194]\n",
      "[ episode 332 ][ timestamp 1 ] state=[-0.01650812 -0.03061145 -0.03207702  0.01840194], action=0, reward=1.0, next_state=[-0.01712035 -0.22525905 -0.03170898  0.30079411]\n",
      "[ episode 332 ][ timestamp 2 ] state=[-0.01712035 -0.22525905 -0.03170898  0.30079411], action=0, reward=1.0, next_state=[-0.02162553 -0.41991503 -0.0256931   0.58331044]\n",
      "[ episode 332 ][ timestamp 3 ] state=[-0.02162553 -0.41991503 -0.0256931   0.58331044], action=0, reward=1.0, next_state=[-0.03002383 -0.61466779 -0.01402689  0.86779022]\n",
      "[ episode 332 ][ timestamp 4 ] state=[-0.03002383 -0.61466779 -0.01402689  0.86779022], action=1, reward=1.0, next_state=[-0.04231718 -0.41935782  0.00332891  0.5707303 ]\n",
      "[ episode 332 ][ timestamp 5 ] state=[-0.04231718 -0.41935782  0.00332891  0.5707303 ], action=0, reward=1.0, next_state=[-0.05070434 -0.61452629  0.01474352  0.86446009]\n",
      "[ episode 332 ][ timestamp 6 ] state=[-0.05070434 -0.61452629  0.01474352  0.86446009], action=1, reward=1.0, next_state=[-0.06299487 -0.41960811  0.03203272  0.57644903]\n",
      "[ episode 332 ][ timestamp 7 ] state=[-0.06299487 -0.41960811  0.03203272  0.57644903], action=0, reward=1.0, next_state=[-0.07138703 -0.61516408  0.0435617   0.87904851]\n",
      "[ episode 332 ][ timestamp 8 ] state=[-0.07138703 -0.61516408  0.0435617   0.87904851], action=1, reward=1.0, next_state=[-0.08369031 -0.42066024  0.06114267  0.60037262]\n",
      "[ episode 332 ][ timestamp 9 ] state=[-0.08369031 -0.42066024  0.06114267  0.60037262], action=1, reward=1.0, next_state=[-0.09210351 -0.22644455  0.07315012  0.32755821]\n",
      "[ episode 332 ][ timestamp 10 ] state=[-0.09210351 -0.22644455  0.07315012  0.32755821], action=0, reward=1.0, next_state=[-0.0966324  -0.42252756  0.07970129  0.64238311]\n",
      "[ episode 332 ][ timestamp 11 ] state=[-0.0966324  -0.42252756  0.07970129  0.64238311], action=1, reward=1.0, next_state=[-0.10508296 -0.2286017   0.09254895  0.37582512]\n",
      "[ episode 332 ][ timestamp 12 ] state=[-0.10508296 -0.2286017   0.09254895  0.37582512], action=0, reward=1.0, next_state=[-0.10965499 -0.42490795  0.10006545  0.6961949 ]\n",
      "[ episode 332 ][ timestamp 13 ] state=[-0.10965499 -0.42490795  0.10006545  0.6961949 ], action=0, reward=1.0, next_state=[-0.11815315 -0.62126481  0.11398935  1.01862697]\n",
      "[ episode 332 ][ timestamp 14 ] state=[-0.11815315 -0.62126481  0.11398935  1.01862697], action=0, reward=1.0, next_state=[-0.13057844 -0.81770631  0.13436189  1.34481728]\n",
      "[ episode 332 ][ timestamp 15 ] state=[-0.13057844 -0.81770631  0.13436189  1.34481728], action=0, reward=1.0, next_state=[-0.14693257 -1.01423769  0.16125823  1.67634099]\n",
      "[ episode 332 ][ timestamp 16 ] state=[-0.14693257 -1.01423769  0.16125823  1.67634099], action=0, reward=1.0, next_state=[-0.16721732 -1.21082143  0.19478505  2.01459561]\n",
      "[ episode 332 ][ timestamp 17 ] state=[-0.16721732 -1.21082143  0.19478505  2.01459561], action=0, reward=-1.0, next_state=[-0.19143375 -1.40736158  0.23507697  2.36073614]\n",
      "[ Ended! ] Episode 332: Exploration_rate=0.1903004112552766. Score=17.\n",
      "[ Experience replay ] starts\n",
      "[ episode 333 ] state=[-0.02653375 -0.01525245 -0.01943731  0.00811976]\n",
      "[ episode 333 ][ timestamp 1 ] state=[-0.02653375 -0.01525245 -0.01943731  0.00811976], action=0, reward=1.0, next_state=[-0.0268388  -0.21009033 -0.01927491  0.29460716]\n",
      "[ episode 333 ][ timestamp 2 ] state=[-0.0268388  -0.21009033 -0.01927491  0.29460716], action=1, reward=1.0, next_state=[-0.03104061 -0.01469896 -0.01338277 -0.00409192]\n",
      "[ episode 333 ][ timestamp 3 ] state=[-0.03104061 -0.01469896 -0.01338277 -0.00409192], action=0, reward=1.0, next_state=[-0.03133459 -0.20962645 -0.01346461  0.28433871]\n",
      "[ episode 333 ][ timestamp 4 ] state=[-0.03133459 -0.20962645 -0.01346461  0.28433871], action=1, reward=1.0, next_state=[-0.03552712 -0.01431507 -0.00777783 -0.01256027]\n",
      "[ episode 333 ][ timestamp 5 ] state=[-0.03552712 -0.01431507 -0.00777783 -0.01256027], action=0, reward=1.0, next_state=[-0.03581342 -0.20932462 -0.00802904  0.27765854]\n",
      "[ episode 333 ][ timestamp 6 ] state=[-0.03581342 -0.20932462 -0.00802904  0.27765854], action=1, reward=1.0, next_state=[-0.03999991 -0.01408905 -0.00247587 -0.01754588]\n",
      "[ episode 333 ][ timestamp 7 ] state=[-0.03999991 -0.01408905 -0.00247587 -0.01754588], action=0, reward=1.0, next_state=[-0.04028169 -0.20917541 -0.00282679  0.27435485]\n",
      "[ episode 333 ][ timestamp 8 ] state=[-0.04028169 -0.20917541 -0.00282679  0.27435485], action=1, reward=1.0, next_state=[-0.0444652  -0.01401324  0.00266031 -0.01921831]\n",
      "[ episode 333 ][ timestamp 9 ] state=[-0.0444652  -0.01401324  0.00266031 -0.01921831], action=0, reward=1.0, next_state=[-0.04474547 -0.20917324  0.00227595  0.27430279]\n",
      "[ episode 333 ][ timestamp 10 ] state=[-0.04474547 -0.20917324  0.00227595  0.27430279], action=1, reward=1.0, next_state=[-0.04892893 -0.01408383  0.007762   -0.01766143]\n",
      "[ episode 333 ][ timestamp 11 ] state=[-0.04892893 -0.01408383  0.007762   -0.01766143], action=0, reward=1.0, next_state=[-0.04921061 -0.20931624  0.00740877  0.27746036]\n",
      "[ episode 333 ][ timestamp 12 ] state=[-0.04921061 -0.20931624  0.00740877  0.27746036], action=1, reward=1.0, next_state=[-0.05339693 -0.01430076  0.01295798 -0.01287667]\n",
      "[ episode 333 ][ timestamp 13 ] state=[-0.05339693 -0.01430076  0.01295798 -0.01287667], action=0, reward=1.0, next_state=[-0.05368295 -0.20960613  0.01270045  0.28386632]\n",
      "[ episode 333 ][ timestamp 14 ] state=[-0.05368295 -0.20960613  0.01270045  0.28386632], action=0, reward=1.0, next_state=[-0.05787507 -0.4049069   0.01837777  0.58052768]\n",
      "[ episode 333 ][ timestamp 15 ] state=[-0.05787507 -0.4049069   0.01837777  0.58052768], action=1, reward=1.0, next_state=[-0.06597321 -0.21004722  0.02998833  0.29369029]\n",
      "[ episode 333 ][ timestamp 16 ] state=[-0.06597321 -0.21004722  0.02998833  0.29369029], action=1, reward=1.0, next_state=[-0.07017415 -0.01536538  0.03586213  0.01061406]\n",
      "[ episode 333 ][ timestamp 17 ] state=[-0.07017415 -0.01536538  0.03586213  0.01061406], action=0, reward=1.0, next_state=[-0.07048146 -0.2109828   0.03607441  0.31439273]\n",
      "[ episode 333 ][ timestamp 18 ] state=[-0.07048146 -0.2109828   0.03607441  0.31439273], action=1, reward=1.0, next_state=[-0.07470112 -0.0163928   0.04236227  0.03330122]\n",
      "[ episode 333 ][ timestamp 19 ] state=[-0.07470112 -0.0163928   0.04236227  0.03330122], action=0, reward=1.0, next_state=[-0.07502897 -0.21209583  0.04302829  0.33904319]\n",
      "[ episode 333 ][ timestamp 20 ] state=[-0.07502897 -0.21209583  0.04302829  0.33904319], action=1, reward=1.0, next_state=[-0.07927089 -0.01761172  0.04980916  0.06023344]\n",
      "[ episode 333 ][ timestamp 21 ] state=[-0.07927089 -0.01761172  0.04980916  0.06023344], action=1, reward=1.0, next_state=[-0.07962312  0.176762    0.05101382 -0.21632769]\n",
      "[ episode 333 ][ timestamp 22 ] state=[-0.07962312  0.176762    0.05101382 -0.21632769], action=0, reward=1.0, next_state=[-0.07608788 -0.01905071  0.04668727  0.09200083]\n",
      "[ episode 333 ][ timestamp 23 ] state=[-0.07608788 -0.01905071  0.04668727  0.09200083], action=0, reward=1.0, next_state=[-0.0764689  -0.21480967  0.04852729  0.39904038]\n",
      "[ episode 333 ][ timestamp 24 ] state=[-0.0764689  -0.21480967  0.04852729  0.39904038], action=1, reward=1.0, next_state=[-0.08076509 -0.02040852  0.0565081   0.12204334]\n",
      "[ episode 333 ][ timestamp 25 ] state=[-0.08076509 -0.02040852  0.0565081   0.12204334], action=1, reward=1.0, next_state=[-0.08117326  0.17386023  0.05894896 -0.15229011]\n",
      "[ episode 333 ][ timestamp 26 ] state=[-0.08117326  0.17386023  0.05894896 -0.15229011], action=0, reward=1.0, next_state=[-0.07769606 -0.02205409  0.05590316  0.15839188]\n",
      "[ episode 333 ][ timestamp 27 ] state=[-0.07769606 -0.02205409  0.05590316  0.15839188], action=1, reward=1.0, next_state=[-0.07813714  0.17222479  0.059071   -0.11614422]\n",
      "[ episode 333 ][ timestamp 28 ] state=[-0.07813714  0.17222479  0.059071   -0.11614422], action=0, reward=1.0, next_state=[-0.07469264 -0.02369162  0.05674811  0.19457459]\n",
      "[ episode 333 ][ timestamp 29 ] state=[-0.07469264 -0.02369162  0.05674811  0.19457459], action=1, reward=1.0, next_state=[-0.07516647  0.1705746   0.0606396  -0.07968066]\n",
      "[ episode 333 ][ timestamp 30 ] state=[-0.07516647  0.1705746   0.0606396  -0.07968066], action=0, reward=1.0, next_state=[-0.07175498 -0.02536186  0.05904599  0.23150094]\n",
      "[ episode 333 ][ timestamp 31 ] state=[-0.07175498 -0.02536186  0.05904599  0.23150094], action=1, reward=1.0, next_state=[-0.07226222  0.16886885  0.06367601 -0.04198796]\n",
      "[ episode 333 ][ timestamp 32 ] state=[-0.07226222  0.16886885  0.06367601 -0.04198796], action=1, reward=1.0, next_state=[-0.06888484  0.36302265  0.06283625 -0.31392033]\n",
      "[ episode 333 ][ timestamp 33 ] state=[-0.06888484  0.36302265  0.06283625 -0.31392033], action=0, reward=1.0, next_state=[-0.06162439  0.16706448  0.05655784 -0.00210148]\n",
      "[ episode 333 ][ timestamp 34 ] state=[-0.06162439  0.16706448  0.05655784 -0.00210148], action=1, reward=1.0, next_state=[-0.0582831   0.36133161  0.05651582 -0.2764171 ]\n",
      "[ episode 333 ][ timestamp 35 ] state=[-0.0582831   0.36133161  0.05651582 -0.2764171 ], action=0, reward=1.0, next_state=[-0.05105647  0.16545081  0.05098747  0.0335418 ]\n",
      "[ episode 333 ][ timestamp 36 ] state=[-0.05105647  0.16545081  0.05098747  0.0335418 ], action=1, reward=1.0, next_state=[-0.04774745  0.35980591  0.05165831 -0.24262816]\n",
      "[ episode 333 ][ timestamp 37 ] state=[-0.04774745  0.35980591  0.05165831 -0.24262816], action=0, reward=1.0, next_state=[-0.04055133  0.16398559  0.04680575  0.06589128]\n",
      "[ episode 333 ][ timestamp 38 ] state=[-0.04055133  0.16398559  0.04680575  0.06589128], action=1, reward=1.0, next_state=[-0.03727162  0.35840631  0.04812357 -0.21166455]\n",
      "[ episode 333 ][ timestamp 39 ] state=[-0.03727162  0.35840631  0.04812357 -0.21166455], action=0, reward=1.0, next_state=[-0.0301035   0.16263051  0.04389028  0.09580204]\n",
      "[ episode 333 ][ timestamp 40 ] state=[-0.0301035   0.16263051  0.04389028  0.09580204], action=1, reward=1.0, next_state=[-0.02685089  0.35709682  0.04580632 -0.18271691]\n",
      "[ episode 333 ][ timestamp 41 ] state=[-0.02685089  0.35709682  0.04580632 -0.18271691], action=0, reward=1.0, next_state=[-0.01970895  0.16135038  0.04215198  0.12405711]\n",
      "[ episode 333 ][ timestamp 42 ] state=[-0.01970895  0.16135038  0.04215198  0.12405711], action=1, reward=1.0, next_state=[-0.01648194  0.35584389  0.04463313 -0.15503499]\n",
      "[ episode 333 ][ timestamp 43 ] state=[-0.01648194  0.35584389  0.04463313 -0.15503499], action=1, reward=1.0, next_state=[-0.00936506  0.55029931  0.04153243 -0.43330985]\n",
      "[ episode 333 ][ timestamp 44 ] state=[-0.00936506  0.55029931  0.04153243 -0.43330985], action=0, reward=1.0, next_state=[ 0.00164092  0.3546147   0.03286623 -0.12782903]\n",
      "[ episode 333 ][ timestamp 45 ] state=[ 0.00164092  0.3546147   0.03286623 -0.12782903], action=0, reward=1.0, next_state=[0.00873322 0.15903771 0.03030965 0.17503895]\n",
      "[ episode 333 ][ timestamp 46 ] state=[0.00873322 0.15903771 0.03030965 0.17503895], action=1, reward=1.0, next_state=[ 0.01191397  0.35371307  0.03381043 -0.10793029]\n",
      "[ episode 333 ][ timestamp 47 ] state=[ 0.01191397  0.35371307  0.03381043 -0.10793029], action=0, reward=1.0, next_state=[0.01898823 0.15812333 0.03165182 0.19522501]\n",
      "[ episode 333 ][ timestamp 48 ] state=[0.01898823 0.15812333 0.03165182 0.19522501], action=1, reward=1.0, next_state=[ 0.0221507   0.35277856  0.03555632 -0.08730751]\n",
      "[ episode 333 ][ timestamp 49 ] state=[ 0.0221507   0.35277856  0.03555632 -0.08730751], action=1, reward=1.0, next_state=[ 0.02920627  0.54737329  0.03381017 -0.36856375]\n",
      "[ episode 333 ][ timestamp 50 ] state=[ 0.02920627  0.54737329  0.03381017 -0.36856375], action=0, reward=1.0, next_state=[ 0.04015374  0.35178765  0.0264389  -0.06541467]\n",
      "[ episode 333 ][ timestamp 51 ] state=[ 0.04015374  0.35178765  0.0264389  -0.06541467], action=1, reward=1.0, next_state=[ 0.04718949  0.54652076  0.0251306  -0.34964011]\n",
      "[ episode 333 ][ timestamp 52 ] state=[ 0.04718949  0.54652076  0.0251306  -0.34964011], action=0, reward=1.0, next_state=[ 0.0581199   0.35105058  0.0181378  -0.0491398 ]\n",
      "[ episode 333 ][ timestamp 53 ] state=[ 0.0581199   0.35105058  0.0181378  -0.0491398 ], action=1, reward=1.0, next_state=[ 0.06514092  0.54590782  0.017155   -0.33604536]\n",
      "[ episode 333 ][ timestamp 54 ] state=[ 0.06514092  0.54590782  0.017155   -0.33604536], action=0, reward=1.0, next_state=[ 0.07605907  0.35054598  0.0104341  -0.03800241]\n",
      "[ episode 333 ][ timestamp 55 ] state=[ 0.07605907  0.35054598  0.0104341  -0.03800241], action=1, reward=1.0, next_state=[ 0.08306999  0.54551676  0.00967405 -0.32737509]\n",
      "[ episode 333 ][ timestamp 56 ] state=[ 0.08306999  0.54551676  0.00967405 -0.32737509], action=0, reward=1.0, next_state=[ 0.09398033  0.35025843  0.00312655 -0.03165717]\n",
      "[ episode 333 ][ timestamp 57 ] state=[ 0.09398033  0.35025843  0.00312655 -0.03165717], action=1, reward=1.0, next_state=[ 0.1009855   0.54533541  0.0024934  -0.323352  ]\n",
      "[ episode 333 ][ timestamp 58 ] state=[ 0.1009855   0.54533541  0.0024934  -0.323352  ], action=0, reward=1.0, next_state=[ 0.1118922   0.35017804 -0.00397364 -0.0298838 ]\n",
      "[ episode 333 ][ timestamp 59 ] state=[ 0.1118922   0.35017804 -0.00397364 -0.0298838 ], action=0, reward=1.0, next_state=[ 0.11889576  0.1551133  -0.00457131  0.26154276]\n",
      "[ episode 333 ][ timestamp 60 ] state=[ 0.11889576  0.1551133  -0.00457131  0.26154276], action=1, reward=1.0, next_state=[ 0.12199803  0.3503002   0.00065954 -0.0325785 ]\n",
      "[ episode 333 ][ timestamp 61 ] state=[ 0.12199803  0.3503002   0.00065954 -0.0325785 ], action=0, reward=1.0, next_state=[1.29004034e-01 1.55168800e-01 7.97269510e-06 2.60312442e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 333 ][ timestamp 62 ] state=[1.29004034e-01 1.55168800e-01 7.97269510e-06 2.60312442e-01], action=1, reward=1.0, next_state=[ 0.13210741  0.35029064  0.00521422 -0.03236797]\n",
      "[ episode 333 ][ timestamp 63 ] state=[ 0.13210741  0.35029064  0.00521422 -0.03236797], action=0, reward=1.0, next_state=[0.13911322 0.1550943  0.00456686 0.26195553]\n",
      "[ episode 333 ][ timestamp 64 ] state=[0.13911322 0.1550943  0.00456686 0.26195553], action=1, reward=1.0, next_state=[ 0.14221511  0.35015077  0.00980597 -0.02928347]\n",
      "[ episode 333 ][ timestamp 65 ] state=[ 0.14221511  0.35015077  0.00980597 -0.02928347], action=0, reward=1.0, next_state=[0.14921812 0.15488957 0.0092203  0.26647715]\n",
      "[ episode 333 ][ timestamp 66 ] state=[0.14921812 0.15488957 0.0092203  0.26647715], action=1, reward=1.0, next_state=[ 0.15231592  0.34987873  0.01454985 -0.02328341]\n",
      "[ episode 333 ][ timestamp 67 ] state=[ 0.15231592  0.34987873  0.01454985 -0.02328341], action=1, reward=1.0, next_state=[ 0.15931349  0.54478903  0.01408418 -0.31134042]\n",
      "[ episode 333 ][ timestamp 68 ] state=[ 0.15931349  0.54478903  0.01408418 -0.31134042], action=0, reward=1.0, next_state=[ 0.17020927  0.34946928  0.00785737 -0.01424925]\n",
      "[ episode 333 ][ timestamp 69 ] state=[ 0.17020927  0.34946928  0.00785737 -0.01424925], action=0, reward=1.0, next_state=[0.17719866 0.15423553 0.00757238 0.28090238]\n",
      "[ episode 333 ][ timestamp 70 ] state=[0.17719866 0.15423553 0.00757238 0.28090238], action=1, reward=1.0, next_state=[ 0.18028337  0.34924866  0.01319043 -0.00938266]\n",
      "[ episode 333 ][ timestamp 71 ] state=[ 0.18028337  0.34924866  0.01319043 -0.00938266], action=1, reward=1.0, next_state=[ 0.18726834  0.54417898  0.01300278 -0.29787483]\n",
      "[ episode 333 ][ timestamp 72 ] state=[ 0.18726834  0.54417898  0.01300278 -0.29787483], action=0, reward=1.0, next_state=[ 0.19815192  0.34887411  0.00704528 -0.00111958]\n",
      "[ episode 333 ][ timestamp 73 ] state=[ 0.19815192  0.34887411  0.00704528 -0.00111958], action=1, reward=1.0, next_state=[ 0.2051294   0.54389432  0.00702289 -0.29157134]\n",
      "[ episode 333 ][ timestamp 74 ] state=[ 0.2051294   0.54389432  0.00702289 -0.29157134], action=0, reward=1.0, next_state=[0.21600729 0.34867294 0.00119146 0.00331822]\n",
      "[ episode 333 ][ timestamp 75 ] state=[0.21600729 0.34867294 0.00119146 0.00331822], action=0, reward=1.0, next_state=[0.22298075 0.15353392 0.00125783 0.29637683]\n",
      "[ episode 333 ][ timestamp 76 ] state=[0.22298075 0.15353392 0.00125783 0.29637683], action=1, reward=1.0, next_state=[0.22605143 0.34863792 0.00718537 0.00409087]\n",
      "[ episode 333 ][ timestamp 77 ] state=[0.22605143 0.34863792 0.00718537 0.00409087], action=0, reward=1.0, next_state=[0.23302418 0.15341366 0.00726718 0.29903218]\n",
      "[ episode 333 ][ timestamp 78 ] state=[0.23302418 0.15341366 0.00726718 0.29903218], action=1, reward=1.0, next_state=[0.23609246 0.34843127 0.01324783 0.00865002]\n",
      "[ episode 333 ][ timestamp 79 ] state=[0.23609246 0.34843127 0.01324783 0.00865002], action=1, reward=1.0, next_state=[ 0.24306108  0.54336075  0.01342083 -0.2798238 ]\n",
      "[ episode 333 ][ timestamp 80 ] state=[ 0.24306108  0.54336075  0.01342083 -0.2798238 ], action=0, reward=1.0, next_state=[0.2539283  0.34804995 0.00782435 0.01706163]\n",
      "[ episode 333 ][ timestamp 81 ] state=[0.2539283  0.34804995 0.00782435 0.01706163], action=1, reward=1.0, next_state=[ 0.2608893   0.54305882  0.00816558 -0.27314239]\n",
      "[ episode 333 ][ timestamp 82 ] state=[ 0.2608893   0.54305882  0.00816558 -0.27314239], action=0, reward=1.0, next_state=[0.27175047 0.34782131 0.00270274 0.02210476]\n",
      "[ episode 333 ][ timestamp 83 ] state=[0.27175047 0.34782131 0.00270274 0.02210476], action=0, reward=1.0, next_state=[0.2787069  0.15266071 0.00314483 0.31563921]\n",
      "[ episode 333 ][ timestamp 84 ] state=[0.2787069  0.15266071 0.00314483 0.31563921], action=1, reward=1.0, next_state=[0.28176011 0.34773772 0.00945761 0.02394971]\n",
      "[ episode 333 ][ timestamp 85 ] state=[0.28176011 0.34773772 0.00945761 0.02394971], action=1, reward=1.0, next_state=[ 0.28871487  0.54272277  0.00993661 -0.26573429]\n",
      "[ episode 333 ][ timestamp 86 ] state=[ 0.28871487  0.54272277  0.00993661 -0.26573429], action=0, reward=1.0, next_state=[0.29956932 0.34746042 0.00462192 0.03006609]\n",
      "[ episode 333 ][ timestamp 87 ] state=[0.29956932 0.34746042 0.00462192 0.03006609], action=0, reward=1.0, next_state=[0.30651853 0.15227249 0.00522325 0.3242037 ]\n",
      "[ episode 333 ][ timestamp 88 ] state=[0.30651853 0.15227249 0.00522325 0.3242037 ], action=1, reward=1.0, next_state=[0.30956398 0.34731968 0.01170732 0.03317253]\n",
      "[ episode 333 ][ timestamp 89 ] state=[0.30956398 0.34731968 0.01170732 0.03317253], action=1, reward=1.0, next_state=[ 0.31651038  0.54227181  0.01237077 -0.25579374]\n",
      "[ episode 333 ][ timestamp 90 ] state=[ 0.31651038  0.54227181  0.01237077 -0.25579374], action=0, reward=1.0, next_state=[0.32735581 0.34697544 0.00725489 0.04076531]\n",
      "[ episode 333 ][ timestamp 91 ] state=[0.32735581 0.34697544 0.00725489 0.04076531], action=1, reward=1.0, next_state=[ 0.33429532  0.54199261  0.0080702  -0.24961982]\n",
      "[ episode 333 ][ timestamp 92 ] state=[ 0.33429532  0.54199261  0.0080702  -0.24961982], action=0, reward=1.0, next_state=[0.34513517 0.34675634 0.0030778  0.04559765]\n",
      "[ episode 333 ][ timestamp 93 ] state=[0.34513517 0.34675634 0.0030778  0.04559765], action=0, reward=1.0, next_state=[0.3520703  0.15159039 0.00398976 0.33925006]\n",
      "[ episode 333 ][ timestamp 94 ] state=[0.3520703  0.15159039 0.00398976 0.33925006], action=1, reward=1.0, next_state=[0.35510211 0.34665535 0.01077476 0.04782794]\n",
      "[ episode 333 ][ timestamp 95 ] state=[0.35510211 0.34665535 0.01077476 0.04782794], action=1, reward=1.0, next_state=[ 0.36203521  0.54162115  0.01173132 -0.24143607]\n",
      "[ episode 333 ][ timestamp 96 ] state=[ 0.36203521  0.54162115  0.01173132 -0.24143607], action=0, reward=1.0, next_state=[0.37286764 0.34633361 0.0069026  0.05492402]\n",
      "[ episode 333 ][ timestamp 97 ] state=[0.37286764 0.34633361 0.0069026  0.05492402], action=1, reward=1.0, next_state=[ 0.37979431  0.54135591  0.00800108 -0.23557312]\n",
      "[ episode 333 ][ timestamp 98 ] state=[ 0.37979431  0.54135591  0.00800108 -0.23557312], action=0, reward=1.0, next_state=[0.39062143 0.34612056 0.00328961 0.05962282]\n",
      "[ episode 333 ][ timestamp 99 ] state=[0.39062143 0.34612056 0.00328961 0.05962282], action=0, reward=1.0, next_state=[0.39754384 0.1509516  0.00448207 0.35334182]\n",
      "[ episode 333 ][ timestamp 100 ] state=[0.39754384 0.1509516  0.00448207 0.35334182], action=1, reward=1.0, next_state=[0.40056287 0.34600953 0.01154891 0.06207559]\n",
      "[ episode 333 ][ timestamp 101 ] state=[0.40056287 0.34600953 0.01154891 0.06207559], action=1, reward=1.0, next_state=[ 0.40748306  0.54096401  0.01279042 -0.22694132]\n",
      "[ episode 333 ][ timestamp 102 ] state=[ 0.40748306  0.54096401  0.01279042 -0.22694132], action=0, reward=1.0, next_state=[0.41830234 0.34566163 0.00825159 0.06974857]\n",
      "[ episode 333 ][ timestamp 103 ] state=[0.41830234 0.34566163 0.00825159 0.06974857], action=1, reward=1.0, next_state=[ 0.42521557  0.54066431  0.00964656 -0.22031956]\n",
      "[ episode 333 ][ timestamp 104 ] state=[ 0.42521557  0.54066431  0.00964656 -0.22031956], action=0, reward=1.0, next_state=[0.43602886 0.34540581 0.00524017 0.07539061]\n",
      "[ episode 333 ][ timestamp 105 ] state=[0.43602886 0.34540581 0.00524017 0.07539061], action=1, reward=1.0, next_state=[ 0.44293698  0.54045225  0.00674798 -0.21563442]\n",
      "[ episode 333 ][ timestamp 106 ] state=[ 0.44293698  0.54045225  0.00674798 -0.21563442], action=0, reward=1.0, next_state=[0.45374602 0.34523448 0.0024353  0.07916946]\n",
      "[ episode 333 ][ timestamp 107 ] state=[0.45374602 0.34523448 0.0024353  0.07916946], action=0, reward=1.0, next_state=[0.46065071 0.1500777  0.00401869 0.37261973]\n",
      "[ episode 333 ][ timestamp 108 ] state=[0.46065071 0.1500777  0.00401869 0.37261973], action=0, reward=1.0, next_state=[ 0.46365226 -0.04510111  0.01147108  0.66656707]\n",
      "[ episode 333 ][ timestamp 109 ] state=[ 0.46365226 -0.04510111  0.01147108  0.66656707], action=1, reward=1.0, next_state=[0.46275024 0.14985944 0.02480242 0.37751791]\n",
      "[ episode 333 ][ timestamp 110 ] state=[0.46275024 0.14985944 0.02480242 0.37751791], action=1, reward=1.0, next_state=[0.46574743 0.34462052 0.03235278 0.09275731]\n",
      "[ episode 333 ][ timestamp 111 ] state=[0.46574743 0.34462052 0.03235278 0.09275731], action=1, reward=1.0, next_state=[ 0.47263984  0.53926418  0.03420793 -0.18954533]\n",
      "[ episode 333 ][ timestamp 112 ] state=[ 0.47263984  0.53926418  0.03420793 -0.18954533], action=0, reward=1.0, next_state=[0.48342513 0.34366995 0.03041702 0.11372953]\n",
      "[ episode 333 ][ timestamp 113 ] state=[0.48342513 0.34366995 0.03041702 0.11372953], action=0, reward=1.0, next_state=[0.49029852 0.14812567 0.03269161 0.41585151]\n",
      "[ episode 333 ][ timestamp 114 ] state=[0.49029852 0.14812567 0.03269161 0.41585151], action=1, reward=1.0, next_state=[0.49326104 0.3427694  0.04100864 0.13365152]\n",
      "[ episode 333 ][ timestamp 115 ] state=[0.49326104 0.3427694  0.04100864 0.13365152], action=1, reward=1.0, next_state=[ 0.50011643  0.53728068  0.04368167 -0.14581693]\n",
      "[ episode 333 ][ timestamp 116 ] state=[ 0.50011643  0.53728068  0.04368167 -0.14581693], action=0, reward=1.0, next_state=[0.51086204 0.34156128 0.04076533 0.16032045]\n",
      "[ episode 333 ][ timestamp 117 ] state=[0.51086204 0.34156128 0.04076533 0.16032045], action=1, reward=1.0, next_state=[ 0.51769326  0.53607663  0.04397174 -0.11922848]\n",
      "[ episode 333 ][ timestamp 118 ] state=[ 0.51769326  0.53607663  0.04397174 -0.11922848], action=0, reward=1.0, next_state=[0.5284148  0.34035315 0.04158717 0.18699648]\n",
      "[ episode 333 ][ timestamp 119 ] state=[0.5284148  0.34035315 0.04158717 0.18699648], action=1, reward=1.0, next_state=[ 0.53522186  0.53485619  0.0453271  -0.09228271]\n",
      "[ episode 333 ][ timestamp 120 ] state=[ 0.53522186  0.53485619  0.0453271  -0.09228271], action=1, reward=1.0, next_state=[ 0.54591898  0.72930014  0.04348145 -0.37032746]\n",
      "[ episode 333 ][ timestamp 121 ] state=[ 0.54591898  0.72930014  0.04348145 -0.37032746], action=0, reward=1.0, next_state=[ 0.56050499  0.53358826  0.0360749  -0.0642576 ]\n",
      "[ episode 333 ][ timestamp 122 ] state=[ 0.56050499  0.53358826  0.0360749  -0.0642576 ], action=0, reward=1.0, next_state=[0.57117675 0.33796816 0.03478975 0.23958537]\n",
      "[ episode 333 ][ timestamp 123 ] state=[0.57117675 0.33796816 0.03478975 0.23958537], action=0, reward=1.0, next_state=[0.57793612 0.14236694 0.03958145 0.54303578]\n",
      "[ episode 333 ][ timestamp 124 ] state=[0.57793612 0.14236694 0.03958145 0.54303578], action=1, reward=1.0, next_state=[0.58078345 0.33691092 0.05044217 0.26308228]\n",
      "[ episode 333 ][ timestamp 125 ] state=[0.58078345 0.33691092 0.05044217 0.26308228], action=1, reward=1.0, next_state=[ 0.58752167  0.53127793  0.05570381 -0.0132737 ]\n",
      "[ episode 333 ][ timestamp 126 ] state=[ 0.58752167  0.53127793  0.05570381 -0.0132737 ], action=1, reward=1.0, next_state=[ 0.59814723  0.7255586   0.05543834 -0.28787424]\n",
      "[ episode 333 ][ timestamp 127 ] state=[ 0.59814723  0.7255586   0.05543834 -0.28787424], action=0, reward=1.0, next_state=[0.6126584  0.5296917  0.04968086 0.02176528]\n",
      "[ episode 333 ][ timestamp 128 ] state=[0.6126584  0.5296917  0.04968086 0.02176528], action=1, reward=1.0, next_state=[ 0.62325224  0.72406727  0.05011616 -0.25483818]\n",
      "[ episode 333 ][ timestamp 129 ] state=[ 0.62325224  0.72406727  0.05011616 -0.25483818], action=0, reward=1.0, next_state=[0.63773358 0.52826691 0.0450194  0.05322159]\n",
      "[ episode 333 ][ timestamp 130 ] state=[0.63773358 0.52826691 0.0450194  0.05322159], action=1, reward=1.0, next_state=[ 0.64829892  0.7227154   0.04608383 -0.22492439]\n",
      "[ episode 333 ][ timestamp 131 ] state=[ 0.64829892  0.7227154   0.04608383 -0.22492439], action=1, reward=1.0, next_state=[ 0.66275323  0.91714946  0.04158534 -0.50272199]\n",
      "[ episode 333 ][ timestamp 132 ] state=[ 0.66275323  0.91714946  0.04158534 -0.50272199], action=0, reward=1.0, next_state=[ 0.68109622  0.72146679  0.0315309  -0.19722919]\n",
      "[ episode 333 ][ timestamp 133 ] state=[ 0.68109622  0.72146679  0.0315309  -0.19722919], action=0, reward=1.0, next_state=[0.69552555 0.52590836 0.02758632 0.1052312 ]\n",
      "[ episode 333 ][ timestamp 134 ] state=[0.69552555 0.52590836 0.02758632 0.1052312 ], action=1, reward=1.0, next_state=[ 0.70604372  0.72062434  0.02969094 -0.1786223 ]\n",
      "[ episode 333 ][ timestamp 135 ] state=[ 0.70604372  0.72062434  0.02969094 -0.1786223 ], action=0, reward=1.0, next_state=[0.72045621 0.52509036 0.0261185  0.12327726]\n",
      "[ episode 333 ][ timestamp 136 ] state=[0.72045621 0.52509036 0.0261185  0.12327726], action=1, reward=1.0, next_state=[ 0.73095802  0.71982858  0.02858404 -0.16105247]\n",
      "[ episode 333 ][ timestamp 137 ] state=[ 0.73095802  0.71982858  0.02858404 -0.16105247], action=1, reward=1.0, next_state=[ 0.74535459  0.9145299   0.02536299 -0.44458259]\n",
      "[ episode 333 ][ timestamp 138 ] state=[ 0.74535459  0.9145299   0.02536299 -0.44458259], action=0, reward=1.0, next_state=[ 0.76364518  0.71905845  0.01647134 -0.1440138 ]\n",
      "[ episode 333 ][ timestamp 139 ] state=[ 0.76364518  0.71905845  0.01647134 -0.1440138 ], action=0, reward=1.0, next_state=[0.77802635 0.52370453 0.01359106 0.15381969]\n",
      "[ episode 333 ][ timestamp 140 ] state=[0.77802635 0.52370453 0.01359106 0.15381969], action=1, reward=1.0, next_state=[ 0.78850044  0.71862927  0.01666746 -0.13454477]\n",
      "[ episode 333 ][ timestamp 141 ] state=[ 0.78850044  0.71862927  0.01666746 -0.13454477], action=0, reward=1.0, next_state=[0.80287303 0.52327259 0.01397656 0.16334955]\n",
      "[ episode 333 ][ timestamp 142 ] state=[0.80287303 0.52327259 0.01397656 0.16334955], action=0, reward=1.0, next_state=[0.81333848 0.32795338 0.01724355 0.46040873]\n",
      "[ episode 333 ][ timestamp 143 ] state=[0.81333848 0.32795338 0.01724355 0.46040873], action=1, reward=1.0, next_state=[0.81989755 0.52282741 0.02645173 0.1732105 ]\n",
      "[ episode 333 ][ timestamp 144 ] state=[0.81989755 0.52282741 0.02645173 0.1732105 ], action=0, reward=1.0, next_state=[0.8303541  0.32733706 0.02991594 0.47411935]\n",
      "[ episode 333 ][ timestamp 145 ] state=[0.8303541  0.32733706 0.02991594 0.47411935], action=1, reward=1.0, next_state=[0.83690084 0.52202404 0.03939832 0.19101351]\n",
      "[ episode 333 ][ timestamp 146 ] state=[0.83690084 0.52202404 0.03939832 0.19101351], action=1, reward=1.0, next_state=[ 0.84734132  0.71656086  0.04321859 -0.08898517]\n",
      "[ episode 333 ][ timestamp 147 ] state=[ 0.84734132  0.71656086  0.04321859 -0.08898517], action=1, reward=1.0, next_state=[ 0.86167254  0.91103754  0.04143889 -0.36772547]\n",
      "[ episode 333 ][ timestamp 148 ] state=[ 0.86167254  0.91103754  0.04143889 -0.36772547], action=0, reward=1.0, next_state=[ 0.87989329  0.71535201  0.03408438 -0.06226962]\n",
      "[ episode 333 ][ timestamp 149 ] state=[ 0.87989329  0.71535201  0.03408438 -0.06226962], action=0, reward=1.0, next_state=[0.89420033 0.51975837 0.03283899 0.24096931]\n",
      "[ episode 333 ][ timestamp 150 ] state=[0.89420033 0.51975837 0.03283899 0.24096931], action=1, reward=1.0, next_state=[ 0.9045955   0.71439621  0.03765838 -0.04117711]\n",
      "[ episode 333 ][ timestamp 151 ] state=[ 0.9045955   0.71439621  0.03765838 -0.04117711], action=0, reward=1.0, next_state=[0.91888342 0.51875504 0.03683483 0.26314552]\n",
      "[ episode 333 ][ timestamp 152 ] state=[0.91888342 0.51875504 0.03683483 0.26314552], action=1, reward=1.0, next_state=[ 0.92925852  0.71333239  0.04209774 -0.01769552]\n",
      "[ episode 333 ][ timestamp 153 ] state=[ 0.92925852  0.71333239  0.04209774 -0.01769552], action=0, reward=1.0, next_state=[0.94352517 0.51763278 0.04174383 0.28796689]\n",
      "[ episode 333 ][ timestamp 154 ] state=[0.94352517 0.51763278 0.04174383 0.28796689], action=0, reward=1.0, next_state=[0.95387782 0.32194117 0.04750317 0.59351771]\n",
      "[ episode 333 ][ timestamp 155 ] state=[0.95387782 0.32194117 0.04750317 0.59351771], action=1, reward=1.0, next_state=[0.96031665 0.51636712 0.05937353 0.31616845]\n",
      "[ episode 333 ][ timestamp 156 ] state=[0.96031665 0.51636712 0.05937353 0.31616845], action=0, reward=1.0, next_state=[0.97064399 0.32045193 0.06569689 0.62696897]\n",
      "[ episode 333 ][ timestamp 157 ] state=[0.97064399 0.32045193 0.06569689 0.62696897], action=1, reward=1.0, next_state=[0.97705303 0.51459835 0.07823627 0.35567857]\n",
      "[ episode 333 ][ timestamp 158 ] state=[0.97705303 0.51459835 0.07823627 0.35567857], action=0, reward=1.0, next_state=[0.98734499 0.31845626 0.08534985 0.67196974]\n",
      "[ episode 333 ][ timestamp 159 ] state=[0.98734499 0.31845626 0.08534985 0.67196974], action=1, reward=1.0, next_state=[0.99371412 0.51229459 0.09878924 0.40733303]\n",
      "[ episode 333 ][ timestamp 160 ] state=[0.99371412 0.51229459 0.09878924 0.40733303], action=1, reward=1.0, next_state=[1.00396001 0.70588713 0.1069359  0.14735689]\n",
      "[ episode 333 ][ timestamp 161 ] state=[1.00396001 0.70588713 0.1069359  0.14735689], action=0, reward=1.0, next_state=[1.01807775 0.50940926 0.10988304 0.47176948]\n",
      "[ episode 333 ][ timestamp 162 ] state=[1.01807775 0.50940926 0.10988304 0.47176948], action=1, reward=1.0, next_state=[1.02826594 0.70282161 0.11931843 0.21564133]\n",
      "[ episode 333 ][ timestamp 163 ] state=[1.02826594 0.70282161 0.11931843 0.21564133], action=0, reward=1.0, next_state=[1.04232237 0.50621389 0.12363126 0.54345252]\n",
      "[ episode 333 ][ timestamp 164 ] state=[1.04232237 0.50621389 0.12363126 0.54345252], action=1, reward=1.0, next_state=[1.05244665 0.69940137 0.13450031 0.29213815]\n",
      "[ episode 333 ][ timestamp 165 ] state=[1.05244665 0.69940137 0.13450031 0.29213815], action=1, reward=1.0, next_state=[1.06643468 0.89237484 0.14034307 0.0447162 ]\n",
      "[ episode 333 ][ timestamp 166 ] state=[1.06643468 0.89237484 0.14034307 0.0447162 ], action=0, reward=1.0, next_state=[1.08428217 0.69554857 0.14123739 0.37817838]\n",
      "[ episode 333 ][ timestamp 167 ] state=[1.08428217 0.69554857 0.14123739 0.37817838], action=1, reward=1.0, next_state=[1.09819315 0.8884118  0.14880096 0.13315005]\n",
      "[ episode 333 ][ timestamp 168 ] state=[1.09819315 0.8884118  0.14880096 0.13315005], action=0, reward=1.0, next_state=[1.11596138 0.69150641 0.15146396 0.4688305 ]\n",
      "[ episode 333 ][ timestamp 169 ] state=[1.11596138 0.69150641 0.15146396 0.4688305 ], action=1, reward=1.0, next_state=[1.12979151 0.88420058 0.16084057 0.22745875]\n",
      "[ episode 333 ][ timestamp 170 ] state=[1.12979151 0.88420058 0.16084057 0.22745875], action=1, reward=1.0, next_state=[ 1.14747552  1.07670225  0.16538975 -0.01048332]\n",
      "[ episode 333 ][ timestamp 171 ] state=[ 1.14747552  1.07670225  0.16538975 -0.01048332], action=1, reward=1.0, next_state=[ 1.16900957  1.26911369  0.16518008 -0.24675888]\n",
      "[ episode 333 ][ timestamp 172 ] state=[ 1.16900957  1.26911369  0.16518008 -0.24675888], action=0, reward=1.0, next_state=[1.19439184 1.07206527 0.1602449  0.09313306]\n",
      "[ episode 333 ][ timestamp 173 ] state=[1.19439184 1.07206527 0.1602449  0.09313306], action=0, reward=1.0, next_state=[1.21583315 0.87505267 0.16210756 0.43177647]\n",
      "[ episode 333 ][ timestamp 174 ] state=[1.21583315 0.87505267 0.16210756 0.43177647], action=1, reward=1.0, next_state=[1.2333342  1.06755272 0.17074309 0.19426326]\n",
      "[ episode 333 ][ timestamp 175 ] state=[1.2333342  1.06755272 0.17074309 0.19426326], action=1, reward=1.0, next_state=[ 1.25468525  1.25987325  0.17462836 -0.04006776]\n",
      "[ episode 333 ][ timestamp 176 ] state=[ 1.25468525  1.25987325  0.17462836 -0.04006776], action=0, reward=1.0, next_state=[1.27988272 1.06273319 0.173827   0.30222513]\n",
      "[ episode 333 ][ timestamp 177 ] state=[1.27988272 1.06273319 0.173827   0.30222513], action=1, reward=1.0, next_state=[1.30113738 1.2550067  0.17987151 0.06900935]\n",
      "[ episode 333 ][ timestamp 178 ] state=[1.30113738 1.2550067  0.17987151 0.06900935], action=0, reward=1.0, next_state=[1.32623752 1.05782303 0.18125169 0.41261054]\n",
      "[ episode 333 ][ timestamp 179 ] state=[1.32623752 1.05782303 0.18125169 0.41261054], action=1, reward=1.0, next_state=[1.34739398 1.2499752  0.1895039  0.1821005 ]\n",
      "[ episode 333 ][ timestamp 180 ] state=[1.34739398 1.2499752  0.1895039  0.1821005 ], action=0, reward=1.0, next_state=[1.37239348 1.05271834 0.19314591 0.52807009]\n",
      "[ episode 333 ][ timestamp 181 ] state=[1.37239348 1.05271834 0.19314591 0.52807009], action=0, reward=1.0, next_state=[1.39344785 0.85547872 0.20370732 0.87486056]\n",
      "[ episode 333 ][ timestamp 182 ] state=[1.39344785 0.85547872 0.20370732 0.87486056], action=1, reward=-1.0, next_state=[1.41055742 1.0473359  0.22120453 0.65250186]\n",
      "[ Ended! ] Episode 333: Exploration_rate=0.18934890919900021. Score=182.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 334 ] state=[-0.01737706 -0.03443965 -0.03603813 -0.006228  ]\n",
      "[ episode 334 ][ timestamp 1 ] state=[-0.01737706 -0.03443965 -0.03603813 -0.006228  ], action=0, reward=1.0, next_state=[-0.01806586 -0.22902673 -0.03616269  0.27487018]\n",
      "[ episode 334 ][ timestamp 2 ] state=[-0.01806586 -0.22902673 -0.03616269  0.27487018], action=1, reward=1.0, next_state=[-0.02264639 -0.03340798 -0.03066528 -0.02899562]\n",
      "[ episode 334 ][ timestamp 3 ] state=[-0.02264639 -0.03340798 -0.03066528 -0.02899562], action=0, reward=1.0, next_state=[-0.02331455 -0.22807706 -0.0312452   0.25385653]\n",
      "[ episode 334 ][ timestamp 4 ] state=[-0.02331455 -0.22807706 -0.0312452   0.25385653], action=1, reward=1.0, next_state=[-0.02787609 -0.03252322 -0.02616807 -0.04851564]\n",
      "[ episode 334 ][ timestamp 5 ] state=[-0.02787609 -0.03252322 -0.02616807 -0.04851564], action=0, reward=1.0, next_state=[-0.02852656 -0.22726036 -0.02713838  0.23579753]\n",
      "[ episode 334 ][ timestamp 6 ] state=[-0.02852656 -0.22726036 -0.02713838  0.23579753], action=1, reward=1.0, next_state=[-0.03307176 -0.0317614  -0.02242243 -0.06532063]\n",
      "[ episode 334 ][ timestamp 7 ] state=[-0.03307176 -0.0317614  -0.02242243 -0.06532063], action=1, reward=1.0, next_state=[-0.03370699  0.16367473 -0.02372884 -0.36499278]\n",
      "[ episode 334 ][ timestamp 8 ] state=[-0.03370699  0.16367473 -0.02372884 -0.36499278], action=0, reward=1.0, next_state=[-0.0304335  -0.03110211 -0.0310287  -0.0798854 ]\n",
      "[ episode 334 ][ timestamp 9 ] state=[-0.0304335  -0.03110211 -0.0310287  -0.0798854 ], action=1, reward=1.0, next_state=[-0.03105554  0.16445059 -0.0326264  -0.38219423]\n",
      "[ episode 334 ][ timestamp 10 ] state=[-0.03105554  0.16445059 -0.0326264  -0.38219423], action=0, reward=1.0, next_state=[-0.02776653 -0.03019327 -0.04027029 -0.09997427]\n",
      "[ episode 334 ][ timestamp 11 ] state=[-0.02776653 -0.03019327 -0.04027029 -0.09997427], action=0, reward=1.0, next_state=[-0.02837039 -0.22471564 -0.04226977  0.17973645]\n",
      "[ episode 334 ][ timestamp 12 ] state=[-0.02837039 -0.22471564 -0.04226977  0.17973645], action=1, reward=1.0, next_state=[-0.03286471 -0.0290151  -0.03867505 -0.12597576]\n",
      "[ episode 334 ][ timestamp 13 ] state=[-0.03286471 -0.0290151  -0.03867505 -0.12597576], action=1, reward=1.0, next_state=[-0.03344501  0.16663895 -0.04119456 -0.43060501]\n",
      "[ episode 334 ][ timestamp 14 ] state=[-0.03344501  0.16663895 -0.04119456 -0.43060501], action=0, reward=1.0, next_state=[-0.03011223 -0.02787619 -0.04980666 -0.15118761]\n",
      "[ episode 334 ][ timestamp 15 ] state=[-0.03011223 -0.02787619 -0.04980666 -0.15118761], action=0, reward=1.0, next_state=[-0.03066975 -0.22225087 -0.05283041  0.12537575]\n",
      "[ episode 334 ][ timestamp 16 ] state=[-0.03066975 -0.22225087 -0.05283041  0.12537575], action=1, reward=1.0, next_state=[-0.03511477 -0.02641343 -0.0503229  -0.18349547]\n",
      "[ episode 334 ][ timestamp 17 ] state=[-0.03511477 -0.02641343 -0.0503229  -0.18349547], action=0, reward=1.0, next_state=[-0.03564304 -0.22078057 -0.05399281  0.09289745]\n",
      "[ episode 334 ][ timestamp 18 ] state=[-0.03564304 -0.22078057 -0.05399281  0.09289745], action=1, reward=1.0, next_state=[-0.04005865 -0.02492797 -0.05213486 -0.2163195 ]\n",
      "[ episode 334 ][ timestamp 19 ] state=[-0.04005865 -0.02492797 -0.05213486 -0.2163195 ], action=0, reward=1.0, next_state=[-0.04055721 -0.21926735 -0.05646125  0.05947279]\n",
      "[ episode 334 ][ timestamp 20 ] state=[-0.04055721 -0.21926735 -0.05646125  0.05947279], action=1, reward=1.0, next_state=[-0.04494256 -0.02338322 -0.05527179 -0.25047598]\n",
      "[ episode 334 ][ timestamp 21 ] state=[-0.04494256 -0.02338322 -0.05527179 -0.25047598], action=0, reward=1.0, next_state=[-0.04541022 -0.2176741  -0.06028131  0.02427365]\n",
      "[ episode 334 ][ timestamp 22 ] state=[-0.04541022 -0.2176741  -0.06028131  0.02427365], action=1, reward=1.0, next_state=[-0.0497637  -0.02174179 -0.05979584 -0.28680296]\n",
      "[ episode 334 ][ timestamp 23 ] state=[-0.0497637  -0.02174179 -0.05979584 -0.28680296], action=0, reward=1.0, next_state=[-0.05019854 -0.21596227 -0.0655319  -0.01356242]\n",
      "[ episode 334 ][ timestamp 24 ] state=[-0.05019854 -0.21596227 -0.0655319  -0.01356242], action=0, reward=1.0, next_state=[-0.05451778 -0.41008619 -0.06580315  0.25774584]\n",
      "[ episode 334 ][ timestamp 25 ] state=[-0.05451778 -0.41008619 -0.06580315  0.25774584], action=0, reward=1.0, next_state=[-0.06271951 -0.60420999 -0.06064823  0.52896919]\n",
      "[ episode 334 ][ timestamp 26 ] state=[-0.06271951 -0.60420999 -0.06064823  0.52896919], action=1, reward=1.0, next_state=[-0.07480371 -0.4082896  -0.05006885  0.21780926]\n",
      "[ episode 334 ][ timestamp 27 ] state=[-0.07480371 -0.4082896  -0.05006885  0.21780926], action=1, reward=1.0, next_state=[-0.0829695  -0.21248899 -0.04571266 -0.09023769]\n",
      "[ episode 334 ][ timestamp 28 ] state=[-0.0829695  -0.21248899 -0.04571266 -0.09023769], action=0, reward=1.0, next_state=[-0.08721928 -0.40692692 -0.04751741  0.18767969]\n",
      "[ episode 334 ][ timestamp 29 ] state=[-0.08721928 -0.40692692 -0.04751741  0.18767969], action=1, reward=1.0, next_state=[-0.09535782 -0.21115847 -0.04376382 -0.11960638]\n",
      "[ episode 334 ][ timestamp 30 ] state=[-0.09535782 -0.21115847 -0.04376382 -0.11960638], action=0, reward=1.0, next_state=[-0.09958099 -0.40562696 -0.04615595  0.1589546 ]\n",
      "[ episode 334 ][ timestamp 31 ] state=[-0.09958099 -0.40562696 -0.04615595  0.1589546 ], action=1, reward=1.0, next_state=[-0.10769353 -0.20987564 -0.04297686 -0.14792471]\n",
      "[ episode 334 ][ timestamp 32 ] state=[-0.10769353 -0.20987564 -0.04297686 -0.14792471], action=0, reward=1.0, next_state=[-0.11189104 -0.40435665 -0.04593535  0.13089613]\n",
      "[ episode 334 ][ timestamp 33 ] state=[-0.11189104 -0.40435665 -0.04593535  0.13089613], action=1, reward=1.0, next_state=[-0.11997817 -0.20860781 -0.04331743 -0.17591764]\n",
      "[ episode 334 ][ timestamp 34 ] state=[-0.11997817 -0.20860781 -0.04331743 -0.17591764], action=0, reward=1.0, next_state=[-0.12415033 -0.40308393 -0.04683578  0.10279155]\n",
      "[ episode 334 ][ timestamp 35 ] state=[-0.12415033 -0.40308393 -0.04683578  0.10279155], action=1, reward=1.0, next_state=[-0.13221201 -0.20732314 -0.04477995 -0.20429231]\n",
      "[ episode 334 ][ timestamp 36 ] state=[-0.13221201 -0.20732314 -0.04477995 -0.20429231], action=0, reward=1.0, next_state=[-0.13635847 -0.40177705 -0.0488658   0.07393526]\n",
      "[ episode 334 ][ timestamp 37 ] state=[-0.13635847 -0.40177705 -0.0488658   0.07393526], action=1, reward=1.0, next_state=[-0.14439401 -0.20598985 -0.04738709 -0.23375581]\n",
      "[ episode 334 ][ timestamp 38 ] state=[-0.14439401 -0.20598985 -0.04738709 -0.23375581], action=0, reward=1.0, next_state=[-0.14851381 -0.40040383 -0.05206221  0.0436112 ]\n",
      "[ episode 334 ][ timestamp 39 ] state=[-0.14851381 -0.40040383 -0.05206221  0.0436112 ], action=1, reward=1.0, next_state=[-0.15652188 -0.20457547 -0.05118998 -0.2650327 ]\n",
      "[ episode 334 ][ timestamp 40 ] state=[-0.15652188 -0.20457547 -0.05118998 -0.2650327 ], action=0, reward=1.0, next_state=[-0.16061339 -0.39893084 -0.05649064  0.01107518]\n",
      "[ episode 334 ][ timestamp 41 ] state=[-0.16061339 -0.39893084 -0.05649064  0.01107518], action=0, reward=1.0, next_state=[-0.16859201 -0.59319905 -0.05626913  0.28541324]\n",
      "[ episode 334 ][ timestamp 42 ] state=[-0.16859201 -0.59319905 -0.05626913  0.28541324], action=0, reward=1.0, next_state=[-0.18045599 -0.78747523 -0.05056087  0.55983189]\n",
      "[ episode 334 ][ timestamp 43 ] state=[-0.18045599 -0.78747523 -0.05056087  0.55983189], action=1, reward=1.0, next_state=[-0.1962055  -0.59168144 -0.03936423  0.25165797]\n",
      "[ episode 334 ][ timestamp 44 ] state=[-0.1962055  -0.59168144 -0.03936423  0.25165797], action=1, reward=1.0, next_state=[-0.20803913 -0.39602014 -0.03433107 -0.05317672]\n",
      "[ episode 334 ][ timestamp 45 ] state=[-0.20803913 -0.39602014 -0.03433107 -0.05317672], action=1, reward=1.0, next_state=[-0.21595953 -0.20042317 -0.03539461 -0.35649064]\n",
      "[ episode 334 ][ timestamp 46 ] state=[-0.21595953 -0.20042317 -0.03539461 -0.35649064], action=0, reward=1.0, next_state=[-0.21996799 -0.39502449 -0.04252442 -0.07517533]\n",
      "[ episode 334 ][ timestamp 47 ] state=[-0.21996799 -0.39502449 -0.04252442 -0.07517533], action=1, reward=1.0, next_state=[-0.22786848 -0.19931953 -0.04402793 -0.3809658 ]\n",
      "[ episode 334 ][ timestamp 48 ] state=[-0.22786848 -0.19931953 -0.04402793 -0.3809658 ], action=0, reward=1.0, next_state=[-0.23185487 -0.39378954 -0.05164724 -0.1024835 ]\n",
      "[ episode 334 ][ timestamp 49 ] state=[-0.23185487 -0.39378954 -0.05164724 -0.1024835 ], action=0, reward=1.0, next_state=[-0.23973066 -0.58813474 -0.05369691  0.17346805]\n",
      "[ episode 334 ][ timestamp 50 ] state=[-0.23973066 -0.58813474 -0.05369691  0.17346805], action=1, reward=1.0, next_state=[-0.25149336 -0.39228703 -0.05022755 -0.1356594 ]\n",
      "[ episode 334 ][ timestamp 51 ] state=[-0.25149336 -0.39228703 -0.05022755 -0.1356594 ], action=1, reward=1.0, next_state=[-0.2593391  -0.19648297 -0.05294074 -0.44375578]\n",
      "[ episode 334 ][ timestamp 52 ] state=[-0.2593391  -0.19648297 -0.05294074 -0.44375578], action=0, reward=1.0, next_state=[-0.26326876 -0.39081744 -0.06181585 -0.16821979]\n",
      "[ episode 334 ][ timestamp 53 ] state=[-0.26326876 -0.39081744 -0.06181585 -0.16821979], action=0, reward=1.0, next_state=[-0.27108511 -0.5850026  -0.06518025  0.10433933]\n",
      "[ episode 334 ][ timestamp 54 ] state=[-0.27108511 -0.5850026  -0.06518025  0.10433933], action=1, reward=1.0, next_state=[-0.28278516 -0.38901005 -0.06309346 -0.20817465]\n",
      "[ episode 334 ][ timestamp 55 ] state=[-0.28278516 -0.38901005 -0.06309346 -0.20817465], action=0, reward=1.0, next_state=[-0.29056536 -0.58317573 -0.06725696  0.06395719]\n",
      "[ episode 334 ][ timestamp 56 ] state=[-0.29056536 -0.58317573 -0.06725696  0.06395719], action=1, reward=1.0, next_state=[-0.30222887 -0.38715717 -0.06597781 -0.24916452]\n",
      "[ episode 334 ][ timestamp 57 ] state=[-0.30222887 -0.38715717 -0.06597781 -0.24916452], action=0, reward=1.0, next_state=[-0.30997202 -0.5812779  -0.0709611   0.02199963]\n",
      "[ episode 334 ][ timestamp 58 ] state=[-0.30997202 -0.5812779  -0.0709611   0.02199963], action=1, reward=1.0, next_state=[-0.32159758 -0.38521383 -0.07052111 -0.29220139]\n",
      "[ episode 334 ][ timestamp 59 ] state=[-0.32159758 -0.38521383 -0.07052111 -0.29220139], action=0, reward=1.0, next_state=[-0.32930185 -0.57926313 -0.07636514 -0.02256696]\n",
      "[ episode 334 ][ timestamp 60 ] state=[-0.32930185 -0.57926313 -0.07636514 -0.02256696], action=1, reward=1.0, next_state=[-0.34088711 -0.3831338  -0.07681648 -0.33833308]\n",
      "[ episode 334 ][ timestamp 61 ] state=[-0.34088711 -0.3831338  -0.07681648 -0.33833308], action=0, reward=1.0, next_state=[-0.34854979 -0.57708342 -0.08358314 -0.07082842]\n",
      "[ episode 334 ][ timestamp 62 ] state=[-0.34854979 -0.57708342 -0.08358314 -0.07082842], action=1, reward=1.0, next_state=[-0.36009146 -0.38086881 -0.08499971 -0.38866767]\n",
      "[ episode 334 ][ timestamp 63 ] state=[-0.36009146 -0.38086881 -0.08499971 -0.38866767], action=0, reward=1.0, next_state=[-0.36770883 -0.5746879  -0.09277306 -0.12394849]\n",
      "[ episode 334 ][ timestamp 64 ] state=[-0.36770883 -0.5746879  -0.09277306 -0.12394849], action=1, reward=1.0, next_state=[-0.37920259 -0.37836774 -0.09525203 -0.44439854]\n",
      "[ episode 334 ][ timestamp 65 ] state=[-0.37920259 -0.37836774 -0.09525203 -0.44439854], action=0, reward=1.0, next_state=[-0.38676995 -0.57202204 -0.10414    -0.18319563]\n",
      "[ episode 334 ][ timestamp 66 ] state=[-0.38676995 -0.57202204 -0.10414    -0.18319563], action=0, reward=1.0, next_state=[-0.39821039 -0.76551166 -0.10780391  0.07490456]\n",
      "[ episode 334 ][ timestamp 67 ] state=[-0.39821039 -0.76551166 -0.10780391  0.07490456], action=1, reward=1.0, next_state=[-0.41352062 -0.56902262 -0.10630582 -0.249751  ]\n",
      "[ episode 334 ][ timestamp 68 ] state=[-0.41352062 -0.56902262 -0.10630582 -0.249751  ], action=1, reward=1.0, next_state=[-0.42490107 -0.37255585 -0.11130084 -0.57398261]\n",
      "[ episode 334 ][ timestamp 69 ] state=[-0.42490107 -0.37255585 -0.11130084 -0.57398261], action=0, reward=1.0, next_state=[-0.43235219 -0.56595586 -0.1227805  -0.31833254]\n",
      "[ episode 334 ][ timestamp 70 ] state=[-0.43235219 -0.56595586 -0.1227805  -0.31833254], action=0, reward=1.0, next_state=[-0.44367131 -0.75913469 -0.12914715 -0.06675252]\n",
      "[ episode 334 ][ timestamp 71 ] state=[-0.44367131 -0.75913469 -0.12914715 -0.06675252], action=1, reward=1.0, next_state=[-0.458854   -0.56242041 -0.1304822  -0.39723042]\n",
      "[ episode 334 ][ timestamp 72 ] state=[-0.458854   -0.56242041 -0.1304822  -0.39723042], action=0, reward=1.0, next_state=[-0.47010241 -0.75547311 -0.1384268  -0.148366  ]\n",
      "[ episode 334 ][ timestamp 73 ] state=[-0.47010241 -0.75547311 -0.1384268  -0.148366  ], action=1, reward=1.0, next_state=[-0.48521187 -0.55866833 -0.14139412 -0.48131695]\n",
      "[ episode 334 ][ timestamp 74 ] state=[-0.48521187 -0.55866833 -0.14139412 -0.48131695], action=0, reward=1.0, next_state=[-0.49638524 -0.75154098 -0.15102046 -0.23632662]\n",
      "[ episode 334 ][ timestamp 75 ] state=[-0.49638524 -0.75154098 -0.15102046 -0.23632662], action=1, reward=1.0, next_state=[-0.51141606 -0.55462029 -0.155747   -0.57257709]\n",
      "[ episode 334 ][ timestamp 76 ] state=[-0.51141606 -0.55462029 -0.155747   -0.57257709], action=0, reward=1.0, next_state=[-0.52250846 -0.74725501 -0.16719854 -0.33272723]\n",
      "[ episode 334 ][ timestamp 77 ] state=[-0.52250846 -0.74725501 -0.16719854 -0.33272723], action=0, reward=1.0, next_state=[-0.53745356 -0.9396516  -0.17385308 -0.09708449]\n",
      "[ episode 334 ][ timestamp 78 ] state=[-0.53745356 -0.9396516  -0.17385308 -0.09708449], action=1, reward=1.0, next_state=[-0.5562466  -0.74251947 -0.17579477 -0.43918093]\n",
      "[ episode 334 ][ timestamp 79 ] state=[-0.5562466  -0.74251947 -0.17579477 -0.43918093], action=0, reward=1.0, next_state=[-0.57109699 -0.93477452 -0.18457839 -0.20666081]\n",
      "[ episode 334 ][ timestamp 80 ] state=[-0.57109699 -0.93477452 -0.18457839 -0.20666081], action=1, reward=1.0, next_state=[-0.58979248 -0.73755888 -0.18871161 -0.55141777]\n",
      "[ episode 334 ][ timestamp 81 ] state=[-0.58979248 -0.73755888 -0.18871161 -0.55141777], action=0, reward=1.0, next_state=[-0.60454365 -0.92959938 -0.19973996 -0.32362354]\n",
      "[ episode 334 ][ timestamp 82 ] state=[-0.60454365 -0.92959938 -0.19973996 -0.32362354], action=0, reward=1.0, next_state=[-0.62313564 -1.12140012 -0.20621243 -0.09997629]\n",
      "[ episode 334 ][ timestamp 83 ] state=[-0.62313564 -1.12140012 -0.20621243 -0.09997629], action=0, reward=1.0, next_state=[-0.64556364 -1.31306187 -0.20821196  0.12122765]\n",
      "[ episode 334 ][ timestamp 84 ] state=[-0.64556364 -1.31306187 -0.20821196  0.12122765], action=1, reward=1.0, next_state=[-0.67182488 -1.11565956 -0.20578741 -0.22925357]\n",
      "[ episode 334 ][ timestamp 85 ] state=[-0.67182488 -1.11565956 -0.20578741 -0.22925357], action=0, reward=-1.0, next_state=[-0.69413807 -1.3073377  -0.21037248 -0.00787823]\n",
      "[ Ended! ] Episode 334: Exploration_rate=0.18840216465300522. Score=85.\n",
      "[ Experience replay ] starts\n",
      "[ episode 335 ] state=[-0.00209227  0.00103428  0.01185679 -0.01661282]\n",
      "[ episode 335 ][ timestamp 1 ] state=[-0.00209227  0.00103428  0.01185679 -0.01661282], action=1, reward=1.0, next_state=[-0.00207158  0.1959842   0.01152454 -0.30553134]\n",
      "[ episode 335 ][ timestamp 2 ] state=[-0.00207158  0.1959842   0.01152454 -0.30553134], action=0, reward=1.0, next_state=[ 0.0018481   0.00069993  0.00541391 -0.00923625]\n",
      "[ episode 335 ][ timestamp 3 ] state=[ 0.0018481   0.00069993  0.00541391 -0.00923625], action=0, reward=1.0, next_state=[ 0.0018621  -0.19449924  0.00522918  0.2851499 ]\n",
      "[ episode 335 ][ timestamp 4 ] state=[ 0.0018621  -0.19449924  0.00522918  0.2851499 ], action=1, reward=1.0, next_state=[-0.00202788  0.00054774  0.01093218 -0.0058792 ]\n",
      "[ episode 335 ][ timestamp 5 ] state=[-0.00202788  0.00054774  0.01093218 -0.0058792 ], action=1, reward=1.0, next_state=[-0.00201693  0.19551122  0.0108146  -0.29509294]\n",
      "[ episode 335 ][ timestamp 6 ] state=[-0.00201693  0.19551122  0.0108146  -0.29509294], action=0, reward=1.0, next_state=[0.0018933  0.00023677 0.00491274 0.00098103]\n",
      "[ episode 335 ][ timestamp 7 ] state=[0.0018933  0.00023677 0.00491274 0.00098103], action=1, reward=1.0, next_state=[ 0.00189803  0.19528792  0.00493236 -0.29014783]\n",
      "[ episode 335 ][ timestamp 8 ] state=[ 0.00189803  0.19528792  0.00493236 -0.29014783], action=0, reward=1.0, next_state=[ 5.80379115e-03  9.59879393e-05 -8.70596744e-04  4.08662235e-03]\n",
      "[ episode 335 ][ timestamp 9 ] state=[ 5.80379115e-03  9.59879393e-05 -8.70596744e-04  4.08662235e-03], action=1, reward=1.0, next_state=[ 0.00580571  0.19523041 -0.00078886 -0.28887086]\n",
      "[ episode 335 ][ timestamp 10 ] state=[ 0.00580571  0.19523041 -0.00078886 -0.28887086], action=0, reward=1.0, next_state=[ 0.00971032  0.00011972 -0.00656628  0.00356316]\n",
      "[ episode 335 ][ timestamp 11 ] state=[ 0.00971032  0.00011972 -0.00656628  0.00356316], action=1, reward=1.0, next_state=[ 0.00971271  0.19533522 -0.00649502 -0.29118425]\n",
      "[ episode 335 ][ timestamp 12 ] state=[ 0.00971271  0.19533522 -0.00649502 -0.29118425], action=0, reward=1.0, next_state=[ 0.01361942  0.00030648 -0.0123187  -0.00055683]\n",
      "[ episode 335 ][ timestamp 13 ] state=[ 0.01361942  0.00030648 -0.0123187  -0.00055683], action=1, reward=1.0, next_state=[ 0.01362555  0.19560292 -0.01232984 -0.29710086]\n",
      "[ episode 335 ][ timestamp 14 ] state=[ 0.01362555  0.19560292 -0.01232984 -0.29710086], action=0, reward=1.0, next_state=[ 0.01753761  0.00065888 -0.01827186 -0.00833192]\n",
      "[ episode 335 ][ timestamp 15 ] state=[ 0.01753761  0.00065888 -0.01827186 -0.00833192], action=1, reward=1.0, next_state=[ 0.01755078  0.19603805 -0.0184385  -0.30672337]\n",
      "[ episode 335 ][ timestamp 16 ] state=[ 0.01755078  0.19603805 -0.0184385  -0.30672337], action=0, reward=1.0, next_state=[ 0.02147154  0.00118362 -0.02457296 -0.01991203]\n",
      "[ episode 335 ][ timestamp 17 ] state=[ 0.02147154  0.00118362 -0.02457296 -0.01991203], action=1, reward=1.0, next_state=[ 0.02149522  0.1966492  -0.0249712  -0.32024561]\n",
      "[ episode 335 ][ timestamp 18 ] state=[ 0.02149522  0.1966492  -0.0249712  -0.32024561], action=0, reward=1.0, next_state=[ 0.0254282   0.00189161 -0.03137612 -0.03554107]\n",
      "[ episode 335 ][ timestamp 19 ] state=[ 0.0254282   0.00189161 -0.03137612 -0.03554107], action=0, reward=1.0, next_state=[ 0.02546603 -0.19276668 -0.03208694  0.24707958]\n",
      "[ episode 335 ][ timestamp 20 ] state=[ 0.02546603 -0.19276668 -0.03208694  0.24707958], action=1, reward=1.0, next_state=[ 0.0216107   0.00279849 -0.02714535 -0.05554912]\n",
      "[ episode 335 ][ timestamp 21 ] state=[ 0.0216107   0.00279849 -0.02714535 -0.05554912], action=1, reward=1.0, next_state=[ 0.02166667  0.19829894 -0.02825633 -0.3566715 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 335 ][ timestamp 22 ] state=[ 0.02166667  0.19829894 -0.02825633 -0.3566715 ], action=0, reward=1.0, next_state=[ 0.02563265  0.00358987 -0.03538976 -0.07303074]\n",
      "[ episode 335 ][ timestamp 23 ] state=[ 0.02563265  0.00358987 -0.03538976 -0.07303074], action=1, reward=1.0, next_state=[ 0.02570445  0.19920084 -0.03685037 -0.37666588]\n",
      "[ episode 335 ][ timestamp 24 ] state=[ 0.02570445  0.19920084 -0.03685037 -0.37666588], action=1, reward=1.0, next_state=[ 0.02968846  0.39482627 -0.04438369 -0.68073638]\n",
      "[ episode 335 ][ timestamp 25 ] state=[ 0.02968846  0.39482627 -0.04438369 -0.68073638], action=0, reward=1.0, next_state=[ 0.03758499  0.20034797 -0.05799842 -0.40235072]\n",
      "[ episode 335 ][ timestamp 26 ] state=[ 0.03758499  0.20034797 -0.05799842 -0.40235072], action=0, reward=1.0, next_state=[ 0.04159195  0.00609454 -0.06604543 -0.12850249]\n",
      "[ episode 335 ][ timestamp 27 ] state=[ 0.04159195  0.00609454 -0.06604543 -0.12850249], action=0, reward=1.0, next_state=[ 0.04171384 -0.18802218 -0.06861548  0.14263451]\n",
      "[ episode 335 ][ timestamp 28 ] state=[ 0.04171384 -0.18802218 -0.06861548  0.14263451], action=1, reward=1.0, next_state=[ 0.03795339  0.00801195 -0.06576279 -0.17088186]\n",
      "[ episode 335 ][ timestamp 29 ] state=[ 0.03795339  0.00801195 -0.06576279 -0.17088186], action=1, reward=1.0, next_state=[ 0.03811363  0.20401051 -0.06918043 -0.48356453]\n",
      "[ episode 335 ][ timestamp 30 ] state=[ 0.03811363  0.20401051 -0.06918043 -0.48356453], action=1, reward=1.0, next_state=[ 0.04219384  0.40003714 -0.07885172 -0.79722396]\n",
      "[ episode 335 ][ timestamp 31 ] state=[ 0.04219384  0.40003714 -0.07885172 -0.79722396], action=0, reward=1.0, next_state=[ 0.05019459  0.20608057 -0.0947962  -0.53035148]\n",
      "[ episode 335 ][ timestamp 32 ] state=[ 0.05019459  0.20608057 -0.0947962  -0.53035148], action=0, reward=1.0, next_state=[ 0.0543162   0.012411   -0.10540323 -0.26897979]\n",
      "[ episode 335 ][ timestamp 33 ] state=[ 0.0543162   0.012411   -0.10540323 -0.26897979], action=1, reward=1.0, next_state=[ 0.05456442  0.20886688 -0.11078282 -0.59295937]\n",
      "[ episode 335 ][ timestamp 34 ] state=[ 0.05456442  0.20886688 -0.11078282 -0.59295937], action=0, reward=1.0, next_state=[ 0.05874176  0.0154558  -0.12264201 -0.33712478]\n",
      "[ episode 335 ][ timestamp 35 ] state=[ 0.05874176  0.0154558  -0.12264201 -0.33712478], action=1, reward=1.0, next_state=[ 0.05905087  0.21209014 -0.12938451 -0.66582732]\n",
      "[ episode 335 ][ timestamp 36 ] state=[ 0.05905087  0.21209014 -0.12938451 -0.66582732], action=0, reward=1.0, next_state=[ 0.06329267  0.01898253 -0.14270105 -0.41652004]\n",
      "[ episode 335 ][ timestamp 37 ] state=[ 0.06329267  0.01898253 -0.14270105 -0.41652004], action=1, reward=1.0, next_state=[ 0.06367233  0.21580799 -0.15103145 -0.75056914]\n",
      "[ episode 335 ][ timestamp 38 ] state=[ 0.06367233  0.21580799 -0.15103145 -0.75056914], action=0, reward=1.0, next_state=[ 0.06798848  0.02305585 -0.16604284 -0.50896687]\n",
      "[ episode 335 ][ timestamp 39 ] state=[ 0.06798848  0.02305585 -0.16604284 -0.50896687], action=0, reward=1.0, next_state=[ 0.0684496  -0.16938544 -0.17622218 -0.27286763]\n",
      "[ episode 335 ][ timestamp 40 ] state=[ 0.0684496  -0.16938544 -0.17622218 -0.27286763], action=1, reward=1.0, next_state=[ 0.06506189  0.02775572 -0.18167953 -0.61554128]\n",
      "[ episode 335 ][ timestamp 41 ] state=[ 0.06506189  0.02775572 -0.18167953 -0.61554128], action=0, reward=1.0, next_state=[ 0.06561701 -0.16442536 -0.19399035 -0.38513456]\n",
      "[ episode 335 ][ timestamp 42 ] state=[ 0.06561701 -0.16442536 -0.19399035 -0.38513456], action=1, reward=1.0, next_state=[ 0.0623285   0.03284511 -0.20169304 -0.73216602]\n",
      "[ episode 335 ][ timestamp 43 ] state=[ 0.0623285   0.03284511 -0.20169304 -0.73216602], action=1, reward=-1.0, next_state=[ 0.0629854   0.23009783 -0.21633636 -1.08094382]\n",
      "[ Ended! ] Episode 335: Exploration_rate=0.18746015382974018. Score=43.\n",
      "[ Experience replay ] starts\n",
      "[ episode 336 ] state=[-0.01439283 -0.04421451 -0.01050571  0.00963085]\n",
      "[ episode 336 ][ timestamp 1 ] state=[-0.01439283 -0.04421451 -0.01050571  0.00963085], action=1, reward=1.0, next_state=[-0.01527712  0.15105652 -0.0103131  -0.28634815]\n",
      "[ episode 336 ][ timestamp 2 ] state=[-0.01527712  0.15105652 -0.0103131  -0.28634815], action=0, reward=1.0, next_state=[-0.01225599 -0.04391684 -0.01604006  0.00306434]\n",
      "[ episode 336 ][ timestamp 3 ] state=[-0.01225599 -0.04391684 -0.01604006  0.00306434], action=1, reward=1.0, next_state=[-0.01313432  0.15143143 -0.01597877 -0.29463595]\n",
      "[ episode 336 ][ timestamp 4 ] state=[-0.01313432  0.15143143 -0.01597877 -0.29463595], action=0, reward=1.0, next_state=[-0.01010569 -0.04345911 -0.02187149 -0.00703501]\n",
      "[ episode 336 ][ timestamp 5 ] state=[-0.01010569 -0.04345911 -0.02187149 -0.00703501], action=1, reward=1.0, next_state=[-0.01097488  0.15196957 -0.02201219 -0.30653763]\n",
      "[ episode 336 ][ timestamp 6 ] state=[-0.01097488  0.15196957 -0.02201219 -0.30653763], action=0, reward=1.0, next_state=[-0.00793548 -0.04283191 -0.02814295 -0.02087726]\n",
      "[ episode 336 ][ timestamp 7 ] state=[-0.00793548 -0.04283191 -0.02814295 -0.02087726], action=1, reward=1.0, next_state=[-0.00879212  0.1526821  -0.02856049 -0.32230507]\n",
      "[ episode 336 ][ timestamp 8 ] state=[-0.00879212  0.1526821  -0.02856049 -0.32230507], action=0, reward=1.0, next_state=[-0.00573848 -0.04202175 -0.03500659 -0.03876405]\n",
      "[ episode 336 ][ timestamp 9 ] state=[-0.00573848 -0.04202175 -0.03500659 -0.03876405], action=1, reward=1.0, next_state=[-0.00657892  0.15358425 -0.03578187 -0.34228313]\n",
      "[ episode 336 ][ timestamp 10 ] state=[-0.00657892  0.15358425 -0.03578187 -0.34228313], action=0, reward=1.0, next_state=[-0.00350723 -0.04101084 -0.04262754 -0.06109496]\n",
      "[ episode 336 ][ timestamp 11 ] state=[-0.00350723 -0.04101084 -0.04262754 -0.06109496], action=1, reward=1.0, next_state=[-0.00432745  0.15469555 -0.04384944 -0.36691657]\n",
      "[ episode 336 ][ timestamp 12 ] state=[-0.00432745  0.15469555 -0.04384944 -0.36691657], action=0, reward=1.0, next_state=[-0.00123354 -0.03977676 -0.05118777 -0.08837611]\n",
      "[ episode 336 ][ timestamp 13 ] state=[-0.00123354 -0.03977676 -0.05118777 -0.08837611], action=1, reward=1.0, next_state=[-0.00202907  0.15604012 -0.05295529 -0.39675934]\n",
      "[ episode 336 ][ timestamp 14 ] state=[-0.00202907  0.15604012 -0.05295529 -0.39675934], action=1, reward=1.0, next_state=[ 0.00109173  0.35187184 -0.06089048 -0.70565671]\n",
      "[ episode 336 ][ timestamp 15 ] state=[ 0.00109173  0.35187184 -0.06089048 -0.70565671], action=0, reward=1.0, next_state=[ 0.00812917  0.15764405 -0.07500361 -0.43274571]\n",
      "[ episode 336 ][ timestamp 16 ] state=[ 0.00812917  0.15764405 -0.07500361 -0.43274571], action=0, reward=1.0, next_state=[ 0.01128205 -0.03634026 -0.08365852 -0.1646177 ]\n",
      "[ episode 336 ][ timestamp 17 ] state=[ 0.01128205 -0.03634026 -0.08365852 -0.1646177 ], action=1, reward=1.0, next_state=[ 0.01055524  0.15987343 -0.08695088 -0.48247582]\n",
      "[ episode 336 ][ timestamp 18 ] state=[ 0.01055524  0.15987343 -0.08695088 -0.48247582], action=0, reward=1.0, next_state=[ 0.01375271 -0.03392058 -0.09660039 -0.21841435]\n",
      "[ episode 336 ][ timestamp 19 ] state=[ 0.01375271 -0.03392058 -0.09660039 -0.21841435], action=1, reward=1.0, next_state=[ 0.0130743   0.16244001 -0.10096868 -0.5399384 ]\n",
      "[ episode 336 ][ timestamp 20 ] state=[ 0.0130743   0.16244001 -0.10096868 -0.5399384 ], action=0, reward=1.0, next_state=[ 0.0163231  -0.03112849 -0.11176745 -0.2806988 ]\n",
      "[ episode 336 ][ timestamp 21 ] state=[ 0.0163231  -0.03112849 -0.11176745 -0.2806988 ], action=1, reward=1.0, next_state=[ 0.01570053  0.1653956  -0.11738143 -0.60643688]\n",
      "[ episode 336 ][ timestamp 22 ] state=[ 0.01570053  0.1653956  -0.11738143 -0.60643688], action=0, reward=1.0, next_state=[ 0.01900844 -0.02790638 -0.12951016 -0.3529101 ]\n",
      "[ episode 336 ][ timestamp 23 ] state=[ 0.01900844 -0.02790638 -0.12951016 -0.3529101 ], action=0, reward=1.0, next_state=[ 0.01845032 -0.22097174 -0.13656837 -0.10370701]\n",
      "[ episode 336 ][ timestamp 24 ] state=[ 0.01845032 -0.22097174 -0.13656837 -0.10370701], action=1, reward=1.0, next_state=[ 0.01403088 -0.02418378 -0.13864251 -0.43616692]\n",
      "[ episode 336 ][ timestamp 25 ] state=[ 0.01403088 -0.02418378 -0.13864251 -0.43616692], action=1, reward=1.0, next_state=[ 0.0135472   0.17260048 -0.14736584 -0.76914138]\n",
      "[ episode 336 ][ timestamp 26 ] state=[ 0.0135472   0.17260048 -0.14736584 -0.76914138], action=1, reward=1.0, next_state=[ 0.01699921  0.36941018 -0.16274867 -1.10432511]\n",
      "[ episode 336 ][ timestamp 27 ] state=[ 0.01699921  0.36941018 -0.16274867 -1.10432511], action=0, reward=1.0, next_state=[ 0.02438742  0.17675859 -0.18483517 -0.86680354]\n",
      "[ episode 336 ][ timestamp 28 ] state=[ 0.02438742  0.17675859 -0.18483517 -0.86680354], action=1, reward=1.0, next_state=[ 0.02792259  0.37384957 -0.20217124 -1.21143694]\n",
      "[ episode 336 ][ timestamp 29 ] state=[ 0.02792259  0.37384957 -0.20217124 -1.21143694], action=0, reward=-1.0, next_state=[ 0.03539958  0.18182769 -0.22639998 -0.98830478]\n",
      "[ Ended! ] Episode 336: Exploration_rate=0.1865228530605915. Score=29.\n",
      "[ Experience replay ] starts\n",
      "[ episode 337 ] state=[ 0.03758918  0.03633719 -0.02843884  0.0092214 ]\n",
      "[ episode 337 ][ timestamp 1 ] state=[ 0.03758918  0.03633719 -0.02843884  0.0092214 ], action=0, reward=1.0, next_state=[ 0.03831593 -0.15836561 -0.02825441  0.29279761]\n",
      "[ episode 337 ][ timestamp 2 ] state=[ 0.03831593 -0.15836561 -0.02825441  0.29279761], action=1, reward=1.0, next_state=[ 0.03514862  0.03714756 -0.02239846 -0.00866077]\n",
      "[ episode 337 ][ timestamp 3 ] state=[ 0.03514862  0.03714756 -0.02239846 -0.00866077], action=0, reward=1.0, next_state=[ 0.03589157 -0.15764613 -0.02257168  0.27687186]\n",
      "[ episode 337 ][ timestamp 4 ] state=[ 0.03589157 -0.15764613 -0.02257168  0.27687186], action=1, reward=1.0, next_state=[ 0.03273864  0.03779045 -0.01703424 -0.02284384]\n",
      "[ episode 337 ][ timestamp 5 ] state=[ 0.03273864  0.03779045 -0.01703424 -0.02284384], action=1, reward=1.0, next_state=[ 0.03349445  0.2331525  -0.01749112 -0.32085222]\n",
      "[ episode 337 ][ timestamp 6 ] state=[ 0.03349445  0.2331525  -0.01749112 -0.32085222], action=1, reward=1.0, next_state=[ 0.0381575   0.42851912 -0.02390816 -0.61899944]\n",
      "[ episode 337 ][ timestamp 7 ] state=[ 0.0381575   0.42851912 -0.02390816 -0.61899944], action=1, reward=1.0, next_state=[ 0.04672789  0.62396671 -0.03628815 -0.91911537]\n",
      "[ episode 337 ][ timestamp 8 ] state=[ 0.04672789  0.62396671 -0.03628815 -0.91911537], action=0, reward=1.0, next_state=[ 0.05920722  0.42935356 -0.05467046 -0.63805422]\n",
      "[ episode 337 ][ timestamp 9 ] state=[ 0.05920722  0.42935356 -0.05467046 -0.63805422], action=0, reward=1.0, next_state=[ 0.06779429  0.23503486 -0.06743154 -0.36307675]\n",
      "[ episode 337 ][ timestamp 10 ] state=[ 0.06779429  0.23503486 -0.06743154 -0.36307675], action=0, reward=1.0, next_state=[ 0.07249499  0.04093286 -0.07469308 -0.0923953 ]\n",
      "[ episode 337 ][ timestamp 11 ] state=[ 0.07249499  0.04093286 -0.07469308 -0.0923953 ], action=0, reward=1.0, next_state=[ 0.07331365 -0.15304345 -0.07654098  0.17581853]\n",
      "[ episode 337 ][ timestamp 12 ] state=[ 0.07331365 -0.15304345 -0.07654098  0.17581853], action=0, reward=1.0, next_state=[ 0.07025278 -0.3469913  -0.07302461  0.44340746]\n",
      "[ episode 337 ][ timestamp 13 ] state=[ 0.07025278 -0.3469913  -0.07302461  0.44340746], action=1, reward=1.0, next_state=[ 0.06331295 -0.15091615 -0.06415646  0.12862842]\n",
      "[ episode 337 ][ timestamp 14 ] state=[ 0.06331295 -0.15091615 -0.06415646  0.12862842], action=0, reward=1.0, next_state=[ 0.06029463 -0.34506317 -0.06158389  0.40040075]\n",
      "[ episode 337 ][ timestamp 15 ] state=[ 0.06029463 -0.34506317 -0.06158389  0.40040075], action=1, reward=1.0, next_state=[ 0.05339336 -0.14912418 -0.05357588  0.0889552 ]\n",
      "[ episode 337 ][ timestamp 16 ] state=[ 0.05339336 -0.14912418 -0.05357588  0.0889552 ], action=0, reward=1.0, next_state=[ 0.05041088 -0.34343888 -0.05179678  0.36426527]\n",
      "[ episode 337 ][ timestamp 17 ] state=[ 0.05041088 -0.34343888 -0.05179678  0.36426527], action=1, reward=1.0, next_state=[ 0.0435421  -0.14762053 -0.04451147  0.05571023]\n",
      "[ episode 337 ][ timestamp 18 ] state=[ 0.0435421  -0.14762053 -0.04451147  0.05571023], action=0, reward=1.0, next_state=[ 0.04058969 -0.34207693 -0.04339727  0.33402386]\n",
      "[ episode 337 ][ timestamp 19 ] state=[ 0.04058969 -0.34207693 -0.04339727  0.33402386], action=1, reward=1.0, next_state=[ 0.03374815 -0.14636505 -0.03671679  0.02797766]\n",
      "[ episode 337 ][ timestamp 20 ] state=[ 0.03374815 -0.14636505 -0.03671679  0.02797766], action=0, reward=1.0, next_state=[ 0.03082085 -0.34094175 -0.03615724  0.30885368]\n",
      "[ episode 337 ][ timestamp 21 ] state=[ 0.03082085 -0.34094175 -0.03615724  0.30885368], action=1, reward=1.0, next_state=[ 0.02400202 -0.14532377 -0.02998016  0.00499058]\n",
      "[ episode 337 ][ timestamp 22 ] state=[ 0.02400202 -0.14532377 -0.02998016  0.00499058], action=0, reward=1.0, next_state=[ 0.02109554 -0.34000322 -0.02988035  0.28806569]\n",
      "[ episode 337 ][ timestamp 23 ] state=[ 0.02109554 -0.34000322 -0.02988035  0.28806569], action=1, reward=1.0, next_state=[ 0.01429548 -0.14446818 -0.02411904 -0.01388946]\n",
      "[ episode 337 ][ timestamp 24 ] state=[ 0.01429548 -0.14446818 -0.02411904 -0.01388946], action=0, reward=1.0, next_state=[ 0.01140611 -0.33923608 -0.02439683  0.27108711]\n",
      "[ episode 337 ][ timestamp 25 ] state=[ 0.01140611 -0.33923608 -0.02439683  0.27108711], action=1, reward=1.0, next_state=[ 0.00462139 -0.14377464 -0.01897508 -0.02918975]\n",
      "[ episode 337 ][ timestamp 26 ] state=[ 0.00462139 -0.14377464 -0.01897508 -0.02918975], action=1, reward=1.0, next_state=[ 0.0017459   0.05161421 -0.01955888 -0.32779861]\n",
      "[ episode 337 ][ timestamp 27 ] state=[ 0.0017459   0.05161421 -0.01955888 -0.32779861], action=0, reward=1.0, next_state=[ 0.00277818 -0.14322391 -0.02611485 -0.04134728]\n",
      "[ episode 337 ][ timestamp 28 ] state=[ 0.00277818 -0.14322391 -0.02611485 -0.04134728], action=0, reward=1.0, next_state=[-8.62941494e-05 -3.37961830e-01 -2.69417964e-02  2.42983113e-01]\n",
      "[ episode 337 ][ timestamp 29 ] state=[-8.62941494e-05 -3.37961830e-01 -2.69417964e-02  2.42983113e-01], action=1, reward=1.0, next_state=[-0.00684553 -0.14246561 -0.02208213 -0.05807472]\n",
      "[ episode 337 ][ timestamp 30 ] state=[-0.00684553 -0.14246561 -0.02208213 -0.05807472], action=0, reward=1.0, next_state=[-0.00969484 -0.3372641  -0.02324363  0.22756015]\n",
      "[ episode 337 ][ timestamp 31 ] state=[-0.00969484 -0.3372641  -0.02324363  0.22756015], action=1, reward=1.0, next_state=[-0.01644013 -0.14181782 -0.01869243 -0.07236309]\n",
      "[ episode 337 ][ timestamp 32 ] state=[-0.01644013 -0.14181782 -0.01869243 -0.07236309], action=0, reward=1.0, next_state=[-0.01927648 -0.33666687 -0.02013969  0.21436418]\n",
      "[ episode 337 ][ timestamp 33 ] state=[-0.01927648 -0.33666687 -0.02013969  0.21436418], action=1, reward=1.0, next_state=[-0.02600982 -0.14126287 -0.0158524  -0.08460306]\n",
      "[ episode 337 ][ timestamp 34 ] state=[-0.02600982 -0.14126287 -0.0158524  -0.08460306], action=0, reward=1.0, next_state=[-0.02883508 -0.33615404 -0.01754446  0.20303656]\n",
      "[ episode 337 ][ timestamp 35 ] state=[-0.02883508 -0.33615404 -0.01754446  0.20303656], action=1, reward=1.0, next_state=[-0.03555816 -0.14078563 -0.01348373 -0.09512876]\n",
      "[ episode 337 ][ timestamp 36 ] state=[-0.03555816 -0.14078563 -0.01348373 -0.09512876], action=0, reward=1.0, next_state=[-0.03837387 -0.33571176 -0.01538631  0.19326975]\n",
      "[ episode 337 ][ timestamp 37 ] state=[-0.03837387 -0.33571176 -0.01538631  0.19326975], action=1, reward=1.0, next_state=[-0.0450881  -0.14037312 -0.01152091 -0.10422692]\n",
      "[ episode 337 ][ timestamp 38 ] state=[-0.0450881  -0.14037312 -0.01152091 -0.10422692], action=0, reward=1.0, next_state=[-0.04789557 -0.33532809 -0.01360545  0.18479905]\n",
      "[ episode 337 ][ timestamp 39 ] state=[-0.04789557 -0.33532809 -0.01360545  0.18479905], action=1, reward=1.0, next_state=[-0.05460213 -0.14001414 -0.00990947 -0.11214464]\n",
      "[ episode 337 ][ timestamp 40 ] state=[-0.05460213 -0.14001414 -0.00990947 -0.11214464], action=0, reward=1.0, next_state=[-0.05740241 -0.3349927  -0.01215236  0.17739551]\n",
      "[ episode 337 ][ timestamp 41 ] state=[-0.05740241 -0.3349927  -0.01215236  0.17739551], action=1, reward=1.0, next_state=[-0.06410227 -0.13969897 -0.00860445 -0.11909617]\n",
      "[ episode 337 ][ timestamp 42 ] state=[-0.06410227 -0.13969897 -0.00860445 -0.11909617], action=0, reward=1.0, next_state=[-0.06689625 -0.33469659 -0.01098638  0.17085976]\n",
      "[ episode 337 ][ timestamp 43 ] state=[-0.06689625 -0.33469659 -0.01098638  0.17085976], action=1, reward=1.0, next_state=[-0.07359018 -0.13941913 -0.00756918 -0.12526869]\n",
      "[ episode 337 ][ timestamp 44 ] state=[-0.07359018 -0.13941913 -0.00756918 -0.12526869], action=0, reward=1.0, next_state=[-0.07637856 -0.33443183 -0.01007456  0.16501667]\n",
      "[ episode 337 ][ timestamp 45 ] state=[-0.07637856 -0.33443183 -0.01007456  0.16501667], action=1, reward=1.0, next_state=[-0.0830672  -0.13916712 -0.00677422 -0.1308274 ]\n",
      "[ episode 337 ][ timestamp 46 ] state=[-0.0830672  -0.13916712 -0.00677422 -0.1308274 ], action=0, reward=1.0, next_state=[-0.08585054 -0.33419138 -0.00939077  0.15971067]\n",
      "[ episode 337 ][ timestamp 47 ] state=[-0.08585054 -0.33419138 -0.00939077  0.15971067], action=1, reward=1.0, next_state=[-0.09253437 -0.13893625 -0.00619656 -0.13591995]\n",
      "[ episode 337 ][ timestamp 48 ] state=[-0.09253437 -0.13893625 -0.00619656 -0.13591995], action=0, reward=1.0, next_state=[-0.09531309 -0.3339689  -0.00891496  0.15480163]\n",
      "[ episode 337 ][ timestamp 49 ] state=[-0.09531309 -0.3339689  -0.00891496  0.15480163], action=1, reward=1.0, next_state=[-0.10199247 -0.13872045 -0.00581892 -0.14068038]\n",
      "[ episode 337 ][ timestamp 50 ] state=[-0.10199247 -0.13872045 -0.00581892 -0.14068038], action=1, reward=1.0, next_state=[-0.10476688  0.05648436 -0.00863253 -0.43519338]\n",
      "[ episode 337 ][ timestamp 51 ] state=[-0.10476688  0.05648436 -0.00863253 -0.43519338], action=0, reward=1.0, next_state=[-0.10363719 -0.13851433 -0.0173364  -0.14524418]\n",
      "[ episode 337 ][ timestamp 52 ] state=[-0.10363719 -0.13851433 -0.0173364  -0.14524418], action=0, reward=1.0, next_state=[-0.10640748 -0.33338377 -0.02024128  0.14191941]\n",
      "[ episode 337 ][ timestamp 53 ] state=[-0.10640748 -0.33338377 -0.02024128  0.14191941], action=1, reward=1.0, next_state=[-0.11307515 -0.13797787 -0.01740289 -0.15707994]\n",
      "[ episode 337 ][ timestamp 54 ] state=[-0.11307515 -0.13797787 -0.01740289 -0.15707994], action=0, reward=1.0, next_state=[-0.11583471 -0.33284639 -0.02054449  0.13006239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 337 ][ timestamp 55 ] state=[-0.11583471 -0.33284639 -0.02054449  0.13006239], action=0, reward=1.0, next_state=[-0.12249164 -0.5276681  -0.01794325  0.41619364]\n",
      "[ episode 337 ][ timestamp 56 ] state=[-0.12249164 -0.5276681  -0.01794325  0.41619364], action=0, reward=1.0, next_state=[-0.133045   -0.72253122 -0.00961937  0.70316623]\n",
      "[ episode 337 ][ timestamp 57 ] state=[-0.133045   -0.72253122 -0.00961937  0.70316623], action=1, reward=1.0, next_state=[-0.14749562 -0.52727728  0.00444395  0.40747082]\n",
      "[ episode 337 ][ timestamp 58 ] state=[-0.14749562 -0.52727728  0.00444395  0.40747082], action=1, reward=1.0, next_state=[-0.15804117 -0.33221863  0.01259337  0.11619225]\n",
      "[ episode 337 ][ timestamp 59 ] state=[-0.15804117 -0.33221863  0.01259337  0.11619225], action=1, reward=1.0, next_state=[-0.16468554 -0.13727936  0.01491721 -0.17249112]\n",
      "[ episode 337 ][ timestamp 60 ] state=[-0.16468554 -0.13727936  0.01491721 -0.17249112], action=0, reward=1.0, next_state=[-0.16743113 -0.3326116   0.01146739  0.12486015]\n",
      "[ episode 337 ][ timestamp 61 ] state=[-0.16743113 -0.3326116   0.01146739  0.12486015], action=1, reward=1.0, next_state=[-0.17408336 -0.1376558   0.01396459 -0.164183  ]\n",
      "[ episode 337 ][ timestamp 62 ] state=[-0.17408336 -0.1376558   0.01396459 -0.164183  ], action=1, reward=1.0, next_state=[-0.17683648  0.05726349  0.01068093 -0.45242797]\n",
      "[ episode 337 ][ timestamp 63 ] state=[-0.17683648  0.05726349  0.01068093 -0.45242797], action=0, reward=1.0, next_state=[-0.17569121 -0.13800786  0.00163237 -0.1563975 ]\n",
      "[ episode 337 ][ timestamp 64 ] state=[-0.17569121 -0.13800786  0.00163237 -0.1563975 ], action=1, reward=1.0, next_state=[-0.17845137  0.05709068 -0.00149558 -0.44856501]\n",
      "[ episode 337 ][ timestamp 65 ] state=[-0.17845137  0.05709068 -0.00149558 -0.44856501], action=0, reward=1.0, next_state=[-0.17730955 -0.13801009 -0.01046688 -0.15635389]\n",
      "[ episode 337 ][ timestamp 66 ] state=[-0.17730955 -0.13801009 -0.01046688 -0.15635389], action=1, reward=1.0, next_state=[-0.18006975  0.05726015 -0.01359395 -0.4523204 ]\n",
      "[ episode 337 ][ timestamp 67 ] state=[-0.18006975  0.05726015 -0.01359395 -0.4523204 ], action=0, reward=1.0, next_state=[-0.17892455 -0.13766695 -0.02264036 -0.16395327]\n",
      "[ episode 337 ][ timestamp 68 ] state=[-0.17892455 -0.13766695 -0.02264036 -0.16395327], action=1, reward=1.0, next_state=[-0.18167789  0.05777167 -0.02591943 -0.46369176]\n",
      "[ episode 337 ][ timestamp 69 ] state=[-0.18167789  0.05777167 -0.02591943 -0.46369176], action=0, reward=1.0, next_state=[-0.18052246 -0.13697459 -0.03519326 -0.17928995]\n",
      "[ episode 337 ][ timestamp 70 ] state=[-0.18052246 -0.13697459 -0.03519326 -0.17928995], action=1, reward=1.0, next_state=[-0.18326195  0.05863284 -0.03877906 -0.4828641 ]\n",
      "[ episode 337 ][ timestamp 71 ] state=[-0.18326195  0.05863284 -0.03877906 -0.4828641 ], action=1, reward=1.0, next_state=[-0.18208929  0.25428005 -0.04843634 -0.78751246]\n",
      "[ episode 337 ][ timestamp 72 ] state=[-0.18208929  0.25428005 -0.04843634 -0.78751246], action=1, reward=1.0, next_state=[-0.17700369  0.45003269 -0.06418659 -1.09503176]\n",
      "[ episode 337 ][ timestamp 73 ] state=[-0.17700369  0.45003269 -0.06418659 -1.09503176], action=0, reward=1.0, next_state=[-0.16800304  0.25581217 -0.08608723 -0.82315882]\n",
      "[ episode 337 ][ timestamp 74 ] state=[-0.16800304  0.25581217 -0.08608723 -0.82315882], action=0, reward=1.0, next_state=[-0.16288679  0.06196677 -0.1025504  -0.55874588]\n",
      "[ episode 337 ][ timestamp 75 ] state=[-0.16288679  0.06196677 -0.1025504  -0.55874588], action=0, reward=1.0, next_state=[-0.16164746 -0.13157752 -0.11372532 -0.30005168]\n",
      "[ episode 337 ][ timestamp 76 ] state=[-0.16164746 -0.13157752 -0.11372532 -0.30005168], action=0, reward=1.0, next_state=[-0.16427901 -0.32491031 -0.11972635 -0.04528905]\n",
      "[ episode 337 ][ timestamp 77 ] state=[-0.16427901 -0.32491031 -0.11972635 -0.04528905], action=1, reward=1.0, next_state=[-0.17077721 -0.12829314 -0.12063214 -0.37321904]\n",
      "[ episode 337 ][ timestamp 78 ] state=[-0.17077721 -0.12829314 -0.12063214 -0.37321904], action=1, reward=1.0, next_state=[-0.17334308  0.06831743 -0.12809652 -0.70137156]\n",
      "[ episode 337 ][ timestamp 79 ] state=[-0.17334308  0.06831743 -0.12809652 -0.70137156], action=0, reward=1.0, next_state=[-0.17197673 -0.12481815 -0.14212395 -0.45159924]\n",
      "[ episode 337 ][ timestamp 80 ] state=[-0.17197673 -0.12481815 -0.14212395 -0.45159924], action=1, reward=1.0, next_state=[-0.17447309  0.0719976  -0.15115593 -0.78549016]\n",
      "[ episode 337 ][ timestamp 81 ] state=[-0.17447309  0.0719976  -0.15115593 -0.78549016], action=0, reward=1.0, next_state=[-0.17303314 -0.12076025 -0.16686574 -0.54392102]\n",
      "[ episode 337 ][ timestamp 82 ] state=[-0.17303314 -0.12076025 -0.16686574 -0.54392102], action=0, reward=1.0, next_state=[-0.17544834 -0.31319279 -0.17774416 -0.30811267]\n",
      "[ episode 337 ][ timestamp 83 ] state=[-0.17544834 -0.31319279 -0.17774416 -0.30811267], action=1, reward=1.0, next_state=[-0.1817122  -0.11604242 -0.18390641 -0.65116114]\n",
      "[ episode 337 ][ timestamp 84 ] state=[-0.1817122  -0.11604242 -0.18390641 -0.65116114], action=0, reward=1.0, next_state=[-0.18403305 -0.30819142 -0.19692963 -0.42156223]\n",
      "[ episode 337 ][ timestamp 85 ] state=[-0.18403305 -0.30819142 -0.19692963 -0.42156223], action=0, reward=1.0, next_state=[-0.19019688 -0.50005804 -0.20536088 -0.19684872]\n",
      "[ episode 337 ][ timestamp 86 ] state=[-0.19019688 -0.50005804 -0.20536088 -0.19684872], action=1, reward=1.0, next_state=[-0.20019804 -0.30268129 -0.20929785 -0.54664538]\n",
      "[ episode 337 ][ timestamp 87 ] state=[-0.20019804 -0.30268129 -0.20929785 -0.54664538], action=0, reward=-1.0, next_state=[-0.20625166 -0.49434365 -0.22023076 -0.32651109]\n",
      "[ Ended! ] Episode 337: Exploration_rate=0.18559023879528855. Score=87.\n",
      "[ Experience replay ] starts\n",
      "[ episode 338 ] state=[ 0.04251037  0.01220402  0.02641322 -0.03234645]\n",
      "[ episode 338 ][ timestamp 1 ] state=[ 0.04251037  0.01220402  0.02641322 -0.03234645], action=0, reward=1.0, next_state=[ 0.04275445 -0.18328656  0.02576629  0.26855171]\n",
      "[ episode 338 ][ timestamp 2 ] state=[ 0.04275445 -0.18328656  0.02576629  0.26855171], action=1, reward=1.0, next_state=[ 0.03908872  0.01145838  0.03113732 -0.01589429]\n",
      "[ episode 338 ][ timestamp 3 ] state=[ 0.03908872  0.01145838  0.03113732 -0.01589429], action=0, reward=1.0, next_state=[ 0.03931789 -0.18409596  0.03081944  0.28644793]\n",
      "[ episode 338 ][ timestamp 4 ] state=[ 0.03931789 -0.18409596  0.03081944  0.28644793], action=1, reward=1.0, next_state=[0.03563597 0.01057322 0.03654839 0.00364231]\n",
      "[ episode 338 ][ timestamp 5 ] state=[0.03563597 0.01057322 0.03654839 0.00364231], action=0, reward=1.0, next_state=[ 0.03584744 -0.18505331  0.03662124  0.30762898]\n",
      "[ episode 338 ][ timestamp 6 ] state=[ 0.03584744 -0.18505331  0.03662124  0.30762898], action=1, reward=1.0, next_state=[0.03214637 0.0095282  0.04277382 0.02671664]\n",
      "[ episode 338 ][ timestamp 7 ] state=[0.03214637 0.0095282  0.04277382 0.02671664], action=0, reward=1.0, next_state=[ 0.03233693 -0.18618023  0.04330815  0.33258244]\n",
      "[ episode 338 ][ timestamp 8 ] state=[ 0.03233693 -0.18618023  0.04330815  0.33258244], action=1, reward=1.0, next_state=[0.02861333 0.0082994  0.0499598  0.05386516]\n",
      "[ episode 338 ][ timestamp 9 ] state=[0.02861333 0.0082994  0.0499598  0.05386516], action=0, reward=1.0, next_state=[ 0.02877932 -0.18750199  0.05103711  0.36188284]\n",
      "[ episode 338 ][ timestamp 10 ] state=[ 0.02877932 -0.18750199  0.05103711  0.36188284], action=1, reward=1.0, next_state=[0.02502928 0.00685879 0.05827476 0.08571969]\n",
      "[ episode 338 ][ timestamp 11 ] state=[0.02502928 0.00685879 0.05827476 0.08571969], action=0, reward=1.0, next_state=[ 0.02516645 -0.18904797  0.05998916  0.39620409]\n",
      "[ episode 338 ][ timestamp 12 ] state=[ 0.02516645 -0.18904797  0.05998916  0.39620409], action=1, reward=1.0, next_state=[0.02138549 0.00517381 0.06791324 0.12302171]\n",
      "[ episode 338 ][ timestamp 13 ] state=[0.02138549 0.00517381 0.06791324 0.12302171], action=1, reward=1.0, next_state=[ 0.02148897  0.19926038  0.07037367 -0.14748589]\n",
      "[ episode 338 ][ timestamp 14 ] state=[ 0.02148897  0.19926038  0.07037367 -0.14748589], action=0, reward=1.0, next_state=[0.02547418 0.00320492 0.06742395 0.16654218]\n",
      "[ episode 338 ][ timestamp 15 ] state=[0.02547418 0.00320492 0.06742395 0.16654218], action=1, reward=1.0, next_state=[ 0.02553828  0.1973002   0.0707548  -0.1041316 ]\n",
      "[ episode 338 ][ timestamp 16 ] state=[ 0.02553828  0.1973002   0.0707548  -0.1041316 ], action=0, reward=1.0, next_state=[0.02948428 0.00123935 0.06867217 0.21000839]\n",
      "[ episode 338 ][ timestamp 17 ] state=[0.02948428 0.00123935 0.06867217 0.21000839], action=1, reward=1.0, next_state=[ 0.02950907  0.19531563  0.07287233 -0.06024611]\n",
      "[ episode 338 ][ timestamp 18 ] state=[ 0.02950907  0.19531563  0.07287233 -0.06024611], action=0, reward=1.0, next_state=[ 0.03341538 -0.00077141  0.07166741  0.25450933]\n",
      "[ episode 338 ][ timestamp 19 ] state=[ 0.03341538 -0.00077141  0.07166741  0.25450933], action=1, reward=1.0, next_state=[ 0.03339995  0.19325797  0.0767576  -0.01473544]\n",
      "[ episode 338 ][ timestamp 20 ] state=[ 0.03339995  0.19325797  0.0767576  -0.01473544], action=0, reward=1.0, next_state=[ 0.03726511 -0.00287607  0.07646289  0.30114395]\n",
      "[ episode 338 ][ timestamp 21 ] state=[ 0.03726511 -0.00287607  0.07646289  0.30114395], action=1, reward=1.0, next_state=[0.03720759 0.1910775  0.08248577 0.03352184]\n",
      "[ episode 338 ][ timestamp 22 ] state=[0.03720759 0.1910775  0.08248577 0.03352184], action=0, reward=1.0, next_state=[ 0.04102914 -0.00512451  0.0831562   0.35104754]\n",
      "[ episode 338 ][ timestamp 23 ] state=[ 0.04102914 -0.00512451  0.0831562   0.35104754], action=1, reward=1.0, next_state=[0.04092665 0.18872248 0.09017716 0.08570157]\n",
      "[ episode 338 ][ timestamp 24 ] state=[0.04092665 0.18872248 0.09017716 0.08570157], action=0, reward=1.0, next_state=[ 0.0447011  -0.00756861  0.09189119  0.40541802]\n",
      "[ episode 338 ][ timestamp 25 ] state=[ 0.0447011  -0.00756861  0.09189119  0.40541802], action=1, reward=1.0, next_state=[0.04454973 0.18613826 0.09999955 0.1430616 ]\n",
      "[ episode 338 ][ timestamp 26 ] state=[0.04454973 0.18613826 0.09999955 0.1430616 ], action=0, reward=1.0, next_state=[ 0.04827249 -0.01026307  0.10286078  0.46554272]\n",
      "[ episode 338 ][ timestamp 27 ] state=[ 0.04827249 -0.01026307  0.10286078  0.46554272], action=1, reward=1.0, next_state=[0.04806723 0.1832665  0.11217163 0.20697048]\n",
      "[ episode 338 ][ timestamp 28 ] state=[0.04806723 0.1832665  0.11217163 0.20697048], action=1, reward=1.0, next_state=[ 0.05173256  0.37662062  0.11631104 -0.0483286 ]\n",
      "[ episode 338 ][ timestamp 29 ] state=[ 0.05173256  0.37662062  0.11631104 -0.0483286 ], action=0, reward=1.0, next_state=[0.05926497 0.18003958 0.11534447 0.27866905]\n",
      "[ episode 338 ][ timestamp 30 ] state=[0.05926497 0.18003958 0.11534447 0.27866905], action=1, reward=1.0, next_state=[0.06286577 0.37334333 0.12091785 0.02447625]\n",
      "[ episode 338 ][ timestamp 31 ] state=[0.06286577 0.37334333 0.12091785 0.02447625], action=0, reward=1.0, next_state=[0.07033263 0.1767135  0.12140738 0.35273069]\n",
      "[ episode 338 ][ timestamp 32 ] state=[0.07033263 0.1767135  0.12140738 0.35273069], action=1, reward=1.0, next_state=[0.0738669  0.3699187  0.12846199 0.10066227]\n",
      "[ episode 338 ][ timestamp 33 ] state=[0.0738669  0.3699187  0.12846199 0.10066227], action=1, reward=1.0, next_state=[ 0.08126528  0.56298793  0.13047524 -0.14889125]\n",
      "[ episode 338 ][ timestamp 34 ] state=[ 0.08126528  0.56298793  0.13047524 -0.14889125], action=0, reward=1.0, next_state=[0.09252503 0.36626242 0.12749741 0.1819398 ]\n",
      "[ episode 338 ][ timestamp 35 ] state=[0.09252503 0.36626242 0.12749741 0.1819398 ], action=1, reward=1.0, next_state=[ 0.09985028  0.5593513   0.13113621 -0.06795986]\n",
      "[ episode 338 ][ timestamp 36 ] state=[ 0.09985028  0.5593513   0.13113621 -0.06795986], action=0, reward=1.0, next_state=[0.11103731 0.36261688 0.12977701 0.26305166]\n",
      "[ episode 338 ][ timestamp 37 ] state=[0.11103731 0.36261688 0.12977701 0.26305166], action=1, reward=1.0, next_state=[0.11828965 0.55567062 0.13503804 0.01395362]\n",
      "[ episode 338 ][ timestamp 38 ] state=[0.11828965 0.55567062 0.13503804 0.01395362], action=0, reward=1.0, next_state=[0.12940306 0.35889647 0.13531712 0.3460084 ]\n",
      "[ episode 338 ][ timestamp 39 ] state=[0.12940306 0.35889647 0.13531712 0.3460084 ], action=1, reward=1.0, next_state=[0.13658099 0.55186018 0.14223728 0.09887071]\n",
      "[ episode 338 ][ timestamp 40 ] state=[0.13658099 0.55186018 0.14223728 0.09887071], action=1, reward=1.0, next_state=[ 0.14761819  0.74468746  0.1442147  -0.14577237]\n",
      "[ episode 338 ][ timestamp 41 ] state=[ 0.14761819  0.74468746  0.1442147  -0.14577237], action=0, reward=1.0, next_state=[0.16251194 0.54782631 0.14129925 0.18870627]\n",
      "[ episode 338 ][ timestamp 42 ] state=[0.16251194 0.54782631 0.14129925 0.18870627], action=1, reward=1.0, next_state=[ 0.17346847  0.74067372  0.14507338 -0.05627805]\n",
      "[ episode 338 ][ timestamp 43 ] state=[ 0.17346847  0.74067372  0.14507338 -0.05627805], action=0, reward=1.0, next_state=[0.18828194 0.54380178 0.14394782 0.27842986]\n",
      "[ episode 338 ][ timestamp 44 ] state=[0.18828194 0.54380178 0.14394782 0.27842986], action=1, reward=1.0, next_state=[0.19915798 0.73660824 0.14951641 0.034386  ]\n",
      "[ episode 338 ][ timestamp 45 ] state=[0.19915798 0.73660824 0.14951641 0.034386  ], action=1, reward=1.0, next_state=[ 0.21389014  0.92930504  0.15020413 -0.20764015]\n",
      "[ episode 338 ][ timestamp 46 ] state=[ 0.21389014  0.92930504  0.15020413 -0.20764015], action=0, reward=1.0, next_state=[0.23247624 0.73239012 0.14605133 0.12840065]\n",
      "[ episode 338 ][ timestamp 47 ] state=[0.23247624 0.73239012 0.14605133 0.12840065], action=1, reward=1.0, next_state=[ 0.24712405  0.92515075  0.14861934 -0.11487535]\n",
      "[ episode 338 ][ timestamp 48 ] state=[ 0.24712405  0.92515075  0.14861934 -0.11487535], action=0, reward=1.0, next_state=[0.26562706 0.72824643 0.14632184 0.22075867]\n",
      "[ episode 338 ][ timestamp 49 ] state=[0.26562706 0.72824643 0.14632184 0.22075867], action=1, reward=1.0, next_state=[ 0.28019199  0.92100684  0.15073701 -0.02242693]\n",
      "[ episode 338 ][ timestamp 50 ] state=[ 0.28019199  0.92100684  0.15073701 -0.02242693], action=0, reward=1.0, next_state=[0.29861213 0.7240807  0.15028847 0.31376181]\n",
      "[ episode 338 ][ timestamp 51 ] state=[0.29861213 0.7240807  0.15028847 0.31376181], action=1, reward=1.0, next_state=[0.31309374 0.91677791 0.15656371 0.07199282]\n",
      "[ episode 338 ][ timestamp 52 ] state=[0.31309374 0.91677791 0.15656371 0.07199282], action=1, reward=1.0, next_state=[ 0.3314293   1.10934923  0.15800356 -0.16748922]\n",
      "[ episode 338 ][ timestamp 53 ] state=[ 0.3314293   1.10934923  0.15800356 -0.16748922], action=1, reward=1.0, next_state=[ 0.35361628  1.3018982   0.15465378 -0.4064549 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 338 ][ timestamp 54 ] state=[ 0.35361628  1.3018982   0.15465378 -0.4064549 ], action=1, reward=1.0, next_state=[ 0.37965425  1.49452778  0.14652468 -0.64666352]\n",
      "[ episode 338 ][ timestamp 55 ] state=[ 0.37965425  1.49452778  0.14652468 -0.64666352], action=0, reward=1.0, next_state=[ 0.4095448   1.29770095  0.13359141 -0.31166267]\n",
      "[ episode 338 ][ timestamp 56 ] state=[ 0.4095448   1.29770095  0.13359141 -0.31166267], action=0, reward=1.0, next_state=[0.43549882 1.10095384 0.12735816 0.01998761]\n",
      "[ episode 338 ][ timestamp 57 ] state=[0.43549882 1.10095384 0.12735816 0.01998761], action=0, reward=1.0, next_state=[0.4575179  0.90425731 0.12775791 0.34998497]\n",
      "[ episode 338 ][ timestamp 58 ] state=[0.4575179  0.90425731 0.12775791 0.34998497], action=1, reward=1.0, next_state=[0.47560304 1.09735276 0.13475761 0.1001611 ]\n",
      "[ episode 338 ][ timestamp 59 ] state=[0.47560304 1.09735276 0.13475761 0.1001611 ], action=1, reward=1.0, next_state=[ 0.4975501   1.29031187  0.13676083 -0.14715456]\n",
      "[ episode 338 ][ timestamp 60 ] state=[ 0.4975501   1.29031187  0.13676083 -0.14715456], action=0, reward=1.0, next_state=[0.52335634 1.09352346 0.13381774 0.18535435]\n",
      "[ episode 338 ][ timestamp 61 ] state=[0.52335634 1.09352346 0.13381774 0.18535435], action=1, reward=1.0, next_state=[ 0.54522681  1.28650222  0.13752483 -0.06230077]\n",
      "[ episode 338 ][ timestamp 62 ] state=[ 0.54522681  1.28650222  0.13752483 -0.06230077], action=0, reward=1.0, next_state=[0.57095685 1.08970388 0.13627881 0.27041454]\n",
      "[ episode 338 ][ timestamp 63 ] state=[0.57095685 1.08970388 0.13627881 0.27041454], action=1, reward=1.0, next_state=[0.59275093 1.28264463 0.1416871  0.0236288 ]\n",
      "[ episode 338 ][ timestamp 64 ] state=[0.59275093 1.28264463 0.1416871  0.0236288 ], action=0, reward=1.0, next_state=[0.61840382 1.08580505 0.14215968 0.35744619]\n",
      "[ episode 338 ][ timestamp 65 ] state=[0.61840382 1.08580505 0.14215968 0.35744619], action=1, reward=1.0, next_state=[0.64011992 1.27864999 0.1493086  0.11275113]\n",
      "[ episode 338 ][ timestamp 66 ] state=[0.64011992 1.27864999 0.1493086  0.11275113], action=1, reward=1.0, next_state=[ 0.66569292  1.47135216  0.15156362 -0.12935235]\n",
      "[ episode 338 ][ timestamp 67 ] state=[ 0.66569292  1.47135216  0.15156362 -0.12935235], action=0, reward=1.0, next_state=[0.69511996 1.27442065 0.14897658 0.20704784]\n",
      "[ episode 338 ][ timestamp 68 ] state=[0.69511996 1.27442065 0.14897658 0.20704784], action=1, reward=1.0, next_state=[ 0.72060838  1.46713309  0.15311753 -0.03518167]\n",
      "[ episode 338 ][ timestamp 69 ] state=[ 0.72060838  1.46713309  0.15311753 -0.03518167], action=0, reward=1.0, next_state=[0.74995104 1.27018479 0.1524139  0.30162532]\n",
      "[ episode 338 ][ timestamp 70 ] state=[0.74995104 1.27018479 0.1524139  0.30162532], action=1, reward=1.0, next_state=[0.77535474 1.46284312 0.15844641 0.06062431]\n",
      "[ episode 338 ][ timestamp 71 ] state=[0.77535474 1.46284312 0.15844641 0.06062431], action=1, reward=1.0, next_state=[ 0.8046116   1.65538039  0.15965889 -0.17817532]\n",
      "[ episode 338 ][ timestamp 72 ] state=[ 0.8046116   1.65538039  0.15965889 -0.17817532], action=0, reward=1.0, next_state=[0.83771921 1.45837668 0.15609539 0.16031243]\n",
      "[ episode 338 ][ timestamp 73 ] state=[0.83771921 1.45837668 0.15609539 0.16031243], action=1, reward=1.0, next_state=[ 0.86688674  1.65095951  0.15930164 -0.07934373]\n",
      "[ episode 338 ][ timestamp 74 ] state=[ 0.86688674  1.65095951  0.15930164 -0.07934373], action=0, reward=1.0, next_state=[0.89990593 1.45395509 0.15771476 0.25905812]\n",
      "[ episode 338 ][ timestamp 75 ] state=[0.89990593 1.45395509 0.15771476 0.25905812], action=1, reward=1.0, next_state=[0.92898503 1.64651522 0.16289592 0.01997895]\n",
      "[ episode 338 ][ timestamp 76 ] state=[0.92898503 1.64651522 0.16289592 0.01997895], action=1, reward=1.0, next_state=[ 0.96191534  1.83897179  0.1632955  -0.21720436]\n",
      "[ episode 338 ][ timestamp 77 ] state=[ 0.96191534  1.83897179  0.1632955  -0.21720436], action=0, reward=1.0, next_state=[0.99869477 1.64193787 0.15895141 0.12221057]\n",
      "[ episode 338 ][ timestamp 78 ] state=[0.99869477 1.64193787 0.15895141 0.12221057], action=1, reward=1.0, next_state=[ 1.03153353  1.8344678   0.16139563 -0.11640853]\n",
      "[ episode 338 ][ timestamp 79 ] state=[ 1.03153353  1.8344678   0.16139563 -0.11640853], action=0, reward=1.0, next_state=[1.06822288 1.63744558 0.15906746 0.22252862]\n",
      "[ episode 338 ][ timestamp 80 ] state=[1.06822288 1.63744558 0.15906746 0.22252862], action=0, reward=1.0, next_state=[1.1009718  1.44044996 0.16351803 0.56086044]\n",
      "[ episode 338 ][ timestamp 81 ] state=[1.1009718  1.44044996 0.16351803 0.56086044], action=1, reward=1.0, next_state=[1.1297808  1.6329451  0.17473524 0.32382971]\n",
      "[ episode 338 ][ timestamp 82 ] state=[1.1297808  1.6329451  0.17473524 0.32382971], action=1, reward=1.0, next_state=[1.1624397  1.82520441 0.18121183 0.0909433 ]\n",
      "[ episode 338 ][ timestamp 83 ] state=[1.1624397  1.82520441 0.18121183 0.0909433 ], action=1, reward=1.0, next_state=[ 1.19894379  2.01732889  0.1830307  -0.13953949]\n",
      "[ episode 338 ][ timestamp 84 ] state=[ 1.19894379  2.01732889  0.1830307  -0.13953949], action=0, reward=1.0, next_state=[1.23929036 1.82012173 0.18023991 0.20484129]\n",
      "[ episode 338 ][ timestamp 85 ] state=[1.23929036 1.82012173 0.18023991 0.20484129], action=1, reward=1.0, next_state=[ 1.2756928   2.01226991  0.18433673 -0.0260079 ]\n",
      "[ episode 338 ][ timestamp 86 ] state=[ 1.2756928   2.01226991  0.18433673 -0.0260079 ], action=1, reward=1.0, next_state=[ 1.3159382   2.20433565  0.18381657 -0.25533696]\n",
      "[ episode 338 ][ timestamp 87 ] state=[ 1.3159382   2.20433565  0.18381657 -0.25533696], action=0, reward=1.0, next_state=[1.36002491 2.0071302  0.17870984 0.08922606]\n",
      "[ episode 338 ][ timestamp 88 ] state=[1.36002491 2.0071302  0.17870984 0.08922606], action=1, reward=1.0, next_state=[ 1.40016751  2.19930048  0.18049436 -0.14217708]\n",
      "[ episode 338 ][ timestamp 89 ] state=[ 1.40016751  2.19930048  0.18049436 -0.14217708], action=0, reward=1.0, next_state=[1.44415352 2.00211433 0.17765082 0.2015749 ]\n",
      "[ episode 338 ][ timestamp 90 ] state=[1.44415352 2.00211433 0.17765082 0.2015749 ], action=1, reward=1.0, next_state=[ 1.48419581  2.19430961  0.18168231 -0.03022568]\n",
      "[ episode 338 ][ timestamp 91 ] state=[ 1.48419581  2.19430961  0.18168231 -0.03022568], action=1, reward=1.0, next_state=[ 1.528082    2.38642417  0.1810778  -0.2605333 ]\n",
      "[ episode 338 ][ timestamp 92 ] state=[ 1.528082    2.38642417  0.1810778  -0.2605333 ], action=0, reward=1.0, next_state=[1.57581049 2.18924155 0.17586713 0.08335118]\n",
      "[ episode 338 ][ timestamp 93 ] state=[1.57581049 2.18924155 0.17586713 0.08335118], action=0, reward=1.0, next_state=[1.61959532 1.99209187 0.17753416 0.42595303]\n",
      "[ episode 338 ][ timestamp 94 ] state=[1.61959532 1.99209187 0.17753416 0.42595303], action=1, reward=1.0, next_state=[1.65943715 2.18431348 0.18605322 0.19407387]\n",
      "[ episode 338 ][ timestamp 95 ] state=[1.65943715 2.18431348 0.18605322 0.19407387], action=1, reward=1.0, next_state=[ 1.70312342  2.37635428  0.1899347  -0.03463135]\n",
      "[ episode 338 ][ timestamp 96 ] state=[ 1.70312342  2.37635428  0.1899347  -0.03463135], action=1, reward=1.0, next_state=[ 1.75065051  2.5683169   0.18924207 -0.2618914 ]\n",
      "[ episode 338 ][ timestamp 97 ] state=[ 1.75065051  2.5683169   0.18924207 -0.2618914 ], action=0, reward=1.0, next_state=[1.80201685 2.37106859 0.18400424 0.08400455]\n",
      "[ episode 338 ][ timestamp 98 ] state=[1.80201685 2.37106859 0.18400424 0.08400455], action=1, reward=1.0, next_state=[ 1.84943822  2.56314158  0.18568433 -0.14544884]\n",
      "[ episode 338 ][ timestamp 99 ] state=[ 1.84943822  2.56314158  0.18568433 -0.14544884], action=0, reward=1.0, next_state=[1.90070105 2.36591313 0.18277535 0.19958637]\n",
      "[ episode 338 ][ timestamp 100 ] state=[1.90070105 2.36591313 0.18277535 0.19958637], action=1, reward=1.0, next_state=[ 1.94801931  2.5580145   0.18676708 -0.03032868]\n",
      "[ episode 338 ][ timestamp 101 ] state=[ 1.94801931  2.5580145   0.18676708 -0.03032868], action=1, reward=1.0, next_state=[ 1.9991796   2.75003553  0.18616051 -0.25876041]\n",
      "[ episode 338 ][ timestamp 102 ] state=[ 1.9991796   2.75003553  0.18616051 -0.25876041], action=0, reward=1.0, next_state=[2.05418031 2.55281148 0.1809853  0.08637986]\n",
      "[ episode 338 ][ timestamp 103 ] state=[2.05418031 2.55281148 0.1809853  0.08637986], action=1, reward=1.0, next_state=[ 2.10523654  2.74493998  0.1827129  -0.14418612]\n",
      "[ episode 338 ][ timestamp 104 ] state=[ 2.10523654  2.74493998  0.1827129  -0.14418612], action=0, reward=1.0, next_state=[2.16013534 2.54773565 0.17982918 0.2001157 ]\n",
      "[ episode 338 ][ timestamp 105 ] state=[2.16013534 2.54773565 0.17982918 0.2001157 ], action=1, reward=1.0, next_state=[ 2.21109006  2.73989102  0.18383149 -0.0308841 ]\n",
      "[ episode 338 ][ timestamp 106 ] state=[ 2.21109006  2.73989102  0.18383149 -0.0308841 ], action=1, reward=1.0, next_state=[ 2.26588788  2.9319661   0.18321381 -0.26039961]\n",
      "[ episode 338 ][ timestamp 107 ] state=[ 2.26588788  2.9319661   0.18321381 -0.26039961], action=0, reward=1.0, next_state=[2.3245272  2.73476601 0.17800581 0.08401381]\n",
      "[ episode 338 ][ timestamp 108 ] state=[2.3245272  2.73476601 0.17800581 0.08401381], action=1, reward=1.0, next_state=[ 2.37922252  2.92694901  0.17968609 -0.14764783]\n",
      "[ episode 338 ][ timestamp 109 ] state=[ 2.37922252  2.92694901  0.17968609 -0.14764783], action=0, reward=-1.0, next_state=[2.4377615  2.72976985 0.17673313 0.19590288]\n",
      "[ Ended! ] Episode 338: Exploration_rate=0.1846622876013121. Score=109.\n",
      "[ Experience replay ] starts\n",
      "[ episode 339 ] state=[ 0.02302375  0.04946729 -0.02713549 -0.03035186]\n",
      "[ episode 339 ][ timestamp 1 ] state=[ 0.02302375  0.04946729 -0.02713549 -0.03035186], action=1, reward=1.0, next_state=[ 0.02401309  0.24496765 -0.02774253 -0.33147131]\n",
      "[ episode 339 ][ timestamp 2 ] state=[ 0.02401309  0.24496765 -0.02774253 -0.33147131], action=1, reward=1.0, next_state=[ 0.02891245  0.44047329 -0.03437195 -0.63277217]\n",
      "[ episode 339 ][ timestamp 3 ] state=[ 0.02891245  0.44047329 -0.03437195 -0.63277217], action=1, reward=1.0, next_state=[ 0.03772191  0.63605747 -0.0470274  -0.93607853]\n",
      "[ episode 339 ][ timestamp 4 ] state=[ 0.03772191  0.63605747 -0.0470274  -0.93607853], action=1, reward=1.0, next_state=[ 0.05044306  0.83178103 -0.06574897 -1.24316024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 339 ][ timestamp 5 ] state=[ 0.05044306  0.83178103 -0.06574897 -1.24316024], action=1, reward=1.0, next_state=[ 0.06707868  1.02768227 -0.09061217 -1.55569345]\n",
      "[ episode 339 ][ timestamp 6 ] state=[ 0.06707868  1.02768227 -0.09061217 -1.55569345], action=0, reward=1.0, next_state=[ 0.08763233  0.83375526 -0.12172604 -1.29259984]\n",
      "[ episode 339 ][ timestamp 7 ] state=[ 0.08763233  0.83375526 -0.12172604 -1.29259984], action=1, reward=1.0, next_state=[ 0.10430743  1.03019588 -0.14757804 -1.62077959]\n",
      "[ episode 339 ][ timestamp 8 ] state=[ 0.10430743  1.03019588 -0.14757804 -1.62077959], action=1, reward=1.0, next_state=[ 0.12491135  1.22671597 -0.17999363 -1.9555861 ]\n",
      "[ episode 339 ][ timestamp 9 ] state=[ 0.12491135  1.22671597 -0.17999363 -1.9555861 ], action=1, reward=-1.0, next_state=[ 0.14944567  1.4232351  -0.21910535 -2.29823546]\n",
      "[ Ended! ] Episode 339: Exploration_rate=0.18373897616330553. Score=9.\n",
      "[ Experience replay ] starts\n",
      "[ episode 340 ] state=[-0.03185152  0.01907287  0.00333619  0.02256361]\n",
      "[ episode 340 ][ timestamp 1 ] state=[-0.03185152  0.01907287  0.00333619  0.02256361], action=1, reward=1.0, next_state=[-0.03147007  0.21414682  0.00378746 -0.26906485]\n",
      "[ episode 340 ][ timestamp 2 ] state=[-0.03147007  0.21414682  0.00378746 -0.26906485], action=1, reward=1.0, next_state=[-0.02718713  0.40921451 -0.00159384 -0.56055079]\n",
      "[ episode 340 ][ timestamp 3 ] state=[-0.02718713  0.40921451 -0.00159384 -0.56055079], action=1, reward=1.0, next_state=[-0.01900284  0.6043588  -0.01280486 -0.85373543]\n",
      "[ episode 340 ][ timestamp 4 ] state=[-0.01900284  0.6043588  -0.01280486 -0.85373543], action=1, reward=1.0, next_state=[-0.00691566  0.79965292 -0.02987956 -1.15041713]\n",
      "[ episode 340 ][ timestamp 5 ] state=[-0.00691566  0.79965292 -0.02987956 -1.15041713], action=1, reward=1.0, next_state=[ 0.00907739  0.99515179 -0.05288791 -1.45231782]\n",
      "[ episode 340 ][ timestamp 6 ] state=[ 0.00907739  0.99515179 -0.05288791 -1.45231782], action=1, reward=1.0, next_state=[ 0.02898043  1.19088202 -0.08193426 -1.76104444]\n",
      "[ episode 340 ][ timestamp 7 ] state=[ 0.02898043  1.19088202 -0.08193426 -1.76104444], action=1, reward=1.0, next_state=[ 0.05279807  1.38683011 -0.11715515 -2.07804228]\n",
      "[ episode 340 ][ timestamp 8 ] state=[ 0.05279807  1.38683011 -0.11715515 -2.07804228], action=0, reward=1.0, next_state=[ 0.08053467  1.19307426 -0.158716   -1.82376563]\n",
      "[ episode 340 ][ timestamp 9 ] state=[ 0.08053467  1.19307426 -0.158716   -1.82376563], action=0, reward=1.0, next_state=[ 0.10439616  1.00003047 -0.19519131 -1.58430632]\n",
      "[ episode 340 ][ timestamp 10 ] state=[ 0.10439616  1.00003047 -0.19519131 -1.58430632], action=1, reward=-1.0, next_state=[ 0.12439677  1.19686443 -0.22687744 -1.93097316]\n",
      "[ Ended! ] Episode 340: Exploration_rate=0.182820281282489. Score=10.\n",
      "[ Experience replay ] starts\n",
      "[ episode 341 ] state=[ 0.02170645 -0.02133633 -0.03047621 -0.03430287]\n",
      "[ episode 341 ][ timestamp 1 ] state=[ 0.02170645 -0.02133633 -0.03047621 -0.03430287], action=1, reward=1.0, next_state=[ 0.02127972  0.17420911 -0.03116227 -0.33644343]\n",
      "[ episode 341 ][ timestamp 2 ] state=[ 0.02127972  0.17420911 -0.03116227 -0.33644343], action=1, reward=1.0, next_state=[ 0.0247639   0.36976035 -0.03789114 -0.63878811]\n",
      "[ episode 341 ][ timestamp 3 ] state=[ 0.0247639   0.36976035 -0.03789114 -0.63878811], action=1, reward=1.0, next_state=[ 0.03215911  0.56538957 -0.0506669  -0.94315865]\n",
      "[ episode 341 ][ timestamp 4 ] state=[ 0.03215911  0.56538957 -0.0506669  -0.94315865], action=1, reward=1.0, next_state=[ 0.0434669   0.76115623 -0.06953008 -1.25132148]\n",
      "[ episode 341 ][ timestamp 5 ] state=[ 0.0434669   0.76115623 -0.06953008 -1.25132148], action=1, reward=1.0, next_state=[ 0.05869002  0.9570968  -0.0945565  -1.56494756]\n",
      "[ episode 341 ][ timestamp 6 ] state=[ 0.05869002  0.9570968  -0.0945565  -1.56494756], action=1, reward=1.0, next_state=[ 0.07783196  1.15321325 -0.12585546 -1.88556632]\n",
      "[ episode 341 ][ timestamp 7 ] state=[ 0.07783196  1.15321325 -0.12585546 -1.88556632], action=1, reward=1.0, next_state=[ 0.10089622  1.34945951 -0.16356678 -2.21451134]\n",
      "[ episode 341 ][ timestamp 8 ] state=[ 0.10089622  1.34945951 -0.16356678 -2.21451134], action=1, reward=1.0, next_state=[ 0.12788542  1.54572574 -0.20785701 -2.55285575]\n",
      "[ episode 341 ][ timestamp 9 ] state=[ 0.12788542  1.54572574 -0.20785701 -2.55285575], action=1, reward=-1.0, next_state=[ 0.15879993  1.74182042 -0.25891412 -2.90133736]\n",
      "[ Ended! ] Episode 341: Exploration_rate=0.18190617987607657. Score=9.\n",
      "[ Experience replay ] starts\n",
      "[ episode 342 ] state=[-0.01045868 -0.01736578 -0.01823658 -0.00295807]\n",
      "[ episode 342 ][ timestamp 1 ] state=[-0.01045868 -0.01736578 -0.01823658 -0.00295807], action=1, reward=1.0, next_state=[-0.010806    0.1780129  -0.01829574 -0.30133862]\n",
      "[ episode 342 ][ timestamp 2 ] state=[-0.010806    0.1780129  -0.01829574 -0.30133862], action=1, reward=1.0, next_state=[-0.00724574  0.37339078 -0.02432252 -0.59973503]\n",
      "[ episode 342 ][ timestamp 3 ] state=[-0.00724574  0.37339078 -0.02432252 -0.59973503], action=1, reward=1.0, next_state=[ 2.22073299e-04  5.68844419e-01 -3.63172170e-02 -8.99978888e-01]\n",
      "[ episode 342 ][ timestamp 4 ] state=[ 2.22073299e-04  5.68844419e-01 -3.63172170e-02 -8.99978888e-01], action=1, reward=1.0, next_state=[ 0.01159896  0.76443919 -0.05431679 -1.2038525 ]\n",
      "[ episode 342 ][ timestamp 5 ] state=[ 0.01159896  0.76443919 -0.05431679 -1.2038525 ], action=1, reward=1.0, next_state=[ 0.02688775  0.96021961 -0.07839384 -1.51305131]\n",
      "[ episode 342 ][ timestamp 6 ] state=[ 0.02688775  0.96021961 -0.07839384 -1.51305131], action=1, reward=1.0, next_state=[ 0.04609214  1.15619839 -0.10865487 -1.82914082]\n",
      "[ episode 342 ][ timestamp 7 ] state=[ 0.04609214  1.15619839 -0.10865487 -1.82914082], action=1, reward=1.0, next_state=[ 0.06921611  1.35234363 -0.14523769 -2.15350536]\n",
      "[ episode 342 ][ timestamp 8 ] state=[ 0.06921611  1.35234363 -0.14523769 -2.15350536], action=1, reward=1.0, next_state=[ 0.09626298  1.54856378 -0.18830779 -2.48728666]\n",
      "[ episode 342 ][ timestamp 9 ] state=[ 0.09626298  1.54856378 -0.18830779 -2.48728666], action=1, reward=-1.0, next_state=[ 0.12723425  1.7446902  -0.23805353 -2.83131161]\n",
      "[ Ended! ] Episode 342: Exploration_rate=0.18099664897669618. Score=9.\n",
      "[ Experience replay ] starts\n",
      "[ episode 343 ] state=[-0.03448542 -0.02425851  0.03300848 -0.01344822]\n",
      "[ episode 343 ][ timestamp 1 ] state=[-0.03448542 -0.02425851  0.03300848 -0.01344822], action=1, reward=1.0, next_state=[-0.03497059  0.17037489  0.03273952 -0.29553656]\n",
      "[ episode 343 ][ timestamp 2 ] state=[-0.03497059  0.17037489  0.03273952 -0.29553656], action=0, reward=1.0, next_state=[-0.03156309 -0.02519814  0.02682878  0.00728947]\n",
      "[ episode 343 ][ timestamp 3 ] state=[-0.03156309 -0.02519814  0.02682878  0.00728947], action=1, reward=1.0, next_state=[-0.03206706  0.16952898  0.02697457 -0.27680938]\n",
      "[ episode 343 ][ timestamp 4 ] state=[-0.03206706  0.16952898  0.02697457 -0.27680938], action=0, reward=1.0, next_state=[-0.02867648 -0.02596722  0.02143839  0.0242578 ]\n",
      "[ episode 343 ][ timestamp 5 ] state=[-0.02867648 -0.02596722  0.02143839  0.0242578 ], action=1, reward=1.0, next_state=[-0.02919582  0.16884083  0.02192354 -0.26158472]\n",
      "[ episode 343 ][ timestamp 6 ] state=[-0.02919582  0.16884083  0.02192354 -0.26158472], action=0, reward=1.0, next_state=[-0.025819   -0.0265871   0.01669185  0.03793173]\n",
      "[ episode 343 ][ timestamp 7 ] state=[-0.025819   -0.0265871   0.01669185  0.03793173], action=1, reward=1.0, next_state=[-0.02635075  0.16829156  0.01745048 -0.24943836]\n",
      "[ episode 343 ][ timestamp 8 ] state=[-0.02635075  0.16829156  0.01745048 -0.24943836], action=1, reward=1.0, next_state=[-0.02298492  0.36316002  0.01246172 -0.53656636]\n",
      "[ episode 343 ][ timestamp 9 ] state=[-0.02298492  0.36316002  0.01246172 -0.53656636], action=1, reward=1.0, next_state=[-0.01572172  0.55810455  0.00173039 -0.8252968 ]\n",
      "[ episode 343 ][ timestamp 10 ] state=[-0.01572172  0.55810455  0.00173039 -0.8252968 ], action=1, reward=1.0, next_state=[-0.00455962  0.7532028  -0.01477555 -1.117435  ]\n",
      "[ episode 343 ][ timestamp 11 ] state=[-0.00455962  0.7532028  -0.01477555 -1.117435  ], action=1, reward=1.0, next_state=[ 0.01050443  0.9485155  -0.03712425 -1.41471593]\n",
      "[ episode 343 ][ timestamp 12 ] state=[ 0.01050443  0.9485155  -0.03712425 -1.41471593], action=1, reward=1.0, next_state=[ 0.02947474  1.14407719 -0.06541857 -1.71876837]\n",
      "[ episode 343 ][ timestamp 13 ] state=[ 0.02947474  1.14407719 -0.06541857 -1.71876837], action=1, reward=1.0, next_state=[ 0.05235629  1.33988503 -0.09979393 -2.03107121]\n",
      "[ episode 343 ][ timestamp 14 ] state=[ 0.05235629  1.33988503 -0.09979393 -2.03107121], action=1, reward=1.0, next_state=[ 0.07915399  1.53588537 -0.14041536 -2.35289973]\n",
      "[ episode 343 ][ timestamp 15 ] state=[ 0.07915399  1.53588537 -0.14041536 -2.35289973], action=1, reward=1.0, next_state=[ 0.10987169  1.7319578  -0.18747335 -2.68526032]\n",
      "[ episode 343 ][ timestamp 16 ] state=[ 0.10987169  1.7319578  -0.18747335 -2.68526032], action=1, reward=-1.0, next_state=[ 0.14451085  1.9278967  -0.24117856 -3.02881377]\n",
      "[ Ended! ] Episode 343: Exploration_rate=0.1800916657318127. Score=16.\n",
      "[ Experience replay ] starts\n",
      "[ episode 344 ] state=[ 0.03886343 -0.02747804 -0.00310958  0.04692053]\n",
      "[ episode 344 ][ timestamp 1 ] state=[ 0.03886343 -0.02747804 -0.00310958  0.04692053], action=0, reward=1.0, next_state=[ 0.03831387 -0.22255527 -0.00217117  0.33862073]\n",
      "[ episode 344 ][ timestamp 2 ] state=[ 0.03831387 -0.22255527 -0.00217117  0.33862073], action=1, reward=1.0, next_state=[ 0.03386276 -0.02740249  0.00460124  0.04525393]\n",
      "[ episode 344 ][ timestamp 3 ] state=[ 0.03386276 -0.02740249  0.00460124  0.04525393], action=0, reward=1.0, next_state=[ 0.03331471 -0.22259012  0.00550632  0.33938503]\n",
      "[ episode 344 ][ timestamp 4 ] state=[ 0.03331471 -0.22259012  0.00550632  0.33938503], action=0, reward=1.0, next_state=[ 0.02886291 -0.41778998  0.01229402  0.63379924]\n",
      "[ episode 344 ][ timestamp 5 ] state=[ 0.02886291 -0.41778998  0.01229402  0.63379924], action=1, reward=1.0, next_state=[ 0.02050711 -0.22284167  0.02497001  0.34501322]\n",
      "[ episode 344 ][ timestamp 6 ] state=[ 0.02050711 -0.22284167  0.02497001  0.34501322], action=1, reward=1.0, next_state=[ 0.01605028 -0.02808366  0.03187027  0.06030769]\n",
      "[ episode 344 ][ timestamp 7 ] state=[ 0.01605028 -0.02808366  0.03187027  0.06030769], action=0, reward=1.0, next_state=[ 0.0154886  -0.22364772  0.03307642  0.3628731 ]\n",
      "[ episode 344 ][ timestamp 8 ] state=[ 0.0154886  -0.22364772  0.03307642  0.3628731 ], action=1, reward=1.0, next_state=[ 0.01101565 -0.02901112  0.04033389  0.08080058]\n",
      "[ episode 344 ][ timestamp 9 ] state=[ 0.01101565 -0.02901112  0.04033389  0.08080058], action=0, reward=1.0, next_state=[ 0.01043543 -0.22468735  0.0419499   0.38593116]\n",
      "[ episode 344 ][ timestamp 10 ] state=[ 0.01043543 -0.22468735  0.0419499   0.38593116], action=1, reward=1.0, next_state=[ 0.00594168 -0.03018525  0.04966852  0.10676434]\n",
      "[ episode 344 ][ timestamp 11 ] state=[ 0.00594168 -0.03018525  0.04966852  0.10676434], action=1, reward=1.0, next_state=[ 0.00533797  0.16419104  0.05180381 -0.16984399]\n",
      "[ episode 344 ][ timestamp 12 ] state=[ 0.00533797  0.16419104  0.05180381 -0.16984399], action=0, reward=1.0, next_state=[ 0.00862179 -0.03163265  0.04840693  0.138721  ]\n",
      "[ episode 344 ][ timestamp 13 ] state=[ 0.00862179 -0.03163265  0.04840693  0.138721  ], action=0, reward=1.0, next_state=[ 0.00798914 -0.22741329  0.05118135  0.44627404]\n",
      "[ episode 344 ][ timestamp 14 ] state=[ 0.00798914 -0.22741329  0.05118135  0.44627404], action=1, reward=1.0, next_state=[ 0.00344088 -0.03305135  0.06010683  0.17015365]\n",
      "[ episode 344 ][ timestamp 15 ] state=[ 0.00344088 -0.03305135  0.06010683  0.17015365], action=1, reward=1.0, next_state=[ 0.00277985  0.16116107  0.0635099  -0.10297813]\n",
      "[ episode 344 ][ timestamp 16 ] state=[ 0.00277985  0.16116107  0.0635099  -0.10297813], action=0, reward=1.0, next_state=[ 0.00600307 -0.03481084  0.06145034  0.20904646]\n",
      "[ episode 344 ][ timestamp 17 ] state=[ 0.00600307 -0.03481084  0.06145034  0.20904646], action=1, reward=1.0, next_state=[ 0.00530685  0.15938107  0.06563127 -0.06363658]\n",
      "[ episode 344 ][ timestamp 18 ] state=[ 0.00530685  0.15938107  0.06563127 -0.06363658], action=0, reward=1.0, next_state=[ 0.00849448 -0.03661748  0.06435854  0.24901003]\n",
      "[ episode 344 ][ timestamp 19 ] state=[ 0.00849448 -0.03661748  0.06435854  0.24901003], action=1, reward=1.0, next_state=[ 0.00776213  0.15752915  0.06933874 -0.02269865]\n",
      "[ episode 344 ][ timestamp 20 ] state=[ 0.00776213  0.15752915  0.06933874 -0.02269865], action=0, reward=1.0, next_state=[ 0.01091271 -0.03851515  0.06888476  0.29103041]\n",
      "[ episode 344 ][ timestamp 21 ] state=[ 0.01091271 -0.03851515  0.06888476  0.29103041], action=1, reward=1.0, next_state=[0.01014241 0.15556043 0.07470537 0.02084357]\n",
      "[ episode 344 ][ timestamp 22 ] state=[0.01014241 0.15556043 0.07470537 0.02084357], action=0, reward=1.0, next_state=[ 0.01325361 -0.04054896  0.07512224  0.33613014]\n",
      "[ episode 344 ][ timestamp 23 ] state=[ 0.01325361 -0.04054896  0.07512224  0.33613014], action=1, reward=1.0, next_state=[0.01244263 0.153428   0.08184485 0.06805049]\n",
      "[ episode 344 ][ timestamp 24 ] state=[0.01244263 0.153428   0.08184485 0.06805049], action=0, reward=1.0, next_state=[ 0.0155112  -0.04276617  0.08320586  0.38539217]\n",
      "[ episode 344 ][ timestamp 25 ] state=[ 0.0155112  -0.04276617  0.08320586  0.38539217], action=1, reward=1.0, next_state=[0.01465587 0.15108206 0.0909137  0.12006009]\n",
      "[ episode 344 ][ timestamp 26 ] state=[0.01465587 0.15108206 0.0909137  0.12006009], action=1, reward=1.0, next_state=[ 0.01767751  0.34479179  0.0933149  -0.14261272]\n",
      "[ episode 344 ][ timestamp 27 ] state=[ 0.01767751  0.34479179  0.0933149  -0.14261272], action=0, reward=1.0, next_state=[0.02457335 0.1484659  0.09046265 0.17798968]\n",
      "[ episode 344 ][ timestamp 28 ] state=[0.02457335 0.1484659  0.09046265 0.17798968], action=0, reward=1.0, next_state=[ 0.02754267 -0.04782634  0.09402244  0.49778386]\n",
      "[ episode 344 ][ timestamp 29 ] state=[ 0.02754267 -0.04782634  0.09402244  0.49778386], action=1, reward=1.0, next_state=[0.02658614 0.14585291 0.10397812 0.23615005]\n",
      "[ episode 344 ][ timestamp 30 ] state=[0.02658614 0.14585291 0.10397812 0.23615005], action=1, reward=1.0, next_state=[ 0.0295032   0.33934751  0.10870112 -0.02200979]\n",
      "[ episode 344 ][ timestamp 31 ] state=[ 0.0295032   0.33934751  0.10870112 -0.02200979], action=0, reward=1.0, next_state=[0.03629015 0.14284815 0.10826092 0.30289483]\n",
      "[ episode 344 ][ timestamp 32 ] state=[0.03629015 0.14284815 0.10826092 0.30289483], action=1, reward=1.0, next_state=[0.03914711 0.33627402 0.11431882 0.04622121]\n",
      "[ episode 344 ][ timestamp 33 ] state=[0.03914711 0.33627402 0.11431882 0.04622121], action=0, reward=1.0, next_state=[0.04587259 0.1397142  0.11524324 0.37267302]\n",
      "[ episode 344 ][ timestamp 34 ] state=[0.04587259 0.1397142  0.11524324 0.37267302], action=1, reward=1.0, next_state=[0.04866688 0.33302655 0.12269671 0.11843447]\n",
      "[ episode 344 ][ timestamp 35 ] state=[0.04866688 0.33302655 0.12269671 0.11843447], action=0, reward=1.0, next_state=[0.05532741 0.13637976 0.12506539 0.44716952]\n",
      "[ episode 344 ][ timestamp 36 ] state=[0.05532741 0.13637976 0.12506539 0.44716952], action=1, reward=1.0, next_state=[0.058055   0.32953116 0.13400878 0.19637878]\n",
      "[ episode 344 ][ timestamp 37 ] state=[0.058055   0.32953116 0.13400878 0.19637878], action=0, reward=1.0, next_state=[0.06464563 0.13277228 0.13793636 0.52815172]\n",
      "[ episode 344 ][ timestamp 38 ] state=[0.06464563 0.13277228 0.13793636 0.52815172], action=0, reward=1.0, next_state=[ 0.06730107 -0.06399327  0.14849939  0.86092149]\n",
      "[ episode 344 ][ timestamp 39 ] state=[ 0.06730107 -0.06399327  0.14849939  0.86092149], action=1, reward=1.0, next_state=[0.06602121 0.1288283  0.16571782 0.61837092]\n",
      "[ episode 344 ][ timestamp 40 ] state=[0.06602121 0.1288283  0.16571782 0.61837092], action=1, reward=1.0, next_state=[0.06859777 0.3212952  0.17808524 0.38212405]\n",
      "[ episode 344 ][ timestamp 41 ] state=[0.06859777 0.3212952  0.17808524 0.38212405], action=1, reward=1.0, next_state=[0.07502368 0.51350071 0.18572772 0.15045622]\n",
      "[ episode 344 ][ timestamp 42 ] state=[0.07502368 0.51350071 0.18572772 0.15045622], action=1, reward=1.0, next_state=[ 0.08529369  0.70554479  0.18873685 -0.07836521]\n",
      "[ episode 344 ][ timestamp 43 ] state=[ 0.08529369  0.70554479  0.18873685 -0.07836521], action=0, reward=1.0, next_state=[0.09940459 0.50828909 0.18716954 0.26742383]\n",
      "[ episode 344 ][ timestamp 44 ] state=[0.09940459 0.50828909 0.18716954 0.26742383], action=1, reward=1.0, next_state=[0.10957037 0.70031548 0.19251802 0.03912201]\n",
      "[ episode 344 ][ timestamp 45 ] state=[0.10957037 0.70031548 0.19251802 0.03912201], action=0, reward=1.0, next_state=[0.12357668 0.503029   0.19330046 0.38583592]\n",
      "[ episode 344 ][ timestamp 46 ] state=[0.12357668 0.503029   0.19330046 0.38583592], action=1, reward=1.0, next_state=[0.13363726 0.69495694 0.20101718 0.15978291]\n",
      "[ episode 344 ][ timestamp 47 ] state=[0.13363726 0.69495694 0.20101718 0.15978291], action=1, reward=1.0, next_state=[ 0.1475364   0.88671864  0.20421284 -0.06336582]\n",
      "[ episode 344 ][ timestamp 48 ] state=[ 0.1475364   0.88671864  0.20421284 -0.06336582], action=0, reward=1.0, next_state=[0.16527077 0.68934347 0.20294552 0.28616717]\n",
      "[ episode 344 ][ timestamp 49 ] state=[0.16527077 0.68934347 0.20294552 0.28616717], action=1, reward=1.0, next_state=[0.17905764 0.88108031 0.20866886 0.06372165]\n",
      "[ episode 344 ][ timestamp 50 ] state=[0.17905764 0.88108031 0.20866886 0.06372165], action=0, reward=-1.0, next_state=[0.19667924 0.68367256 0.2099433  0.41431429]\n",
      "[ Ended! ] Episode 344: Exploration_rate=0.17919120740315364. Score=50.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 345 ] state=[ 0.01001406  0.03834866 -0.01681694 -0.03926449]\n",
      "[ episode 345 ][ timestamp 1 ] state=[ 0.01001406  0.03834866 -0.01681694 -0.03926449], action=0, reward=1.0, next_state=[ 0.01078104 -0.15652815 -0.01760223  0.24806544]\n",
      "[ episode 345 ][ timestamp 2 ] state=[ 0.01078104 -0.15652815 -0.01760223  0.24806544], action=1, reward=1.0, next_state=[ 0.00765047  0.03884071 -0.01264093 -0.05011723]\n",
      "[ episode 345 ][ timestamp 3 ] state=[ 0.00765047  0.03884071 -0.01264093 -0.05011723], action=0, reward=1.0, next_state=[ 0.00842729 -0.15609773 -0.01364327  0.23855072]\n",
      "[ episode 345 ][ timestamp 4 ] state=[ 0.00842729 -0.15609773 -0.01364327  0.23855072], action=1, reward=1.0, next_state=[ 0.00530533  0.03921645 -0.00887226 -0.05840427]\n",
      "[ episode 345 ][ timestamp 5 ] state=[ 0.00530533  0.03921645 -0.00887226 -0.05840427], action=0, reward=1.0, next_state=[ 0.00608966 -0.15577718 -0.01004034  0.23146624]\n",
      "[ episode 345 ][ timestamp 6 ] state=[ 0.00608966 -0.15577718 -0.01004034  0.23146624], action=1, reward=1.0, next_state=[ 0.00297412  0.03948679 -0.00541102 -0.06436676]\n",
      "[ episode 345 ][ timestamp 7 ] state=[ 0.00297412  0.03948679 -0.00541102 -0.06436676], action=0, reward=1.0, next_state=[ 0.00376385 -0.15555716 -0.00669835  0.22660406]\n",
      "[ episode 345 ][ timestamp 8 ] state=[ 0.00376385 -0.15555716 -0.00669835  0.22660406], action=1, reward=1.0, next_state=[ 0.00065271  0.03965988 -0.00216627 -0.06818423]\n",
      "[ episode 345 ][ timestamp 9 ] state=[ 0.00065271  0.03965988 -0.00216627 -0.06818423], action=0, reward=1.0, next_state=[ 0.00144591 -0.15543095 -0.00352995  0.22381444]\n",
      "[ episode 345 ][ timestamp 10 ] state=[ 0.00144591 -0.15543095 -0.00352995  0.22381444], action=1, reward=1.0, next_state=[-0.00166271  0.03974127  0.00094633 -0.06997987]\n",
      "[ episode 345 ][ timestamp 11 ] state=[-0.00166271  0.03974127  0.00094633 -0.06997987], action=0, reward=1.0, next_state=[-0.00086789 -0.15539423 -0.00045326  0.22300148]\n",
      "[ episode 345 ][ timestamp 12 ] state=[-0.00086789 -0.15539423 -0.00045326  0.22300148], action=1, reward=1.0, next_state=[-0.00397577  0.03973419  0.00400677 -0.06982439]\n",
      "[ episode 345 ][ timestamp 13 ] state=[-0.00397577  0.03973419  0.00400677 -0.06982439], action=0, reward=1.0, next_state=[-0.00318109 -0.15544497  0.00261028  0.22411999]\n",
      "[ episode 345 ][ timestamp 14 ] state=[-0.00318109 -0.15544497  0.00261028  0.22411999], action=1, reward=1.0, next_state=[-0.00628999  0.03963958  0.00709268 -0.06773841]\n",
      "[ episode 345 ][ timestamp 15 ] state=[-0.00628999  0.03963958  0.00709268 -0.06773841], action=0, reward=1.0, next_state=[-0.00549719 -0.15558334  0.00573791  0.22717383]\n",
      "[ episode 345 ][ timestamp 16 ] state=[-0.00549719 -0.15558334  0.00573791  0.22717383], action=1, reward=1.0, next_state=[-0.00860886  0.03945614  0.01028139 -0.06369364]\n",
      "[ episode 345 ][ timestamp 17 ] state=[-0.00860886  0.03945614  0.01028139 -0.06369364], action=0, reward=1.0, next_state=[-0.00781974 -0.1558117   0.00900751  0.23221531]\n",
      "[ episode 345 ][ timestamp 18 ] state=[-0.00781974 -0.1558117   0.00900751  0.23221531], action=1, reward=1.0, next_state=[-0.01093597  0.0391804   0.01365182 -0.05761279]\n",
      "[ episode 345 ][ timestamp 19 ] state=[-0.01093597  0.0391804   0.01365182 -0.05761279], action=0, reward=1.0, next_state=[-0.01015236 -0.15613461  0.01249956  0.23934593]\n",
      "[ episode 345 ][ timestamp 20 ] state=[-0.01015236 -0.15613461  0.01249956  0.23934593], action=1, reward=1.0, next_state=[-0.01327506  0.03880657  0.01728648 -0.04936822]\n",
      "[ episode 345 ][ timestamp 21 ] state=[-0.01327506  0.03880657  0.01728648 -0.04936822], action=0, reward=1.0, next_state=[-0.01249893 -0.15655894  0.01629912  0.24871822]\n",
      "[ episode 345 ][ timestamp 22 ] state=[-0.01249893 -0.15655894  0.01629912  0.24871822], action=1, reward=1.0, next_state=[-0.0156301   0.0383265   0.02127348 -0.03877937]\n",
      "[ episode 345 ][ timestamp 23 ] state=[-0.0156301   0.0383265   0.02127348 -0.03877937], action=0, reward=1.0, next_state=[-0.01486357 -0.15709395  0.0204979   0.26053891]\n",
      "[ episode 345 ][ timestamp 24 ] state=[-0.01486357 -0.15709395  0.0204979   0.26053891], action=1, reward=1.0, next_state=[-0.01800545  0.03772948  0.02570867 -0.02560889]\n",
      "[ episode 345 ][ timestamp 25 ] state=[-0.01800545  0.03772948  0.02570867 -0.02560889], action=0, reward=1.0, next_state=[-0.01725086 -0.15775154  0.0251965   0.27507327]\n",
      "[ episode 345 ][ timestamp 26 ] state=[-0.01725086 -0.15775154  0.0251965   0.27507327], action=1, reward=1.0, next_state=[-0.02040589  0.03700202  0.03069796 -0.00955737]\n",
      "[ episode 345 ][ timestamp 27 ] state=[-0.02040589  0.03700202  0.03069796 -0.00955737], action=0, reward=1.0, next_state=[-0.01966585 -0.15854642  0.03050681  0.29265089]\n",
      "[ episode 345 ][ timestamp 28 ] state=[-0.01966585 -0.15854642  0.03050681  0.29265089], action=1, reward=1.0, next_state=[-0.02283678  0.03612758  0.03635983  0.00974336]\n",
      "[ episode 345 ][ timestamp 29 ] state=[-0.02283678  0.03612758  0.03635983  0.00974336], action=1, reward=1.0, next_state=[-0.02211423  0.23070973  0.0365547  -0.27124951]\n",
      "[ episode 345 ][ timestamp 30 ] state=[-0.02211423  0.23070973  0.0365547  -0.27124951], action=0, reward=1.0, next_state=[-0.01750004  0.03508574  0.03112971  0.03273513]\n",
      "[ episode 345 ][ timestamp 31 ] state=[-0.01750004  0.03508574  0.03112971  0.03273513], action=1, reward=1.0, next_state=[-0.01679832  0.22974777  0.03178441 -0.24996579]\n",
      "[ episode 345 ][ timestamp 32 ] state=[-0.01679832  0.22974777  0.03178441 -0.24996579], action=0, reward=1.0, next_state=[-0.01220337  0.03418668  0.0267851   0.05257073]\n",
      "[ episode 345 ][ timestamp 33 ] state=[-0.01220337  0.03418668  0.0267851   0.05257073], action=0, reward=1.0, next_state=[-0.01151963 -0.1613089   0.02783651  0.35358278]\n",
      "[ episode 345 ][ timestamp 34 ] state=[-0.01151963 -0.1613089   0.02783651  0.35358278], action=1, reward=1.0, next_state=[-0.01474581  0.0334064   0.03490817  0.06980586]\n",
      "[ episode 345 ][ timestamp 35 ] state=[-0.01474581  0.0334064   0.03490817  0.06980586], action=1, reward=1.0, next_state=[-0.01407768  0.22801095  0.03630428 -0.2116622 ]\n",
      "[ episode 345 ][ timestamp 36 ] state=[-0.01407768  0.22801095  0.03630428 -0.2116622 ], action=0, reward=1.0, next_state=[-0.00951746  0.03238924  0.03207104  0.09224812]\n",
      "[ episode 345 ][ timestamp 37 ] state=[-0.00951746  0.03238924  0.03207104  0.09224812], action=1, reward=1.0, next_state=[-0.00886968  0.22703718  0.033916   -0.19014638]\n",
      "[ episode 345 ][ timestamp 38 ] state=[-0.00886968  0.22703718  0.033916   -0.19014638], action=0, reward=1.0, next_state=[-0.00432893  0.03144685  0.03011307  0.11303979]\n",
      "[ episode 345 ][ timestamp 39 ] state=[-0.00432893  0.03144685  0.03011307  0.11303979], action=1, reward=1.0, next_state=[-0.0037      0.22612466  0.03237387 -0.16999263]\n",
      "[ episode 345 ][ timestamp 40 ] state=[-0.0037      0.22612466  0.03237387 -0.16999263], action=0, reward=1.0, next_state=[0.0008225  0.03055465 0.02897402 0.13272492]\n",
      "[ episode 345 ][ timestamp 41 ] state=[0.0008225  0.03055465 0.02897402 0.13272492], action=1, reward=1.0, next_state=[ 0.00143359  0.22524985  0.03162852 -0.15067812]\n",
      "[ episode 345 ][ timestamp 42 ] state=[ 0.00143359  0.22524985  0.03162852 -0.15067812], action=0, reward=1.0, next_state=[0.00593859 0.02968961 0.02861495 0.15181275]\n",
      "[ episode 345 ][ timestamp 43 ] state=[0.00593859 0.02968961 0.02861495 0.15181275], action=1, reward=1.0, next_state=[ 0.00653238  0.22439039  0.03165121 -0.1317072 ]\n",
      "[ episode 345 ][ timestamp 44 ] state=[ 0.00653238  0.22439039  0.03165121 -0.1317072 ], action=0, reward=1.0, next_state=[0.01102019 0.02882968 0.02901706 0.17079084]\n",
      "[ episode 345 ][ timestamp 45 ] state=[0.01102019 0.02882968 0.02901706 0.17079084], action=1, reward=1.0, next_state=[ 0.01159678  0.22352455  0.03243288 -0.11259871]\n",
      "[ episode 345 ][ timestamp 46 ] state=[ 0.01159678  0.22352455  0.03243288 -0.11259871], action=0, reward=1.0, next_state=[0.01606727 0.02795324 0.03018091 0.19013758]\n",
      "[ episode 345 ][ timestamp 47 ] state=[0.01606727 0.02795324 0.03018091 0.19013758], action=1, reward=1.0, next_state=[ 0.01662634  0.22263071  0.03398366 -0.0928738 ]\n",
      "[ episode 345 ][ timestamp 48 ] state=[ 0.01662634  0.22263071  0.03398366 -0.0928738 ], action=0, reward=1.0, next_state=[0.02107895 0.02703856 0.03212618 0.21033429]\n",
      "[ episode 345 ][ timestamp 49 ] state=[0.02107895 0.02703856 0.03212618 0.21033429], action=0, reward=1.0, next_state=[ 0.02161972 -0.16852766  0.03633287  0.51297572]\n",
      "[ episode 345 ][ timestamp 50 ] state=[ 0.02161972 -0.16852766  0.03633287  0.51297572], action=1, reward=1.0, next_state=[0.01824917 0.02606423 0.04659238 0.23196005]\n",
      "[ episode 345 ][ timestamp 51 ] state=[0.01824917 0.02606423 0.04659238 0.23196005], action=1, reward=1.0, next_state=[ 0.01877045  0.22049052  0.05123158 -0.0456697 ]\n",
      "[ episode 345 ][ timestamp 52 ] state=[ 0.01877045  0.22049052  0.05123158 -0.0456697 ], action=0, reward=1.0, next_state=[0.02318026 0.0246728  0.05031819 0.262727  ]\n",
      "[ episode 345 ][ timestamp 53 ] state=[0.02318026 0.0246728  0.05031819 0.262727  ], action=1, reward=1.0, next_state=[ 0.02367372  0.21904174  0.05557273 -0.01367009]\n",
      "[ episode 345 ][ timestamp 54 ] state=[ 0.02367372  0.21904174  0.05557273 -0.01367009], action=1, reward=1.0, next_state=[ 0.02805455  0.41332449  0.05529933 -0.28831434]\n",
      "[ episode 345 ][ timestamp 55 ] state=[ 0.02805455  0.41332449  0.05529933 -0.28831434], action=0, reward=1.0, next_state=[0.03632104 0.21745934 0.04953304 0.02128399]\n",
      "[ episode 345 ][ timestamp 56 ] state=[0.03632104 0.21745934 0.04953304 0.02128399], action=1, reward=1.0, next_state=[ 0.04067023  0.41183724  0.04995872 -0.25536848]\n",
      "[ episode 345 ][ timestamp 57 ] state=[ 0.04067023  0.41183724  0.04995872 -0.25536848], action=0, reward=1.0, next_state=[0.04890697 0.2160389  0.04485135 0.05264434]\n",
      "[ episode 345 ][ timestamp 58 ] state=[0.04890697 0.2160389  0.04485135 0.05264434], action=1, reward=1.0, next_state=[ 0.05322775  0.41049     0.04590424 -0.22555711]\n",
      "[ episode 345 ][ timestamp 59 ] state=[ 0.05322775  0.41049     0.04590424 -0.22555711], action=1, reward=1.0, next_state=[ 0.06143755  0.60492686  0.0413931  -0.50341406]\n",
      "[ episode 345 ][ timestamp 60 ] state=[ 0.06143755  0.60492686  0.0413931  -0.50341406], action=0, reward=1.0, next_state=[ 0.07353609  0.4092467   0.03132481 -0.19797914]\n",
      "[ episode 345 ][ timestamp 61 ] state=[ 0.07353609  0.4092467   0.03132481 -0.19797914], action=0, reward=1.0, next_state=[0.08172102 0.21369103 0.02736523 0.10441844]\n",
      "[ episode 345 ][ timestamp 62 ] state=[0.08172102 0.21369103 0.02736523 0.10441844], action=1, reward=1.0, next_state=[ 0.08599484  0.40841034  0.0294536  -0.1795068 ]\n",
      "[ episode 345 ][ timestamp 63 ] state=[ 0.08599484  0.40841034  0.0294536  -0.1795068 ], action=0, reward=1.0, next_state=[0.09416305 0.21287956 0.02586346 0.12232027]\n",
      "[ episode 345 ][ timestamp 64 ] state=[0.09416305 0.21287956 0.02586346 0.12232027], action=1, reward=1.0, next_state=[ 0.09842064  0.40762161  0.02830987 -0.16209209]\n",
      "[ episode 345 ][ timestamp 65 ] state=[ 0.09842064  0.40762161  0.02830987 -0.16209209], action=0, reward=1.0, next_state=[0.10657307 0.21210606 0.02506803 0.13938571]\n",
      "[ episode 345 ][ timestamp 66 ] state=[0.10657307 0.21210606 0.02506803 0.13938571], action=1, reward=1.0, next_state=[ 0.1108152   0.40686017  0.02785574 -0.14528445]\n",
      "[ episode 345 ][ timestamp 67 ] state=[ 0.1108152   0.40686017  0.02785574 -0.14528445], action=0, reward=1.0, next_state=[0.1189524  0.2113506  0.02495005 0.15605466]\n",
      "[ episode 345 ][ timestamp 68 ] state=[0.1189524  0.2113506  0.02495005 0.15605466], action=1, reward=1.0, next_state=[ 0.12317941  0.40610661  0.02807115 -0.12865387]\n",
      "[ episode 345 ][ timestamp 69 ] state=[ 0.12317941  0.40610661  0.02807115 -0.12865387], action=0, reward=1.0, next_state=[0.13130154 0.21059401 0.02549807 0.17275133]\n",
      "[ episode 345 ][ timestamp 70 ] state=[0.13130154 0.21059401 0.02549807 0.17275133], action=1, reward=1.0, next_state=[ 0.13551342  0.40534192  0.0289531  -0.11177996]\n",
      "[ episode 345 ][ timestamp 71 ] state=[ 0.13551342  0.40534192  0.0289531  -0.11177996], action=0, reward=1.0, next_state=[0.14362026 0.20981731 0.0267175  0.18989505]\n",
      "[ episode 345 ][ timestamp 72 ] state=[0.14362026 0.20981731 0.0267175  0.18989505], action=1, reward=1.0, next_state=[ 0.14781661  0.40454705  0.0305154  -0.0942413 ]\n",
      "[ episode 345 ][ timestamp 73 ] state=[ 0.14781661  0.40454705  0.0305154  -0.0942413 ], action=0, reward=1.0, next_state=[0.15590755 0.20900132 0.02863057 0.20791087]\n",
      "[ episode 345 ][ timestamp 74 ] state=[0.15590755 0.20900132 0.02863057 0.20791087], action=1, reward=1.0, next_state=[ 0.16008758  0.40370242  0.03278879 -0.07560485]\n",
      "[ episode 345 ][ timestamp 75 ] state=[ 0.16008758  0.40370242  0.03278879 -0.07560485], action=0, reward=1.0, next_state=[0.16816162 0.20812613 0.03127669 0.22724007]\n",
      "[ episode 345 ][ timestamp 76 ] state=[0.16816162 0.20812613 0.03127669 0.22724007], action=1, reward=1.0, next_state=[ 0.17232415  0.40278747  0.03582149 -0.05541528]\n",
      "[ episode 345 ][ timestamp 77 ] state=[ 0.17232415  0.40278747  0.03582149 -0.05541528], action=0, reward=1.0, next_state=[0.1803799  0.20717069 0.03471319 0.24835092]\n",
      "[ episode 345 ][ timestamp 78 ] state=[0.1803799  0.20717069 0.03471319 0.24835092], action=1, reward=1.0, next_state=[ 0.18452331  0.40178014  0.03968021 -0.03318377]\n",
      "[ episode 345 ][ timestamp 79 ] state=[ 0.18452331  0.40178014  0.03968021 -0.03318377], action=0, reward=1.0, next_state=[0.19255891 0.20611228 0.03901653 0.2717499 ]\n",
      "[ episode 345 ][ timestamp 80 ] state=[0.19255891 0.20611228 0.03901653 0.2717499 ], action=1, reward=1.0, next_state=[ 0.19668116  0.4006564   0.04445153 -0.00837624]\n",
      "[ episode 345 ][ timestamp 81 ] state=[ 0.19668116  0.4006564   0.04445153 -0.00837624], action=1, reward=1.0, next_state=[ 0.20469429  0.5951136   0.044284   -0.28670946]\n",
      "[ episode 345 ][ timestamp 82 ] state=[ 0.20469429  0.5951136   0.044284   -0.28670946], action=0, reward=1.0, next_state=[0.21659656 0.399389   0.03854981 0.01960486]\n",
      "[ episode 345 ][ timestamp 83 ] state=[0.21659656 0.399389   0.03854981 0.01960486], action=0, reward=1.0, next_state=[0.22458434 0.20373601 0.03894191 0.32419714]\n",
      "[ episode 345 ][ timestamp 84 ] state=[0.22458434 0.20373601 0.03894191 0.32419714], action=1, reward=1.0, next_state=[0.22865906 0.39828246 0.04542585 0.04404473]\n",
      "[ episode 345 ][ timestamp 85 ] state=[0.22865906 0.39828246 0.04542585 0.04404473], action=1, reward=1.0, next_state=[ 0.23662471  0.59272458  0.04630675 -0.23396697]\n",
      "[ episode 345 ][ timestamp 86 ] state=[ 0.23662471  0.59272458  0.04630675 -0.23396697], action=0, reward=1.0, next_state=[0.2484792  0.39697264 0.04162741 0.07295551]\n",
      "[ episode 345 ][ timestamp 87 ] state=[0.2484792  0.39697264 0.04162741 0.07295551], action=1, reward=1.0, next_state=[ 0.25641865  0.59147385  0.04308652 -0.20630864]\n",
      "[ episode 345 ][ timestamp 88 ] state=[ 0.25641865  0.59147385  0.04308652 -0.20630864], action=0, reward=1.0, next_state=[0.26824813 0.39576309 0.03896035 0.09964855]\n",
      "[ episode 345 ][ timestamp 89 ] state=[0.26824813 0.39576309 0.03896035 0.09964855], action=1, reward=1.0, next_state=[ 0.27616339  0.59030564  0.04095332 -0.18049238]\n",
      "[ episode 345 ][ timestamp 90 ] state=[ 0.27616339  0.59030564  0.04095332 -0.18049238], action=0, reward=1.0, next_state=[0.2879695  0.39462232 0.03734347 0.1248234 ]\n",
      "[ episode 345 ][ timestamp 91 ] state=[0.2879695  0.39462232 0.03734347 0.1248234 ], action=1, reward=1.0, next_state=[ 0.29586195  0.58918993  0.03983994 -0.15584811]\n",
      "[ episode 345 ][ timestamp 92 ] state=[ 0.29586195  0.58918993  0.03983994 -0.15584811], action=0, reward=1.0, next_state=[0.30764575 0.39352088 0.03672298 0.14913242]\n",
      "[ episode 345 ][ timestamp 93 ] state=[0.30764575 0.39352088 0.03672298 0.14913242], action=1, reward=1.0, next_state=[ 0.31551617  0.58809825  0.03970562 -0.13174273]\n",
      "[ episode 345 ][ timestamp 94 ] state=[ 0.31551617  0.58809825  0.03970562 -0.13174273], action=0, reward=1.0, next_state=[0.32727813 0.39243069 0.03707077 0.17319767]\n",
      "[ episode 345 ][ timestamp 95 ] state=[0.32727813 0.39243069 0.03707077 0.17319767], action=1, reward=1.0, next_state=[ 0.33512674  0.58700301  0.04053472 -0.10756398]\n",
      "[ episode 345 ][ timestamp 96 ] state=[ 0.33512674  0.58700301  0.04053472 -0.10756398], action=0, reward=1.0, next_state=[0.3468668  0.39132433 0.03838344 0.19762687]\n",
      "[ episode 345 ][ timestamp 97 ] state=[0.3468668  0.39132433 0.03838344 0.19762687], action=1, reward=1.0, next_state=[ 0.35469329  0.58587685  0.04233598 -0.08270499]\n",
      "[ episode 345 ][ timestamp 98 ] state=[ 0.35469329  0.58587685  0.04233598 -0.08270499], action=0, reward=1.0, next_state=[0.36641083 0.3901744  0.04068188 0.22302871]\n",
      "[ episode 345 ][ timestamp 99 ] state=[0.36641083 0.3901744  0.04068188 0.22302871], action=1, reward=1.0, next_state=[ 0.37421432  0.58469199  0.04514246 -0.05654908]\n",
      "[ episode 345 ][ timestamp 100 ] state=[ 0.37421432  0.58469199  0.04514246 -0.05654908], action=0, reward=1.0, next_state=[0.38590816 0.38895281 0.04401147 0.25002794]\n",
      "[ episode 345 ][ timestamp 101 ] state=[0.38590816 0.38895281 0.04401147 0.25002794], action=1, reward=1.0, next_state=[ 0.39368721  0.58341953  0.04901203 -0.02845447]\n",
      "[ episode 345 ][ timestamp 102 ] state=[ 0.39368721  0.58341953  0.04901203 -0.02845447], action=1, reward=1.0, next_state=[ 0.4053556   0.7778056   0.04844294 -0.30527966]\n",
      "[ episode 345 ][ timestamp 103 ] state=[ 0.4053556   0.7778056   0.04844294 -0.30527966], action=0, reward=1.0, next_state=[0.42091171 0.58202799 0.04233735 0.00227891]\n",
      "[ episode 345 ][ timestamp 104 ] state=[0.42091171 0.58202799 0.04233735 0.00227891], action=0, reward=1.0, next_state=[0.43255227 0.38632523 0.04238293 0.30801345]\n",
      "[ episode 345 ][ timestamp 105 ] state=[0.43255227 0.38632523 0.04238293 0.30801345], action=0, reward=1.0, next_state=[0.44027878 0.1906258  0.0485432  0.61375583]\n",
      "[ episode 345 ][ timestamp 106 ] state=[0.44027878 0.1906258  0.0485432  0.61375583], action=1, reward=1.0, next_state=[0.4440913  0.38503701 0.06081831 0.33674864]\n",
      "[ episode 345 ][ timestamp 107 ] state=[0.4440913  0.38503701 0.06081831 0.33674864], action=1, reward=1.0, next_state=[0.45179204 0.57924311 0.06755329 0.06384764]\n",
      "[ episode 345 ][ timestamp 108 ] state=[0.45179204 0.57924311 0.06755329 0.06384764], action=1, reward=1.0, next_state=[ 0.4633769   0.77333475  0.06883024 -0.20678022]\n",
      "[ episode 345 ][ timestamp 109 ] state=[ 0.4633769   0.77333475  0.06883024 -0.20678022], action=0, reward=1.0, next_state=[0.47884359 0.57729951 0.06469464 0.10679649]\n",
      "[ episode 345 ][ timestamp 110 ] state=[0.47884359 0.57729951 0.06469464 0.10679649], action=1, reward=1.0, next_state=[ 0.49038958  0.77143758  0.06683057 -0.16479446]\n",
      "[ episode 345 ][ timestamp 111 ] state=[ 0.49038958  0.77143758  0.06683057 -0.16479446], action=0, reward=1.0, next_state=[0.50581833 0.57542576 0.06353468 0.14820048]\n",
      "[ episode 345 ][ timestamp 112 ] state=[0.50581833 0.57542576 0.06353468 0.14820048], action=0, reward=1.0, next_state=[0.51732685 0.37945424 0.06649869 0.46023128]\n",
      "[ episode 345 ][ timestamp 113 ] state=[0.51732685 0.37945424 0.06649869 0.46023128], action=1, reward=1.0, next_state=[0.52491593 0.57357633 0.07570331 0.18922794]\n",
      "[ episode 345 ][ timestamp 114 ] state=[0.52491593 0.57357633 0.07570331 0.18922794], action=1, reward=1.0, next_state=[ 0.53638746  0.76753819  0.07948787 -0.07864603]\n",
      "[ episode 345 ][ timestamp 115 ] state=[ 0.53638746  0.76753819  0.07948787 -0.07864603], action=1, reward=1.0, next_state=[ 0.55173822  0.96143599  0.07791495 -0.34522955]\n",
      "[ episode 345 ][ timestamp 116 ] state=[ 0.55173822  0.96143599  0.07791495 -0.34522955], action=0, reward=1.0, next_state=[ 0.57096694  0.76529714  0.07101036 -0.02903003]\n",
      "[ episode 345 ][ timestamp 117 ] state=[ 0.57096694  0.76529714  0.07101036 -0.02903003], action=1, reward=1.0, next_state=[ 0.58627289  0.95933271  0.07042976 -0.29849037]\n",
      "[ episode 345 ][ timestamp 118 ] state=[ 0.58627289  0.95933271  0.07042976 -0.29849037], action=0, reward=1.0, next_state=[0.60545954 0.76328118 0.06445995 0.01554709]\n",
      "[ episode 345 ][ timestamp 119 ] state=[0.60545954 0.76328118 0.06445995 0.01554709], action=1, reward=1.0, next_state=[ 0.62072517  0.95742231  0.06477089 -0.25612171]\n",
      "[ episode 345 ][ timestamp 120 ] state=[ 0.62072517  0.95742231  0.06477089 -0.25612171], action=0, reward=1.0, next_state=[0.63987361 0.76143827 0.05964846 0.05626725]\n",
      "[ episode 345 ][ timestamp 121 ] state=[0.63987361 0.76143827 0.05964846 0.05626725], action=1, reward=1.0, next_state=[ 0.65510238  0.95565648  0.0607738  -0.21701571]\n",
      "[ episode 345 ][ timestamp 122 ] state=[ 0.65510238  0.95565648  0.0607738  -0.21701571], action=0, reward=1.0, next_state=[0.67421551 0.75972077 0.05643349 0.09420177]\n",
      "[ episode 345 ][ timestamp 123 ] state=[0.67421551 0.75972077 0.05643349 0.09420177], action=1, reward=1.0, next_state=[ 0.68940992  0.95399036  0.05831752 -0.18015608]\n",
      "[ episode 345 ][ timestamp 124 ] state=[ 0.68940992  0.95399036  0.05831752 -0.18015608], action=1, reward=1.0, next_state=[ 0.70848973  1.14823142  0.0547144  -0.45388671]\n",
      "[ episode 345 ][ timestamp 125 ] state=[ 0.70848973  1.14823142  0.0547144  -0.45388671], action=0, reward=1.0, next_state=[ 0.73145436  0.95238019  0.04563667 -0.1444715 ]\n",
      "[ episode 345 ][ timestamp 126 ] state=[ 0.73145436  0.95238019  0.04563667 -0.1444715 ], action=1, reward=1.0, next_state=[ 0.75050196  1.14681988  0.04274724 -0.42241483]\n",
      "[ episode 345 ][ timestamp 127 ] state=[ 0.75050196  1.14681988  0.04274724 -0.42241483], action=0, reward=1.0, next_state=[ 0.77343836  0.9511192   0.03429894 -0.11656812]\n",
      "[ episode 345 ][ timestamp 128 ] state=[ 0.77343836  0.9511192   0.03429894 -0.11656812], action=1, reward=1.0, next_state=[ 0.79246074  1.14573335  0.03196758 -0.39823574]\n",
      "[ episode 345 ][ timestamp 129 ] state=[ 0.79246074  1.14573335  0.03196758 -0.39823574], action=0, reward=1.0, next_state=[ 0.81537541  0.95017281  0.02400286 -0.09564794]\n",
      "[ episode 345 ][ timestamp 130 ] state=[ 0.81537541  0.95017281  0.02400286 -0.09564794], action=0, reward=1.0, next_state=[0.83437887 0.75471521 0.02208991 0.20451017]\n",
      "[ episode 345 ][ timestamp 131 ] state=[0.83437887 0.75471521 0.02208991 0.20451017], action=1, reward=1.0, next_state=[ 0.84947317  0.94951441  0.02618011 -0.08112343]\n",
      "[ episode 345 ][ timestamp 132 ] state=[ 0.84947317  0.94951441  0.02618011 -0.08112343], action=1, reward=1.0, next_state=[ 0.86846346  1.14425148  0.02455764 -0.36543286]\n",
      "[ episode 345 ][ timestamp 133 ] state=[ 0.86846346  1.14425148  0.02455764 -0.36543286], action=0, reward=1.0, next_state=[ 0.89134849  0.9487893   0.01724898 -0.06510877]\n",
      "[ episode 345 ][ timestamp 134 ] state=[ 0.89134849  0.9487893   0.01724898 -0.06510877], action=1, reward=1.0, next_state=[ 0.91032427  1.14365975  0.01594681 -0.35230002]\n",
      "[ episode 345 ][ timestamp 135 ] state=[ 0.91032427  1.14365975  0.01594681 -0.35230002], action=0, reward=1.0, next_state=[ 0.93319747  0.9483147   0.00890081 -0.05463154]\n",
      "[ episode 345 ][ timestamp 136 ] state=[ 0.93319747  0.9483147   0.00890081 -0.05463154], action=1, reward=1.0, next_state=[ 0.95216376  1.14330791  0.00780818 -0.34449296]\n",
      "[ episode 345 ][ timestamp 137 ] state=[ 0.95216376  1.14330791  0.00780818 -0.34449296], action=0, reward=1.0, next_state=[ 9.75029922e-01  9.48075755e-01  9.18317776e-04 -4.93580777e-02]\n",
      "[ episode 345 ][ timestamp 138 ] state=[ 9.75029922e-01  9.48075755e-01  9.18317776e-04 -4.93580777e-02], action=1, reward=1.0, next_state=[ 9.93991437e-01  1.14318453e+00 -6.88437785e-05 -3.41751126e-01]\n",
      "[ episode 345 ][ timestamp 139 ] state=[ 9.93991437e-01  1.14318453e+00 -6.88437785e-05 -3.41751126e-01], action=0, reward=1.0, next_state=[ 1.01685513  0.94806355 -0.00690387 -0.04908991]\n",
      "[ episode 345 ][ timestamp 140 ] state=[ 1.01685513  0.94806355 -0.00690387 -0.04908991], action=1, reward=1.0, next_state=[ 1.0358164   1.14328382 -0.00788566 -0.34394304]\n",
      "[ episode 345 ][ timestamp 141 ] state=[ 1.0358164   1.14328382 -0.00788566 -0.34394304], action=0, reward=1.0, next_state=[ 1.05868207  0.94827493 -0.01476453 -0.05375717]\n",
      "[ episode 345 ][ timestamp 142 ] state=[ 1.05868207  0.94827493 -0.01476453 -0.05375717], action=1, reward=1.0, next_state=[ 1.07764757  1.14360544 -0.01583967 -0.35106161]\n",
      "[ episode 345 ][ timestamp 143 ] state=[ 1.07764757  1.14360544 -0.01583967 -0.35106161], action=0, reward=1.0, next_state=[ 1.10051968  0.94871229 -0.0228609  -0.06341522]\n",
      "[ episode 345 ][ timestamp 144 ] state=[ 1.10051968  0.94871229 -0.0228609  -0.06341522], action=1, reward=1.0, next_state=[ 1.11949393  1.14415442 -0.02412921 -0.36322234]\n",
      "[ episode 345 ][ timestamp 145 ] state=[ 1.11949393  1.14415442 -0.02412921 -0.36322234], action=0, reward=1.0, next_state=[ 1.14237702  0.94938357 -0.03139365 -0.07824441]\n",
      "[ episode 345 ][ timestamp 146 ] state=[ 1.14237702  0.94938357 -0.03139365 -0.07824441], action=0, reward=1.0, next_state=[ 1.16136469  0.7547254  -0.03295854  0.20437075]\n",
      "[ episode 345 ][ timestamp 147 ] state=[ 1.16136469  0.7547254  -0.03295854  0.20437075], action=1, reward=1.0, next_state=[ 1.1764592   0.9503028  -0.02887113 -0.09852408]\n",
      "[ episode 345 ][ timestamp 148 ] state=[ 1.1764592   0.9503028  -0.02887113 -0.09852408], action=1, reward=1.0, next_state=[ 1.19546525  1.14582638 -0.03084161 -0.40017416]\n",
      "[ episode 345 ][ timestamp 149 ] state=[ 1.19546525  1.14582638 -0.03084161 -0.40017416], action=0, reward=1.0, next_state=[ 1.21838178  0.95115519 -0.03884509 -0.11737224]\n",
      "[ episode 345 ][ timestamp 150 ] state=[ 1.21838178  0.95115519 -0.03884509 -0.11737224], action=1, reward=1.0, next_state=[ 1.23740488  1.14681157 -0.04119254 -0.42205299]\n",
      "[ episode 345 ][ timestamp 151 ] state=[ 1.23740488  1.14681157 -0.04119254 -0.42205299], action=0, reward=1.0, next_state=[ 1.26034111  0.95229669 -0.04963359 -0.14263536]\n",
      "[ episode 345 ][ timestamp 152 ] state=[ 1.26034111  0.95229669 -0.04963359 -0.14263536], action=0, reward=1.0, next_state=[ 1.27938705  0.75791941 -0.0524863   0.13398521]\n",
      "[ episode 345 ][ timestamp 153 ] state=[ 1.27938705  0.75791941 -0.0524863   0.13398521], action=1, reward=1.0, next_state=[ 1.29454544  0.95375235 -0.0498066  -0.17478357]\n",
      "[ episode 345 ][ timestamp 154 ] state=[ 1.29454544  0.95375235 -0.0498066  -0.17478357], action=0, reward=1.0, next_state=[ 1.31362048  0.75937729 -0.05330227  0.10178037]\n",
      "[ episode 345 ][ timestamp 155 ] state=[ 1.31362048  0.75937729 -0.05330227  0.10178037], action=0, reward=1.0, next_state=[ 1.32880803  0.56505814 -0.05126666  0.37718167]\n",
      "[ episode 345 ][ timestamp 156 ] state=[ 1.32880803  0.56505814 -0.05126666  0.37718167], action=1, reward=1.0, next_state=[ 1.34010919  0.76086931 -0.04372303  0.06878503]\n",
      "[ episode 345 ][ timestamp 157 ] state=[ 1.34010919  0.76086931 -0.04372303  0.06878503], action=0, reward=1.0, next_state=[ 1.35532658  0.5664006  -0.04234733  0.34735884]\n",
      "[ episode 345 ][ timestamp 158 ] state=[ 1.35532658  0.5664006  -0.04234733  0.34735884], action=1, reward=1.0, next_state=[ 1.36665459  0.7620985  -0.03540015  0.04162877]\n",
      "[ episode 345 ][ timestamp 159 ] state=[ 1.36665459  0.7620985  -0.03540015  0.04162877], action=1, reward=1.0, next_state=[ 1.38189656  0.95770972 -0.03456758 -0.26200971]\n",
      "[ episode 345 ][ timestamp 160 ] state=[ 1.38189656  0.95770972 -0.03456758 -0.26200971], action=0, reward=1.0, next_state=[ 1.40105075  0.76309782 -0.03980777  0.0195729 ]\n",
      "[ episode 345 ][ timestamp 161 ] state=[ 1.40105075  0.76309782 -0.03980777  0.0195729 ], action=1, reward=1.0, next_state=[ 1.41631271  0.95876738 -0.03941631 -0.28539931]\n",
      "[ episode 345 ][ timestamp 162 ] state=[ 1.41631271  0.95876738 -0.03941631 -0.28539931], action=0, reward=1.0, next_state=[ 1.43548806  0.76422911 -0.0451243  -0.00540394]\n",
      "[ episode 345 ][ timestamp 163 ] state=[ 1.43548806  0.76422911 -0.0451243  -0.00540394], action=0, reward=1.0, next_state=[ 1.45077264  0.56978237 -0.04523238  0.27270722]\n",
      "[ episode 345 ][ timestamp 164 ] state=[ 1.45077264  0.56978237 -0.04523238  0.27270722], action=1, reward=1.0, next_state=[ 1.46216829  0.76551958 -0.03977823 -0.03389207]\n",
      "[ episode 345 ][ timestamp 165 ] state=[ 1.46216829  0.76551958 -0.03977823 -0.03389207], action=0, reward=1.0, next_state=[ 1.47747868  0.57098997 -0.04045607  0.2459798 ]\n",
      "[ episode 345 ][ timestamp 166 ] state=[ 1.47747868  0.57098997 -0.04045607  0.2459798 ], action=1, reward=1.0, next_state=[ 1.48889848  0.76666568 -0.03553648 -0.05918444]\n",
      "[ episode 345 ][ timestamp 167 ] state=[ 1.48889848  0.76666568 -0.03553648 -0.05918444], action=0, reward=1.0, next_state=[ 1.50423179  0.57207079 -0.03672017  0.22207807]\n",
      "[ episode 345 ][ timestamp 168 ] state=[ 1.50423179  0.57207079 -0.03672017  0.22207807], action=1, reward=1.0, next_state=[ 1.51567321  0.76769783 -0.03227861 -0.08195798]\n",
      "[ episode 345 ][ timestamp 169 ] state=[ 1.51567321  0.76769783 -0.03227861 -0.08195798], action=0, reward=1.0, next_state=[ 1.53102717  0.57305311 -0.03391776  0.20036876]\n",
      "[ episode 345 ][ timestamp 170 ] state=[ 1.53102717  0.57305311 -0.03391776  0.20036876], action=1, reward=1.0, next_state=[ 1.54248823  0.76864333 -0.02991039 -0.10281774]\n",
      "[ episode 345 ][ timestamp 171 ] state=[ 1.54248823  0.76864333 -0.02991039 -0.10281774], action=0, reward=1.0, next_state=[ 1.55786109  0.57396251 -0.03196674  0.18028052]\n",
      "[ episode 345 ][ timestamp 172 ] state=[ 1.55786109  0.57396251 -0.03196674  0.18028052], action=1, reward=1.0, next_state=[ 1.56934034  0.76952697 -0.02836113 -0.12231292]\n",
      "[ episode 345 ][ timestamp 173 ] state=[ 1.56934034  0.76952697 -0.02836113 -0.12231292], action=0, reward=1.0, next_state=[ 1.58473088  0.57482258 -0.03080739  0.16128916]\n",
      "[ episode 345 ][ timestamp 174 ] state=[ 1.58473088  0.57482258 -0.03080739  0.16128916], action=1, reward=1.0, next_state=[ 1.59622734  0.77037172 -0.02758161 -0.1409513 ]\n",
      "[ episode 345 ][ timestamp 175 ] state=[ 1.59622734  0.77037172 -0.02758161 -0.1409513 ], action=0, reward=1.0, next_state=[ 1.61163477  0.57565543 -0.03040064  0.14290407]\n",
      "[ episode 345 ][ timestamp 176 ] state=[ 1.61163477  0.57565543 -0.03040064  0.14290407], action=1, reward=1.0, next_state=[ 1.62314788  0.77119928 -0.02754255 -0.15921258]\n",
      "[ episode 345 ][ timestamp 177 ] state=[ 1.62314788  0.77119928 -0.02754255 -0.15921258], action=0, reward=1.0, next_state=[ 1.63857186  0.57648225 -0.03072681  0.1246557 ]\n",
      "[ episode 345 ][ timestamp 178 ] state=[ 1.63857186  0.57648225 -0.03072681  0.1246557 ], action=1, reward=1.0, next_state=[ 1.65010151  0.77203062 -0.02823369 -0.17756066]\n",
      "[ episode 345 ][ timestamp 179 ] state=[ 1.65010151  0.77203062 -0.02823369 -0.17756066], action=0, reward=1.0, next_state=[ 1.66554212  0.57732385 -0.0317849   0.10608349]\n",
      "[ episode 345 ][ timestamp 180 ] state=[ 1.66554212  0.57732385 -0.0317849   0.10608349], action=1, reward=1.0, next_state=[ 1.6770886   0.77288654 -0.02966323 -0.19645556]\n",
      "[ episode 345 ][ timestamp 181 ] state=[ 1.6770886   0.77288654 -0.02966323 -0.19645556], action=0, reward=1.0, next_state=[ 1.69254633  0.57820117 -0.03359235  0.08672432]\n",
      "[ episode 345 ][ timestamp 182 ] state=[ 1.69254633  0.57820117 -0.03359235  0.08672432], action=0, reward=1.0, next_state=[ 1.70411035  0.38357643 -0.03185786  0.36862243]\n",
      "[ episode 345 ][ timestamp 183 ] state=[ 1.70411035  0.38357643 -0.03185786  0.36862243], action=1, reward=1.0, next_state=[ 1.71178188  0.57913622 -0.02448541  0.06606696]\n",
      "[ episode 345 ][ timestamp 184 ] state=[ 1.71178188  0.57913622 -0.02448541  0.06606696], action=0, reward=1.0, next_state=[ 1.72336461  0.38437373 -0.02316407  0.35092514]\n",
      "[ episode 345 ][ timestamp 185 ] state=[ 1.72336461  0.38437373 -0.02316407  0.35092514], action=0, reward=1.0, next_state=[ 1.73105208  0.18958873 -0.01614557  0.63621463]\n",
      "[ episode 345 ][ timestamp 186 ] state=[ 1.73105208  0.18958873 -0.01614557  0.63621463], action=1, reward=1.0, next_state=[ 1.73484385  0.38493209 -0.00342128  0.33849119]\n",
      "[ episode 345 ][ timestamp 187 ] state=[ 1.73484385  0.38493209 -0.00342128  0.33849119], action=1, reward=1.0, next_state=[1.7425425  0.58010256 0.00334855 0.04473135]\n",
      "[ episode 345 ][ timestamp 188 ] state=[1.7425425  0.58010256 0.00334855 0.04473135], action=1, reward=1.0, next_state=[ 1.75414455  0.77517633  0.00424317 -0.2468932 ]\n",
      "[ episode 345 ][ timestamp 189 ] state=[ 1.75414455  0.77517633  0.00424317 -0.2468932 ], action=0, reward=1.0, next_state=[ 1.76964807e+00  5.79994036e-01 -6.94689313e-04  4.71250939e-02]\n",
      "[ episode 345 ][ timestamp 190 ] state=[ 1.76964807e+00  5.79994036e-01 -6.94689313e-04  4.71250939e-02], action=1, reward=1.0, next_state=[ 1.78124795e+00  7.75125942e-01  2.47812566e-04 -2.45776933e-01]\n",
      "[ episode 345 ][ timestamp 191 ] state=[ 1.78124795e+00  7.75125942e-01  2.47812566e-04 -2.45776933e-01], action=0, reward=1.0, next_state=[ 1.79675047  0.58000045 -0.00466773  0.04698415]\n",
      "[ episode 345 ][ timestamp 192 ] state=[ 1.79675047  0.58000045 -0.00466773  0.04698415], action=0, reward=1.0, next_state=[ 1.80835048  0.38494574 -0.00372804  0.33819072]\n",
      "[ episode 345 ][ timestamp 193 ] state=[ 1.80835048  0.38494574 -0.00372804  0.33819072], action=1, reward=1.0, next_state=[1.8160494  0.58012054 0.00303577 0.04433451]\n",
      "[ episode 345 ][ timestamp 194 ] state=[1.8160494  0.58012054 0.00303577 0.04433451], action=1, reward=1.0, next_state=[ 1.82765181  0.77519883  0.00392246 -0.24738906]\n",
      "[ episode 345 ][ timestamp 195 ] state=[ 1.82765181  0.77519883  0.00392246 -0.24738906], action=0, reward=1.0, next_state=[ 1.84315579e+00  5.80021082e-01 -1.02531968e-03  4.65285151e-02]\n",
      "[ episode 345 ][ timestamp 196 ] state=[ 1.84315579e+00  5.80021082e-01 -1.02531968e-03  4.65285151e-02], action=1, reward=1.0, next_state=[ 1.85475621e+00  7.75157721e-01 -9.47493810e-05 -2.46477733e-01]\n",
      "[ episode 345 ][ timestamp 197 ] state=[ 1.85475621e+00  7.75157721e-01 -9.47493810e-05 -2.46477733e-01], action=0, reward=1.0, next_state=[ 1.87025936  0.58003712 -0.0050243   0.04617531]\n",
      "[ episode 345 ][ timestamp 198 ] state=[ 1.87025936  0.58003712 -0.0050243   0.04617531], action=0, reward=1.0, next_state=[ 1.8818601   0.38498758 -0.0041008   0.33726879]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 345 ][ timestamp 199 ] state=[ 1.8818601   0.38498758 -0.0041008   0.33726879], action=1, reward=1.0, next_state=[1.88955986 0.58016764 0.00264458 0.04329552]\n",
      "[ episode 345 ][ timestamp 200 ] state=[1.88955986 0.58016764 0.00264458 0.04329552], action=1, reward=1.0, next_state=[ 1.90116321  0.77525157  0.00351049 -0.24855184]\n",
      "[ episode 345 ][ timestamp 201 ] state=[ 1.90116321  0.77525157  0.00351049 -0.24855184], action=0, reward=1.0, next_state=[ 1.91666824e+00  5.80079664e-01 -1.46054835e-03  4.52362999e-02]\n",
      "[ episode 345 ][ timestamp 202 ] state=[ 1.91666824e+00  5.80079664e-01 -1.46054835e-03  4.52362999e-02], action=1, reward=1.0, next_state=[ 1.92826983e+00  7.75222528e-01 -5.55822353e-04 -2.47907085e-01]\n",
      "[ episode 345 ][ timestamp 203 ] state=[ 1.92826983e+00  7.75222528e-01 -5.55822353e-04 -2.47907085e-01], action=0, reward=1.0, next_state=[ 1.94377428  0.58010852 -0.00551396  0.04460047]\n",
      "[ episode 345 ][ timestamp 204 ] state=[ 1.94377428  0.58010852 -0.00551396  0.04460047], action=0, reward=1.0, next_state=[ 1.95537645  0.38506607 -0.00462195  0.3355386 ]\n",
      "[ episode 345 ][ timestamp 205 ] state=[ 1.95537645  0.38506607 -0.00462195  0.3355386 ], action=0, reward=1.0, next_state=[1.96307778 0.1900102  0.00208882 0.62676043]\n",
      "[ episode 345 ][ timestamp 206 ] state=[1.96307778 0.1900102  0.00208882 0.62676043], action=1, reward=1.0, next_state=[1.96687798 0.38510293 0.01462403 0.33473608]\n",
      "[ episode 345 ][ timestamp 207 ] state=[1.96687798 0.38510293 0.01462403 0.33473608], action=1, reward=1.0, next_state=[1.97458004 0.58001373 0.02131875 0.04670046]\n",
      "[ episode 345 ][ timestamp 208 ] state=[1.97458004 0.58001373 0.02131875 0.04670046], action=1, reward=1.0, next_state=[ 1.98618031  0.7748236   0.02225276 -0.23918071]\n",
      "[ episode 345 ][ timestamp 209 ] state=[ 1.98618031  0.7748236   0.02225276 -0.23918071], action=0, reward=1.0, next_state=[2.00167678 0.57939094 0.01746914 0.06043747]\n",
      "[ episode 345 ][ timestamp 210 ] state=[2.00167678 0.57939094 0.01746914 0.06043747], action=1, reward=1.0, next_state=[ 2.0132646   0.77425812  0.01867789 -0.22668303]\n",
      "[ episode 345 ][ timestamp 211 ] state=[ 2.0132646   0.77425812  0.01867789 -0.22668303], action=0, reward=1.0, next_state=[2.02874977 0.57887429 0.01414423 0.07183258]\n",
      "[ episode 345 ][ timestamp 212 ] state=[2.02874977 0.57887429 0.01414423 0.07183258], action=0, reward=1.0, next_state=[2.04032725 0.38355244 0.01558088 0.36894431]\n",
      "[ episode 345 ][ timestamp 213 ] state=[2.04032725 0.38355244 0.01558088 0.36894431], action=0, reward=1.0, next_state=[2.0479983  0.18821261 0.02295977 0.66649908]\n",
      "[ episode 345 ][ timestamp 214 ] state=[2.0479983  0.18821261 0.02295977 0.66649908], action=1, reward=1.0, next_state=[2.05176255 0.38300784 0.03628975 0.38113283]\n",
      "[ episode 345 ][ timestamp 215 ] state=[2.05176255 0.38300784 0.03628975 0.38113283], action=1, reward=1.0, next_state=[2.05942271 0.5775962  0.04391241 0.10010931]\n",
      "[ episode 345 ][ timestamp 216 ] state=[2.05942271 0.5775962  0.04391241 0.10010931], action=1, reward=1.0, next_state=[ 2.07097463  0.7720622   0.04591459 -0.1784024 ]\n",
      "[ episode 345 ][ timestamp 217 ] state=[ 2.07097463  0.7720622   0.04591459 -0.1784024 ], action=0, reward=1.0, next_state=[2.08641588 0.57631429 0.04234655 0.12840417]\n",
      "[ episode 345 ][ timestamp 218 ] state=[2.08641588 0.57631429 0.04234655 0.12840417], action=1, reward=1.0, next_state=[ 2.09794216  0.77080483  0.04491463 -0.15062394]\n",
      "[ episode 345 ][ timestamp 219 ] state=[ 2.09794216  0.77080483  0.04491463 -0.15062394], action=0, reward=1.0, next_state=[2.11335826 0.57506947 0.04190215 0.15588346]\n",
      "[ episode 345 ][ timestamp 220 ] state=[2.11335826 0.57506947 0.04190215 0.15588346], action=1, reward=1.0, next_state=[ 2.12485965  0.76956721  0.04501982 -0.12329143]\n",
      "[ episode 345 ][ timestamp 221 ] state=[ 2.12485965  0.76956721  0.04501982 -0.12329143], action=0, reward=1.0, next_state=[2.14025099 0.57383016 0.04255399 0.18324802]\n",
      "[ episode 345 ][ timestamp 222 ] state=[2.14025099 0.57383016 0.04255399 0.18324802], action=1, reward=1.0, next_state=[ 2.1517276   0.7683182   0.04621895 -0.09571284]\n",
      "[ episode 345 ][ timestamp 223 ] state=[ 2.1517276   0.7683182   0.04621895 -0.09571284], action=1, reward=1.0, next_state=[ 2.16709396  0.96274829  0.04430469 -0.373463  ]\n",
      "[ episode 345 ][ timestamp 224 ] state=[ 2.16709396  0.96274829  0.04430469 -0.373463  ], action=0, reward=1.0, next_state=[ 2.18634893  0.76702589  0.03683543 -0.06714618]\n",
      "[ episode 345 ][ timestamp 225 ] state=[ 2.18634893  0.76702589  0.03683543 -0.06714618], action=1, reward=1.0, next_state=[ 2.20168944  0.9616009   0.03549251 -0.34798354]\n",
      "[ episode 345 ][ timestamp 226 ] state=[ 2.20168944  0.9616009   0.03549251 -0.34798354], action=0, reward=1.0, next_state=[ 2.22092146  0.76599258  0.02853284 -0.04432323]\n",
      "[ episode 345 ][ timestamp 227 ] state=[ 2.22092146  0.76599258  0.02853284 -0.04432323], action=1, reward=1.0, next_state=[ 2.23624131  0.96069401  0.02764638 -0.32786898]\n",
      "[ episode 345 ][ timestamp 228 ] state=[ 2.23624131  0.96069401  0.02764638 -0.32786898], action=0, reward=1.0, next_state=[ 2.25545519  0.7651896   0.021089   -0.02659744]\n",
      "[ episode 345 ][ timestamp 229 ] state=[ 2.25545519  0.7651896   0.021089   -0.02659744], action=1, reward=1.0, next_state=[ 2.27075899  0.96000287  0.02055705 -0.31255266]\n",
      "[ episode 345 ][ timestamp 230 ] state=[ 2.27075899  0.96000287  0.02055705 -0.31255266], action=0, reward=1.0, next_state=[ 2.28995904  0.76459418  0.01430599 -0.01345821]\n",
      "[ episode 345 ][ timestamp 231 ] state=[ 2.28995904  0.76459418  0.01430599 -0.01345821], action=1, reward=1.0, next_state=[ 2.30525093  0.95950808  0.01403683 -0.30159332]\n",
      "[ episode 345 ][ timestamp 232 ] state=[ 2.30525093  0.95950808  0.01403683 -0.30159332], action=0, reward=1.0, next_state=[ 2.32444109  0.76418891  0.00800496 -0.00451673]\n",
      "[ episode 345 ][ timestamp 233 ] state=[ 2.32444109  0.76418891  0.00800496 -0.00451673], action=1, reward=1.0, next_state=[ 2.33972487  0.95919514  0.00791463 -0.29466328]\n",
      "[ episode 345 ][ timestamp 234 ] state=[ 2.33972487  0.95919514  0.00791463 -0.29466328], action=0, reward=1.0, next_state=[2.35890877e+00 7.63961256e-01 2.02136318e-03 5.05255154e-04]\n",
      "[ episode 345 ][ timestamp 235 ] state=[2.35890877e+00 7.63961256e-01 2.02136318e-03 5.05255154e-04], action=1, reward=1.0, next_state=[ 2.37418799e+00  9.59054160e-01  2.03146829e-03 -2.91539222e-01]\n",
      "[ episode 345 ][ timestamp 236 ] state=[ 2.37418799e+00  9.59054160e-01  2.03146829e-03 -2.91539222e-01], action=0, reward=1.0, next_state=[ 2.39336908e+00  7.63903302e-01 -3.79931616e-03  1.78371210e-03]\n",
      "[ episode 345 ][ timestamp 237 ] state=[ 2.39336908e+00  7.63903302e-01 -3.79931616e-03  1.78371210e-03], action=1, reward=-1.0, next_state=[ 2.40864714  0.95907953 -0.00376364 -0.29209552]\n",
      "[ Ended! ] Episode 345: Exploration_rate=0.17829525136613786. Score=237.\n",
      "[ Experience replay ] starts\n",
      "[ episode 346 ] state=[-0.03365501  0.04724422  0.03509834  0.02636796]\n",
      "[ episode 346 ][ timestamp 1 ] state=[-0.03365501  0.04724422  0.03509834  0.02636796], action=0, reward=1.0, next_state=[-0.03271012 -0.14836303  0.0356257   0.32991492]\n",
      "[ episode 346 ][ timestamp 2 ] state=[-0.03271012 -0.14836303  0.0356257   0.32991492], action=0, reward=1.0, next_state=[-0.03567738 -0.34397354  0.042224    0.63361623]\n",
      "[ episode 346 ][ timestamp 3 ] state=[-0.03567738 -0.34397354  0.042224    0.63361623], action=0, reward=1.0, next_state=[-0.04255685 -0.53965828  0.05489632  0.93929189]\n",
      "[ episode 346 ][ timestamp 4 ] state=[-0.04255685 -0.53965828  0.05489632  0.93929189], action=1, reward=1.0, next_state=[-0.05335002 -0.34531762  0.07368216  0.66435146]\n",
      "[ episode 346 ][ timestamp 5 ] state=[-0.05335002 -0.34531762  0.07368216  0.66435146], action=1, reward=1.0, next_state=[-0.06025637 -0.1512938   0.08696919  0.39574835]\n",
      "[ episode 346 ][ timestamp 6 ] state=[-0.06025637 -0.1512938   0.08696919  0.39574835], action=0, reward=1.0, next_state=[-0.06328225 -0.34753513  0.09488416  0.71453455]\n",
      "[ episode 346 ][ timestamp 7 ] state=[-0.06328225 -0.34753513  0.09488416  0.71453455], action=1, reward=1.0, next_state=[-0.07023295 -0.15384582  0.10917485  0.45316155]\n",
      "[ episode 346 ][ timestamp 8 ] state=[-0.07023295 -0.15384582  0.10917485  0.45316155], action=0, reward=1.0, next_state=[-0.07330987 -0.35032858  0.11823808  0.77816468]\n",
      "[ episode 346 ][ timestamp 9 ] state=[-0.07330987 -0.35032858  0.11823808  0.77816468], action=1, reward=1.0, next_state=[-0.08031644 -0.15701372  0.13380137  0.52489802]\n",
      "[ episode 346 ][ timestamp 10 ] state=[-0.08031644 -0.15701372  0.13380137  0.52489802], action=1, reward=1.0, next_state=[-0.08345671  0.03599667  0.14429933  0.27719047]\n",
      "[ episode 346 ][ timestamp 11 ] state=[-0.08345671  0.03599667  0.14429933  0.27719047], action=0, reward=1.0, next_state=[-0.08273678 -0.16085751  0.14984314  0.61167979]\n",
      "[ episode 346 ][ timestamp 12 ] state=[-0.08273678 -0.16085751  0.14984314  0.61167979], action=1, reward=1.0, next_state=[-0.08595393  0.03188769  0.16207674  0.36969091]\n",
      "[ episode 346 ][ timestamp 13 ] state=[-0.08595393  0.03188769  0.16207674  0.36969091], action=1, reward=1.0, next_state=[-0.08531618  0.22438047  0.16947056  0.13217809]\n",
      "[ episode 346 ][ timestamp 14 ] state=[-0.08531618  0.22438047  0.16947056  0.13217809], action=0, reward=1.0, next_state=[-0.08082857  0.02728745  0.17211412  0.47316854]\n",
      "[ episode 346 ][ timestamp 15 ] state=[-0.08082857  0.02728745  0.17211412  0.47316854], action=1, reward=1.0, next_state=[-0.08028282  0.21961412  0.18157749  0.23929309]\n",
      "[ episode 346 ][ timestamp 16 ] state=[-0.08028282  0.21961412  0.18157749  0.23929309], action=0, reward=1.0, next_state=[-0.07589054  0.02242554  0.18636335  0.58330423]\n",
      "[ episode 346 ][ timestamp 17 ] state=[-0.07589054  0.02242554  0.18636335  0.58330423], action=1, reward=1.0, next_state=[-0.07544202  0.21451519  0.19802944  0.35463314]\n",
      "[ episode 346 ][ timestamp 18 ] state=[-0.07544202  0.21451519  0.19802944  0.35463314], action=1, reward=1.0, next_state=[-0.07115172  0.40635161  0.2051221   0.13034321]\n",
      "[ episode 346 ][ timestamp 19 ] state=[-0.07115172  0.40635161  0.2051221   0.13034321], action=0, reward=1.0, next_state=[-0.06302469  0.2089723   0.20772896  0.48008931]\n",
      "[ episode 346 ][ timestamp 20 ] state=[-0.06302469  0.2089723   0.20772896  0.48008931], action=1, reward=-1.0, next_state=[-0.05884524  0.40064991  0.21733075  0.25938802]\n",
      "[ Ended! ] Episode 346: Exploration_rate=0.17740377510930716. Score=20.\n",
      "[ Experience replay ] starts\n",
      "[ episode 347 ] state=[-0.04010403  0.00733398 -0.01989423 -0.03092663]\n",
      "[ episode 347 ][ timestamp 1 ] state=[-0.04010403  0.00733398 -0.01989423 -0.03092663], action=0, reward=1.0, next_state=[-0.03995735 -0.18749711 -0.02051276  0.25541366]\n",
      "[ episode 347 ][ timestamp 2 ] state=[-0.03995735 -0.18749711 -0.02051276  0.25541366], action=1, reward=1.0, next_state=[-0.04370729  0.00791162 -0.01540448 -0.0436681 ]\n",
      "[ episode 347 ][ timestamp 3 ] state=[-0.04370729  0.00791162 -0.01540448 -0.0436681 ], action=0, reward=1.0, next_state=[-0.04354906 -0.18698609 -0.01627785  0.24411504]\n",
      "[ episode 347 ][ timestamp 4 ] state=[-0.04354906 -0.18698609 -0.01627785  0.24411504], action=1, reward=1.0, next_state=[-0.04728878  0.00836454 -0.01139555 -0.05365756]\n",
      "[ episode 347 ][ timestamp 5 ] state=[-0.04728878  0.00836454 -0.01139555 -0.05365756], action=0, reward=1.0, next_state=[-0.04712149 -0.18659218 -0.0124687   0.23540831]\n",
      "[ episode 347 ][ timestamp 6 ] state=[-0.04712149 -0.18659218 -0.0124687   0.23540831], action=1, reward=1.0, next_state=[-0.05085333  0.00870568 -0.00776053 -0.0611814 ]\n",
      "[ episode 347 ][ timestamp 7 ] state=[-0.05085333  0.00870568 -0.00776053 -0.0611814 ], action=0, reward=1.0, next_state=[-0.05067922 -0.18630415 -0.00898416  0.22904296]\n",
      "[ episode 347 ][ timestamp 8 ] state=[-0.05067922 -0.18630415 -0.00898416  0.22904296], action=1, reward=1.0, next_state=[-0.0544053   0.00894503 -0.0044033  -0.0664603 ]\n",
      "[ episode 347 ][ timestamp 9 ] state=[-0.0544053   0.00894503 -0.0044033  -0.0664603 ], action=0, reward=1.0, next_state=[-0.0542264  -0.18611352 -0.00573251  0.22483012]\n",
      "[ episode 347 ][ timestamp 10 ] state=[-0.0542264  -0.18611352 -0.00573251  0.22483012], action=1, reward=1.0, next_state=[-0.05794867  0.00908989 -0.0012359  -0.06965553]\n",
      "[ episode 347 ][ timestamp 11 ] state=[-0.05794867  0.00908989 -0.0012359  -0.06965553], action=0, reward=1.0, next_state=[-0.05776687 -0.18601432 -0.00262901  0.2226372 ]\n",
      "[ episode 347 ][ timestamp 12 ] state=[-0.05776687 -0.18601432 -0.00262901  0.2226372 ], action=1, reward=1.0, next_state=[-0.06148716  0.00914511  0.00182373 -0.07087386]\n",
      "[ episode 347 ][ timestamp 13 ] state=[-0.06148716  0.00914511  0.00182373 -0.07087386], action=0, reward=1.0, next_state=[-0.06130426 -0.18600294  0.00040625  0.22238391]\n",
      "[ episode 347 ][ timestamp 14 ] state=[-0.06130426 -0.18600294  0.00040625  0.22238391], action=1, reward=1.0, next_state=[-0.06502432  0.00911321  0.00485393 -0.07017084]\n",
      "[ episode 347 ][ timestamp 15 ] state=[-0.06502432  0.00911321  0.00485393 -0.07017084], action=0, reward=1.0, next_state=[-0.06484205 -0.186078    0.00345051  0.22403956]\n",
      "[ episode 347 ][ timestamp 16 ] state=[-0.06484205 -0.186078    0.00345051  0.22403956], action=1, reward=1.0, next_state=[-0.06856361  0.00899447  0.00793131 -0.06755294]\n",
      "[ episode 347 ][ timestamp 17 ] state=[-0.06856361  0.00899447  0.00793131 -0.06755294], action=0, reward=1.0, next_state=[-0.06838372 -0.18624029  0.00658025  0.22762176]\n",
      "[ episode 347 ][ timestamp 18 ] state=[-0.06838372 -0.18624029  0.00658025  0.22762176], action=1, reward=1.0, next_state=[-0.07210853  0.00878701  0.01113268 -0.06297827]\n",
      "[ episode 347 ][ timestamp 19 ] state=[-0.07210853  0.00878701  0.01113268 -0.06297827], action=0, reward=1.0, next_state=[-0.07193279 -0.18649278  0.00987312  0.23319619]\n",
      "[ episode 347 ][ timestamp 20 ] state=[-0.07193279 -0.18649278  0.00987312  0.23319619], action=1, reward=1.0, next_state=[-0.07566264  0.00848672  0.01453704 -0.05635615]\n",
      "[ episode 347 ][ timestamp 21 ] state=[-0.07566264  0.00848672  0.01453704 -0.05635615], action=0, reward=1.0, next_state=[-0.07549291 -0.18684062  0.01340992  0.24087764]\n",
      "[ episode 347 ][ timestamp 22 ] state=[-0.07549291 -0.18684062  0.01340992  0.24087764], action=0, reward=1.0, next_state=[-0.07922972 -0.38215153  0.01822747  0.53776007]\n",
      "[ episode 347 ][ timestamp 23 ] state=[-0.07922972 -0.38215153  0.01822747  0.53776007], action=1, reward=1.0, next_state=[-0.08687275 -0.18729053  0.02898267  0.25087569]\n",
      "[ episode 347 ][ timestamp 24 ] state=[-0.08687275 -0.18729053  0.02898267  0.25087569], action=1, reward=1.0, next_state=[-0.09061856  0.00740582  0.03400019 -0.03252646]\n",
      "[ episode 347 ][ timestamp 25 ] state=[-0.09061856  0.00740582  0.03400019 -0.03252646], action=0, reward=1.0, next_state=[-0.09047045 -0.1881868   0.03334966  0.27068702]\n",
      "[ episode 347 ][ timestamp 26 ] state=[-0.09047045 -0.1881868   0.03334966  0.27068702], action=1, reward=1.0, next_state=[-0.09423418  0.00644378  0.0387634  -0.01129352]\n",
      "[ episode 347 ][ timestamp 27 ] state=[-0.09423418  0.00644378  0.0387634  -0.01129352], action=0, reward=1.0, next_state=[-0.09410531 -0.18921204  0.03853753  0.29336332]\n",
      "[ episode 347 ][ timestamp 28 ] state=[-0.09410531 -0.18921204  0.03853753  0.29336332], action=1, reward=1.0, next_state=[-0.09788955  0.00533988  0.04440479  0.01307935]\n",
      "[ episode 347 ][ timestamp 29 ] state=[-0.09788955  0.00533988  0.04440479  0.01307935], action=1, reward=1.0, next_state=[-0.09778275  0.19979781  0.04466638 -0.2652693 ]\n",
      "[ episode 347 ][ timestamp 30 ] state=[-0.09778275  0.19979781  0.04466638 -0.2652693 ], action=1, reward=1.0, next_state=[-0.09378679  0.39425473  0.03936099 -0.54353621]\n",
      "[ episode 347 ][ timestamp 31 ] state=[-0.09378679  0.39425473  0.03936099 -0.54353621], action=0, reward=1.0, next_state=[-0.0859017   0.19860238  0.02849027 -0.23871585]\n",
      "[ episode 347 ][ timestamp 32 ] state=[-0.0859017   0.19860238  0.02849027 -0.23871585], action=0, reward=1.0, next_state=[-0.08192965  0.00308525  0.02371595  0.06281583]\n",
      "[ episode 347 ][ timestamp 33 ] state=[-0.08192965  0.00308525  0.02371595  0.06281583], action=0, reward=1.0, next_state=[-0.08186795 -0.19236856  0.02497227  0.36288594]\n",
      "[ episode 347 ][ timestamp 34 ] state=[-0.08186795 -0.19236856  0.02497227  0.36288594], action=1, reward=1.0, next_state=[-0.08571532  0.00238972  0.03222999  0.07818069]\n",
      "[ episode 347 ][ timestamp 35 ] state=[-0.08571532  0.00238972  0.03222999  0.07818069], action=1, reward=1.0, next_state=[-0.08566752  0.19703517  0.0337936  -0.20416188]\n",
      "[ episode 347 ][ timestamp 36 ] state=[-0.08566752  0.19703517  0.0337936  -0.20416188], action=1, reward=1.0, next_state=[-0.08172682  0.39165796  0.02971036 -0.48599595]\n",
      "[ episode 347 ][ timestamp 37 ] state=[-0.08172682  0.39165796  0.02971036 -0.48599595], action=0, reward=1.0, next_state=[-0.07389366  0.19612964  0.01999044 -0.18409935]\n",
      "[ episode 347 ][ timestamp 38 ] state=[-0.07389366  0.19612964  0.01999044 -0.18409935], action=0, reward=1.0, next_state=[-0.06997107  0.00072744  0.01630846  0.11482218]\n",
      "[ episode 347 ][ timestamp 39 ] state=[-0.06997107  0.00072744  0.01630846  0.11482218], action=1, reward=1.0, next_state=[-0.06995652  0.19561197  0.0186049  -0.17267126]\n",
      "[ episode 347 ][ timestamp 40 ] state=[-0.06995652  0.19561197  0.0186049  -0.17267126], action=0, reward=1.0, next_state=[-0.06604428  0.00022874  0.01515148  0.12582238]\n",
      "[ episode 347 ][ timestamp 41 ] state=[-0.06604428  0.00022874  0.01515148  0.12582238], action=1, reward=1.0, next_state=[-0.06603971  0.19513039  0.01766792 -0.16204218]\n",
      "[ episode 347 ][ timestamp 42 ] state=[-0.06603971  0.19513039  0.01766792 -0.16204218], action=0, reward=1.0, next_state=[-0.0621371  -0.00023997  0.01442708  0.13616174]\n",
      "[ episode 347 ][ timestamp 43 ] state=[-0.0621371  -0.00023997  0.01442708  0.13616174], action=1, reward=1.0, next_state=[-0.0621419   0.19467239  0.01715031 -0.15193497]\n",
      "[ episode 347 ][ timestamp 44 ] state=[-0.0621419   0.19467239  0.01715031 -0.15193497], action=0, reward=1.0, next_state=[-0.05824845 -0.00069088  0.01411162  0.14610878]\n",
      "[ episode 347 ][ timestamp 45 ] state=[-0.05824845 -0.00069088  0.01411162  0.14610878], action=1, reward=1.0, next_state=[-0.05826227  0.19422617  0.01703379 -0.142089  ]\n",
      "[ episode 347 ][ timestamp 46 ] state=[-0.05826227  0.19422617  0.01703379 -0.142089  ], action=0, reward=1.0, next_state=[-0.05437774 -0.00113554  0.01419201  0.15591874]\n",
      "[ episode 347 ][ timestamp 47 ] state=[-0.05437774 -0.00113554  0.01419201  0.15591874], action=1, reward=1.0, next_state=[-0.05440045  0.19378037  0.01731039 -0.13225337]\n",
      "[ episode 347 ][ timestamp 48 ] state=[-0.05440045  0.19378037  0.01731039 -0.13225337], action=0, reward=1.0, next_state=[-0.05052485 -0.00158521  0.01466532  0.1658401 ]\n",
      "[ episode 347 ][ timestamp 49 ] state=[-0.05052485 -0.00158521  0.01466532  0.1658401 ], action=1, reward=1.0, next_state=[-0.05055655  0.19332378  0.01798212 -0.12218049]\n",
      "[ episode 347 ][ timestamp 50 ] state=[-0.05055655  0.19332378  0.01798212 -0.12218049], action=1, reward=1.0, next_state=[-0.04669008  0.38818354  0.01553851 -0.40913643]\n",
      "[ episode 347 ][ timestamp 51 ] state=[-0.04669008  0.38818354  0.01553851 -0.40913643], action=0, reward=1.0, next_state=[-0.0389264   0.19284477  0.00735578 -0.11159551]\n",
      "[ episode 347 ][ timestamp 52 ] state=[-0.0389264   0.19284477  0.00735578 -0.11159551], action=0, reward=1.0, next_state=[-0.03506951 -0.00238181  0.00512387  0.18339902]\n",
      "[ episode 347 ][ timestamp 53 ] state=[-0.03506951 -0.00238181  0.00512387  0.18339902], action=1, reward=1.0, next_state=[-0.03511715  0.19266646  0.00879185 -0.10766312]\n",
      "[ episode 347 ][ timestamp 54 ] state=[-0.03511715  0.19266646  0.00879185 -0.10766312], action=0, reward=1.0, next_state=[-0.03126382 -0.00258037  0.00663859  0.18778057]\n",
      "[ episode 347 ][ timestamp 55 ] state=[-0.03126382 -0.00258037  0.00663859  0.18778057], action=1, reward=1.0, next_state=[-0.03131542  0.19244597  0.0103942  -0.10280077]\n",
      "[ episode 347 ][ timestamp 56 ] state=[-0.03131542  0.19244597  0.0103942  -0.10280077], action=0, reward=1.0, next_state=[-0.0274665  -0.00282338  0.00833819  0.19314329]\n",
      "[ episode 347 ][ timestamp 57 ] state=[-0.0274665  -0.00282338  0.00833819  0.19314329], action=1, reward=1.0, next_state=[-0.02752297  0.1921783   0.01220105 -0.09689768]\n",
      "[ episode 347 ][ timestamp 58 ] state=[-0.02752297  0.1921783   0.01220105 -0.09689768], action=0, reward=1.0, next_state=[-0.02367941 -0.00311637  0.0102631   0.19960955]\n",
      "[ episode 347 ][ timestamp 59 ] state=[-0.02367941 -0.00311637  0.0102631   0.19960955], action=0, reward=1.0, next_state=[-0.02374173 -0.1983836   0.01425529  0.49551226]\n",
      "[ episode 347 ][ timestamp 60 ] state=[-0.02374173 -0.1983836   0.01425529  0.49551226], action=0, reward=1.0, next_state=[-0.02770941 -0.39370365  0.02416553  0.79265348]\n",
      "[ episode 347 ][ timestamp 61 ] state=[-0.02770941 -0.39370365  0.02416553  0.79265348], action=1, reward=1.0, next_state=[-0.03558348 -0.19892164  0.0400186   0.50766975]\n",
      "[ episode 347 ][ timestamp 62 ] state=[-0.03558348 -0.19892164  0.0400186   0.50766975], action=1, reward=1.0, next_state=[-0.03956191 -0.00438572  0.050172    0.22786183]\n",
      "[ episode 347 ][ timestamp 63 ] state=[-0.03956191 -0.00438572  0.050172    0.22786183], action=1, reward=1.0, next_state=[-0.03964963  0.18998466  0.05472924 -0.04858249]\n",
      "[ episode 347 ][ timestamp 64 ] state=[-0.03964963  0.18998466  0.05472924 -0.04858249], action=0, reward=1.0, next_state=[-0.03584993 -0.00587761  0.05375759  0.2608534 ]\n",
      "[ episode 347 ][ timestamp 65 ] state=[-0.03584993 -0.00587761  0.05375759  0.2608534 ], action=1, reward=1.0, next_state=[-0.03596748  0.18843738  0.05897465 -0.01440091]\n",
      "[ episode 347 ][ timestamp 66 ] state=[-0.03596748  0.18843738  0.05897465 -0.01440091], action=1, reward=1.0, next_state=[-0.03219874  0.38266615  0.05868664 -0.28790907]\n",
      "[ episode 347 ][ timestamp 67 ] state=[-0.03219874  0.38266615  0.05868664 -0.28790907], action=0, reward=1.0, next_state=[-0.02454541  0.18675854  0.05292845  0.02269042]\n",
      "[ episode 347 ][ timestamp 68 ] state=[-0.02454541  0.18675854  0.05292845  0.02269042], action=1, reward=1.0, next_state=[-0.02081024  0.38108306  0.05338226 -0.25283447]\n",
      "[ episode 347 ][ timestamp 69 ] state=[-0.02081024  0.38108306  0.05338226 -0.25283447], action=0, reward=1.0, next_state=[-0.01318858  0.18524111  0.04832557  0.05619693]\n",
      "[ episode 347 ][ timestamp 70 ] state=[-0.01318858  0.18524111  0.04832557  0.05619693], action=1, reward=1.0, next_state=[-0.00948376  0.37963804  0.04944951 -0.22085585]\n",
      "[ episode 347 ][ timestamp 71 ] state=[-0.00948376  0.37963804  0.04944951 -0.22085585], action=0, reward=1.0, next_state=[-0.001891    0.18384542  0.04503239  0.08700631]\n",
      "[ episode 347 ][ timestamp 72 ] state=[-0.001891    0.18384542  0.04503239  0.08700631], action=1, reward=1.0, next_state=[ 0.00178591  0.37829391  0.04677252 -0.19113568]\n",
      "[ episode 347 ][ timestamp 73 ] state=[ 0.00178591  0.37829391  0.04677252 -0.19113568], action=0, reward=1.0, next_state=[0.00935179 0.18253513 0.04294981 0.11592746]\n",
      "[ episode 347 ][ timestamp 74 ] state=[0.00935179 0.18253513 0.04294981 0.11592746], action=1, reward=1.0, next_state=[ 0.01300249  0.3770162   0.04526836 -0.16290176]\n",
      "[ episode 347 ][ timestamp 75 ] state=[ 0.01300249  0.3770162   0.04526836 -0.16290176], action=0, reward=1.0, next_state=[0.02054281 0.18127643 0.04201032 0.14371147]\n",
      "[ episode 347 ][ timestamp 76 ] state=[0.02054281 0.18127643 0.04201032 0.14371147], action=1, reward=1.0, next_state=[ 0.02416834  0.37577234  0.04488455 -0.13542759]\n",
      "[ episode 347 ][ timestamp 77 ] state=[ 0.02416834  0.37577234  0.04488455 -0.13542759], action=1, reward=1.0, next_state=[ 0.03168379  0.5702236   0.042176   -0.4136191 ]\n",
      "[ episode 347 ][ timestamp 78 ] state=[ 0.03168379  0.5702236   0.042176   -0.4136191 ], action=0, reward=1.0, next_state=[ 0.04308826  0.37453     0.03390362 -0.10794366]\n",
      "[ episode 347 ][ timestamp 79 ] state=[ 0.04308826  0.37453     0.03390362 -0.10794366], action=0, reward=1.0, next_state=[0.05057886 0.17893902 0.03174474 0.19523996]\n",
      "[ episode 347 ][ timestamp 80 ] state=[0.05057886 0.17893902 0.03174474 0.19523996], action=1, reward=1.0, next_state=[ 0.05415764  0.37359284  0.03564954 -0.08726228]\n",
      "[ episode 347 ][ timestamp 81 ] state=[ 0.05415764  0.37359284  0.03564954 -0.08726228], action=0, reward=1.0, next_state=[0.0616295  0.17797851 0.0339043  0.21645153]\n",
      "[ episode 347 ][ timestamp 82 ] state=[0.0616295  0.17797851 0.0339043  0.21645153], action=1, reward=1.0, next_state=[ 0.06518907  0.37259978  0.03823333 -0.06534665]\n",
      "[ episode 347 ][ timestamp 83 ] state=[ 0.06518907  0.37259978  0.03823333 -0.06534665], action=0, reward=1.0, next_state=[0.07264106 0.17695112 0.03692639 0.23914973]\n",
      "[ episode 347 ][ timestamp 84 ] state=[0.07264106 0.17695112 0.03692639 0.23914973], action=1, reward=1.0, next_state=[ 0.07618009  0.37152663  0.04170939 -0.04166067]\n",
      "[ episode 347 ][ timestamp 85 ] state=[ 0.07618009  0.37152663  0.04170939 -0.04166067], action=0, reward=1.0, next_state=[0.08361062 0.17583216 0.04087618 0.26388473]\n",
      "[ episode 347 ][ timestamp 86 ] state=[0.08361062 0.17583216 0.04087618 0.26388473], action=1, reward=1.0, next_state=[ 0.08712726  0.37034755  0.04615387 -0.01563038]\n",
      "[ episode 347 ][ timestamp 87 ] state=[ 0.08712726  0.37034755  0.04615387 -0.01563038], action=0, reward=1.0, next_state=[0.09453421 0.17459512 0.04584126 0.29125   ]\n",
      "[ episode 347 ][ timestamp 88 ] state=[0.09453421 0.17459512 0.04584126 0.29125   ], action=1, reward=1.0, next_state=[0.09802612 0.36903448 0.05166626 0.01336997]\n",
      "[ episode 347 ][ timestamp 89 ] state=[0.09802612 0.36903448 0.05166626 0.01336997], action=1, reward=1.0, next_state=[ 0.10540681  0.56337886  0.05193366 -0.26257448]\n",
      "[ episode 347 ][ timestamp 90 ] state=[ 0.10540681  0.56337886  0.05193366 -0.26257448], action=0, reward=1.0, next_state=[0.11667438 0.36755555 0.04668217 0.0460261 ]\n",
      "[ episode 347 ][ timestamp 91 ] state=[0.11667438 0.36755555 0.04668217 0.0460261 ], action=1, reward=1.0, next_state=[ 0.12402549  0.5619781   0.04760269 -0.23157044]\n",
      "[ episode 347 ][ timestamp 92 ] state=[ 0.12402549  0.5619781   0.04760269 -0.23157044], action=0, reward=1.0, next_state=[0.13526506 0.36620941 0.04297129 0.07573985]\n",
      "[ episode 347 ][ timestamp 93 ] state=[0.13526506 0.36620941 0.04297129 0.07573985], action=1, reward=1.0, next_state=[ 0.14258924  0.56068983  0.04448608 -0.20308182]\n",
      "[ episode 347 ][ timestamp 94 ] state=[ 0.14258924  0.56068983  0.04448608 -0.20308182], action=0, reward=1.0, next_state=[0.15380304 0.36496084 0.04042445 0.1032958 ]\n",
      "[ episode 347 ][ timestamp 95 ] state=[0.15380304 0.36496084 0.04042445 0.1032958 ], action=0, reward=1.0, next_state=[0.16110226 0.16928358 0.04249036 0.40845345]\n",
      "[ episode 347 ][ timestamp 96 ] state=[0.16110226 0.16928358 0.04249036 0.40845345], action=1, reward=1.0, next_state=[0.16448793 0.36377812 0.05065943 0.12946336]\n",
      "[ episode 347 ][ timestamp 97 ] state=[0.16448793 0.36377812 0.05065943 0.12946336], action=1, reward=1.0, next_state=[ 0.17176349  0.55813915  0.0532487  -0.14681665]\n",
      "[ episode 347 ][ timestamp 98 ] state=[ 0.17176349  0.55813915  0.0532487  -0.14681665], action=0, reward=1.0, next_state=[0.18292627 0.36229669 0.05031237 0.16217839]\n",
      "[ episode 347 ][ timestamp 99 ] state=[0.18292627 0.36229669 0.05031237 0.16217839], action=1, reward=1.0, next_state=[ 0.19017221  0.55666362  0.05355593 -0.11421749]\n",
      "[ episode 347 ][ timestamp 100 ] state=[ 0.19017221  0.55666362  0.05355593 -0.11421749], action=0, reward=1.0, next_state=[0.20130548 0.36081681 0.05127158 0.19486944]\n",
      "[ episode 347 ][ timestamp 101 ] state=[0.20130548 0.36081681 0.05127158 0.19486944], action=1, reward=1.0, next_state=[ 0.20852182  0.55516929  0.05516897 -0.08120894]\n",
      "[ episode 347 ][ timestamp 102 ] state=[ 0.20852182  0.55516929  0.05516897 -0.08120894], action=0, reward=1.0, next_state=[0.2196252  0.35930167 0.05354479 0.22835695]\n",
      "[ episode 347 ][ timestamp 103 ] state=[0.2196252  0.35930167 0.05354479 0.22835695], action=1, reward=1.0, next_state=[ 0.22681124  0.55361918  0.05811193 -0.04696692]\n",
      "[ episode 347 ][ timestamp 104 ] state=[ 0.22681124  0.55361918  0.05811193 -0.04696692], action=0, reward=1.0, next_state=[0.23788362 0.35771417 0.05717259 0.26346985]\n",
      "[ episode 347 ][ timestamp 105 ] state=[0.23788362 0.35771417 0.05717259 0.26346985], action=1, reward=1.0, next_state=[ 0.2450379   0.55197542  0.06244199 -0.01064634]\n",
      "[ episode 347 ][ timestamp 106 ] state=[ 0.2450379   0.55197542  0.06244199 -0.01064634], action=0, reward=1.0, next_state=[0.25607741 0.35601613 0.06222906 0.30106577]\n",
      "[ episode 347 ][ timestamp 107 ] state=[0.25607741 0.35601613 0.06222906 0.30106577], action=1, reward=1.0, next_state=[0.26319773 0.55019847 0.06825038 0.02863958]\n",
      "[ episode 347 ][ timestamp 108 ] state=[0.26319773 0.55019847 0.06825038 0.02863958], action=0, reward=1.0, next_state=[0.2742017  0.35416752 0.06882317 0.34205147]\n",
      "[ episode 347 ][ timestamp 109 ] state=[0.2742017  0.35416752 0.06882317 0.34205147], action=1, reward=1.0, next_state=[0.28128505 0.54824625 0.0756642  0.0718406 ]\n",
      "[ episode 347 ][ timestamp 110 ] state=[0.28128505 0.54824625 0.0756642  0.0718406 ], action=0, reward=1.0, next_state=[0.29224998 0.35212568 0.07710101 0.38740381]\n",
      "[ episode 347 ][ timestamp 111 ] state=[0.29224998 0.35212568 0.07710101 0.38740381], action=0, reward=1.0, next_state=[0.29929249 0.15599878 0.08484909 0.70336542]\n",
      "[ episode 347 ][ timestamp 112 ] state=[0.29929249 0.15599878 0.08484909 0.70336542], action=1, reward=1.0, next_state=[0.30241247 0.34984874 0.0989164  0.43855227]\n",
      "[ episode 347 ][ timestamp 113 ] state=[0.30241247 0.34984874 0.0989164  0.43855227], action=1, reward=1.0, next_state=[0.30940944 0.5434417  0.10768744 0.17861634]\n",
      "[ episode 347 ][ timestamp 114 ] state=[0.30940944 0.5434417  0.10768744 0.17861634], action=0, reward=1.0, next_state=[0.32027828 0.34695669 0.11125977 0.50323554]\n",
      "[ episode 347 ][ timestamp 115 ] state=[0.32027828 0.34695669 0.11125977 0.50323554], action=1, reward=1.0, next_state=[0.32721741 0.54034913 0.12132448 0.24758341]\n",
      "[ episode 347 ][ timestamp 116 ] state=[0.32721741 0.54034913 0.12132448 0.24758341], action=1, reward=1.0, next_state=[ 0.33802439  0.73354831  0.12627615 -0.00450315]\n",
      "[ episode 347 ][ timestamp 117 ] state=[ 0.33802439  0.73354831  0.12627615 -0.00450315], action=0, reward=1.0, next_state=[0.35269536 0.53686283 0.12618609 0.32520258]\n",
      "[ episode 347 ][ timestamp 118 ] state=[0.35269536 0.53686283 0.12618609 0.32520258], action=1, reward=1.0, next_state=[0.36343262 0.72998342 0.13269014 0.07482527]\n",
      "[ episode 347 ][ timestamp 119 ] state=[0.36343262 0.72998342 0.13269014 0.07482527], action=0, reward=1.0, next_state=[0.37803228 0.53323346 0.13418664 0.40625245]\n",
      "[ episode 347 ][ timestamp 120 ] state=[0.37803228 0.53323346 0.13418664 0.40625245], action=1, reward=1.0, next_state=[0.38869695 0.72622279 0.14231169 0.15870336]\n",
      "[ episode 347 ][ timestamp 121 ] state=[0.38869695 0.72622279 0.14231169 0.15870336], action=0, reward=1.0, next_state=[0.40322141 0.52938055 0.14548576 0.49268039]\n",
      "[ episode 347 ][ timestamp 122 ] state=[0.40322141 0.52938055 0.14548576 0.49268039], action=1, reward=1.0, next_state=[0.41380902 0.72218317 0.15533937 0.24915381]\n",
      "[ episode 347 ][ timestamp 123 ] state=[0.41380902 0.72218317 0.15533937 0.24915381], action=1, reward=1.0, next_state=[0.42825268 0.91478503 0.16032244 0.00921601]\n",
      "[ episode 347 ][ timestamp 124 ] state=[0.42825268 0.91478503 0.16032244 0.00921601], action=0, reward=1.0, next_state=[0.44654838 0.7177704  0.16050676 0.34788128]\n",
      "[ episode 347 ][ timestamp 125 ] state=[0.44654838 0.7177704  0.16050676 0.34788128], action=1, reward=1.0, next_state=[0.46090379 0.91028885 0.16746439 0.10980207]\n",
      "[ episode 347 ][ timestamp 126 ] state=[0.46090379 0.91028885 0.16746439 0.10980207], action=1, reward=1.0, next_state=[ 0.47910957  1.10266464  0.16966043 -0.12572006]\n",
      "[ episode 347 ][ timestamp 127 ] state=[ 0.47910957  1.10266464  0.16966043 -0.12572006], action=0, reward=1.0, next_state=[0.50116286 0.90556969 0.16714603 0.21531879]\n",
      "[ episode 347 ][ timestamp 128 ] state=[0.50116286 0.90556969 0.16714603 0.21531879], action=1, reward=1.0, next_state=[ 0.51927426  1.09795681  0.1714524  -0.02032768]\n",
      "[ episode 347 ][ timestamp 129 ] state=[ 0.51927426  1.09795681  0.1714524  -0.02032768], action=0, reward=1.0, next_state=[0.54123339 0.90084379 0.17104585 0.32116716]\n",
      "[ episode 347 ][ timestamp 130 ] state=[0.54123339 0.90084379 0.17104585 0.32116716], action=1, reward=1.0, next_state=[0.55925027 1.09316967 0.17746919 0.0869308 ]\n",
      "[ episode 347 ][ timestamp 131 ] state=[0.55925027 1.09316967 0.17746919 0.0869308 ], action=1, reward=1.0, next_state=[ 0.58111366  1.28536258  0.17920781 -0.14492808]\n",
      "[ episode 347 ][ timestamp 132 ] state=[ 0.58111366  1.28536258  0.17920781 -0.14492808], action=0, reward=1.0, next_state=[0.60682091 1.08818726 0.17630925 0.19850385]\n",
      "[ episode 347 ][ timestamp 133 ] state=[0.60682091 1.08818726 0.17630925 0.19850385], action=1, reward=1.0, next_state=[ 0.62858466  1.2804068   0.18027933 -0.03378892]\n",
      "[ episode 347 ][ timestamp 134 ] state=[ 0.62858466  1.2804068   0.18027933 -0.03378892], action=0, reward=1.0, next_state=[0.65419279 1.08321911 0.17960355 0.30991457]\n",
      "[ episode 347 ][ timestamp 135 ] state=[0.65419279 1.08321911 0.17960355 0.30991457], action=1, reward=1.0, next_state=[0.67585718 1.27538834 0.18580184 0.07881742]\n",
      "[ episode 347 ][ timestamp 136 ] state=[0.67585718 1.27538834 0.18580184 0.07881742], action=1, reward=1.0, next_state=[ 0.70136494  1.4674281   0.18737819 -0.14997229]\n",
      "[ episode 347 ][ timestamp 137 ] state=[ 0.70136494  1.4674281   0.18737819 -0.14997229], action=0, reward=1.0, next_state=[0.73071351 1.27018625 0.18437874 0.1954791 ]\n",
      "[ episode 347 ][ timestamp 138 ] state=[0.73071351 1.27018625 0.18437874 0.1954791 ], action=1, reward=1.0, next_state=[ 0.75611723  1.46225791  0.18828832 -0.03384433]\n",
      "[ episode 347 ][ timestamp 139 ] state=[ 0.75611723  1.46225791  0.18828832 -0.03384433], action=0, reward=1.0, next_state=[0.78536239 1.26500488 0.18761144 0.31183611]\n",
      "[ episode 347 ][ timestamp 140 ] state=[0.78536239 1.26500488 0.18761144 0.31183611], action=1, reward=1.0, next_state=[0.81066249 1.45702782 0.19384816 0.08369073]\n",
      "[ episode 347 ][ timestamp 141 ] state=[0.81066249 1.45702782 0.19384816 0.08369073], action=1, reward=1.0, next_state=[ 0.83980304  1.64891911  0.19552197 -0.14211998]\n",
      "[ episode 347 ][ timestamp 142 ] state=[ 0.83980304  1.64891911  0.19552197 -0.14211998], action=0, reward=1.0, next_state=[0.87278142 1.45161275 0.19267957 0.20531837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 347 ][ timestamp 143 ] state=[0.87278142 1.45161275 0.19267957 0.20531837], action=0, reward=1.0, next_state=[0.90181368 1.25433257 0.19678594 0.55206047]\n",
      "[ episode 347 ][ timestamp 144 ] state=[0.90181368 1.25433257 0.19678594 0.55206047], action=1, reward=1.0, next_state=[0.92690033 1.446226   0.20782715 0.32725801]\n",
      "[ episode 347 ][ timestamp 145 ] state=[0.92690033 1.446226   0.20782715 0.32725801], action=1, reward=-1.0, next_state=[0.95582485 1.63787703 0.21437231 0.1066298 ]\n",
      "[ Ended! ] Episode 347: Exploration_rate=0.17651675623376062. Score=145.\n",
      "[ Experience replay ] starts\n",
      "[ episode 348 ] state=[ 0.02803236 -0.04122525 -0.00141153 -0.04072042]\n",
      "[ episode 348 ][ timestamp 1 ] state=[ 0.02803236 -0.04122525 -0.00141153 -0.04072042], action=1, reward=1.0, next_state=[ 0.02720785  0.15391691 -0.00222594 -0.33384836]\n",
      "[ episode 348 ][ timestamp 2 ] state=[ 0.02720785  0.15391691 -0.00222594 -0.33384836], action=0, reward=1.0, next_state=[ 0.03028619 -0.04117329 -0.00890291 -0.04186822]\n",
      "[ episode 348 ][ timestamp 3 ] state=[ 0.03028619 -0.04117329 -0.00890291 -0.04186822], action=1, reward=1.0, next_state=[ 0.02946272  0.15407519 -0.00974027 -0.33734675]\n",
      "[ episode 348 ][ timestamp 4 ] state=[ 0.02946272  0.15407519 -0.00974027 -0.33734675], action=0, reward=1.0, next_state=[ 0.03254423 -0.04090681 -0.0164872  -0.04775122]\n",
      "[ episode 348 ][ timestamp 5 ] state=[ 0.03254423 -0.04090681 -0.0164872  -0.04775122], action=1, reward=1.0, next_state=[ 0.03172609  0.15444763 -0.01744223 -0.34559006]\n",
      "[ episode 348 ][ timestamp 6 ] state=[ 0.03172609  0.15444763 -0.01744223 -0.34559006], action=0, reward=1.0, next_state=[ 0.03481505 -0.04042192 -0.02435403 -0.05845796]\n",
      "[ episode 348 ][ timestamp 7 ] state=[ 0.03481505 -0.04042192 -0.02435403 -0.05845796], action=1, reward=1.0, next_state=[ 0.03400661  0.1550406  -0.02552319 -0.35872417]\n",
      "[ episode 348 ][ timestamp 8 ] state=[ 0.03400661  0.1550406  -0.02552319 -0.35872417], action=0, reward=1.0, next_state=[ 0.03710742 -0.03970939 -0.03269767 -0.07419733]\n",
      "[ episode 348 ][ timestamp 9 ] state=[ 0.03710742 -0.03970939 -0.03269767 -0.07419733], action=1, reward=1.0, next_state=[ 0.03631323  0.15586569 -0.03418162 -0.37701455]\n",
      "[ episode 348 ][ timestamp 10 ] state=[ 0.03631323  0.15586569 -0.03418162 -0.37701455], action=0, reward=1.0, next_state=[ 0.03943055 -0.03875454 -0.04172191 -0.09530217]\n",
      "[ episode 348 ][ timestamp 11 ] state=[ 0.03943055 -0.03875454 -0.04172191 -0.09530217], action=0, reward=1.0, next_state=[ 0.03865545 -0.23325444 -0.04362795  0.18393111]\n",
      "[ episode 348 ][ timestamp 12 ] state=[ 0.03865545 -0.23325444 -0.04362795  0.18393111], action=1, reward=1.0, next_state=[ 0.03399037 -0.03753627 -0.03994933 -0.12218934]\n",
      "[ episode 348 ][ timestamp 13 ] state=[ 0.03399037 -0.03753627 -0.03994933 -0.12218934], action=0, reward=1.0, next_state=[ 0.03323964 -0.23206378 -0.04239312  0.15762713]\n",
      "[ episode 348 ][ timestamp 14 ] state=[ 0.03323964 -0.23206378 -0.04239312  0.15762713], action=1, reward=1.0, next_state=[ 0.02859836 -0.03636132 -0.03924058 -0.14812265]\n",
      "[ episode 348 ][ timestamp 15 ] state=[ 0.02859836 -0.03636132 -0.03924058 -0.14812265], action=0, reward=1.0, next_state=[ 0.02787114 -0.23090002 -0.04220303  0.13192698]\n",
      "[ episode 348 ][ timestamp 16 ] state=[ 0.02787114 -0.23090002 -0.04220303  0.13192698], action=1, reward=1.0, next_state=[ 0.02325314 -0.03519974 -0.03956449 -0.17376606]\n",
      "[ episode 348 ][ timestamp 17 ] state=[ 0.02325314 -0.03519974 -0.03956449 -0.17376606], action=0, reward=1.0, next_state=[ 0.02254914 -0.22973376 -0.04303981  0.10617769]\n",
      "[ episode 348 ][ timestamp 18 ] state=[ 0.02254914 -0.22973376 -0.04303981  0.10617769], action=1, reward=1.0, next_state=[ 0.01795447 -0.0340223  -0.04091626 -0.19976743]\n",
      "[ episode 348 ][ timestamp 19 ] state=[ 0.01795447 -0.0340223  -0.04091626 -0.19976743], action=0, reward=1.0, next_state=[ 0.01727402 -0.22853589 -0.04491161  0.07973273]\n",
      "[ episode 348 ][ timestamp 20 ] state=[ 0.01727402 -0.22853589 -0.04491161  0.07973273], action=1, reward=1.0, next_state=[ 0.0127033  -0.03279985 -0.04331695 -0.22677484]\n",
      "[ episode 348 ][ timestamp 21 ] state=[ 0.0127033  -0.03279985 -0.04331695 -0.22677484], action=0, reward=1.0, next_state=[ 0.01204731 -0.22727684 -0.04785245  0.0519358 ]\n",
      "[ episode 348 ][ timestamp 22 ] state=[ 0.01204731 -0.22727684 -0.04785245  0.0519358 ], action=1, reward=1.0, next_state=[ 0.00750177 -0.03150257 -0.04681373 -0.25545271]\n",
      "[ episode 348 ][ timestamp 23 ] state=[ 0.00750177 -0.03150257 -0.04681373 -0.25545271], action=0, reward=1.0, next_state=[ 0.00687172 -0.22592594 -0.05192279  0.02210465]\n",
      "[ episode 348 ][ timestamp 24 ] state=[ 0.00687172 -0.22592594 -0.05192279  0.02210465], action=1, reward=1.0, next_state=[ 0.0023532  -0.0300993  -0.05148069 -0.28649789]\n",
      "[ episode 348 ][ timestamp 25 ] state=[ 0.0023532  -0.0300993  -0.05148069 -0.28649789], action=0, reward=1.0, next_state=[ 0.00175121 -0.22445071 -0.05721065 -0.01048564]\n",
      "[ episode 348 ][ timestamp 26 ] state=[ 0.00175121 -0.22445071 -0.05721065 -0.01048564], action=1, reward=1.0, next_state=[-0.0027378  -0.02855694 -0.05742036 -0.32065631]\n",
      "[ episode 348 ][ timestamp 27 ] state=[-0.0027378  -0.02855694 -0.05742036 -0.32065631], action=0, reward=1.0, next_state=[-0.00330894 -0.22281615 -0.06383349 -0.04662004]\n",
      "[ episode 348 ][ timestamp 28 ] state=[-0.00330894 -0.22281615 -0.06383349 -0.04662004], action=0, reward=1.0, next_state=[-0.00776526 -0.41696745 -0.06476589  0.22525947]\n",
      "[ episode 348 ][ timestamp 29 ] state=[-0.00776526 -0.41696745 -0.06476589  0.22525947], action=1, reward=1.0, next_state=[-0.01610461 -0.22098253 -0.0602607  -0.08712943]\n",
      "[ episode 348 ][ timestamp 30 ] state=[-0.01610461 -0.22098253 -0.0602607  -0.08712943], action=0, reward=1.0, next_state=[-0.02052426 -0.41519125 -0.06200329  0.18594895]\n",
      "[ episode 348 ][ timestamp 31 ] state=[-0.02052426 -0.41519125 -0.06200329  0.18594895], action=1, reward=1.0, next_state=[-0.02882809 -0.21923949 -0.05828431 -0.12563115]\n",
      "[ episode 348 ][ timestamp 32 ] state=[-0.02882809 -0.21923949 -0.05828431 -0.12563115], action=0, reward=1.0, next_state=[-0.03321288 -0.41348013 -0.06079693  0.14810916]\n",
      "[ episode 348 ][ timestamp 33 ] state=[-0.03321288 -0.41348013 -0.06079693  0.14810916], action=1, reward=1.0, next_state=[-0.04148248 -0.21754263 -0.05783475 -0.16311736]\n",
      "[ episode 348 ][ timestamp 34 ] state=[-0.04148248 -0.21754263 -0.05783475 -0.16311736], action=0, reward=1.0, next_state=[-0.04583333 -0.41179101 -0.0610971   0.1107741 ]\n",
      "[ episode 348 ][ timestamp 35 ] state=[-0.04583333 -0.41179101 -0.0610971   0.1107741 ], action=1, reward=1.0, next_state=[-0.05406915 -0.2158492  -0.05888162 -0.20054159]\n",
      "[ episode 348 ][ timestamp 36 ] state=[-0.05406915 -0.2158492  -0.05888162 -0.20054159], action=0, reward=1.0, next_state=[-0.05838614 -0.41008174 -0.06289245  0.07300112]\n",
      "[ episode 348 ][ timestamp 37 ] state=[-0.05838614 -0.41008174 -0.06289245  0.07300112], action=1, reward=1.0, next_state=[-0.06658777 -0.21411717 -0.06143243 -0.23884278]\n",
      "[ episode 348 ][ timestamp 38 ] state=[-0.06658777 -0.21411717 -0.06143243 -0.23884278], action=0, reward=1.0, next_state=[-0.07087011 -0.40831016 -0.06620928  0.03384745]\n",
      "[ episode 348 ][ timestamp 39 ] state=[-0.07087011 -0.40831016 -0.06620928  0.03384745], action=1, reward=1.0, next_state=[-0.07903632 -0.2123043  -0.06553233 -0.27896847]\n",
      "[ episode 348 ][ timestamp 40 ] state=[-0.07903632 -0.2123043  -0.06553233 -0.27896847], action=0, reward=1.0, next_state=[-0.0832824  -0.40643317 -0.0711117  -0.00765292]\n",
      "[ episode 348 ][ timestamp 41 ] state=[-0.0832824  -0.40643317 -0.0711117  -0.00765292], action=0, reward=1.0, next_state=[-0.09141107 -0.60046704 -0.07126476  0.26177306]\n",
      "[ episode 348 ][ timestamp 42 ] state=[-0.09141107 -0.60046704 -0.07126476  0.26177306], action=1, reward=1.0, next_state=[-0.10342041 -0.404404   -0.0660293  -0.05250912]\n",
      "[ episode 348 ][ timestamp 43 ] state=[-0.10342041 -0.404404   -0.0660293  -0.05250912], action=1, reward=1.0, next_state=[-0.11150849 -0.20840045 -0.06707948 -0.36527226]\n",
      "[ episode 348 ][ timestamp 44 ] state=[-0.11150849 -0.20840045 -0.06707948 -0.36527226], action=1, reward=1.0, next_state=[-0.1156765  -0.01239253 -0.07438493 -0.6783295 ]\n",
      "[ episode 348 ][ timestamp 45 ] state=[-0.1156765  -0.01239253 -0.07438493 -0.6783295 ], action=0, reward=1.0, next_state=[-0.11592435 -0.20640658 -0.08795152 -0.40996219]\n",
      "[ episode 348 ][ timestamp 46 ] state=[-0.11592435 -0.20640658 -0.08795152 -0.40996219], action=0, reward=1.0, next_state=[-0.12005248 -0.40017868 -0.09615076 -0.14625192]\n",
      "[ episode 348 ][ timestamp 47 ] state=[-0.12005248 -0.40017868 -0.09615076 -0.14625192], action=0, reward=1.0, next_state=[-0.12805605 -0.5938016  -0.0990758   0.11461618]\n",
      "[ episode 348 ][ timestamp 48 ] state=[-0.12805605 -0.5938016  -0.0990758   0.11461618], action=1, reward=1.0, next_state=[-0.13993208 -0.3974099  -0.09678347 -0.20760737]\n",
      "[ episode 348 ][ timestamp 49 ] state=[-0.13993208 -0.3974099  -0.09678347 -0.20760737], action=0, reward=1.0, next_state=[-0.14788028 -0.59102426 -0.10093562  0.0530451 ]\n",
      "[ episode 348 ][ timestamp 50 ] state=[-0.14788028 -0.59102426 -0.10093562  0.0530451 ], action=0, reward=1.0, next_state=[-0.15970077 -0.78456495 -0.09987472  0.31225383]\n",
      "[ episode 348 ][ timestamp 51 ] state=[-0.15970077 -0.78456495 -0.09987472  0.31225383], action=1, reward=1.0, next_state=[-0.17539207 -0.58817251 -0.09362964 -0.01018118]\n",
      "[ episode 348 ][ timestamp 52 ] state=[-0.17539207 -0.58817251 -0.09362964 -0.01018118], action=0, reward=1.0, next_state=[-0.18715552 -0.78183565 -0.09383327  0.25155424]\n",
      "[ episode 348 ][ timestamp 53 ] state=[-0.18715552 -0.78183565 -0.09383327  0.25155424], action=1, reward=1.0, next_state=[-0.20279223 -0.58550778 -0.08880218 -0.06918859]\n",
      "[ episode 348 ][ timestamp 54 ] state=[-0.20279223 -0.58550778 -0.08880218 -0.06918859], action=0, reward=1.0, next_state=[-0.21450238 -0.77925177 -0.09018595  0.19420875]\n",
      "[ episode 348 ][ timestamp 55 ] state=[-0.21450238 -0.77925177 -0.09018595  0.19420875], action=1, reward=1.0, next_state=[-0.23008742 -0.58296324 -0.08630178 -0.12550622]\n",
      "[ episode 348 ][ timestamp 56 ] state=[-0.23008742 -0.58296324 -0.08630178 -0.12550622], action=0, reward=1.0, next_state=[-0.24174669 -0.77674961 -0.0888119   0.13875028]\n",
      "[ episode 348 ][ timestamp 57 ] state=[-0.24174669 -0.77674961 -0.0888119   0.13875028], action=1, reward=1.0, next_state=[-0.25728168 -0.5804753  -0.0860369  -0.18057724]\n",
      "[ episode 348 ][ timestamp 58 ] state=[-0.25728168 -0.5804753  -0.0860369  -0.18057724], action=0, reward=1.0, next_state=[-0.26889118 -0.77426747 -0.08964844  0.08377213]\n",
      "[ episode 348 ][ timestamp 59 ] state=[-0.26889118 -0.77426747 -0.08964844  0.08377213], action=0, reward=1.0, next_state=[-0.28437653 -0.9679976  -0.087973    0.34687902]\n",
      "[ episode 348 ][ timestamp 60 ] state=[-0.28437653 -0.9679976  -0.087973    0.34687902], action=1, reward=1.0, next_state=[-0.30373648 -0.77174165 -0.08103542  0.0278028 ]\n",
      "[ episode 348 ][ timestamp 61 ] state=[-0.30373648 -0.77174165 -0.08103542  0.0278028 ], action=0, reward=1.0, next_state=[-0.31917132 -0.96561363 -0.08047936  0.29385811]\n",
      "[ episode 348 ][ timestamp 62 ] state=[-0.31917132 -0.96561363 -0.08047936  0.29385811], action=1, reward=1.0, next_state=[-0.33848359 -0.769442   -0.0746022  -0.0230823 ]\n",
      "[ episode 348 ][ timestamp 63 ] state=[-0.33848359 -0.769442   -0.0746022  -0.0230823 ], action=0, reward=1.0, next_state=[-0.35387243 -0.96341921 -0.07506385  0.24516149]\n",
      "[ episode 348 ][ timestamp 64 ] state=[-0.35387243 -0.96341921 -0.07506385  0.24516149], action=1, reward=1.0, next_state=[-0.37314081 -0.76730987 -0.07016062 -0.07022221]\n",
      "[ episode 348 ][ timestamp 65 ] state=[-0.37314081 -0.76730987 -0.07016062 -0.07022221], action=0, reward=1.0, next_state=[-0.38848701 -0.96135947 -0.07156506  0.19952578]\n",
      "[ episode 348 ][ timestamp 66 ] state=[-0.38848701 -0.96135947 -0.07156506  0.19952578], action=1, reward=1.0, next_state=[-0.4077142  -0.7652908  -0.06757455 -0.11484659]\n",
      "[ episode 348 ][ timestamp 67 ] state=[-0.4077142  -0.7652908  -0.06757455 -0.11484659], action=0, reward=1.0, next_state=[-0.42302002 -0.9593827  -0.06987148  0.15577499]\n",
      "[ episode 348 ][ timestamp 68 ] state=[-0.42302002 -0.9593827  -0.06987148  0.15577499], action=1, reward=1.0, next_state=[-0.44220767 -0.76333352 -0.06675598 -0.15810673]\n",
      "[ episode 348 ][ timestamp 69 ] state=[-0.44220767 -0.76333352 -0.06675598 -0.15810673], action=0, reward=1.0, next_state=[-0.45747434 -0.95743936 -0.06991811  0.11279183]\n",
      "[ episode 348 ][ timestamp 70 ] state=[-0.45747434 -0.95743936 -0.06991811  0.11279183], action=0, reward=1.0, next_state=[-0.47662313 -1.15149341 -0.06766228  0.38262254]\n",
      "[ episode 348 ][ timestamp 71 ] state=[-0.47662313 -1.15149341 -0.06766228  0.38262254], action=1, reward=1.0, next_state=[-0.499653   -0.95547927 -0.06000983  0.06939657]\n",
      "[ episode 348 ][ timestamp 72 ] state=[-0.499653   -0.95547927 -0.06000983  0.06939657], action=1, reward=1.0, next_state=[-0.51876258 -0.7595506  -0.05862189 -0.24159971]\n",
      "[ episode 348 ][ timestamp 73 ] state=[-0.51876258 -0.7595506  -0.05862189 -0.24159971], action=0, reward=1.0, next_state=[-0.53395359 -0.9537883  -0.06345389  0.03203139]\n",
      "[ episode 348 ][ timestamp 74 ] state=[-0.53395359 -0.9537883  -0.06345389  0.03203139], action=1, reward=1.0, next_state=[-0.55302936 -0.75781649 -0.06281326 -0.27997766]\n",
      "[ episode 348 ][ timestamp 75 ] state=[-0.55302936 -0.75781649 -0.06281326 -0.27997766], action=0, reward=1.0, next_state=[-0.56818569 -0.95198879 -0.06841281 -0.00774855]\n",
      "[ episode 348 ][ timestamp 76 ] state=[-0.56818569 -0.95198879 -0.06841281 -0.00774855], action=0, reward=1.0, next_state=[-0.58722547 -1.14606631 -0.06856779  0.26258904]\n",
      "[ episode 348 ][ timestamp 77 ] state=[-0.58722547 -1.14606631 -0.06856779  0.26258904], action=1, reward=1.0, next_state=[-0.61014679 -0.95003601 -0.063316   -0.05090857]\n",
      "[ episode 348 ][ timestamp 78 ] state=[-0.61014679 -0.95003601 -0.063316   -0.05090857], action=0, reward=1.0, next_state=[-0.62914751 -1.14419563 -0.06433418  0.2211448 ]\n",
      "[ episode 348 ][ timestamp 79 ] state=[-0.62914751 -1.14419563 -0.06433418  0.2211448 ], action=1, reward=1.0, next_state=[-0.65203142 -0.94821591 -0.05991128 -0.09111783]\n",
      "[ episode 348 ][ timestamp 80 ] state=[-0.65203142 -0.94821591 -0.05991128 -0.09111783], action=1, reward=1.0, next_state=[-0.67099574 -0.75228868 -0.06173364 -0.40208478]\n",
      "[ episode 348 ][ timestamp 81 ] state=[-0.67099574 -0.75228868 -0.06173364 -0.40208478], action=0, reward=1.0, next_state=[-0.68604152 -0.94648317 -0.06977533 -0.12948609]\n",
      "[ episode 348 ][ timestamp 82 ] state=[-0.68604152 -0.94648317 -0.06977533 -0.12948609], action=0, reward=1.0, next_state=[-0.70497118 -1.14053981 -0.07236505  0.14039326]\n",
      "[ episode 348 ][ timestamp 83 ] state=[-0.70497118 -1.14053981 -0.07236505  0.14039326], action=1, reward=1.0, next_state=[-0.72778198 -0.94446005 -0.06955719 -0.17421337]\n",
      "[ episode 348 ][ timestamp 84 ] state=[-0.72778198 -0.94446005 -0.06955719 -0.17421337], action=0, reward=1.0, next_state=[-0.74667118 -1.13852113 -0.07304146  0.09574103]\n",
      "[ episode 348 ][ timestamp 85 ] state=[-0.74667118 -1.13852113 -0.07304146  0.09574103], action=1, reward=1.0, next_state=[-0.7694416  -0.94243245 -0.07112664 -0.21906284]\n",
      "[ episode 348 ][ timestamp 86 ] state=[-0.7694416  -0.94243245 -0.07112664 -0.21906284], action=1, reward=1.0, next_state=[-0.78829025 -0.74636965 -0.07550789 -0.53330703]\n",
      "[ episode 348 ][ timestamp 87 ] state=[-0.78829025 -0.74636965 -0.07550789 -0.53330703], action=0, reward=1.0, next_state=[-0.80321764 -0.94035298 -0.08617403 -0.26533937]\n",
      "[ episode 348 ][ timestamp 88 ] state=[-0.80321764 -0.94035298 -0.08617403 -0.26533937], action=0, reward=1.0, next_state=[-8.22024702e-01 -1.13414605e+00 -9.14808197e-02 -1.03223719e-03]\n",
      "[ episode 348 ][ timestamp 89 ] state=[-8.22024702e-01 -1.13414605e+00 -9.14808197e-02 -1.03223719e-03], action=0, reward=1.0, next_state=[-0.84470762 -1.32784506 -0.09150146  0.2614435 ]\n",
      "[ episode 348 ][ timestamp 90 ] state=[-0.84470762 -1.32784506 -0.09150146  0.2614435 ], action=1, reward=1.0, next_state=[-0.87126452 -1.13154418 -0.08627259 -0.05863995]\n",
      "[ episode 348 ][ timestamp 91 ] state=[-0.87126452 -1.13154418 -0.08627259 -0.05863995], action=1, reward=1.0, next_state=[-0.89389541 -0.93529798 -0.08744539 -0.37724712]\n",
      "[ episode 348 ][ timestamp 92 ] state=[-0.89389541 -0.93529798 -0.08744539 -0.37724712], action=0, reward=1.0, next_state=[-0.91260137 -1.1290762  -0.09499034 -0.1133666 ]\n",
      "[ episode 348 ][ timestamp 93 ] state=[-0.91260137 -1.1290762  -0.09499034 -0.1133666 ], action=0, reward=1.0, next_state=[-0.93518289 -1.32271776 -0.09725767  0.14790109]\n",
      "[ episode 348 ][ timestamp 94 ] state=[-0.93518289 -1.32271776 -0.09725767  0.14790109], action=1, reward=1.0, next_state=[-0.96163725 -1.12634732 -0.09429965 -0.17381124]\n",
      "[ episode 348 ][ timestamp 95 ] state=[-0.96163725 -1.12634732 -0.09429965 -0.17381124], action=0, reward=1.0, next_state=[-0.98416419 -1.32000203 -0.09777587  0.0876972 ]\n",
      "[ episode 348 ][ timestamp 96 ] state=[-0.98416419 -1.32000203 -0.09777587  0.0876972 ], action=0, reward=1.0, next_state=[-1.01056423 -1.51359639 -0.09602193  0.34800144]\n",
      "[ episode 348 ][ timestamp 97 ] state=[-1.01056423 -1.51359639 -0.09602193  0.34800144], action=1, reward=1.0, next_state=[-1.04083616 -1.31724918 -0.0890619   0.02665027]\n",
      "[ episode 348 ][ timestamp 98 ] state=[-1.04083616 -1.31724918 -0.0890619   0.02665027], action=0, reward=1.0, next_state=[-1.06718114 -1.51098851 -0.08852889  0.28995787]\n",
      "[ episode 348 ][ timestamp 99 ] state=[-1.06718114 -1.51098851 -0.08852889  0.28995787], action=1, reward=1.0, next_state=[-1.09740091 -1.31472306 -0.08272974 -0.02928092]\n",
      "[ episode 348 ][ timestamp 100 ] state=[-1.09740091 -1.31472306 -0.08272974 -0.02928092], action=0, reward=1.0, next_state=[-1.12369538 -1.50856716 -0.08331535  0.23619597]\n",
      "[ episode 348 ][ timestamp 101 ] state=[-1.12369538 -1.50856716 -0.08331535  0.23619597], action=0, reward=1.0, next_state=[-1.15386672 -1.70240608 -0.07859143  0.50147939]\n",
      "[ episode 348 ][ timestamp 102 ] state=[-1.15386672 -1.70240608 -0.07859143  0.50147939], action=0, reward=1.0, next_state=[-1.18791484 -1.89633734 -0.06856185  0.76839627]\n",
      "[ episode 348 ][ timestamp 103 ] state=[-1.18791484 -1.89633734 -0.06856185  0.76839627], action=1, reward=1.0, next_state=[-1.22584159 -1.70034196 -0.05319392  0.45495253]\n",
      "[ episode 348 ][ timestamp 104 ] state=[-1.22584159 -1.70034196 -0.05319392  0.45495253], action=1, reward=1.0, next_state=[-1.25984843 -1.50450982 -0.04409487  0.14598817]\n",
      "[ episode 348 ][ timestamp 105 ] state=[-1.25984843 -1.50450982 -0.04409487  0.14598817], action=1, reward=1.0, next_state=[-1.28993862 -1.30878504 -0.04117511 -0.16027331]\n",
      "[ episode 348 ][ timestamp 106 ] state=[-1.28993862 -1.30878504 -0.04117511 -0.16027331], action=1, reward=1.0, next_state=[-1.31611432 -1.11309854 -0.04438057 -0.46565634]\n",
      "[ episode 348 ][ timestamp 107 ] state=[-1.31611432 -1.11309854 -0.04438057 -0.46565634], action=1, reward=1.0, next_state=[-1.33837629 -0.91737852 -0.0536937  -0.7719909 ]\n",
      "[ episode 348 ][ timestamp 108 ] state=[-1.33837629 -0.91737852 -0.0536937  -0.7719909 ], action=1, reward=1.0, next_state=[-1.35672387 -0.72156047 -0.06913352 -1.08107303]\n",
      "[ episode 348 ][ timestamp 109 ] state=[-1.35672387 -0.72156047 -0.06913352 -1.08107303], action=1, reward=1.0, next_state=[-1.37115507 -0.52559739 -0.09075498 -1.39462455]\n",
      "[ episode 348 ][ timestamp 110 ] state=[-1.37115507 -0.52559739 -0.09075498 -1.39462455], action=1, reward=1.0, next_state=[-1.38166702 -0.32947086 -0.11864747 -1.71424898]\n",
      "[ episode 348 ][ timestamp 111 ] state=[-1.38166702 -0.32947086 -0.11864747 -1.71424898], action=1, reward=1.0, next_state=[-1.38825644 -0.13320384 -0.15293245 -2.04138036]\n",
      "[ episode 348 ][ timestamp 112 ] state=[-1.38825644 -0.13320384 -0.15293245 -2.04138036], action=0, reward=1.0, next_state=[-1.39092052 -0.32645774 -0.19376006 -1.7996699 ]\n",
      "[ episode 348 ][ timestamp 113 ] state=[-1.39092052 -0.32645774 -0.19376006 -1.7996699 ], action=1, reward=-1.0, next_state=[-1.39744967 -0.12976802 -0.22975345 -2.14579322]\n",
      "[ Ended! ] Episode 348: Exploration_rate=0.1756341724525918. Score=113.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 349 ] state=[-0.01600016  0.0045856  -0.01332611  0.01794386]\n",
      "[ episode 349 ][ timestamp 1 ] state=[-0.01600016  0.0045856  -0.01332611  0.01794386], action=1, reward=1.0, next_state=[-0.01590845  0.1998961  -0.01296723 -0.27891364]\n",
      "[ episode 349 ][ timestamp 2 ] state=[-0.01590845  0.1998961  -0.01296723 -0.27891364], action=0, reward=1.0, next_state=[-0.01191052  0.00496151 -0.0185455   0.0096514 ]\n",
      "[ episode 349 ][ timestamp 3 ] state=[-0.01191052  0.00496151 -0.0185455   0.0096514 ], action=0, reward=1.0, next_state=[-0.01181129 -0.18988963 -0.01835248  0.29642579]\n",
      "[ episode 349 ][ timestamp 4 ] state=[-0.01181129 -0.18988963 -0.01835248  0.29642579], action=1, reward=1.0, next_state=[-0.01560909  0.00548908 -0.01242396 -0.00198824]\n",
      "[ episode 349 ][ timestamp 5 ] state=[-0.01560909  0.00548908 -0.01242396 -0.00198824], action=1, reward=1.0, next_state=[-0.0154993   0.20078698 -0.01246373 -0.29856504]\n",
      "[ episode 349 ][ timestamp 6 ] state=[-0.0154993   0.20078698 -0.01246373 -0.29856504], action=0, reward=1.0, next_state=[-0.01148356  0.00584489 -0.01843503 -0.00983886]\n",
      "[ episode 349 ][ timestamp 7 ] state=[-0.01148356  0.00584489 -0.01843503 -0.00983886], action=0, reward=1.0, next_state=[-0.01136667 -0.18900789 -0.0186318   0.27697106]\n",
      "[ episode 349 ][ timestamp 8 ] state=[-0.01136667 -0.18900789 -0.0186318   0.27697106], action=1, reward=1.0, next_state=[-0.01514682  0.00637485 -0.01309238 -0.02152962]\n",
      "[ episode 349 ][ timestamp 9 ] state=[-0.01514682  0.00637485 -0.01309238 -0.02152962], action=0, reward=1.0, next_state=[-0.01501933 -0.18855692 -0.01352297  0.26699393]\n",
      "[ episode 349 ][ timestamp 10 ] state=[-0.01501933 -0.18855692 -0.01352297  0.26699393], action=1, reward=1.0, next_state=[-0.01879047  0.00675539 -0.0081831  -0.02992339]\n",
      "[ episode 349 ][ timestamp 11 ] state=[-0.01879047  0.00675539 -0.0081831  -0.02992339], action=1, reward=1.0, next_state=[-0.01865536  0.20199373 -0.00878156 -0.3251769 ]\n",
      "[ episode 349 ][ timestamp 12 ] state=[-0.01865536  0.20199373 -0.00878156 -0.3251769 ], action=0, reward=1.0, next_state=[-0.01461548  0.00699791 -0.0152851  -0.03527619]\n",
      "[ episode 349 ][ timestamp 13 ] state=[-0.01461548  0.00699791 -0.0152851  -0.03527619], action=1, reward=1.0, next_state=[-0.01447553  0.20233568 -0.01599063 -0.33274226]\n",
      "[ episode 349 ][ timestamp 14 ] state=[-0.01447553  0.20233568 -0.01599063 -0.33274226], action=0, reward=1.0, next_state=[-0.01042881  0.00744494 -0.02264547 -0.04514456]\n",
      "[ episode 349 ][ timestamp 15 ] state=[-0.01042881  0.00744494 -0.02264547 -0.04514456], action=0, reward=1.0, next_state=[-0.01027991 -0.18734509 -0.02354836  0.24030837]\n",
      "[ episode 349 ][ timestamp 16 ] state=[-0.01027991 -0.18734509 -0.02354836  0.24030837], action=1, reward=1.0, next_state=[-0.01402681  0.0081052  -0.01874219 -0.05970836]\n",
      "[ episode 349 ][ timestamp 17 ] state=[-0.01402681  0.0081052  -0.01874219 -0.05970836], action=0, reward=1.0, next_state=[-0.01386471 -0.18674309 -0.01993636  0.22700285]\n",
      "[ episode 349 ][ timestamp 18 ] state=[-0.01386471 -0.18674309 -0.01993636  0.22700285], action=1, reward=1.0, next_state=[-0.01759957  0.00865802 -0.0153963  -0.07190147]\n",
      "[ episode 349 ][ timestamp 19 ] state=[-0.01759957  0.00865802 -0.0153963  -0.07190147], action=1, reward=1.0, next_state=[-0.01742641  0.20399728 -0.01683433 -0.36940196]\n",
      "[ episode 349 ][ timestamp 20 ] state=[-0.01742641  0.20399728 -0.01683433 -0.36940196], action=0, reward=1.0, next_state=[-0.01334647  0.00911851 -0.02422237 -0.08207429]\n",
      "[ episode 349 ][ timestamp 21 ] state=[-0.01334647  0.00911851 -0.02422237 -0.08207429], action=0, reward=1.0, next_state=[-0.0131641  -0.18564799 -0.02586386  0.20286908]\n",
      "[ episode 349 ][ timestamp 22 ] state=[-0.0131641  -0.18564799 -0.02586386  0.20286908], action=1, reward=1.0, next_state=[-0.01687706  0.00983412 -0.02180648 -0.09785914]\n",
      "[ episode 349 ][ timestamp 23 ] state=[-0.01687706  0.00983412 -0.02180648 -0.09785914], action=0, reward=1.0, next_state=[-0.01668037 -0.18496862 -0.02376366  0.1878649 ]\n",
      "[ episode 349 ][ timestamp 24 ] state=[-0.01668037 -0.18496862 -0.02376366  0.1878649 ], action=1, reward=1.0, next_state=[-0.02037975  0.01048511 -0.02000636 -0.11221878]\n",
      "[ episode 349 ][ timestamp 25 ] state=[-0.02037975  0.01048511 -0.02000636 -0.11221878], action=0, reward=1.0, next_state=[-0.02017004 -0.18434454 -0.02225074  0.17408573]\n",
      "[ episode 349 ][ timestamp 26 ] state=[-0.02017004 -0.18434454 -0.02225074  0.17408573], action=1, reward=1.0, next_state=[-0.02385693  0.01108868 -0.01876902 -0.12553271]\n",
      "[ episode 349 ][ timestamp 27 ] state=[-0.02385693  0.01108868 -0.01876902 -0.12553271], action=0, reward=1.0, next_state=[-0.02363516 -0.18375943 -0.02127968  0.1611702 ]\n",
      "[ episode 349 ][ timestamp 28 ] state=[-0.02363516 -0.18375943 -0.02127968  0.1611702 ], action=1, reward=1.0, next_state=[-0.02731035  0.0116606  -0.01805627 -0.13814923]\n",
      "[ episode 349 ][ timestamp 29 ] state=[-0.02731035  0.0116606  -0.01805627 -0.13814923], action=0, reward=1.0, next_state=[-0.02707714 -0.18319814 -0.02081926  0.14878298]\n",
      "[ episode 349 ][ timestamp 30 ] state=[-0.02707714 -0.18319814 -0.02081926  0.14878298], action=1, reward=1.0, next_state=[-0.0307411   0.01221566 -0.0178436  -0.15039461]\n",
      "[ episode 349 ][ timestamp 31 ] state=[-0.0307411   0.01221566 -0.0178436  -0.15039461], action=0, reward=1.0, next_state=[-0.03049679 -0.1826463  -0.02085149  0.13660606]\n",
      "[ episode 349 ][ timestamp 32 ] state=[-0.03049679 -0.1826463  -0.02085149  0.13660606], action=0, reward=1.0, next_state=[-0.03414971 -0.37746348 -0.01811937  0.42263841]\n",
      "[ episode 349 ][ timestamp 33 ] state=[-0.03414971 -0.37746348 -0.01811937  0.42263841], action=1, reward=1.0, next_state=[-0.04169898 -0.18208958 -0.0096666   0.12429886]\n",
      "[ episode 349 ][ timestamp 34 ] state=[-0.04169898 -0.18208958 -0.0096666   0.12429886], action=1, reward=1.0, next_state=[-0.04534077  0.01316952 -0.00718062 -0.17141804]\n",
      "[ episode 349 ][ timestamp 35 ] state=[-0.04534077  0.01316952 -0.00718062 -0.17141804], action=1, reward=1.0, next_state=[-0.04507738  0.2083935  -0.01060898 -0.46635755]\n",
      "[ episode 349 ][ timestamp 36 ] state=[-0.04507738  0.2083935  -0.01060898 -0.46635755], action=0, reward=1.0, next_state=[-0.04090951  0.01342304 -0.01993614 -0.17703731]\n",
      "[ episode 349 ][ timestamp 37 ] state=[-0.04090951  0.01342304 -0.01993614 -0.17703731], action=0, reward=1.0, next_state=[-0.04064105 -0.18140801 -0.02347688  0.10929037]\n",
      "[ episode 349 ][ timestamp 38 ] state=[-0.04064105 -0.18140801 -0.02347688  0.10929037], action=1, reward=1.0, next_state=[-0.04426921  0.01404235 -0.02129107 -0.19070596]\n",
      "[ episode 349 ][ timestamp 39 ] state=[-0.04426921  0.01404235 -0.02129107 -0.19070596], action=0, reward=1.0, next_state=[-0.04398837 -0.18076864 -0.02510519  0.0951852 ]\n",
      "[ episode 349 ][ timestamp 40 ] state=[-0.04398837 -0.18076864 -0.02510519  0.0951852 ], action=1, reward=1.0, next_state=[-0.04760374  0.01470397 -0.02320149 -0.20531147]\n",
      "[ episode 349 ][ timestamp 41 ] state=[-0.04760374  0.01470397 -0.02320149 -0.20531147], action=0, reward=1.0, next_state=[-0.04730966 -0.18007864 -0.02730772  0.07996318]\n",
      "[ episode 349 ][ timestamp 42 ] state=[-0.04730966 -0.18007864 -0.02730772  0.07996318], action=1, reward=1.0, next_state=[-0.05091123  0.01542391 -0.02570846 -0.22120879]\n",
      "[ episode 349 ][ timestamp 43 ] state=[-0.05091123  0.01542391 -0.02570846 -0.22120879], action=0, reward=1.0, next_state=[-0.05060275 -0.17932131 -0.03013263  0.06325507]\n",
      "[ episode 349 ][ timestamp 44 ] state=[-0.05060275 -0.17932131 -0.03013263  0.06325507], action=1, reward=1.0, next_state=[-0.05418918  0.01621942 -0.02886753 -0.23878053]\n",
      "[ episode 349 ][ timestamp 45 ] state=[-0.05418918  0.01621942 -0.02886753 -0.23878053], action=0, reward=1.0, next_state=[-0.05386479 -0.1784785  -0.03364314  0.04465879]\n",
      "[ episode 349 ][ timestamp 46 ] state=[-0.05386479 -0.1784785  -0.03364314  0.04465879], action=1, reward=1.0, next_state=[-0.05743436  0.01710932 -0.03274996 -0.25844614]\n",
      "[ episode 349 ][ timestamp 47 ] state=[-0.05743436  0.01710932 -0.03274996 -0.25844614], action=1, reward=1.0, next_state=[-0.05709218  0.21268314 -0.03791889 -0.56127633]\n",
      "[ episode 349 ][ timestamp 48 ] state=[-0.05709218  0.21268314 -0.03791889 -0.56127633], action=0, reward=1.0, next_state=[-0.05283851  0.01811329 -0.04914441 -0.28077683]\n",
      "[ episode 349 ][ timestamp 49 ] state=[-0.05283851  0.01811329 -0.04914441 -0.28077683], action=1, reward=1.0, next_state=[-0.05247625  0.21390056 -0.05475995 -0.5885458 ]\n",
      "[ episode 349 ][ timestamp 50 ] state=[-0.05247625  0.21390056 -0.05475995 -0.5885458 ], action=0, reward=1.0, next_state=[-0.04819824  0.01958648 -0.06653087 -0.31360295]\n",
      "[ episode 349 ][ timestamp 51 ] state=[-0.04819824  0.01958648 -0.06653087 -0.31360295], action=0, reward=1.0, next_state=[-0.04780651 -0.17452774 -0.07280293 -0.04262145]\n",
      "[ episode 349 ][ timestamp 52 ] state=[-0.04780651 -0.17452774 -0.07280293 -0.04262145], action=0, reward=1.0, next_state=[-0.05129706 -0.3685343  -0.07365535  0.22623235]\n",
      "[ episode 349 ][ timestamp 53 ] state=[-0.05129706 -0.3685343  -0.07365535  0.22623235], action=1, reward=1.0, next_state=[-0.05866775 -0.17244121 -0.06913071 -0.08874487]\n",
      "[ episode 349 ][ timestamp 54 ] state=[-0.05866775 -0.17244121 -0.06913071 -0.08874487], action=0, reward=1.0, next_state=[-0.06211657 -0.36650766 -0.07090561  0.18135125]\n",
      "[ episode 349 ][ timestamp 55 ] state=[-0.06211657 -0.36650766 -0.07090561  0.18135125], action=1, reward=1.0, next_state=[-0.06944672 -0.1704465  -0.06727858 -0.1328303 ]\n",
      "[ episode 349 ][ timestamp 56 ] state=[-0.06944672 -0.1704465  -0.06727858 -0.1328303 ], action=0, reward=1.0, next_state=[-0.07285565 -0.36454346 -0.06993519  0.13789148]\n",
      "[ episode 349 ][ timestamp 57 ] state=[-0.07285565 -0.36454346 -0.06993519  0.13789148], action=0, reward=1.0, next_state=[-0.08014652 -0.55859767 -0.06717736  0.40771706]\n",
      "[ episode 349 ][ timestamp 58 ] state=[-0.08014652 -0.55859767 -0.06717736  0.40771706], action=1, reward=1.0, next_state=[-0.09131848 -0.3625907  -0.05902302  0.09463448]\n",
      "[ episode 349 ][ timestamp 59 ] state=[-0.09131848 -0.3625907  -0.05902302  0.09463448], action=0, reward=1.0, next_state=[-0.09857029 -0.5568192  -0.05713033  0.36812721]\n",
      "[ episode 349 ][ timestamp 60 ] state=[-0.09857029 -0.5568192  -0.05713033  0.36812721], action=1, reward=1.0, next_state=[-0.10970667 -0.36093398 -0.04976778  0.05799157]\n",
      "[ episode 349 ][ timestamp 61 ] state=[-0.10970667 -0.36093398 -0.04976778  0.05799157], action=0, reward=1.0, next_state=[-0.11692535 -0.55530833 -0.04860795  0.33456641]\n",
      "[ episode 349 ][ timestamp 62 ] state=[-0.11692535 -0.55530833 -0.04860795  0.33456641], action=1, reward=1.0, next_state=[-0.12803152 -0.3595295  -0.04191662  0.02695991]\n",
      "[ episode 349 ][ timestamp 63 ] state=[-0.12803152 -0.3595295  -0.04191662  0.02695991], action=0, reward=1.0, next_state=[-0.13522211 -0.55402604 -0.04137742  0.30612859]\n",
      "[ episode 349 ][ timestamp 64 ] state=[-0.13522211 -0.55402604 -0.04137742  0.30612859], action=1, reward=1.0, next_state=[-0.14630263 -0.35833964 -0.03525485  0.00068873]\n",
      "[ episode 349 ][ timestamp 65 ] state=[-0.14630263 -0.35833964 -0.03525485  0.00068873], action=0, reward=1.0, next_state=[-0.15346942 -0.55293871 -0.03524108  0.28204318]\n",
      "[ episode 349 ][ timestamp 66 ] state=[-0.15346942 -0.55293871 -0.03524108  0.28204318], action=1, reward=1.0, next_state=[-0.1645282  -0.35733227 -0.02960021 -0.02154303]\n",
      "[ episode 349 ][ timestamp 67 ] state=[-0.1645282  -0.35733227 -0.02960021 -0.02154303], action=0, reward=1.0, next_state=[-0.17167484 -0.5520175  -0.03003107  0.26165569]\n",
      "[ episode 349 ][ timestamp 68 ] state=[-0.17167484 -0.5520175  -0.03003107  0.26165569], action=1, reward=1.0, next_state=[-0.18271519 -0.35648002 -0.02479796 -0.04034608]\n",
      "[ episode 349 ][ timestamp 69 ] state=[-0.18271519 -0.35648002 -0.02479796 -0.04034608], action=0, reward=1.0, next_state=[-0.1898448  -0.55123776 -0.02560488  0.24441085]\n",
      "[ episode 349 ][ timestamp 70 ] state=[-0.1898448  -0.55123776 -0.02560488  0.24441085], action=1, reward=1.0, next_state=[-0.20086955 -0.35575962 -0.02071666 -0.05623725]\n",
      "[ episode 349 ][ timestamp 71 ] state=[-0.20086955 -0.35575962 -0.02071666 -0.05623725], action=0, reward=1.0, next_state=[-0.20798474 -0.5505785  -0.02184141  0.22983809]\n",
      "[ episode 349 ][ timestamp 72 ] state=[-0.20798474 -0.5505785  -0.02184141  0.22983809], action=1, reward=1.0, next_state=[-0.21899631 -0.35515136 -0.01724465 -0.06965357]\n",
      "[ episode 349 ][ timestamp 73 ] state=[-0.21899631 -0.35515136 -0.01724465 -0.06965357], action=0, reward=1.0, next_state=[-0.22609934 -0.55002188 -0.01863772  0.21753909]\n",
      "[ episode 349 ][ timestamp 74 ] state=[-0.22609934 -0.55002188 -0.01863772  0.21753909], action=1, reward=1.0, next_state=[-0.23709978 -0.35463853 -0.01428694 -0.08096422]\n",
      "[ episode 349 ][ timestamp 75 ] state=[-0.23709978 -0.35463853 -0.01428694 -0.08096422], action=0, reward=1.0, next_state=[-0.24419255 -0.54955279 -0.01590622  0.20717712]\n",
      "[ episode 349 ][ timestamp 76 ] state=[-0.24419255 -0.54955279 -0.01590622  0.20717712], action=1, reward=1.0, next_state=[-0.2551836  -0.35420704 -0.01176268 -0.09048067]\n",
      "[ episode 349 ][ timestamp 77 ] state=[-0.2551836  -0.35420704 -0.01176268 -0.09048067], action=0, reward=1.0, next_state=[-0.26226774 -0.54915844 -0.01357229  0.19846805]\n",
      "[ episode 349 ][ timestamp 78 ] state=[-0.26226774 -0.54915844 -0.01357229  0.19846805], action=0, reward=1.0, next_state=[-0.27325091 -0.74408366 -0.00960293  0.48683882]\n",
      "[ episode 349 ][ timestamp 79 ] state=[-0.27325091 -0.74408366 -0.00960293  0.48683882], action=1, reward=1.0, next_state=[-2.88132587e-01 -5.48827535e-01  1.33844615e-04  1.91144918e-01]\n",
      "[ episode 349 ][ timestamp 80 ] state=[-2.88132587e-01 -5.48827535e-01  1.33844615e-04  1.91144918e-01], action=1, reward=1.0, next_state=[-0.29910914 -0.3537075   0.00395674 -0.10149578]\n",
      "[ episode 349 ][ timestamp 81 ] state=[-0.29910914 -0.3537075   0.00395674 -0.10149578], action=0, reward=1.0, next_state=[-0.30618329 -0.54888593  0.00192683  0.19243285]\n",
      "[ episode 349 ][ timestamp 82 ] state=[-0.30618329 -0.54888593  0.00192683  0.19243285], action=1, reward=1.0, next_state=[-0.31716101 -0.3537916   0.00577548 -0.09964162]\n",
      "[ episode 349 ][ timestamp 83 ] state=[-0.31716101 -0.3537916   0.00577548 -0.09964162], action=0, reward=1.0, next_state=[-0.32423684 -0.54899584  0.00378265  0.19485785]\n",
      "[ episode 349 ][ timestamp 84 ] state=[-0.32423684 -0.54899584  0.00378265  0.19485785], action=1, reward=1.0, next_state=[-0.33521675 -0.3539282   0.00767981 -0.09662942]\n",
      "[ episode 349 ][ timestamp 85 ] state=[-0.33521675 -0.3539282   0.00767981 -0.09662942], action=0, reward=1.0, next_state=[-0.34229532 -0.54915938  0.00574722  0.19846655]\n",
      "[ episode 349 ][ timestamp 86 ] state=[-0.34229532 -0.54915938  0.00574722  0.19846655], action=0, reward=1.0, next_state=[-0.35327851 -0.74436306  0.00971655  0.49295691]\n",
      "[ episode 349 ][ timestamp 87 ] state=[-0.35327851 -0.74436306  0.00971655  0.49295691], action=0, reward=1.0, next_state=[-0.36816577 -0.9396207   0.01957569  0.78868616]\n",
      "[ episode 349 ][ timestamp 88 ] state=[-0.36816577 -0.9396207   0.01957569  0.78868616], action=1, reward=1.0, next_state=[-0.38695818 -0.744773    0.03534941  0.5022255 ]\n",
      "[ episode 349 ][ timestamp 89 ] state=[-0.38695818 -0.744773    0.03534941  0.5022255 ], action=1, reward=1.0, next_state=[-0.40185364 -0.55016668  0.04539392  0.22088895]\n",
      "[ episode 349 ][ timestamp 90 ] state=[-0.40185364 -0.55016668  0.04539392  0.22088895], action=1, reward=1.0, next_state=[-0.41285698 -0.35572199  0.0498117  -0.05713641]\n",
      "[ episode 349 ][ timestamp 91 ] state=[-0.41285698 -0.35572199  0.0498117  -0.05713641], action=0, reward=1.0, next_state=[-0.41997142 -0.55152145  0.04866897  0.25083709]\n",
      "[ episode 349 ][ timestamp 92 ] state=[-0.41997142 -0.55152145  0.04866897  0.25083709], action=1, reward=1.0, next_state=[-0.43100184 -0.35712707  0.05368572 -0.02610619]\n",
      "[ episode 349 ][ timestamp 93 ] state=[-0.43100184 -0.35712707  0.05368572 -0.02610619], action=1, reward=1.0, next_state=[-0.43814439 -0.16281447  0.05316359 -0.30137914]\n",
      "[ episode 349 ][ timestamp 94 ] state=[-0.43814439 -0.16281447  0.05316359 -0.30137914], action=0, reward=1.0, next_state=[-0.44140068 -0.35865226  0.04713601  0.00758525]\n",
      "[ episode 349 ][ timestamp 95 ] state=[-0.44140068 -0.35865226  0.04713601  0.00758525], action=0, reward=1.0, next_state=[-0.44857372 -0.5544174   0.04728771  0.31475967]\n",
      "[ episode 349 ][ timestamp 96 ] state=[-0.44857372 -0.5544174   0.04728771  0.31475967], action=1, reward=1.0, next_state=[-0.45966207 -0.35999983  0.05358291  0.03735672]\n",
      "[ episode 349 ][ timestamp 97 ] state=[-0.45966207 -0.35999983  0.05358291  0.03735672], action=1, reward=1.0, next_state=[-0.46686206 -0.16568558  0.05433004 -0.2379505 ]\n",
      "[ episode 349 ][ timestamp 98 ] state=[-0.46686206 -0.16568558  0.05433004 -0.2379505 ], action=0, reward=1.0, next_state=[-0.47017578 -0.36153991  0.04957103  0.07136269]\n",
      "[ episode 349 ][ timestamp 99 ] state=[-0.47017578 -0.36153991  0.04957103  0.07136269], action=0, reward=1.0, next_state=[-0.47740657 -0.5573362   0.05099829  0.37926427]\n",
      "[ episode 349 ][ timestamp 100 ] state=[-0.47740657 -0.5573362   0.05099829  0.37926427], action=1, reward=1.0, next_state=[-0.4885533  -0.36297417  0.05858357  0.10308727]\n",
      "[ episode 349 ][ timestamp 101 ] state=[-0.4885533  -0.36297417  0.05858357  0.10308727], action=1, reward=1.0, next_state=[-0.49581278 -0.16873859  0.06064532 -0.17055256]\n",
      "[ episode 349 ][ timestamp 102 ] state=[-0.49581278 -0.16873859  0.06064532 -0.17055256], action=0, reward=1.0, next_state=[-0.49918755 -0.36467377  0.05723427  0.1406287 ]\n",
      "[ episode 349 ][ timestamp 103 ] state=[-0.49918755 -0.36467377  0.05723427  0.1406287 ], action=1, reward=1.0, next_state=[-0.50648103 -0.17041626  0.06004684 -0.13346275]\n",
      "[ episode 349 ][ timestamp 104 ] state=[-0.50648103 -0.17041626  0.06004684 -0.13346275], action=1, reward=1.0, next_state=[-0.50988935  0.02379647  0.05737758 -0.40661364]\n",
      "[ episode 349 ][ timestamp 105 ] state=[-0.50988935  0.02379647  0.05737758 -0.40661364], action=0, reward=1.0, next_state=[-0.50941343 -0.17209017  0.04924531 -0.09640747]\n",
      "[ episode 349 ][ timestamp 106 ] state=[-0.50941343 -0.17209017  0.04924531 -0.09640747], action=0, reward=1.0, next_state=[-0.51285523 -0.36788207  0.04731716  0.21139661]\n",
      "[ episode 349 ][ timestamp 107 ] state=[-0.51285523 -0.36788207  0.04731716  0.21139661], action=0, reward=1.0, next_state=[-0.52021287 -0.56364749  0.05154509  0.51862214]\n",
      "[ episode 349 ][ timestamp 108 ] state=[-0.52021287 -0.56364749  0.05154509  0.51862214], action=0, reward=1.0, next_state=[-0.53148582 -0.75945582  0.06191754  0.82709208]\n",
      "[ episode 349 ][ timestamp 109 ] state=[-0.53148582 -0.75945582  0.06191754  0.82709208], action=1, reward=1.0, next_state=[-0.54667494 -0.5652327   0.07845938  0.55450781]\n",
      "[ episode 349 ][ timestamp 110 ] state=[-0.54667494 -0.5652327   0.07845938  0.55450781], action=1, reward=1.0, next_state=[-0.55797959 -0.37129503  0.08954953  0.28753963]\n",
      "[ episode 349 ][ timestamp 111 ] state=[-0.55797959 -0.37129503  0.08954953  0.28753963], action=1, reward=1.0, next_state=[-0.56540549 -0.17755665  0.09530033  0.02438889]\n",
      "[ episode 349 ][ timestamp 112 ] state=[-0.56540549 -0.17755665  0.09530033  0.02438889], action=1, reward=1.0, next_state=[-0.56895662  0.01607858  0.09578811 -0.23677008]\n",
      "[ episode 349 ][ timestamp 113 ] state=[-0.56895662  0.01607858  0.09578811 -0.23677008], action=1, reward=1.0, next_state=[-0.56863505  0.2097108   0.0910527  -0.4977683 ]\n",
      "[ episode 349 ][ timestamp 114 ] state=[-0.56863505  0.2097108   0.0910527  -0.4977683 ], action=0, reward=1.0, next_state=[-0.56444084  0.01343095  0.08109734 -0.17783561]\n",
      "[ episode 349 ][ timestamp 115 ] state=[-0.56444084  0.01343095  0.08109734 -0.17783561], action=0, reward=1.0, next_state=[-0.56417222 -0.18275226  0.07754063  0.13928854]\n",
      "[ episode 349 ][ timestamp 116 ] state=[-0.56417222 -0.18275226  0.07754063  0.13928854], action=1, reward=1.0, next_state=[-0.56782726  0.01117841  0.0803264  -0.1279593 ]\n",
      "[ episode 349 ][ timestamp 117 ] state=[-0.56782726  0.01117841  0.0803264  -0.1279593 ], action=1, reward=1.0, next_state=[-0.56760369  0.20506326  0.07776721 -0.39425824]\n",
      "[ episode 349 ][ timestamp 118 ] state=[-0.56760369  0.20506326  0.07776721 -0.39425824], action=0, reward=1.0, next_state=[-0.56350243  0.0089289   0.06988205 -0.07810536]\n",
      "[ episode 349 ][ timestamp 119 ] state=[-0.56350243  0.0089289   0.06988205 -0.07810536], action=1, reward=1.0, next_state=[-0.56332385  0.20298309  0.06831994 -0.34794758]\n",
      "[ episode 349 ][ timestamp 120 ] state=[-0.56332385  0.20298309  0.06831994 -0.34794758], action=0, reward=1.0, next_state=[-0.55926419  0.00695928  0.06136099 -0.03452739]\n",
      "[ episode 349 ][ timestamp 121 ] state=[-0.55926419  0.00695928  0.06136099 -0.03452739], action=1, reward=1.0, next_state=[-0.559125    0.20115007  0.06067044 -0.30723656]\n",
      "[ episode 349 ][ timestamp 122 ] state=[-0.559125    0.20115007  0.06067044 -0.30723656], action=0, reward=1.0, next_state=[-0.555102    0.00521844  0.05452571  0.00394631]\n",
      "[ episode 349 ][ timestamp 123 ] state=[-0.555102    0.00521844  0.05452571  0.00394631], action=1, reward=1.0, next_state=[-0.55499763  0.19951774  0.05460463 -0.27104688]\n",
      "[ episode 349 ][ timestamp 124 ] state=[-0.55499763  0.19951774  0.05460463 -0.27104688], action=0, reward=1.0, next_state=[-0.55100728  0.00366083  0.0491837   0.03834639]\n",
      "[ episode 349 ][ timestamp 125 ] state=[-0.55100728  0.00366083  0.0491837   0.03834639], action=1, reward=1.0, next_state=[-0.55093406  0.19804425  0.04995062 -0.23842196]\n",
      "[ episode 349 ][ timestamp 126 ] state=[-0.55093406  0.19804425  0.04995062 -0.23842196], action=1, reward=1.0, next_state=[-0.54697318  0.39241834  0.04518218 -0.51494006]\n",
      "[ episode 349 ][ timestamp 127 ] state=[-0.54697318  0.39241834  0.04518218 -0.51494006], action=0, reward=1.0, next_state=[-0.53912481  0.19669019  0.03488338 -0.20836842]\n",
      "[ episode 349 ][ timestamp 128 ] state=[-0.53912481  0.19669019  0.03488338 -0.20836842], action=1, reward=1.0, next_state=[-0.53519101  0.39129643  0.03071602 -0.48984655]\n",
      "[ episode 349 ][ timestamp 129 ] state=[-0.53519101  0.39129643  0.03071602 -0.48984655], action=1, reward=1.0, next_state=[-0.52736508  0.5859719   0.02091908 -0.77269292]\n",
      "[ episode 349 ][ timestamp 130 ] state=[-0.52736508  0.5859719   0.02091908 -0.77269292], action=0, reward=1.0, next_state=[-0.51564564  0.39056846  0.00546523 -0.47350214]\n",
      "[ episode 349 ][ timestamp 131 ] state=[-0.51564564  0.39056846  0.00546523 -0.47350214], action=0, reward=1.0, next_state=[-0.50783427  0.19536975 -0.00400482 -0.17910168]\n",
      "[ episode 349 ][ timestamp 132 ] state=[-0.50783427  0.19536975 -0.00400482 -0.17910168], action=0, reward=1.0, next_state=[-5.03926875e-01  3.05341540e-04 -7.58685025e-03  1.12315179e-01]\n",
      "[ episode 349 ][ timestamp 133 ] state=[-5.03926875e-01  3.05341540e-04 -7.58685025e-03  1.12315179e-01], action=1, reward=1.0, next_state=[-0.50392077  0.19553518 -0.00534055 -0.18275166]\n",
      "[ episode 349 ][ timestamp 134 ] state=[-0.50392077  0.19553518 -0.00534055 -0.18275166], action=0, reward=1.0, next_state=[-5.00010065e-01  4.90051117e-04 -8.99557992e-03  1.08241745e-01]\n",
      "[ episode 349 ][ timestamp 135 ] state=[-5.00010065e-01  4.90051117e-04 -8.99557992e-03  1.08241745e-01], action=1, reward=1.0, next_state=[-0.50000026  0.19573975 -0.00683075 -0.18726561]\n",
      "[ episode 349 ][ timestamp 136 ] state=[-0.50000026  0.19573975 -0.00683075 -0.18726561], action=0, reward=1.0, next_state=[-0.49608547  0.00071619 -0.01057606  0.10325468]\n",
      "[ episode 349 ][ timestamp 137 ] state=[-0.49608547  0.00071619 -0.01057606  0.10325468], action=1, reward=1.0, next_state=[-0.49607115  0.19598809 -0.00851096 -0.19274611]\n",
      "[ episode 349 ][ timestamp 138 ] state=[-0.49607115  0.19598809 -0.00851096 -0.19274611], action=0, reward=1.0, next_state=[-0.49215138  0.00098892 -0.01236589  0.09723987]\n",
      "[ episode 349 ][ timestamp 139 ] state=[-0.49215138  0.00098892 -0.01236589  0.09723987], action=1, reward=1.0, next_state=[-0.4921316   0.1962859  -0.01042109 -0.19931868]\n",
      "[ episode 349 ][ timestamp 140 ] state=[-0.4921316   0.1962859  -0.01042109 -0.19931868], action=0, reward=1.0, next_state=[-0.48820589  0.00131454 -0.01440746  0.09005874]\n",
      "[ episode 349 ][ timestamp 141 ] state=[-0.48820589  0.00131454 -0.01440746  0.09005874], action=1, reward=1.0, next_state=[-0.4881796   0.19664    -0.01260629 -0.2071347 ]\n",
      "[ episode 349 ][ timestamp 142 ] state=[-0.4881796   0.19664    -0.01260629 -0.2071347 ], action=0, reward=1.0, next_state=[-0.4842468   0.00170057 -0.01674898  0.08154507]\n",
      "[ episode 349 ][ timestamp 143 ] state=[-0.4842468   0.00170057 -0.01674898  0.08154507], action=1, reward=1.0, next_state=[-0.48421278  0.19705856 -0.01511808 -0.21637479]\n",
      "[ episode 349 ][ timestamp 144 ] state=[-0.48421278  0.19705856 -0.01511808 -0.21637479], action=0, reward=1.0, next_state=[-0.48027161  0.00215596 -0.01944558  0.07150116]\n",
      "[ episode 349 ][ timestamp 145 ] state=[-0.48027161  0.00215596 -0.01944558  0.07150116], action=0, reward=1.0, next_state=[-0.48022849 -0.19268189 -0.01801555  0.35798604]\n",
      "[ episode 349 ][ timestamp 146 ] state=[-0.48022849 -0.19268189 -0.01801555  0.35798604], action=1, reward=1.0, next_state=[-0.48408213  0.00269148 -0.01085583  0.05967725]\n",
      "[ episode 349 ][ timestamp 147 ] state=[-0.48408213  0.00269148 -0.01085583  0.05967725], action=1, reward=1.0, next_state=[-0.4840283   0.19796739 -0.00966229 -0.2364109 ]\n",
      "[ episode 349 ][ timestamp 148 ] state=[-0.4840283   0.19796739 -0.00966229 -0.2364109 ], action=0, reward=1.0, next_state=[-0.48006895  0.0029848  -0.0143905   0.05320866]\n",
      "[ episode 349 ][ timestamp 149 ] state=[-0.48006895  0.0029848  -0.0143905   0.05320866], action=1, reward=1.0, next_state=[-0.48000926  0.19831011 -0.01332633 -0.24397963]\n",
      "[ episode 349 ][ timestamp 150 ] state=[-0.48000926  0.19831011 -0.01332633 -0.24397963], action=0, reward=1.0, next_state=[-0.47604306  0.00338101 -0.01820592  0.04447023]\n",
      "[ episode 349 ][ timestamp 151 ] state=[-0.47604306  0.00338101 -0.01820592  0.04447023], action=1, reward=1.0, next_state=[-0.47597544  0.19875923 -0.01731652 -0.25390078]\n",
      "[ episode 349 ][ timestamp 152 ] state=[-0.47597544  0.19875923 -0.01731652 -0.25390078], action=0, reward=1.0, next_state=[-0.47200025  0.00388876 -0.02239454  0.0332703 ]\n",
      "[ episode 349 ][ timestamp 153 ] state=[-0.47200025  0.00388876 -0.02239454  0.0332703 ], action=1, reward=1.0, next_state=[-0.47192248  0.19932458 -0.02172913 -0.26639337]\n",
      "[ episode 349 ][ timestamp 154 ] state=[-0.47192248  0.19932458 -0.02172913 -0.26639337], action=0, reward=1.0, next_state=[-0.46793598  0.00451938 -0.027057    0.01935759]\n",
      "[ episode 349 ][ timestamp 155 ] state=[-0.46793598  0.00451938 -0.027057    0.01935759], action=1, reward=1.0, next_state=[-0.4678456   0.2000187  -0.02666984 -0.28173784]\n",
      "[ episode 349 ][ timestamp 156 ] state=[-0.4678456   0.2000187  -0.02666984 -0.28173784], action=0, reward=1.0, next_state=[-0.46384522  0.00528712 -0.0323046   0.00241565]\n",
      "[ episode 349 ][ timestamp 157 ] state=[-0.46384522  0.00528712 -0.0323046   0.00241565], action=1, reward=1.0, next_state=[-0.46373948  0.20085712 -0.03225629 -0.30028219]\n",
      "[ episode 349 ][ timestamp 158 ] state=[-0.46373948  0.20085712 -0.03225629 -0.30028219], action=0, reward=1.0, next_state=[-0.45972234  0.00620943 -0.03826193 -0.01794424]\n",
      "[ episode 349 ][ timestamp 159 ] state=[-0.45972234  0.00620943 -0.03826193 -0.01794424], action=1, reward=1.0, next_state=[-0.45959815  0.20185861 -0.03862082 -0.32244949]\n",
      "[ episode 349 ][ timestamp 160 ] state=[-0.45959815  0.20185861 -0.03862082 -0.32244949], action=0, reward=1.0, next_state=[-0.45556098  0.0073073  -0.04506981 -0.04219182]\n",
      "[ episode 349 ][ timestamp 161 ] state=[-0.45556098  0.0073073  -0.04506981 -0.04219182], action=1, reward=1.0, next_state=[-0.45541483  0.20304559 -0.04591364 -0.34874715]\n",
      "[ episode 349 ][ timestamp 162 ] state=[-0.45541483  0.20304559 -0.04591364 -0.34874715], action=0, reward=1.0, next_state=[-0.45135392  0.00860571 -0.05288859 -0.07088856]\n",
      "[ episode 349 ][ timestamp 163 ] state=[-0.45135392  0.00860571 -0.05288859 -0.07088856], action=0, reward=1.0, next_state=[-0.45118181 -0.18571968 -0.05430636  0.20464995]\n",
      "[ episode 349 ][ timestamp 164 ] state=[-0.45118181 -0.18571968 -0.05430636  0.20464995], action=1, reward=1.0, next_state=[-0.4548962   0.01013513 -0.05021336 -0.10465739]\n",
      "[ episode 349 ][ timestamp 165 ] state=[-0.4548962   0.01013513 -0.05021336 -0.10465739], action=0, reward=1.0, next_state=[-0.4546935  -0.1842326  -0.05230651  0.17177021]\n",
      "[ episode 349 ][ timestamp 166 ] state=[-0.4546935  -0.1842326  -0.05230651  0.17177021], action=1, reward=1.0, next_state=[-0.45837815  0.01159746 -0.0488711  -0.13694424]\n",
      "[ episode 349 ][ timestamp 167 ] state=[-0.45837815  0.01159746 -0.0488711  -0.13694424], action=0, reward=1.0, next_state=[-0.4581462  -0.18279167 -0.05160999  0.13992893]\n",
      "[ episode 349 ][ timestamp 168 ] state=[-0.4581462  -0.18279167 -0.05160999  0.13992893], action=1, reward=1.0, next_state=[-0.46180203  0.01303001 -0.04881141 -0.16857909]\n",
      "[ episode 349 ][ timestamp 169 ] state=[-0.46180203  0.01303001 -0.04881141 -0.16857909], action=0, reward=1.0, next_state=[-0.46154143 -0.18136052 -0.05218299  0.10831456]\n",
      "[ episode 349 ][ timestamp 170 ] state=[-0.46154143 -0.18136052 -0.05218299  0.10831456], action=1, reward=1.0, next_state=[-0.46516864  0.01446888 -0.0500167  -0.20036452]\n",
      "[ episode 349 ][ timestamp 171 ] state=[-0.46516864  0.01446888 -0.0500167  -0.20036452], action=0, reward=1.0, next_state=[-0.46487927 -0.17990337 -0.05402399  0.07613045]\n",
      "[ episode 349 ][ timestamp 172 ] state=[-0.46487927 -0.17990337 -0.05402399  0.07613045], action=0, reward=1.0, next_state=[-0.46847733 -0.37421089 -0.05250138  0.35129118]\n",
      "[ episode 349 ][ timestamp 173 ] state=[-0.46847733 -0.37421089 -0.05250138  0.35129118], action=1, reward=1.0, next_state=[-0.47596155 -0.17838315 -0.04547556  0.042526  ]\n",
      "[ episode 349 ][ timestamp 174 ] state=[-0.47596155 -0.17838315 -0.04547556  0.042526  ], action=1, reward=1.0, next_state=[-0.47952921  0.0173604  -0.04462504 -0.26415099]\n",
      "[ episode 349 ][ timestamp 175 ] state=[-0.47952921  0.0173604  -0.04462504 -0.26415099], action=0, reward=1.0, next_state=[-0.47918201 -0.17709713 -0.04990806  0.01412952]\n",
      "[ episode 349 ][ timestamp 176 ] state=[-0.47918201 -0.17709713 -0.04990806  0.01412952], action=0, reward=1.0, next_state=[-0.48272395 -0.37146913 -0.04962547  0.2906576 ]\n",
      "[ episode 349 ][ timestamp 177 ] state=[-0.48272395 -0.37146913 -0.04962547  0.2906576 ], action=1, reward=1.0, next_state=[-0.49015333 -0.17567598 -0.04381231 -0.01725445]\n",
      "[ episode 349 ][ timestamp 178 ] state=[-0.49015333 -0.17567598 -0.04381231 -0.01725445], action=0, reward=1.0, next_state=[-0.49366685 -0.37014312 -0.0441574   0.26128964]\n",
      "[ episode 349 ][ timestamp 179 ] state=[-0.49366685 -0.37014312 -0.0441574   0.26128964], action=1, reward=1.0, next_state=[-0.50106971 -0.17441956 -0.03893161 -0.04498759]\n",
      "[ episode 349 ][ timestamp 180 ] state=[-0.50106971 -0.17441956 -0.03893161 -0.04498759], action=0, reward=1.0, next_state=[-0.5045581  -0.36896225 -0.03983136  0.23516233]\n",
      "[ episode 349 ][ timestamp 181 ] state=[-0.5045581  -0.36896225 -0.03983136  0.23516233], action=1, reward=1.0, next_state=[-0.51193735 -0.17329451 -0.03512812 -0.0698138 ]\n",
      "[ episode 349 ][ timestamp 182 ] state=[-0.51193735 -0.17329451 -0.03512812 -0.0698138 ], action=0, reward=1.0, next_state=[-0.51540324 -0.36789569 -0.03652439  0.21158234]\n",
      "[ episode 349 ][ timestamp 183 ] state=[-0.51540324 -0.36789569 -0.03652439  0.21158234], action=1, reward=1.0, next_state=[-0.52276115 -0.17227108 -0.03229275 -0.09239466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 349 ][ timestamp 184 ] state=[-0.52276115 -0.17227108 -0.03229275 -0.09239466], action=0, reward=1.0, next_state=[-0.52620657 -0.36691565 -0.03414064  0.18992755]\n",
      "[ episode 349 ][ timestamp 185 ] state=[-0.52620657 -0.36691565 -0.03414064  0.18992755], action=1, reward=1.0, next_state=[-0.53354489 -0.17132233 -0.03034209 -0.11332686]\n",
      "[ episode 349 ][ timestamp 186 ] state=[-0.53354489 -0.17132233 -0.03034209 -0.11332686], action=0, reward=1.0, next_state=[-0.53697133 -0.36599666 -0.03260862  0.16963104]\n",
      "[ episode 349 ][ timestamp 187 ] state=[-0.53697133 -0.36599666 -0.03260862  0.16963104], action=1, reward=1.0, next_state=[-0.54429127 -0.17042351 -0.029216   -0.13315797]\n",
      "[ episode 349 ][ timestamp 188 ] state=[-0.54429127 -0.17042351 -0.029216   -0.13315797], action=0, reward=1.0, next_state=[-0.54769974 -0.36511505 -0.03187916  0.15016643]\n",
      "[ episode 349 ][ timestamp 189 ] state=[-0.54769974 -0.36511505 -0.03187916  0.15016643], action=1, reward=1.0, next_state=[-0.55500204 -0.16955146 -0.02887583 -0.15240081]\n",
      "[ episode 349 ][ timestamp 190 ] state=[-0.55500204 -0.16955146 -0.02887583 -0.15240081], action=0, reward=1.0, next_state=[-0.55839307 -0.3642483  -0.03192385  0.13103439]\n",
      "[ episode 349 ][ timestamp 191 ] state=[-0.55839307 -0.3642483  -0.03192385  0.13103439], action=1, reward=1.0, next_state=[-0.56567803 -0.16868393 -0.02930316 -0.17154671]\n",
      "[ episode 349 ][ timestamp 192 ] state=[-0.56567803 -0.16868393 -0.02930316 -0.17154671], action=0, reward=1.0, next_state=[-0.56905171 -0.36337449 -0.0327341   0.11174986]\n",
      "[ episode 349 ][ timestamp 193 ] state=[-0.56905171 -0.36337449 -0.0327341   0.11174986], action=1, reward=1.0, next_state=[-0.5763192  -0.16779914 -0.0304991  -0.19107811]\n",
      "[ episode 349 ][ timestamp 194 ] state=[-0.5763192  -0.16779914 -0.0304991  -0.19107811], action=0, reward=1.0, next_state=[-0.57967518 -0.3624718  -0.03432066  0.09182974]\n",
      "[ episode 349 ][ timestamp 195 ] state=[-0.57967518 -0.3624718  -0.03432066  0.09182974], action=1, reward=1.0, next_state=[-0.58692462 -0.16687516 -0.03248407 -0.21148074]\n",
      "[ episode 349 ][ timestamp 196 ] state=[-0.58692462 -0.16687516 -0.03248407 -0.21148074], action=0, reward=1.0, next_state=[-0.59026212 -0.36151797 -0.03671368  0.0707808 ]\n",
      "[ episode 349 ][ timestamp 197 ] state=[-0.59026212 -0.36151797 -0.03671368  0.0707808 ], action=0, reward=1.0, next_state=[-0.59749248 -0.55609486 -0.03529807  0.35165807]\n",
      "[ episode 349 ][ timestamp 198 ] state=[-0.59749248 -0.55609486 -0.03529807  0.35165807], action=1, reward=1.0, next_state=[-0.60861438 -0.36048919 -0.0282649   0.04805685]\n",
      "[ episode 349 ][ timestamp 199 ] state=[-0.60861438 -0.36048919 -0.0282649   0.04805685], action=0, reward=1.0, next_state=[-0.61582416 -0.55519468 -0.02730377  0.33168965]\n",
      "[ episode 349 ][ timestamp 200 ] state=[-0.61582416 -0.55519468 -0.02730377  0.33168965], action=0, reward=1.0, next_state=[-0.62692806 -0.74991756 -0.02066997  0.6156388 ]\n",
      "[ episode 349 ][ timestamp 201 ] state=[-0.62692806 -0.74991756 -0.02066997  0.6156388 ], action=1, reward=1.0, next_state=[-0.64192641 -0.55451301 -0.0083572   0.31651804]\n",
      "[ episode 349 ][ timestamp 202 ] state=[-0.64192641 -0.55451301 -0.0083572   0.31651804], action=1, reward=1.0, next_state=[-0.65301667 -0.35927302 -0.00202684  0.0212113 ]\n",
      "[ episode 349 ][ timestamp 203 ] state=[-0.65301667 -0.35927302 -0.00202684  0.0212113 ], action=0, reward=1.0, next_state=[-0.66020213 -0.55436585 -0.00160261  0.31325405]\n",
      "[ episode 349 ][ timestamp 204 ] state=[-0.66020213 -0.55436585 -0.00160261  0.31325405], action=1, reward=1.0, next_state=[-0.67128945 -0.3592211   0.00466247  0.02006614]\n",
      "[ episode 349 ][ timestamp 205 ] state=[-0.67128945 -0.3592211   0.00466247  0.02006614], action=1, reward=1.0, next_state=[-0.67847387 -0.16416633  0.00506379 -0.27114209]\n",
      "[ episode 349 ][ timestamp 206 ] state=[-0.67847387 -0.16416633  0.00506379 -0.27114209], action=0, reward=1.0, next_state=[-6.81757196e-01 -3.59360169e-01 -3.59049999e-04  2.31336721e-02]\n",
      "[ episode 349 ][ timestamp 207 ] state=[-6.81757196e-01 -3.59360169e-01 -3.59049999e-04  2.31336721e-02], action=0, reward=1.0, next_state=[-6.88944399e-01 -5.54476969e-01  1.03623443e-04  3.15703293e-01]\n",
      "[ episode 349 ][ timestamp 208 ] state=[-6.88944399e-01 -5.54476969e-01  1.03623443e-04  3.15703293e-01], action=0, reward=1.0, next_state=[-0.70003394 -0.7496004   0.00641769  0.6084189 ]\n",
      "[ episode 349 ][ timestamp 209 ] state=[-0.70003394 -0.7496004   0.00641769  0.6084189 ], action=1, reward=1.0, next_state=[-0.71502595 -0.55456875  0.01858607  0.31776424]\n",
      "[ episode 349 ][ timestamp 210 ] state=[-0.71502595 -0.55456875  0.01858607  0.31776424], action=1, reward=1.0, next_state=[-0.72611732 -0.35971638  0.02494135  0.03100016]\n",
      "[ episode 349 ][ timestamp 211 ] state=[-0.72611732 -0.35971638  0.02494135  0.03100016], action=1, reward=1.0, next_state=[-0.73331165 -0.16496082  0.02556136 -0.25371033]\n",
      "[ episode 349 ][ timestamp 212 ] state=[-0.73331165 -0.16496082  0.02556136 -0.25371033], action=0, reward=1.0, next_state=[-0.73661087 -0.36043825  0.02048715  0.04692425]\n",
      "[ episode 349 ][ timestamp 213 ] state=[-0.73661087 -0.36043825  0.02048715  0.04692425], action=1, reward=1.0, next_state=[-0.74381963 -0.16561597  0.02142563 -0.23922504]\n",
      "[ episode 349 ][ timestamp 214 ] state=[-0.74381963 -0.16561597  0.02142563 -0.23922504], action=0, reward=1.0, next_state=[-0.74713195 -0.36103734  0.01664113  0.06013839]\n",
      "[ episode 349 ][ timestamp 215 ] state=[-0.74713195 -0.36103734  0.01664113  0.06013839], action=1, reward=1.0, next_state=[-0.7543527  -0.1661579   0.0178439  -0.22724804]\n",
      "[ episode 349 ][ timestamp 216 ] state=[-0.7543527  -0.1661579   0.0178439  -0.22724804], action=0, reward=1.0, next_state=[-0.75767585 -0.36153025  0.01329894  0.07100967]\n",
      "[ episode 349 ][ timestamp 217 ] state=[-0.75767585 -0.36153025  0.01329894  0.07100967], action=1, reward=1.0, next_state=[-0.76490646 -0.16660146  0.01471913 -0.21744789]\n",
      "[ episode 349 ][ timestamp 218 ] state=[-0.76490646 -0.16660146  0.01471913 -0.21744789], action=0, reward=1.0, next_state=[-0.76823849 -0.3619307   0.01037018  0.0798415 ]\n",
      "[ episode 349 ][ timestamp 219 ] state=[-0.76823849 -0.3619307   0.01037018  0.0798415 ], action=1, reward=1.0, next_state=[-0.7754771  -0.16695893  0.01196701 -0.20955165]\n",
      "[ episode 349 ][ timestamp 220 ] state=[-0.7754771  -0.16695893  0.01196701 -0.20955165], action=0, reward=1.0, next_state=[-0.77881628 -0.36224993  0.00777597  0.08688209]\n",
      "[ episode 349 ][ timestamp 221 ] state=[-0.77881628 -0.36224993  0.00777597  0.08688209], action=1, reward=1.0, next_state=[-0.78606128 -0.1672403   0.00951361 -0.2033374 ]\n",
      "[ episode 349 ][ timestamp 222 ] state=[-0.78606128 -0.1672403   0.00951361 -0.2033374 ], action=0, reward=1.0, next_state=[-0.78940609 -0.362497    0.00544687  0.09233137]\n",
      "[ episode 349 ][ timestamp 223 ] state=[-0.78940609 -0.362497    0.00544687  0.09233137], action=1, reward=1.0, next_state=[-0.79665603 -0.16745355  0.00729349 -0.19862811]\n",
      "[ episode 349 ][ timestamp 224 ] state=[-0.79665603 -0.16745355  0.00729349 -0.19862811], action=0, reward=1.0, next_state=[-0.8000051  -0.36267905  0.00332093  0.09634663]\n",
      "[ episode 349 ][ timestamp 225 ] state=[-0.8000051  -0.36267905  0.00332093  0.09634663], action=1, reward=1.0, next_state=[-0.80725868 -0.16760486  0.00524786 -0.1952867 ]\n",
      "[ episode 349 ][ timestamp 226 ] state=[-0.80725868 -0.16760486  0.00524786 -0.1952867 ], action=0, reward=1.0, next_state=[-0.81061078 -0.36280148  0.00134213  0.09904707]\n",
      "[ episode 349 ][ timestamp 227 ] state=[-0.81061078 -0.36280148  0.00134213  0.09904707], action=1, reward=1.0, next_state=[-0.8178668  -0.16769879  0.00332307 -0.19321212]\n",
      "[ episode 349 ][ timestamp 228 ] state=[-0.8178668  -0.16769879  0.00332307 -0.19321212], action=1, reward=1.0, next_state=[-8.21220781e-01  2.73754692e-02 -5.41170693e-04 -4.84844908e-01]\n",
      "[ episode 349 ][ timestamp 229 ] state=[-8.21220781e-01  2.73754692e-02 -5.41170693e-04 -4.84844908e-01], action=0, reward=1.0, next_state=[-0.82067327 -0.16773884 -0.01023807 -0.19233259]\n",
      "[ episode 349 ][ timestamp 230 ] state=[-0.82067327 -0.16773884 -0.01023807 -0.19233259], action=0, reward=1.0, next_state=[-0.82402805 -0.36271285 -0.01408472  0.09710315]\n",
      "[ episode 349 ][ timestamp 231 ] state=[-0.82402805 -0.36271285 -0.01408472  0.09710315], action=0, reward=1.0, next_state=[-0.8312823  -0.55763013 -0.01214266  0.38530931]\n",
      "[ episode 349 ][ timestamp 232 ] state=[-0.8312823  -0.55763013 -0.01214266  0.38530931], action=0, reward=1.0, next_state=[-0.84243491 -0.75257761 -0.00443647  0.67413912]\n",
      "[ episode 349 ][ timestamp 233 ] state=[-0.84243491 -0.75257761 -0.00443647  0.67413912], action=1, reward=1.0, next_state=[-0.85748646 -0.55739428  0.00904631  0.38006269]\n",
      "[ episode 349 ][ timestamp 234 ] state=[-0.85748646 -0.55739428  0.00904631  0.38006269], action=1, reward=1.0, next_state=[-0.86863435 -0.36240195  0.01664756  0.09024574]\n",
      "[ episode 349 ][ timestamp 235 ] state=[-0.86863435 -0.36240195  0.01664756  0.09024574], action=1, reward=1.0, next_state=[-0.87588238 -0.16752253  0.01845248 -0.19713873]\n",
      "[ episode 349 ][ timestamp 236 ] state=[-0.87588238 -0.16752253  0.01845248 -0.19713873], action=0, reward=1.0, next_state=[-0.87923284 -0.36290349  0.0145097   0.10130753]\n",
      "[ episode 349 ][ timestamp 237 ] state=[-0.87923284 -0.36290349  0.0145097   0.10130753], action=0, reward=1.0, next_state=[-0.8864909  -0.55823035  0.01653586  0.39853268]\n",
      "[ episode 349 ][ timestamp 238 ] state=[-0.8864909  -0.55823035  0.01653586  0.39853268], action=1, reward=1.0, next_state=[-0.89765551 -0.36334684  0.02450651  0.1111087 ]\n",
      "[ episode 349 ][ timestamp 239 ] state=[-0.89765551 -0.36334684  0.02450651  0.1111087 ], action=1, reward=1.0, next_state=[-0.90492245 -0.16858447  0.02672868 -0.17374294]\n",
      "[ episode 349 ][ timestamp 240 ] state=[-0.90492245 -0.16858447  0.02672868 -0.17374294], action=0, reward=1.0, next_state=[-0.90829414 -0.36407856  0.02325382  0.12725076]\n",
      "[ episode 349 ][ timestamp 241 ] state=[-0.90829414 -0.36407856  0.02325382  0.12725076], action=1, reward=1.0, next_state=[-0.91557571 -0.16929732  0.02579884 -0.1580061 ]\n",
      "[ episode 349 ][ timestamp 242 ] state=[-0.91557571 -0.16929732  0.02579884 -0.1580061 ], action=0, reward=1.0, next_state=[-0.91896166 -0.36477895  0.02263872  0.14270279]\n",
      "[ episode 349 ][ timestamp 243 ] state=[-0.91896166 -0.36477895  0.02263872  0.14270279], action=1, reward=1.0, next_state=[-0.92625723 -0.16998842  0.02549277 -0.14275293]\n",
      "[ episode 349 ][ timestamp 244 ] state=[-0.92625723 -0.16998842  0.02549277 -0.14275293], action=0, reward=1.0, next_state=[-0.929657   -0.36546601  0.02263771  0.15786226]\n",
      "[ episode 349 ][ timestamp 245 ] state=[-0.929657   -0.36546601  0.02263771  0.15786226], action=1, reward=1.0, next_state=[-0.93696632 -0.17067536  0.02579496 -0.12759394]\n",
      "[ episode 349 ][ timestamp 246 ] state=[-0.93696632 -0.17067536  0.02579496 -0.12759394], action=1, reward=1.0, next_state=[-0.94037983  0.02406774  0.02324308 -0.41202854]\n",
      "[ episode 349 ][ timestamp 247 ] state=[-0.94037983  0.02406774  0.02324308 -0.41202854], action=0, reward=1.0, next_state=[-0.93989848 -0.17137585  0.01500251 -0.11210948]\n",
      "[ episode 349 ][ timestamp 248 ] state=[-0.93989848 -0.17137585  0.01500251 -0.11210948], action=0, reward=1.0, next_state=[-0.94332599 -0.36670953  0.01276032  0.18526863]\n",
      "[ episode 349 ][ timestamp 249 ] state=[-0.94332599 -0.36670953  0.01276032  0.18526863], action=0, reward=1.0, next_state=[-0.95066018 -0.56201171  0.01646569  0.48194948]\n",
      "[ episode 349 ][ timestamp 250 ] state=[-0.95066018 -0.56201171  0.01646569  0.48194948], action=1, reward=1.0, next_state=[-0.96190042 -0.36712599  0.02610468  0.19450123]\n",
      "[ episode 349 ][ timestamp 251 ] state=[-0.96190042 -0.36712599  0.02610468  0.19450123], action=1, reward=1.0, next_state=[-0.96924294 -0.17238699  0.02999471 -0.08983384]\n",
      "[ episode 349 ][ timestamp 252 ] state=[-0.96924294 -0.17238699  0.02999471 -0.08983384], action=0, reward=1.0, next_state=[-0.97269068 -0.36792575  0.02819803  0.21215948]\n",
      "[ episode 349 ][ timestamp 253 ] state=[-0.97269068 -0.36792575  0.02819803  0.21215948], action=1, reward=1.0, next_state=[-0.98004919 -0.17321807  0.03244122 -0.0714968 ]\n",
      "[ episode 349 ][ timestamp 254 ] state=[-0.98004919 -0.17321807  0.03244122 -0.0714968 ], action=0, reward=1.0, next_state=[-0.98351355 -0.36878973  0.03101128  0.23124238]\n",
      "[ episode 349 ][ timestamp 255 ] state=[-0.98351355 -0.36878973  0.03101128  0.23124238], action=1, reward=1.0, next_state=[-0.99088935 -0.17412432  0.03563613 -0.05149949]\n",
      "[ episode 349 ][ timestamp 256 ] state=[-0.99088935 -0.17412432  0.03563613 -0.05149949], action=0, reward=1.0, next_state=[-0.99437183 -0.36973865  0.03460614  0.25221052]\n",
      "[ episode 349 ][ timestamp 257 ] state=[-0.99437183 -0.36973865  0.03460614  0.25221052], action=0, reward=1.0, next_state=[-1.00176661 -0.56533723  0.03965035  0.5556049 ]\n",
      "[ episode 349 ][ timestamp 258 ] state=[-1.00176661 -0.56533723  0.03965035  0.5556049 ], action=1, reward=1.0, next_state=[-1.01307335 -0.37079376  0.05076245  0.27567319]\n",
      "[ episode 349 ][ timestamp 259 ] state=[-1.01307335 -0.37079376  0.05076245  0.27567319], action=1, reward=1.0, next_state=[-1.02048923e+00 -1.76431417e-01  5.62759141e-02 -5.77016867e-04]\n",
      "[ episode 349 ][ timestamp 260 ] state=[-1.02048923e+00 -1.76431417e-01  5.62759141e-02 -5.77016867e-04], action=1, reward=1.0, next_state=[-1.02401786  0.01784019  0.05626437 -0.27498672]\n",
      "[ episode 349 ][ timestamp 261 ] state=[-1.02401786  0.01784019  0.05626437 -0.27498672], action=0, reward=1.0, next_state=[-1.02366105 -0.1780375   0.05076464  0.03489788]\n",
      "[ episode 349 ][ timestamp 262 ] state=[-1.02366105 -0.1780375   0.05076464  0.03489788], action=1, reward=1.0, next_state=[-1.0272218   0.0163211   0.0514626  -0.24134606]\n",
      "[ episode 349 ][ timestamp 263 ] state=[-1.0272218   0.0163211   0.0514626  -0.24134606], action=0, reward=1.0, next_state=[-1.02689538 -0.17949676  0.04663568  0.06711519]\n",
      "[ episode 349 ][ timestamp 264 ] state=[-1.02689538 -0.17949676  0.04663568  0.06711519], action=1, reward=1.0, next_state=[-1.03048532  0.01492662  0.04797798 -0.21049689]\n",
      "[ episode 349 ][ timestamp 265 ] state=[-1.03048532  0.01492662  0.04797798 -0.21049689], action=0, reward=1.0, next_state=[-1.03018678 -0.18084732  0.04376804  0.09692622]\n",
      "[ episode 349 ][ timestamp 266 ] state=[-1.03018678 -0.18084732  0.04376804  0.09692622], action=1, reward=1.0, next_state=[-1.03380373  0.01362089  0.04570657 -0.18163305]\n",
      "[ episode 349 ][ timestamp 267 ] state=[-1.03380373  0.01362089  0.04570657 -0.18163305], action=0, reward=1.0, next_state=[-1.03353131 -0.18212428  0.04207391  0.12511111]\n",
      "[ episode 349 ][ timestamp 268 ] state=[-1.03353131 -0.18212428  0.04207391  0.12511111], action=1, reward=1.0, next_state=[-1.0371738   0.01237045  0.04457613 -0.15400672]\n",
      "[ episode 349 ][ timestamp 269 ] state=[-1.0371738   0.01237045  0.04457613 -0.15400672], action=0, reward=1.0, next_state=[-1.03692639 -0.18336047  0.04149599  0.15239906]\n",
      "[ episode 349 ][ timestamp 270 ] state=[-1.03692639 -0.18336047  0.04149599  0.15239906], action=0, reward=1.0, next_state=[-1.0405936  -0.37905127  0.04454397  0.4578789 ]\n",
      "[ episode 349 ][ timestamp 271 ] state=[-1.0405936  -0.37905127  0.04454397  0.4578789 ], action=1, reward=1.0, next_state=[-1.04817462 -0.18458641  0.05370155  0.17956254]\n",
      "[ episode 349 ][ timestamp 272 ] state=[-1.04817462 -0.18458641  0.05370155  0.17956254], action=1, reward=1.0, next_state=[-1.05186635  0.00972759  0.0572928  -0.09570761]\n",
      "[ episode 349 ][ timestamp 273 ] state=[-1.05186635  0.00972759  0.0572928  -0.09570761], action=0, reward=1.0, next_state=[-1.0516718  -0.18616672  0.05537865  0.21448659]\n",
      "[ episode 349 ][ timestamp 274 ] state=[-1.0516718  -0.18616672  0.05537865  0.21448659], action=1, reward=1.0, next_state=[-1.05539513  0.00812158  0.05966838 -0.06022608]\n",
      "[ episode 349 ][ timestamp 275 ] state=[-1.05539513  0.00812158  0.05966838 -0.06022608], action=0, reward=1.0, next_state=[-1.0552327  -0.18780288  0.05846386  0.2506697 ]\n",
      "[ episode 349 ][ timestamp 276 ] state=[-1.0552327  -0.18780288  0.05846386  0.2506697 ], action=1, reward=1.0, next_state=[-1.05898876  0.00643758  0.06347726 -0.0230146 ]\n",
      "[ episode 349 ][ timestamp 277 ] state=[-1.05898876  0.00643758  0.06347726 -0.0230146 ], action=0, reward=1.0, next_state=[-1.05886001 -0.18953455  0.06301696  0.28900134]\n",
      "[ episode 349 ][ timestamp 278 ] state=[-1.05886001 -0.18953455  0.06301696  0.28900134], action=1, reward=1.0, next_state=[-1.0626507   0.00463482  0.06879699  0.01684012]\n",
      "[ episode 349 ][ timestamp 279 ] state=[-1.0626507   0.00463482  0.06879699  0.01684012], action=0, reward=1.0, next_state=[-1.062558   -0.19140287  0.06913379  0.3304114 ]\n",
      "[ episode 349 ][ timestamp 280 ] state=[-1.062558   -0.19140287  0.06913379  0.3304114 ], action=1, reward=1.0, next_state=[-1.06638606  0.00267035  0.07574202  0.06030612]\n",
      "[ episode 349 ][ timestamp 281 ] state=[-1.06638606  0.00267035  0.07574202  0.06030612], action=0, reward=1.0, next_state=[-1.06633265 -0.19345126  0.07694814  0.37589197]\n",
      "[ episode 349 ][ timestamp 282 ] state=[-1.06633265 -0.19345126  0.07694814  0.37589197], action=1, reward=1.0, next_state=[-1.07020168e+00  4.98225765e-04  8.44659824e-02  1.08429035e-01]\n",
      "[ episode 349 ][ timestamp 283 ] state=[-1.07020168e+00  4.98225765e-04  8.44659824e-02  1.08429035e-01], action=1, reward=1.0, next_state=[-1.07019171  0.19431459  0.08663456 -0.15645556]\n",
      "[ episode 349 ][ timestamp 284 ] state=[-1.07019171  0.19431459  0.08663456 -0.15645556], action=0, reward=1.0, next_state=[-1.06630542 -0.00193404  0.08350545  0.16225208]\n",
      "[ episode 349 ][ timestamp 285 ] state=[-1.06630542 -0.00193404  0.08350545  0.16225208], action=1, reward=1.0, next_state=[-1.0663441   0.19189935  0.08675049 -0.1029628 ]\n",
      "[ episode 349 ][ timestamp 286 ] state=[-1.0663441   0.19189935  0.08675049 -0.1029628 ], action=0, reward=1.0, next_state=[-1.06250612 -0.00435181  0.08469124  0.21577961]\n",
      "[ episode 349 ][ timestamp 287 ] state=[-1.06250612 -0.00435181  0.08469124  0.21577961], action=0, reward=1.0, next_state=[-1.06259315 -0.20057597  0.08900683  0.53393038]\n",
      "[ episode 349 ][ timestamp 288 ] state=[-1.06259315 -0.20057597  0.08900683  0.53393038], action=1, reward=1.0, next_state=[-1.06660467 -0.00681108  0.09968544  0.27056704]\n",
      "[ episode 349 ][ timestamp 289 ] state=[-1.06660467 -0.00681108  0.09968544  0.27056704], action=1, reward=1.0, next_state=[-1.06674089  0.18675751  0.10509678  0.0109146 ]\n",
      "[ episode 349 ][ timestamp 290 ] state=[-1.06674089  0.18675751  0.10509678  0.0109146 ], action=0, reward=1.0, next_state=[-1.06300574 -0.0097024   0.10531507  0.3348201 ]\n",
      "[ episode 349 ][ timestamp 291 ] state=[-1.06300574 -0.0097024   0.10531507  0.3348201 ], action=1, reward=1.0, next_state=[-1.06319979  0.18377538  0.11201147  0.0771168 ]\n",
      "[ episode 349 ][ timestamp 292 ] state=[-1.06319979  0.18377538  0.11201147  0.0771168 ], action=0, reward=1.0, next_state=[-1.05952428 -0.01275925  0.11355381  0.40293386]\n",
      "[ episode 349 ][ timestamp 293 ] state=[-1.05952428 -0.01275925  0.11355381  0.40293386], action=1, reward=1.0, next_state=[-1.05977947  0.18058445  0.12161249  0.14809922]\n",
      "[ episode 349 ][ timestamp 294 ] state=[-1.05977947  0.18058445  0.12161249  0.14809922], action=1, reward=1.0, next_state=[-1.05616778  0.373774    0.12457447 -0.10387885]\n",
      "[ episode 349 ][ timestamp 295 ] state=[-1.05616778  0.373774    0.12457447 -0.10387885], action=0, reward=1.0, next_state=[-1.0486923   0.17710737  0.12249689  0.22536527]\n",
      "[ episode 349 ][ timestamp 296 ] state=[-1.0486923   0.17710737  0.12249689  0.22536527], action=1, reward=1.0, next_state=[-1.04515015  0.37028507  0.1270042  -0.02630587]\n",
      "[ episode 349 ][ timestamp 297 ] state=[-1.04515015  0.37028507  0.1270042  -0.02630587], action=1, reward=1.0, next_state=[-1.03774445  0.56337853  0.12647808 -0.2763743 ]\n",
      "[ episode 349 ][ timestamp 298 ] state=[-1.03774445  0.56337853  0.12647808 -0.2763743 ], action=0, reward=1.0, next_state=[-1.02647688  0.36670037  0.12095059  0.05337193]\n",
      "[ episode 349 ][ timestamp 299 ] state=[-1.02647688  0.36670037  0.12095059  0.05337193], action=0, reward=1.0, next_state=[-1.01914287  0.17007046  0.12201803  0.38163488]\n",
      "[ episode 349 ][ timestamp 300 ] state=[-1.01914287  0.17007046  0.12201803  0.38163488], action=1, reward=1.0, next_state=[-1.01574146  0.36326768  0.12965073  0.12977802]\n",
      "[ episode 349 ][ timestamp 301 ] state=[-1.01574146  0.36326768  0.12965073  0.12977802], action=0, reward=1.0, next_state=[-1.00847611  0.16654984  0.13224629  0.46038885]\n",
      "[ episode 349 ][ timestamp 302 ] state=[-1.00847611  0.16654984  0.13224629  0.46038885], action=1, reward=1.0, next_state=[-1.00514511  0.35957891  0.14145407  0.21214067]\n",
      "[ episode 349 ][ timestamp 303 ] state=[-1.00514511  0.35957891  0.14145407  0.21214067], action=1, reward=1.0, next_state=[-0.99795353  0.55242488  0.14569688 -0.03279014]\n",
      "[ episode 349 ][ timestamp 304 ] state=[-0.99795353  0.55242488  0.14569688 -0.03279014], action=1, reward=1.0, next_state=[-0.98690504  0.74518963  0.14504108 -0.27619024]\n",
      "[ episode 349 ][ timestamp 305 ] state=[-0.98690504  0.74518963  0.14504108 -0.27619024], action=0, reward=1.0, next_state=[-0.97200124  0.54832829  0.13951727  0.05849392]\n",
      "[ episode 349 ][ timestamp 306 ] state=[-0.97200124  0.54832829  0.13951727  0.05849392], action=0, reward=1.0, next_state=[-0.96103468  0.35151029  0.14068715  0.3917374 ]\n",
      "[ episode 349 ][ timestamp 307 ] state=[-0.96103468  0.35151029  0.14068715  0.3917374 ], action=1, reward=1.0, next_state=[-0.95400447  0.5443846   0.1485219   0.14651009]\n",
      "[ episode 349 ][ timestamp 308 ] state=[-0.95400447  0.5443846   0.1485219   0.14651009], action=0, reward=1.0, next_state=[-0.94311678  0.34748239  0.1514521   0.4821169 ]\n",
      "[ episode 349 ][ timestamp 309 ] state=[-0.94311678  0.34748239  0.1514521   0.4821169 ], action=1, reward=1.0, next_state=[-0.93616713  0.54017862  0.16109444  0.24073812]\n",
      "[ episode 349 ][ timestamp 310 ] state=[-0.93616713  0.54017862  0.16109444  0.24073812], action=1, reward=1.0, next_state=[-0.92536356  0.7326767   0.1659092   0.00288679]\n",
      "[ episode 349 ][ timestamp 311 ] state=[-0.92536356  0.7326767   0.1659092   0.00288679], action=0, reward=1.0, next_state=[-0.91071003  0.53561207  0.16596694  0.34297862]\n",
      "[ episode 349 ][ timestamp 312 ] state=[-0.91071003  0.53561207  0.16596694  0.34297862], action=1, reward=1.0, next_state=[-0.89999778  0.72803197  0.17282651  0.1068854 ]\n",
      "[ episode 349 ][ timestamp 313 ] state=[-0.89999778  0.72803197  0.17282651  0.1068854 ], action=0, reward=1.0, next_state=[-0.88543715  0.53090896  0.17496422  0.44872342]\n",
      "[ episode 349 ][ timestamp 314 ] state=[-0.88543715  0.53090896  0.17496422  0.44872342], action=0, reward=1.0, next_state=[-0.87481897  0.33379988  0.18393869  0.7910505 ]\n",
      "[ episode 349 ][ timestamp 315 ] state=[-0.87481897  0.33379988  0.18393869  0.7910505 ], action=1, reward=1.0, next_state=[-0.86814297  0.52598421  0.1997597   0.56141052]\n",
      "[ episode 349 ][ timestamp 316 ] state=[-0.86814297  0.52598421  0.1997597   0.56141052], action=1, reward=-1.0, next_state=[-0.85762328  0.71782521  0.21098791  0.3377109 ]\n",
      "[ Ended! ] Episode 349: Exploration_rate=0.17475600159032884. Score=316.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 350 ] state=[-0.01924429 -0.0270123  -0.04969916 -0.03368877]\n",
      "[ episode 350 ][ timestamp 1 ] state=[-0.01924429 -0.0270123  -0.04969916 -0.03368877], action=0, reward=1.0, next_state=[-0.01978453 -0.22138762 -0.05037294  0.24290867]\n",
      "[ episode 350 ][ timestamp 2 ] state=[-0.01978453 -0.22138762 -0.05037294  0.24290867], action=1, reward=1.0, next_state=[-0.02421229 -0.02558369 -0.04551477 -0.06522806]\n",
      "[ episode 350 ][ timestamp 3 ] state=[-0.02421229 -0.02558369 -0.04551477 -0.06522806], action=0, reward=1.0, next_state=[-0.02472396 -0.22002453 -0.04681933  0.21275443]\n",
      "[ episode 350 ][ timestamp 4 ] state=[-0.02472396 -0.22002453 -0.04681933  0.21275443], action=1, reward=1.0, next_state=[-0.02912445 -0.02426554 -0.04256424 -0.09432213]\n",
      "[ episode 350 ][ timestamp 5 ] state=[-0.02912445 -0.02426554 -0.04256424 -0.09432213], action=0, reward=1.0, next_state=[-0.02960976 -0.2187524  -0.04445068  0.18463382]\n",
      "[ episode 350 ][ timestamp 6 ] state=[-0.02960976 -0.2187524  -0.04445068  0.18463382], action=1, reward=1.0, next_state=[-0.03398481 -0.02302356 -0.04075801 -0.12173364]\n",
      "[ episode 350 ][ timestamp 7 ] state=[-0.03398481 -0.02302356 -0.04075801 -0.12173364], action=0, reward=1.0, next_state=[-0.03444528 -0.21753859 -0.04319268  0.15781705]\n",
      "[ episode 350 ][ timestamp 8 ] state=[-0.03444528 -0.21753859 -0.04319268  0.15781705], action=0, reward=1.0, next_state=[-0.03879605 -0.41201638 -0.04003634  0.43656697]\n",
      "[ episode 350 ][ timestamp 9 ] state=[-0.03879605 -0.41201638 -0.04003634  0.43656697], action=1, reward=1.0, next_state=[-0.04703638 -0.21635124 -0.031305    0.13153692]\n",
      "[ episode 350 ][ timestamp 10 ] state=[-0.04703638 -0.21635124 -0.031305    0.13153692], action=0, reward=1.0, next_state=[-0.05136341 -0.4110111  -0.02867426  0.41418148]\n",
      "[ episode 350 ][ timestamp 11 ] state=[-0.05136341 -0.4110111  -0.02867426  0.41418148], action=1, reward=1.0, next_state=[-0.05958363 -0.2154947  -0.02039063  0.11259836]\n",
      "[ episode 350 ][ timestamp 12 ] state=[-0.05958363 -0.2154947  -0.02039063  0.11259836], action=0, reward=1.0, next_state=[-0.06389352 -0.41031863 -0.01813866  0.39877907]\n",
      "[ episode 350 ][ timestamp 13 ] state=[-0.06389352 -0.41031863 -0.01813866  0.39877907], action=1, reward=1.0, next_state=[-0.07209989 -0.21494411 -0.01016308  0.10043304]\n",
      "[ episode 350 ][ timestamp 14 ] state=[-0.07209989 -0.21494411 -0.01016308  0.10043304], action=1, reward=1.0, next_state=[-0.07639878 -0.019678   -0.00815442 -0.19543891]\n",
      "[ episode 350 ][ timestamp 15 ] state=[-0.07639878 -0.019678   -0.00815442 -0.19543891], action=0, reward=1.0, next_state=[-0.07679234 -0.21468236 -0.0120632   0.09466054]\n",
      "[ episode 350 ][ timestamp 16 ] state=[-0.07679234 -0.21468236 -0.0120632   0.09466054], action=0, reward=1.0, next_state=[-0.08108598 -0.40962935 -0.01016999  0.38351326]\n",
      "[ episode 350 ][ timestamp 17 ] state=[-0.08108598 -0.40962935 -0.01016999  0.38351326], action=1, reward=1.0, next_state=[-0.08927857 -0.2143645  -0.00249972  0.0876412 ]\n",
      "[ episode 350 ][ timestamp 18 ] state=[-0.08927857 -0.2143645  -0.00249972  0.0876412 ], action=0, reward=1.0, next_state=[-0.09356586 -0.40945053 -0.0007469   0.37953441]\n",
      "[ episode 350 ][ timestamp 19 ] state=[-0.09356586 -0.40945053 -0.0007469   0.37953441], action=1, reward=1.0, next_state=[-0.10175487 -0.21431798  0.00684379  0.08661608]\n",
      "[ episode 350 ][ timestamp 20 ] state=[-0.10175487 -0.21431798  0.00684379  0.08661608], action=1, reward=1.0, next_state=[-0.10604123 -0.01929479  0.00857611 -0.20389979]\n",
      "[ episode 350 ][ timestamp 21 ] state=[-0.10604123 -0.01929479  0.00857611 -0.20389979], action=0, reward=1.0, next_state=[-0.10642713 -0.21453833  0.00449812  0.0914761 ]\n",
      "[ episode 350 ][ timestamp 22 ] state=[-0.10642713 -0.21453833  0.00449812  0.0914761 ], action=1, reward=1.0, next_state=[-0.11071789 -0.01948114  0.00632764 -0.19978429]\n",
      "[ episode 350 ][ timestamp 23 ] state=[-0.11071789 -0.01948114  0.00632764 -0.19978429], action=0, reward=1.0, next_state=[-0.11110752 -0.21469302  0.00233195  0.09488798]\n",
      "[ episode 350 ][ timestamp 24 ] state=[-0.11110752 -0.21469302  0.00233195  0.09488798], action=1, reward=1.0, next_state=[-0.11540138 -0.01960457  0.00422971 -0.19705831]\n",
      "[ episode 350 ][ timestamp 25 ] state=[-0.11540138 -0.01960457  0.00422971 -0.19705831], action=0, reward=1.0, next_state=[-0.11579347 -0.21478677  0.00028855  0.0969559 ]\n",
      "[ episode 350 ][ timestamp 26 ] state=[-0.11579347 -0.21478677  0.00028855  0.0969559 ], action=1, reward=1.0, next_state=[-0.1200892  -0.01966895  0.00222766 -0.19563598]\n",
      "[ episode 350 ][ timestamp 27 ] state=[-0.1200892  -0.01966895  0.00222766 -0.19563598], action=0, reward=1.0, next_state=[-0.12048258 -0.2148227  -0.00168506  0.09774885]\n",
      "[ episode 350 ][ timestamp 28 ] state=[-0.12048258 -0.2148227  -0.00168506  0.09774885], action=1, reward=1.0, next_state=[-0.12477904 -0.01967664  0.00026992 -0.19546524]\n",
      "[ episode 350 ][ timestamp 29 ] state=[-0.12477904 -0.01967664  0.00026992 -0.19546524], action=0, reward=1.0, next_state=[-0.12517257 -0.21480245 -0.00363938  0.09730283]\n",
      "[ episode 350 ][ timestamp 30 ] state=[-0.12517257 -0.21480245 -0.00363938  0.09730283], action=1, reward=1.0, next_state=[-0.12946862 -0.01962852 -0.00169333 -0.19652609]\n",
      "[ episode 350 ][ timestamp 31 ] state=[-0.12946862 -0.01962852 -0.00169333 -0.19652609], action=0, reward=1.0, next_state=[-0.12986119 -0.21472621 -0.00562385  0.09562218]\n",
      "[ episode 350 ][ timestamp 32 ] state=[-0.12986119 -0.21472621 -0.00562385  0.09562218], action=1, reward=1.0, next_state=[-0.13415571 -0.01952411 -0.00371141 -0.19882974]\n",
      "[ episode 350 ][ timestamp 33 ] state=[-0.13415571 -0.01952411 -0.00371141 -0.19882974], action=0, reward=1.0, next_state=[-0.13454619 -0.21459278 -0.007688    0.0926801 ]\n",
      "[ episode 350 ][ timestamp 34 ] state=[-0.13454619 -0.21459278 -0.007688    0.0926801 ], action=0, reward=1.0, next_state=[-0.13883805 -0.4096037  -0.0058344   0.38292758]\n",
      "[ episode 350 ][ timestamp 35 ] state=[-0.13883805 -0.4096037  -0.0058344   0.38292758], action=1, reward=1.0, next_state=[-0.14703012 -0.2143994   0.00182415  0.08841081]\n",
      "[ episode 350 ][ timestamp 36 ] state=[-0.14703012 -0.2143994   0.00182415  0.08841081], action=1, reward=1.0, next_state=[-0.15131811 -0.01930364  0.00359237 -0.20369604]\n",
      "[ episode 350 ][ timestamp 37 ] state=[-0.15131811 -0.01930364  0.00359237 -0.20369604], action=0, reward=1.0, next_state=[-0.15170419 -0.21447678 -0.00048155  0.09011794]\n",
      "[ episode 350 ][ timestamp 38 ] state=[-0.15170419 -0.21447678 -0.00048155  0.09011794], action=1, reward=1.0, next_state=[-0.15599372 -0.01934793  0.00132081 -0.20271688]\n",
      "[ episode 350 ][ timestamp 39 ] state=[-0.15599372 -0.01934793  0.00132081 -0.20271688], action=0, reward=1.0, next_state=[-0.15638068 -0.21448875 -0.00273353  0.0903824 ]\n",
      "[ episode 350 ][ timestamp 40 ] state=[-0.15638068 -0.21448875 -0.00273353  0.0903824 ], action=1, reward=1.0, next_state=[-0.16067045 -0.01932772 -0.00092588 -0.2031617 ]\n",
      "[ episode 350 ][ timestamp 41 ] state=[-0.16067045 -0.01932772 -0.00092588 -0.2031617 ], action=0, reward=1.0, next_state=[-0.16105701 -0.21443642 -0.00498912  0.08922901]\n",
      "[ episode 350 ][ timestamp 42 ] state=[-0.16105701 -0.21443642 -0.00498912  0.08922901], action=0, reward=1.0, next_state=[-0.16534574 -0.40948651 -0.00320454  0.38033371]\n",
      "[ episode 350 ][ timestamp 43 ] state=[-0.16534574 -0.40948651 -0.00320454  0.38033371], action=1, reward=1.0, next_state=[-0.17353547 -0.2143192   0.00440214  0.08664211]\n",
      "[ episode 350 ][ timestamp 44 ] state=[-0.17353547 -0.2143192   0.00440214  0.08664211], action=1, reward=1.0, next_state=[-0.17782185 -0.01926062  0.00613498 -0.20464869]\n",
      "[ episode 350 ][ timestamp 45 ] state=[-0.17782185 -0.01926062  0.00613498 -0.20464869], action=0, reward=1.0, next_state=[-0.17820706 -0.21446977  0.00204201  0.08996319]\n",
      "[ episode 350 ][ timestamp 46 ] state=[-0.17820706 -0.21446977  0.00204201  0.08996319], action=1, reward=1.0, next_state=[-0.18249646 -0.01937715  0.00384127 -0.20207478]\n",
      "[ episode 350 ][ timestamp 47 ] state=[-0.18249646 -0.01937715  0.00384127 -0.20207478], action=0, reward=1.0, next_state=[-1.82884003e-01 -2.14553822e-01 -2.00225703e-04  9.18174012e-02]\n",
      "[ episode 350 ][ timestamp 48 ] state=[-1.82884003e-01 -2.14553822e-01 -2.00225703e-04  9.18174012e-02], action=1, reward=1.0, next_state=[-0.18717508 -0.019429    0.00163612 -0.20092869]\n",
      "[ episode 350 ][ timestamp 49 ] state=[-0.18717508 -0.019429    0.00163612 -0.20092869], action=0, reward=1.0, next_state=[-0.18756366 -0.21457431 -0.00238245  0.09226991]\n",
      "[ episode 350 ][ timestamp 50 ] state=[-0.18756366 -0.21457431 -0.00238245  0.09226991], action=1, reward=1.0, next_state=[-0.19185515 -0.0194183  -0.00053705 -0.20116373]\n",
      "[ episode 350 ][ timestamp 51 ] state=[-0.19185515 -0.0194183  -0.00053705 -0.20116373], action=0, reward=1.0, next_state=[-0.19224351 -0.21453256 -0.00456033  0.09134973]\n",
      "[ episode 350 ][ timestamp 52 ] state=[-0.19224351 -0.21453256 -0.00456033  0.09134973], action=1, reward=1.0, next_state=[-0.19653416 -0.01934554 -0.00273333 -0.20276848]\n",
      "[ episode 350 ][ timestamp 53 ] state=[-0.19653416 -0.01934554 -0.00273333 -0.20276848], action=0, reward=1.0, next_state=[-0.19692107 -0.2144283  -0.0067887   0.08905096]\n",
      "[ episode 350 ][ timestamp 54 ] state=[-0.19692107 -0.2144283  -0.0067887   0.08905096], action=1, reward=1.0, next_state=[-0.20120964 -0.0192097  -0.00500768 -0.20576605]\n",
      "[ episode 350 ][ timestamp 55 ] state=[-0.20120964 -0.0192097  -0.00500768 -0.20576605], action=0, reward=1.0, next_state=[-0.20159383 -0.21425969 -0.009123    0.085333  ]\n",
      "[ episode 350 ][ timestamp 56 ] state=[-0.20159383 -0.21425969 -0.009123    0.085333  ], action=0, reward=1.0, next_state=[-0.20587903 -0.40924968 -0.00741634  0.3751237 ]\n",
      "[ episode 350 ][ timestamp 57 ] state=[-0.20587903 -0.40924968 -0.00741634  0.3751237 ], action=1, reward=1.0, next_state=[-2.14064021e-01 -2.14023179e-01  8.61294124e-05  8.01116135e-02]\n",
      "[ episode 350 ][ timestamp 58 ] state=[-2.14064021e-01 -2.14023179e-01  8.61294124e-05  8.01116135e-02], action=1, reward=1.0, next_state=[-0.21834448 -0.01890246  0.00168836 -0.21254414]\n",
      "[ episode 350 ][ timestamp 59 ] state=[-0.21834448 -0.01890246  0.00168836 -0.21254414], action=0, reward=1.0, next_state=[-0.21872253 -0.21404851 -0.00256252  0.0806709 ]\n",
      "[ episode 350 ][ timestamp 60 ] state=[-0.21872253 -0.21404851 -0.00256252  0.0806709 ], action=1, reward=1.0, next_state=[-0.2230035  -0.01888992 -0.0009491  -0.21281941]\n",
      "[ episode 350 ][ timestamp 61 ] state=[-0.2230035  -0.01888992 -0.0009491  -0.21281941], action=1, reward=1.0, next_state=[-0.2233813   0.17624559 -0.00520549 -0.50580158]\n",
      "[ episode 350 ][ timestamp 62 ] state=[-0.2233813   0.17624559 -0.00520549 -0.50580158], action=0, reward=1.0, next_state=[-0.21985639 -0.01880262 -0.01532152 -0.21476363]\n",
      "[ episode 350 ][ timestamp 63 ] state=[-0.21985639 -0.01880262 -0.01532152 -0.21476363], action=0, reward=1.0, next_state=[-0.22023244 -0.21370222 -0.0196168   0.0730471 ]\n",
      "[ episode 350 ][ timestamp 64 ] state=[-0.22023244 -0.21370222 -0.0196168   0.0730471 ], action=0, reward=1.0, next_state=[-0.22450649 -0.40853753 -0.01815585  0.35947686]\n",
      "[ episode 350 ][ timestamp 65 ] state=[-0.22450649 -0.40853753 -0.01815585  0.35947686], action=1, reward=1.0, next_state=[-0.23267724 -0.21316225 -0.01096632  0.06112472]\n",
      "[ episode 350 ][ timestamp 66 ] state=[-0.23267724 -0.21316225 -0.01096632  0.06112472], action=0, reward=1.0, next_state=[-0.23694048 -0.40812527 -0.00974382  0.35032762]\n",
      "[ episode 350 ][ timestamp 67 ] state=[-0.23694048 -0.40812527 -0.00974382  0.35032762], action=1, reward=1.0, next_state=[-0.24510299 -0.21286611 -0.00273727  0.05458815]\n",
      "[ episode 350 ][ timestamp 68 ] state=[-0.24510299 -0.21286611 -0.00273727  0.05458815], action=0, reward=1.0, next_state=[-0.24936031 -0.4079487  -0.00164551  0.34640619]\n",
      "[ episode 350 ][ timestamp 69 ] state=[-0.24936031 -0.4079487  -0.00164551  0.34640619], action=1, reward=1.0, next_state=[-0.25751928 -0.21280338  0.00528262  0.05320483]\n",
      "[ episode 350 ][ timestamp 70 ] state=[-0.25751928 -0.21280338  0.00528262  0.05320483], action=0, reward=1.0, next_state=[-0.26177535 -0.40800068  0.00634671  0.34754977]\n",
      "[ episode 350 ][ timestamp 71 ] state=[-0.26177535 -0.40800068  0.00634671  0.34754977], action=1, reward=1.0, next_state=[-0.26993537 -0.21296957  0.01329771  0.05687493]\n",
      "[ episode 350 ][ timestamp 72 ] state=[-0.26993537 -0.21296957  0.01329771  0.05687493], action=1, reward=1.0, next_state=[-0.27419476 -0.01804079  0.01443521 -0.23158299]\n",
      "[ episode 350 ][ timestamp 73 ] state=[-0.27419476 -0.01804079  0.01443521 -0.23158299], action=1, reward=1.0, next_state=[-0.27455557  0.17687195  0.00980355 -0.51967784]\n",
      "[ episode 350 ][ timestamp 74 ] state=[-0.27455557  0.17687195  0.00980355 -0.51967784], action=1, reward=1.0, next_state=[-2.71018134e-01  3.71854527e-01 -5.90008524e-04 -8.09255446e-01]\n",
      "[ episode 350 ][ timestamp 75 ] state=[-2.71018134e-01  3.71854527e-01 -5.90008524e-04 -8.09255446e-01], action=0, reward=1.0, next_state=[-0.26358104  0.17674067 -0.01677512 -0.51675817]\n",
      "[ episode 350 ][ timestamp 76 ] state=[-0.26358104  0.17674067 -0.01677512 -0.51675817], action=0, reward=1.0, next_state=[-0.26004623 -0.01814111 -0.02711028 -0.22940829]\n",
      "[ episode 350 ][ timestamp 77 ] state=[-0.26004623 -0.01814111 -0.02711028 -0.22940829], action=0, reward=1.0, next_state=[-0.26040905 -0.21286537 -0.03169845  0.05460133]\n",
      "[ episode 350 ][ timestamp 78 ] state=[-0.26040905 -0.21286537 -0.03169845  0.05460133], action=0, reward=1.0, next_state=[-0.26466636 -0.40751881 -0.03060642  0.33711703]\n",
      "[ episode 350 ][ timestamp 79 ] state=[-0.26466636 -0.40751881 -0.03060642  0.33711703], action=1, reward=1.0, next_state=[-0.27281674 -0.21197499 -0.02386408  0.03494178]\n",
      "[ episode 350 ][ timestamp 80 ] state=[-0.27281674 -0.21197499 -0.02386408  0.03494178], action=0, reward=1.0, next_state=[-0.27705624 -0.40674673 -0.02316524  0.32000084]\n",
      "[ episode 350 ][ timestamp 81 ] state=[-0.27705624 -0.40674673 -0.02316524  0.32000084], action=1, reward=1.0, next_state=[-0.28519117 -0.21130267 -0.01676523  0.02010342]\n",
      "[ episode 350 ][ timestamp 82 ] state=[-0.28519117 -0.21130267 -0.01676523  0.02010342], action=0, reward=1.0, next_state=[-0.28941722 -0.40618022 -0.01636316  0.30744993]\n",
      "[ episode 350 ][ timestamp 83 ] state=[-0.28941722 -0.40618022 -0.01636316  0.30744993], action=1, reward=1.0, next_state=[-0.29754083 -0.21082898 -0.01021416  0.00965174]\n",
      "[ episode 350 ][ timestamp 84 ] state=[-0.29754083 -0.21082898 -0.01021416  0.00965174], action=1, reward=1.0, next_state=[-0.30175741 -0.01556204 -0.01002112 -0.28623629]\n",
      "[ episode 350 ][ timestamp 85 ] state=[-0.30175741 -0.01556204 -0.01002112 -0.28623629], action=0, reward=1.0, next_state=[-0.30206865 -0.21053965 -0.01574585  0.00326928]\n",
      "[ episode 350 ][ timestamp 86 ] state=[-0.30206865 -0.21053965 -0.01574585  0.00326928], action=0, reward=1.0, next_state=[-0.30627944 -0.40543229 -0.01568047  0.2909429 ]\n",
      "[ episode 350 ][ timestamp 87 ] state=[-0.30627944 -0.40543229 -0.01568047  0.2909429 ], action=1, reward=1.0, next_state=[-0.31438809 -0.2100903  -0.00986161 -0.00664393]\n",
      "[ episode 350 ][ timestamp 88 ] state=[-0.31438809 -0.2100903  -0.00986161 -0.00664393], action=0, reward=1.0, next_state=[-0.31858989 -0.40506945 -0.00999449  0.2829113 ]\n",
      "[ episode 350 ][ timestamp 89 ] state=[-0.31858989 -0.40506945 -0.00999449  0.2829113 ], action=1, reward=1.0, next_state=[-0.32669128 -0.20980638 -0.00433626 -0.012907  ]\n",
      "[ episode 350 ][ timestamp 90 ] state=[-0.32669128 -0.20980638 -0.00433626 -0.012907  ], action=0, reward=1.0, next_state=[-0.33088741 -0.40486587 -0.0045944   0.27840463]\n",
      "[ episode 350 ][ timestamp 91 ] state=[-0.33088741 -0.40486587 -0.0045944   0.27840463], action=1, reward=1.0, next_state=[-0.33898473 -0.20967868  0.00097369 -0.01572381]\n",
      "[ episode 350 ][ timestamp 92 ] state=[-0.33898473 -0.20967868  0.00097369 -0.01572381], action=0, reward=1.0, next_state=[-0.3431783  -0.40481458  0.00065922  0.27726617]\n",
      "[ episode 350 ][ timestamp 93 ] state=[-0.3431783  -0.40481458  0.00065922  0.27726617], action=1, reward=1.0, next_state=[-0.35127459 -0.20970204  0.00620454 -0.01520877]\n",
      "[ episode 350 ][ timestamp 94 ] state=[-0.35127459 -0.20970204  0.00620454 -0.01520877], action=0, reward=1.0, next_state=[-0.35546863 -0.40491242  0.00590036  0.27942529]\n",
      "[ episode 350 ][ timestamp 95 ] state=[-0.35546863 -0.40491242  0.00590036  0.27942529], action=1, reward=1.0, next_state=[-0.36356688 -0.20987514  0.01148887 -0.01139085]\n",
      "[ episode 350 ][ timestamp 96 ] state=[-0.36356688 -0.20987514  0.01148887 -0.01139085], action=0, reward=1.0, next_state=[-0.36776438 -0.40515995  0.01126105  0.28489469]\n",
      "[ episode 350 ][ timestamp 97 ] state=[-0.36776438 -0.40515995  0.01126105  0.28489469], action=1, reward=1.0, next_state=[-0.37586758 -0.2102004   0.01695895 -0.00421541]\n",
      "[ episode 350 ][ timestamp 98 ] state=[-0.37586758 -0.2102004   0.01695895 -0.00421541], action=1, reward=1.0, next_state=[-0.38007159 -0.01532572  0.01687464 -0.2914997 ]\n",
      "[ episode 350 ][ timestamp 99 ] state=[-0.38007159 -0.01532572  0.01687464 -0.2914997 ], action=0, reward=1.0, next_state=[-0.38037811 -0.21068417  0.01104464  0.00645715]\n",
      "[ episode 350 ][ timestamp 100 ] state=[-0.38037811 -0.21068417  0.01104464  0.00645715], action=0, reward=1.0, next_state=[-0.38459179 -0.40596276  0.01117379  0.30260423]\n",
      "[ episode 350 ][ timestamp 101 ] state=[-0.38459179 -0.40596276  0.01117379  0.30260423], action=1, reward=1.0, next_state=[-0.39271104 -0.21100182  0.01722587  0.01346611]\n",
      "[ episode 350 ][ timestamp 102 ] state=[-0.39271104 -0.21100182  0.01722587  0.01346611], action=0, reward=1.0, next_state=[-0.39693108 -0.40636653  0.01749519  0.31153385]\n",
      "[ episode 350 ][ timestamp 103 ] state=[-0.39693108 -0.40636653  0.01749519  0.31153385], action=1, reward=1.0, next_state=[-0.40505841 -0.21149814  0.02372587  0.02441932]\n",
      "[ episode 350 ][ timestamp 104 ] state=[-0.40505841 -0.21149814  0.02372587  0.02441932], action=1, reward=1.0, next_state=[-0.40928837 -0.01672433  0.02421426 -0.26068441]\n",
      "[ episode 350 ][ timestamp 105 ] state=[-0.40928837 -0.01672433  0.02421426 -0.26068441], action=0, reward=1.0, next_state=[-0.40962286 -0.21218342  0.01900057  0.03953658]\n",
      "[ episode 350 ][ timestamp 106 ] state=[-0.40962286 -0.21218342  0.01900057  0.03953658], action=0, reward=1.0, next_state=[-0.41386653 -0.40757262  0.0197913   0.33815329]\n",
      "[ episode 350 ][ timestamp 107 ] state=[-0.41386653 -0.40757262  0.0197913   0.33815329], action=1, reward=1.0, next_state=[-0.42201798 -0.2127378   0.02655437  0.05177657]\n",
      "[ episode 350 ][ timestamp 108 ] state=[-0.42201798 -0.2127378   0.02655437  0.05177657], action=1, reward=1.0, next_state=[-0.42627274 -0.01800648  0.0275899  -0.23241137]\n",
      "[ episode 350 ][ timestamp 109 ] state=[-0.42627274 -0.01800648  0.0275899  -0.23241137], action=0, reward=1.0, next_state=[-0.42663287 -0.21351157  0.02294167  0.06884505]\n",
      "[ episode 350 ][ timestamp 110 ] state=[-0.42663287 -0.21351157  0.02294167  0.06884505], action=1, reward=1.0, next_state=[-0.4309031  -0.01872591  0.02431857 -0.21651228]\n",
      "[ episode 350 ][ timestamp 111 ] state=[-0.4309031  -0.01872591  0.02431857 -0.21651228], action=0, reward=1.0, next_state=[-0.43127762 -0.21418692  0.01998833  0.08374149]\n",
      "[ episode 350 ][ timestamp 112 ] state=[-0.43127762 -0.21418692  0.01998833  0.08374149], action=1, reward=1.0, next_state=[-0.43556136 -0.01935711  0.02166316 -0.20256866]\n",
      "[ episode 350 ][ timestamp 113 ] state=[-0.43556136 -0.01935711  0.02166316 -0.20256866], action=0, reward=1.0, next_state=[-0.4359485  -0.21478207  0.01761178  0.09686847]\n",
      "[ episode 350 ][ timestamp 114 ] state=[-0.4359485  -0.21478207  0.01761178  0.09686847], action=1, reward=1.0, next_state=[-0.44024414 -0.01991691  0.01954915 -0.19020635]\n",
      "[ episode 350 ][ timestamp 115 ] state=[-0.44024414 -0.01991691  0.01954915 -0.19020635], action=1, reward=1.0, next_state=[-0.44064248  0.17491999  0.01574503 -0.47665878]\n",
      "[ episode 350 ][ timestamp 116 ] state=[-0.44064248  0.17491999  0.01574503 -0.47665878], action=0, reward=1.0, next_state=[-0.43714408 -0.02042069  0.00621185 -0.17905522]\n",
      "[ episode 350 ][ timestamp 117 ] state=[-0.43714408 -0.02042069  0.00621185 -0.17905522], action=0, reward=1.0, next_state=[-0.43755249 -0.21563098  0.00263075  0.11558084]\n",
      "[ episode 350 ][ timestamp 118 ] state=[-0.43755249 -0.21563098  0.00263075  0.11558084], action=1, reward=1.0, next_state=[-0.44186511 -0.02054683  0.00494236 -0.17627095]\n",
      "[ episode 350 ][ timestamp 119 ] state=[-0.44186511 -0.02054683  0.00494236 -0.17627095], action=0, reward=1.0, next_state=[-0.44227605 -0.21573916  0.00141694  0.11796703]\n",
      "[ episode 350 ][ timestamp 120 ] state=[-0.44227605 -0.21573916  0.00141694  0.11796703], action=1, reward=1.0, next_state=[-0.44659083 -0.02063754  0.00377628 -0.17426853]\n",
      "[ episode 350 ][ timestamp 121 ] state=[-0.44659083 -0.02063754  0.00377628 -0.17426853], action=0, reward=1.0, next_state=[-4.47003581e-01 -2.15813330e-01  2.90913375e-04  1.19603295e-01]\n",
      "[ episode 350 ][ timestamp 122 ] state=[-4.47003581e-01 -2.15813330e-01  2.90913375e-04  1.19603295e-01], action=1, reward=1.0, next_state=[-0.45131985 -0.02069555  0.00268298 -0.17298784]\n",
      "[ episode 350 ][ timestamp 123 ] state=[-0.45131985 -0.02069555  0.00268298 -0.17298784], action=0, reward=1.0, next_state=[-0.45173376 -0.2158558  -0.00077678  0.12054028]\n",
      "[ episode 350 ][ timestamp 124 ] state=[-0.45173376 -0.2158558  -0.00077678  0.12054028], action=1, reward=1.0, next_state=[-0.45605087 -0.02072272  0.00163403 -0.17238762]\n",
      "[ episode 350 ][ timestamp 125 ] state=[-0.45605087 -0.02072272  0.00163403 -0.17238762], action=0, reward=1.0, next_state=[-0.45646533 -0.21586802 -0.00181372  0.12081035]\n",
      "[ episode 350 ][ timestamp 126 ] state=[-0.45646533 -0.21586802 -0.00181372  0.12081035], action=1, reward=1.0, next_state=[-0.46078269 -0.02072013  0.00060248 -0.17244424]\n",
      "[ episode 350 ][ timestamp 127 ] state=[-0.46078269 -0.02072013  0.00060248 -0.17244424], action=0, reward=1.0, next_state=[-0.46119709 -0.2158507  -0.0028464   0.12042869]\n",
      "[ episode 350 ][ timestamp 128 ] state=[-0.46119709 -0.2158507  -0.0028464   0.12042869], action=1, reward=1.0, next_state=[-4.65514106e-01 -2.06880865e-02 -4.37828282e-04 -1.73150890e-01]\n",
      "[ episode 350 ][ timestamp 129 ] state=[-4.65514106e-01 -2.06880865e-02 -4.37828282e-04 -1.73150890e-01], action=0, reward=1.0, next_state=[-0.46592787 -0.21580377 -0.00390085  0.11939388]\n",
      "[ episode 350 ][ timestamp 130 ] state=[-0.46592787 -0.21580377 -0.00390085  0.11939388], action=1, reward=1.0, next_state=[-0.47024394 -0.02062615 -0.00151297 -0.17451717]\n",
      "[ episode 350 ][ timestamp 131 ] state=[-0.47024394 -0.02062615 -0.00151297 -0.17451717], action=1, reward=1.0, next_state=[-0.47065647  0.17451743 -0.00500331 -0.467677  ]\n",
      "[ episode 350 ][ timestamp 132 ] state=[-0.47065647  0.17451743 -0.00500331 -0.467677  ], action=0, reward=1.0, next_state=[-0.46716612 -0.02053348 -0.01435685 -0.17657527]\n",
      "[ episode 350 ][ timestamp 133 ] state=[-0.46716612 -0.02053348 -0.01435685 -0.17657527], action=0, reward=1.0, next_state=[-0.46757679 -0.21544706 -0.01788836  0.1115442 ]\n",
      "[ episode 350 ][ timestamp 134 ] state=[-0.46757679 -0.21544706 -0.01788836  0.1115442 ], action=1, reward=1.0, next_state=[-0.47188573 -0.02007341 -0.01565747 -0.18672829]\n",
      "[ episode 350 ][ timestamp 135 ] state=[-0.47188573 -0.02007341 -0.01565747 -0.18672829], action=0, reward=1.0, next_state=[-0.4722872  -0.21496788 -0.01939204  0.10097448]\n",
      "[ episode 350 ][ timestamp 136 ] state=[-0.4722872  -0.21496788 -0.01939204  0.10097448], action=1, reward=1.0, next_state=[-0.47658655 -0.01957346 -0.01737255 -0.19776295]\n",
      "[ episode 350 ][ timestamp 137 ] state=[-0.47658655 -0.01957346 -0.01737255 -0.19776295], action=0, reward=1.0, next_state=[-0.47697802 -0.21444267 -0.02132781  0.08938949]\n",
      "[ episode 350 ][ timestamp 138 ] state=[-0.47697802 -0.21444267 -0.02132781  0.08938949], action=1, reward=1.0, next_state=[-0.48126688 -0.01902161 -0.01954002 -0.20994534]\n",
      "[ episode 350 ][ timestamp 139 ] state=[-0.48126688 -0.01902161 -0.01954002 -0.20994534], action=0, reward=1.0, next_state=[-0.48164731 -0.2138588  -0.02373893  0.07651025]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 350 ][ timestamp 140 ] state=[-0.48164731 -0.2138588  -0.02373893  0.07651025], action=0, reward=1.0, next_state=[-0.48592449 -0.40863253 -0.02220872  0.36160995]\n",
      "[ episode 350 ][ timestamp 141 ] state=[-0.48592449 -0.40863253 -0.02220872  0.36160995], action=1, reward=1.0, next_state=[-0.49409714 -0.21320207 -0.01497652  0.06200771]\n",
      "[ episode 350 ][ timestamp 142 ] state=[-0.49409714 -0.21320207 -0.01497652  0.06200771], action=0, reward=1.0, next_state=[-0.49836118 -0.40810612 -0.01373637  0.34992807]\n",
      "[ episode 350 ][ timestamp 143 ] state=[-0.49836118 -0.40810612 -0.01373637  0.34992807], action=1, reward=1.0, next_state=[-0.5065233  -0.21279153 -0.00673781  0.05294547]\n",
      "[ episode 350 ][ timestamp 144 ] state=[-0.5065233  -0.21279153 -0.00673781  0.05294547], action=1, reward=1.0, next_state=[-0.51077913 -0.01757362 -0.0056789  -0.24185565]\n",
      "[ episode 350 ][ timestamp 145 ] state=[-0.51077913 -0.01757362 -0.0056789  -0.24185565], action=0, reward=1.0, next_state=[-0.5111306  -0.212614   -0.01051601  0.0490306 ]\n",
      "[ episode 350 ][ timestamp 146 ] state=[-0.5111306  -0.212614   -0.01051601  0.0490306 ], action=0, reward=1.0, next_state=[-0.51538288 -0.40758359 -0.0095354   0.33837717]\n",
      "[ episode 350 ][ timestamp 147 ] state=[-0.51538288 -0.40758359 -0.0095354   0.33837717], action=1, reward=1.0, next_state=[-0.52353455 -0.21232726 -0.00276785  0.04270263]\n",
      "[ episode 350 ][ timestamp 148 ] state=[-0.52353455 -0.21232726 -0.00276785  0.04270263], action=0, reward=1.0, next_state=[-0.5277811  -0.40740941 -0.0019138   0.33451099]\n",
      "[ episode 350 ][ timestamp 149 ] state=[-0.5277811  -0.40740941 -0.0019138   0.33451099], action=1, reward=1.0, next_state=[-0.53592929 -0.21226028  0.00477642  0.04122516]\n",
      "[ episode 350 ][ timestamp 150 ] state=[-0.53592929 -0.21226028  0.00477642  0.04122516], action=0, reward=1.0, next_state=[-0.54017449 -0.40745039  0.00560092  0.33541126]\n",
      "[ episode 350 ][ timestamp 151 ] state=[-0.54017449 -0.40745039  0.00560092  0.33541126], action=1, reward=1.0, next_state=[-0.5483235  -0.2124086   0.01230915  0.04449982]\n",
      "[ episode 350 ][ timestamp 152 ] state=[-0.5483235  -0.2124086   0.01230915  0.04449982], action=1, reward=1.0, next_state=[-0.55257167 -0.0174653   0.01319914 -0.24427418]\n",
      "[ episode 350 ][ timestamp 153 ] state=[-0.55257167 -0.0174653   0.01319914 -0.24427418], action=0, reward=1.0, next_state=[-0.55292098 -0.21277326  0.00831366  0.05254268]\n",
      "[ episode 350 ][ timestamp 154 ] state=[-0.55292098 -0.21277326  0.00831366  0.05254268], action=0, reward=1.0, next_state=[-0.55717644 -0.40801343  0.00936451  0.347837  ]\n",
      "[ episode 350 ][ timestamp 155 ] state=[-0.55717644 -0.40801343  0.00936451  0.347837  ], action=1, reward=1.0, next_state=[-0.56533671 -0.21302592  0.01632125  0.05812168]\n",
      "[ episode 350 ][ timestamp 156 ] state=[-0.56533671 -0.21302592  0.01632125  0.05812168], action=1, reward=1.0, next_state=[-0.56959723 -0.01814174  0.01748369 -0.22936742]\n",
      "[ episode 350 ][ timestamp 157 ] state=[-0.56959723 -0.01814174  0.01748369 -0.22936742], action=0, reward=1.0, next_state=[-0.56996007 -0.21350912  0.01289634  0.0687788 ]\n",
      "[ episode 350 ][ timestamp 158 ] state=[-0.56996007 -0.21350912  0.01289634  0.0687788 ], action=1, reward=1.0, next_state=[-0.57423025 -0.01857441  0.01427191 -0.21980753]\n",
      "[ episode 350 ][ timestamp 159 ] state=[-0.57423025 -0.01857441  0.01427191 -0.21980753], action=0, reward=1.0, next_state=[-0.57460174 -0.21389743  0.00987576  0.07734296]\n",
      "[ episode 350 ][ timestamp 160 ] state=[-0.57460174 -0.21389743  0.00987576  0.07734296], action=1, reward=1.0, next_state=[-0.57887969 -0.01891843  0.01142262 -0.21220784]\n",
      "[ episode 350 ][ timestamp 161 ] state=[-0.57887969 -0.01891843  0.01142262 -0.21220784], action=0, reward=1.0, next_state=[-0.57925805 -0.21420182  0.00717847  0.08405631]\n",
      "[ episode 350 ][ timestamp 162 ] state=[-0.57925805 -0.21420182  0.00717847  0.08405631], action=1, reward=1.0, next_state=[-0.58354209 -0.0191835   0.00885959 -0.20635318]\n",
      "[ episode 350 ][ timestamp 163 ] state=[-0.58354209 -0.0191835   0.00885959 -0.20635318], action=0, reward=1.0, next_state=[-0.58392576 -0.21443102  0.00473253  0.08911128]\n",
      "[ episode 350 ][ timestamp 164 ] state=[-0.58392576 -0.21443102  0.00473253  0.08911128], action=1, reward=1.0, next_state=[-0.58821438 -0.01937722  0.00651475 -0.20207478]\n",
      "[ episode 350 ][ timestamp 165 ] state=[-0.58821438 -0.01937722  0.00651475 -0.20207478], action=0, reward=1.0, next_state=[-0.58860193 -0.21459173  0.00247326  0.0926561 ]\n",
      "[ episode 350 ][ timestamp 166 ] state=[-0.58860193 -0.21459173  0.00247326  0.0926561 ], action=0, reward=1.0, next_state=[-0.59289376 -0.40974905  0.00432638  0.38611832]\n",
      "[ episode 350 ][ timestamp 167 ] state=[-0.59289376 -0.40974905  0.00432638  0.38611832], action=1, reward=1.0, next_state=[-0.60108874 -0.21468878  0.01204875  0.0948026 ]\n",
      "[ episode 350 ][ timestamp 168 ] state=[-0.60108874 -0.21468878  0.01204875  0.0948026 ], action=1, reward=1.0, next_state=[-0.60538252 -0.01974157  0.0139448  -0.19405473]\n",
      "[ episode 350 ][ timestamp 169 ] state=[-0.60538252 -0.01974157  0.0139448  -0.19405473], action=0, reward=1.0, next_state=[-0.60577735 -0.2150602   0.0100637   0.10299435]\n",
      "[ episode 350 ][ timestamp 170 ] state=[-0.60577735 -0.2150602   0.0100637   0.10299435], action=1, reward=1.0, next_state=[-0.61007855 -0.0200839   0.01212359 -0.1864966 ]\n",
      "[ episode 350 ][ timestamp 171 ] state=[-0.61007855 -0.0200839   0.01212359 -0.1864966 ], action=1, reward=1.0, next_state=[-0.61048023  0.17486251  0.00839366 -0.47533048]\n",
      "[ episode 350 ][ timestamp 172 ] state=[-0.61048023  0.17486251  0.00839366 -0.47533048], action=0, reward=1.0, next_state=[-0.60698298 -0.02037696 -0.00111295 -0.18001389]\n",
      "[ episode 350 ][ timestamp 173 ] state=[-0.60698298 -0.02037696 -0.00111295 -0.18001389], action=0, reward=1.0, next_state=[-0.60739052 -0.21548296 -0.00471323  0.11231774]\n",
      "[ episode 350 ][ timestamp 174 ] state=[-0.60739052 -0.21548296 -0.00471323  0.11231774], action=1, reward=1.0, next_state=[-0.61170018 -0.0202938  -0.00246687 -0.18184845]\n",
      "[ episode 350 ][ timestamp 175 ] state=[-0.61170018 -0.0202938  -0.00246687 -0.18184845], action=0, reward=1.0, next_state=[-0.61210605 -0.21538036 -0.00610384  0.11005525]\n",
      "[ episode 350 ][ timestamp 176 ] state=[-0.61210605 -0.21538036 -0.00610384  0.11005525], action=1, reward=1.0, next_state=[-0.61641366 -0.02017148 -0.00390274 -0.18454714]\n",
      "[ episode 350 ][ timestamp 177 ] state=[-0.61641366 -0.02017148 -0.00390274 -0.18454714], action=0, reward=1.0, next_state=[-0.61681709 -0.21523737 -0.00759368  0.10690207]\n",
      "[ episode 350 ][ timestamp 178 ] state=[-0.61681709 -0.21523737 -0.00759368  0.10690207], action=1, reward=1.0, next_state=[-0.62112184 -0.02000743 -0.00545564 -0.18816692]\n",
      "[ episode 350 ][ timestamp 179 ] state=[-0.62112184 -0.02000743 -0.00545564 -0.18816692], action=0, reward=1.0, next_state=[-0.62152199 -0.2150509  -0.00921898  0.10278998]\n",
      "[ episode 350 ][ timestamp 180 ] state=[-0.62152199 -0.2150509  -0.00921898  0.10278998], action=1, reward=1.0, next_state=[-0.62582301 -0.01979805 -0.00716318 -0.19278718]\n",
      "[ episode 350 ][ timestamp 181 ] state=[-0.62582301 -0.01979805 -0.00716318 -0.19278718], action=0, reward=1.0, next_state=[-0.62621897 -0.21481681 -0.01101892  0.09762748]\n",
      "[ episode 350 ][ timestamp 182 ] state=[-0.62621897 -0.21481681 -0.01101892  0.09762748], action=1, reward=1.0, next_state=[-0.6305153  -0.01953868 -0.00906637 -0.19851142]\n",
      "[ episode 350 ][ timestamp 183 ] state=[-0.6305153  -0.01953868 -0.00906637 -0.19851142], action=0, reward=1.0, next_state=[-0.63090608 -0.21452979 -0.0130366   0.09129774]\n",
      "[ episode 350 ][ timestamp 184 ] state=[-0.63090608 -0.21452979 -0.0130366   0.09129774], action=0, reward=1.0, next_state=[-0.63519667 -0.40946248 -0.01121065  0.37983928]\n",
      "[ episode 350 ][ timestamp 185 ] state=[-0.63519667 -0.40946248 -0.01121065  0.37983928], action=1, reward=1.0, next_state=[-0.64338592 -0.21418314 -0.00361386  0.08364281]\n",
      "[ episode 350 ][ timestamp 186 ] state=[-0.64338592 -0.21418314 -0.00361386  0.08364281], action=1, reward=1.0, next_state=[-0.64766958 -0.01900957 -0.001941   -0.2101781 ]\n",
      "[ episode 350 ][ timestamp 187 ] state=[-0.64766958 -0.01900957 -0.001941   -0.2101781 ], action=1, reward=1.0, next_state=[-0.64804978  0.17614008 -0.00614457 -0.50347268]\n",
      "[ episode 350 ][ timestamp 188 ] state=[-0.64804978  0.17614008 -0.00614457 -0.50347268], action=0, reward=1.0, next_state=[-0.64452697 -0.01889473 -0.01621402 -0.21273247]\n",
      "[ episode 350 ][ timestamp 189 ] state=[-0.64452697 -0.01889473 -0.01621402 -0.21273247], action=0, reward=1.0, next_state=[-0.64490487 -0.21378116 -0.02046867  0.07479203]\n",
      "[ episode 350 ][ timestamp 190 ] state=[-0.64490487 -0.21378116 -0.02046867  0.07479203], action=1, reward=1.0, next_state=[-0.64918049 -0.01837184 -0.01897283 -0.22427792]\n",
      "[ episode 350 ][ timestamp 191 ] state=[-0.64918049 -0.01837184 -0.01897283 -0.22427792], action=0, reward=1.0, next_state=[-0.64954793 -0.21321756 -0.02345839  0.06236038]\n",
      "[ episode 350 ][ timestamp 192 ] state=[-0.64954793 -0.21321756 -0.02345839  0.06236038], action=0, reward=1.0, next_state=[-0.65381228 -0.40799545 -0.02221118  0.3475507 ]\n",
      "[ episode 350 ][ timestamp 193 ] state=[-0.65381228 -0.40799545 -0.02221118  0.3475507 ], action=1, reward=1.0, next_state=[-0.66197219 -0.21256473 -0.01526016  0.04794739]\n",
      "[ episode 350 ][ timestamp 194 ] state=[-0.66197219 -0.21256473 -0.01526016  0.04794739], action=0, reward=1.0, next_state=[-0.66622348 -0.40746458 -0.01430122  0.3357768 ]\n",
      "[ episode 350 ][ timestamp 195 ] state=[-0.66622348 -0.40746458 -0.01430122  0.3357768 ], action=0, reward=1.0, next_state=[-0.67437277 -0.60238011 -0.00758568  0.62391579]\n",
      "[ episode 350 ][ timestamp 196 ] state=[-0.67437277 -0.60238011 -0.00758568  0.62391579], action=1, reward=1.0, next_state=[-0.68642038 -0.40715308  0.00489264  0.3288535 ]\n",
      "[ episode 350 ][ timestamp 197 ] state=[-0.68642038 -0.40715308  0.00489264  0.3288535 ], action=1, reward=1.0, next_state=[-0.69456344 -0.21210112  0.01146971  0.03771749]\n",
      "[ episode 350 ][ timestamp 198 ] state=[-0.69456344 -0.21210112  0.01146971  0.03771749], action=1, reward=1.0, next_state=[-0.69880546 -0.01714551  0.01222405 -0.25132468]\n",
      "[ episode 350 ][ timestamp 199 ] state=[-0.69880546 -0.01714551  0.01222405 -0.25132468], action=0, reward=1.0, next_state=[-0.69914837 -0.21243987  0.00719756  0.04518876]\n",
      "[ episode 350 ][ timestamp 200 ] state=[-0.69914837 -0.21243987  0.00719756  0.04518876], action=1, reward=1.0, next_state=[-0.70339717 -0.01742186  0.00810134 -0.24521461]\n",
      "[ episode 350 ][ timestamp 201 ] state=[-0.70339717 -0.01742186  0.00810134 -0.24521461], action=0, reward=1.0, next_state=[-0.70374561 -0.21265858  0.00319704  0.05001262]\n",
      "[ episode 350 ][ timestamp 202 ] state=[-0.70374561 -0.21265858  0.00319704  0.05001262], action=1, reward=1.0, next_state=[-0.70799878 -0.01758261  0.0041973  -0.2416599 ]\n",
      "[ episode 350 ][ timestamp 203 ] state=[-0.70799878 -0.01758261  0.0041973  -0.2416599 ], action=0, reward=1.0, next_state=[-7.08350430e-01 -2.12764269e-01 -6.35901200e-04  5.23440075e-02]\n",
      "[ episode 350 ][ timestamp 204 ] state=[-7.08350430e-01 -2.12764269e-01 -6.35901200e-04  5.23440075e-02], action=1, reward=1.0, next_state=[-7.12605715e-01 -1.76332055e-02  4.10978949e-04 -2.40539484e-01]\n",
      "[ episode 350 ][ timestamp 205 ] state=[-7.12605715e-01 -1.76332055e-02  4.10978949e-04 -2.40539484e-01], action=0, reward=1.0, next_state=[-0.71295838 -0.21276103 -0.00439981  0.05227305]\n",
      "[ episode 350 ][ timestamp 206 ] state=[-0.71295838 -0.21276103 -0.00439981  0.05227305], action=1, reward=1.0, next_state=[-0.7172136  -0.01757626 -0.00335435 -0.2417948 ]\n",
      "[ episode 350 ][ timestamp 207 ] state=[-0.7172136  -0.01757626 -0.00335435 -0.2417948 ], action=0, reward=1.0, next_state=[-0.71756512 -0.21265014 -0.00819025  0.04982819]\n",
      "[ episode 350 ][ timestamp 208 ] state=[-0.71756512 -0.21265014 -0.00819025  0.04982819], action=1, reward=1.0, next_state=[-0.72181813 -0.01741171 -0.00719368 -0.24542753]\n",
      "[ episode 350 ][ timestamp 209 ] state=[-0.72181813 -0.01741171 -0.00719368 -0.24542753], action=0, reward=1.0, next_state=[-0.72216636 -0.21243018 -0.01210223  0.04497768]\n",
      "[ episode 350 ][ timestamp 210 ] state=[-0.72216636 -0.21243018 -0.01210223  0.04497768], action=0, reward=1.0, next_state=[-0.72641497 -0.40737652 -0.01120268  0.3338178 ]\n",
      "[ episode 350 ][ timestamp 211 ] state=[-0.72641497 -0.40737652 -0.01120268  0.3338178 ], action=1, reward=1.0, next_state=[-0.7345625  -0.21209693 -0.00452632  0.03762328]\n",
      "[ episode 350 ][ timestamp 212 ] state=[-0.7345625  -0.21209693 -0.00452632  0.03762328], action=1, reward=1.0, next_state=[-0.73880443 -0.01691037 -0.00377386 -0.2564843 ]\n",
      "[ episode 350 ][ timestamp 213 ] state=[-0.73880443 -0.01691037 -0.00377386 -0.2564843 ], action=0, reward=1.0, next_state=[-0.73914264 -0.21197823 -0.00890354  0.03500591]\n",
      "[ episode 350 ][ timestamp 214 ] state=[-0.73914264 -0.21197823 -0.00890354  0.03500591], action=0, reward=1.0, next_state=[-0.74338221 -0.40697138 -0.00820343  0.32486643]\n",
      "[ episode 350 ][ timestamp 215 ] state=[-0.74338221 -0.40697138 -0.00820343  0.32486643], action=1, reward=1.0, next_state=[-0.75152163 -0.21173359 -0.0017061   0.02960782]\n",
      "[ episode 350 ][ timestamp 216 ] state=[-0.75152163 -0.21173359 -0.0017061   0.02960782], action=1, reward=1.0, next_state=[-0.75575631 -0.01658722 -0.00111394 -0.26361291]\n",
      "[ episode 350 ][ timestamp 217 ] state=[-0.75575631 -0.01658722 -0.00111394 -0.26361291], action=0, reward=1.0, next_state=[-0.75608805 -0.21169325 -0.0063862   0.02871846]\n",
      "[ episode 350 ][ timestamp 218 ] state=[-0.75608805 -0.21169325 -0.0063862   0.02871846], action=0, reward=1.0, next_state=[-0.76032192 -0.40672304 -0.00581183  0.31937965]\n",
      "[ episode 350 ][ timestamp 219 ] state=[-0.76032192 -0.40672304 -0.00581183  0.31937965], action=1, reward=1.0, next_state=[-7.68456376e-01 -2.11518801e-01  5.75763418e-04  2.48695648e-02]\n",
      "[ episode 350 ][ timestamp 220 ] state=[-7.68456376e-01 -2.11518801e-01  5.75763418e-04  2.48695648e-02], action=1, reward=1.0, next_state=[-0.77268675 -0.01640511  0.00107315 -0.26763165]\n",
      "[ episode 350 ][ timestamp 221 ] state=[-0.77268675 -0.01640511  0.00107315 -0.26763165], action=0, reward=1.0, next_state=[-0.77301485 -0.21154236 -0.00427948  0.02538957]\n",
      "[ episode 350 ][ timestamp 222 ] state=[-0.77301485 -0.21154236 -0.00427948  0.02538957], action=0, reward=1.0, next_state=[-0.7772457  -0.40660268 -0.00377169  0.3167192 ]\n",
      "[ episode 350 ][ timestamp 223 ] state=[-0.7772457  -0.40660268 -0.00377169  0.3167192 ], action=1, reward=1.0, next_state=[-0.78537776 -0.21142721  0.0025627   0.02284921]\n",
      "[ episode 350 ][ timestamp 224 ] state=[-0.78537776 -0.21142721  0.0025627   0.02284921], action=0, reward=1.0, next_state=[-0.7896063  -0.40658582  0.00301968  0.31633959]\n",
      "[ episode 350 ][ timestamp 225 ] state=[-0.7896063  -0.40658582  0.00301968  0.31633959], action=1, reward=1.0, next_state=[-0.79773802 -0.21150701  0.00934647  0.0246105 ]\n",
      "[ episode 350 ][ timestamp 226 ] state=[-0.79773802 -0.21150701  0.00934647  0.0246105 ], action=1, reward=1.0, next_state=[-0.80196816 -0.01652033  0.00983868 -0.26510892]\n",
      "[ episode 350 ][ timestamp 227 ] state=[-0.80196816 -0.01652033  0.00983868 -0.26510892], action=0, reward=1.0, next_state=[-0.80229856 -0.21178132  0.0045365   0.03066091]\n",
      "[ episode 350 ][ timestamp 228 ] state=[-0.80229856 -0.21178132  0.0045365   0.03066091], action=1, reward=1.0, next_state=[-0.80653419 -0.01672472  0.00514972 -0.26058725]\n",
      "[ episode 350 ][ timestamp 229 ] state=[-0.80653419 -0.01672472  0.00514972 -0.26058725], action=0, reward=1.0, next_state=[-8.06868684e-01 -2.11919803e-01 -6.20220097e-05  3.37155019e-02]\n",
      "[ episode 350 ][ timestamp 230 ] state=[-8.06868684e-01 -2.11919803e-01 -6.20220097e-05  3.37155019e-02], action=1, reward=1.0, next_state=[-8.11107080e-01 -1.67969620e-02  6.12288028e-04 -2.58986993e-01]\n",
      "[ episode 350 ][ timestamp 231 ] state=[-8.11107080e-01 -1.67969620e-02  6.12288028e-04 -2.58986993e-01], action=0, reward=1.0, next_state=[-0.81144302 -0.21192765 -0.00456745  0.033889  ]\n",
      "[ episode 350 ][ timestamp 232 ] state=[-0.81144302 -0.21192765 -0.00456745  0.033889  ], action=1, reward=1.0, next_state=[-0.81568157 -0.0167405  -0.00388967 -0.2602315 ]\n",
      "[ episode 350 ][ timestamp 233 ] state=[-0.81568157 -0.0167405  -0.00388967 -0.2602315 ], action=0, reward=1.0, next_state=[-0.81601638 -0.21180671 -0.0090943   0.03122204]\n",
      "[ episode 350 ][ timestamp 234 ] state=[-0.81601638 -0.21180671 -0.0090943   0.03122204], action=1, reward=1.0, next_state=[-0.82025252 -0.01655553 -0.00846986 -0.26431631]\n",
      "[ episode 350 ][ timestamp 235 ] state=[-0.82025252 -0.01655553 -0.00846986 -0.26431631], action=0, reward=1.0, next_state=[-0.82058363 -0.21155557 -0.01375619  0.02568315]\n",
      "[ episode 350 ][ timestamp 236 ] state=[-0.82058363 -0.21155557 -0.01375619  0.02568315], action=0, reward=1.0, next_state=[-0.82481474 -0.40647757 -0.01324252  0.3139943 ]\n",
      "[ episode 350 ][ timestamp 237 ] state=[-0.82481474 -0.40647757 -0.01324252  0.3139943 ], action=1, reward=1.0, next_state=[-0.83294429 -0.2111695  -0.00696264  0.0171647 ]\n",
      "[ episode 350 ][ timestamp 238 ] state=[-0.83294429 -0.2111695  -0.00696264  0.0171647 ], action=0, reward=1.0, next_state=[-0.83716768 -0.40619091 -0.00661934  0.30764272]\n",
      "[ episode 350 ][ timestamp 239 ] state=[-0.83716768 -0.40619091 -0.00661934  0.30764272], action=1, reward=1.0, next_state=[-8.45291498e-01 -2.10975272e-01 -4.66490124e-04  1.28796009e-02]\n",
      "[ episode 350 ][ timestamp 240 ] state=[-8.45291498e-01 -2.10975272e-01 -4.66490124e-04  1.28796009e-02], action=1, reward=1.0, next_state=[-8.49511003e-01 -1.58466335e-02 -2.08898107e-04 -2.79950473e-01]\n",
      "[ episode 350 ][ timestamp 241 ] state=[-8.49511003e-01 -1.58466335e-02 -2.08898107e-04 -2.79950473e-01], action=0, reward=1.0, next_state=[-0.84982794 -0.2109656  -0.00580791  0.01266656]\n",
      "[ episode 350 ][ timestamp 242 ] state=[-0.84982794 -0.2109656  -0.00580791  0.01266656], action=1, reward=1.0, next_state=[-0.85404725 -0.01576084 -0.00555458 -0.28184316]\n",
      "[ episode 350 ][ timestamp 243 ] state=[-0.85404725 -0.01576084 -0.00555458 -0.28184316], action=0, reward=1.0, next_state=[-0.85436246 -0.21080313 -0.01119144  0.00908272]\n",
      "[ episode 350 ][ timestamp 244 ] state=[-0.85436246 -0.21080313 -0.01119144  0.00908272], action=1, reward=1.0, next_state=[-0.85857853 -0.01552248 -0.01100979 -0.28711013]\n",
      "[ episode 350 ][ timestamp 245 ] state=[-0.85857853 -0.01552248 -0.01100979 -0.28711013], action=0, reward=1.0, next_state=[-0.85888898 -0.2104857  -0.01675199  0.00208017]\n",
      "[ episode 350 ][ timestamp 246 ] state=[-0.85888898 -0.2104857  -0.01675199  0.00208017], action=0, reward=1.0, next_state=[-0.86309869 -0.40536345 -0.01671038  0.28943092]\n",
      "[ episode 350 ][ timestamp 247 ] state=[-0.86309869 -0.40536345 -0.01671038  0.28943092], action=1, reward=1.0, next_state=[-0.87120596 -0.21000725 -0.01092177 -0.00847509]\n",
      "[ episode 350 ][ timestamp 248 ] state=[-0.87120596 -0.21000725 -0.01092177 -0.00847509], action=0, reward=1.0, next_state=[-0.8754061  -0.40497088 -0.01109127  0.28074198]\n",
      "[ episode 350 ][ timestamp 249 ] state=[-0.8754061  -0.40497088 -0.01109127  0.28074198], action=1, reward=1.0, next_state=[-0.88350552 -0.20969248 -0.00547643 -0.01541836]\n",
      "[ episode 350 ][ timestamp 250 ] state=[-0.88350552 -0.20969248 -0.00547643 -0.01541836], action=0, reward=1.0, next_state=[-0.88769937 -0.40473547 -0.0057848   0.27553167]\n",
      "[ episode 350 ][ timestamp 251 ] state=[-0.88769937 -0.40473547 -0.0057848   0.27553167], action=1, reward=1.0, next_state=[-8.95794081e-01 -2.09531463e-01 -2.74162029e-04 -1.89701623e-02]\n",
      "[ episode 350 ][ timestamp 252 ] state=[-8.95794081e-01 -2.09531463e-01 -2.74162029e-04 -1.89701623e-02], action=0, reward=1.0, next_state=[-8.99984711e-01 -4.04649482e-01 -6.53565275e-04  2.73626251e-01]\n",
      "[ episode 350 ][ timestamp 253 ] state=[-8.99984711e-01 -4.04649482e-01 -6.53565275e-04  2.73626251e-01], action=1, reward=1.0, next_state=[-0.9080777  -0.20951821  0.00481896 -0.01926274]\n",
      "[ episode 350 ][ timestamp 254 ] state=[-0.9080777  -0.20951821  0.00481896 -0.01926274], action=0, reward=1.0, next_state=[-0.91226806 -0.40470894  0.0044337   0.27493672]\n",
      "[ episode 350 ][ timestamp 255 ] state=[-0.91226806 -0.40470894  0.0044337   0.27493672], action=0, reward=1.0, next_state=[-0.92036224 -0.59989387  0.00993244  0.56901474]\n",
      "[ episode 350 ][ timestamp 256 ] state=[-0.92036224 -0.59989387  0.00993244  0.56901474], action=1, reward=1.0, next_state=[-0.93236012 -0.40491262  0.02131273  0.27947739]\n",
      "[ episode 350 ][ timestamp 257 ] state=[-0.93236012 -0.40491262  0.02131273  0.27947739], action=1, reward=1.0, next_state=[-0.94045837 -0.21010108  0.02690228 -0.00640808]\n",
      "[ episode 350 ][ timestamp 258 ] state=[-0.94045837 -0.21010108  0.02690228 -0.00640808], action=1, reward=1.0, next_state=[-0.94466039 -0.01537507  0.02677412 -0.29048309]\n",
      "[ episode 350 ][ timestamp 259 ] state=[-0.94466039 -0.01537507  0.02677412 -0.29048309], action=0, reward=1.0, next_state=[-0.9449679  -0.21086837  0.02096446  0.0105224 ]\n",
      "[ episode 350 ][ timestamp 260 ] state=[-0.9449679  -0.21086837  0.02096446  0.0105224 ], action=0, reward=1.0, next_state=[-0.94918526 -0.4062846   0.02117491  0.30974544]\n",
      "[ episode 350 ][ timestamp 261 ] state=[-0.94918526 -0.4062846   0.02117491  0.30974544], action=1, reward=1.0, next_state=[-0.95731096 -0.21147065  0.02736982  0.02381498]\n",
      "[ episode 350 ][ timestamp 262 ] state=[-0.95731096 -0.21147065  0.02736982  0.02381498], action=0, reward=1.0, next_state=[-0.96154037 -0.4069742   0.02784611  0.32500619]\n",
      "[ episode 350 ][ timestamp 263 ] state=[-0.96154037 -0.4069742   0.02784611  0.32500619], action=1, reward=1.0, next_state=[-0.96967985 -0.21225957  0.03434624  0.04123318]\n",
      "[ episode 350 ][ timestamp 264 ] state=[-0.96967985 -0.21225957  0.03434624  0.04123318], action=1, reward=1.0, next_state=[-0.97392504 -0.01764654  0.0351709  -0.24041839]\n",
      "[ episode 350 ][ timestamp 265 ] state=[-0.97392504 -0.01764654  0.0351709  -0.24041839], action=0, reward=1.0, next_state=[-0.97427797 -0.2132528   0.03036253  0.06314766]\n",
      "[ episode 350 ][ timestamp 266 ] state=[-0.97427797 -0.2132528   0.03036253  0.06314766], action=1, reward=1.0, next_state=[-0.97854303 -0.01857903  0.03162549 -0.21980319]\n",
      "[ episode 350 ][ timestamp 267 ] state=[-0.97854303 -0.01857903  0.03162549 -0.21980319], action=0, reward=1.0, next_state=[-0.97891461 -0.21413844  0.02722942  0.08268558]\n",
      "[ episode 350 ][ timestamp 268 ] state=[-0.97891461 -0.21413844  0.02722942  0.08268558], action=1, reward=1.0, next_state=[-0.98319738 -0.01941719  0.02888314 -0.20128356]\n",
      "[ episode 350 ][ timestamp 269 ] state=[-0.98319738 -0.01941719  0.02888314 -0.20128356], action=0, reward=1.0, next_state=[-0.98358572 -0.21494006  0.02485746  0.10036888]\n",
      "[ episode 350 ][ timestamp 270 ] state=[-0.98358572 -0.21494006  0.02485746  0.10036888], action=1, reward=1.0, next_state=[-0.98788453 -0.02018301  0.02686484 -0.1843691 ]\n",
      "[ episode 350 ][ timestamp 271 ] state=[-0.98788453 -0.02018301  0.02686484 -0.1843691 ], action=0, reward=1.0, next_state=[-0.98828819 -0.21567885  0.02317746  0.11666616]\n",
      "[ episode 350 ][ timestamp 272 ] state=[-0.98828819 -0.21567885  0.02317746  0.11666616], action=1, reward=1.0, next_state=[-0.99260176 -0.02089653  0.02551078 -0.16861529]\n",
      "[ episode 350 ][ timestamp 273 ] state=[-0.99260176 -0.02089653  0.02551078 -0.16861529], action=0, reward=1.0, next_state=[-0.99301969 -0.21637417  0.02213848  0.13200512]\n",
      "[ episode 350 ][ timestamp 274 ] state=[-0.99301969 -0.21637417  0.02213848  0.13200512], action=1, reward=1.0, next_state=[-0.99734718 -0.02157622  0.02477858 -0.15361202]\n",
      "[ episode 350 ][ timestamp 275 ] state=[-0.99734718 -0.02157622  0.02477858 -0.15361202], action=0, reward=1.0, next_state=[-0.9977787  -0.21704403  0.02170634  0.14678385]\n",
      "[ episode 350 ][ timestamp 276 ] state=[-0.9977787  -0.21704403  0.02170634  0.14678385], action=1, reward=1.0, next_state=[-1.00211958 -0.02223954  0.02464202 -0.13897288]\n",
      "[ episode 350 ][ timestamp 277 ] state=[-1.00211958 -0.02223954  0.02464202 -0.13897288], action=0, reward=1.0, next_state=[-1.00256437 -0.21770561  0.02186256  0.16138122]\n",
      "[ episode 350 ][ timestamp 278 ] state=[-1.00256437 -0.21770561  0.02186256  0.16138122], action=1, reward=1.0, next_state=[-1.00691848 -0.02290335  0.02509018 -0.12432525]\n",
      "[ episode 350 ][ timestamp 279 ] state=[-1.00691848 -0.02290335  0.02509018 -0.12432525], action=1, reward=1.0, next_state=[-1.00737655  0.17185033  0.02260368 -0.40898809]\n",
      "[ episode 350 ][ timestamp 280 ] state=[-1.00737655  0.17185033  0.02260368 -0.40898809], action=0, reward=1.0, next_state=[-1.00393955 -0.02358469  0.01442392 -0.10926554]\n",
      "[ episode 350 ][ timestamp 281 ] state=[-1.00393955 -0.02358469  0.01442392 -0.10926554], action=1, reward=1.0, next_state=[-1.00441124  0.17132763  0.0122386  -0.39736312]\n",
      "[ episode 350 ][ timestamp 282 ] state=[-1.00441124  0.17132763  0.0122386  -0.39736312], action=0, reward=1.0, next_state=[-1.00098469 -0.0239658   0.00429134 -0.10084686]\n",
      "[ episode 350 ][ timestamp 283 ] state=[-1.00098469 -0.0239658   0.00429134 -0.10084686], action=0, reward=1.0, next_state=[-1.001464   -0.21914899  0.00227441  0.19318688]\n",
      "[ episode 350 ][ timestamp 284 ] state=[-1.001464   -0.21914899  0.00227441  0.19318688], action=1, reward=1.0, next_state=[-1.00584698 -0.02405964  0.00613814 -0.0987777 ]\n",
      "[ episode 350 ][ timestamp 285 ] state=[-1.00584698 -0.02405964  0.00613814 -0.0987777 ], action=1, reward=1.0, next_state=[-1.00632817  0.1709738   0.00416259 -0.38951775]\n",
      "[ episode 350 ][ timestamp 286 ] state=[-1.00632817  0.1709738   0.00416259 -0.38951775], action=0, reward=1.0, next_state=[-1.0029087  -0.02420698 -0.00362777 -0.09552532]\n",
      "[ episode 350 ][ timestamp 287 ] state=[-1.0029087  -0.02420698 -0.00362777 -0.09552532], action=0, reward=1.0, next_state=[-1.00339284 -0.21927675 -0.00553827  0.19601085]\n",
      "[ episode 350 ][ timestamp 288 ] state=[-1.00339284 -0.21927675 -0.00553827  0.19601085], action=1, reward=1.0, next_state=[-1.00777837 -0.02407602 -0.00161806 -0.098414  ]\n",
      "[ episode 350 ][ timestamp 289 ] state=[-1.00777837 -0.02407602 -0.00161806 -0.098414  ], action=0, reward=1.0, next_state=[-1.00825989 -0.21917475 -0.00358634  0.19375799]\n",
      "[ episode 350 ][ timestamp 290 ] state=[-1.00825989 -0.21917475 -0.00358634  0.19375799], action=1, reward=1.0, next_state=[-1.01264339e+00 -2.40016763e-02  2.88824147e-04 -1.00054106e-01]\n",
      "[ episode 350 ][ timestamp 291 ] state=[-1.01264339e+00 -2.40016763e-02  2.88824147e-04 -1.00054106e-01], action=0, reward=1.0, next_state=[-1.01312342 -0.21912777 -0.00171226  0.19271993]\n",
      "[ episode 350 ][ timestamp 292 ] state=[-1.01312342 -0.21912777 -0.00171226  0.19271993], action=1, reward=1.0, next_state=[-1.01750598 -0.02398136  0.00214214 -0.10050265]\n",
      "[ episode 350 ][ timestamp 293 ] state=[-1.01750598 -0.02398136  0.00214214 -0.10050265], action=0, reward=1.0, next_state=[-1.01798560e+00 -2.19133948e-01  1.32087615e-04  1.92855346e-01]\n",
      "[ episode 350 ][ timestamp 294 ] state=[-1.01798560e+00 -2.19133948e-01  1.32087615e-04  1.92855346e-01], action=1, reward=1.0, next_state=[-1.02236828 -0.02401389  0.00398919 -0.09978591]\n",
      "[ episode 350 ][ timestamp 295 ] state=[-1.02236828 -0.02401389  0.00398919 -0.09978591], action=1, reward=1.0, next_state=[-1.02284856  0.17105067  0.00199348 -0.39120759]\n",
      "[ episode 350 ][ timestamp 296 ] state=[-1.02284856  0.17105067  0.00199348 -0.39120759], action=0, reward=1.0, next_state=[-1.01942755 -0.02409952 -0.00583068 -0.09789681]\n",
      "[ episode 350 ][ timestamp 297 ] state=[-1.01942755 -0.02409952 -0.00583068 -0.09789681], action=0, reward=1.0, next_state=[-1.01990954 -0.21913742 -0.00778861  0.19294086]\n",
      "[ episode 350 ][ timestamp 298 ] state=[-1.01990954 -0.21913742 -0.00778861  0.19294086], action=1, reward=1.0, next_state=[-1.02429229 -0.02390492 -0.00392979 -0.10218883]\n",
      "[ episode 350 ][ timestamp 299 ] state=[-1.02429229 -0.02390492 -0.00392979 -0.10218883], action=0, reward=1.0, next_state=[-1.02477039 -0.21897034 -0.00597357  0.18925167]\n",
      "[ episode 350 ][ timestamp 300 ] state=[-1.02477039 -0.21897034 -0.00597357  0.18925167], action=1, reward=1.0, next_state=[-1.02914979 -0.02376344 -0.00218854 -0.10530968]\n",
      "[ episode 350 ][ timestamp 301 ] state=[-1.02914979 -0.02376344 -0.00218854 -0.10530968], action=1, reward=1.0, next_state=[-1.02962506  0.17138981 -0.00429473 -0.39868227]\n",
      "[ episode 350 ][ timestamp 302 ] state=[-1.02962506  0.17138981 -0.00429473 -0.39868227], action=0, reward=1.0, next_state=[-1.02619726 -0.02367095 -0.01226838 -0.10735648]\n",
      "[ episode 350 ][ timestamp 303 ] state=[-1.02619726 -0.02367095 -0.01226838 -0.10735648], action=0, reward=1.0, next_state=[-1.02667068 -0.21861497 -0.01441551  0.18143073]\n",
      "[ episode 350 ][ timestamp 304 ] state=[-1.02667068 -0.21861497 -0.01441551  0.18143073], action=1, reward=1.0, next_state=[-1.03104298 -0.02328974 -0.01078689 -0.11576469]\n",
      "[ episode 350 ][ timestamp 305 ] state=[-1.03104298 -0.02328974 -0.01078689 -0.11576469], action=0, reward=1.0, next_state=[-1.03150878 -0.21825548 -0.01310219  0.17349563]\n",
      "[ episode 350 ][ timestamp 306 ] state=[-1.03150878 -0.21825548 -0.01310219  0.17349563], action=1, reward=1.0, next_state=[-1.03587389 -0.02294849 -0.00963227 -0.12329165]\n",
      "[ episode 350 ][ timestamp 307 ] state=[-1.03587389 -0.02294849 -0.00963227 -0.12329165], action=0, reward=1.0, next_state=[-1.03633286 -0.21793112 -0.01209811  0.16633689]\n",
      "[ episode 350 ][ timestamp 308 ] state=[-1.03633286 -0.21793112 -0.01209811  0.16633689], action=1, reward=1.0, next_state=[-1.04069148 -0.0226381  -0.00877137 -0.13013796]\n",
      "[ episode 350 ][ timestamp 309 ] state=[-1.04069148 -0.0226381  -0.00877137 -0.13013796], action=0, reward=1.0, next_state=[-1.04114424 -0.21763331 -0.01137413  0.15976486]\n",
      "[ episode 350 ][ timestamp 310 ] state=[-1.04114424 -0.21763331 -0.01137413  0.15976486], action=1, reward=1.0, next_state=[-1.04549691 -0.02235039 -0.00817883 -0.1364845 ]\n",
      "[ episode 350 ][ timestamp 311 ] state=[-1.04549691 -0.02235039 -0.00817883 -0.1364845 ], action=0, reward=1.0, next_state=[-1.04594392 -0.21735424 -0.01090852  0.15360694]\n",
      "[ episode 350 ][ timestamp 312 ] state=[-1.04594392 -0.21735424 -0.01090852  0.15360694], action=1, reward=1.0, next_state=[-1.050291   -0.02207781 -0.00783638 -0.14249732]\n",
      "[ episode 350 ][ timestamp 313 ] state=[-1.050291   -0.02207781 -0.00783638 -0.14249732], action=0, reward=1.0, next_state=[-1.05073256 -0.21708666 -0.01068633  0.1477031 ]\n",
      "[ episode 350 ][ timestamp 314 ] state=[-1.05073256 -0.21708666 -0.01068633  0.1477031 ], action=0, reward=1.0, next_state=[-1.05507429 -0.41205396 -0.00773227  0.43699564]\n",
      "[ episode 350 ][ timestamp 315 ] state=[-1.05507429 -0.41205396 -0.00773227  0.43699564], action=0, reward=1.0, next_state=[-1.06331537e+00 -6.07065613e-01  1.00764744e-03  7.27231109e-01]\n",
      "[ episode 350 ][ timestamp 316 ] state=[-1.06331537e+00 -6.07065613e-01  1.00764744e-03  7.27231109e-01], action=1, reward=1.0, next_state=[-1.07545668 -0.41195761  0.01555227  0.4348655 ]\n",
      "[ episode 350 ][ timestamp 317 ] state=[-1.07545668 -0.41195761  0.01555227  0.4348655 ], action=1, reward=1.0, next_state=[-1.08369583 -0.21705924  0.02424958  0.14712549]\n",
      "[ episode 350 ][ timestamp 318 ] state=[-1.08369583 -0.21705924  0.02424958  0.14712549], action=1, reward=1.0, next_state=[-1.08803702 -0.0222928   0.02719209 -0.13780961]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 350 ][ timestamp 319 ] state=[-1.08803702 -0.0222928   0.02719209 -0.13780961], action=0, reward=1.0, next_state=[-1.08848287 -0.21779345  0.0244359   0.16332645]\n",
      "[ episode 350 ][ timestamp 320 ] state=[-1.08848287 -0.21779345  0.0244359   0.16332645], action=0, reward=1.0, next_state=[-1.09283874 -0.41325654  0.02770243  0.46361699]\n",
      "[ episode 350 ][ timestamp 321 ] state=[-1.09283874 -0.41325654  0.02770243  0.46361699], action=1, reward=1.0, next_state=[-1.10110387 -0.2185368   0.03697477  0.17979292]\n",
      "[ episode 350 ][ timestamp 322 ] state=[-1.10110387 -0.2185368   0.03697477  0.17979292], action=1, reward=1.0, next_state=[-1.10547461 -0.02396292  0.04057062 -0.1010003 ]\n",
      "[ episode 350 ][ timestamp 323 ] state=[-1.10547461 -0.02396292  0.04057062 -0.1010003 ], action=1, reward=1.0, next_state=[-1.10595387  0.17055481  0.03855062 -0.38061232]\n",
      "[ episode 350 ][ timestamp 324 ] state=[-1.10595387  0.17055481  0.03855062 -0.38061232], action=1, reward=1.0, next_state=[-1.10254277  0.36510874  0.03093837 -0.66089531]\n",
      "[ episode 350 ][ timestamp 325 ] state=[-1.10254277  0.36510874  0.03093837 -0.66089531], action=0, reward=1.0, next_state=[-1.0952406   0.16957024  0.01772047 -0.3586335 ]\n",
      "[ episode 350 ][ timestamp 326 ] state=[-1.0952406   0.16957024  0.01772047 -0.3586335 ], action=0, reward=1.0, next_state=[-1.09184919 -0.02579908  0.0105478  -0.06041598]\n",
      "[ episode 350 ][ timestamp 327 ] state=[-1.09184919 -0.02579908  0.0105478  -0.06041598], action=0, reward=1.0, next_state=[-1.09236517 -0.22107066  0.00933948  0.23557609]\n",
      "[ episode 350 ][ timestamp 328 ] state=[-1.09236517 -0.22107066  0.00933948  0.23557609], action=1, reward=1.0, next_state=[-1.09678659 -0.02608339  0.014051   -0.0541463 ]\n",
      "[ episode 350 ][ timestamp 329 ] state=[-1.09678659 -0.02608339  0.014051   -0.0541463 ], action=1, reward=1.0, next_state=[-1.09730826  0.1688343   0.01296807 -0.34236312]\n",
      "[ episode 350 ][ timestamp 330 ] state=[-1.09730826  0.1688343   0.01296807 -0.34236312], action=0, reward=1.0, next_state=[-1.09393157 -0.02646972  0.00612081 -0.04561921]\n",
      "[ episode 350 ][ timestamp 331 ] state=[-1.09393157 -0.02646972  0.00612081 -0.04561921], action=1, reward=1.0, next_state=[-1.09446096  0.16856393  0.00520843 -0.33636469]\n",
      "[ episode 350 ][ timestamp 332 ] state=[-1.09446096  0.16856393  0.00520843 -0.33636469], action=1, reward=1.0, next_state=[-1.09108968  0.36361137 -0.00151887 -0.62740062]\n",
      "[ episode 350 ][ timestamp 333 ] state=[-1.09108968  0.36361137 -0.00151887 -0.62740062], action=0, reward=1.0, next_state=[-1.08381746  0.16851065 -0.01406688 -0.33519643]\n",
      "[ episode 350 ][ timestamp 334 ] state=[-1.08381746  0.16851065 -0.01406688 -0.33519643], action=0, reward=1.0, next_state=[-1.08044724 -0.0264083  -0.02077081 -0.04698244]\n",
      "[ episode 350 ][ timestamp 335 ] state=[-1.08044724 -0.0264083  -0.02077081 -0.04698244], action=0, reward=1.0, next_state=[-1.08097541 -0.22122635 -0.02171046  0.23907541]\n",
      "[ episode 350 ][ timestamp 336 ] state=[-1.08097541 -0.22122635 -0.02171046  0.23907541], action=1, reward=1.0, next_state=[-1.08539994 -0.02580109 -0.01692895 -0.06037577]\n",
      "[ episode 350 ][ timestamp 337 ] state=[-1.08539994 -0.02580109 -0.01692895 -0.06037577], action=1, reward=1.0, next_state=[-1.08591596  0.16955945 -0.01813647 -0.35835146]\n",
      "[ episode 350 ][ timestamp 338 ] state=[-1.08591596  0.16955945 -0.01813647 -0.35835146], action=0, reward=1.0, next_state=[-1.08252477 -0.02530004 -0.02530349 -0.07144213]\n",
      "[ episode 350 ][ timestamp 339 ] state=[-1.08252477 -0.02530004 -0.02530349 -0.07144213], action=0, reward=1.0, next_state=[-1.08303077 -0.22005026 -0.02673234  0.21315126]\n",
      "[ episode 350 ][ timestamp 340 ] state=[-1.08303077 -0.22005026 -0.02673234  0.21315126], action=1, reward=1.0, next_state=[-1.08743178 -0.02455652 -0.02246931 -0.08784296]\n",
      "[ episode 350 ][ timestamp 341 ] state=[-1.08743178 -0.02455652 -0.02246931 -0.08784296], action=0, reward=1.0, next_state=[-1.08792291 -0.21934931 -0.02422617  0.19766705]\n",
      "[ episode 350 ][ timestamp 342 ] state=[-1.08792291 -0.21934931 -0.02422617  0.19766705], action=1, reward=1.0, next_state=[-1.09230989 -0.02388937 -0.02027283 -0.10255862]\n",
      "[ episode 350 ][ timestamp 343 ] state=[-1.09230989 -0.02388937 -0.02027283 -0.10255862], action=0, reward=1.0, next_state=[-1.09278768 -0.21871501 -0.022324    0.18365998]\n",
      "[ episode 350 ][ timestamp 344 ] state=[-1.09278768 -0.21871501 -0.022324    0.18365998], action=1, reward=1.0, next_state=[-1.09716198 -0.02328087 -0.0186508  -0.1159809 ]\n",
      "[ episode 350 ][ timestamp 345 ] state=[-1.09716198 -0.02328087 -0.0186508  -0.1159809 ], action=0, reward=1.0, next_state=[-1.0976276  -0.21813069 -0.02097042  0.17075998]\n",
      "[ episode 350 ][ timestamp 346 ] state=[-1.0976276  -0.21813069 -0.02097042  0.17075998], action=1, reward=1.0, next_state=[-1.10199021 -0.02271496 -0.01755522 -0.12846401]\n",
      "[ episode 350 ][ timestamp 347 ] state=[-1.10199021 -0.02271496 -0.01755522 -0.12846401], action=0, reward=1.0, next_state=[-1.10244451 -0.21758108 -0.0201245   0.15862916]\n",
      "[ episode 350 ][ timestamp 348 ] state=[-1.10244451 -0.21758108 -0.0201245   0.15862916], action=1, reward=1.0, next_state=[-1.10679613 -0.02217688 -0.01695192 -0.140334  ]\n",
      "[ episode 350 ][ timestamp 349 ] state=[-1.10679613 -0.02217688 -0.01695192 -0.140334  ], action=0, reward=1.0, next_state=[-1.10723967 -0.21705199 -0.0197586   0.14695304]\n",
      "[ episode 350 ][ timestamp 350 ] state=[-1.10723967 -0.21705199 -0.0197586   0.14695304], action=1, reward=1.0, next_state=[-1.11158071 -0.02165274 -0.01681954 -0.15189727]\n",
      "[ episode 350 ][ timestamp 351 ] state=[-1.11158071 -0.02165274 -0.01681954 -0.15189727], action=0, reward=1.0, next_state=[-1.11201376 -0.21652987 -0.01985748  0.13543236]\n",
      "[ episode 350 ][ timestamp 352 ] state=[-1.11201376 -0.21652987 -0.01985748  0.13543236], action=1, reward=1.0, next_state=[-1.11634436 -0.0211292  -0.01714884 -0.16344857]\n",
      "[ episode 350 ][ timestamp 353 ] state=[-1.11634436 -0.0211292  -0.01714884 -0.16344857], action=0, reward=1.0, next_state=[-1.11676695 -0.21600151 -0.02041781  0.12377541]\n",
      "[ episode 350 ][ timestamp 354 ] state=[-1.11676695 -0.21600151 -0.02041781  0.12377541], action=1, reward=1.0, next_state=[-1.12108698 -0.02059309 -0.0179423  -0.17527855]\n",
      "[ episode 350 ][ timestamp 355 ] state=[-1.12108698 -0.02059309 -0.0179423  -0.17527855], action=0, reward=1.0, next_state=[-1.12149884 -0.21545372 -0.02144787  0.1116906 ]\n",
      "[ episode 350 ][ timestamp 356 ] state=[-1.12149884 -0.21545372 -0.02144787  0.1116906 ], action=1, reward=1.0, next_state=[-1.12580791 -0.02003111 -0.01921406 -0.18768109]\n",
      "[ episode 350 ][ timestamp 357 ] state=[-1.12580791 -0.02003111 -0.01921406 -0.18768109], action=1, reward=1.0, next_state=[-1.12620853  0.17536039 -0.02296768 -0.48636283]\n",
      "[ episode 350 ][ timestamp 358 ] state=[-1.12620853  0.17536039 -0.02296768 -0.48636283], action=0, reward=1.0, next_state=[-1.12270133 -0.01943007 -0.03269494 -0.20100611]\n",
      "[ episode 350 ][ timestamp 359 ] state=[-1.12270133 -0.01943007 -0.03269494 -0.20100611], action=0, reward=1.0, next_state=[-1.12308993 -0.21406953 -0.03671506  0.08118645]\n",
      "[ episode 350 ][ timestamp 360 ] state=[-1.12308993 -0.21406953 -0.03671506  0.08118645], action=1, reward=1.0, next_state=[-1.12737132 -0.01844102 -0.03509133 -0.22285034]\n",
      "[ episode 350 ][ timestamp 361 ] state=[-1.12737132 -0.01844102 -0.03509133 -0.22285034], action=0, reward=1.0, next_state=[-1.12774014 -0.2130443  -0.03954834  0.05856013]\n",
      "[ episode 350 ][ timestamp 362 ] state=[-1.12774014 -0.2130443  -0.03954834  0.05856013], action=0, reward=1.0, next_state=[-1.13200103 -0.40757754 -0.03837713  0.33850764]\n",
      "[ episode 350 ][ timestamp 363 ] state=[-1.13200103 -0.40757754 -0.03837713  0.33850764], action=0, reward=1.0, next_state=[-1.14015258 -0.60213298 -0.03160698  0.61884582]\n",
      "[ episode 350 ][ timestamp 364 ] state=[-1.14015258 -0.60213298 -0.03160698  0.61884582], action=1, reward=1.0, next_state=[-1.15219524 -0.40658414 -0.01923006  0.31637815]\n",
      "[ episode 350 ][ timestamp 365 ] state=[-1.15219524 -0.40658414 -0.01923006  0.31637815], action=1, reward=1.0, next_state=[-1.16032692 -0.21119363 -0.0129025   0.01769329]\n",
      "[ episode 350 ][ timestamp 366 ] state=[-1.16032692 -0.21119363 -0.0129025   0.01769329], action=1, reward=1.0, next_state=[-1.16455079 -0.01588905 -0.01254864 -0.27903244]\n",
      "[ episode 350 ][ timestamp 367 ] state=[-1.16455079 -0.01588905 -0.01254864 -0.27903244], action=0, reward=1.0, next_state=[-1.16486857 -0.21082976 -0.01812928  0.0096664 ]\n",
      "[ episode 350 ][ timestamp 368 ] state=[-1.16486857 -0.21082976 -0.01812928  0.0096664 ], action=0, reward=1.0, next_state=[-1.16908517 -0.40568708 -0.01793596  0.29657464]\n",
      "[ episode 350 ][ timestamp 369 ] state=[-1.16908517 -0.40568708 -0.01793596  0.29657464], action=1, reward=1.0, next_state=[-1.17719891 -0.21031409 -0.01200446 -0.00171059]\n",
      "[ episode 350 ][ timestamp 370 ] state=[-1.17719891 -0.21031409 -0.01200446 -0.00171059], action=0, reward=1.0, next_state=[-1.18140519 -0.40526184 -0.01203868  0.28716074]\n",
      "[ episode 350 ][ timestamp 371 ] state=[-1.18140519 -0.40526184 -0.01203868  0.28716074], action=1, reward=1.0, next_state=[-1.18951043 -0.2099703  -0.00629546 -0.00929464]\n",
      "[ episode 350 ][ timestamp 372 ] state=[-1.18951043 -0.2099703  -0.00629546 -0.00929464], action=0, reward=1.0, next_state=[-1.19370983 -0.4050014  -0.00648135  0.28139536]\n",
      "[ episode 350 ][ timestamp 373 ] state=[-1.19370983 -0.4050014  -0.00648135  0.28139536], action=1, reward=1.0, next_state=[-1.20180986e+00 -2.09787600e-01 -8.53446601e-04 -1.33246940e-02]\n",
      "[ episode 350 ][ timestamp 374 ] state=[-1.20180986e+00 -2.09787600e-01 -8.53446601e-04 -1.33246940e-02], action=0, reward=1.0, next_state=[-1.20600561e+00 -4.04897301e-01 -1.11994048e-03  2.79088838e-01]\n",
      "[ episode 350 ][ timestamp 375 ] state=[-1.20600561e+00 -4.04897301e-01 -1.11994048e-03  2.79088838e-01], action=1, reward=1.0, next_state=[-1.21410356 -0.20975939  0.00446184 -0.01394711]\n",
      "[ episode 350 ][ timestamp 376 ] state=[-1.21410356 -0.20975939  0.00446184 -0.01394711], action=0, reward=1.0, next_state=[-1.21829875 -0.40494505  0.00418289  0.28014024]\n",
      "[ episode 350 ][ timestamp 377 ] state=[-1.21829875 -0.40494505  0.00418289  0.28014024], action=1, reward=1.0, next_state=[-1.22639765 -0.20988301  0.0097857  -0.01122049]\n",
      "[ episode 350 ][ timestamp 378 ] state=[-1.22639765 -0.20988301  0.0097857  -0.01122049], action=0, reward=1.0, next_state=[-1.23059531 -0.40514393  0.00956129  0.28453381]\n",
      "[ episode 350 ][ timestamp 379 ] state=[-1.23059531 -0.40514393  0.00956129  0.28453381], action=1, reward=1.0, next_state=[-1.23869819 -0.21015964  0.01525197 -0.00511828]\n",
      "[ episode 350 ][ timestamp 380 ] state=[-1.23869819 -0.21015964  0.01525197 -0.00511828], action=0, reward=1.0, next_state=[-1.24290138 -0.40549697  0.0151496   0.29233754]\n",
      "[ episode 350 ][ timestamp 381 ] state=[-1.24290138 -0.40549697  0.0151496   0.29233754], action=1, reward=1.0, next_state=[-1.25101132 -0.21059426  0.02099635  0.00447084]\n",
      "[ episode 350 ][ timestamp 382 ] state=[-1.25101132 -0.21059426  0.02099635  0.00447084], action=0, reward=1.0, next_state=[-1.2552232  -0.40601094  0.02108577  0.30370372]\n",
      "[ episode 350 ][ timestamp 383 ] state=[-1.2552232  -0.40601094  0.02108577  0.30370372], action=1, reward=1.0, next_state=[-1.26334342 -0.21119574  0.02715984  0.01774464]\n",
      "[ episode 350 ][ timestamp 384 ] state=[-1.26334342 -0.21119574  0.02715984  0.01774464], action=0, reward=1.0, next_state=[-1.26756734 -0.40669645  0.02751473  0.31887157]\n",
      "[ episode 350 ][ timestamp 385 ] state=[-1.26756734 -0.40669645  0.02751473  0.31887157], action=1, reward=1.0, next_state=[-1.27570127 -0.21197696  0.03389217  0.03499119]\n",
      "[ episode 350 ][ timestamp 386 ] state=[-1.27570127 -0.21197696  0.03389217  0.03499119], action=0, reward=1.0, next_state=[-1.27994081 -0.40756812  0.03459199  0.33817185]\n",
      "[ episode 350 ][ timestamp 387 ] state=[-1.27994081 -0.40756812  0.03459199  0.33817185], action=1, reward=1.0, next_state=[-1.28809217 -0.21295505  0.04135543  0.05659489]\n",
      "[ episode 350 ][ timestamp 388 ] state=[-1.28809217 -0.21295505  0.04135543  0.05659489], action=0, reward=1.0, next_state=[-1.29235127 -0.40864482  0.04248732  0.3620336 ]\n",
      "[ episode 350 ][ timestamp 389 ] state=[-1.29235127 -0.40864482  0.04248732  0.3620336 ], action=1, reward=1.0, next_state=[-1.30052417 -0.21415171  0.049728    0.08304473]\n",
      "[ episode 350 ][ timestamp 390 ] state=[-1.30052417 -0.21415171  0.049728    0.08304473], action=0, reward=1.0, next_state=[-1.3048072  -0.40994992  0.05138889  0.39099299]\n",
      "[ episode 350 ][ timestamp 391 ] state=[-1.3048072  -0.40994992  0.05138889  0.39099299], action=0, reward=1.0, next_state=[-1.3130062  -0.60576211  0.05920875  0.69942521]\n",
      "[ episode 350 ][ timestamp 392 ] state=[-1.3130062  -0.60576211  0.05920875  0.69942521], action=1, reward=1.0, next_state=[-1.32512144 -0.41150884  0.07319726  0.42595311]\n",
      "[ episode 350 ][ timestamp 393 ] state=[-1.32512144 -0.41150884  0.07319726  0.42595311], action=0, reward=1.0, next_state=[-1.33335162 -0.60758712  0.08171632  0.74078375]\n",
      "[ episode 350 ][ timestamp 394 ] state=[-1.33335162 -0.60758712  0.08171632  0.74078375], action=1, reward=1.0, next_state=[-1.34550336 -0.41368273  0.09653199  0.47489559]\n",
      "[ episode 350 ][ timestamp 395 ] state=[-1.34550336 -0.41368273  0.09653199  0.47489559], action=0, reward=1.0, next_state=[-1.35377701 -0.61002584  0.1060299   0.79637546]\n",
      "[ episode 350 ][ timestamp 396 ] state=[-1.35377701 -0.61002584  0.1060299   0.79637546], action=1, reward=1.0, next_state=[-1.36597753 -0.41650625  0.12195741  0.53884068]\n",
      "[ episode 350 ][ timestamp 397 ] state=[-1.36597753 -0.41650625  0.12195741  0.53884068], action=0, reward=1.0, next_state=[-1.37430766 -0.61311259  0.13273423  0.86732638]\n",
      "[ episode 350 ][ timestamp 398 ] state=[-1.37430766 -0.61311259  0.13273423  0.86732638], action=1, reward=1.0, next_state=[-1.38656991 -0.42002229  0.15008075  0.61914802]\n",
      "[ episode 350 ][ timestamp 399 ] state=[-1.38656991 -0.42002229  0.15008075  0.61914802], action=0, reward=1.0, next_state=[-1.39497035 -0.6168865   0.16246372  0.9550832 ]\n",
      "[ episode 350 ][ timestamp 400 ] state=[-1.39497035 -0.6168865   0.16246372  0.9550832 ], action=1, reward=1.0, next_state=[-1.40730808 -0.42427851  0.18156538  0.71753016]\n",
      "[ episode 350 ][ timestamp 401 ] state=[-1.40730808 -0.42427851  0.18156538  0.71753016], action=0, reward=1.0, next_state=[-1.41579365 -0.62138658  0.19591598  1.06141965]\n",
      "[ episode 350 ][ timestamp 402 ] state=[-1.41579365 -0.62138658  0.19591598  1.06141965], action=0, reward=-1.0, next_state=[-1.42822139 -0.81848643  0.21714438  1.40864511]\n",
      "[ Ended! ] Episode 350: Exploration_rate=0.17388222158237718. Score=402.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 351 ] state=[ 0.01814884  0.04693025  0.03649043 -0.03501287]\n",
      "[ episode 351 ][ timestamp 1 ] state=[ 0.01814884  0.04693025  0.03649043 -0.03501287], action=1, reward=1.0, next_state=[ 0.01908744  0.24151044  0.03579018 -0.31596304]\n",
      "[ episode 351 ][ timestamp 2 ] state=[ 0.01908744  0.24151044  0.03579018 -0.31596304], action=1, reward=1.0, next_state=[ 0.02391765  0.4361048   0.02947092 -0.59714759]\n",
      "[ episode 351 ][ timestamp 3 ] state=[ 0.02391765  0.4361048   0.02947092 -0.59714759], action=1, reward=1.0, next_state=[ 0.03263975  0.63080222  0.01752796 -0.88040371]\n",
      "[ episode 351 ][ timestamp 4 ] state=[ 0.03263975  0.63080222  0.01752796 -0.88040371], action=1, reward=1.0, next_state=[ 4.52557901e-02  8.25681717e-01 -8.01094484e-05 -1.16752510e+00]\n",
      "[ episode 351 ][ timestamp 5 ] state=[ 4.52557901e-02  8.25681717e-01 -8.01094484e-05 -1.16752510e+00], action=0, reward=1.0, next_state=[ 0.06176942  0.63056081 -0.02343061 -0.87486729]\n",
      "[ episode 351 ][ timestamp 6 ] state=[ 0.06176942  0.63056081 -0.02343061 -0.87486729], action=0, reward=1.0, next_state=[ 0.07438064  0.43576509 -0.04092796 -0.58964188]\n",
      "[ episode 351 ][ timestamp 7 ] state=[ 0.07438064  0.43576509 -0.04092796 -0.58964188], action=0, reward=1.0, next_state=[ 0.08309594  0.2412394  -0.05272079 -0.31012716]\n",
      "[ episode 351 ][ timestamp 8 ] state=[ 0.08309594  0.2412394  -0.05272079 -0.31012716], action=1, reward=1.0, next_state=[ 0.08792073  0.43707131 -0.05892334 -0.61895962]\n",
      "[ episode 351 ][ timestamp 9 ] state=[ 0.08792073  0.43707131 -0.05892334 -0.61895962], action=0, reward=1.0, next_state=[ 0.09666216  0.24281973 -0.07130253 -0.34540137]\n",
      "[ episode 351 ][ timestamp 10 ] state=[ 0.09666216  0.24281973 -0.07130253 -0.34540137], action=1, reward=1.0, next_state=[ 0.10151855  0.4388797  -0.07821056 -0.65968924]\n",
      "[ episode 351 ][ timestamp 11 ] state=[ 0.10151855  0.4388797  -0.07821056 -0.65968924], action=0, reward=1.0, next_state=[ 0.11029615  0.24492829 -0.09140434 -0.39262193]\n",
      "[ episode 351 ][ timestamp 12 ] state=[ 0.11029615  0.24492829 -0.09140434 -0.39262193], action=0, reward=1.0, next_state=[ 0.11519471  0.05121429 -0.09925678 -0.13009938]\n",
      "[ episode 351 ][ timestamp 13 ] state=[ 0.11519471  0.05121429 -0.09925678 -0.13009938], action=0, reward=1.0, next_state=[ 0.116219   -0.14235606 -0.10185877  0.12969344]\n",
      "[ episode 351 ][ timestamp 14 ] state=[ 0.116219   -0.14235606 -0.10185877  0.12969344], action=1, reward=1.0, next_state=[ 0.11337188  0.05406634 -0.0992649  -0.19330777]\n",
      "[ episode 351 ][ timestamp 15 ] state=[ 0.11337188  0.05406634 -0.0992649  -0.19330777], action=0, reward=1.0, next_state=[ 0.1144532  -0.13950584 -0.10313106  0.06648519]\n",
      "[ episode 351 ][ timestamp 16 ] state=[ 0.1144532  -0.13950584 -0.10313106  0.06648519], action=1, reward=1.0, next_state=[ 0.11166309  0.05693191 -0.10180135 -0.25687266]\n",
      "[ episode 351 ][ timestamp 17 ] state=[ 0.11166309  0.05693191 -0.10180135 -0.25687266], action=0, reward=1.0, next_state=[ 0.11280172 -0.13660042 -0.10693881  0.00204495]\n",
      "[ episode 351 ][ timestamp 18 ] state=[ 0.11280172 -0.13660042 -0.10693881  0.00204495], action=0, reward=1.0, next_state=[ 0.11006972 -0.33003913 -0.10689791  0.25916536]\n",
      "[ episode 351 ][ timestamp 19 ] state=[ 0.11006972 -0.33003913 -0.10689791  0.25916536], action=1, reward=1.0, next_state=[ 0.10346893 -0.1335664  -0.1017146  -0.06522965]\n",
      "[ episode 351 ][ timestamp 20 ] state=[ 0.10346893 -0.1335664  -0.1017146  -0.06522965], action=0, reward=1.0, next_state=[ 0.10079761 -0.32709408 -0.10301919  0.19370895]\n",
      "[ episode 351 ][ timestamp 21 ] state=[ 0.10079761 -0.32709408 -0.10301919  0.19370895], action=1, reward=1.0, next_state=[ 0.09425572 -0.13066088 -0.09914501 -0.12961278]\n",
      "[ episode 351 ][ timestamp 22 ] state=[ 0.09425572 -0.13066088 -0.09914501 -0.12961278], action=0, reward=1.0, next_state=[ 0.09164251 -0.3242331  -0.10173727  0.13021874]\n",
      "[ episode 351 ][ timestamp 23 ] state=[ 0.09164251 -0.3242331  -0.10173727  0.13021874], action=1, reward=1.0, next_state=[ 0.08515784 -0.12781206 -0.09913289 -0.19274853]\n",
      "[ episode 351 ][ timestamp 24 ] state=[ 0.08515784 -0.12781206 -0.09913289 -0.19274853], action=0, reward=1.0, next_state=[ 0.0826016  -0.32138644 -0.10298786  0.06709012]\n",
      "[ episode 351 ][ timestamp 25 ] state=[ 0.0826016  -0.32138644 -0.10298786  0.06709012], action=1, reward=1.0, next_state=[ 0.07617387 -0.12495028 -0.10164606 -0.25622781]\n",
      "[ episode 351 ][ timestamp 26 ] state=[ 0.07617387 -0.12495028 -0.10164606 -0.25622781], action=0, reward=1.0, next_state=[ 0.07367487 -0.31848519 -0.10677062  0.00274364]\n",
      "[ episode 351 ][ timestamp 27 ] state=[ 0.07367487 -0.31848519 -0.10677062  0.00274364], action=0, reward=1.0, next_state=[ 0.06730516 -0.51192676 -0.10671575  0.2599227 ]\n",
      "[ episode 351 ][ timestamp 28 ] state=[ 0.06730516 -0.51192676 -0.10671575  0.2599227 ], action=1, reward=1.0, next_state=[ 0.05706663 -0.31545606 -0.10151729 -0.06442177]\n",
      "[ episode 351 ][ timestamp 29 ] state=[ 0.05706663 -0.31545606 -0.10151729 -0.06442177], action=0, reward=1.0, next_state=[ 0.05075751 -0.50898706 -0.10280573  0.19458531]\n",
      "[ episode 351 ][ timestamp 30 ] state=[ 0.05075751 -0.50898706 -0.10280573  0.19458531], action=1, reward=1.0, next_state=[ 0.04057777 -0.31255626 -0.09891402 -0.12867688]\n",
      "[ episode 351 ][ timestamp 31 ] state=[ 0.04057777 -0.31255626 -0.09891402 -0.12867688], action=0, reward=1.0, next_state=[ 0.03432664 -0.50613233 -0.10148756  0.13123461]\n",
      "[ episode 351 ][ timestamp 32 ] state=[ 0.03432664 -0.50613233 -0.10148756  0.13123461], action=1, reward=1.0, next_state=[ 0.024204   -0.3097141  -0.09886287 -0.1916629 ]\n",
      "[ episode 351 ][ timestamp 33 ] state=[ 0.024204   -0.3097141  -0.09886287 -0.1916629 ], action=0, reward=1.0, next_state=[ 0.01800971 -0.50329298 -0.10269612  0.06826921]\n",
      "[ episode 351 ][ timestamp 34 ] state=[ 0.01800971 -0.50329298 -0.10269612  0.06826921], action=0, reward=1.0, next_state=[ 0.00794385 -0.69680411 -0.10133074  0.32686698]\n",
      "[ episode 351 ][ timestamp 35 ] state=[ 0.00794385 -0.69680411 -0.10133074  0.32686698], action=1, reward=1.0, next_state=[-0.00599223 -0.50039646 -0.0947934   0.00402645]\n",
      "[ episode 351 ][ timestamp 36 ] state=[-0.00599223 -0.50039646 -0.0947934   0.00402645], action=0, reward=1.0, next_state=[-0.01600016 -0.69404013 -0.09471287  0.26536036]\n",
      "[ episode 351 ][ timestamp 37 ] state=[-0.01600016 -0.69404013 -0.09471287  0.26536036], action=1, reward=1.0, next_state=[-0.02988096 -0.49770295 -0.08940566 -0.05562943]\n",
      "[ episode 351 ][ timestamp 38 ] state=[-0.02988096 -0.49770295 -0.08940566 -0.05562943], action=0, reward=1.0, next_state=[-0.03983502 -0.69143677 -0.09051825  0.20756038]\n",
      "[ episode 351 ][ timestamp 39 ] state=[-0.03983502 -0.69143677 -0.09051825  0.20756038], action=1, reward=1.0, next_state=[-0.05366375 -0.4951449  -0.08636704 -0.11224805]\n",
      "[ episode 351 ][ timestamp 40 ] state=[-0.05366375 -0.4951449  -0.08636704 -0.11224805], action=0, reward=1.0, next_state=[-0.06356665 -0.68892992 -0.08861201  0.15198569]\n",
      "[ episode 351 ][ timestamp 41 ] state=[-0.06356665 -0.68892992 -0.08861201  0.15198569], action=1, reward=1.0, next_state=[-0.07734525 -0.49265825 -0.08557229 -0.16728457]\n",
      "[ episode 351 ][ timestamp 42 ] state=[-0.07734525 -0.49265825 -0.08557229 -0.16728457], action=0, reward=1.0, next_state=[-0.08719842 -0.6864577  -0.08891798  0.09722336]\n",
      "[ episode 351 ][ timestamp 43 ] state=[-0.08719842 -0.6864577  -0.08891798  0.09722336], action=1, reward=1.0, next_state=[-0.10092757 -0.49018132 -0.08697352 -0.22213556]\n",
      "[ episode 351 ][ timestamp 44 ] state=[-0.10092757 -0.49018132 -0.08697352 -0.22213556], action=0, reward=1.0, next_state=[-0.1107312  -0.68395942 -0.09141623  0.04189494]\n",
      "[ episode 351 ][ timestamp 45 ] state=[-0.1107312  -0.68395942 -0.09141623  0.04189494], action=0, reward=1.0, next_state=[-0.12441038 -0.87765966 -0.09057833  0.30439314]\n",
      "[ episode 351 ][ timestamp 46 ] state=[-0.12441038 -0.87765966 -0.09057833  0.30439314], action=1, reward=1.0, next_state=[-0.14196358 -0.68137147 -0.08449047 -0.01542578]\n",
      "[ episode 351 ][ timestamp 47 ] state=[-0.14196358 -0.68137147 -0.08449047 -0.01542578], action=0, reward=1.0, next_state=[-0.15559101 -0.87518649 -0.08479898  0.24944903]\n",
      "[ episode 351 ][ timestamp 48 ] state=[-0.15559101 -0.87518649 -0.08479898  0.24944903], action=1, reward=1.0, next_state=[-0.17309474 -0.67896236 -0.07981    -0.06873057]\n",
      "[ episode 351 ][ timestamp 49 ] state=[-0.17309474 -0.67896236 -0.07981    -0.06873057], action=0, reward=1.0, next_state=[-0.18667398 -0.87285476 -0.08118461  0.19774302]\n",
      "[ episode 351 ][ timestamp 50 ] state=[-0.18667398 -0.87285476 -0.08118461  0.19774302], action=1, reward=1.0, next_state=[-0.20413108 -0.67667111 -0.07722975 -0.11940527]\n",
      "[ episode 351 ][ timestamp 51 ] state=[-0.20413108 -0.67667111 -0.07722975 -0.11940527], action=1, reward=1.0, next_state=[-0.2176645  -0.48053247 -0.07961786 -0.43541926]\n",
      "[ episode 351 ][ timestamp 52 ] state=[-0.2176645  -0.48053247 -0.07961786 -0.43541926], action=0, reward=1.0, next_state=[-0.22727515 -0.67444237 -0.08832624 -0.16885875]\n",
      "[ episode 351 ][ timestamp 53 ] state=[-0.22727515 -0.67444237 -0.08832624 -0.16885875], action=0, reward=1.0, next_state=[-0.240764   -0.86819631 -0.09170342  0.09470505]\n",
      "[ episode 351 ][ timestamp 54 ] state=[-0.240764   -0.86819631 -0.09170342  0.09470505], action=1, reward=1.0, next_state=[-0.25812792 -0.67188781 -0.08980932 -0.22544345]\n",
      "[ episode 351 ][ timestamp 55 ] state=[-0.25812792 -0.67188781 -0.08980932 -0.22544345], action=0, reward=1.0, next_state=[-0.27156568 -0.8656191  -0.09431819  0.03761388]\n",
      "[ episode 351 ][ timestamp 56 ] state=[-0.27156568 -0.8656191  -0.09431819  0.03761388], action=0, reward=1.0, next_state=[-0.28887806 -1.05927085 -0.09356591  0.29911197]\n",
      "[ episode 351 ][ timestamp 57 ] state=[-0.28887806 -1.05927085 -0.09356591  0.29911197], action=1, reward=1.0, next_state=[-0.31006348 -0.86294839 -0.08758367 -0.02155187]\n",
      "[ episode 351 ][ timestamp 58 ] state=[-0.31006348 -0.86294839 -0.08758367 -0.02155187], action=0, reward=1.0, next_state=[-0.32732245 -1.05671224 -0.08801471  0.24226317]\n",
      "[ episode 351 ][ timestamp 59 ] state=[-0.32732245 -1.05671224 -0.08801471  0.24226317], action=1, reward=1.0, next_state=[-0.34845669 -0.86045053 -0.08316944 -0.07683279]\n",
      "[ episode 351 ][ timestamp 60 ] state=[-0.34845669 -0.86045053 -0.08316944 -0.07683279], action=0, reward=1.0, next_state=[-0.3656657  -1.0542878  -0.0847061   0.18849445]\n",
      "[ episode 351 ][ timestamp 61 ] state=[-0.3656657  -1.0542878  -0.0847061   0.18849445], action=1, reward=1.0, next_state=[-0.38675146 -0.85806255 -0.08093621 -0.12966192]\n",
      "[ episode 351 ][ timestamp 62 ] state=[-0.38675146 -0.85806255 -0.08093621 -0.12966192], action=0, reward=1.0, next_state=[-0.40391271 -1.05193742 -0.08352945  0.13642912]\n",
      "[ episode 351 ][ timestamp 63 ] state=[-0.40391271 -1.05193742 -0.08352945  0.13642912], action=1, reward=1.0, next_state=[-0.42495146 -0.85572455 -0.08080087 -0.18139313]\n",
      "[ episode 351 ][ timestamp 64 ] state=[-0.42495146 -0.85572455 -0.08080087 -0.18139313], action=0, reward=1.0, next_state=[-0.44206595 -1.04960291 -0.08442873  0.08474597]\n",
      "[ episode 351 ][ timestamp 65 ] state=[-0.44206595 -1.04960291 -0.08442873  0.08474597], action=1, reward=1.0, next_state=[-0.46305801 -0.85337857 -0.08273381 -0.23333469]\n",
      "[ episode 351 ][ timestamp 66 ] state=[-0.46305801 -0.85337857 -0.08273381 -0.23333469], action=0, reward=1.0, next_state=[-0.48012558 -1.04722693 -0.0874005   0.03214726]\n",
      "[ episode 351 ][ timestamp 67 ] state=[-0.48012558 -1.04722693 -0.0874005   0.03214726], action=0, reward=1.0, next_state=[-0.50107012 -1.24099386 -0.08675756  0.29602521]\n",
      "[ episode 351 ][ timestamp 68 ] state=[-0.50107012 -1.24099386 -0.08675756  0.29602521], action=1, reward=1.0, next_state=[-0.52589    -1.04474913 -0.08083705 -0.02270949]\n",
      "[ episode 351 ][ timestamp 69 ] state=[-0.52589    -1.04474913 -0.08083705 -0.02270949], action=0, reward=1.0, next_state=[-0.54678498 -1.23862435 -0.08129124  0.24341346]\n",
      "[ episode 351 ][ timestamp 70 ] state=[-0.54678498 -1.23862435 -0.08129124  0.24341346], action=0, reward=1.0, next_state=[-0.57155746 -1.43249675 -0.07642297  0.50938841]\n",
      "[ episode 351 ][ timestamp 71 ] state=[-0.57155746 -1.43249675 -0.07642297  0.50938841], action=1, reward=1.0, next_state=[-0.6002074  -1.23638601 -0.0662352   0.19363444]\n",
      "[ episode 351 ][ timestamp 72 ] state=[-0.6002074  -1.23638601 -0.0662352   0.19363444], action=1, reward=1.0, next_state=[-0.62493512 -1.04038218 -0.06236252 -0.11918555]\n",
      "[ episode 351 ][ timestamp 73 ] state=[-0.62493512 -1.04038218 -0.06236252 -0.11918555], action=0, reward=1.0, next_state=[-0.64574276 -1.23455775 -0.06474623  0.15318893]\n",
      "[ episode 351 ][ timestamp 74 ] state=[-0.64574276 -1.23455775 -0.06474623  0.15318893], action=0, reward=1.0, next_state=[-0.67043392 -1.42869576 -0.06168245  0.42476367]\n",
      "[ episode 351 ][ timestamp 75 ] state=[-0.67043392 -1.42869576 -0.06168245  0.42476367], action=1, reward=1.0, next_state=[-0.69900783 -1.23275676 -0.05318718  0.11329098]\n",
      "[ episode 351 ][ timestamp 76 ] state=[-0.69900783 -1.23275676 -0.05318718  0.11329098], action=0, reward=1.0, next_state=[-0.72366297 -1.42707785 -0.05092136  0.38873076]\n",
      "[ episode 351 ][ timestamp 77 ] state=[-0.72366297 -1.42707785 -0.05092136  0.38873076], action=1, reward=1.0, next_state=[-0.75220453 -1.2312715  -0.04314674  0.08043753]\n",
      "[ episode 351 ][ timestamp 78 ] state=[-0.75220453 -1.2312715  -0.04314674  0.08043753], action=1, reward=1.0, next_state=[-0.77682996 -1.03555844 -0.04153799 -0.22554004]\n",
      "[ episode 351 ][ timestamp 79 ] state=[-0.77682996 -1.03555844 -0.04153799 -0.22554004], action=0, reward=1.0, next_state=[-0.79754112 -1.23006288 -0.04604879  0.0537563 ]\n",
      "[ episode 351 ][ timestamp 80 ] state=[-0.79754112 -1.23006288 -0.04604879  0.0537563 ], action=1, reward=1.0, next_state=[-0.82214238 -1.03431194 -0.04497366 -0.25309241]\n",
      "[ episode 351 ][ timestamp 81 ] state=[-0.82214238 -1.03431194 -0.04497366 -0.25309241], action=0, reward=1.0, next_state=[-0.84282862 -1.22876382 -0.05003551  0.02507268]\n",
      "[ episode 351 ][ timestamp 82 ] state=[-0.84282862 -1.22876382 -0.05003551  0.02507268], action=0, reward=1.0, next_state=[-0.8674039  -1.42313384 -0.04953406  0.30155852]\n",
      "[ episode 351 ][ timestamp 83 ] state=[-0.8674039  -1.42313384 -0.04953406  0.30155852], action=1, reward=1.0, next_state=[-0.89586657 -1.22734217 -0.04350289 -0.00632581]\n",
      "[ episode 351 ][ timestamp 84 ] state=[-0.89586657 -1.22734217 -0.04350289 -0.00632581], action=0, reward=1.0, next_state=[-0.92041342 -1.42181411 -0.04362941  0.27232029]\n",
      "[ episode 351 ][ timestamp 85 ] state=[-0.92041342 -1.42181411 -0.04362941  0.27232029], action=1, reward=1.0, next_state=[-0.9488497  -1.22609764 -0.038183   -0.03379802]\n",
      "[ episode 351 ][ timestamp 86 ] state=[-0.9488497  -1.22609764 -0.038183   -0.03379802], action=0, reward=1.0, next_state=[-0.97337165 -1.42065182 -0.03885896  0.24659746]\n",
      "[ episode 351 ][ timestamp 87 ] state=[-0.97337165 -1.42065182 -0.03885896  0.24659746], action=1, reward=1.0, next_state=[-1.00178469 -1.22499705 -0.03392701 -0.0580848 ]\n",
      "[ episode 351 ][ timestamp 88 ] state=[-1.00178469 -1.22499705 -0.03392701 -0.0580848 ], action=0, reward=1.0, next_state=[-1.02628463 -1.41961653 -0.03508871  0.22370381]\n",
      "[ episode 351 ][ timestamp 89 ] state=[-1.02628463 -1.41961653 -0.03508871  0.22370381], action=1, reward=1.0, next_state=[-1.05467696 -1.2240111  -0.03061463 -0.0798377 ]\n",
      "[ episode 351 ][ timestamp 90 ] state=[-1.05467696 -1.2240111  -0.03061463 -0.0798377 ], action=0, reward=1.0, next_state=[-1.07915718 -1.41868111 -0.03221138  0.20303119]\n",
      "[ episode 351 ][ timestamp 91 ] state=[-1.07915718 -1.41868111 -0.03221138  0.20303119], action=0, reward=1.0, next_state=[-1.1075308  -1.61332795 -0.02815076  0.48538148]\n",
      "[ episode 351 ][ timestamp 92 ] state=[-1.1075308  -1.61332795 -0.02815076  0.48538148], action=1, reward=1.0, next_state=[-1.13979736 -1.41782029 -0.01844313  0.18396095]\n",
      "[ episode 351 ][ timestamp 93 ] state=[-1.13979736 -1.41782029 -0.01844313  0.18396095], action=0, reward=1.0, next_state=[-1.16815377 -1.61267356 -0.01476391  0.47076917]\n",
      "[ episode 351 ][ timestamp 94 ] state=[-1.16815377 -1.61267356 -0.01476391  0.47076917], action=1, reward=1.0, next_state=[-1.20040724 -1.41734621 -0.00534853  0.17346965]\n",
      "[ episode 351 ][ timestamp 95 ] state=[-1.20040724 -1.41734621 -0.00534853  0.17346965], action=0, reward=1.0, next_state=[-1.22875416 -1.6123912  -0.00187914  0.4644605 ]\n",
      "[ episode 351 ][ timestamp 96 ] state=[-1.22875416 -1.6123912  -0.00187914  0.4644605 ], action=1, reward=1.0, next_state=[-1.26100199 -1.41724275  0.00741007  0.17118587]\n",
      "[ episode 351 ][ timestamp 97 ] state=[-1.26100199 -1.41724275  0.00741007  0.17118587], action=0, reward=1.0, next_state=[-1.28934684 -1.61246997  0.01083379  0.4661972 ]\n",
      "[ episode 351 ][ timestamp 98 ] state=[-1.28934684 -1.61246997  0.01083379  0.4661972 ], action=1, reward=1.0, next_state=[-1.32159624 -1.41750276  0.02015774  0.17694862]\n",
      "[ episode 351 ][ timestamp 99 ] state=[-1.32159624 -1.41750276  0.02015774  0.17694862], action=0, reward=1.0, next_state=[-1.3499463  -1.6129073   0.02369671  0.47592185]\n",
      "[ episode 351 ][ timestamp 100 ] state=[-1.3499463  -1.6129073   0.02369671  0.47592185], action=0, reward=1.0, next_state=[-1.38220444 -1.8083557   0.03321515  0.77597833]\n",
      "[ episode 351 ][ timestamp 101 ] state=[-1.38220444 -1.8083557   0.03321515  0.77597833], action=1, reward=1.0, next_state=[-1.41837156 -1.61370595  0.04873471  0.49392821]\n",
      "[ episode 351 ][ timestamp 102 ] state=[-1.41837156 -1.61370595  0.04873471  0.49392821], action=1, reward=1.0, next_state=[-1.45064568 -1.41930398  0.05861328  0.21699381]\n",
      "[ episode 351 ][ timestamp 103 ] state=[-1.45064568 -1.41930398  0.05861328  0.21699381], action=0, reward=1.0, next_state=[-1.47903176 -1.61521273  0.06295315  0.52757473]\n",
      "[ episode 351 ][ timestamp 104 ] state=[-1.47903176 -1.61521273  0.06295315  0.52757473], action=0, reward=1.0, next_state=[-1.51133601 -1.81116131  0.07350465  0.83941137]\n",
      "[ episode 351 ][ timestamp 105 ] state=[-1.51133601 -1.81116131  0.07350465  0.83941137], action=1, reward=1.0, next_state=[-1.54755924 -1.61711582  0.09029287  0.57072   ]\n",
      "[ episode 351 ][ timestamp 106 ] state=[-1.54755924 -1.61711582  0.09029287  0.57072   ], action=0, reward=1.0, next_state=[-1.57990155 -1.81338026  0.10170727  0.89042745]\n",
      "[ episode 351 ][ timestamp 107 ] state=[-1.57990155 -1.81338026  0.10170727  0.89042745], action=0, reward=1.0, next_state=[-1.61616916 -2.00972416  0.11951582  1.21327174]\n",
      "[ episode 351 ][ timestamp 108 ] state=[-1.61616916 -2.00972416  0.11951582  1.21327174], action=1, reward=1.0, next_state=[-1.65636364 -1.81632991  0.14378126  0.96030379]\n",
      "[ episode 351 ][ timestamp 109 ] state=[-1.65636364 -1.81632991  0.14378126  0.96030379], action=1, reward=1.0, next_state=[-1.69269024 -1.6234026   0.16298733  0.71602517]\n",
      "[ episode 351 ][ timestamp 110 ] state=[-1.69269024 -1.6234026   0.16298733  0.71602517], action=1, reward=1.0, next_state=[-1.72515829 -1.43086673  0.17730784  0.4787553 ]\n",
      "[ episode 351 ][ timestamp 111 ] state=[-1.72515829 -1.43086673  0.17730784  0.4787553 ], action=0, reward=1.0, next_state=[-1.75377563 -1.62799041  0.18688294  0.82166089]\n",
      "[ episode 351 ][ timestamp 112 ] state=[-1.75377563 -1.62799041  0.18688294  0.82166089], action=1, reward=1.0, next_state=[-1.78633544 -1.43584962  0.20331616  0.59309231]\n",
      "[ episode 351 ][ timestamp 113 ] state=[-1.78633544 -1.43584962  0.20331616  0.59309231], action=0, reward=-1.0, next_state=[-1.81505243 -1.63314984  0.21517801  0.94231074]\n",
      "[ Ended! ] Episode 351: Exploration_rate=0.1730128104744653. Score=113.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 352 ] state=[-0.02012448 -0.00445563 -0.00859701 -0.04066189]\n",
      "[ episode 352 ][ timestamp 1 ] state=[-0.02012448 -0.00445563 -0.00859701 -0.04066189], action=1, reward=1.0, next_state=[-0.02021359  0.19078854 -0.00941025 -0.33604481]\n",
      "[ episode 352 ][ timestamp 2 ] state=[-0.02021359  0.19078854 -0.00941025 -0.33604481], action=1, reward=1.0, next_state=[-0.01639782  0.38604314 -0.01613115 -0.63168032]\n",
      "[ episode 352 ][ timestamp 3 ] state=[-0.01639782  0.38604314 -0.01613115 -0.63168032], action=0, reward=1.0, next_state=[-0.00867696  0.19114992 -0.02876475 -0.34412088]\n",
      "[ episode 352 ][ timestamp 4 ] state=[-0.00867696  0.19114992 -0.02876475 -0.34412088], action=1, reward=1.0, next_state=[-0.00485396  0.38666902 -0.03564717 -0.64573387]\n",
      "[ episode 352 ][ timestamp 5 ] state=[-0.00485396  0.38666902 -0.03564717 -0.64573387], action=0, reward=1.0, next_state=[ 0.00287942  0.19206145 -0.04856185 -0.36448602]\n",
      "[ episode 352 ][ timestamp 6 ] state=[ 0.00287942  0.19206145 -0.04856185 -0.36448602], action=1, reward=1.0, next_state=[ 0.00672065  0.38783871 -0.05585157 -0.67207728]\n",
      "[ episode 352 ][ timestamp 7 ] state=[ 0.00672065  0.38783871 -0.05585157 -0.67207728], action=0, reward=1.0, next_state=[ 0.01447743  0.19353579 -0.06929311 -0.39748919]\n",
      "[ episode 352 ][ timestamp 8 ] state=[ 0.01447743  0.19353579 -0.06929311 -0.39748919], action=0, reward=1.0, next_state=[ 0.01834814 -0.00053817 -0.0772429  -0.12743273]\n",
      "[ episode 352 ][ timestamp 9 ] state=[ 0.01834814 -0.00053817 -0.0772429  -0.12743273], action=1, reward=1.0, next_state=[ 0.01833738  0.19560048 -0.07979155 -0.44345028]\n",
      "[ episode 352 ][ timestamp 10 ] state=[ 0.01833738  0.19560048 -0.07979155 -0.44345028], action=0, reward=1.0, next_state=[ 0.02224939  0.00169285 -0.08866056 -0.1769481 ]\n",
      "[ episode 352 ][ timestamp 11 ] state=[ 0.02224939  0.00169285 -0.08866056 -0.1769481 ], action=1, reward=1.0, next_state=[ 0.02228324  0.19796437 -0.09219952 -0.49623108]\n",
      "[ episode 352 ][ timestamp 12 ] state=[ 0.02228324  0.19796437 -0.09219952 -0.49623108], action=1, reward=1.0, next_state=[ 0.02624253  0.39425725 -0.10212414 -0.81648808]\n",
      "[ episode 352 ][ timestamp 13 ] state=[ 0.02624253  0.39425725 -0.10212414 -0.81648808], action=0, reward=1.0, next_state=[ 0.03412768  0.20067066 -0.1184539  -0.55759345]\n",
      "[ episode 352 ][ timestamp 14 ] state=[ 0.03412768  0.20067066 -0.1184539  -0.55759345], action=0, reward=1.0, next_state=[ 0.03814109  0.00739332 -0.12960577 -0.30445309]\n",
      "[ episode 352 ][ timestamp 15 ] state=[ 0.03814109  0.00739332 -0.12960577 -0.30445309], action=0, reward=1.0, next_state=[ 0.03828896 -0.18566636 -0.13569483 -0.05528988]\n",
      "[ episode 352 ][ timestamp 16 ] state=[ 0.03828896 -0.18566636 -0.13569483 -0.05528988], action=1, reward=1.0, next_state=[ 0.03457563  0.01111395 -0.13680063 -0.38751898]\n",
      "[ episode 352 ][ timestamp 17 ] state=[ 0.03457563  0.01111395 -0.13680063 -0.38751898], action=0, reward=1.0, next_state=[ 0.03479791 -0.18182788 -0.14455101 -0.14090415]\n",
      "[ episode 352 ][ timestamp 18 ] state=[ 0.03479791 -0.18182788 -0.14455101 -0.14090415], action=0, reward=1.0, next_state=[ 0.03116135 -0.3746156  -0.14736909  0.10291131]\n",
      "[ episode 352 ][ timestamp 19 ] state=[ 0.03116135 -0.3746156  -0.14736909  0.10291131], action=0, reward=1.0, next_state=[ 0.02366904 -0.56735199 -0.14531087  0.34571236]\n",
      "[ episode 352 ][ timestamp 20 ] state=[ 0.02366904 -0.56735199 -0.14531087  0.34571236], action=0, reward=1.0, next_state=[ 0.012322   -0.76014029 -0.13839662  0.5892759 ]\n",
      "[ episode 352 ][ timestamp 21 ] state=[ 0.012322   -0.76014029 -0.13839662  0.5892759 ], action=1, reward=1.0, next_state=[-0.00288081 -0.56337951 -0.1266111   0.2563979 ]\n",
      "[ episode 352 ][ timestamp 22 ] state=[-0.00288081 -0.56337951 -0.1266111   0.2563979 ], action=1, reward=1.0, next_state=[-0.0141484  -0.36669868 -0.12148314 -0.07338615]\n",
      "[ episode 352 ][ timestamp 23 ] state=[-0.0141484  -0.36669868 -0.12148314 -0.07338615], action=1, reward=1.0, next_state=[-0.02148237 -0.17006351 -0.12295087 -0.40179336]\n",
      "[ episode 352 ][ timestamp 24 ] state=[-0.02148237 -0.17006351 -0.12295087 -0.40179336], action=0, reward=1.0, next_state=[-0.02488364 -0.36324659 -0.13098673 -0.15026278]\n",
      "[ episode 352 ][ timestamp 25 ] state=[-0.02488364 -0.36324659 -0.13098673 -0.15026278], action=0, reward=1.0, next_state=[-0.03214857 -0.55627334 -0.13399199  0.09839693]\n",
      "[ episode 352 ][ timestamp 26 ] state=[-0.03214857 -0.55627334 -0.13399199  0.09839693], action=1, reward=1.0, next_state=[-0.04327404 -0.35951087 -0.13202405 -0.23337713]\n",
      "[ episode 352 ][ timestamp 27 ] state=[-0.04327404 -0.35951087 -0.13202405 -0.23337713], action=1, reward=1.0, next_state=[-0.05046426 -0.16277389 -0.13669159 -0.56461686]\n",
      "[ episode 352 ][ timestamp 28 ] state=[-0.05046426 -0.16277389 -0.13669159 -0.56461686], action=1, reward=1.0, next_state=[-0.05371973  0.0339744  -0.14798393 -0.89704875]\n",
      "[ episode 352 ][ timestamp 29 ] state=[-0.05371973  0.0339744  -0.14798393 -0.89704875], action=0, reward=1.0, next_state=[-0.05304025 -0.1588651  -0.16592491 -0.65429966]\n",
      "[ episode 352 ][ timestamp 30 ] state=[-0.05304025 -0.1588651  -0.16592491 -0.65429966], action=1, reward=1.0, next_state=[-0.05621755  0.03813083 -0.1790109  -0.99429364]\n",
      "[ episode 352 ][ timestamp 31 ] state=[-0.05621755  0.03813083 -0.1790109  -0.99429364], action=0, reward=1.0, next_state=[-0.05545493 -0.1542039  -0.19889677 -0.7627503 ]\n",
      "[ episode 352 ][ timestamp 32 ] state=[-0.05545493 -0.1542039  -0.19889677 -0.7627503 ], action=0, reward=-1.0, next_state=[-0.05853901 -0.34611195 -0.21415178 -0.53865424]\n",
      "[ Ended! ] Episode 352: Exploration_rate=0.17214774642209296. Score=32.\n",
      "[ Experience replay ] starts\n",
      "[ episode 353 ] state=[-0.03551988  0.02725787  0.01127134  0.04140675]\n",
      "[ episode 353 ][ timestamp 1 ] state=[-0.03551988  0.02725787  0.01127134  0.04140675], action=0, reward=1.0, next_state=[-0.03497472 -0.16802388  0.01209947  0.33762447]\n",
      "[ episode 353 ][ timestamp 2 ] state=[-0.03497472 -0.16802388  0.01209947  0.33762447], action=1, reward=1.0, next_state=[-0.0383352   0.02692382  0.01885196  0.04878148]\n",
      "[ episode 353 ][ timestamp 3 ] state=[-0.0383352   0.02692382  0.01885196  0.04878148], action=1, reward=1.0, next_state=[-0.03779673  0.22177045  0.01982759 -0.23789438]\n",
      "[ episode 353 ][ timestamp 4 ] state=[-0.03779673  0.22177045  0.01982759 -0.23789438], action=1, reward=1.0, next_state=[-0.03336132  0.41660361  0.01506971 -0.52425774]\n",
      "[ episode 353 ][ timestamp 5 ] state=[-0.03336132  0.41660361  0.01506971 -0.52425774], action=1, reward=1.0, next_state=[-0.02502924  0.61151027  0.00458455 -0.81215422]\n",
      "[ episode 353 ][ timestamp 6 ] state=[-0.02502924  0.61151027  0.00458455 -0.81215422], action=0, reward=1.0, next_state=[-0.01279904  0.41632582 -0.01165853 -0.51803277]\n",
      "[ episode 353 ][ timestamp 7 ] state=[-0.01279904  0.41632582 -0.01165853 -0.51803277], action=1, reward=1.0, next_state=[-0.00447252  0.61160996 -0.02201919 -0.8143666 ]\n",
      "[ episode 353 ][ timestamp 8 ] state=[-0.00447252  0.61160996 -0.02201919 -0.8143666 ], action=0, reward=1.0, next_state=[ 0.00775968  0.41679636 -0.03830652 -0.52869016]\n",
      "[ episode 353 ][ timestamp 9 ] state=[ 0.00775968  0.41679636 -0.03830652 -0.52869016], action=0, reward=1.0, next_state=[ 0.0160956   0.22223368 -0.04888032 -0.2483196 ]\n",
      "[ episode 353 ][ timestamp 10 ] state=[ 0.0160956   0.22223368 -0.04888032 -0.2483196 ], action=1, reward=1.0, next_state=[ 0.02054028  0.41801839 -0.05384672 -0.55601099]\n",
      "[ episode 353 ][ timestamp 11 ] state=[ 0.02054028  0.41801839 -0.05384672 -0.55601099], action=0, reward=1.0, next_state=[ 0.02890065  0.22369215 -0.06496693 -0.28076738]\n",
      "[ episode 353 ][ timestamp 12 ] state=[ 0.02890065  0.22369215 -0.06496693 -0.28076738], action=1, reward=1.0, next_state=[ 0.03337449  0.41967776 -0.07058228 -0.59321247]\n",
      "[ episode 353 ][ timestamp 13 ] state=[ 0.03337449  0.41967776 -0.07058228 -0.59321247], action=0, reward=1.0, next_state=[ 0.04176804  0.22561112 -0.08244653 -0.32357128]\n",
      "[ episode 353 ][ timestamp 14 ] state=[ 0.04176804  0.22561112 -0.08244653 -0.32357128], action=0, reward=1.0, next_state=[ 0.04628027  0.03175401 -0.08891796 -0.05798518]\n",
      "[ episode 353 ][ timestamp 15 ] state=[ 0.04628027  0.03175401 -0.08891796 -0.05798518], action=1, reward=1.0, next_state=[ 0.04691535  0.22803092 -0.09007766 -0.37734488]\n",
      "[ episode 353 ][ timestamp 16 ] state=[ 0.04691535  0.22803092 -0.09007766 -0.37734488], action=0, reward=1.0, next_state=[ 0.05147596  0.03429606 -0.09762456 -0.11436779]\n",
      "[ episode 353 ][ timestamp 17 ] state=[ 0.05147596  0.03429606 -0.09762456 -0.11436779], action=0, reward=1.0, next_state=[ 0.05216189 -0.15930136 -0.09991191  0.14598957]\n",
      "[ episode 353 ][ timestamp 18 ] state=[ 0.05216189 -0.15930136 -0.09991191  0.14598957], action=1, reward=1.0, next_state=[ 0.04897586  0.0370989  -0.09699212 -0.1764669 ]\n",
      "[ episode 353 ][ timestamp 19 ] state=[ 0.04897586  0.0370989  -0.09699212 -0.1764669 ], action=0, reward=1.0, next_state=[ 0.04971784 -0.15651083 -0.10052146  0.08411175]\n",
      "[ episode 353 ][ timestamp 20 ] state=[ 0.04971784 -0.15651083 -0.10052146  0.08411175], action=1, reward=1.0, next_state=[ 0.04658762  0.03989763 -0.09883923 -0.2385173 ]\n",
      "[ episode 353 ][ timestamp 21 ] state=[ 0.04658762  0.03989763 -0.09883923 -0.2385173 ], action=0, reward=1.0, next_state=[ 0.04738557 -0.15368359 -0.10360957  0.02142589]\n",
      "[ episode 353 ][ timestamp 22 ] state=[ 0.04738557 -0.15368359 -0.10360957  0.02142589], action=1, reward=1.0, next_state=[ 0.0443119   0.04275987 -0.10318105 -0.30206585]\n",
      "[ episode 353 ][ timestamp 23 ] state=[ 0.0443119   0.04275987 -0.10318105 -0.30206585], action=0, reward=1.0, next_state=[ 0.0451671  -0.15075175 -0.10922237 -0.04362362]\n",
      "[ episode 353 ][ timestamp 24 ] state=[ 0.0451671  -0.15075175 -0.10922237 -0.04362362], action=1, reward=1.0, next_state=[ 0.04215206  0.04575313 -0.11009484 -0.3686721 ]\n",
      "[ episode 353 ][ timestamp 25 ] state=[ 0.04215206  0.04575313 -0.11009484 -0.3686721 ], action=0, reward=1.0, next_state=[ 0.04306713 -0.14764637 -0.11746829 -0.11263175]\n",
      "[ episode 353 ][ timestamp 26 ] state=[ 0.04306713 -0.14764637 -0.11746829 -0.11263175], action=0, reward=1.0, next_state=[ 0.0401142  -0.34090632 -0.11972092  0.14080412]\n",
      "[ episode 353 ][ timestamp 27 ] state=[ 0.0401142  -0.34090632 -0.11972092  0.14080412], action=1, reward=1.0, next_state=[ 0.03329607 -0.14429129 -0.11690484 -0.18712131]\n",
      "[ episode 353 ][ timestamp 28 ] state=[ 0.03329607 -0.14429129 -0.11690484 -0.18712131], action=1, reward=1.0, next_state=[ 0.03041025  0.05229234 -0.12064726 -0.51427584]\n",
      "[ episode 353 ][ timestamp 29 ] state=[ 0.03041025  0.05229234 -0.12064726 -0.51427584], action=0, reward=1.0, next_state=[ 0.03145609 -0.14094234 -0.13093278 -0.26191508]\n",
      "[ episode 353 ][ timestamp 30 ] state=[ 0.03145609 -0.14094234 -0.13093278 -0.26191508], action=0, reward=1.0, next_state=[ 0.02863725 -0.33397589 -0.13617108 -0.0132275 ]\n",
      "[ episode 353 ][ timestamp 31 ] state=[ 0.02863725 -0.33397589 -0.13617108 -0.0132275 ], action=1, reward=1.0, next_state=[ 0.02195773 -0.13719047 -0.13643563 -0.34558387]\n",
      "[ episode 353 ][ timestamp 32 ] state=[ 0.02195773 -0.13719047 -0.13643563 -0.34558387], action=0, reward=1.0, next_state=[ 0.01921392 -0.3301346  -0.14334731 -0.09884494]\n",
      "[ episode 353 ][ timestamp 33 ] state=[ 0.01921392 -0.3301346  -0.14334731 -0.09884494], action=1, reward=1.0, next_state=[ 0.01261123 -0.1332802  -0.14532421 -0.43309786]\n",
      "[ episode 353 ][ timestamp 34 ] state=[ 0.01261123 -0.1332802  -0.14532421 -0.43309786], action=0, reward=1.0, next_state=[ 0.00994562 -0.32607787 -0.15398617 -0.18952486]\n",
      "[ episode 353 ][ timestamp 35 ] state=[ 0.00994562 -0.32607787 -0.15398617 -0.18952486], action=0, reward=1.0, next_state=[ 0.00342407 -0.51869999 -0.15777666  0.05089629]\n",
      "[ episode 353 ][ timestamp 36 ] state=[ 0.00342407 -0.51869999 -0.15777666  0.05089629], action=0, reward=1.0, next_state=[-0.00694993 -0.71124913 -0.15675874  0.28993842]\n",
      "[ episode 353 ][ timestamp 37 ] state=[-0.00694993 -0.71124913 -0.15675874  0.28993842], action=0, reward=1.0, next_state=[-0.02117492 -0.90382895 -0.15095997  0.52936761]\n",
      "[ episode 353 ][ timestamp 38 ] state=[-0.02117492 -0.90382895 -0.15095997  0.52936761], action=1, reward=1.0, next_state=[-0.0392515  -0.70694168 -0.14037262  0.19318162]\n",
      "[ episode 353 ][ timestamp 39 ] state=[-0.0392515  -0.70694168 -0.14037262  0.19318162], action=1, reward=1.0, next_state=[-0.05339033 -0.51011994 -0.13650898 -0.14028123]\n",
      "[ episode 353 ][ timestamp 40 ] state=[-0.05339033 -0.51011994 -0.13650898 -0.14028123], action=0, reward=1.0, next_state=[-0.06359273 -0.70304956 -0.13931461  0.10641189]\n",
      "[ episode 353 ][ timestamp 41 ] state=[-0.06359273 -0.70304956 -0.13931461  0.10641189], action=1, reward=1.0, next_state=[-0.07765372 -0.50623462 -0.13718637 -0.22677636]\n",
      "[ episode 353 ][ timestamp 42 ] state=[-0.07765372 -0.50623462 -0.13718637 -0.22677636], action=0, reward=1.0, next_state=[-0.08777841 -0.69915657 -0.1417219   0.01968132]\n",
      "[ episode 353 ][ timestamp 43 ] state=[-0.08777841 -0.69915657 -0.1417219   0.01968132], action=0, reward=1.0, next_state=[-0.10176154 -0.89199167 -0.14132827  0.2645071 ]\n",
      "[ episode 353 ][ timestamp 44 ] state=[-0.10176154 -0.89199167 -0.14132827  0.2645071 ], action=1, reward=1.0, next_state=[-0.11960138 -0.69516511 -0.13603813 -0.06920146]\n",
      "[ episode 353 ][ timestamp 45 ] state=[-0.11960138 -0.69516511 -0.13603813 -0.06920146], action=0, reward=1.0, next_state=[-0.13350468 -0.88810104 -0.13742216  0.17765669]\n",
      "[ episode 353 ][ timestamp 46 ] state=[-0.13350468 -0.88810104 -0.13742216  0.17765669], action=1, reward=1.0, next_state=[-0.1512667  -0.69130742 -0.13386903 -0.15502589]\n",
      "[ episode 353 ][ timestamp 47 ] state=[-0.1512667  -0.69130742 -0.13386903 -0.15502589], action=0, reward=1.0, next_state=[-0.16509285 -0.88428394 -0.13696954  0.09260898]\n",
      "[ episode 353 ][ timestamp 48 ] state=[-0.16509285 -0.88428394 -0.13696954  0.09260898], action=1, reward=1.0, next_state=[-0.18277853 -0.68749172 -0.13511736 -0.23995796]\n",
      "[ episode 353 ][ timestamp 49 ] state=[-0.18277853 -0.68749172 -0.13511736 -0.23995796], action=0, reward=1.0, next_state=[-0.19652836 -0.88045075 -0.13991652  0.00723877]\n",
      "[ episode 353 ][ timestamp 50 ] state=[-0.19652836 -0.88045075 -0.13991652  0.00723877], action=0, reward=1.0, next_state=[-0.21413738 -1.07331772 -0.13977175  0.2527107 ]\n",
      "[ episode 353 ][ timestamp 51 ] state=[-0.21413738 -1.07331772 -0.13977175  0.2527107 ], action=1, reward=1.0, next_state=[-0.23560373 -0.87650543 -0.13471753 -0.08058794]\n",
      "[ episode 353 ][ timestamp 52 ] state=[-0.23560373 -0.87650543 -0.13471753 -0.08058794], action=0, reward=1.0, next_state=[-0.25313384 -1.06946478 -0.13632929  0.16674131]\n",
      "[ episode 353 ][ timestamp 53 ] state=[-0.25313384 -1.06946478 -0.13632929  0.16674131], action=1, reward=1.0, next_state=[-0.27452313 -0.87268145 -0.13299447 -0.16565169]\n",
      "[ episode 353 ][ timestamp 54 ] state=[-0.27452313 -0.87268145 -0.13299447 -0.16565169], action=0, reward=1.0, next_state=[-0.29197676 -1.06567372 -0.1363075   0.08229511]\n",
      "[ episode 353 ][ timestamp 55 ] state=[-0.29197676 -1.06567372 -0.1363075   0.08229511], action=1, reward=1.0, next_state=[-0.31329024 -0.86888782 -0.1346616  -0.25009624]\n",
      "[ episode 353 ][ timestamp 56 ] state=[-0.31329024 -0.86888782 -0.1346616  -0.25009624], action=0, reward=1.0, next_state=[-0.33066799 -1.06185549 -0.13966352 -0.00273616]\n",
      "[ episode 353 ][ timestamp 57 ] state=[-0.33066799 -1.06185549 -0.13966352 -0.00273616], action=0, reward=1.0, next_state=[-0.3519051  -1.25472691 -0.13971825  0.24282625]\n",
      "[ episode 353 ][ timestamp 58 ] state=[-0.3519051  -1.25472691 -0.13971825  0.24282625], action=1, reward=1.0, next_state=[-0.37699964 -1.05791448 -0.13486172 -0.09045922]\n",
      "[ episode 353 ][ timestamp 59 ] state=[-0.37699964 -1.05791448 -0.13486172 -0.09045922], action=0, reward=1.0, next_state=[-0.39815793 -1.25087152 -0.13667091  0.15681898]\n",
      "[ episode 353 ][ timestamp 60 ] state=[-0.39815793 -1.25087152 -0.13667091  0.15681898], action=1, reward=1.0, next_state=[-0.42317536 -1.05408438 -0.13353453 -0.17566546]\n",
      "[ episode 353 ][ timestamp 61 ] state=[-0.42317536 -1.05408438 -0.13353453 -0.17566546], action=0, reward=1.0, next_state=[-0.44425705 -1.24706765 -0.13704783  0.07208981]\n",
      "[ episode 353 ][ timestamp 62 ] state=[-0.44425705 -1.24706765 -0.13704783  0.07208981], action=1, reward=1.0, next_state=[-0.4691984  -1.0502742  -0.13560604 -0.2604986 ]\n",
      "[ episode 353 ][ timestamp 63 ] state=[-0.4691984  -1.0502742  -0.13560604 -0.2604986 ], action=0, reward=1.0, next_state=[-0.49020389 -1.24322604 -0.14081601 -0.01347402]\n",
      "[ episode 353 ][ timestamp 64 ] state=[-0.49020389 -1.24322604 -0.14081601 -0.01347402], action=1, reward=1.0, next_state=[-0.51506841 -1.04639489 -0.14108549 -0.34706154]\n",
      "[ episode 353 ][ timestamp 65 ] state=[-0.51506841 -1.04639489 -0.14108549 -0.34706154], action=0, reward=1.0, next_state=[-0.5359963  -1.2392577  -0.14802672 -0.10198344]\n",
      "[ episode 353 ][ timestamp 66 ] state=[-0.5359963  -1.2392577  -0.14802672 -0.10198344], action=0, reward=1.0, next_state=[-0.56078146 -1.43198236 -0.15006639  0.14058101]\n",
      "[ episode 353 ][ timestamp 67 ] state=[-0.56078146 -1.43198236 -0.15006639  0.14058101], action=1, reward=1.0, next_state=[-0.58942111 -1.23506533 -0.14725477 -0.19542896]\n",
      "[ episode 353 ][ timestamp 68 ] state=[-0.58942111 -1.23506533 -0.14725477 -0.19542896], action=0, reward=1.0, next_state=[-0.61412241 -1.42780769 -0.15116335  0.04741906]\n",
      "[ episode 353 ][ timestamp 69 ] state=[-0.61412241 -1.42780769 -0.15116335  0.04741906], action=1, reward=1.0, next_state=[-0.64267857 -1.23087781 -0.15021497 -0.28888022]\n",
      "[ episode 353 ][ timestamp 70 ] state=[-0.64267857 -1.23087781 -0.15021497 -0.28888022], action=0, reward=1.0, next_state=[-0.66729612 -1.42357413 -0.15599257 -0.04708798]\n",
      "[ episode 353 ][ timestamp 71 ] state=[-0.66729612 -1.42357413 -0.15599257 -0.04708798], action=1, reward=1.0, next_state=[-0.69576761 -1.22659935 -0.15693433 -0.38463864]\n",
      "[ episode 353 ][ timestamp 72 ] state=[-0.69576761 -1.22659935 -0.15693433 -0.38463864], action=0, reward=1.0, next_state=[-0.72029959 -1.41918575 -0.1646271  -0.1452586 ]\n",
      "[ episode 353 ][ timestamp 73 ] state=[-0.72029959 -1.41918575 -0.1646271  -0.1452586 ], action=1, reward=1.0, next_state=[-0.74868331 -1.22213585 -0.16753228 -0.48501918]\n",
      "[ episode 353 ][ timestamp 74 ] state=[-0.74868331 -1.22213585 -0.16753228 -0.48501918], action=0, reward=1.0, next_state=[-0.77312602 -1.41454665 -0.17723266 -0.24946822]\n",
      "[ episode 353 ][ timestamp 75 ] state=[-0.77312602 -1.41454665 -0.17723266 -0.24946822], action=1, reward=1.0, next_state=[-0.80141696 -1.21739493 -0.18222202 -0.59239738]\n",
      "[ episode 353 ][ timestamp 76 ] state=[-0.80141696 -1.21739493 -0.18222202 -0.59239738], action=0, reward=1.0, next_state=[-0.82576486 -1.40956132 -0.19406997 -0.3621975 ]\n",
      "[ episode 353 ][ timestamp 77 ] state=[-0.82576486 -1.40956132 -0.19406997 -0.3621975 ], action=1, reward=1.0, next_state=[-0.85395608 -1.21228703 -0.20131392 -0.70925302]\n",
      "[ episode 353 ][ timestamp 78 ] state=[-0.85395608 -1.21228703 -0.20131392 -0.70925302], action=0, reward=-1.0, next_state=[-0.87820182 -1.40413608 -0.21549898 -0.48607844]\n",
      "[ Ended! ] Episode 353: Exploration_rate=0.1712870076899825. Score=78.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 354 ] state=[ 0.0185754   0.01235829 -0.0303771   0.00765918]\n",
      "[ episode 354 ][ timestamp 1 ] state=[ 0.0185754   0.01235829 -0.0303771   0.00765918], action=0, reward=1.0, next_state=[ 0.01882256 -0.18231514 -0.03022391  0.29060511]\n",
      "[ episode 354 ][ timestamp 2 ] state=[ 0.01882256 -0.18231514 -0.03022391  0.29060511], action=1, reward=1.0, next_state=[ 0.01517626  0.01322445 -0.02441181 -0.01145479]\n",
      "[ episode 354 ][ timestamp 3 ] state=[ 0.01517626  0.01322445 -0.02441181 -0.01145479], action=0, reward=1.0, next_state=[ 0.01544075 -0.18153905 -0.02464091  0.27342706]\n",
      "[ episode 354 ][ timestamp 4 ] state=[ 0.01544075 -0.18153905 -0.02464091  0.27342706], action=1, reward=1.0, next_state=[ 0.01180997  0.01392566 -0.01917237 -0.0269247 ]\n",
      "[ episode 354 ][ timestamp 5 ] state=[ 0.01180997  0.01392566 -0.01917237 -0.0269247 ], action=0, reward=1.0, next_state=[ 0.01208848 -0.18091617 -0.01971086  0.25964801]\n",
      "[ episode 354 ][ timestamp 6 ] state=[ 0.01208848 -0.18091617 -0.01971086  0.25964801], action=1, reward=1.0, next_state=[ 0.00847016  0.01448154 -0.0145179  -0.03918624]\n",
      "[ episode 354 ][ timestamp 7 ] state=[ 0.00847016  0.01448154 -0.0145179  -0.03918624], action=0, reward=1.0, next_state=[ 0.00875979 -0.18042925 -0.01530162  0.24888102]\n",
      "[ episode 354 ][ timestamp 8 ] state=[ 0.00875979 -0.18042925 -0.01530162  0.24888102], action=0, reward=1.0, next_state=[ 0.0051512  -0.37532937 -0.010324    0.53669848]\n",
      "[ episode 354 ][ timestamp 9 ] state=[ 0.0051512  -0.37532937 -0.010324    0.53669848], action=1, reward=1.0, next_state=[-0.00235538 -0.1800638   0.00040997  0.24078051]\n",
      "[ episode 354 ][ timestamp 10 ] state=[-0.00235538 -0.1800638   0.00040997  0.24078051], action=1, reward=1.0, next_state=[-0.00595666  0.0150523   0.00522558 -0.05177307]\n",
      "[ episode 354 ][ timestamp 11 ] state=[-0.00595666  0.0150523   0.00522558 -0.05177307], action=0, reward=1.0, next_state=[-0.00565561 -0.18014419  0.00419011  0.24255398]\n",
      "[ episode 354 ][ timestamp 12 ] state=[-0.00565561 -0.18014419  0.00419011  0.24255398], action=1, reward=1.0, next_state=[-0.0092585   0.01491766  0.00904119 -0.04880434]\n",
      "[ episode 354 ][ timestamp 13 ] state=[-0.0092585   0.01491766  0.00904119 -0.04880434], action=0, reward=1.0, next_state=[-0.00896014 -0.18033276  0.00806511  0.2467174 ]\n",
      "[ episode 354 ][ timestamp 14 ] state=[-0.00896014 -0.18033276  0.00806511  0.2467174 ], action=1, reward=1.0, next_state=[-0.0125668   0.01467308  0.01299946 -0.04341074]\n",
      "[ episode 354 ][ timestamp 15 ] state=[-0.0125668   0.01467308  0.01299946 -0.04341074], action=0, reward=1.0, next_state=[-0.01227334 -0.18063284  0.01213124  0.25334513]\n",
      "[ episode 354 ][ timestamp 16 ] state=[-0.01227334 -0.18063284  0.01213124  0.25334513], action=1, reward=1.0, next_state=[-0.01588599  0.01431381  0.01719814 -0.03548683]\n",
      "[ episode 354 ][ timestamp 17 ] state=[-0.01588599  0.01431381  0.01719814 -0.03548683], action=0, reward=1.0, next_state=[-0.01559972 -0.18105049  0.01648841  0.26257229]\n",
      "[ episode 354 ][ timestamp 18 ] state=[-0.01559972 -0.18105049  0.01648841  0.26257229], action=1, reward=1.0, next_state=[-0.01922073  0.01383226  0.02173985 -0.02486475]\n",
      "[ episode 354 ][ timestamp 19 ] state=[-0.01922073  0.01383226  0.02173985 -0.02486475], action=1, reward=1.0, next_state=[-0.01894408  0.20863581  0.02124256 -0.31061   ]\n",
      "[ episode 354 ][ timestamp 20 ] state=[-0.01894408  0.20863581  0.02124256 -0.31061   ], action=0, reward=1.0, next_state=[-0.01477137  0.01321775  0.01503036 -0.01130421]\n",
      "[ episode 354 ][ timestamp 21 ] state=[-0.01477137  0.01321775  0.01503036 -0.01130421], action=1, reward=1.0, next_state=[-0.01450701  0.20812096  0.01480427 -0.29920724]\n",
      "[ episode 354 ][ timestamp 22 ] state=[-0.01450701  0.20812096  0.01480427 -0.29920724], action=0, reward=1.0, next_state=[-0.01034459  0.01279115  0.00882013 -0.00189233]\n",
      "[ episode 354 ][ timestamp 23 ] state=[-0.01034459  0.01279115  0.00882013 -0.00189233], action=1, reward=1.0, next_state=[-0.01008877  0.2077855   0.00878228 -0.2917794 ]\n",
      "[ episode 354 ][ timestamp 24 ] state=[-0.01008877  0.2077855   0.00878228 -0.2917794 ], action=0, reward=1.0, next_state=[-0.00593306  0.01253944  0.00294669  0.00366036]\n",
      "[ episode 354 ][ timestamp 25 ] state=[-0.00593306  0.01253944  0.00294669  0.00366036], action=1, reward=1.0, next_state=[-0.00568227  0.20761901  0.0030199  -0.28809139]\n",
      "[ episode 354 ][ timestamp 26 ] state=[-0.00568227  0.20761901  0.0030199  -0.28809139], action=0, reward=1.0, next_state=[-0.00152989  0.01245412 -0.00274193  0.00554245]\n",
      "[ episode 354 ][ timestamp 27 ] state=[-0.00152989  0.01245412 -0.00274193  0.00554245], action=1, reward=1.0, next_state=[-0.00128081  0.20761529 -0.00263108 -0.28800433]\n",
      "[ episode 354 ][ timestamp 28 ] state=[-0.00128081  0.20761529 -0.00263108 -0.28800433], action=0, reward=1.0, next_state=[ 0.0028715   0.01253096 -0.00839116  0.00384762]\n",
      "[ episode 354 ][ timestamp 29 ] state=[ 0.0028715   0.01253096 -0.00839116  0.00384762], action=0, reward=1.0, next_state=[ 0.00312212 -0.18246966 -0.00831421  0.29387127]\n",
      "[ episode 354 ][ timestamp 30 ] state=[ 0.00312212 -0.18246966 -0.00831421  0.29387127], action=1, reward=1.0, next_state=[-0.00052728  0.01276984 -0.00243679 -0.0014222 ]\n",
      "[ episode 354 ][ timestamp 31 ] state=[-0.00052728  0.01276984 -0.00243679 -0.0014222 ], action=1, reward=1.0, next_state=[-2.71879751e-04  2.07926654e-01 -2.46523080e-03 -2.94872969e-01]\n",
      "[ episode 354 ][ timestamp 32 ] state=[-2.71879751e-04  2.07926654e-01 -2.46523080e-03 -2.94872969e-01], action=0, reward=1.0, next_state=[ 0.00388665  0.01283993 -0.00836269 -0.00296856]\n",
      "[ episode 354 ][ timestamp 33 ] state=[ 0.00388665  0.01283993 -0.00836269 -0.00296856], action=0, reward=1.0, next_state=[ 0.00414345 -0.18216109 -0.00842206  0.28706415]\n",
      "[ episode 354 ][ timestamp 34 ] state=[ 0.00414345 -0.18216109 -0.00842206  0.28706415], action=1, reward=1.0, next_state=[ 0.00050023  0.01307995 -0.00268078 -0.00826308]\n",
      "[ episode 354 ][ timestamp 35 ] state=[ 0.00050023  0.01307995 -0.00268078 -0.00826308], action=0, reward=1.0, next_state=[ 0.00076183 -0.18200345 -0.00284604  0.28357283]\n",
      "[ episode 354 ][ timestamp 36 ] state=[ 0.00076183 -0.18200345 -0.00284604  0.28357283], action=1, reward=1.0, next_state=[-0.00287824  0.01315897  0.00282542 -0.01000637]\n",
      "[ episode 354 ][ timestamp 37 ] state=[-0.00287824  0.01315897  0.00282542 -0.01000637], action=0, reward=1.0, next_state=[-0.00261506 -0.18200338  0.00262529  0.28356667]\n",
      "[ episode 354 ][ timestamp 38 ] state=[-0.00261506 -0.18200338  0.00262529  0.28356667], action=1, reward=1.0, next_state=[-0.00625513  0.01308103  0.00829662 -0.0082871 ]\n",
      "[ episode 354 ][ timestamp 39 ] state=[-0.00625513  0.01308103  0.00829662 -0.0082871 ], action=1, reward=1.0, next_state=[-0.00599351  0.20808302  0.00813088 -0.29834084]\n",
      "[ episode 354 ][ timestamp 40 ] state=[-0.00599351  0.20808302  0.00813088 -0.29834084], action=0, reward=1.0, next_state=[-0.00183185  0.01284611  0.00216406 -0.00310471]\n",
      "[ episode 354 ][ timestamp 41 ] state=[-0.00183185  0.01284611  0.00216406 -0.00310471], action=1, reward=1.0, next_state=[-0.00157493  0.20793696  0.00210197 -0.29510406]\n",
      "[ episode 354 ][ timestamp 42 ] state=[-0.00157493  0.20793696  0.00210197 -0.29510406], action=0, reward=1.0, next_state=[ 0.00258381  0.0127851  -0.00380011 -0.00175895]\n",
      "[ episode 354 ][ timestamp 43 ] state=[ 0.00258381  0.0127851  -0.00380011 -0.00175895], action=0, reward=1.0, next_state=[ 0.00283952 -0.18228214 -0.00383529  0.28972258]\n",
      "[ episode 354 ][ timestamp 44 ] state=[ 0.00283952 -0.18228214 -0.00383529  0.28972258], action=1, reward=1.0, next_state=[-0.00080613  0.01289429  0.00195916 -0.00416748]\n",
      "[ episode 354 ][ timestamp 45 ] state=[-0.00080613  0.01289429  0.00195916 -0.00416748], action=0, reward=1.0, next_state=[-0.00054824 -0.18225571  0.00187581  0.28913294]\n",
      "[ episode 354 ][ timestamp 46 ] state=[-0.00054824 -0.18225571  0.00187581  0.28913294], action=1, reward=1.0, next_state=[-0.00419335  0.01283945  0.00765847 -0.00295779]\n",
      "[ episode 354 ][ timestamp 47 ] state=[-0.00419335  0.01283945  0.00765847 -0.00295779], action=1, reward=1.0, next_state=[-0.00393657  0.20785073  0.00759931 -0.29321457]\n",
      "[ episode 354 ][ timestamp 48 ] state=[-0.00393657  0.20785073  0.00759931 -0.29321457], action=0, reward=1.0, next_state=[0.00022045 0.01262126 0.00173502 0.00185536]\n",
      "[ episode 354 ][ timestamp 49 ] state=[0.00022045 0.01262126 0.00173502 0.00185536], action=1, reward=1.0, next_state=[ 0.00047287  0.20771829  0.00177213 -0.29027964]\n",
      "[ episode 354 ][ timestamp 50 ] state=[ 0.00047287  0.20771829  0.00177213 -0.29027964], action=0, reward=1.0, next_state=[ 0.00462724  0.01257111 -0.00403346  0.00296167]\n",
      "[ episode 354 ][ timestamp 51 ] state=[ 0.00462724  0.01257111 -0.00403346  0.00296167], action=0, reward=1.0, next_state=[ 0.00487866 -0.18249276 -0.00397423  0.29436926]\n",
      "[ episode 354 ][ timestamp 52 ] state=[ 0.00487866 -0.18249276 -0.00397423  0.29436926], action=1, reward=1.0, next_state=[0.00122881 0.01268562 0.00191316 0.00043558]\n",
      "[ episode 354 ][ timestamp 53 ] state=[0.00122881 0.01268562 0.00191316 0.00043558], action=1, reward=1.0, next_state=[ 0.00148252  0.20778009  0.00192187 -0.29164311]\n",
      "[ episode 354 ][ timestamp 54 ] state=[ 0.00148252  0.20778009  0.00192187 -0.29164311], action=0, reward=1.0, next_state=[ 0.00563812  0.01263078 -0.00391099  0.00164533]\n",
      "[ episode 354 ][ timestamp 55 ] state=[ 0.00563812  0.01263078 -0.00391099  0.00164533], action=1, reward=1.0, next_state=[ 0.00589074  0.20780861 -0.00387809 -0.29226899]\n",
      "[ episode 354 ][ timestamp 56 ] state=[ 0.00589074  0.20780861 -0.00387809 -0.29226899], action=0, reward=1.0, next_state=[ 0.01004691  0.01274216 -0.00972347 -0.00081169]\n",
      "[ episode 354 ][ timestamp 57 ] state=[ 0.01004691  0.01274216 -0.00972347 -0.00081169], action=0, reward=1.0, next_state=[ 0.01030175 -0.182239   -0.0097397   0.28878758]\n",
      "[ episode 354 ][ timestamp 58 ] state=[ 0.01030175 -0.182239   -0.0097397   0.28878758], action=1, reward=1.0, next_state=[ 0.00665697  0.01302048 -0.00396395 -0.00695117]\n",
      "[ episode 354 ][ timestamp 59 ] state=[ 0.00665697  0.01302048 -0.00396395 -0.00695117], action=0, reward=1.0, next_state=[ 0.00691738 -0.1820444  -0.00410297  0.28447845]\n",
      "[ episode 354 ][ timestamp 60 ] state=[ 0.00691738 -0.1820444  -0.00410297  0.28447845], action=0, reward=1.0, next_state=[ 0.00327649 -0.3771076   0.0015866   0.57586451]\n",
      "[ episode 354 ][ timestamp 61 ] state=[ 0.00327649 -0.3771076   0.0015866   0.57586451], action=1, reward=1.0, next_state=[-0.00426566 -0.18200792  0.01310389  0.28368182]\n",
      "[ episode 354 ][ timestamp 62 ] state=[-0.00426566 -0.18200792  0.01310389  0.28368182], action=1, reward=1.0, next_state=[-0.00790582  0.0129247   0.01877752 -0.00483958]\n",
      "[ episode 354 ][ timestamp 63 ] state=[-0.00790582  0.0129247   0.01877752 -0.00483958], action=1, reward=1.0, next_state=[-0.00764732  0.20777239  0.01868073 -0.29153932]\n",
      "[ episode 354 ][ timestamp 64 ] state=[-0.00764732  0.20777239  0.01868073 -0.29153932], action=0, reward=1.0, next_state=[-0.00349188  0.01238913  0.01284994  0.00697625]\n",
      "[ episode 354 ][ timestamp 65 ] state=[-0.00349188  0.01238913  0.01284994  0.00697625], action=1, reward=1.0, next_state=[-0.00324409  0.20732446  0.01298947 -0.28162483]\n",
      "[ episode 354 ][ timestamp 66 ] state=[-0.00324409  0.20732446  0.01298947 -0.28162483], action=0, reward=1.0, next_state=[0.0009024  0.01201966 0.00735697 0.01512646]\n",
      "[ episode 354 ][ timestamp 67 ] state=[0.0009024  0.01201966 0.00735697 0.01512646], action=1, reward=1.0, next_state=[ 0.00114279  0.20703534  0.0076595  -0.27522621]\n",
      "[ episode 354 ][ timestamp 68 ] state=[ 0.00114279  0.20703534  0.0076595  -0.27522621], action=0, reward=1.0, next_state=[0.0052835  0.01180494 0.00215498 0.01986266]\n",
      "[ episode 354 ][ timestamp 69 ] state=[0.0052835  0.01180494 0.00215498 0.01986266], action=1, reward=1.0, next_state=[ 0.0055196   0.20689592  0.00255223 -0.27213957]\n",
      "[ episode 354 ][ timestamp 70 ] state=[ 0.0055196   0.20689592  0.00255223 -0.27213957], action=0, reward=1.0, next_state=[ 0.00965751  0.01173765 -0.00289056  0.02134725]\n",
      "[ episode 354 ][ timestamp 71 ] state=[ 0.00965751  0.01173765 -0.00289056  0.02134725], action=1, reward=1.0, next_state=[ 0.00989227  0.20690093 -0.00246362 -0.27224628]\n",
      "[ episode 354 ][ timestamp 72 ] state=[ 0.00989227  0.20690093 -0.00246362 -0.27224628], action=0, reward=1.0, next_state=[ 0.01403029  0.01181422 -0.00790854  0.0196586 ]\n",
      "[ episode 354 ][ timestamp 73 ] state=[ 0.01403029  0.01181422 -0.00790854  0.0196586 ], action=1, reward=1.0, next_state=[ 0.01426657  0.20704869 -0.00751537 -0.27550904]\n",
      "[ episode 354 ][ timestamp 74 ] state=[ 0.01426657  0.20704869 -0.00751537 -0.27550904], action=0, reward=1.0, next_state=[ 0.01840754  0.01203477 -0.01302555  0.01479409]\n",
      "[ episode 354 ][ timestamp 75 ] state=[ 0.01840754  0.01203477 -0.01302555  0.01479409], action=0, reward=1.0, next_state=[ 0.01864824 -0.18289798 -0.01272967  0.30333901]\n",
      "[ episode 354 ][ timestamp 76 ] state=[ 0.01864824 -0.18289798 -0.01272967  0.30333901], action=1, reward=1.0, next_state=[ 0.01499028  0.01240305 -0.00666289  0.00666877]\n",
      "[ episode 354 ][ timestamp 77 ] state=[ 0.01499028  0.01240305 -0.00666289  0.00666877], action=0, reward=1.0, next_state=[ 0.01523834 -0.18262271 -0.00652951  0.29724205]\n",
      "[ episode 354 ][ timestamp 78 ] state=[ 0.01523834 -0.18262271 -0.00652951  0.29724205], action=1, reward=1.0, next_state=[ 0.01158589  0.01259171 -0.00058467  0.002507  ]\n",
      "[ episode 354 ][ timestamp 79 ] state=[ 0.01158589  0.01259171 -0.00058467  0.002507  ], action=0, reward=1.0, next_state=[ 0.01183772 -0.18252185 -0.00053453  0.2950054 ]\n",
      "[ episode 354 ][ timestamp 80 ] state=[ 0.01183772 -0.18252185 -0.00053453  0.2950054 ], action=1, reward=1.0, next_state=[0.00818728 0.01260771 0.00536558 0.00215394]\n",
      "[ episode 354 ][ timestamp 81 ] state=[0.00818728 0.01260771 0.00536558 0.00215394], action=1, reward=1.0, next_state=[ 0.00843944  0.20765231  0.00540866 -0.28883127]\n",
      "[ episode 354 ][ timestamp 82 ] state=[ 0.00843944  0.20765231  0.00540866 -0.28883127], action=0, reward=1.0, next_state=[ 0.01259248  0.01245365 -0.00036797  0.00555258]\n",
      "[ episode 354 ][ timestamp 83 ] state=[ 0.01259248  0.01245365 -0.00036797  0.00555258], action=1, reward=1.0, next_state=[ 1.28415572e-02  2.07580873e-01 -2.56917848e-04 -2.87246425e-01]\n",
      "[ episode 354 ][ timestamp 84 ] state=[ 1.28415572e-02  2.07580873e-01 -2.56917848e-04 -2.87246425e-01], action=1, reward=1.0, next_state=[ 0.01699317  0.40270649 -0.00600185 -0.58001037]\n",
      "[ episode 354 ][ timestamp 85 ] state=[ 0.01699317  0.40270649 -0.00600185 -0.58001037], action=0, reward=1.0, next_state=[ 0.0250473   0.20766915 -0.01760205 -0.28922417]\n",
      "[ episode 354 ][ timestamp 86 ] state=[ 0.0250473   0.20766915 -0.01760205 -0.28922417], action=0, reward=1.0, next_state=[ 0.02920069  0.01280257 -0.02338654 -0.00214431]\n",
      "[ episode 354 ][ timestamp 87 ] state=[ 0.02920069  0.01280257 -0.02338654 -0.00214431], action=0, reward=1.0, next_state=[ 0.02945674 -0.18197631 -0.02342942  0.2830691 ]\n",
      "[ episode 354 ][ timestamp 88 ] state=[ 0.02945674 -0.18197631 -0.02342942  0.2830691 ], action=1, reward=1.0, next_state=[ 0.02581721  0.01347185 -0.01776804 -0.0169103 ]\n",
      "[ episode 354 ][ timestamp 89 ] state=[ 0.02581721  0.01347185 -0.01776804 -0.0169103 ], action=0, reward=1.0, next_state=[ 0.02608665 -0.18139084 -0.01810625  0.27011407]\n",
      "[ episode 354 ][ timestamp 90 ] state=[ 0.02608665 -0.18139084 -0.01810625  0.27011407], action=1, reward=1.0, next_state=[ 0.02245883  0.01398475 -0.01270397 -0.02822422]\n",
      "[ episode 354 ][ timestamp 91 ] state=[ 0.02245883  0.01398475 -0.01270397 -0.02822422], action=0, reward=1.0, next_state=[ 0.02273853 -0.18095274 -0.01326845  0.26042354]\n",
      "[ episode 354 ][ timestamp 92 ] state=[ 0.02273853 -0.18095274 -0.01326845  0.26042354], action=1, reward=1.0, next_state=[ 0.01911947  0.01435609 -0.00805998 -0.03641471]\n",
      "[ episode 354 ][ timestamp 93 ] state=[ 0.01911947  0.01435609 -0.00805998 -0.03641471], action=0, reward=1.0, next_state=[ 0.01940659 -0.18064936 -0.00878827  0.25371435]\n",
      "[ episode 354 ][ timestamp 94 ] state=[ 0.01940659 -0.18064936 -0.00878827  0.25371435], action=1, reward=1.0, next_state=[ 0.01579361  0.01459696 -0.00371399 -0.04172755]\n",
      "[ episode 354 ][ timestamp 95 ] state=[ 0.01579361  0.01459696 -0.00371399 -0.04172755], action=0, reward=1.0, next_state=[ 0.01608555 -0.18047153 -0.00454854  0.24978127]\n",
      "[ episode 354 ][ timestamp 96 ] state=[ 0.01608555 -0.18047153 -0.00454854  0.24978127], action=1, reward=1.0, next_state=[ 0.01247612  0.01471508  0.00044709 -0.04433288]\n",
      "[ episode 354 ][ timestamp 97 ] state=[ 0.01247612  0.01471508  0.00044709 -0.04433288], action=0, reward=1.0, next_state=[ 0.01277042 -0.18041328 -0.00043957  0.24849107]\n",
      "[ episode 354 ][ timestamp 98 ] state=[ 0.01277042 -0.18041328 -0.00043957  0.24849107], action=1, reward=1.0, next_state=[ 0.00916215  0.01471494  0.00453025 -0.04433048]\n",
      "[ episode 354 ][ timestamp 99 ] state=[ 0.00916215  0.01471494  0.00453025 -0.04433048], action=0, reward=1.0, next_state=[ 0.00945645 -0.18047167  0.00364364  0.24977834]\n",
      "[ episode 354 ][ timestamp 100 ] state=[ 0.00945645 -0.18047167  0.00364364  0.24977834], action=0, reward=1.0, next_state=[ 0.00584702 -0.37564547  0.00863921  0.54360831]\n",
      "[ episode 354 ][ timestamp 101 ] state=[ 0.00584702 -0.37564547  0.00863921  0.54360831], action=1, reward=1.0, next_state=[-0.00166589 -0.18064598  0.01951137  0.2536599 ]\n",
      "[ episode 354 ][ timestamp 102 ] state=[-0.00166589 -0.18064598  0.01951137  0.2536599 ], action=1, reward=1.0, next_state=[-0.00527881  0.01419201  0.02458457 -0.03280549]\n",
      "[ episode 354 ][ timestamp 103 ] state=[-0.00527881  0.01419201  0.02458457 -0.03280549], action=1, reward=1.0, next_state=[-0.00499497  0.20895294  0.02392846 -0.31763147]\n",
      "[ episode 354 ][ timestamp 104 ] state=[-0.00499497  0.20895294  0.02392846 -0.31763147], action=0, reward=1.0, next_state=[-0.00081591  0.0134985   0.01757583 -0.01749943]\n",
      "[ episode 354 ][ timestamp 105 ] state=[-0.00081591  0.0134985   0.01757583 -0.01749943], action=1, reward=1.0, next_state=[-0.00054594  0.20836404  0.01722585 -0.30458557]\n",
      "[ episode 354 ][ timestamp 106 ] state=[-0.00054594  0.20836404  0.01722585 -0.30458557], action=0, reward=1.0, next_state=[ 0.00362134  0.01300089  0.01113413 -0.00652017]\n",
      "[ episode 354 ][ timestamp 107 ] state=[ 0.00362134  0.01300089  0.01113413 -0.00652017], action=1, reward=1.0, next_state=[ 0.00388136  0.20796141  0.01100373 -0.29566946]\n",
      "[ episode 354 ][ timestamp 108 ] state=[ 0.00388136  0.20796141  0.01100373 -0.29566946], action=0, reward=1.0, next_state=[0.00804058 0.01268433 0.00509034 0.00046346]\n",
      "[ episode 354 ][ timestamp 109 ] state=[0.00804058 0.01268433 0.00509034 0.00046346], action=1, reward=1.0, next_state=[ 0.00829427  0.20773291  0.00509961 -0.29060906]\n",
      "[ episode 354 ][ timestamp 110 ] state=[ 0.00829427  0.20773291  0.00509961 -0.29060906], action=0, reward=1.0, next_state=[ 0.01244893  0.01253861 -0.00071257  0.00367785]\n",
      "[ episode 354 ][ timestamp 111 ] state=[ 0.01244893  0.01253861 -0.00071257  0.00367785], action=0, reward=1.0, next_state=[ 0.0126997  -0.18257311 -0.00063901  0.29613587]\n",
      "[ episode 354 ][ timestamp 112 ] state=[ 0.0126997  -0.18257311 -0.00063901  0.29613587], action=1, reward=1.0, next_state=[0.00904824 0.01255794 0.0052837  0.00325147]\n",
      "[ episode 354 ][ timestamp 113 ] state=[0.00904824 0.01255794 0.0052837  0.00325147], action=1, reward=1.0, next_state=[ 0.0092994   0.20760372  0.00534873 -0.28775971]\n",
      "[ episode 354 ][ timestamp 114 ] state=[ 0.0092994   0.20760372  0.00534873 -0.28775971], action=0, reward=1.0, next_state=[ 0.01345147  0.0124059  -0.00040646  0.00660535]\n",
      "[ episode 354 ][ timestamp 115 ] state=[ 0.01345147  0.0124059  -0.00040646  0.00660535], action=1, reward=1.0, next_state=[ 1.36995903e-02  2.07533683e-01 -2.74354173e-04 -2.86205792e-01]\n",
      "[ episode 354 ][ timestamp 116 ] state=[ 1.36995903e-02  2.07533683e-01 -2.74354173e-04 -2.86205792e-01], action=1, reward=1.0, next_state=[ 0.01785026  0.40265955 -0.00599847 -0.57897524]\n",
      "[ episode 354 ][ timestamp 117 ] state=[ 0.01785026  0.40265955 -0.00599847 -0.57897524], action=0, reward=1.0, next_state=[ 0.02590345  0.20762217 -0.01757797 -0.28818798]\n",
      "[ episode 354 ][ timestamp 118 ] state=[ 0.02590345  0.20762217 -0.01757797 -0.28818798], action=0, reward=1.0, next_state=[ 0.0300559   0.01275524 -0.02334173 -0.0011004 ]\n",
      "[ episode 354 ][ timestamp 119 ] state=[ 0.0300559   0.01275524 -0.02334173 -0.0011004 ], action=0, reward=1.0, next_state=[ 0.030311   -0.18202431 -0.02336374  0.2841275 ]\n",
      "[ episode 354 ][ timestamp 120 ] state=[ 0.030311   -0.18202431 -0.02336374  0.2841275 ], action=1, reward=1.0, next_state=[ 0.02667052  0.01342294 -0.01768119 -0.01583169]\n",
      "[ episode 354 ][ timestamp 121 ] state=[ 0.02667052  0.01342294 -0.01768119 -0.01583169], action=0, reward=1.0, next_state=[ 0.02693898 -0.18144103 -0.01799783  0.27122059]\n",
      "[ episode 354 ][ timestamp 122 ] state=[ 0.02693898 -0.18144103 -0.01799783  0.27122059], action=1, reward=1.0, next_state=[ 0.02331016  0.01393305 -0.01257341 -0.02708416]\n",
      "[ episode 354 ][ timestamp 123 ] state=[ 0.02331016  0.01393305 -0.01257341 -0.02708416], action=0, reward=1.0, next_state=[ 0.02358882 -0.18100635 -0.0131151   0.26160535]\n",
      "[ episode 354 ][ timestamp 124 ] state=[ 0.02358882 -0.18100635 -0.0131151   0.26160535], action=1, reward=1.0, next_state=[ 0.01996869  0.01430034 -0.00788299 -0.03518521]\n",
      "[ episode 354 ][ timestamp 125 ] state=[ 0.01996869  0.01430034 -0.00788299 -0.03518521], action=0, reward=1.0, next_state=[ 0.0202547  -0.18070769 -0.00858669  0.25500016]\n",
      "[ episode 354 ][ timestamp 126 ] state=[ 0.0202547  -0.18070769 -0.00858669  0.25500016], action=1, reward=1.0, next_state=[ 0.01664054  0.0145358  -0.00348669 -0.04037874]\n",
      "[ episode 354 ][ timestamp 127 ] state=[ 0.01664054  0.0145358  -0.00348669 -0.04037874], action=0, reward=1.0, next_state=[ 0.01693126 -0.18053597 -0.00429427  0.25120207]\n",
      "[ episode 354 ][ timestamp 128 ] state=[ 0.01693126 -0.18053597 -0.00429427  0.25120207], action=1, reward=1.0, next_state=[ 0.01332054  0.01464703  0.00072978 -0.04283226]\n",
      "[ episode 354 ][ timestamp 129 ] state=[ 0.01332054  0.01464703  0.00072978 -0.04283226], action=0, reward=1.0, next_state=[ 1.36134794e-02 -1.80485374e-01 -1.26870046e-04  2.50080831e-01]\n",
      "[ episode 354 ][ timestamp 130 ] state=[ 1.36134794e-02 -1.80485374e-01 -1.26870046e-04  2.50080831e-01], action=1, reward=1.0, next_state=[ 0.01000377  0.01463839  0.00487475 -0.04264211]\n",
      "[ episode 354 ][ timestamp 131 ] state=[ 0.01000377  0.01463839  0.00487475 -0.04264211], action=0, reward=1.0, next_state=[ 0.01029654 -0.18055312  0.0040219   0.25157485]\n",
      "[ episode 354 ][ timestamp 132 ] state=[ 0.01029654 -0.18055312  0.0040219   0.25157485], action=1, reward=1.0, next_state=[ 0.00668548  0.01451116  0.0090534  -0.03983678]\n",
      "[ episode 354 ][ timestamp 133 ] state=[ 0.00668548  0.01451116  0.0090534  -0.03983678], action=0, reward=1.0, next_state=[ 0.0069757  -0.18073943  0.00825667  0.25568878]\n",
      "[ episode 354 ][ timestamp 134 ] state=[ 0.0069757  -0.18073943  0.00825667  0.25568878], action=1, reward=1.0, next_state=[ 0.00336091  0.01426366  0.01337044 -0.03437847]\n",
      "[ episode 354 ][ timestamp 135 ] state=[ 0.00336091  0.01426366  0.01337044 -0.03437847], action=0, reward=1.0, next_state=[ 0.00364619 -0.18104745  0.01268287  0.2624928 ]\n",
      "[ episode 354 ][ timestamp 136 ] state=[ 0.00364619 -0.18104745  0.01268287  0.2624928 ], action=1, reward=1.0, next_state=[ 2.52362507e-05  1.38911926e-02  1.79327281e-02 -2.61629705e-02]\n",
      "[ episode 354 ][ timestamp 137 ] state=[ 2.52362507e-05  1.38911926e-02  1.79327281e-02 -2.61629705e-02], action=1, reward=1.0, next_state=[ 3.03060103e-04  2.08751445e-01  1.74094687e-02 -3.13134413e-01]\n",
      "[ episode 354 ][ timestamp 138 ] state=[ 3.03060103e-04  2.08751445e-01  1.74094687e-02 -3.13134413e-01], action=0, reward=1.0, next_state=[ 0.00447809  0.01338586  0.01114678 -0.01501233]\n",
      "[ episode 354 ][ timestamp 139 ] state=[ 0.00447809  0.01338586  0.01114678 -0.01501233], action=1, reward=1.0, next_state=[ 0.00474581  0.2083462   0.01084653 -0.30415757]\n",
      "[ episode 354 ][ timestamp 140 ] state=[ 0.00474581  0.2083462   0.01084653 -0.30415757], action=0, reward=1.0, next_state=[ 0.00891273  0.01307136  0.00476338 -0.00807373]\n",
      "[ episode 354 ][ timestamp 141 ] state=[ 0.00891273  0.01307136  0.00476338 -0.00807373], action=0, reward=1.0, next_state=[ 0.00917416 -0.18211858  0.00460191  0.28610828]\n",
      "[ episode 354 ][ timestamp 142 ] state=[ 0.00917416 -0.18211858  0.00460191  0.28610828], action=1, reward=1.0, next_state=[ 0.00553179  0.01293744  0.01032407 -0.00511969]\n",
      "[ episode 354 ][ timestamp 143 ] state=[ 0.00553179  0.01293744  0.01032407 -0.00511969], action=1, reward=1.0, next_state=[ 0.00579053  0.20790982  0.01022168 -0.29452745]\n",
      "[ episode 354 ][ timestamp 144 ] state=[ 0.00579053  0.20790982  0.01022168 -0.29452745], action=0, reward=1.0, next_state=[0.00994873 0.01264365 0.00433113 0.00136164]\n",
      "[ episode 354 ][ timestamp 145 ] state=[0.00994873 0.01264365 0.00433113 0.00136164], action=0, reward=1.0, next_state=[ 0.0102016  -0.18254015  0.00435836  0.29540793]\n",
      "[ episode 354 ][ timestamp 146 ] state=[ 0.0102016  -0.18254015  0.00435836  0.29540793], action=1, reward=1.0, next_state=[0.0065508  0.01251939 0.01026652 0.00410275]\n",
      "[ episode 354 ][ timestamp 147 ] state=[0.0065508  0.01251939 0.01026652 0.00410275], action=1, reward=1.0, next_state=[ 0.00680119  0.20749262  0.01034858 -0.28532337]\n",
      "[ episode 354 ][ timestamp 148 ] state=[ 0.00680119  0.20749262  0.01034858 -0.28532337], action=0, reward=1.0, next_state=[0.01095104 0.01222461 0.00464211 0.01060538]\n",
      "[ episode 354 ][ timestamp 149 ] state=[0.01095104 0.01222461 0.00464211 0.01060538], action=1, reward=1.0, next_state=[ 0.01119553  0.20727968  0.00485422 -0.2806093 ]\n",
      "[ episode 354 ][ timestamp 150 ] state=[ 0.01119553  0.20727968  0.00485422 -0.2806093 ], action=0, reward=1.0, next_state=[ 0.01534113  0.01208883 -0.00075797  0.01360067]\n",
      "[ episode 354 ][ timestamp 151 ] state=[ 0.01534113  0.01208883 -0.00075797  0.01360067], action=1, reward=1.0, next_state=[ 0.0155829   0.20722164 -0.00048596 -0.2793213 ]\n",
      "[ episode 354 ][ timestamp 152 ] state=[ 0.0155829   0.20722164 -0.00048596 -0.2793213 ], action=0, reward=1.0, next_state=[ 0.01972734  0.01210662 -0.00607238  0.01320831]\n",
      "[ episode 354 ][ timestamp 153 ] state=[ 0.01972734  0.01210662 -0.00607238  0.01320831], action=0, reward=1.0, next_state=[ 0.01996947 -0.18292772 -0.00580821  0.30396916]\n",
      "[ episode 354 ][ timestamp 154 ] state=[ 0.01996947 -0.18292772 -0.00580821  0.30396916], action=1, reward=1.0, next_state=[0.01631091 0.01227652 0.00027117 0.00946013]\n",
      "[ episode 354 ][ timestamp 155 ] state=[0.01631091 0.01227652 0.00027117 0.00946013], action=1, reward=1.0, next_state=[ 0.01655645  0.20739459  0.00046037 -0.28313722]\n",
      "[ episode 354 ][ timestamp 156 ] state=[ 0.01655645  0.20739459  0.00046037 -0.28313722], action=0, reward=1.0, next_state=[ 0.02070434  0.01226607 -0.00520237  0.00969087]\n",
      "[ episode 354 ][ timestamp 157 ] state=[ 0.02070434  0.01226607 -0.00520237  0.00969087], action=0, reward=1.0, next_state=[ 0.02094966 -0.18278089 -0.00500856  0.30072785]\n",
      "[ episode 354 ][ timestamp 158 ] state=[ 0.02094966 -0.18278089 -0.00500856  0.30072785], action=1, reward=1.0, next_state=[0.01729404 0.01241209 0.001006   0.00646954]\n",
      "[ episode 354 ][ timestamp 159 ] state=[0.01729404 0.01241209 0.001006   0.00646954], action=1, reward=1.0, next_state=[ 0.01754228  0.2075196   0.00113539 -0.28589581]\n",
      "[ episode 354 ][ timestamp 160 ] state=[ 0.01754228  0.2075196   0.00113539 -0.28589581], action=0, reward=1.0, next_state=[ 0.02169267  0.01238148 -0.00458252  0.007145  ]\n",
      "[ episode 354 ][ timestamp 161 ] state=[ 0.02169267  0.01238148 -0.00458252  0.007145  ], action=1, reward=1.0, next_state=[ 0.0219403   0.20756885 -0.00443962 -0.28698024]\n",
      "[ episode 354 ][ timestamp 162 ] state=[ 0.0219403   0.20756885 -0.00443962 -0.28698024], action=0, reward=1.0, next_state=[ 0.02609168  0.01251049 -0.01017923  0.00429916]\n",
      "[ episode 354 ][ timestamp 163 ] state=[ 0.02609168  0.01251049 -0.01017923  0.00429916], action=0, reward=1.0, next_state=[ 0.02634189 -0.18246401 -0.01009325  0.29375312]\n",
      "[ episode 354 ][ timestamp 164 ] state=[ 0.02634189 -0.18246401 -0.01009325  0.29375312], action=1, reward=1.0, next_state=[ 0.02269261  0.01280038 -0.00421818 -0.00209591]\n",
      "[ episode 354 ][ timestamp 165 ] state=[ 0.02269261  0.01280038 -0.00421818 -0.00209591], action=0, reward=1.0, next_state=[ 0.02294862 -0.18226082 -0.0042601   0.28925315]\n",
      "[ episode 354 ][ timestamp 166 ] state=[ 0.02294862 -0.18226082 -0.0042601   0.28925315], action=1, reward=1.0, next_state=[ 0.0193034   0.01292162  0.00152496 -0.00477032]\n",
      "[ episode 354 ][ timestamp 167 ] state=[ 0.0193034   0.01292162  0.00152496 -0.00477032], action=0, reward=1.0, next_state=[ 0.01956183 -0.18222217  0.00142955  0.28839336]\n",
      "[ episode 354 ][ timestamp 168 ] state=[ 0.01956183 -0.18222217  0.00142955  0.28839336], action=1, reward=1.0, next_state=[ 0.01591739  0.01287937  0.00719742 -0.00383835]\n",
      "[ episode 354 ][ timestamp 169 ] state=[ 0.01591739  0.01287937  0.00719742 -0.00383835], action=1, reward=1.0, next_state=[ 0.01617498  0.20789736  0.00712066 -0.29424174]\n",
      "[ episode 354 ][ timestamp 170 ] state=[ 0.01617498  0.20789736  0.00712066 -0.29424174], action=0, reward=1.0, next_state=[0.02033293 0.01267462 0.00123582 0.0006784 ]\n",
      "[ episode 354 ][ timestamp 171 ] state=[0.02033293 0.01267462 0.00123582 0.0006784 ], action=0, reward=1.0, next_state=[ 0.02058642 -0.18246504  0.00124939  0.29375099]\n",
      "[ episode 354 ][ timestamp 172 ] state=[ 0.02058642 -0.18246504  0.00124939  0.29375099], action=1, reward=1.0, next_state=[0.01693712 0.01263908 0.00712441 0.00146236]\n",
      "[ episode 354 ][ timestamp 173 ] state=[0.01693712 0.01263908 0.00712441 0.00146236], action=0, reward=1.0, next_state=[ 0.0171899  -0.18258432  0.00715366  0.29638458]\n",
      "[ episode 354 ][ timestamp 174 ] state=[ 0.0171899  -0.18258432  0.00715366  0.29638458], action=1, reward=1.0, next_state=[0.01353821 0.01243493 0.01308135 0.00596636]\n",
      "[ episode 354 ][ timestamp 175 ] state=[0.01353821 0.01243493 0.01308135 0.00596636], action=1, reward=1.0, next_state=[ 0.01378691  0.20736685  0.01320067 -0.28256071]\n",
      "[ episode 354 ][ timestamp 176 ] state=[ 0.01378691  0.20736685  0.01320067 -0.28256071], action=0, reward=1.0, next_state=[0.01793425 0.01205913 0.00754946 0.01425625]\n",
      "[ episode 354 ][ timestamp 177 ] state=[0.01793425 0.01205913 0.00754946 0.01425625], action=1, reward=1.0, next_state=[ 0.01817543  0.207072    0.00783458 -0.27603521]\n",
      "[ episode 354 ][ timestamp 178 ] state=[ 0.01817543  0.207072    0.00783458 -0.27603521], action=0, reward=1.0, next_state=[0.02231687 0.01183915 0.00231388 0.01910842]\n",
      "[ episode 354 ][ timestamp 179 ] state=[0.02231687 0.01183915 0.00231388 0.01910842], action=1, reward=1.0, next_state=[ 0.02255365  0.20692784  0.00269605 -0.27284355]\n",
      "[ episode 354 ][ timestamp 180 ] state=[ 0.02255365  0.20692784  0.00269605 -0.27284355], action=0, reward=1.0, next_state=[ 0.02669221  0.01176753 -0.00276082  0.02068849]\n",
      "[ episode 354 ][ timestamp 181 ] state=[ 0.02669221  0.01176753 -0.00276082  0.02068849], action=1, reward=1.0, next_state=[ 0.02692756  0.20692896 -0.00234705 -0.27286422]\n",
      "[ episode 354 ][ timestamp 182 ] state=[ 0.02692756  0.20692896 -0.00234705 -0.27286422], action=0, reward=1.0, next_state=[ 0.03106614  0.01184058 -0.00780434  0.01907751]\n",
      "[ episode 354 ][ timestamp 183 ] state=[ 0.03106614  0.01184058 -0.00780434  0.01907751], action=0, reward=1.0, next_state=[ 0.03130295 -0.18316858 -0.00742279  0.3092879 ]\n",
      "[ episode 354 ][ timestamp 184 ] state=[ 0.03130295 -0.18316858 -0.00742279  0.3092879 ], action=1, reward=1.0, next_state=[ 0.02763958  0.01205834 -0.00123703  0.0142733 ]\n",
      "[ episode 354 ][ timestamp 185 ] state=[ 0.02763958  0.01205834 -0.00123703  0.0142733 ], action=1, reward=1.0, next_state=[ 0.02788075  0.20719801 -0.00095156 -0.27879966]\n",
      "[ episode 354 ][ timestamp 186 ] state=[ 0.02788075  0.20719801 -0.00095156 -0.27879966], action=0, reward=1.0, next_state=[ 0.03202471  0.01208964 -0.00652756  0.01358299]\n",
      "[ episode 354 ][ timestamp 187 ] state=[ 0.03202471  0.01208964 -0.00652756  0.01358299], action=0, reward=1.0, next_state=[ 0.0322665  -0.18293809 -0.0062559   0.30419927]\n",
      "[ episode 354 ][ timestamp 188 ] state=[ 0.0322665  -0.18293809 -0.0062559   0.30419927], action=1, reward=1.0, next_state=[ 0.02860774  0.01227245 -0.00017191  0.00954996]\n",
      "[ episode 354 ][ timestamp 189 ] state=[ 0.02860774  0.01227245 -0.00017191  0.00954996], action=1, reward=1.0, next_state=[ 2.88531874e-02  2.07396871e-01  1.90887597e-05 -2.83187200e-01]\n",
      "[ episode 354 ][ timestamp 190 ] state=[ 2.88531874e-02  2.07396871e-01  1.90887597e-05 -2.83187200e-01], action=0, reward=1.0, next_state=[ 0.03300112  0.01227465 -0.00564466  0.00950175]\n",
      "[ episode 354 ][ timestamp 191 ] state=[ 0.03300112  0.01227465 -0.00564466  0.00950175], action=0, reward=1.0, next_state=[ 0.03324662 -0.1827659  -0.00545462  0.30039839]\n",
      "[ episode 354 ][ timestamp 192 ] state=[ 0.03324662 -0.1827659  -0.00545462  0.30039839], action=1, reward=1.0, next_state=[0.0295913  0.01243337 0.00055335 0.00600018]\n",
      "[ episode 354 ][ timestamp 193 ] state=[0.0295913  0.01243337 0.00055335 0.00600018], action=0, reward=1.0, next_state=[ 0.02983997 -0.18269651  0.00067335  0.29885765]\n",
      "[ episode 354 ][ timestamp 194 ] state=[ 0.02983997 -0.18269651  0.00067335  0.29885765], action=1, reward=1.0, next_state=[0.02618604 0.01241584 0.0066505  0.00638716]\n",
      "[ episode 354 ][ timestamp 195 ] state=[0.02618604 0.01241584 0.0066505  0.00638716], action=1, reward=1.0, next_state=[ 0.02643435  0.20744178  0.00677825 -0.28419006]\n",
      "[ episode 354 ][ timestamp 196 ] state=[ 0.02643435  0.20744178  0.00677825 -0.28419006], action=0, reward=1.0, next_state=[0.03058319 0.01222381 0.00109445 0.01062296]\n",
      "[ episode 354 ][ timestamp 197 ] state=[0.03058319 0.01222381 0.00109445 0.01062296], action=1, reward=1.0, next_state=[ 0.03082767  0.20733005  0.00130691 -0.28171446]\n",
      "[ episode 354 ][ timestamp 198 ] state=[ 0.03082767  0.20733005  0.00130691 -0.28171446], action=0, reward=1.0, next_state=[ 0.03497427  0.01218948 -0.00432738  0.01138038]\n",
      "[ episode 354 ][ timestamp 199 ] state=[ 0.03497427  0.01218948 -0.00432738  0.01138038], action=0, reward=1.0, next_state=[ 0.03521806 -0.18287014 -0.00409978  0.30269483]\n",
      "[ episode 354 ][ timestamp 200 ] state=[ 0.03521806 -0.18287014 -0.00409978  0.30269483], action=1, reward=1.0, next_state=[0.03156065 0.01231    0.00195412 0.00872174]\n",
      "[ episode 354 ][ timestamp 201 ] state=[0.03156065 0.01231    0.00195412 0.00872174], action=1, reward=1.0, next_state=[ 0.03180685  0.20740387  0.00212856 -0.28334399]\n",
      "[ episode 354 ][ timestamp 202 ] state=[ 0.03180685  0.20740387  0.00212856 -0.28334399], action=0, reward=1.0, next_state=[ 0.03595493  0.01225163 -0.00353832  0.01000951]\n",
      "[ episode 354 ][ timestamp 203 ] state=[ 0.03595493  0.01225163 -0.00353832  0.01000951], action=0, reward=1.0, next_state=[ 0.03619996 -0.1828194  -0.00333813  0.30157395]\n",
      "[ episode 354 ][ timestamp 204 ] state=[ 0.03619996 -0.1828194  -0.00333813  0.30157395], action=1, reward=1.0, next_state=[0.03254358 0.01234997 0.00269334 0.00784012]\n",
      "[ episode 354 ][ timestamp 205 ] state=[0.03254358 0.01234997 0.00269334 0.00784012], action=1, reward=1.0, next_state=[ 0.03279057  0.20743319  0.00285015 -0.28399181]\n",
      "[ episode 354 ][ timestamp 206 ] state=[ 0.03279057  0.20743319  0.00285015 -0.28399181], action=0, reward=1.0, next_state=[ 0.03693924  0.0122707  -0.00282969  0.00958867]\n",
      "[ episode 354 ][ timestamp 207 ] state=[ 0.03693924  0.0122707  -0.00282969  0.00958867], action=0, reward=1.0, next_state=[ 0.03718465 -0.18281055 -0.00263792  0.30137746]\n",
      "[ episode 354 ][ timestamp 208 ] state=[ 0.03718465 -0.18281055 -0.00263792  0.30137746], action=0, reward=1.0, next_state=[ 0.03352844 -0.37789481  0.00338963  0.59322728]\n",
      "[ episode 354 ][ timestamp 209 ] state=[ 0.03352844 -0.37789481  0.00338963  0.59322728], action=1, reward=1.0, next_state=[ 0.02597055 -0.18282047  0.01525418  0.301614  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 354 ][ timestamp 210 ] state=[ 0.02597055 -0.18282047  0.01525418  0.301614  ], action=1, reward=1.0, next_state=[0.02231414 0.01208079 0.02128646 0.01378069]\n",
      "[ episode 354 ][ timestamp 211 ] state=[0.02231414 0.01208079 0.02128646 0.01378069], action=1, reward=1.0, next_state=[ 0.02255575  0.2068911   0.02156207 -0.27211083]\n",
      "[ episode 354 ][ timestamp 212 ] state=[ 0.02255575  0.2068911   0.02156207 -0.27211083], action=0, reward=1.0, next_state=[0.02669357 0.01146821 0.01611986 0.02729411]\n",
      "[ episode 354 ][ timestamp 213 ] state=[0.02669357 0.01146821 0.01611986 0.02729411], action=1, reward=1.0, next_state=[ 0.02692294  0.20635533  0.01666574 -0.26025955]\n",
      "[ episode 354 ][ timestamp 214 ] state=[ 0.02692294  0.20635533  0.01666574 -0.26025955], action=0, reward=1.0, next_state=[0.03105004 0.01099948 0.01146055 0.03763302]\n",
      "[ episode 354 ][ timestamp 215 ] state=[0.03105004 0.01099948 0.01146055 0.03763302], action=1, reward=1.0, next_state=[ 0.03127003  0.20595523  0.01221321 -0.25141206]\n",
      "[ episode 354 ][ timestamp 216 ] state=[ 0.03127003  0.20595523  0.01221321 -0.25141206], action=0, reward=1.0, next_state=[0.03538914 0.01066102 0.00718497 0.04509799]\n",
      "[ episode 354 ][ timestamp 217 ] state=[0.03538914 0.01066102 0.00718497 0.04509799], action=1, reward=1.0, next_state=[ 0.03560236  0.20567921  0.00808693 -0.24530938]\n",
      "[ episode 354 ][ timestamp 218 ] state=[ 0.03560236  0.20567921  0.00808693 -0.24530938], action=0, reward=1.0, next_state=[0.03971594 0.0104427  0.00318074 0.04991334]\n",
      "[ episode 354 ][ timestamp 219 ] state=[0.03971594 0.0104427  0.00318074 0.04991334], action=1, reward=1.0, next_state=[ 0.0399248   0.2055189   0.00417901 -0.24176434]\n",
      "[ episode 354 ][ timestamp 220 ] state=[ 0.0399248   0.2055189   0.00417901 -0.24176434], action=0, reward=1.0, next_state=[ 0.04403518  0.0103375  -0.00065628  0.05223382]\n",
      "[ episode 354 ][ timestamp 221 ] state=[ 0.04403518  0.0103375  -0.00065628  0.05223382], action=1, reward=1.0, next_state=[ 0.04424193  0.20546886  0.00038839 -0.24065609]\n",
      "[ episode 354 ][ timestamp 222 ] state=[ 0.04424193  0.20546886  0.00038839 -0.24065609], action=0, reward=1.0, next_state=[ 0.0483513   0.01034136 -0.00442473  0.05214932]\n",
      "[ episode 354 ][ timestamp 223 ] state=[ 0.0483513   0.01034136 -0.00442473  0.05214932], action=1, reward=1.0, next_state=[ 0.04855813  0.20552647 -0.00338174 -0.24192635]\n",
      "[ episode 354 ][ timestamp 224 ] state=[ 0.04855813  0.20552647 -0.00338174 -0.24192635], action=0, reward=1.0, next_state=[ 0.05266866  0.01045299 -0.00822027  0.04968797]\n",
      "[ episode 354 ][ timestamp 225 ] state=[ 0.05266866  0.01045299 -0.00822027  0.04968797], action=1, reward=1.0, next_state=[ 0.05287772  0.20569184 -0.00722651 -0.24557715]\n",
      "[ episode 354 ][ timestamp 226 ] state=[ 0.05287772  0.20569184 -0.00722651 -0.24557715], action=0, reward=1.0, next_state=[ 0.05699156  0.01067385 -0.01213805  0.04481763]\n",
      "[ episode 354 ][ timestamp 227 ] state=[ 0.05699156  0.01067385 -0.01213805  0.04481763], action=1, reward=1.0, next_state=[ 0.05720503  0.20596773 -0.0112417  -0.25167011]\n",
      "[ episode 354 ][ timestamp 228 ] state=[ 0.05720503  0.20596773 -0.0112417  -0.25167011], action=1, reward=1.0, next_state=[ 0.06132439  0.40124839 -0.0162751  -0.54787758]\n",
      "[ episode 354 ][ timestamp 229 ] state=[ 0.06132439  0.40124839 -0.0162751  -0.54787758], action=0, reward=1.0, next_state=[ 0.06934936  0.20635882 -0.02723265 -0.26036661]\n",
      "[ episode 354 ][ timestamp 230 ] state=[ 0.06934936  0.20635882 -0.02723265 -0.26036661], action=0, reward=1.0, next_state=[ 0.07347653  0.01163599 -0.03243998  0.02360392]\n",
      "[ episode 354 ][ timestamp 231 ] state=[ 0.07347653  0.01163599 -0.03243998  0.02360392], action=0, reward=1.0, next_state=[ 0.07370925 -0.18300608 -0.03196791  0.30587774]\n",
      "[ episode 354 ][ timestamp 232 ] state=[ 0.07370925 -0.18300608 -0.03196791  0.30587774], action=1, reward=1.0, next_state=[ 0.07004913  0.01255649 -0.02585035  0.0032868 ]\n",
      "[ episode 354 ][ timestamp 233 ] state=[ 0.07004913  0.01255649 -0.02585035  0.0032868 ], action=0, reward=1.0, next_state=[ 0.07030026 -0.18218538 -0.02578462  0.28770285]\n",
      "[ episode 354 ][ timestamp 234 ] state=[ 0.07030026 -0.18218538 -0.02578462  0.28770285], action=1, reward=1.0, next_state=[ 0.06665655  0.01329461 -0.02003056 -0.0129995 ]\n",
      "[ episode 354 ][ timestamp 235 ] state=[ 0.06665655  0.01329461 -0.02003056 -0.0129995 ], action=0, reward=1.0, next_state=[ 0.06692244 -0.18153444 -0.02029055  0.27329685]\n",
      "[ episode 354 ][ timestamp 236 ] state=[ 0.06692244 -0.18153444 -0.02029055  0.27329685], action=1, reward=1.0, next_state=[ 0.06329176  0.01387107 -0.01482461 -0.02571608]\n",
      "[ episode 354 ][ timestamp 237 ] state=[ 0.06329176  0.01387107 -0.01482461 -0.02571608], action=0, reward=1.0, next_state=[ 0.06356918 -0.18103518 -0.01533893  0.26225289]\n",
      "[ episode 354 ][ timestamp 238 ] state=[ 0.06356918 -0.18103518 -0.01533893  0.26225289], action=1, reward=1.0, next_state=[ 0.05994847  0.01430232 -0.01009388 -0.03522837]\n",
      "[ episode 354 ][ timestamp 239 ] state=[ 0.05994847  0.01430232 -0.01009388 -0.03522837], action=0, reward=1.0, next_state=[ 0.06023452 -0.18067344 -0.01079844  0.25425282]\n",
      "[ episode 354 ][ timestamp 240 ] state=[ 0.06023452 -0.18067344 -0.01079844  0.25425282], action=1, reward=1.0, next_state=[ 0.05662105  0.01460102 -0.00571339 -0.04181647]\n",
      "[ episode 354 ][ timestamp 241 ] state=[ 0.05662105  0.01460102 -0.00571339 -0.04181647], action=0, reward=1.0, next_state=[ 0.05691307 -0.18043854 -0.00654972  0.24905837]\n",
      "[ episode 354 ][ timestamp 242 ] state=[ 0.05691307 -0.18043854 -0.00654972  0.24905837], action=1, reward=1.0, next_state=[ 0.0533043   0.01477633 -0.00156855 -0.04568326]\n",
      "[ episode 354 ][ timestamp 243 ] state=[ 0.0533043   0.01477633 -0.00156855 -0.04568326], action=0, reward=1.0, next_state=[ 0.05359983 -0.18032309 -0.00248221  0.24650436]\n",
      "[ episode 354 ][ timestamp 244 ] state=[ 0.05359983 -0.18032309 -0.00248221  0.24650436], action=1, reward=1.0, next_state=[ 0.04999337  0.01483423  0.00244787 -0.04696048]\n",
      "[ episode 354 ][ timestamp 245 ] state=[ 0.04999337  0.01483423  0.00244787 -0.04696048], action=0, reward=1.0, next_state=[ 0.05029005 -0.18032274  0.00150866  0.24649377]\n",
      "[ episode 354 ][ timestamp 246 ] state=[ 0.05029005 -0.18032274  0.00150866  0.24649377], action=1, reward=1.0, next_state=[ 0.0466836   0.01477763  0.00643854 -0.04571291]\n",
      "[ episode 354 ][ timestamp 247 ] state=[ 0.0466836   0.01477763  0.00643854 -0.04571291], action=0, reward=1.0, next_state=[ 0.04697915 -0.18043605  0.00552428  0.24899446]\n",
      "[ episode 354 ][ timestamp 248 ] state=[ 0.04697915 -0.18043605  0.00552428  0.24899446], action=1, reward=1.0, next_state=[ 0.04337043  0.01460657  0.01050417 -0.04194088]\n",
      "[ episode 354 ][ timestamp 249 ] state=[ 0.04337043  0.01460657  0.01050417 -0.04194088], action=0, reward=1.0, next_state=[ 0.04366256 -0.18066442  0.00966535  0.25403761]\n",
      "[ episode 354 ][ timestamp 250 ] state=[ 0.04366256 -0.18066442  0.00966535  0.25403761], action=1, reward=1.0, next_state=[ 0.04004927  0.0143182   0.01474611 -0.03558109]\n",
      "[ episode 354 ][ timestamp 251 ] state=[ 0.04004927  0.0143182   0.01474611 -0.03558109], action=0, reward=1.0, next_state=[ 0.04033563 -0.18101207  0.01403448  0.26171767]\n",
      "[ episode 354 ][ timestamp 252 ] state=[ 0.04033563 -0.18101207  0.01403448  0.26171767], action=1, reward=1.0, next_state=[ 0.03671539  0.01390676  0.01926884 -0.02650578]\n",
      "[ episode 354 ][ timestamp 253 ] state=[ 0.03671539  0.01390676  0.01926884 -0.02650578], action=0, reward=1.0, next_state=[ 0.03699353 -0.18148615  0.01873872  0.27219386]\n",
      "[ episode 354 ][ timestamp 254 ] state=[ 0.03699353 -0.18148615  0.01873872  0.27219386], action=1, reward=1.0, next_state=[ 0.0333638   0.01336348  0.0241826  -0.0145204 ]\n",
      "[ episode 354 ][ timestamp 255 ] state=[ 0.0333638   0.01336348  0.0241826  -0.0145204 ], action=1, reward=1.0, next_state=[ 0.03363107  0.20813042  0.02389219 -0.29947641]\n",
      "[ episode 354 ][ timestamp 256 ] state=[ 0.03363107  0.20813042  0.02389219 -0.29947641], action=0, reward=1.0, next_state=[0.03779368 0.0126762  0.01790266 0.00064488]\n",
      "[ episode 354 ][ timestamp 257 ] state=[0.03779368 0.0126762  0.01790266 0.00064488], action=1, reward=1.0, next_state=[ 0.03804721  0.20753689  0.01791556 -0.28633621]\n",
      "[ episode 354 ][ timestamp 258 ] state=[ 0.03804721  0.20753689  0.01791556 -0.28633621], action=0, reward=1.0, next_state=[0.04219794 0.01216408 0.01218884 0.01194287]\n",
      "[ episode 354 ][ timestamp 259 ] state=[0.04219794 0.01216408 0.01218884 0.01194287], action=1, reward=1.0, next_state=[ 0.04244123  0.20710913  0.01242769 -0.27686955]\n",
      "[ episode 354 ][ timestamp 260 ] state=[ 0.04244123  0.20710913  0.01242769 -0.27686955], action=0, reward=1.0, next_state=[0.04658341 0.0118121  0.0068903  0.01970702]\n",
      "[ episode 354 ][ timestamp 261 ] state=[0.04658341 0.0118121  0.0068903  0.01970702], action=1, reward=1.0, next_state=[ 0.04681965  0.20683456  0.00728444 -0.270794  ]\n",
      "[ episode 354 ][ timestamp 262 ] state=[ 0.04681965  0.20683456  0.00728444 -0.270794  ], action=0, reward=1.0, next_state=[0.05095634 0.01160942 0.00186856 0.02417755]\n",
      "[ episode 354 ][ timestamp 263 ] state=[0.05095634 0.01160942 0.00186856 0.02417755], action=1, reward=1.0, next_state=[ 0.05118853  0.20670453  0.00235211 -0.26791524]\n",
      "[ episode 354 ][ timestamp 264 ] state=[ 0.05118853  0.20670453  0.00235211 -0.26791524], action=0, reward=1.0, next_state=[ 0.05532262  0.01154909 -0.00300619  0.02550863]\n",
      "[ episode 354 ][ timestamp 265 ] state=[ 0.05532262  0.01154909 -0.00300619  0.02550863], action=1, reward=1.0, next_state=[ 0.0555536   0.20671402 -0.00249602 -0.26812127]\n",
      "[ episode 354 ][ timestamp 266 ] state=[ 0.0555536   0.20671402 -0.00249602 -0.26812127], action=0, reward=1.0, next_state=[ 0.05968788  0.01162778 -0.00785844  0.02377335]\n",
      "[ episode 354 ][ timestamp 267 ] state=[ 0.05968788  0.01162778 -0.00785844  0.02377335], action=0, reward=1.0, next_state=[ 0.05992044 -0.1833806  -0.00738298  0.31396653]\n",
      "[ episode 354 ][ timestamp 268 ] state=[ 0.05992044 -0.1833806  -0.00738298  0.31396653], action=1, reward=1.0, next_state=[ 0.05625283  0.01184574 -0.00110365  0.01896442]\n",
      "[ episode 354 ][ timestamp 269 ] state=[ 0.05625283  0.01184574 -0.00110365  0.01896442], action=0, reward=1.0, next_state=[ 0.05648974 -0.18326036 -0.00072436  0.31129893]\n",
      "[ episode 354 ][ timestamp 270 ] state=[ 0.05648974 -0.18326036 -0.00072436  0.31129893], action=1, reward=1.0, next_state=[0.05282453 0.0118719  0.00550162 0.01838765]\n",
      "[ episode 354 ][ timestamp 271 ] state=[0.05282453 0.0118719  0.00550162 0.01838765], action=1, reward=1.0, next_state=[ 0.05306197  0.20691452  0.00586937 -0.27255438]\n",
      "[ episode 354 ][ timestamp 272 ] state=[ 0.05306197  0.20691452  0.00586937 -0.27255438], action=0, reward=1.0, next_state=[0.05720026 0.01170931 0.00041829 0.02197397]\n",
      "[ episode 354 ][ timestamp 273 ] state=[0.05720026 0.01170931 0.00041829 0.02197397], action=0, reward=1.0, next_state=[ 0.05743445 -0.18341863  0.00085777  0.31478884]\n",
      "[ episode 354 ][ timestamp 274 ] state=[ 0.05743445 -0.18341863  0.00085777  0.31478884], action=1, reward=1.0, next_state=[0.05376608 0.01169109 0.00715354 0.02237655]\n",
      "[ episode 354 ][ timestamp 275 ] state=[0.05376608 0.01169109 0.00715354 0.02237655], action=1, reward=1.0, next_state=[ 0.0539999   0.20670972  0.00760107 -0.2680408 ]\n",
      "[ episode 354 ][ timestamp 276 ] state=[ 0.0539999   0.20670972  0.00760107 -0.2680408 ], action=0, reward=1.0, next_state=[0.05813409 0.01148013 0.00224026 0.02702984]\n",
      "[ episode 354 ][ timestamp 277 ] state=[0.05813409 0.01148013 0.00224026 0.02702984], action=1, reward=1.0, next_state=[ 0.0583637   0.20656988  0.00278085 -0.26494542]\n",
      "[ episode 354 ][ timestamp 278 ] state=[ 0.0583637   0.20656988  0.00278085 -0.26494542], action=0, reward=1.0, next_state=[ 0.06249509  0.01140835 -0.00251805  0.02861331]\n",
      "[ episode 354 ][ timestamp 279 ] state=[ 0.06249509  0.01140835 -0.00251805  0.02861331], action=1, reward=1.0, next_state=[ 0.06272326  0.20656632 -0.00194579 -0.26486302]\n",
      "[ episode 354 ][ timestamp 280 ] state=[ 0.06272326  0.20656632 -0.00194579 -0.26486302], action=0, reward=1.0, next_state=[ 0.06685459  0.01147219 -0.00724305  0.02720555]\n",
      "[ episode 354 ][ timestamp 281 ] state=[ 0.06685459  0.01147219 -0.00724305  0.02720555], action=0, reward=1.0, next_state=[ 0.06708403 -0.18354514 -0.00669894  0.31759444]\n",
      "[ episode 354 ][ timestamp 282 ] state=[ 0.06708403 -0.18354514 -0.00669894  0.31759444], action=1, reward=1.0, next_state=[ 0.06341313  0.01167158 -0.00034705  0.02280646]\n",
      "[ episode 354 ][ timestamp 283 ] state=[ 0.06341313  0.01167158 -0.00034705  0.02280646], action=1, reward=1.0, next_state=[ 6.36465586e-02  2.06798504e-01  1.09080700e-04 -2.69985945e-01]\n",
      "[ episode 354 ][ timestamp 284 ] state=[ 6.36465586e-02  2.06798504e-01  1.09080700e-04 -2.69985945e-01], action=0, reward=1.0, next_state=[ 0.06778253  0.011675   -0.00529064  0.02273138]\n",
      "[ episode 354 ][ timestamp 285 ] state=[ 0.06778253  0.011675   -0.00529064  0.02273138], action=0, reward=1.0, next_state=[ 0.06801603 -0.18337068 -0.00483601  0.31374037]\n",
      "[ episode 354 ][ timestamp 286 ] state=[ 0.06801603 -0.18337068 -0.00483601  0.31374037], action=1, reward=1.0, next_state=[0.06434861 0.01181982 0.0014388  0.01953625]\n",
      "[ episode 354 ][ timestamp 287 ] state=[0.06434861 0.01181982 0.0014388  0.01953625], action=1, reward=1.0, next_state=[ 0.06458501  0.20692111  0.00182952 -0.27269237]\n",
      "[ episode 354 ][ timestamp 288 ] state=[ 0.06458501  0.20692111  0.00182952 -0.27269237], action=0, reward=1.0, next_state=[ 0.06872343  0.0117731  -0.00362433  0.02056703]\n",
      "[ episode 354 ][ timestamp 289 ] state=[ 0.06872343  0.0117731  -0.00362433  0.02056703], action=0, reward=1.0, next_state=[ 0.0689589  -0.18329669 -0.00321298  0.31210424]\n",
      "[ episode 354 ][ timestamp 290 ] state=[ 0.0689589  -0.18329669 -0.00321298  0.31210424], action=1, reward=1.0, next_state=[0.06529296 0.01187089 0.0030291  0.01840977]\n",
      "[ episode 354 ][ timestamp 291 ] state=[0.06529296 0.01187089 0.0030291  0.01840977], action=1, reward=1.0, next_state=[ 0.06553038  0.20694927  0.0033973  -0.2733159 ]\n",
      "[ episode 354 ][ timestamp 292 ] state=[ 0.06553038  0.20694927  0.0033973  -0.2733159 ], action=0, reward=1.0, next_state=[ 0.06966937  0.01177901 -0.00206902  0.0204366 ]\n",
      "[ episode 354 ][ timestamp 293 ] state=[ 0.06966937  0.01177901 -0.00206902  0.0204366 ], action=1, reward=1.0, next_state=[ 0.06990495  0.20693057 -0.00166029 -0.27289841]\n",
      "[ episode 354 ][ timestamp 294 ] state=[ 0.06990495  0.20693057 -0.00166029 -0.27289841], action=0, reward=1.0, next_state=[ 0.07404356  0.01183235 -0.00711826  0.0192604 ]\n",
      "[ episode 354 ][ timestamp 295 ] state=[ 0.07404356  0.01183235 -0.00711826  0.0192604 ], action=0, reward=1.0, next_state=[ 0.0742802  -0.1831868  -0.00673305  0.30968896]\n",
      "[ episode 354 ][ timestamp 296 ] state=[ 0.0742802  -0.1831868  -0.00673305  0.30968896], action=1, reward=1.0, next_state=[ 0.07061647  0.01203044 -0.00053927  0.01489024]\n",
      "[ episode 354 ][ timestamp 297 ] state=[ 0.07061647  0.01203044 -0.00053927  0.01489024], action=0, reward=1.0, next_state=[ 7.08570768e-02 -1.83083777e-01 -2.41466863e-04  3.07402975e-01]\n",
      "[ episode 354 ][ timestamp 298 ] state=[ 7.08570768e-02 -1.83083777e-01 -2.41466863e-04  3.07402975e-01], action=1, reward=1.0, next_state=[0.0671954  0.01204161 0.00590659 0.01464391]\n",
      "[ episode 354 ][ timestamp 299 ] state=[0.0671954  0.01204161 0.00590659 0.01464391], action=0, reward=1.0, next_state=[ 0.06743623 -0.18316454  0.00619947  0.30918456]\n",
      "[ episode 354 ][ timestamp 300 ] state=[ 0.06743623 -0.18316454  0.00619947  0.30918456], action=1, reward=1.0, next_state=[0.06377294 0.01186853 0.01238316 0.01846321]\n",
      "[ episode 354 ][ timestamp 301 ] state=[0.06377294 0.01186853 0.01238316 0.01846321], action=1, reward=1.0, next_state=[ 0.06401031  0.20681072  0.01275243 -0.27028711]\n",
      "[ episode 354 ][ timestamp 302 ] state=[ 0.06401031  0.20681072  0.01275243 -0.27028711], action=1, reward=1.0, next_state=[ 0.06814653  0.40174839  0.00734668 -0.55892073]\n",
      "[ episode 354 ][ timestamp 303 ] state=[ 0.06814653  0.40174839  0.00734668 -0.55892073], action=0, reward=1.0, next_state=[ 0.0761815   0.20652409 -0.00383173 -0.26393227]\n",
      "[ episode 354 ][ timestamp 304 ] state=[ 0.0761815   0.20652409 -0.00383173 -0.26393227], action=0, reward=1.0, next_state=[ 0.08031198  0.01145704 -0.00911038  0.02753963]\n",
      "[ episode 354 ][ timestamp 305 ] state=[ 0.08031198  0.01145704 -0.00911038  0.02753963], action=0, reward=1.0, next_state=[ 0.08054112 -0.18353308 -0.00855958  0.31733426]\n",
      "[ episode 354 ][ timestamp 306 ] state=[ 0.08054112 -0.18353308 -0.00855958  0.31733426], action=0, reward=1.0, next_state=[ 0.07687046 -0.37853208 -0.0022129   0.60730556]\n",
      "[ episode 354 ][ timestamp 307 ] state=[ 0.07687046 -0.37853208 -0.0022129   0.60730556], action=1, reward=1.0, next_state=[ 0.06929982 -0.18337925  0.00993321  0.31392645]\n",
      "[ episode 354 ][ timestamp 308 ] state=[ 0.06929982 -0.18337925  0.00993321  0.31392645], action=1, reward=1.0, next_state=[0.06563223 0.0115998  0.01621174 0.02439262]\n",
      "[ episode 354 ][ timestamp 309 ] state=[0.06563223 0.0115998  0.01621174 0.02439262], action=1, reward=1.0, next_state=[ 0.06586423  0.20648555  0.01669959 -0.26313156]\n",
      "[ episode 354 ][ timestamp 310 ] state=[ 0.06586423  0.20648555  0.01669959 -0.26313156], action=0, reward=1.0, next_state=[0.06999394 0.01112926 0.01143696 0.03477147]\n",
      "[ episode 354 ][ timestamp 311 ] state=[0.06999394 0.01112926 0.01143696 0.03477147], action=1, reward=1.0, next_state=[ 0.07021652  0.20608535  0.01213239 -0.25428114]\n",
      "[ episode 354 ][ timestamp 312 ] state=[ 0.07021652  0.20608535  0.01213239 -0.25428114], action=1, reward=1.0, next_state=[ 0.07433823  0.40103199  0.00704677 -0.54311274]\n",
      "[ episode 354 ][ timestamp 313 ] state=[ 0.07433823  0.40103199  0.00704677 -0.54311274], action=0, reward=1.0, next_state=[ 0.08235887  0.20581172 -0.00381549 -0.24821787]\n",
      "[ episode 354 ][ timestamp 314 ] state=[ 0.08235887  0.20581172 -0.00381549 -0.24821787], action=0, reward=1.0, next_state=[ 0.0864751   0.01074446 -0.00877984  0.04325913]\n",
      "[ episode 354 ][ timestamp 315 ] state=[ 0.0864751   0.01074446 -0.00877984  0.04325913], action=1, reward=1.0, next_state=[ 0.08668999  0.20599121 -0.00791466 -0.25218094]\n",
      "[ episode 354 ][ timestamp 316 ] state=[ 0.08668999  0.20599121 -0.00791466 -0.25218094], action=0, reward=1.0, next_state=[ 0.09080982  0.01098316 -0.01295828  0.03799508]\n",
      "[ episode 354 ][ timestamp 317 ] state=[ 0.09080982  0.01098316 -0.01295828  0.03799508], action=0, reward=1.0, next_state=[ 0.09102948 -0.18395059 -0.01219838  0.32656153]\n",
      "[ episode 354 ][ timestamp 318 ] state=[ 0.09102948 -0.18395059 -0.01219838  0.32656153], action=1, reward=1.0, next_state=[ 0.08735047  0.01134289 -0.00566715  0.03005687]\n",
      "[ episode 354 ][ timestamp 319 ] state=[ 0.08735047  0.01134289 -0.00566715  0.03005687], action=0, reward=1.0, next_state=[ 0.08757733 -0.18369734 -0.00506601  0.32094638]\n",
      "[ episode 354 ][ timestamp 320 ] state=[ 0.08757733 -0.18369734 -0.00506601  0.32094638], action=1, reward=1.0, next_state=[0.08390338 0.01149639 0.00135292 0.02667014]\n",
      "[ episode 354 ][ timestamp 321 ] state=[0.08390338 0.01149639 0.00135292 0.02667014], action=1, reward=1.0, next_state=[ 0.08413331  0.20659892  0.00188632 -0.26558561]\n",
      "[ episode 354 ][ timestamp 322 ] state=[ 0.08413331  0.20659892  0.00188632 -0.26558561], action=0, reward=1.0, next_state=[ 0.08826529  0.01145009 -0.00342539  0.02769168]\n",
      "[ episode 354 ][ timestamp 323 ] state=[ 0.08826529  0.01145009 -0.00342539  0.02769168], action=1, reward=1.0, next_state=[ 0.08849429  0.206621   -0.00287156 -0.26607003]\n",
      "[ episode 354 ][ timestamp 324 ] state=[ 0.08849429  0.206621   -0.00287156 -0.26607003], action=0, reward=1.0, next_state=[ 0.09262671  0.01154015 -0.00819296  0.0257058 ]\n",
      "[ episode 354 ][ timestamp 325 ] state=[ 0.09262671  0.01154015 -0.00819296  0.0257058 ], action=0, reward=1.0, next_state=[ 0.09285751 -0.18346336 -0.00767884  0.31579254]\n",
      "[ episode 354 ][ timestamp 326 ] state=[ 0.09285751 -0.18346336 -0.00767884  0.31579254], action=1, reward=1.0, next_state=[ 0.08918824  0.01176713 -0.00136299  0.02069789]\n",
      "[ episode 354 ][ timestamp 327 ] state=[ 0.08918824  0.01176713 -0.00136299  0.02069789], action=0, reward=1.0, next_state=[ 0.08942359 -0.18333525 -0.00094903  0.31295047]\n",
      "[ episode 354 ][ timestamp 328 ] state=[ 0.08942359 -0.18333525 -0.00094903  0.31295047], action=1, reward=1.0, next_state=[0.08575688 0.01180021 0.00530998 0.0199684 ]\n",
      "[ episode 354 ][ timestamp 329 ] state=[0.08575688 0.01180021 0.00530998 0.0199684 ], action=0, reward=1.0, next_state=[ 0.08599288 -0.18339749  0.00570934  0.31432194]\n",
      "[ episode 354 ][ timestamp 330 ] state=[ 0.08599288 -0.18339749  0.00570934  0.31432194], action=1, reward=1.0, next_state=[0.08232493 0.01164267 0.01199578 0.02344501]\n",
      "[ episode 354 ][ timestamp 331 ] state=[0.08232493 0.01164267 0.01199578 0.02344501], action=1, reward=1.0, next_state=[ 0.08255779  0.20659055  0.01246468 -0.2654291 ]\n",
      "[ episode 354 ][ timestamp 332 ] state=[ 0.08255779  0.20659055  0.01246468 -0.2654291 ], action=0, reward=1.0, next_state=[0.0866896  0.01129293 0.0071561  0.03115909]\n",
      "[ episode 354 ][ timestamp 333 ] state=[0.0866896  0.01129293 0.0071561  0.03115909], action=0, reward=1.0, next_state=[ 0.08691546 -0.1839309   0.00777928  0.32609123]\n",
      "[ episode 354 ][ timestamp 334 ] state=[ 0.08691546 -0.1839309   0.00777928  0.32609123], action=1, reward=1.0, next_state=[0.08323684 0.01107943 0.01430111 0.03587167]\n",
      "[ episode 354 ][ timestamp 335 ] state=[0.08323684 0.01107943 0.01430111 0.03587167], action=1, reward=1.0, next_state=[ 0.08345843  0.20599341  0.01501854 -0.25226503]\n",
      "[ episode 354 ][ timestamp 336 ] state=[ 0.08345843  0.20599341  0.01501854 -0.25226503], action=0, reward=1.0, next_state=[0.0875783  0.01066026 0.00997324 0.04511694]\n",
      "[ episode 354 ][ timestamp 337 ] state=[0.0875783  0.01066026 0.00997324 0.04511694], action=1, reward=1.0, next_state=[ 0.0877915   0.20563779  0.01087558 -0.24440273]\n",
      "[ episode 354 ][ timestamp 338 ] state=[ 0.0877915   0.20563779  0.01087558 -0.24440273], action=0, reward=1.0, next_state=[0.09190426 0.0103622  0.00598752 0.05169069]\n",
      "[ episode 354 ][ timestamp 339 ] state=[0.09190426 0.0103622  0.00598752 0.05169069], action=1, reward=1.0, next_state=[ 0.0921115   0.20539779  0.00702134 -0.23909713]\n",
      "[ episode 354 ][ timestamp 340 ] state=[ 0.0921115   0.20539779  0.00702134 -0.23909713], action=0, reward=1.0, next_state=[0.09621946 0.01017624 0.0022394  0.05579224]\n",
      "[ episode 354 ][ timestamp 341 ] state=[0.09621946 0.01017624 0.0022394  0.05579224], action=1, reward=1.0, next_state=[ 0.09642298  0.20526601  0.00335524 -0.2361833 ]\n",
      "[ episode 354 ][ timestamp 342 ] state=[ 0.09642298  0.20526601  0.00335524 -0.2361833 ], action=0, reward=1.0, next_state=[ 0.1005283   0.01009629 -0.00136843  0.05755608]\n",
      "[ episode 354 ][ timestamp 343 ] state=[ 0.1005283   0.01009629 -0.00136843  0.05755608], action=1, reward=1.0, next_state=[ 1.00730228e-01  2.05237831e-01 -2.17303754e-04 -2.35558284e-01]\n",
      "[ episode 354 ][ timestamp 344 ] state=[ 1.00730228e-01  2.05237831e-01 -2.17303754e-04 -2.35558284e-01], action=0, reward=1.0, next_state=[ 0.10483498  0.01011898 -0.00492847  0.05705609]\n",
      "[ episode 354 ][ timestamp 345 ] state=[ 0.10483498  0.01011898 -0.00492847  0.05705609], action=0, reward=1.0, next_state=[ 0.10503736 -0.18493196 -0.00378735  0.34817998]\n",
      "[ episode 354 ][ timestamp 346 ] state=[ 0.10503736 -0.18493196 -0.00378735  0.34817998], action=1, reward=1.0, next_state=[0.10133873 0.01024366 0.00317625 0.05430518]\n",
      "[ episode 354 ][ timestamp 347 ] state=[0.10133873 0.01024366 0.00317625 0.05430518], action=0, reward=1.0, next_state=[ 0.1015436  -0.18492369  0.00426236  0.34798855]\n",
      "[ episode 354 ][ timestamp 348 ] state=[ 0.1015436  -0.18492369  0.00426236  0.34798855], action=1, reward=1.0, next_state=[0.09784512 0.01013738 0.01122213 0.05665273]\n",
      "[ episode 354 ][ timestamp 349 ] state=[0.09784512 0.01013738 0.01122213 0.05665273], action=1, reward=1.0, next_state=[ 0.09804787  0.20509664  0.01235518 -0.23246851]\n",
      "[ episode 354 ][ timestamp 350 ] state=[ 0.09804787  0.20509664  0.01235518 -0.23246851], action=0, reward=1.0, next_state=[0.10214981 0.00980035 0.00770581 0.0640859 ]\n",
      "[ episode 354 ][ timestamp 351 ] state=[0.10214981 0.00980035 0.00770581 0.0640859 ], action=1, reward=1.0, next_state=[ 0.10234581  0.20481097  0.00898753 -0.22615587]\n",
      "[ episode 354 ][ timestamp 352 ] state=[ 0.10234581  0.20481097  0.00898753 -0.22615587], action=1, reward=1.0, next_state=[ 0.10644203  0.39980333  0.00446441 -0.5159903 ]\n",
      "[ episode 354 ][ timestamp 353 ] state=[ 0.10644203  0.39980333  0.00446441 -0.5159903 ], action=0, reward=1.0, next_state=[ 0.1144381   0.2046188  -0.00585539 -0.22190388]\n",
      "[ episode 354 ][ timestamp 354 ] state=[ 0.1144381   0.2046188  -0.00585539 -0.22190388], action=0, reward=1.0, next_state=[ 0.11853047  0.00958103 -0.01029347  0.06892628]\n",
      "[ episode 354 ][ timestamp 355 ] state=[ 0.11853047  0.00958103 -0.01029347  0.06892628], action=1, reward=1.0, next_state=[ 0.11872209  0.20484903 -0.00891495 -0.22698643]\n",
      "[ episode 354 ][ timestamp 356 ] state=[ 0.11872209  0.20484903 -0.00891495 -0.22698643], action=0, reward=1.0, next_state=[ 0.12281908  0.00985561 -0.01345467  0.06287111]\n",
      "[ episode 354 ][ timestamp 357 ] state=[ 0.12281908  0.00985561 -0.01345467  0.06287111], action=1, reward=1.0, next_state=[ 0.12301619  0.20516786 -0.01219725 -0.2340263 ]\n",
      "[ episode 354 ][ timestamp 358 ] state=[ 0.12301619  0.20516786 -0.01219725 -0.2340263 ], action=0, reward=1.0, next_state=[ 0.12711955  0.01022229 -0.01687778  0.0547844 ]\n",
      "[ episode 354 ][ timestamp 359 ] state=[ 0.12711955  0.01022229 -0.01687778  0.0547844 ], action=0, reward=1.0, next_state=[ 0.12732399 -0.18465364 -0.01578209  0.34209484]\n",
      "[ episode 354 ][ timestamp 360 ] state=[ 0.12732399 -0.18465364 -0.01578209  0.34209484], action=1, reward=1.0, next_state=[ 0.12363092  0.01068925 -0.00894019  0.04447725]\n",
      "[ episode 354 ][ timestamp 361 ] state=[ 0.12363092  0.01068925 -0.00894019  0.04447725], action=1, reward=1.0, next_state=[ 0.1238447   0.20593825 -0.00805065 -0.25101293]\n",
      "[ episode 354 ][ timestamp 362 ] state=[ 0.1238447   0.20593825 -0.00805065 -0.25101293], action=0, reward=1.0, next_state=[ 0.12796347  0.01093218 -0.01307091  0.03911983]\n",
      "[ episode 354 ][ timestamp 363 ] state=[ 0.12796347  0.01093218 -0.01307091  0.03911983], action=0, reward=1.0, next_state=[ 0.12818211 -0.18399992 -0.01228851  0.32765026]\n",
      "[ episode 354 ][ timestamp 364 ] state=[ 0.12818211 -0.18399992 -0.01228851  0.32765026], action=1, reward=1.0, next_state=[ 0.12450211  0.01129481 -0.00573551  0.03111757]\n",
      "[ episode 354 ][ timestamp 365 ] state=[ 0.12450211  0.01129481 -0.00573551  0.03111757], action=0, reward=1.0, next_state=[ 0.12472801 -0.18374443 -0.00511315  0.32198537]\n",
      "[ episode 354 ][ timestamp 366 ] state=[ 0.12472801 -0.18374443 -0.00511315  0.32198537], action=1, reward=1.0, next_state=[0.12105312 0.01144996 0.00132655 0.02769436]\n",
      "[ episode 354 ][ timestamp 367 ] state=[0.12105312 0.01144996 0.00132655 0.02769436], action=0, reward=1.0, next_state=[ 0.12128212 -0.18369099  0.00188044  0.32079553]\n",
      "[ episode 354 ][ timestamp 368 ] state=[ 0.12128212 -0.18369099  0.00188044  0.32079553], action=1, reward=1.0, next_state=[0.1176083  0.01140413 0.00829635 0.02870622]\n",
      "[ episode 354 ][ timestamp 369 ] state=[0.1176083  0.01140413 0.00829635 0.02870622], action=1, reward=1.0, next_state=[ 0.11783638  0.20640613  0.00887048 -0.26134762]\n",
      "[ episode 354 ][ timestamp 370 ] state=[ 0.11783638  0.20640613  0.00887048 -0.26134762], action=0, reward=1.0, next_state=[0.12196451 0.01115869 0.00364352 0.03411991]\n",
      "[ episode 354 ][ timestamp 371 ] state=[0.12196451 0.01115869 0.00364352 0.03411991], action=1, reward=1.0, next_state=[ 0.12218768  0.2062282   0.00432592 -0.25741122]\n",
      "[ episode 354 ][ timestamp 372 ] state=[ 0.12218768  0.2062282   0.00432592 -0.25741122], action=0, reward=1.0, next_state=[ 0.12631224  0.01104476 -0.0008223   0.03663302]\n",
      "[ episode 354 ][ timestamp 373 ] state=[ 0.12631224  0.01104476 -0.0008223   0.03663302], action=1, reward=1.0, next_state=[ 1.26533139e-01  2.06178491e-01 -8.96427808e-05 -2.56309237e-01]\n",
      "[ episode 354 ][ timestamp 374 ] state=[ 1.26533139e-01  2.06178491e-01 -8.96427808e-05 -2.56309237e-01], action=0, reward=1.0, next_state=[ 0.13065671  0.01105782 -0.00521583  0.03634541]\n",
      "[ episode 354 ][ timestamp 375 ] state=[ 0.13065671  0.01105782 -0.00521583  0.03634541], action=1, reward=1.0, next_state=[ 0.13087786  0.20625418 -0.00448892 -0.25797859]\n",
      "[ episode 354 ][ timestamp 376 ] state=[ 0.13087786  0.20625418 -0.00448892 -0.25797859], action=0, reward=1.0, next_state=[ 0.13500295  0.0111966  -0.00964849  0.0332851 ]\n",
      "[ episode 354 ][ timestamp 377 ] state=[ 0.13500295  0.0111966  -0.00964849  0.0332851 ], action=0, reward=1.0, next_state=[ 0.13522688 -0.18378567 -0.00898279  0.32290827]\n",
      "[ episode 354 ][ timestamp 378 ] state=[ 0.13522688 -0.18378567 -0.00898279  0.32290827], action=1, reward=1.0, next_state=[ 0.13155117  0.01146303 -0.00252462  0.02740613]\n",
      "[ episode 354 ][ timestamp 379 ] state=[ 0.13155117  0.01146303 -0.00252462  0.02740613], action=0, reward=1.0, next_state=[ 0.13178043 -0.18362262 -0.0019765   0.31929144]\n",
      "[ episode 354 ][ timestamp 380 ] state=[ 0.13178043 -0.18362262 -0.0019765   0.31929144], action=1, reward=1.0, next_state=[0.12810798 0.01152742 0.00440933 0.02598585]\n",
      "[ episode 354 ][ timestamp 381 ] state=[0.12810798 0.01152742 0.00440933 0.02598585], action=0, reward=1.0, next_state=[ 0.12833852 -0.18365748  0.00492904  0.32005671]\n",
      "[ episode 354 ][ timestamp 382 ] state=[ 0.12833852 -0.18365748  0.00492904  0.32005671], action=1, reward=1.0, next_state=[0.12466537 0.01139393 0.01133018 0.02893228]\n",
      "[ episode 354 ][ timestamp 383 ] state=[0.12466537 0.01139393 0.01133018 0.02893228], action=1, reward=1.0, next_state=[ 0.12489325  0.20635158  0.01190882 -0.26015443]\n",
      "[ episode 354 ][ timestamp 384 ] state=[ 0.12489325  0.20635158  0.01190882 -0.26015443], action=0, reward=1.0, next_state=[0.12902028 0.01106167 0.00670574 0.03626078]\n",
      "[ episode 354 ][ timestamp 385 ] state=[0.12902028 0.01106167 0.00670574 0.03626078], action=0, reward=1.0, next_state=[ 0.12924152 -0.1841558   0.00743095  0.33105187]\n",
      "[ episode 354 ][ timestamp 386 ] state=[ 0.12924152 -0.1841558   0.00743095  0.33105187], action=1, reward=1.0, next_state=[0.1255584  0.01085959 0.01405199 0.04072154]\n",
      "[ episode 354 ][ timestamp 387 ] state=[0.1255584  0.01085959 0.01405199 0.04072154], action=1, reward=1.0, next_state=[ 0.12577559  0.20577725  0.01486642 -0.24749493]\n",
      "[ episode 354 ][ timestamp 388 ] state=[ 0.12577559  0.20577725  0.01486642 -0.24749493], action=0, reward=1.0, next_state=[0.12989114 0.01044617 0.00991652 0.04983988]\n",
      "[ episode 354 ][ timestamp 389 ] state=[0.12989114 0.01044617 0.00991652 0.04983988], action=1, reward=1.0, next_state=[ 0.13010006  0.20542453  0.01091332 -0.23969788]\n",
      "[ episode 354 ][ timestamp 390 ] state=[ 0.13010006  0.20542453  0.01091332 -0.23969788], action=0, reward=1.0, next_state=[0.13420855 0.01014839 0.00611936 0.05640734]\n",
      "[ episode 354 ][ timestamp 391 ] state=[0.13420855 0.01014839 0.00611936 0.05640734], action=1, reward=1.0, next_state=[ 0.13441152  0.20518207  0.00724751 -0.23433862]\n",
      "[ episode 354 ][ timestamp 392 ] state=[ 0.13441152  0.20518207  0.00724751 -0.23433862], action=1, reward=1.0, next_state=[ 0.13851516  0.40019973  0.00256074 -0.52472667]\n",
      "[ episode 354 ][ timestamp 393 ] state=[ 0.13851516  0.40019973  0.00256074 -0.52472667], action=0, reward=1.0, next_state=[ 0.14651916  0.20504183 -0.0079338  -0.23123793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 354 ][ timestamp 394 ] state=[ 0.14651916  0.20504183 -0.0079338  -0.23123793], action=1, reward=1.0, next_state=[ 0.15061999  0.40027625 -0.01255856 -0.52641285]\n",
      "[ episode 354 ][ timestamp 395 ] state=[ 0.15061999  0.40027625 -0.01255856 -0.52641285], action=0, reward=1.0, next_state=[ 0.15862552  0.20533324 -0.02308681 -0.23771352]\n",
      "[ episode 354 ][ timestamp 396 ] state=[ 0.15862552  0.20533324 -0.02308681 -0.23771352], action=0, reward=1.0, next_state=[ 0.16273218  0.01054859 -0.02784108  0.04759867]\n",
      "[ episode 354 ][ timestamp 397 ] state=[ 0.16273218  0.01054859 -0.02784108  0.04759867], action=1, reward=1.0, next_state=[ 0.16294315  0.20605847 -0.02688911 -0.25373672]\n",
      "[ episode 354 ][ timestamp 398 ] state=[ 0.16294315  0.20605847 -0.02688911 -0.25373672], action=0, reward=1.0, next_state=[ 0.16706432  0.01133057 -0.03196385  0.03034509]\n",
      "[ episode 354 ][ timestamp 399 ] state=[ 0.16706432  0.01133057 -0.03196385  0.03034509], action=0, reward=1.0, next_state=[ 0.16729093 -0.18331876 -0.03135694  0.31277419]\n",
      "[ episode 354 ][ timestamp 400 ] state=[ 0.16729093 -0.18331876 -0.03135694  0.31277419], action=1, reward=1.0, next_state=[ 0.16362456  0.01223554 -0.02510146  0.0103695 ]\n",
      "[ episode 354 ][ timestamp 401 ] state=[ 0.16362456  0.01223554 -0.02510146  0.0103695 ], action=0, reward=1.0, next_state=[ 0.16386927 -0.1825176  -0.02489407  0.29502812]\n",
      "[ episode 354 ][ timestamp 402 ] state=[ 0.16386927 -0.1825176  -0.02489407  0.29502812], action=1, reward=1.0, next_state=[ 0.16021892  0.01295025 -0.01899351 -0.0054009 ]\n",
      "[ episode 354 ][ timestamp 403 ] state=[ 0.16021892  0.01295025 -0.01899351 -0.0054009 ], action=0, reward=1.0, next_state=[ 0.16047792 -0.18189423 -0.01910153  0.28122935]\n",
      "[ episode 354 ][ timestamp 404 ] state=[ 0.16047792 -0.18189423 -0.01910153  0.28122935], action=1, reward=1.0, next_state=[ 0.15684004  0.01349491 -0.01347694 -0.0174164 ]\n",
      "[ episode 354 ][ timestamp 405 ] state=[ 0.15684004  0.01349491 -0.01347694 -0.0174164 ], action=1, reward=1.0, next_state=[ 0.15710994  0.20880752 -0.01382527 -0.31432081]\n",
      "[ episode 354 ][ timestamp 406 ] state=[ 0.15710994  0.20880752 -0.01382527 -0.31432081], action=0, reward=1.0, next_state=[ 0.16128609  0.01388521 -0.02011168 -0.02602979]\n",
      "[ episode 354 ][ timestamp 407 ] state=[ 0.16128609  0.01388521 -0.02011168 -0.02602979], action=0, reward=1.0, next_state=[ 0.16156379 -0.18094264 -0.02063228  0.26024044]\n",
      "[ episode 354 ][ timestamp 408 ] state=[ 0.16156379 -0.18094264 -0.02063228  0.26024044], action=1, reward=1.0, next_state=[ 0.15794494  0.01446768 -0.01542747 -0.0388781 ]\n",
      "[ episode 354 ][ timestamp 409 ] state=[ 0.15794494  0.01446768 -0.01542747 -0.0388781 ], action=0, reward=1.0, next_state=[ 0.15823429 -0.18042969 -0.01620503  0.24889766]\n",
      "[ episode 354 ][ timestamp 410 ] state=[ 0.15823429 -0.18042969 -0.01620503  0.24889766], action=1, reward=1.0, next_state=[ 0.1546257   0.0149199  -0.01122708 -0.04885231]\n",
      "[ episode 354 ][ timestamp 411 ] state=[ 0.1546257   0.0149199  -0.01122708 -0.04885231], action=0, reward=1.0, next_state=[ 0.1549241  -0.18003928 -0.01220412  0.24026734]\n",
      "[ episode 354 ][ timestamp 412 ] state=[ 0.1549241  -0.18003928 -0.01220412  0.24026734], action=1, reward=1.0, next_state=[ 0.15132331  0.01525486 -0.00739878 -0.05623999]\n",
      "[ episode 354 ][ timestamp 413 ] state=[ 0.15132331  0.01525486 -0.00739878 -0.05623999], action=0, reward=1.0, next_state=[ 0.15162841 -0.17976023 -0.00852358  0.23409942]\n",
      "[ episode 354 ][ timestamp 414 ] state=[ 0.15162841 -0.17976023 -0.00852358  0.23409942], action=0, reward=1.0, next_state=[ 0.1480332  -0.37475936 -0.00384159  0.52408159]\n",
      "[ episode 354 ][ timestamp 415 ] state=[ 0.1480332  -0.37475936 -0.00384159  0.52408159], action=1, reward=1.0, next_state=[ 0.14053802 -0.17958356  0.00664004  0.23019062]\n",
      "[ episode 354 ][ timestamp 416 ] state=[ 0.14053802 -0.17958356  0.00664004  0.23019062], action=1, reward=1.0, next_state=[ 0.13694634  0.01544288  0.01124385 -0.06039043]\n",
      "[ episode 354 ][ timestamp 417 ] state=[ 0.13694634  0.01544288  0.01124385 -0.06039043], action=0, reward=1.0, next_state=[ 0.1372552  -0.17983846  0.01003605  0.23581869]\n",
      "[ episode 354 ][ timestamp 418 ] state=[ 0.1372552  -0.17983846  0.01003605  0.23581869], action=1, reward=1.0, next_state=[ 0.13365843  0.01513867  0.01475242 -0.05368174]\n",
      "[ episode 354 ][ timestamp 419 ] state=[ 0.13365843  0.01513867  0.01475242 -0.05368174], action=0, reward=1.0, next_state=[ 0.13396121 -0.18019167  0.01367879  0.24361895]\n",
      "[ episode 354 ][ timestamp 420 ] state=[ 0.13396121 -0.18019167  0.01367879  0.24361895], action=1, reward=1.0, next_state=[ 0.13035737  0.01473226  0.01855116 -0.04471815]\n",
      "[ episode 354 ][ timestamp 421 ] state=[ 0.13035737  0.01473226  0.01855116 -0.04471815], action=0, reward=1.0, next_state=[ 0.13065202 -0.18065073  0.0176568   0.25375963]\n",
      "[ episode 354 ][ timestamp 422 ] state=[ 0.13065202 -0.18065073  0.0176568   0.25375963], action=1, reward=1.0, next_state=[ 0.127039    0.01421472  0.02273199 -0.03330215]\n",
      "[ episode 354 ][ timestamp 423 ] state=[ 0.127039    0.01421472  0.02273199 -0.03330215], action=0, reward=1.0, next_state=[ 0.1273233  -0.18122572  0.02206595  0.2664654 ]\n",
      "[ episode 354 ][ timestamp 424 ] state=[ 0.1273233  -0.18122572  0.02206595  0.2664654 ], action=1, reward=1.0, next_state=[ 0.12369878  0.01357447  0.02739526 -0.01917688]\n",
      "[ episode 354 ][ timestamp 425 ] state=[ 0.12369878  0.01357447  0.02739526 -0.01917688], action=1, reward=1.0, next_state=[ 0.12397027  0.20829305  0.02701172 -0.30309196]\n",
      "[ episode 354 ][ timestamp 426 ] state=[ 0.12397027  0.20829305  0.02701172 -0.30309196], action=0, reward=1.0, next_state=[ 0.12813613  0.01279675  0.02094988 -0.00201401]\n",
      "[ episode 354 ][ timestamp 427 ] state=[ 0.12813613  0.01279675  0.02094988 -0.00201401], action=1, reward=1.0, next_state=[ 0.12839207  0.20761208  0.0209096  -0.28801407]\n",
      "[ episode 354 ][ timestamp 428 ] state=[ 0.12839207  0.20761208  0.0209096  -0.28801407], action=0, reward=1.0, next_state=[0.13254431 0.01219829 0.01514932 0.01118953]\n",
      "[ episode 354 ][ timestamp 429 ] state=[0.13254431 0.01219829 0.01514932 0.01118953], action=1, reward=1.0, next_state=[ 0.13278828  0.20709974  0.01537311 -0.27667537]\n",
      "[ episode 354 ][ timestamp 430 ] state=[ 0.13278828  0.20709974  0.01537311 -0.27667537], action=1, reward=1.0, next_state=[ 0.13693027  0.40199903  0.0098396  -0.56447025]\n",
      "[ episode 354 ][ timestamp 431 ] state=[ 0.13693027  0.40199903  0.0098396  -0.56447025], action=0, reward=1.0, next_state=[ 0.14497025  0.20674041 -0.0014498  -0.26870371]\n",
      "[ episode 354 ][ timestamp 432 ] state=[ 0.14497025  0.20674041 -0.0014498  -0.26870371], action=0, reward=1.0, next_state=[ 0.14910506  0.01163918 -0.00682388  0.02352159]\n",
      "[ episode 354 ][ timestamp 433 ] state=[ 0.14910506  0.01163918 -0.00682388  0.02352159], action=0, reward=1.0, next_state=[ 0.14933784 -0.18338425 -0.00635344  0.31404372]\n",
      "[ episode 354 ][ timestamp 434 ] state=[ 0.14933784 -0.18338425 -0.00635344  0.31404372], action=1, reward=1.0, next_state=[ 1.45670159e-01  1.18276313e-02 -7.25689828e-05  1.93639137e-02]\n",
      "[ episode 354 ][ timestamp 435 ] state=[ 1.45670159e-01  1.18276313e-02 -7.25689828e-05  1.93639137e-02], action=0, reward=1.0, next_state=[ 0.14590671 -0.18329328  0.00031471  0.31202394]\n",
      "[ episode 354 ][ timestamp 436 ] state=[ 0.14590671 -0.18329328  0.00031471  0.31202394], action=1, reward=1.0, next_state=[0.14224085 0.01182419 0.00655519 0.01944028]\n",
      "[ episode 354 ][ timestamp 437 ] state=[0.14224085 0.01182419 0.00655519 0.01944028], action=1, reward=1.0, next_state=[ 0.14247733  0.20685152  0.00694399 -0.27116722]\n",
      "[ episode 354 ][ timestamp 438 ] state=[ 0.14247733  0.20685152  0.00694399 -0.27116722], action=0, reward=1.0, next_state=[0.14661436 0.01163117 0.00152065 0.02369776]\n",
      "[ episode 354 ][ timestamp 439 ] state=[0.14661436 0.01163117 0.00152065 0.02369776], action=0, reward=1.0, next_state=[ 0.14684698 -0.18351255  0.0019946   0.31686008]\n",
      "[ episode 354 ][ timestamp 440 ] state=[ 0.14684698 -0.18351255  0.0019946   0.31686008], action=1, reward=1.0, next_state=[0.14317673 0.01158093 0.00833181 0.02480685]\n",
      "[ episode 354 ][ timestamp 441 ] state=[0.14317673 0.01158093 0.00833181 0.02480685], action=1, reward=1.0, next_state=[ 0.14340835  0.20658241  0.00882794 -0.26523569]\n",
      "[ episode 354 ][ timestamp 442 ] state=[ 0.14340835  0.20658241  0.00882794 -0.26523569], action=0, reward=1.0, next_state=[0.14754    0.01133558 0.00352323 0.03021852]\n",
      "[ episode 354 ][ timestamp 443 ] state=[0.14754    0.01133558 0.00352323 0.03021852], action=1, reward=1.0, next_state=[ 0.14776671  0.20640683  0.0041276  -0.26135071]\n",
      "[ episode 354 ][ timestamp 444 ] state=[ 0.14776671  0.20640683  0.0041276  -0.26135071], action=0, reward=1.0, next_state=[ 0.15189485  0.0112262  -0.00109941  0.03263125]\n",
      "[ episode 354 ][ timestamp 445 ] state=[ 0.15189485  0.0112262  -0.00109941  0.03263125], action=1, reward=1.0, next_state=[ 0.15211937  0.2063639  -0.00044679 -0.26039835]\n",
      "[ episode 354 ][ timestamp 446 ] state=[ 0.15211937  0.2063639  -0.00044679 -0.26039835], action=0, reward=1.0, next_state=[ 0.15624665  0.01124833 -0.00565476  0.03214362]\n",
      "[ episode 354 ][ timestamp 447 ] state=[ 0.15624665  0.01124833 -0.00565476  0.03214362], action=0, reward=1.0, next_state=[ 0.15647162 -0.18379207 -0.00501188  0.32303706]\n",
      "[ episode 354 ][ timestamp 448 ] state=[ 0.15647162 -0.18379207 -0.00501188  0.32303706], action=1, reward=1.0, next_state=[0.15279577 0.01140088 0.00144886 0.02877781]\n",
      "[ episode 354 ][ timestamp 449 ] state=[0.15279577 0.01140088 0.00144886 0.02877781], action=1, reward=1.0, next_state=[ 0.15302379  0.20650203  0.00202441 -0.26344763]\n",
      "[ episode 354 ][ timestamp 450 ] state=[ 0.15302379  0.20650203  0.00202441 -0.26344763], action=0, reward=1.0, next_state=[ 0.15715383  0.01135124 -0.00324454  0.02987313]\n",
      "[ episode 354 ][ timestamp 451 ] state=[ 0.15715383  0.01135124 -0.00324454  0.02987313], action=0, reward=1.0, next_state=[ 0.15738086 -0.18372403 -0.00264708  0.3215306 ]\n",
      "[ episode 354 ][ timestamp 452 ] state=[ 0.15738086 -0.18372403 -0.00264708  0.3215306 ], action=1, reward=1.0, next_state=[0.15370638 0.01143551 0.00378354 0.02801407]\n",
      "[ episode 354 ][ timestamp 453 ] state=[0.15370638 0.01143551 0.00378354 0.02801407], action=1, reward=1.0, next_state=[ 0.15393509  0.206503    0.00434382 -0.26347271]\n",
      "[ episode 354 ][ timestamp 454 ] state=[ 0.15393509  0.206503    0.00434382 -0.26347271], action=0, reward=1.0, next_state=[ 0.15806515  0.01131932 -0.00092564  0.03057713]\n",
      "[ episode 354 ][ timestamp 455 ] state=[ 0.15806515  0.01131932 -0.00092564  0.03057713], action=0, reward=1.0, next_state=[ 1.58291534e-01 -1.83789345e-01 -3.14095052e-04  3.22967861e-01]\n",
      "[ episode 354 ][ timestamp 456 ] state=[ 1.58291534e-01 -1.83789345e-01 -3.14095052e-04  3.22967861e-01], action=1, reward=1.0, next_state=[0.15461575 0.01133708 0.00614526 0.0301859 ]\n",
      "[ episode 354 ][ timestamp 457 ] state=[0.15461575 0.01133708 0.00614526 0.0301859 ], action=1, reward=1.0, next_state=[ 0.15484249  0.20637037  0.00674898 -0.26055181]\n",
      "[ episode 354 ][ timestamp 458 ] state=[ 0.15484249  0.20637037  0.00674898 -0.26055181], action=0, reward=1.0, next_state=[0.1589699  0.01115272 0.00153794 0.03425216]\n",
      "[ episode 354 ][ timestamp 459 ] state=[0.1589699  0.01115272 0.00153794 0.03425216], action=1, reward=1.0, next_state=[ 0.15919295  0.20625259  0.00222299 -0.25794513]\n",
      "[ episode 354 ][ timestamp 460 ] state=[ 0.15919295  0.20625259  0.00222299 -0.25794513], action=0, reward=1.0, next_state=[ 0.163318    0.01109897 -0.00293592  0.03543813]\n",
      "[ episode 354 ][ timestamp 461 ] state=[ 0.163318    0.01109897 -0.00293592  0.03543813], action=0, reward=1.0, next_state=[ 0.16353998 -0.18398076 -0.00222715  0.3271933 ]\n",
      "[ episode 354 ][ timestamp 462 ] state=[ 0.16353998 -0.18398076 -0.00222715  0.3271933 ], action=1, reward=1.0, next_state=[0.15986037 0.01117283 0.00431671 0.03380886]\n",
      "[ episode 354 ][ timestamp 463 ] state=[0.15986037 0.01117283 0.00431671 0.03380886], action=1, reward=1.0, next_state=[ 0.16008382  0.20623261  0.00499289 -0.25750898]\n",
      "[ episode 354 ][ timestamp 464 ] state=[ 0.16008382  0.20623261  0.00499289 -0.25750898], action=0, reward=1.0, next_state=[ 1.64208475e-01  1.10397381e-02 -1.57289088e-04  3.67445930e-02]\n",
      "[ episode 354 ][ timestamp 465 ] state=[ 1.64208475e-01  1.10397381e-02 -1.57289088e-04  3.67445930e-02], action=1, reward=1.0, next_state=[ 0.16442927  0.20616394  0.0005776  -0.25598796]\n",
      "[ episode 354 ][ timestamp 466 ] state=[ 0.16442927  0.20616394  0.0005776  -0.25598796], action=0, reward=1.0, next_state=[ 0.16855255  0.01103375 -0.00454216  0.0368771 ]\n",
      "[ episode 354 ][ timestamp 467 ] state=[ 0.16855255  0.01103375 -0.00454216  0.0368771 ], action=0, reward=1.0, next_state=[ 0.16877322 -0.18402277 -0.00380461  0.32812348]\n",
      "[ episode 354 ][ timestamp 468 ] state=[ 0.16877322 -0.18402277 -0.00380461  0.32812348], action=1, reward=1.0, next_state=[0.16509277 0.01115314 0.00275786 0.03424318]\n",
      "[ episode 354 ][ timestamp 469 ] state=[0.16509277 0.01115314 0.00275786 0.03424318], action=1, reward=1.0, next_state=[ 0.16531583  0.20623543  0.00344272 -0.25756834]\n",
      "[ episode 354 ][ timestamp 470 ] state=[ 0.16531583  0.20623543  0.00344272 -0.25756834], action=0, reward=1.0, next_state=[ 0.16944054  0.0110645  -0.00170865  0.03619848]\n",
      "[ episode 354 ][ timestamp 471 ] state=[ 0.16944054  0.0110645  -0.00170865  0.03619848], action=1, reward=1.0, next_state=[ 0.16966183  0.20621091 -0.00098468 -0.25702305]\n",
      "[ episode 354 ][ timestamp 472 ] state=[ 0.16966183  0.20621091 -0.00098468 -0.25702305], action=0, reward=1.0, next_state=[ 0.17378605  0.01110303 -0.00612514  0.03534913]\n",
      "[ episode 354 ][ timestamp 473 ] state=[ 0.17378605  0.01110303 -0.00612514  0.03534913], action=0, reward=1.0, next_state=[ 0.17400811 -0.18393055 -0.00541816  0.32609323]\n",
      "[ episode 354 ][ timestamp 474 ] state=[ 0.17400811 -0.18393055 -0.00541816  0.32609323], action=1, reward=1.0, next_state=[0.1703295  0.01126812 0.00110371 0.03170659]\n",
      "[ episode 354 ][ timestamp 475 ] state=[0.1703295  0.01126812 0.00110371 0.03170659], action=1, reward=1.0, next_state=[ 0.17055486  0.20637423  0.00173784 -0.2606279 ]\n",
      "[ episode 354 ][ timestamp 476 ] state=[ 0.17055486  0.20637423  0.00173784 -0.2606279 ], action=0, reward=1.0, next_state=[ 0.17468234  0.01122751 -0.00347472  0.03260266]\n",
      "[ episode 354 ][ timestamp 477 ] state=[ 0.17468234  0.01122751 -0.00347472  0.03260266], action=0, reward=1.0, next_state=[ 0.17490689 -0.18384444 -0.00282267  0.32418725]\n",
      "[ episode 354 ][ timestamp 478 ] state=[ 0.17490689 -0.18384444 -0.00282267  0.32418725], action=1, reward=1.0, next_state=[0.17123001 0.01131759 0.00366108 0.03061551]\n",
      "[ episode 354 ][ timestamp 479 ] state=[0.17123001 0.01131759 0.00366108 0.03061551], action=0, reward=1.0, next_state=[ 0.17145636 -0.18385667  0.00427339  0.3244513 ]\n",
      "[ episode 354 ][ timestamp 480 ] state=[ 0.17145636 -0.18385667  0.00427339  0.3244513 ], action=1, reward=1.0, next_state=[0.16777922 0.01120417 0.01076242 0.03311908]\n",
      "[ episode 354 ][ timestamp 481 ] state=[0.16777922 0.01120417 0.01076242 0.03311908], action=1, reward=1.0, next_state=[ 0.16800331  0.20617015  0.0114248  -0.25614886]\n",
      "[ episode 354 ][ timestamp 482 ] state=[ 0.16800331  0.20617015  0.0114248  -0.25614886], action=0, reward=1.0, next_state=[0.17212671 0.01088696 0.00630182 0.04011563]\n",
      "[ episode 354 ][ timestamp 483 ] state=[0.17212671 0.01088696 0.00630182 0.04011563], action=1, reward=1.0, next_state=[ 0.17234445  0.20591798  0.00710413 -0.25057237]\n",
      "[ episode 354 ][ timestamp 484 ] state=[ 0.17234445  0.20591798  0.00710413 -0.25057237], action=0, reward=1.0, next_state=[0.17646281 0.0106953  0.00209269 0.04434285]\n",
      "[ episode 354 ][ timestamp 485 ] state=[0.17646281 0.0106953  0.00209269 0.04434285], action=1, reward=1.0, next_state=[ 0.17667672  0.20578718  0.00297954 -0.24767908]\n",
      "[ episode 354 ][ timestamp 486 ] state=[ 0.17667672  0.20578718  0.00297954 -0.24767908], action=0, reward=1.0, next_state=[ 0.18079246  0.01062281 -0.00197404  0.04594217]\n",
      "[ episode 354 ][ timestamp 487 ] state=[ 0.18079246  0.01062281 -0.00197404  0.04594217], action=0, reward=1.0, next_state=[ 0.18100492 -0.18447078 -0.0010552   0.33800162]\n",
      "[ episode 354 ][ timestamp 488 ] state=[ 0.18100492 -0.18447078 -0.0010552   0.33800162], action=0, reward=1.0, next_state=[ 0.1773155  -0.3795777   0.00570484  0.6303516 ]\n",
      "[ episode 354 ][ timestamp 489 ] state=[ 0.1773155  -0.3795777   0.00570484  0.6303516 ], action=1, reward=1.0, next_state=[ 0.16972395 -0.18453582  0.01831187  0.33947075]\n",
      "[ episode 354 ][ timestamp 490 ] state=[ 0.16972395 -0.18453582  0.01831187  0.33947075], action=1, reward=1.0, next_state=[0.16603323 0.01032085 0.02510128 0.05261814]\n",
      "[ episode 354 ][ timestamp 491 ] state=[0.16603323 0.01032085 0.02510128 0.05261814], action=1, reward=1.0, next_state=[ 0.16623965  0.20507405  0.02615365 -0.23204064]\n",
      "[ episode 354 ][ timestamp 492 ] state=[ 0.16623965  0.20507405  0.02615365 -0.23204064], action=1, reward=1.0, next_state=[ 0.17034113  0.39981272  0.02151283 -0.51636045]\n",
      "[ episode 354 ][ timestamp 493 ] state=[ 0.17034113  0.39981272  0.02151283 -0.51636045], action=0, reward=1.0, next_state=[ 0.17833738  0.20439455  0.01118562 -0.21697674]\n",
      "[ episode 354 ][ timestamp 494 ] state=[ 0.17833738  0.20439455  0.01118562 -0.21697674], action=0, reward=1.0, next_state=[0.18242527 0.0091145  0.00684609 0.07921353]\n",
      "[ episode 354 ][ timestamp 495 ] state=[0.18242527 0.0091145  0.00684609 0.07921353], action=1, reward=1.0, next_state=[ 0.18260756  0.20413764  0.00843036 -0.2113016 ]\n",
      "[ episode 354 ][ timestamp 496 ] state=[ 0.18260756  0.20413764  0.00843036 -0.2113016 ], action=0, reward=1.0, next_state=[0.18669032 0.00889618 0.00420433 0.08402869]\n",
      "[ episode 354 ][ timestamp 497 ] state=[0.18669032 0.00889618 0.00420433 0.08402869], action=1, reward=1.0, next_state=[ 0.18686824  0.20395761  0.0058849  -0.2073248 ]\n",
      "[ episode 354 ][ timestamp 498 ] state=[ 0.18686824  0.20395761  0.0058849  -0.2073248 ], action=0, reward=1.0, next_state=[0.19094739 0.008752   0.00173841 0.08720869]\n",
      "[ episode 354 ][ timestamp 499 ] state=[0.19094739 0.008752   0.00173841 0.08720869], action=1, reward=1.0, next_state=[ 0.19112243  0.20384899  0.00348258 -0.20492527]\n",
      "[ episode 354 ][ timestamp 500 ] state=[ 0.19112243  0.20384899  0.00348258 -0.20492527], action=0, reward=-1.0, next_state=[ 0.19519941  0.00867741 -0.00061593  0.08885421]\n",
      "[ Ended! ] Episode 354: Exploration_rate=0.17043057265153258. Score=500.\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Solved! ] Score is now 500\n"
     ]
    }
   ],
   "source": [
    "# create policy\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "policy = Policy(observation_space, action_space)\n",
    "\n",
    "# create agent\n",
    "agent = Agent(policy)\n",
    "\n",
    "# play game\n",
    "for i_episode in count(1):\n",
    "    state = env.reset()\n",
    "    print(\"[ episode {} ] state={}\".format(i_episode, state))\n",
    "    for t in range(1, 10000):\n",
    "        action = agent.select_action(state)\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            reward *= -1\n",
    "        agent.remember(state, action, reward, state_next, done)\n",
    "        print(\"[ episode {} ][ timestamp {} ] state={}, action={}, reward={}, next_state={}\".format(i_episode, t, state, action, reward, state_next))\n",
    "        state = state_next\n",
    "        if done:\n",
    "            break\n",
    "    print(\"[ Ended! ] Episode {}: Exploration_rate={}. Score={}.\".format(i_episode, agent.exploration_rate, t))\n",
    "\n",
    "    agent.experience_replay()\n",
    "    # end game criteria\n",
    "    if t > env.spec.reward_threshold:\n",
    "        print(\"[ Solved! ] Score is now {}\".format(t))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('pytorch_p36': conda)",
   "language": "python",
   "name": "python361064bitpytorchp36conda143b13e29122453f97130b8bdfe91e87"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
